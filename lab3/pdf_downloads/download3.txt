A Unified Maximum Likelihood Approach for Estimating Symmetric
Properties of Discrete Distributions

Jayadev Acharya 1 Hirakendu Das 2 Alon Orlitsky 3 Ananda Theertha Suresh 4

Abstract
Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications.
Recently, researchers applied different estimators and analysis tools to derive asymptotically
sample-optimal approximations for each of these
properties. We show that a single, simple, plug-in
estimatorâ€”profile maximum likelihood (PML)â€“
is sample competitive for all symmetric properties, and in particular is asymptotically sampleoptimal for all the above properties.

1. Introduction
1.1. Symmetric distribution properties
Pk
def
Let âˆ† = {(p1 , . . . ,pk ) : pi â‰¥ 0, i=1 pi = 1, 1 â‰¤ k â‰¤ âˆž}
denote the collection of all discrete distributions over finite
or infinite support. A distribution property is a mapping
f : âˆ† â†’ R. It is symmetric if it remains unchanged under
relabeling of domain symbols, namely if it is determined
by just the probability multiset {p1 , p2 , . . . ,pk }. Many
important properties are symmetric. For example:
Support size S(p) = |{x : p(x) > 0}|, plays an important
role in population and vocabulary estimation.
P
Support coverage Sm (p) = x (1âˆ’(1âˆ’p(x))m ), the expected number of elements observed in m samples, arises
in ecological and biological studies, e.g., (Colwell et al.,
2012).
P
1
Shannon entropy H(p) =
x p(x) log p(x) , central to
information theory (Cover & Thomas, 2006), has numerous
*
Equal contribution 1 Cornell University, Ithaca, NY 2 Yahoo
Inc!, Sunnyvale, CA 3 University of California, San Diego
4
Google Research.
Correspondence to: Jayadev Acharya
<acharya@cornell.edu>, Hirakendu Das <hdas@yahooinc.com>, Alon Orlitsky <alon@ucsd.edu>, Ananda Theertha
Suresh <theertha@google.com>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

applications.
P
Distance to uniform kpâˆ’uk1 = x |p(x)âˆ’1/|X ||, where
u is the uniform distribution over the domain X of p. This
distance measure appears in the error of hypothesis testing,
and the uniform distribution is arguably one of the commonest discrete distributions.
1.2. Distribution estimation
Considerable research, over many years, has focused on estimating distribution properties. In the common setting, an
unknown underlying distribution p âˆˆ âˆ† generates n indedef
pendent samples X n = X1 , , . . . ,Xn , and the objective is
to estimate a given property f (p) as accurately as possible.
Specifically, an estimator for a distribution p over X is a
function fË† : X n â†’ R mapping observed samples to a
property estimate. The sample complexity of fË† is the smallest number of samples it requires to estimate a property f
with accuracy Îµ and confidence probability Î´, for all distributions in a collection P âŠ† âˆ†,
Ë†

def

C f (f, P, Î´, Îµ) =
n
o
min n : p(|f (p) âˆ’ fË†(X n )| â‰¥ Îµ) â‰¤ Î´ âˆ€p âˆˆ P .
The sample complexity of estimating f is the lowest sample complexity of any estimator,
Ë†

C âˆ— (f, P, Î´, Îµ) = min C f (f, P, Î´, Îµ).
fË†

By taking the median of about log 1Î´ independent estimators, the error rate can be driven down from a constant to
Î´. Therefore, the sample complexity depends on Î´ only
through a factor of at most log 1Î´ . For simplicity, we thereË†
Ë†
fore abbreviate C f (f, P, 1/3, Îµ) by C f (f, P, Îµ).
1.3. Result summary
Recent research has shown that while simple estimators for
the aforementioned properties require sample size n proportional to the support size k, more sophisticated techniques need only a sub-linear sample size n = Î˜(k/ log k).

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

However, each of the problems was approximated via different estimators and analysis techniques, that for some
properties were rather complex.
Motivated by the principle of maximum likelihood, we
show that a single, simple, plug-in estimatorâ€”profile maximum likelihood (PML) (Orlitsky et al., 2004b)â€” is competitive for estimating any symmetric property. Its sample
complexity is at most quadratically worse than that of any
estimator.
Specifically, we show that if a symmetric property can be
estimated using n samples with confidence Î´, then the PML
plug-in estimator can
âˆš estimate it using as many samples
with confidence Î´Â·e n . While this increase may seem high,
note that it is sub-exponential. We show that if a property
has an estimator that has a small bounded difference constant (how much the estimator changes when we change
one sample), then the error probability reduces exponentially with n (Please see Section 7.1). Combined, these two
facts imply that for properties with locally-smooth estimators, the PML plug-in estimator is optimal up to a constant:
C PML = Î˜(C âˆ— ). We then show that all the above properties have locally-smooth estimators, hence they can be estimated by the PML plug-in estimator with up to a constant
factor more than the optimal number of samples.
1.4. Outline
The rest of the paper is organized as follows. In Section 2
we describe existing results and those shown in this paper.
In Section 3 we formally define the quantities involved and
state the results. In Section 4 we define profiles and PML.
In Section 5, we outline the new approach. In Section 6,
we demonstrate auxiliary results for maximum likelihood
estimators. In Section 7, we outline how we apply maximum likelihood to support, support coverage, entropy, and
uniformity. In Section 8, we provide the details for support, and support coverage and in the appendix we outline
results for distance to uniformity and entropy.

2. Previous and New Results

cal frequency estimator assigns to each symbol x, the fracdef
tion pÌ‚(x) = Nx /n of times it appears in the sample xn .
For example, if x7 =bananas, empirical frequency would
assign pÌ‚(a) = 3/7, pÌ‚(n) = 2/7, and pÌ‚(b) = pÌ‚(s) = 1/7.
It can be readily shown that SML is exactly the empirical
frequency estimator.
While the SML plug-in estimator performs well in the
limit of many samples, sophisticated techniques have recently yielded more accurate estimators for several important symmetric properties.
Support size. With finitely many samples, S(p) cannot
be estimated to any accuracy as many symbols with arbitrarily small probability may not be observed. Motivated by databases, where each entry appears at least
once, (Raskhodnikova et al., 2009) considered distributions
whose non-zero probabilities are at least k1 ,
def

âˆ†â‰¥ k1 = {p âˆˆ âˆ† : p(x) âˆˆ {0} âˆª [1/k, 1]} ,
def

and estimated the normalized support SÌƒ(p) = S(p)/k.
It can be shown that C SML (SÌƒ(p), âˆ†â‰¥ k1 , Îµ) = Î˜(k log 1Îµ ).
Yet (Valiant & Valiant, 2011a;
 Wu & Yang, 2015) showed
that C âˆ— (SÌƒ(p), âˆ†â‰¥ k1 , Îµ) = Î˜ logk k Â· log2 1Îµ .
Support coverage. Here too we consider the normalized
def
coverage SÌƒm (p) = Sm (p)/m. (Good & Toulmin,
1956) proposed the Good Toulmin (GT) estimator that
achieves C GT (SÌƒm (p), âˆ†, Îµ) = m/2. Recently, (Orlitsky et al., 2016) derived a simple estimator showing that
C âˆ— (SÌƒm (p), âˆ†, Îµ) = Î˜( logmm Â· log 1Îµ ). (Zou et al., 2016)
derived a more complex estimator with similar dependence
on m but worse dependence on Îµ.
Shannon entropy. Since elements with arbitrarily small
probability can contribute to an arbitrarily high entropy,
H(p) cannot be estimated over aribtrary support with
finitely many samples. Therefore researchers are mostly
interested in estimating entropy of distributions with support size at most k.
def

2.1. Previous Results
Plug-in estimation is a general approach for estimating distribution properties. It uses the samples X n to find an approximation pÌ‚ of p, and lets f (pÌ‚) estimate f (p).
One of the most common distribution estimators, dating
back to Fisher is maximum likelihood, that for clarity we
call sequence maximum likelihood (SML) (Aldrich, 1997).
To any sample xn it assigns the distribution p that maximizes p(xn ). The SML estimate is exceedingly simple to
def
derive. The multiplicity Nx = Nx (xn ) of symbol x is the
number of times it appears in the sequence xn . The empiri-

âˆ†k = {p âˆˆ âˆ† : S(p) â‰¤ k}.
It can be shown that C SML (H(p), âˆ†k , Îµ) = Î˜( kÎµ ) (Paninski, 2003). Moreover, (Paninski, 2003) showed that
C âˆ— (H(p), âˆ†k , Îµ) is sublinear in k, (Valiant & Valiant,
2011a) showed that the optimal dependence on k is k/ log k
and (Wu & Yang, 2016; Jiao et al., 2015) obtained the
optimal dependence on both k,and Îµ, and showed that
C âˆ— (H(p), âˆ†k , Îµ) = Î˜ logk k Â· 1Îµ .
Distance to uniform. (Valiant& Valiant,
 2011b) showed
k
1
âˆ—
that C (kp âˆ’ uk1 , âˆ†k , Îµ) = O log k Â· Îµ2 , and (Jiao et al.,
2016) showed that this bound is tight.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

These results are summarized in Table 1.
Other properties were considered as well. (Bar-Yossef
et al., 2001; Acharya et al., 2015; Caferov et al., 2015;
Obremski & Skorski, 2017) estimated ReÌnyi entropy
and (Bu et al., 2016) estimated KL divergence. (Canonne,
2015) surveyed testing whether distributions have certain
properties, and (Jiao et al., 2014) studied the performance
of SML estimators for several properties. Closest to this
work in terms of approach and techniques are (Acharya
et al., 2011; 2012; 2013a;b; Valiant & Valiant, 2013; Orlitsky & Suresh, 2015) that design algorithms whose sample
complexity is provably close to the best possible regardless
of the domain size.
2.2. Profile Maximum Likelihood
Symmetric distribution properties do not depend on the
symbol labels. They are determined by a simple sufficient
statistic: the number of elements appearing any given number of times. The profile of a sequence X n , denoted Ï•(X n )
is the multiset of the multiplicities of all the symbols appearing in X n . For example, Ï•(a b r a c a d a b r a) =
{1, 1, 2, 2, 5}, as two symbols appearing once, two appearing twice, and one symbol appearing five times, removing the association of the individual symbols with the
multiplicities. Profiles are also referred to as histograms
of histograms (Batu et al., 2000), histogram order statistics (Paninski, 2003), and fingerprints (Valiant & Valiant,
2011a).
Motivated by the principle of maximum likelihood, (Orlitsky et al., 2004b; 2017b) discarded the symbol labels, and
considered the profile maximum likelihood (PML) distribution that maximizes the probability of the observed profile.
A number of PML properties were established. (Orlitsky
et al., 2004b; 2005) proved PMLâ€™s existence, consistency,
and some of its properties. (Orlitsky et al., 2004d; 2005;
Orlitsky & Pan, 2009; Pan et al., 2009) described additional properties and derived the PML distributions of several short and simple profiles. (Orlitsky et al., 2017b;c) provide a unified review of several of these results. (Anevski
et al., 2013) contains a combination of previously-known
and new results. A related distribution-estimation approach
is described in (Orlitsky et al., 2004c; 2003).
Several approaches were taken to computing the PML
distribution.
Algebraic computation was considered
in (Acharya et al., 2010). A combination of the EM and
MCMC algorithms have shown excellent results for calculating the PML distribution and applying it to support-size
estimation (Orlitsky et al., 2004a; 2006; Pan, 2012) and a
summary of some of the results appears in (Orlitsky et al.,
2017a). (Vontobel, 2012; 2014) derived the Bethe approximation of these algorithms.

Following the first draft of this work, (Vatedka & Vontobel, 2016) showed that both theoretically and empirically
plug-in estimators obtained from the PML estimate yield
good estimates for symmetric functionals of Markov distributions.
2.3. New Results
We show that replacing the SML plug-in estimator by PML
yields a unified estimator that, like the best results shown
via specialized techniques developed, is optimal.
Theorem 1. There is a unified approach based on PML
distribution that achieves the optimal sample complexity
for the problems of estimating the entropy, support, support coverage, and distance to uniformity.
We prove in Corollary 1 that the PML approach is competitive with respect to any symmetric property. For symmetric properties, these results are perhaps a justification of
Fisherâ€™s thoughts on Maximum Likelihood:

â€œOf course nobody has been able to prove that maximum
likelihood estimates are best under all circumstances. Maximum
likelihood estimates computed with all the information available
may turn out to be inconsistent. Throwing away a substantial part
of the information may render them consistent.â€
R. A. Fisherâ€™s thoughts on Maximum Likelihood (Le Cam, 1979).

To prove these PML guarantees, we establish two results
that are of interest on their own right.
â€¢ With n samples, PML estimates any symmetric property of âˆš
p with essentially the same accuracy, and at
most e3 n times the error, of any other estimator. This
follows by combining Theorem 3 with Lemma 1.
â€¢ For a large class of symmetric properties, including
all those mentioned above, if there is an estimator that
uses n samples, and has an error probability 1/3, we
design an estimator using O(n) samples, whose error probability is nearly exponential in n. We remark
that this decay is much faster than applying the median
trick. This result follows by combining McDiarmidâ€™s
inequality with Lemma 2.
Combined, these results prove that PML plug-in estimators
are sample-optimal.
We also introduce the notion of Î²-approximate ML distributions, described in Definition 1. These distributions are
more relaxed version of PML, hence may be more easily
computed, yet they provide essentially the same performance guarantees.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Property name
Entropy
Support size
Support coverage
Distance to u

f (p)
H(p)
SÌƒ(p)
SÌƒm (p)
kp âˆ’ uk1

P
âˆ†k
âˆ†â‰¥ k1
âˆ†
âˆ†X

C SML
k
Îµ

k log
m
k
Îµ2

1
Îµ

Câˆ—
k 1
log k Îµ
2
k
log k log
m
log m log
k 1
log k Îµ2

1
Îµ
1
Îµ

PML
Theorem 5 and Section 8.1
Theorem 5 and Section 8.2
Theorem 5 and Section A
Theorem 5 and Section A

Table 1. Estimation complexity for various properties up to a constant factor. For all properties shown, PML achieves the best known
results up to a constant factor. The details of where the optimal sample complexity was derived for each problem is discussed in
Section 2.1.

3. Formal Definitions and Results
In the past, different sophisticated estimators were used for
every property in Table 1. We show that the simple plug-in
estimator that uses any PML approximation pÌƒ, has optimal
performance guarantees for all these properties.
In the next theorem, assume n is at least the optimal sample
complexity of estimating entropy, support, support coverage, and distance to uniformity (given in Table 1) respectively.
âˆš
Theorem 2. For all Îµ > c/n0.2 , any plug-in exp (âˆ’ n)approximate PML pÌƒ satisfies,
Entropy
C pÌƒ (H(p), âˆ†k , Îµ)  C âˆ— (H(p), âˆ†k , Îµ),
Support size
C pÌƒ (S(p)/k, âˆ†â‰¥ k1 , Îµ)  C âˆ— (S(p)/k, âˆ†â‰¥ k1 , Îµ),

n
Lemma
âˆš 1 ((Hardy & Ramanujan, 1918)). |Î¦ |
exp(3 n).

â‰¤

For a distribution p, the probability of a profile Ï• is defined
as
X
def
p(Ï•) =
p(X n ),
X n :Ï•(X n )=Ï•

the probability of observing a sequence
with profile
P
Qn Ï•. Under i.i.d. sampling, p(Ï•) =
X n :Ï•(X n )=Ï•
i=1 p(Xi ).
For example, the probability of observing a sequence with
profile Ï• = {1, 2} is the probability of observing a sequence with one symbol appearing once, and one symbol
appearing twice. A sequence with a symbol x appearing
twice and y appearing once (e.g., x y x) has probability
p(x)2 p(y). Appropriately normalized, for any p, the probability of the profile {1, 2} is
  X
n
Y
X
3
p(a)2 p(b),
p(Xi ) =
p({1, 2}) =
1
n
i=1
a6=bâˆˆX

Ï•(X )={1,2}

(1)
Support coverage
C pÌƒ (Sm (p)/m, âˆ†, Îµ)  C âˆ— (Sm (p)/m, âˆ†, Îµ),
Distance to uniformity
C pÌƒ (kp âˆ’ uk1 , âˆ†X , Îµ)  C âˆ— (kp âˆ’ uk1 , âˆ†k , Îµ).

4. PML: Profile Maximum Likelihood
4.1. Preliminaries
For a sequence X n , recall that the multilplicity Nx is the
number of times x appears in X n . Discarding, the labels,
profile of a sequence (Orlitsky et al., 2004b) is defined below. Let Î¦n be all profiles of length-n sequences. Then,
Î¦4 = {{1, 1, 1, 1}, {1, 1, 2}, {1, 3}, {2, 2}, {4}}. In particular, a profile of a length-n sequence is an unordered
partition of n. Therefore, |Î¦n |, the number of profiles
of length-n sequences is equal to the partition number of
n. Then, by the Hardy-Ramanujan bounds on the partition
number,
For a, b > 0, denote a . b or b & a if for some universal
constant c, a/b â‰¤ c. Denote a  b if both a . b and a & b.

where the normalization factor is independent of p. The
summation is a monomial symmetric polynomial in the
probability values. See (Pan, 2012) for more examples.
4.2. PML Estimation Scheme
Recall that pX n is the distribution maximizing the probability of X n . Similarly, define (Orlitsky et al., 2004b):
def

pÏ• = max p(Ï•)
pâˆˆP

as the distribution in P that maximizes the probability of
observing a sequence with profile Ï•.
For example, for Ï• = {1, 2}. For P = âˆ†k , from (1),
X
pÏ• = arg max
p(a)2 p(b).
pâˆˆâˆ†k

a6=b

Note that in contrast, SML only maximizes one term of this
expression.
We give two examples from the table in (Orlitsky et al.,
2004b) to distinguish between SML and PML distributions,

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

and also show an instance where PML outputs distributions
with a larger support than those appearing in the sample.
Example 1. Let X = {a, b, . . . , z}. Suppose X n = x y x,
then the SML distribution is (2/3, 1/3). However, the distribution in âˆ† that maximizes the probability of the profile
Ï•(x y x) = {1, 2} is (1/2, 1/2). Another example, illustrating the power of PML to predict new symbols is X n =
a b a c, with profile Ï•(a b a c) = {1, 1, 2}. The SML distribution is (1/2, 1/4, 1/4), but the PML is a uniform distribution over 5 elements, namely (1/5, 1/5, 1/5, 1/5, 1/5).
Suppose we want to estimate a symmetric property f (p)
of an unknown distribution p âˆˆ P given n independent
samples. Our high level approach using PML is described
below.
Input: Class of distributions P, symmetric function
f (Â·), sample X n

estimation the optimal dependence is 1Îµ , whereas (Valiant
& Valiant, 2011a) yields Îµ12 . This is more prominent for
support size and support coverage, which have optimal
dependence of polylog( 1Îµ ), whereas (Valiant & Valiant,
2011a) gives a Îµ12 dependence. Besides, we analyze the
first method proposed for estimating symmetric properties,
designed from the first principles, and show that in fact it
is competitive with the optimal estimators for various problems.

5. Proof Outline
Our arguments have two components. In Section 6 we
prove a general result for the performance of plug-in estimation via maximum likelihood approaches.
Let P be a class of distributions over Z, and f : P â†’ R be
a function. For z âˆˆ Z, let

1. Compute pÏ• : arg maxpâˆˆP p(Ï•(X n )).
2. Output f (pÏ• ).
There are a few advantages of this approach (as is true with
any plug-in approach): (i) the computation of PML is agnostic to the function f at hand, (ii) there are no parameters
to be tuned, (iii) techniques such as Poisson sampling or
median tricks are not necessary, (iv) well motivated by the
maximum-likelihood principle.
Comparison to the linear-programming plug-in estimator (Valiant & Valiant, 2011a). Our approach is perhaps closest in flavor to the plug-in estimator of (Valiant &
Valiant, 2011a). Indeed, as mentioned in (Valiant, 2012),
their linear-programming estimator is motivated by the
question of estimating the PML. Their result was the first
estimator to provide sample complexity bounds in terms
of the alphabet size, and accuracy the problems of entropy
and support estimation. Before we explain the differences
of the two approaches, we briefly explain their approach.
Define, Ï•Âµ (X n ) to be the number of elements that appear
Âµ times. For example, when X n = a b r a c a d a b r a,
Ï•1 = 2, Ï•2 = 2, and Ï•5 = 1. (Valiant & Valiant, 2011a)
design a linear program that uses SML for high values of
Âµ, and formulate a linear program to find a distribution for
which E[Ï•Âµ ]â€™s are close to the observed Ï•Âµ â€™s. They then
plug-in this estimate to estimate the property. On the other
hand, our approach, by the nature of ML principle, tries to
find the distribution that best explains the entire profile of
the observed data, not just some partial characteristics. It
therefore has the potential to estimate any symmetric property and estimate the distribution closely in any distance
measures, competitive with the best possible. For example, the guarantees of the linear program approach are suboptimal in terms of the desired accuracy Îµ. For entropy

def

pz = arg max p(z)
pâˆˆP

be the maximum-likelihood estimator of z in P. Upon observing z, f (pz ) is the ML estimator of f . In Theorem 4,
we show that if there is an estimator that achieves error
probability Î´, then the ML estimator has an error probability at most Î´|Z|. We note that variations of this result in the
asymptotic statistics were studied before (see (Lehmann &
Casella, 1998)). Our contribution is to use these results in
the context of symmetric properties and show sample complexity bounds in the non-asymptotic regime.
We emphasize that, throughout this paper Z will be the set
of profiles of length n, and P will be distributions induced
over profiles by length-n i.i.d. samples. Therefore, we
have |Z| = |Î¦n |. By Lemma 1, if there is a profile based
estimator with error probability Î´, then theâˆšPML approach
will have error probability at most Î´ exp(3 n). Such arguments were used in hypothesis testing to show the existence
of competitive testing algorithms for fundamental statistical problems (Acharya et al., 2011; 2012).
At its face value this seems like a weak result. Our second
key step is to prove that for the properties we are interested,
it is possible to obtain very sharp guarantees. For example,
we show that if we can estimate the entropy to an accuracy
Â±Îµ with error probability 1/3 using n samples, then we can
estimate the entropy to accuracy Â±2Îµ with error probability
exp(âˆ’n0.9 ) using only 2n samples. Using this sharp concentration, the new error probability term dominates |Î¦n |,
and we obtain our results. The arguments for sharp concentration are based on modifications to existing estimators
and a new analysis. Most of these results are technical and
are in the appendix.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

6. Maximum Likelihood Property Estimation
We establish performance guarantees of ML property estimation in a general set-up. Recall that P is a collection
of distributions over Z, and f : P â†’ R. Given a sample Z from an unknown p âˆˆ P, we want to estimate f (p).
The maximum likelihood approach is the following twostep procedure.
1. Find pZ = arg maxpâˆˆP p(Z).
2. Output f (pZ ).
We bound the performance of this approach in the following theorem.
Theorem 3. Suppose there is an estimator fË† : Z â†’ R,
such that for any p, and Z âˆ¼ p,





(2)
Pr f (p) âˆ’ fË†(Z) > Îµ < Î´,
then
Pr (|f (p) âˆ’ f (pZ )| > 2Îµ) â‰¤ Î´ Â· |Z| .

(3)

Proof. Consider symbols with p(z) â‰¥ Î´ and p(z) < Î´
separately. A distribution p with p(z) â‰¥ Î´ outputs z with
probability
at least Î´. For (2) to hold, we must have,



Ë†
f (p) âˆ’ f (z) < Îµ. By the definition of ML, pz (z) â‰¥




p(z) â‰¥ Î´, and for (2) to hold for pz , f (pz ) âˆ’ fË†(z) < Îµ.
By the triangle inequality, for all such z,

 


 

|f (p) âˆ’ f (pz )| â‰¤ f (p) âˆ’ fË†(z) + f (pz ) âˆ’ fË†(z) â‰¤ 2Îµ.
Thus if p(z) â‰¥ Î´, then PML satisfies the required guarantee with zero probability of error, and any error occurs
only when p(z) < Î´. We bound this probability as follows.
When Z âˆ¼ p,
X
Pr (p(Z) < Î´) â‰¤
p(z) < Î´ Â· |Z| .
zâˆˆZ:p(z)<Î´

For some problems, it might be easier to just approximate
the ML, instead of finding it exactly. We define an approximation ML as follows:
Definition 1 (Î²-approximate ML). Let Î² â‰¤ 1. For Z âˆˆ Z,
pÌƒZ âˆˆ P is a Î²-approximate ML distribution if pÌƒz (z) â‰¥
Î² Â· pz (z). When Z is profiles of length-n, a Î²-approximate
PML is a distribution pÌƒÏ• such that pÌƒÏ• (Ï•) â‰¥ Î² Â· pÏ• (Ï•).
The next result proves guarantees for any Î²-approximate
ML estimator.
Theorem 4. Suppose there exists an estimator satisfying (2). For any p âˆˆ P and Z âˆ¼ p, any Î²-approximate
ML pÌƒZ satisfies:
Pr (|f (p) âˆ’ f (pÌƒZ )| > 2Îµ) â‰¤ Î´ Â· |Z|/Î².

The proof is very similar to the previous theorem and is
presented in the Appendix B.
6.1. Competitiveness of ML via Median Trick
Suppose for a property f (p), there is an estimator with
sample complexity n that achieves an accuracy Â±Îµ with
probability of error at most 1/3. The standard method to
boost the error probability is the median trick: (i) Obtain
O(log(1/Î´)) independent estimates using O(n log(1/Î´))
independent samples. (ii) Output the median of these estimates. This is an Îµ-accurate estimator of f (p) with error
probability at most Î´. By definition, estimators are a mapping from the samples to R. However, in many applications
the estimators map from a much smaller (some sufficient
statistic) of the samples. Denote by Zn the space consisting of all sufficient statistics that the estimator uses. For
example, estimators for symmetric properties, such as entropy typically use the profile of the sequence, and hence
Zn = Î¦n . Using the median-trick, we get the following
result.
Corollary 1. Let fË† : Zn â†’ R be an estimator of f (p) with
accuracy Îµ and error-probability 1/3. The ML estimator
achieves accuracy 2Îµ with probability at least 2/3 using


n0
0
> n samples.
min n :
20 log(3Zn0 )
Proof. Since n is the number of samples to get error probability 1/3, by the Chernoff bound, the error after n0 samples is at most exp(âˆ’(n0 /(20n))). Therefore, the error probability of the ML estimator for accuracy 2Îµ is at
most exp(âˆ’(n0 /(20n)))Zn0 , which we desire to be at most
1/3.
For estimators
that use the profile of sequences, |Î¦n | <
âˆš
exp(3 n). Plugging this in the previous result shows that
the PML based approach has a sample complexity of at
most O(n2 ). This result holds for all symmetric properties, independent of Îµ, and the alphabet size k. For the
problems mentioned earlier, something much better in possible, namely the PML approach is optimal up to constant
factors.

7. Sample optimality of PML
7.1. Sharp Concentration for Some Properties
To obtain sample-optimality guarantees for PML, we need
to drive the error probability down much faster than the
median trick. We achieve this by using McDiarmidâ€™s inequality stated below. Let fË† : X âˆ— â†’ R. Suppose fË† gets
n independent samples X n from an unknown distribution.
Moreover, changing one of the Xj to any Xj0 changed fË† by

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

at most câˆ— . Then McDiarmidâ€™s inequality (bounded difference inequality, (Boucheron et al., 2013)) states that,





2t2


Pr fË†(X n ) âˆ’ E[fË†(X n )] > t â‰¤ 2 exp âˆ’ 2 . (4)
ncâˆ—
This inequality can be used to show strong error probability
bounds for many problems. We mention a simple application for estimating discrete distributions.
Example 2. It is well known (Devroye & Lugosi, 2001)
that SML requires Î˜(k/Îµ2 ) samples to estimate p in `1 distance
probability
at least 2/3. In this case, fË†(X n ) =

P  Nwith
 x âˆ’ p(x), and therefore câˆ— is at most 2/n. Using
x n
McDiarmidâ€™s inequality, it follows that SML has an error
probability of Î´ = 2 exp(âˆ’k/2), while still using Î˜(k/Îµ2 )
samples.

coverage, and distance to uniformity
âˆš to an accuracy of 4Îµ
with probability at least 1 âˆ’ exp(âˆ’ n).
Proof. Let Î± = 0.05. By Lemma 2, for each property of
interest, there are estimators based on the profiles of the
samples such that using near-optimal number of samples,
they have bias Îµ and maximum change if we change any of
the samples is at most c0 nÎ± /n for some constant c0 . Hence,
by McDiarmidâ€™s inequality, an accuracy
of 2Îµ is achieved


2 1âˆ’2Î± 0 2
with probability at least 1âˆ’2 exp âˆ’2Îµ n
/c . Now
suppose pÌƒ is any Î²-approximate PML distribution. Then by
Theorem 4
Pr (|f (p)âˆ’f (pÌƒ)| > 4Îµ) <

Î´ Â· |Î¦n |
Î²

âˆš
2
2 exp(âˆ’2Îµ2 n1âˆ’2Î± /c0 + 3 n)
Î²
âˆš
â‰¤ exp(âˆ’ n),
â‰¤

Let Bn be the bias of an estimator
fË†(X n ) of f (p), namely

def 

Bn = f (p) âˆ’ E[fË†(X n )] . By the triangle inequality,




f (p) âˆ’ fË†(X n )

 


 

â‰¤ f (p) âˆ’ E[fË†(X n )] + fË†(X n ) âˆ’ E[fË†(X n )]




= Bn + fË†(X n ) âˆ’ E[fË†(X n )] .
Plugging this in (4),





2t2


Pr f (p) âˆ’ fË†(X n )] > t + Bn â‰¤ 2 exp âˆ’ 2 . (5)
ncâˆ—
With this in hand, we need to show that câˆ— can be bounded
for estimators for the properties we consider. In particular,
we will show that
Lemma 2. Let Î± > 0 be a fixed constant. For entropy,
support, support coverage, and distance to uniformity there
exist profile based estimators that use the optimal number
of samples (given in Table 1), have bias Îµ and if we change
Î±
any of the samples, changes by at most c Â· nn , where c is a
positive constant.
We prove this lemma by proposing several modifications to
the existing sample-optimal estimators. The modified estimators will preserve the sample complexity up to constant
factors and also have a small câˆ— . The proof details are given
in the appendix.
Using (5) with Lemma 2,
Theorem 5. Let n be the optimal sample complexity of estimating entropy, support, support coverage and distance to
uniformity (given in table 1) and c be a large positiveâˆš
constant. Let Îµ â‰¥ c/n0.2 , then any for any Î² > exp (âˆ’ n),
the Î²-PML estimator estimates entropy, support, support

âˆš
where âˆš
in the last step we used Îµ2 n1âˆ’2Î± & c0 n, and Î² >
exp(âˆ’ n).

8. Support and Support Coverage
We analyze both support coverage and the support estimation via a single approach. We first start with support coverage. Recall that the goal is to estimate Sm (p), the expected
number of distinct symbols that we see after observing m
samples from p. By the linearity of expectation,
X
X
Sm (p) =
E[INx (X m )>0 ] =
(1 âˆ’ (1 âˆ’ p(x))m ) .
xâˆˆX

xâˆˆX

The problem is closely related to the support coverage
problem (Orlitsky et al., 2016), where the goal is to estimate Ut (X n ), the number of new distinct symbols that we
observe in n Â· t additional samples. Hence
" n
#
X
Sm (p) = E
Ï•i + E[Ut ],
i=1

where t = (m âˆ’ n)/n. We show that the modification of
an estimator in (Orlitsky et al., 2016) is also near-optimal
and satisfies conditions in Lemma 2. We propose to use the
following estimator
SÌ‚m (p) =

n
X

Ï•i + UtSGT (X n ),

i=1

Pn

where UtSGT (X n ) = i=1 Ï•i (âˆ’t)i Pr(Z â‰¥ i) and Z is a
Poisson random variable with mean r and t = (m âˆ’ n)/n.
The above theorem also works for any Îµ & 1/n0.25âˆ’Î· for
any Î· > 0

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

We remark that the proof also holds for Binomial smoothed
random variables as discussed in (Orlitsky et al., 2016).
We need to bound the maximum coefficient and the bias to
apply Lemma 2. We first bound the maximum coefficient
of this estimator.
Lemma 3. For all n â‰¤ m/2, the maximum coefficient of
SÌ‚m (p) is at most 1 + er(tâˆ’1) .
Proof. For any i, the coefficient of Ï•i is 1 + (âˆ’t)i Pr(Z â‰¥
âˆ’r
i
Pt
i). It can be upper bounded as 1 + i=0 e i!(rt) = 1 +
er(tâˆ’1) .
The next lemma bounds the bias of the estimator.

8.2. Support Estimator
Recall that the quantity of interest in support estimation is
SÌƒ(p), which we wish to estimate to an accuracy of Îµ.
Proof of Lemma 2 for support. Note that we are interested
in distributions with all the non zero probabilities are at
least 1/k. We propose to estimate SÌƒ(p) using SÌ‚m (p)/k,
for m = k log 3Îµ . If we choose r = log 3Îµ , then by
Lemma 3, the maximum coefficient of SÌ‚m (p)/k is at most
3
k
2 m
n log Îµ , which for n â‰¥
log2 3Îµ is at most
ke
Î± log(k/21/Î± )
k Î± /k < nÎ± /n.
To bound the bias, note that for this choice of m
X
0 â‰¤ S(p) âˆ’ Sm (p) =
(1 âˆ’ p(x))m
x

Lemma 4. For all n â‰¤ m/2, the bias of the estimator is
bounded by

â‰¤

X

3

eâˆ’mp(x) â‰¤ keâˆ’ log Îµ =

x

|E[SÌ‚m (p)] âˆ’ Sm (p)| â‰¤ 2 + 2er(tâˆ’1) + min(m, S(p))eâˆ’r .
Proof. As before let t = (m âˆ’ n)/n.
E[SÌ‚m (p)] âˆ’ Sm (p)
n
X
X
=
E[Ï•i ] + E[UtSGT (X n )] âˆ’
(1 âˆ’ (1 âˆ’ p(x))m )
i=1

=

xâˆˆX

E[UtSGT (X n )]

âˆ’

X

((1 âˆ’ p(x))n âˆ’ (1 âˆ’ p(x))m ) .

kÎµ
.
3

Similarly, by Lemma 4,
1
|E[SÌ‚m (p)] âˆ’ S(p)|
k
1
1
â‰¤ |E[SÌ‚m (p)] âˆ’ Sm (p)| + |S(p) âˆ’ Sm (p)|
k
k
1
Îµ
â‰¤ (2 + 2er(tâˆ’1) + keâˆ’r ) + â‰¤ Îµ,
k
3
for all Îµ > 12nÎ± /n.

xâˆˆX

Hence by Lemma 8 and Corollary 2, in (Orlitsky et al.,
2016), we get
|E[SÌ‚m (p)] âˆ’ Sm (p)| â‰¤ 2+2er(tâˆ’1) + min(m, S(p))eâˆ’r .
Using the above two lemmas we prove results for both the
observed support coverage and support estimator.
8.1. Support Coverage Estimator
Recall that the quantity of interest in support coverage estimation is Sm (p)/m, which we wish to estimate to an accuracy of Îµ.
Proof of Lemma 2 for observed. If we choose r = log 3Îµ ,
then by Lemma 3, the maximum coefficient of SÌ‚m (p)/m
m

3

1/Î±

2 n log Îµ
is at most m
e
, which for m â‰¤ Î± n log(n/2
log(3/Îµ)
Î±
Î±
most n /m < n /n. Similarly, by Lemma 4,

)

is at

1
1
|E[SÌ‚m (p)] âˆ’ Sm (p)| â‰¤ (2 + 2er(tâˆ’1) + meâˆ’r ) â‰¤ Îµ,
m
m
for all Îµ > 6nÎ± /n.

9. Discussion and Future Directions
We studied estimation of symmetric properties of discrete
distributions using the principle of maximum likelihood,
and proved optimality of this approach for a number of
problems. A number of directions are of interest. We believe that the lower bound requirement on Îµ is perhaps an
artifact of our proof technique, and that the PML based approach is indeed optimal for all ranges of Îµ. Approximation
algorithms for estimating the PML distributions would be
a fruitful direction to pursue. Given our results, approximations stronger than exp(âˆ’Îµ2 n) would be very interesting. In the particular case when the desired accuracy is a
constant, even an exponential approximation would be sufficient for many properties. We plan to apply the heuristics proposed by (Vontobel, 2012) for various problems we
consider, and compare with the state of the art provable
methods.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Acknowledgements
The authors thank the reviewers for valuable feedback and
the NSF for support through grants CIF-1564355, CIF1619448, CRII-CIF-1657471, and a Cornell University
start-up grant. Jayadev Acharya thanks Clement Canonne,
Jiantao Jiao, and Pascal Vontobel for interesting discussions.

References

Bu, Yuheng, Zou, Shaofeng, Liang, Yingbin, and Veeravalli, Venugopal V. Estimation of KL divergence between large-alphabet distributions. In ISIT, 2016.
Caferov, Cafer, Kaya, BarÄ±sÌ§, ODonnell, Ryan, and Say,
AC Cem. Optimal bounds for estimating entropy with
pmf queries. In International Symposium on Mathematical Foundations of Computer Science, pp. 187â€“198.
Springer, 2015.

Acharya, Jayadev, Das, Hirakendu, Mohimani, Hosein, Orlitsky, Alon, and Pan, Shengjun. Exact calculation of
pattern probabilities. In ISIT, pp. 1498 â€“1502, 2010.

Cai, T Tony, Low, Mark G, et al. Testing composite hypotheses, hermite polynomials and optimal estimation of
a nonsmooth functional. The Annals of Statistics, 39(2):
1012â€“1041, 2011.

Acharya, Jayadev, Das, Hirakendu, Jafarpour, Ashkan, Orlitsky, Alon, and Pan, Shengjun. Competitive closeness
testing. COLT, 19:47â€“68, 2011.

Canonne, CleÌment L. A survey on distribution testing:
Your data is big. but is it blue? Electronic Colloquium
on Computational Complexity (ECCC), 22:63, 2015.

Acharya, Jayadev, Das, Hirakendu, Jafarpour, Ashkan,
Orlitsky, Alon, Pan, Shengjun, and Suresh,
Ananda Theertha.
Competitive classification and
closeness testing. In COLT, 2012.

Colwell, Robert K, Chao, Anne, Gotelli, Nicholas J,
Lin, Shang-Yi, Mao, Chang Xuan, Chazdon, Robin L,
and Longino, John T. Models and estimators linking
individual-based and sample-based rarefaction, extrapolation and comparison of assemblages. Journal of plant
ecology, 5(1):3â€“21, 2012.

Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and
Suresh, Ananda Theertha. Optimal probability estimation with applications to prediction and classification. In
COLT, 2013a.
Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and
Suresh, Ananda Theertha. A competitive test for uniformity of monotone distributions. In AISTATS, 2013b.
Acharya,
Jayadev,
Orlitsky,
Alon,
Suresh,
Ananda Theertha, and Tyagi, Himanshu.
The
complexity of estimating ReÌnyi entropy. In SODA,
2015.
Aldrich, John. R.a. fisher and the making of maximum likelihood 1912-1922. Statistical Science, 12(3):162â€“176,
09 1997.
Anevski, Dragi, Gill, Richard D, and Zohren, Stefan. Estimating a probability mass function with unknown labels.
arXiv preprint arXiv:1312.1200, 2013.
Bar-Yossef, Ziv, Kumar, Ravi, and Sivakumar, D. Sampling
algorithms: lower bounds and applications. In Symposium on Theory of computing, pp. 266â€“275. ACM, 2001.
Batu, Tugkan, Fortnow, Lance, Rubinfeld, Ronitt, Smith,
Warren D., and White, Patrick. Testing that distributions
are close. In FOCS, pp. 259â€“269, 2000.
Boucheron, S., Lugosi, G., and Massart, P. Concentration
Inequalities: A Nonasymptotic Theory of Independence.
OUP Oxford, 2013.

Cover, Thomas M. and Thomas, Joy A. Elements of information theory (2. ed.). Wiley, 2006.
Devroye, Luc and Lugosi, GaÌbor. Combinatorial methods
in density estimation. Springer, 2001.
Good, IJ and Toulmin, GH. The number of new species,
and the increase in population coverage, when a sample
is increased. Biometrika, 43(1-2):45â€“63, 1956.
Hardy, Godfrey H and Ramanujan, Srinivasa. Asymptotic
formulaÃ¦ in combinatory analysis. Proceedings of the
London Mathematical Society, 2(1):75â€“115, 1918.
Jiao, Jiantao, Venkat, Kartik, Han, Yanjun, and Weissman, Tsachy.
Maximum likelihood estimation of
functionals of discrete distributions. arXiv preprint
arXiv:1406.6959, 2014.
Jiao, Jiantao, Venkat, Kartik, Han, Yanjun, and Weissman,
Tsachy. Minimax estimation of functionals of discrete
distributions. IEEE Transactions on Information Theory,
61(5):2835â€“2885, 2015.
Jiao, Jiantao, Han, Yanjun, and Weissman, Tsachy. Minimax estimation of the L1 distance. In ISIT, pp. 750â€“754,
2016.
Le Cam, Lucien Marie. Maximum likelihood: an introduction. JSTOR, 1979.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Lehmann, Erich Leo and Casella, George. Theory of point
estimation, volume 31. Springer Science & Business
Media, 1998.
Obremski, Maciej and Skorski, Maciej. Renyi entropy estimation revisited. In APPROX, 2017.
Orlitsky, A., Pan, S., Sajama, Santhanam, P., Viswanathan,
K., and Zhang, J. Pattern maximum likelihood: Computation and experiments. Arxiv, 2017a.
Orlitsky, Alon and Pan, Shengjun. The maximum likelihood probability of skewed patterns. In ISIT, 2009.
Orlitsky, Alon and Suresh, Ananda Theertha. Competitive
distribution estimation: Why is good-turing good. In
NIPS, pp. 2143â€“2151, 2015.
Orlitsky, Alon, Santhanam, Narayana P., and Zhang, Junan.
Always good turing: Asymptotically optimal probability
estimation. In FOCS, 2003.
Orlitsky, Alon, Sajama, S, Santhanam, NP, Viswanathan,
K, and Zhang, Junan. Algorithms for modeling distributions over large alphabets. In ISIT, 2004a.
Orlitsky, Alon, Santhanam, Narayana P., Viswanathan, Krishnamurthy, and Zhang, Junan. On modeling profiles
instead of values. In UAI, 2004b.
Orlitsky, Alon, Santhanam, Narayana P, and Zhang, Junan. Universal compression of memoryless sources over
unknown alphabets. IEEE Transactions on Information
Theory, 50(7):1469â€“1481, 2004c.
Orlitsky,
Alon,
Santhanam,
Narayana Prasad,
Viswanathan, Krishna, and Zhang, Junan.
Low
(size) and order in distribution modeling. 2004d.
Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krishnamurthy, and Zhang, Junan. Convergence of profile based estimators. In Proceedings of the 2005 IEEE
International Symposium on Information Theory (ISIT),
pp. 1843 â€“1847, 2005.
Orlitsky,
Alon,
Santhanam,
Narayana Prasad,
Viswanathan, Krishna, and Zhang, Junan.
Theoretical and experimental results on modeling low
probabilities. In Information Theory Workshop, 2006.

Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krishnamurthy, and Zhang, Junan. On estimating the probability multiset part ii: Properties of the pattern maximum
likelihood estimator. Arxiv, 2017c.
Pan, Shengjun. On the theory and application of pattern
maximum likelihood. PhD thesis, UC San Diego, 2012.
Pan, Shengjun, Acyarya, Jayadev, and Orlitsky, Alon. The
maximum likelihood probability of unique-singleton,
ternary, and length-7 patterns. pp. 1135â€“1139, 2009.
Paninski, Liam. Estimation of entropy and mutual information. Neural computation, 15(6):1191â€“1253, 2003.
Raskhodnikova, Sofya, Ron, Dana, Shpilka, Amir, and
Smith, Adam. Strong lower bounds for approximating
distribution support size and the distinct elements problem. SIAM Journal on Computing, 39(3):813â€“842, 2009.
Timan, A. F. Theory of Approximation of Functions of a
Real Variable. Pergamon Press, 1963.
Valiant, Gregory and Valiant, Paul. Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts. In STOC, 2011a.
Valiant, Gregory and Valiant, Paul. The power of linear
estimators. In FOCS, pp. 403â€“412. IEEE, 2011b.
Valiant, Gregory and Valiant, Paul. Instance-by-instance
optimal identity testing. Electronic Colloquium on Computational Complexity (ECCC), 20:111, 2013.
Valiant, Gregory John. Algorithmic approaches to statistical questions. PhD thesis, University of California,
Berkeley, 2012.
Vatedka, Shashank and Vontobel, Pascal O. Pattern maximum likelihood estimation of finite-state discrete-time
markov chains. In ISIT, 2016.
Vontobel, Pascal O. The bethe approximation of the pattern maximum likelihood distribution. In IEEE ISIT, pp.
2012â€“2016, 2012.
Vontobel, Pascal O. The bethe and sinkhorn approximations of the pattern maximum likelihood estimate and
their connections to the valiant-valiant estimate. In Information Theory and Applications Workshop, ITA, pp.
1â€“10, 2014.

Orlitsky, Alon, Suresh, Ananda Theertha, and Wu, Yihong.
Optimal prediction of the number of unseen species.
Proceedings of the National Academy of Sciences, 2016.
doi: 10.1073/pnas.1607774113.

Wu, Yihong and Yang, Pengkun. Chebyshev polynomials,
moment matching, and optimal estimation of the unseen.
CoRR, abs/1504.01227, 2015.

Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krishnamurthy, and Zhang, Junan. On estimating the probability multiset part i: The pattern maximum likelihood
approach. Arxiv, 2017b.

Wu, Yihong and Yang, Pengkun. Minimax rates of entropy estimation on large alphabets via best polynomial
approximation. IEEE Trans. Information Theory, 62(6):
3702â€“3720, 2016.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Zou, James, Valiant, Gregory, Valiant, Paul, Karczewski,
Konrad, Chan, Siu On, Samocha, Kaitlin, Lek, Monkol,
Sunyaev, Shamil, Daly, Mark, and MacArthur, Daniel G.
Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects. Nature Communications, 7:13293 EP,
10, 2016.

white

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

A. Entropy and Distance to Uniformity

A.1. Entropy

The known optimal estimators for entropy and distance to
uniformity both depend on the best polynomial approximation of the corresponding functions and the splitting
trick (Wu & Yang, 2016; Jiao et al., 2015). Building on
their techniques, we show that a slight modification of their
estimators satisfy conditions in Lemma 2. Both these functions can be written as functionals of the form:
X
f (p) =
g(p(x)),

The following lemma is adapted from Proposition 4 in (Wu
& Yang, 2016) where we make the constants explicit.

x



where g(y) = âˆ’y log y for entropy and g(y) = y âˆ’ k1  for
uniformity.
Both (Wu & Yang, 2016; Jiao et al., 2015) first approximate
g(y) with PL,g (y) polynomial of some degree L. Clearly
a larger degree implies a smaller bias/approximation error,
but estimating a higher degree polynomial also implies a
larger statistical estimation error. Therefore, the approach
is the following:
â€¢ For small values of p(x), we estimate the polynomial
PL
PL,g (p(x)) = i=1 bi Â· (p(x))i .
â€¢ For large values of p(x) we simply use the empirical
estimator for g(p(x)).
However, it is not a priori known which symbols have high
probability and which have low probability. Hence, they
both assume that they receive 2n samples from p. They
0
0
then divide them into two set of samples, X1 , . . . , Xn , and
0
X1 , . . . , Xn . Let Nx , and Nx be the number of appearances of symbol x in the first and second half respectively.
They propose to use the estimator of the following form:
(
(
) )
X
2n
gÌ‚(X1 ) = max min
gx , fmax , 0 .

Lemma 6. Let gn = 1/(2n) and Î± > 0. Suppose c
1 =
1
1
>
2c2 , and c2 > 35, Further suppose that n3 16c
+
Î±2
c2
log k Â· log n. There exists a polynomial approximation of
âˆ’y log y with degree L = 0.25Î±, over [0, c1 logn n ] such that
Î±
maxi |bi | â‰¤
 of theentropy estimator is
n /n and the bias
at most O

c1
Î±2

+

1
c2

+

1
n3.9

k
n log n

.

Proof. Our estimator is similar to that of (Wu & Yang,
0
2016; Jiao et al., 2016) except for the case when Nx <
0
c2 log n, and Nx > c1 log n. For any p(x), and Nx and Nx
both distributed Bin(np(x)). By the Chernoff bounds for
binomial distributions, the probability of this event can be
bounded by,
 0

max Pr Nx < c2 log n, Nx > 2c2 log n â‰¤
p(x)

1

âˆš

n0.1

2c2

â‰¤

1
.
n4.9

Therefore, the additional bias the modification introduces
is at most k log k/n4.9 which is smaller than the bias term
of (Wu & Yang, 2016; Jiao et al., 2016).
The largest coefficient can be bounded by using that the
best polynomial approximation of degree L of x log x in
the interval [0, 1] has all coefficients at most 23L . Therefore, the largest change we have (after appropriately normalizing) is the largest value of bi which is
23L eL
n

2

/n

.

For L = 0.25Î± log n, this is at most

na
n .

x

where fmax is the maximum value of the property f and
ï£±
0
ï£´
x ), for Nx < c2 log n, and Nx < c1 log n,
ï£²GL,g (N

0
gx = g Nnx , for Nx < c2 log n, and Nx â‰¥ c1 log n,
ï£´
0
ï£³ Nx 
g n + gn , for Nx â‰¥ c2 log n,

The proof of Lemma 2 for entropy follows from the above

lemma and Lemma 5 and by substituting n = O logk k 1Îµ .

where gn is P
the first order bias correction term for g,
i
L
GL,g (Nx ) = i=1 bi Nx /ni is the unbiased estimator for
PL,g , and c1 and c2 are two constants which we decide
later. We remark that unlike previous works, we set gx to 0
for some values of Nx and Nx0 to ensure that câˆ— is bounded.
The following lemma bounds câˆ— for any such estimator gÌ‚.
Lemma 5. For any estimator gÌ‚ defined as above, changing
any one of the values changes the estimator by at most




2
Lg
c1 log(n)
8 max eL /n max |bi |,
,g
, gn ,
n
n

We state the following result stated in (Jiao et al., 2016).

where Lg = n maxiâˆˆN |g(i/n) âˆ’ g((i âˆ’ 1)/n)|.

A.2. Distance to Uniformity

Lemma 7. Let c1 > 2c2 , c2 = 35. There is an estimator
for distance to uniformity that changes by at most nÎ± /n
when a sample
q is changed, and the bias of the estimator is
at most O( Î±1

c1 log n
kÂ·n ).

Proof. Estimating the distance to uniformity has two regions based on Nx0 and Nx .
Case 1: k1 < c2 log n/n. In this case, we use the estimator defined in the last section for g(x) = |x âˆ’ 1/k|.

A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions

Case 2: k1 > c2 log n/n. In this case, we have a slight
change to the conditions under which we use various estimators.
 q
 0



log n
, &  Nx âˆ’
â€¢ For Nx âˆ’ k1  < c2 kn
gx = GL,g (Nx ),
 0
 q



log n
â€¢ For Nx âˆ’ k1  < c2 kn
, &  Nx âˆ’
gx = 0,
 q
 0


log n
:
â€¢ For Nx âˆ’ k1  â‰¥ c2 kn

Nx
gx = n .

1
k



<



<

1
k

q

c1 log n
kn :

q

c1 log n
kn :

The estimator proposed in (Jiao et al., 2016) is slightly different, assigning GL,g (Nx ) for the first two cases. We
design the second case to bound the maximum deviation.
q bias of their estimator was shown to be at most
 The
log n
O L1 kÂ·n
log n , which can be shown by using Equation
Equation 7.2.2 of (Timan, 1963)
!
p
Ï„ (1 âˆ’ Ï„ )
.
(6)
E|xâˆ’Ï„ |,L,[0,1] â‰¤ O
L
By our choice of c1 , c2 , our modification changes the bias
by at most 1/n4 < Îµ2 .
To bound the largest deviation, we use the fact from Lemma
2 in (Cai et al., 2011) that the largest coefficient of the best
degree-L polynomial approximation of |x| in [âˆ’1, 1] has
all coefficients at most 23L . Similar argument as with entropy yields that after appropriate normalization, the largest
difference in estimation will be at most nÎ± /n.
The proof of Lemma 2 for entropy follows from the
above
 lemma
 and Lemma 5 and by substituting n =
k 1
O log k Îµ2 .

B. Proof of Approximate ML Performance
Proof. We consider symbols such that p(z) â‰¥ Î´/Î² and
p(z) < Î´/Î² separately. For an z with p(z) â‰¥ Î´/Î², by the
definition of f (pZ ),
pÌƒz (z) â‰¥ pz (z)Î² â‰¥ p(z)Î² â‰¥ Î´.
Applying (2) to pÌƒz , we have for Z âˆ¼ pÌƒz ,





Î´ > Pr f (pÌƒz ) âˆ’ fË†(Z) > Îµ

n
o


â‰¥ pÌƒz (z) Â· I f (pÌƒz ) âˆ’ fË†(z) > Îµ

n
o


â‰¥ Î´ Â· I f (pÌƒz ) âˆ’ fË†(z) > Îµ ,

where
I is the indicator
function, and therefore,
o
n


= 0.
This implies that
I f (pÌƒz ) âˆ’ fË†(z) > Îµ




f (pÌƒz ) âˆ’ fË†(z) < Îµ. By an identical reasoning, since




p(z) > Î´/Î², we have f (p) âˆ’ fË†(z) < Îµ. By the triangle inequality,

 


 

|f (p) âˆ’ f (pÌƒz )| â‰¤ f (p) âˆ’ fË†(z) + f (pÌƒz ) âˆ’ fË†(z) < 2Îµ.
Thus if p(z) â‰¥ Î´/Î², then PML satisfies the required guarantee with zero probability of error, and any error occurs
only when p(z) < Î´/Î². We bound this probability as follows. When Z âˆ¼ p,
X
Pr (p(Z) â‰¤ Î´/Î²) â‰¤
p(z) â‰¤ Î´ Â· |Z| /Î².
zâˆˆZ:p(z)<Î´/Î²

