Post-Inference Prior Swapping

Appendix for “Post-Inference Prior Swapping”
A. Details on the IS Example (Sec. 2.1)
Here we provide details on the IS example (for a normal πf and Laplace π) given in Sec. 2.1.
We made the following statement: if pf (θ|xn ) = N (θ|m, s2 ), in order for |µh − Epf [µ̂IS
h ]| < δ, we need


1
(|µh − m| − δ)2 .
T ≥ exp
2s2
To show this, we first give an upper bound on the expected value of the maximum of T zero-mean s2 -variance
Gaussian random variables. Let {θ̃t }Tt=1 ∼ g, where g(θ) = N (θ|0, s2 ), and let Z = maxt {θ̃t }Tt=1 . Then, for
some b > 0,

T
n
oT  X
h
i
exp{bEg [Z]} ≤ Eg [exp{bZ}] = Eg max exp{bθ̃t }
≤
Eg exp{bθ̃t } = T exp{b2 s2 /2},
t

t=1

t=1

where the first inequality is due to Jensen’s inequality, and the final equality is due to the definition of a Gaussian
moment generating function. The above implies that
Eg [Z] ≤
Setting b =

q

2
s2

bs2
log T
+
.
b
2

log T , we have that
h
i
p
Eg max{θ̃t }Tt=1 = Eg [Z] ≤ s 2 log T .
t

PT
However, note that for all {θ̃t }Tt=1 , and weights {w(θ̃t )}Tt=1 (such that t=1 w(θ̃t ) = 1), the IS estimate µ̂IS
h for
h(θ) = θ must be less than or equal to maxt {θ̃t }Tt=1 (since the weighted average of {θ̃t }Tt=1 cannot be larger than
the maximum of this set). Therefore,
h
i
p
 
T
Eg µ̂IS
≤
E
max
{
θ̃
}
g
t
h
t=1 ≤ s 2 log T ,
t

and equivalently

T ≥ exp


 IS 2
1
E
µ̂
.
g
h
2s2

In our example, we wanted the expected estimate to be within δ of µh , i.e. we wanted |µh − Eg [µ̂IS
h ]| < δ ⇐⇒
δ − µh ≤ Eg [µ̂IS
h ] ≤ µh + δ, and therefore,




 IS 2
1
1
2
E
µ̂
≥
exp
(δ
−
µ
)
.
T ≥ exp
g
h
h
2s2
2s2
Finally, notice that the original statement involved samples {θ̃t }Tt=1 ∼ pf (θ|xn ) = N (m, s2 ) (instead of from
g = N (0, s2 )). But this is equivalent to setting pf (θ|xn ) = g(θ), and shifting our goal so that we want δ − |µh −
m| ≤ Epf [µ̂IS
h ] ≤ |µh − m| + δ. This gives us the desired bound:

T ≥ exp

 2
1
Epf µ̂IS
h
2
2s




≥ exp


1
2
(δ
−
|µ
−
m|)
.
h
2s2

Post-Inference Prior Swapping

B. Prior Swapping Pseudocode (for a false posterior PDF inference result p̃f (θ))
Here we give pseudocode for the prior swapping procedure, given some false posterior PDF inference result p̃f (θ),
p̃ (θ)π(θ)
using the prior swap functions ps (θ) ∝ fπf (θ) and ∇θ log ps (θ) ∝ ∇θ log p̃f (θ)+∇θ log π(θ)−∇θ log πf (θ),
as described in Sec. 2.2.
In Alg. 2, we show prior swapping via the Metropolis-Hastings algorithm, which makes repeated use of ps (θ).
In Alg. 3 we show prior swapping via Hamiltonian Monte Carlo, which makes repeated use of ∇θ log ps (θ). A
special case of Alg. 3, which occurs when we set the number of simulation steps to L = 1 (in line 6), is prior
swapping via Langevin dynamics.

1
2
3
4
5
6
7
8

1
2
3
4
5
6
7
8
9
10
11
12
13
14

Algorithm 2: Prior swapping via Metropolis-Hastings.
Input: Prior swap function ps (θ), and proposal q.
Output: Samples {θt }Tt=1 ∼ ps (θ) as T → ∞.
Initialize θ0 .
. Initialize Markov chain.
for t = 1, . . . , T do
Draw θs ∼ q(θs | θt−1 ).
. Propose new sample.
Draw u ∼ Unif([0,
1]).
n
o

s )q(θt |θs )
if u < min 1, ppss(θ
then
(θt )q(θs |θt )
Set θt ← θs .
. Accept proposed sample.

else
Set θt ← θt−1 .

. Reject proposed sample.

Algorithm 3: Prior swapping via Hamiltonian Monte Carlo.
Input: Prior swap function ps (θ), its gradient-log ∇θ log ps (θ), and step-size .
Output: Samples {θt }Tt=1 ∼ ps (θ) as T → ∞.
Initialize θ0 .
. Initialize Markov chain.
for t = 1, . . . , T do
Draw rt ∼ N (0, I).
Set (θe0 , re0 ) ← (θt−1 , rt−1 )
Set re0 ← re0 + 2 ∇θ log ps (θe0 ) .
. Propose new sample (next 4 lines).
for l = 1, . . . , L do
Set θel ← θel−1 + e
rl−1 .
Set rel ← rel−1 + ∇θ log ps (θel ).
Set reL ← reL + 2 ∇θ log ps (θeL ) .
Draw u ∼ Unif([0, 1]).
n
o
p (θe )e
r> r
e
if u < min 1, ps (θst−1L)r>L rLt−1 then
t−1
Set θt ← θbL .
. Accept proposed sample.
else
Set θt ← θt−1 .

. Reject proposed sample.

Post-Inference Prior Swapping

C. Proofs of Theoretical Guarantees
Here, we prove the theorems stated in Sec. 2.3.
T

f
Throughout this analysis, we assume that we have T samples {θ̃t }t=1
⊂ X ⊂ Rd from the false-posterior
pf (θ|xn ), and that b ∈ R+ denotes the bandwidth of our semiparametric false-posterior density estimator p̃sp
f (θ).
Let Hölder class Σ(2, L) on X be defined as the set of all ` = b2c times differentiable functions f : X → R
whose derivative f (l) satisfies

2−`

|f (`) (θ) − f (`) (θ0 )| ≤ L |θ − θ0 |

for all θ, θ0 ∈ X .

Let the class of densities P(2, L) be

P(2, L) =


Z


f ∈ Σ(2, L)  f ≥ 0, f (θ)dθ = 1 .

Let data xn = {x1 , . . . , xn } ⊂ Y ⊂ Rp , let Z ⊂ Y be any set such that xn ⊂ Z, and let FZ (L) denote the set of
densities p : Y → R that satisfy
| log p(x) − log p(x0 )| ≤ L|x − x0 |,

for all x, x0 ∈ Z.

In the following theorems, we assume that the false-posterior density pf (θ|xn ) is bounded, i.e. that there exists
some B > 0 such that pf (θ|xn ) ≤ B for all θ ∈ Rd ; that the prior swap density ps (θ) ∈ P(2, L); and that the
model family p(xn |θ) ∈ FZ (L) for some Z.
Theorem 2.1. For any α = (α1 , . . . , αk ) ⊂ Rp and k > 0 let p̃α
f (θ) be defined as in Eq. (8). Then, there exists
M > 0 such that

pf (θ|xn )
p̃α
f (θ)

< M , for all θ ∈ Rd .

Proof. To prove that there exists M > 0 such that
pf (θ|xn ) =

pf (θ|xn )
p̃α
f (θ)

< M , note that the false posterior can be written

n
n
Y
Y
1
1
L(θ|xi ) =
p(xi |θ),
πf (θ)
πf (θ)
Z1
Z1
i=1
i=1

and the parametric estimate p̃α
f (θ) is defined to be
p̃α
f (θ) =

k
Y
1
πf (θ)
p(αj |θ)n/k .
Z2
j=1

Let d = maxi,j |xi − αj |. For any i ∈ {1, . . . , n}, j ∈ {1, . . . , k},



p(xi |θ) 

| log p(xi |θ) − log p(αj |θ)| ≤ Ld =⇒ log
≤ Ld,
p(αj |θ) 
and





p(xi |θ)
p(xi |θ) 
p(xi |θ)
exp log
≤ exp log
≤ exp{Ld} =⇒
≤ exp{Ld}.
p(αj |θ)
p(αj |θ) 
p(αj |θ)
Therefore
Qn
pf (θ|xn )
Z2
Z2
i=1 p(xi |θ)
≤
≤
exp{nLd} = M.
Qk
n/k
p̃α
(θ)
Z
Z
1
1
f
j=1 p(αj |θ)

Post-Inference Prior Swapping

Corollary 2.1.1. For {θt }Tt=1 ∼ pα
s (θ) ∝

p̃α
f (θ)π(θ)
πf (θ) ,

pf (θt |xn )
p̃α
f (θt )
PT
PSis
µ̂h = t=1

w(θt ) =

that satisfies Varp [h(θ)] < ∞, the variance of IS estimate

pf (θr |xn )
T
r=1 p̃α
f (θr )

P

−1

, and test function

h(θt )w(θt ) is finite.

Proof. This follows directly from the sufficient conditions for finite variance IS estimates given by (Geweke,
1989), which we have proved are satisfied for µ̂PSis
in Theorem 2.1.
h
T

−1/(4+d)

f
Theorem 2.2. Given false posterior samples {θ̃t }t=1
∼ pf (θ|xn ) and b  Tf
n
consistent for p(θ|x ), i.e. its mean-squared error satisfies
Z

c
sp
n 2
sup
E
(ps (θ) − p(θ|x )) dθ < 4/(4+d)
n
p(θ|x )∈P(2,L)
Tf

, the estimator psp
s is

for some c > 0 and 0 < b ≤ 1.
Proof. To prove mean-square consistency of our semiparametric prior swap density estimator psp
s , we give a
bound on the mean-squared error (MSE), and show that it tends to zero as we increase the number of samples Tf
drawn from the false-posterior. To prove this, we bound the bias and variance of the estimator, and use this to
bound the MSE. In the following, to avoid cluttering notation, we will drop the subscript pf in Epf [·].
We first bound the bias of our semiparametric prior swap estimator. For any p(θ|xn ) ∈ P(2, L), we can write the
bias as

 



π(θ)
sp
n π(θ) 
n

−
p
(θ|x
)
|E [psp
(θ)]
−
p(θ|x
)|
=
c
E
p̃
(θ)
f
1
s
f
πf (θ)
πf (θ) 



 π(θ) h sp i
= c2 
E p̃f (θ) − pf (θ|xn )
πf (θ)
 h

i

n 
= c3 E p̃sp
(θ)
−
p
(θ|x
)

f
f
≤ ch2
 h

i

n 
for some c > 0, where we have used the fact that E p̃sp
(θ)
−
p
(θ|x
)
 ≤ c̃h2 for some c̃ > 0 (given in (Hjort
f
f
& Glad, 1995; Wasserman, 2006)).
We next bound the variance of our semiparametric prior swap estimator. For any p(θ|xn ) ∈ P(2, L), we can write
the variance of our estimator as


π(θ)
sp
sp
Var [ps (θ)] = c1 Var p̃f (θ)
πf (θ)
h
i
2
π(θ)
sp
=
Var
p̃
(θ)
f
πf (θ)2
c
≤
Tf hd
h
i2
h
i
sp
c
≤ c̃ for
for some c > 0, where we have used the facts that Var p̃sp
f (θ) ≤ T hd for some c > 0 and E p̃f (θ)
some c̃ > 0 (given in (Hjort & Glad, 1995; Wasserman, 2006)). Next, we will use these two results to bound the
mean-squared error of our semiparametric prior swap estimator, which shows that it is mean-square consistent.
We can write the mean-squared error as the sum of the variance and the bias-squared, and therefore,
Z

c2
n 2
E
(psp
(θ)
−
p(θ|x
))
dθ
≤ c1 h2 +
s
T hd
c
= 4/(4+d)
Tf
−1/(4+d)

for some c > 0, using the fact that h  Tf

.

Post-Inference Prior Swapping

D. Further Empirical Results
Here we show further empirical results on a logistic regression model with hierarchical target prior given by
π = N (0, α−1 I), α ∼ Gamma(γ, 1). We use synthetic data so that we are able to compare the timing and
posterior error of different methods as we tune n and d.
In this experiment, we assume that we are given samples from a false posterior pf (θ|xn ), and we want to mostefficiently compute the target posterior under prior π(θ). In addition to the prior swapping methods, we can
run standard iterative inference algorithms, such as MCMC or variational inference (VI), on the target posterior
(initializing them, for example, at the false posterior mode) as comparisons. The following experiments aim to
show that, once the data size n grows large enough, prior swapping methods become more efficient than standard
inference algorithms. They also aim to show that the held-out test error of prior swapping matches that of these
standard inference algorithms. In these experiments, we also add a prior swap method called prior swapping VI;
this method involves making a VI approximation to pf (θ|xn ), and using it for p̃f (θ). Prior swapping VI allows us
to see whether the test error is similar to standard VI inference algorithms, which compute some approximation
to the posterior. Finally, we show results over a range of target prior hyperparameter values γ to show that prior
swapping maintains accuracy (i.e. has a similar error as standard inference algorithms) over the full range.
We show results in Fig. 6. In (a) and (b) we vary the number of observations (n=10-120,000) and see that prior
swapping has a constant wall time while the wall times of both MCMC and VI increase with n. In (b) we see that
the prior swapping methods achieve the same test error as the standard inference methods. In (c) and (d) we vary
the number of dimensions (d=1-40). In this case, all methods have increasing wall time, and again the test errors
match. In (e), (f), and (g), we vary the prior hyperparameter (γ=1-1.05). For prior swapping, we infer a single
p̃f (θ) (using γ = 1.025) with both MCMC and VI applied to pf (θ|xn ), and compute all other hyperparameter
results using this p̃f (θ). This demonstrates that prior swapping can quickly infer correct results over a range
of hyperparameters. Here, the prior swapping semiparametric method matches the test error of MCMC slightly
better than the parametric method.
(b)0.16

2,3

Test Error

0.14
0.13
0.12

6

2

4

6

8

Number of observations

10

1,4,5

0.11

3,4,5

2

4

0.1

x10 4
4

(d)0.3

10

0.09
0

x 10

2

4

6

8

x10 4

10

Number of observations

0.25

9

1

8

Test Error

1

8

2
0

(c) 11

0.15

10

Log wall time (log(seconds))

Log wall time (log(seconds))

(a) 12

3,4,5

7

2,3

0.2

0.15

6
5

0.1

1,4,5

2

4
3
0

5

4

x 10

10

15

20

25

30

Number of dimensions

0.05
0

35

5

10

15

20

25

30

Number of dimensions

35

5

10

9

1

8

2,3,4,5

7

6

1

1.01

1.02

1.03

Hyperparameter

1.04

1.05

10

x 10

(g)

0.17
0.16

8

0.15

Test Error

(f)
Total wall time (seconds)

1. Target posterior inf. (MCMC)
2. Target posterior inf. (VI)
3. Prior swap VI
4. Prior swap semiparametric
5. Prior swap parametric

Log wall time (log(seconds))

(e)

11

6

4

2,3

0.14
0.13
0.12

1,4,5

0.11
2
0.1
0

1.
MCMC

2.
VI

3. PS 4. PS 5. PS
VI Semi Para

0.09

1

1.01

1.02

1.03

1.04

1.05

Hyperparameter

Figure 6. Bayesian hierarchical logistic regression: (a-b) Wall time and test error comparisons for varying data size n. As n
is increased, wall time remains constant for prior swapping but grows for standard inference methods. (c-d) Wall time and test
error comparisons for varying model dimensionality d. (e-g) Wall time and test error comparisons for inferences on a set of
prior hyperparameters γ ∈ [1, 1.05]. Here, a single false posterior p̃f (θ) (computed at γ = 1.025) is used for prior swapping
on all other hyperparameters.

