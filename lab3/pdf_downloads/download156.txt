AdaNet: Adaptive Structural Learning of Artificial Neural Networks

A. Related work
There have been several major lines of research on the theoretical understanding of neural networks. The first one
deals with understanding the properties of the objective
function used when training neural networks (Choromanska et al., 2014; Sagun et al., 2014; Zhang et al., 2015; Livni
et al., 2014; Kawaguchi, 2016). The second involves studying the black-box optimization algorithms that are often
used for training these networks (Hardt et al., 2015; Lian
et al., 2015). The third analyzes the statistical and generalization properties of neural networks (Bartlett, 1998;
Zhang et al., 2016; Neyshabur et al., 2015; Sun et al., 2016).
The fourth adopts a generative point of view assuming that
the data actually comes from a particular network, which it
shows how to recover (Arora et al., 2014; 2015). The fifth
investigates the expressive ability of neural networks, analyzing what types of mappings they can learn (Cohen et al.,
2015; Eldan & Shamir, 2015; Telgarsky, 2016; Daniely
et al., 2016). This paper is most closely related to the
work on statistical and generalization properties of neural
networks. However, instead of analyzing the problem of
learning with a fixed architecture, we study a more general
task of learning both architecture and model parameters simultaneously. On the other hand, the insights that we gain
by studying this more general setting can also be directly
applied to the setting with a fixed architecture.
There has also been extensive work involving structure
learning for neural networks (Kwok & Yeung, 1997; Leung et al., 2003; Islam et al., 2003; Lehtokangas, 1999; Islam et al., 2009; Ma & Khorasani, 2003; Narasimha et al.,
2008; Han & Qiao, 2013; Kotani et al., 1997; Alvarez &
Salzmann, 2016). All these publications seek to grow and
prune the neural network architecture using some heuristic. More recently, search-based approaches have been an
area of active research (Ha et al., 2016; Chen et al., 2015;
Zoph & Le, 2016; Baker et al., 2016). In this line of work,
a learning meta-algorithm is used to search for an efficient
architecture. Once a better architecture is found, previously
trained networks are discarded. This search requires a significant amount of computational resources. Additionally,
(Saxena & Verbeek, 2016) presented an approach to overcome the tedious process of exploring individual network
architectures via a so-called fabric that embeds an exponentially large number of architectures.
To the best of our knowledge, none of these methods comes
with a theoretical guarantee on their performance. Furthermore, optimization problems associated with these methods are intractable. In contrast, the structure learning algorithms introduced in this paper are directly based on datadependent generalization bounds and aim to solve a convex
optimization problem by adaptively growing the network
and preserving previously trained components.

Finally, (Janzamin et al., 2015) is another paper that analyzes the generalization and training of two-layer neural
networks through tensor methods. Our work uses different
methods, applies to arbitrary networks, and also learns a
network structure from a single input layer.

B. Proofs
We will use the following structural learning guarantee for
ensembles of hypotheses.
Theorem 2 (DeepBoost Generalization Bound, Theorem 1,
(Cortes et al., 2014)). Let H be a hypothesis set admitting a decomposition H = ∪li=1 Hi for some l > 1. Fix
ρ > 0. Then, for any δ > 0, with probability at least 1 − δ
over the draw of a sample P
S from Dm , the following inT
equality holds for any f = t=1 αt ht with αt ∈ R+ and
PT
t=1 αt = 1:
r
T
2 log l
4X
b
αt Rm (Hkt ) +
R(f ) ≤ RS,ρ +
ρ t=1
ρ
m
s
 2 
4
ρ m
log l log( 2δ )
+
log
+
,
ρ2
log l
m
2m
where, for each ht ∈ H, kt denotes the smallest k ∈ [l]
such that ht ∈ Hkt .
Theorem 1. Fix ρ > 0. Then, for any δ > 0, with
probability at least 1 − δ over the draw of a sample S
of size
from Dm , the following inequality holds for all
Pm
l
f = k=1 wk · hk ∈ F:
l

X 
e k) + 2
bS,ρ (f ) + 4
wk  Rm (H
R(f ) ≤ R
1
ρ
ρ

r

k=1

log l
m

+ C(ρ, l, m, δ),
where C(ρ, l, m, δ) =
 q

e 1 log l .
O
ρ
m

q

4
ρ2

 l
2
log( ρlogml ) log
m +

log( δ2 )
2m

=

Proof. This result follows directly from Theorem 2.
Theorem 1 can be straightforwardly generalized to the
multi-class classification setting by using the ensemble
margin bounds of Kuznetsov et al. (2014).
Lemma 1. For any k > 1, the empirical Rademacher
complexity of Hk for a sample S of size m can be upperbounded as follows in terms of those of Hs s with s < k:
b S (Hk ) ≤ 2
R

k−1
X
s=1

1

b S (Hs ).
Λk,s nsq R

AdaNet: Adaptive Structural Learning of Artificial Neural Networks

b S (Hk ) can be expressed as follows:
Proof. By definition, R


m
k−1
X X

b S (Hk ) = 1 E
R
σi
us · (ϕs ◦ hs )(xi ).
 supn
σ
m
hs ∈Hs s i=1
s=1
kus kp ≤Λk,s

By the sub-additivity of the supremum, it can be upperbounded as follows:


m
k−1
X
X 1 

b S (Hk ) ≤
E sup
σi us · (ϕs ◦ hs )(xi ).
R
n
σ
s
m
h
∈H
s
s
s=1
i=1
kus kp ≤Λk,s

We now bound each term of this sum, starting with the following chain of equalities:


m
X
1 

E sup
σi us · (ϕs ◦ hs )(xi )
m σ hs ∈Hsns i=1
kus kp ≤Λk,s

Λk,s
=
E
m σ

 #
X

 m
σi (ϕs ◦ hs )(xi )
sup 

ns 

"

hs ∈Hs

1
q

=

Λk,s ns
E
m σ

h∈Hs

i=1



1
q

=

q

i=1

#
 m

X
sup 
σi (ϕs ◦ h)(xi )

"

Λk,s ns 
E
m σ

sup

h∈Hs
σ∈{−1,+1}

σ

m
X



σi (ϕs ◦ h)(xi ) ,

i=1

where the second equality holds by definition of the dual
norm and the third equality by the following equality:
n
n
hX
i q1
hX
i q1
sup kzkq = sup
|zi |q =
[ sup |zi |]q
zi ∈Z

zi ∈Z

i=1

i=1 zi ∈Z

1
q

= n sup |zi |.
zi ∈Z

The following chain of inequalities concludes the proof:


1
m
q
X
Λk,s ns 

σi (ϕs ◦ h)(xi )
E  sup σ
σ
m
h∈Hs
i=1
σ∈{−1,+1}

1
q

"
#
m
X
Λk,s ns
≤
E sup
σi (ϕs ◦ h)(xi )
m σ h∈Hs i=1
"
#
m
X
Λk,s
+
E sup
−σi (ϕj ◦ h)(xi )
m σ h∈Hs i=1
"
#
1
m
X
2Λk,s nsq
=
E sup
σi (ϕs ◦ h)(xi )
σ h∈H
m
s i=1
"
#
1
m
X
2Λk,s nsq
≤
E sup
σi h(xi )
σ h∈H
m
s i=1
1

b S (Hs ),
≤ 2Λk,s nsq R

where the second inequality holds by Talagrand’s contraction lemma.
Qk
Qk
Lemma 2. Let Λk = s=1 2Λs,s−1 and Nk = s=1 ns−1 .
Then, for any k ≥ 1, the empirical Rademacher complexity
of Hk∗ for a sample S of size m can be upper bounded as
follows:
r
1
log(2n0 )
q
∗
b
.
RS (Hk ) ≤ r∞ Λk Nk
2m
Proof. The empirical Rademacher complexity of H1 can
be bounded as follows:
#
"
m
X
1
b S (H1 ) =
R
sup
σi u · Ψ(xi )
E
m σ kukp ≤Λ1,0 i=1
"
#
m
X
1
=
E
sup u ·
σi Ψ(xi )
m σ kukp ≤Λ1,0
i=1
" m
 #

X
Λ1,0
σi [Ψ(xi )]
=
E 


m σ
q
i=1
" m
1
 #

X
Λ1,0 n0q
≤
σi [Ψ(xi )]
E 


m σ
∞
i=1
"
1

#
m
X

Λ1,0 n0q
=
E max 
σi [Ψ(xi )]j 
m σ j∈[1,n1 ]  i=1


1
m
q
X
Λ1,0 n0 

=
E  max
σi s[Ψ(xi )]j 
m σ j∈[1,n1 ] i=1
s∈{−1,+1}

√

p

2 log(2n0 )
≤ Λ1,0 n0 r∞ m
m
r
1
2 log(2n0 )
= r∞ Λ1,0 n0q
.
m
1
q

The result then follows by application of Lemma 1.
Qk
Corollary 1. Fix ρ > 0. Let Λk = s=1 4Λs,s−1 and
Qk
Nk = s=1 ns−1 . Then, for any δ > 0, with probability at
least 1 − δ over the draw of a sample S of P
size m from Dm ,
l
the following inequality holds for all f = k=1 wk · hk ∈
F∗ :
r


l
1

2 X
2 log(2n0 )
q
b
wk  r∞ Λk N
R(f ) ≤ RS,ρ (f ) +
k
1
ρ
m
k=1
r
2 log l
+
+ C(ρ, l, m, δ),
ρ
m
q
 log l log( δ2 )
ρ2 m
4
where C(ρ, l, m, δ) =
=
ρ2 log( log l ) m + 2m
 q

e 1 log l , and where r∞ = ES∼Dm [r∞ ].
O
ρ
m

AdaNet: Adaptive Structural Learning of Artificial Neural Networks

optimization problem:
min Ft (w, h).

argmin
l

t−1
h∈∪s=1

+1

Hs0

w∈R

Remarkably, the subnetwork that solves this infinite dimensional optimization problem can be obtained directly
in closed-form:
Figure 4. Illustration of a neural network designed by
A DA N ET.CVX. Units at each layer (other than the output
layer) are only connected to units in the layer below.

Proof. Since F∗ is the convex hull of H∗ , we can apply
e ∗ ) instead of Rm (H
e k ). Observe
Theorem 1 with Rm (H
k
∗
e is the union of H∗ and
that, since for any k ∈ [l], H
k
k
e ∗ ) from a bound
its reflection, to derive a bound on Rm (H
k
e k ) it suffices to double each Λs,s−1 . Combining
on Rm (H
this observation with the bound of Lemma 2 completes the
proof.

Theorem 3 (A DA N ET.CVX Optimization). Let (w∗ , h∗ )
be the solution to the following optimization problem:
argmin min Ft (w, h).
l

t−1
h∈∪s=1
Hs0

Let Dt be a distribution over the sample (xi , yi )m
i=1 such
that Dt (i) ∝ Φ0 (1 − yi ft−1 (xi )), and denote t,h =
Ei∼Dt [yi h(xi )].
Then,
∗

In this section, we present an alternative algorithm,
A DA N ET.CVX, that generates candidate subnetworks in
closed-form using Banach space duality.
As in Section 5, let ft−1 denote the A DA N ET model after
t − 1 rounds, and let lt−1 be the depth of the architecture.
A DA N ET.CVX will consider lt−1 + 1 candidate subnetworks, one for each layer in the model plus an additional
one for extending the model.
Let h(s) denote the candidate subnetwork associated to
layer s ∈ [lt−1 + 1]. We define h(s) to be a single unit
in layer s that is connected to units of ft−1 in layer s − 1:

∗

w∗ h∗ = w(s ) h(s ) ,
∗

C. Alternative algorithm

w∈R

∗

where (w(s ) , h(s ) ) are defined by:
s∗ = argmax Λs,s−1 kt,hs−1,t−1 kq .
s∈[lt −1]
(s)

ui

=

Λs,s−1 |t,hs−1,t−1,i |q−1 sgn(t,hs−1,t−1,i )
q

kt,hs−1,t−1 kqp
∗

h(s

)

∗

= u(s

)

· (ϕs ◦ hs−1,t−1 )
m
∗
1 X 
w(s ) = argmin
Φ 1 − yi ft−1 (xi )
w∈R m i=1

∗
− yi wh(s ) (xi ) + Γs∗ |w|.

Proof. By definition,

h(s) ∈ {x 7→ u · (ϕs−1 ◦ hs−1,t−1 )(x) :
u ∈ Rns−1,t−1 , kukp ≤ Λs,s−1 }.
See Figure 4 for an illustration of the type of neural network
designed using these candidate subnetworks.
For convenience, we denote this space of subnetworks by
Hs0 :
Hs0 = {x 7→ u · (ϕs−1 ◦ hs−1,t−1 )(x) :
u ∈ Rns−1,t−1 , kukp ≤ Λs,s−1 }.
Now, recall the notation
Ft (w, h)
m

1 X 
=
Φ 1 − yi (ft−1 (xi ) − wh(xi )) + Γh |w|
m i=1
used in Section 5. As in A DA N ET, the candidate subnetwork chosen by A DA N ET.CVX is given by the following

Ft (w, h)
m

1 X 
Φ 1 − yi ft−1 (xi ) − wh(xi ) + Γh |w|.
=
m i=1
l

+1

t−1
Notice that the minimizer over ∪s=1
Hs0 can be determined by comparing the minimizers over each Hs0 .

Moreover, since the penalty term Γh |w| has the same contribution for every h ∈ Hs0 , it has no impact on the optimal
choice of h over Hs0 . Thus, to find the minimizer over each
Hs0 , we can compute the derivative of Ft − Γh |w| with respect to w:
d(Ft − Γh |η|)
(w, h)
dw
m


−1 X
=
yi h(xi )Φ0 1 − yi ft−1 (xi ) .
m i=1

AdaNet: Adaptive Structural Learning of Artificial Neural Networks

Now, if we let
A DA N ET.CVX(S = ((xi , yi )m
i=1 )
1 f0 ← 0
2 for t ← 1 to T do
3
s∗ ← argmaxs∈[lt−1 +1] Λs,s−1 kt,hs−1,t−1 kq .



Dt (i)St = Φ0 1 − yi ft−1 (xi ) ,
then this expression is equal to
"m
#
X
St
St
−
yi h(xi )Dt (i)
= (2t,h − 1) ,
m
m
i=1

4

where t,h = Ei∼Dt [yi h(xi )]. Thus, it follows that for any
s ∈ [lt−1 + 1],
d(Ft − Γh |w|)
(w, h) = argmax t,h .
dw
h∈Hs0

argmax
h∈Hs0

5
6
7
8

(s∗ )

ui

←

Λs∗ ,s∗ −1 |t,hs∗ −1,t−1,i |q−1 sgn(t,hs−1,t−1,i )
q

∗

kt,hs∗ −1,t−1 kqp

h0 ← u(s ) · (φs∗ −1 ◦ hs∗ −1,t−1 )
η 0 ← M INIMIZE(F̃t (η, h0 ))
ft ← ft−1 + η 0 · h0
return fT
Figure 5. Pseudocode of the A DA N ET.CVX algorithm.

Note that we still need to search for the optimal descent
coordinate over an infinite dimensional space. However,
we can write
max t,h

h∈Hs0

= max0 E [yi h(xi )]
h∈Hs i∼Dt

=
=

max

E [yi u · (ϕs−1 ◦ hs−1,t−1 )(xi )]

max

u · E [yi (ϕs−1 ◦ hs−1,t−1 )(xi )].

u∈Rns−1,t−1 i∼Dt
u∈Rns−1,t−1

i∼Dt

Now, if we denote by u(s) the connection weights associated to h(s) , then we claim that
(s)
ui

=

Λs,s−1 |t,hs−1,t−1,i |q−1 sgn(t,hs−1,t−1,i )
q

,

kt,hs−1,t−1 kqp

which is a consequence of Banach space duality. To see
this, note first that by Hölder’s inequality, every u ∈
Rns−1,t−1 with kukp ≤ Λs,s−1 satisfies:
u · E [yi (ϕs−1 ◦ hs−1,t−1 )(xi )]

Thus, u(s) and the associated network h(s) is the coordinate that maximizes the derivative of Ft with respect
to w among all subnetworks in Hs0 . Moreover, h(s) also
achieves the value: Λs,s−1 kt,hs−1,t−1 kq .
This implies that by computing Λs,s−1 kt,hs−1,t−1 kq for
every s ∈ [lt−1 + 1], we can find the descent coordinate
across all s ∈ [lt−1 + 1] that improves the objective by
the largest amount. Moreover, we can then solve for the
optimal step size in this direction to compute the weight
update.
The theorem above defines the choice of descent coordinate at each round and motivates the following algorithm,
A DA N ET.CVX. At each round, A DA N ET.CVX can design the optimal candidate subnetwork within its searched
space in closed form, leading to an extremely efficient update. However, this comes at the cost of a more restrictive
search space than the one used in A DA N ET. The pseudocode of A DA N ET.CVX is provided in Figure 5.

i∼Dt

≤ kukp k E [yi (ϕs−1 ◦ hs−1,t−1 )(xi )]kq

D. More experiments

≤ Λs,s−1 k E [yi (ϕs−1 ◦ hs−1,t−1 )(xi )]kq .

In this section, we report the results of some additional experiments.

i∼Dt

i∼Dt

At the same time, our choice of u(s) also attains this upper
bound:
u(s) · t,hs−1,t−1
ns−1,t−1

=

X

(s)

ui t,hs−1,t−1,i

i=1
ns−1,t−1

=
=

X

Λs,s−1

i=1

kt,hs−1,t−1 kqp

q

Λs,s−1
q

kt,hs−1,t−1 kqp

|t,hs−1,t−1,i |q

kt,hs−1,t−1 kqq

= Λs,s−1 kt,hs−1,t−1 kq .

In our first set of experiments, we compared how many trials were needed for a bandit algorithm (Snoek et al., 2012)
to find a close-to-optimal set of hyperparameters. Our experiment (see Table 5) shows that both A DA N ET and traditional NNs often find close-to-optimal parameter values
within the first 200 trials. The number of trials were averaged over 10 folds. Note that optimal parameters for
A DA N ET result in better accuracy than those of traditional
NNs. This serves as further evidence that A DA N ET is more
efficient in both finding close-to-optimal accuracy values
and in finding the best optimal network architecture when
compared to traditional NNs.
It should be noted that, in general, the number of trials that

AdaNet: Adaptive Structural Learning of Artificial Neural Networks
Table 5. Performance of hyperparameter search on the CIFAR
cat-dog task.
Algorithm

Average number of trials

A DA N ET
NN

165.8
136

are needed to find close-to-optimal parameters depends on
the algorithm used to perform hyperparameter optimization.
Our final experiment consisted of first running the
A DA N ET algorithm to learn an architecture and weights
and then running back-propagation algorithm on the resulting architecture with weights learned by A DA N ET as
initialization. We used the cat-dog label pair task and
3,000 back-propagation steps with the same learning rate
as the one used to train each subnetwork. Using the same
cross-validation setup as in Section 6 for our CIFAR-10 experiments, this led to a test accuracy of 0.6908 (with a standard deviation of 0.01224), which is slightly worse than the
accuracy of 0.6924 obtained by running the A DA N ET algorithm alone. This further demonstrates that A DA N ET is
able to learn both the network architecture and its weights
simultaneously.

E. Implementation details
In this section, we provide some additional implementation details regarding the experiments presented in Section 6. Our system involves two components implemented
on a CPU: 1) a subnetwork model that handles each subnetwork; 2) an A DA N ET model to compose multiple subnetworks and a classification layer to combine all output
weights. For both these components, we used the Tensorflow package. We implemented a custom layer using matrix multiplication, including embeddings from the other
subnetworks. In addition, our loss function was based on
Eq. (5) and optimized via stochastic optimization (Kingma
& Ba, 2014).

