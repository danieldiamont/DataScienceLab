Supplementary Material for Efficient Nonmyopic Active Search
Shali Jiang 1 Gustavo Malkomes 1 Geoff Converse 2 Alyssa Shofner 3 Benjamin Moseley 1 Roman Garnett 1

1. Hardness of Active Search
In this section, we present the proof of Theorem 1. We
assume that active search policies have access to the correct
marginal probabilities f (x; D) = Pr(y = 1 | x, D), for
any given point x and labeled data D, which may include
“ficticious” observations. Further, the computational cost
will be analyzed as the number of calls to f , i.e., f (x; D)
has unit cost. Note that the optimal policy operates in such
a computational model, with exponentially many calls (in
terms of |X |) to the marginal probability function f .

Our proof technique consists of constructing an explicit active search instance where a small “secret” set of points S
encodes the location of a larger “hidden” group of positive
points. A particular feasibly exponential-cost policy identifies this small set first, and then obtains a large reward by
collecting the revealed targets. We will show that an algorithm with limited computational power (i.e., polynomial
in n = |X |) will not be able to identify the set S. As a
consequence, its performance will be arbitrarily worse than
an optimal solution as the size of the instance increases.

points C∗ , where ∗ can be regarded as a m-bit integral index
1 ≤ ∗ ≤ 2m . Figure 1(b) illustrates these points.

“Isolated points.” The remaining points share the property
that observing any single one of them does not change the
marginal probabilities of any other point. These points
are illustrated in Figures 1(a) (black dots) and 1(c). The
marginal probabilities for any point
xb in this category is
p
c
defined
to
be
f
(x
;
∅)
=
p
=
1
−
1/2, where we define
b
b
√
c = m/2 for convenience. These points can be further
classified into two categories:
• A “secret set,” denoted by S; see Figure 1(a) (black
dots). These points encode which of the clumps C∗
contains the positive points, using a scheme we describe below. For ease of exposition, we partition
the set S into m subsets
dc,
√ S1 , . . . , Sm , each of size
2
where we define d = m. Thus |S| = mdc = m /2 =
B/2; the size of this secret set is exactly half the budget.
The key of this construction is that each subset Si encodes one bit bi of information about which clump C∗
contains the positive points, using a simple encoding
scheme: the binary representation of the positive clump
C∗ ’s index is ∗ = b1 b2 . . . bm . Each bit is encoded with
a two-step mechanism. First, each Si is partitioned into
d groups of c points, each group encoding a “meta” bit
of information bij , 1 ≤ i ≤ m, 1 ≤ j ≤ d, by a logical
OR . These meta-bits, not in the problem instance, are
illustrated by the white dots in 1(a). Finally, the metabits associated with Si encode the bit bi via a logical
XOR ,1 bi = bi1 ⊕ · · · ⊕ bid .

The crux of the proof is to construct a class of instances H
that we present next. Figure 1 shows a schematic representation of an example instance I ∈ H. The instances in H
differ from each other by a permutation of the labels. An
instance has n = |X | = 22m points, where m is a parameter
of the instance. The search budget is defined to be B = m2 .
The points in each instance can be categorized as follows.
“Clumps.” These points are partitioned into 2m groups,
which we will call “clumps,” each of size B. All points
in a clump share the same label. Additionally, exactly one
of the clumps comprises all positive points; the remaining
points are all negative. The clump containing the positive
points is chosen uniformly at random; therefore, the prior
marginal probability for any point xc in this category is
f (xc ; ∅) = pc = 2−m . We denote the clump of all positive
1

Washington University in St. Louis, St. Louis, MO, USA
2
Simpson College, Indianola, IA, USA 3 University of South
Carolina, Columbia, SC, USA. Correspondence to: Shali Jiang
<jiang.s@wustl.edu>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

• Independent points. The remaining 22m − 2m B −
mdc points are totally independent from the others;
revealing them conveys no information for any other
point. We denote this set of points by R.
Observation 1. At least d points from Si need to be observed in order to infer one bit bi of information about the
positive clump.
A virtual bit bi is computed by the XOR operation of the
d associated meta-bits bi = bi1 ⊕ · · · ⊕ bid . Notice that
each bij has same marginal probability of being positive,
1

Sum of the bits modulo 2.

Supplementary Material for Efficient Nonmyopic Active Search

…
…

OR
OR

…
…

XOR

…

B

OR

c

d

(

(

m

…
…
…

B

OR
OR

…

XOR

…

...
c

d

(

…
…

B

OR
OR

…
…

(

2m

22m

2m B

mdc

(

...

OR

(

(

d

(

c

(

…

XOR

…
OR

(a) Secret set S of mdc isolated points.

(b) 2m B points from clumps Cj .

(c) Isolated and independent points R.

Figure 1: An instance of active search where any efficient algorithm can be arbitrarily worse than an optimal policy.
i.e., ∀i, j, Pr(bij = 1) = (1 − pb )c = 1/2. We also have
∀i, Pr(bi = 1) = 1/2. It is necessary to observe all d
meta-bits bij from the same group Si to infer the bit bi ,
since observing a fraction of the inputs of an XOR does not
change the marginal belief about its outcome. So observing
d − 1 or fewer points from S conveys no information about
the positive clump. Equivalently, ∀x, Pr(y = 1 | x, D) =
Pr(y = 1 | x) if |D ∩ S| ≤ d − 1.
Observation 2. Observing any number of clump points
does not change the marginal probability of any point in the
secret set S.

We need to make sure that no external information can
help to identify the secret set S. Notice that the knowledge
of bi does not change the marginal probability of any bij ;
hence, no point x in S will have a different probability after
observing bi . This means that observing points outside S
does not help distinguish S from the remaining isolated
points R.
Now we formally restate Theorem 1 and provide its proof.
The theorem implies that a polynomial time algorithm cannot achieve a constant approximation ratio.
Theorem 1. Any (possibly
policy for active
√ 1randomized)

log
n
search that performs o n 2
inference calls f (x, D),
with |D| ≤ B, has approximation ratio with
 respect
 to the
expected utility of the optimal policy of O

√ 1
log n

where

|X | = n is the number of points, and B is the budget.2

Proof. Consider a random instance I ∈ H and fix a policy
A. Let α be the total number of inference calls performed
2
Note we used the little-o notation for the number of inference
calls, and the big-O notation of the approximation ratio.

by A throughout its execution. At the ith inference call
Pr(y = 1 | x, Di ), A might use an arbitrary training set Di
of size at most B. We will show that A has a very small
probability of collecting a large reward on I.

Before analyzing the algorithm A, we present a lower bound
on the performance of an optimal policy. Consider the
following policy with unlimited computational power: In
the first iteration, compute the marginal probability of an
arbitrary fixed clump point, conditioning on observing every
possible subset of the isolated points of√size d with labels
1
all equal to 1. This set of O(nd ) = O(n 2 log n ) inference
calls will reveal the location of the secret set S: exactly
those points will modify the probabilities of the fixed clump
point. Now the policy spends the first half of its budget
querying the points in S (recall |S| = B/2). These outcomes
reveal the hidden clump of positives C∗ . The policy now
spends the second half of the budget querying (collecting)
these positive points. The expected performance of this
strategy is B/2 + pb B/2 > B/2. Certainly this is a lower
bound on the optimal performance; hence
OPT

>

B
.
2

(1)

Now consider the algorithm A at the ith inference. By
Observations 1 and 2, A cannot differentiate between the
points in S and those in R unless |Di ∩S| ≥ d. Suppose that
before the ith inference, the algorithm has no information
about S. Then the chance of A choosing a Di such that
|Di ∩ S| ≥ d is no better than that of a random selection
from n0 points, where n0 = n − 2m B is the number of
isolated points. We can upper bound the probability of A
choosing a dataset Di such that |Di ∩ S| ≥ d, by counting
how many subsets would contain at least d points from S,

Supplementary Material for Efficient Nonmyopic Active Search

among all subsets of the n0 points of size at most B:


B/2 n0 −d

d
B−d
Pr |Di ∩ S| ≥ d ≤
.

n0

(2)

B

We only consider the isolated points because an algorithm
A that only queries the isolated points has a higher probability of hitting the secret set. Also note that technically
0
the denomenator in (2) should be nB − i + 1 since one
would not choose the same subsets Dj , j < i as those before the ith inference. But asymptotically i ≤ α (assuming
0
α = O(2n ) for now) is ofmuch
lower order than nB ,

0
0
therefore nB − α + 1 = Θ nB . Denote ph as the probability of algorithm A ever “hiting” the secret set after α
inferences; then ph can be union-bounded:
 n0 −d
α B/2
d
ph ≤
B−d
n0
<

α

B

B d d
B
2
0
d
(n )

α

=

Note B = m2 = 14 log2 n and n0 = n − 2m B = n −
√ 1
n 4 log2 n = Θ(n), so
 0 d
 d !
2n
2n
=Θ
B2
B2


!√ 12 log n
n


= Θ 1

4
log
n
32
 √1

= Θ n 2 log n .

We have now derived

So for any α = O n
constant, we have

Θ(n
√1
2

α
√1
2

log n

log n−ε

ph < O



The expected number of targets found after B queries on
the clump points is





1
B−1
B
1
+ 1− m
+ 1− m
(· · · )
2m
2
2m − 1
2 −1
B(B + 1)
=
.
(5)
2m+1

Combining (4) and (5), we get that the expected performance in the case when A never hits S can be upper
bounded by
B(B + 1)
− √2 
+B 1−2 m .
m+1
2

The overall expected performance of A can be upper
bounded by


2n0 d
B2

ph <

can be upper bounded by pretending that the algorithm had
a larger budget of size 2B; in which half the budget (i.e.,
B) is spent on querying points in R, and the other half on
querying the clump points. The expected number of targets
found after B queries on R is


p
− √2
(4)
Bpb = B(1 − c 1/2) = B 1 − 2 m .



1
nε

.
)

, where ε is a positive



.

(3)

If A ever hits the secret set S, for simplicity, we will assume
that it achieves maximal performance B. If A never finds
the secret set, we can further consider the following two
cases. If the algorithm queries points x ∈ R, no marginal
probabilities are changed; if a point x ∈ Cj is queried, for
any clump j, only the marginal probabilities of the clumps
are changed. The expected performance in these two cases

EA < Bph +

B(B + 1)
− √2 
+B 1−2 m ,
m+1
2

where we have used the trivial upper bound 1 > (1 − ph ).
Finally, combining with the lower bound of OPT in (1), the
approximation ratio can be upper bounded by
EA

OPT

<

Bph +

B(B+1)
2m+1

+B 1−2
B/2

− √2m 

B+1
− √2 
= 2ph + m + 2 1 − 2 m
2

√


1
log2 n
− √2log2n
√
=O
+
+2 1−2
nε
4 n


1
=O √
.
log n
√1
√1


for any α = O n 2 log n−ε = o n 2 log n . Note that it
√


− √2 2 
1
is easy to verify that 2 1 − 2 log n = Θ √log
with
n
L’Hôpital’s rule.

2. Results on Individual Activity Classes
Figure 2 shows the learning curves of two-step lookahead
and our method, on the first six activity classes (of the total
120) for the ECFP 4 fingerprint. We can see clear patterns
indicating a transition from exploration to exploitation, especially for activity classes 1 and 2. The number of targets
found by ENS first grows slowly, but after a certain point,

Supplementary Material for Efficient Nonmyopic Active Search

grows increasingly faster through the end of the budget. For
the two-step-lookahead myopic policy, most of the time
it behaves the opposite: it first greedly discovers targets
faster than ENS, but in later stages, it usually levels off, not
knowing where else to go. Note the point when ENS transitions from exploration to exploitation can be different for
different problems, but we see a very clear pattern when
these results are averaged over all 120 activity classes, as
shown by the difference of the two average learning curves
in Figure 3 of the main text. We also show the difference
curve for the other fingerprint in Figure 3 here.

3. UCB-Style Score
We mentioned in Section 5 of the main text that we have
investigated the UCB-style (Auer, 2002) score function
p
(6)
α(x, D) = π + γ π(1 − π),

where π = Pr(y = 1 | x, D) and γ is a tradeoff parameter
between exploitation (first term) and exploration (second
term). The results of this score varying the hyperparameter γ
are shown in Figure 4, for the CiteSeerx dataset and the drug
discovery dataset with fingerprint ECFP 4, using exactly the
same experimental setting as other methods (20 experiments,
same random seed for each, sample model, etc.). Note with
a fixed γ, the score α is maximized at some probability
π = p∗ ; it is easy to derive that
p∗ =

1
1
+ p
2 2 γ2 + 1

(7)

by setting the derivative to zero. To better present the results,
we use p∗ ∈ [0.5, 1] as the hyper-parameterization of the
score in Figure 4. In summary, on the CiteSeerx dataset, as
shown in Figure 4(a), the performance is maximized at some
p∗ near 0.6, but there does not seem to be a clear pattern.
But when we average the performance on 2 400 experiments
on the ECFP 4 data, as shown in Figure 4(c), we see that the
α score is monotonically performing better with larger p∗
(or smaller γ), and converges to the greedy policy (p∗ = 1).

4. Effect of Pruning: Detailed Results
Table 1 shows the detailed results of our pruning study
described in Section 5.4 of the main text.
Table 1: Average number of pruned points in each iteration
for the two chemical datasets.
fingerprint

# pruned

# total

pruned %

ECFP 4
G pi DAPH 3

94 995
93 173

100 518
100 518

94.5%
92.7%

5. Mean Difference Curves for CiteSeerx and
BMG Data
Figures 5 and 6 show the the mean difference curve between
ENS and two-step, for various budgets, respectively for the
CiteSeerx and BMG data.

Acknowledgments
We would like to thank Brendan Juba for insightful discussion. SJ, GM, and RG were supported by the National Science Foundation (NSF) under award number IIA–1355406.
GM was also supported by the Brazilian Federal Agency
for Support and Evaluation of Graduate Education (CAPES).
GC and AS were supported by NSF under award number
CNS–1560191. BM was supported by a Google Research
Award, a Yahoo Research Award, and by NSF under award
number CCF–1617724.

References
ASM Alloy Center Database.
URL http://mio.
asminternational.org/ac/.
Auer, Peter. Using Confidence Bounds for Exploitation–
Exploration Trade-offs. Journal of Machine Learning
Research, 3:397–422, 2002.
Kawazoe, Yoshiyuki, Yu, Jing-Zhi, Tsai, An-Pang, and
Masumoto, Tsuyoshi (eds.). Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys, volume 37A of
Condensed Matter. Springer–Verlag, 1997.
Wang, Xuezhi, Garnett, Roman, and Schneider, Jeff. Active Search on Graphs. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 731–738, 2013.
Ward, Logan, Agrawal, Ankit, Choudhary, Alok, and
Wolverton, Christopher. A General-Purpose Machine
Learning Framework for Predicting Properties of Inorganic Materials. 2016. arXiv preprint arXiv:1606.09551
[cond-mat.mtrl-sci].

Supplementary Material for Efficient Nonmyopic Active Search

protein 1
ENS

300

ENS

300

two-step
actives found

actives found

protein 2

200

100

two-step

200

100

0

0
0

100

200

300

400

500

0

100

200

300

number of queries

number of queries

protein 3

protein 4

400

500

400

500

400

500

300

ENS

actives found

actives found

200

100

0

two-step

200

0
0

150

100

200

300

400

500

0

100

200

300

number of queries

number of queries

protein 5

protein 6

ENS

ENS

300

actives found

two-step
actives found

ENS

400

two-step

100

50

two-step

200

100

0

0
0

100

200

300

number of queries

400

500

0

100

200

300

number of queries

Figure 2: Number of active compounds found as a function of the number of queries, for protein (activity class) 1 to 6,
fingerprint ECFP 4, averaged over 20 experiments for each protein.

Supplementary Material for Efficient Nonmyopic Active Search

difference in utility

20

mean difference
95% CI
0

−20

−40
0

100

200

300

400

500

number of queries

Figure 3: The average difference in cumulative targets found
between our method and the two-step policy, averaged over
all 120 activity classes and 20 experiments, on the fingerprints GpiDAPH 3.

Supplementary Material for Efficient Nonmyopic Active Search

average number of targets

average number of targets

450
150

140

UCB -style
two-step
130

400

350

UCB -style
two-step
300

0.5

0.6

0.7

0.8

p

0.9

1

∗

0.5

0.6

0.7

0.8

p

(a) CiteSeer data

0.9

1

∗

(b) BMG data

x

average number of actives

300

250

200

UCB -style
two-step
150
0.5

0.6

0.7

0.8

p

0.9

1

∗

(c) ECFP 4 data

Figure 4: Number of targets found by the UCB-style policy (6), as a function of the hyperparameter p∗ as derived in (7),
averaged over 20 experiments. Note for CiteSeerx and BMG datasets, the grid size of p∗ is 0.01, but for ECFP 4, we can only
afford grid size of 0.1. To put these results into perspective, we also show the two-step performances by the red horizontal
line, indicating two-step performs better than the UCB-style policy on all three domains. All these results are with budget
500.

difference of number of targets found

difference of number of targets found

Supplementary Material for Efficient Nonmyopic Active Search

mean difference
95% CI

10

5

0

0

20

40

60

80

100

30

mean difference
95% CI

20

10

0

−10

0

100

number of queries

mean difference
95% CI
40

20

0

difference of number of targets found

100

200

300

300

(b) Budget 300; p = 0.020
difference of number of targets found

difference of number of targets found

(a) Budget 100; p = 0.017
60

0

200

number of queries

400

500

60

mean difference
95% CI
40

20

0

0

200

400

600

number of queries

number of queries

(c) Budget 500; p = 9.9 × 10−5

(d) Budget 700; p = 1.0 × 10−4

mean difference
95% CI

40

20

0

−20

0

200

400

600

800

number of queries
(e) Budget 900; p = 0.046

Figure 5: Mean difference curves between our policy and two-step lookahead in terms of number of actives found, along
with the confidence interval, for CiteSeerx data, with budget varying from 100 to 900. Also shown are the p-values of a
two-sided paired t-test testing the null hypothesis that the performance of the policies is equal at termination.

mean difference
95% CI

6

4

2

0

−2

0

20

40

60

80

mean difference
95% CI

10

5

0

0

100

100

200

300

number of queries

number of queries

(a) Budget 100; p = 0.068

(b) Budget 300; p = 6.1 × 10−3

mean difference
95% CI

20

10

0

0

difference of number of targets found

difference of number of targets found

8

difference of number of targets found

difference of number of targets found

difference of number of targets found

Supplementary Material for Efficient Nonmyopic Active Search

100

200

300

400

500

mean difference
95% CI

20

10

0

0

200

400

600

number of queries

number of queries

(c) Budget 500; p = 2.1 × 10−4

(d) Budget 700; p = 3.7 × 10−3

mean difference
95% CI

20

0

0

200

400

600

800

number of queries
(e) Budget 900; p = 0.091

Figure 6: Mean difference curves between our policy and two-step lookahead in terms of number of actives found, along
with the confidence interval, for BMG data, with budget varying from 100 to 900. Also shown are the p-values of a two-sided
paired t-test testing the null hypothesis that the performance of the policies is equal at termination.

