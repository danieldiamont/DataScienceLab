Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

A. Proofs of Main Results
In this section we present proofs of the results from Section 5.
A.1. Proof of Theorem 5.5 and Corollaries
We first demonstrate how we decompose the estimation error in the unbiased Lasso Granger estimator θbu into the sum of
two components:
1
b − θ∗
e > (Xθ
e ∗+−X
e θ)
MX
T −p
1
b − (θ ∗ − θ)
b
e >  + 1 MX
e > X(θ
e ∗ − θ)
=
MX
T −p
T −p
1
b
e >  + (MΣ
e n − I)(θ ∗ − θ).
MX
=
T −p

θbu − θ ∗ = θb +

b we have
e n − I)(θ ∗ − θ),
e > /√T − p and ∆ = √T − p(MΣ
Letting Z = MX
p

T − p(θbu − θ ∗ ) = Z + ∆.

(A.1)

Note that, clearly, E[Z] = 0. Thus, ∆ encapsulates the bias in θbu . We divide this proof into two parts. We first establish in
Lemma A.1 that θbu is an asymptotically unbiased estimator of θ ∗ by proving that k∆k∞ = o(1). We then proceed to prove
in Lemma A.2 that Z is asymptotically normally distributed.
b  (√T − p)/ log(pd) and µ 
Lemma
A.1.
Suppose
Assumptions
5.3
and
5.4
are
satisfied.
Let
s
=
supp(
θ)
0
p
√
b
e n − I)(θ ∗ − θ).
log(pd)/(T − p). Then k∆k∞ = o(1), where ∆ = T − p(MΣ
The proof of Lemma A.1, presented in Appendix B.1, uses Hőlder’s inequality to decompose k∆k1 into the product
√
b 1 . We bound kMΣ
e n − Ik∞ · kθ ∗ − θk
e n − Ik∞ by constructing a martingale difference sequence (see
T − pkMΣ
Definition G.1 in Appendix G) and applying a Bernstein-type inequality (Lemma F.8) to this sequence. We then bound
b 1 via a standard argument for Lasso-type estimators that relies on the restricted eigenvalue condition. We amend
kθ ∗ − θk
this argument to work in our non-i.i.d. setting by appealing to martingale theory and present a restricted eigenvalue condition
for martingale difference sequences in Appendix F.
b  (√T − p)/ log(pd) and µ 
Lemma A.2. Suppose Assumptions 5.3 and 5.4 are satisfied. Let s0 = supp(θ)
p
D
e n M> ]1/2 ) = MX
e > /(σ √T − p[MΣ
e n M> ]1/2 ) −
log(pd)/(T − p). Then we have Z/(σ[MΣ
→ N (0, Ipd×pd ).
The proof of Lemma A.2, deferred to Appendix B.2, relies on constructing a martingale difference sequence equal to
e n M> ]1/2 ), and applying the Martingale Central Limit Theorem (Hall & Heyde, 1980).
Z/(σ[MΣ
Having established Lemmas A.1 and A.2, we are now ready to present a proof of Theorem 5.5.

Proof of Theorem 5.5. We write the estimation error of the unbiased Lasso Granger estimator as
p

T − p(θbu − θ ∗ ) = Z + ∆.

P
b Then, by Lemma A.1 we have that ∆ −
e > /√T − p and ∆ = √T − p(MΣ
e n − I)(θ ∗ − θ).
where Z = MX
→ 0. By
D
> 1/2
e
Lemma A.2, we have that Z/(σ[MΣn M ] ) −→ N (0, I). Therefore, by the Slutsky Theorem (Van der Vaart, 2000), we
√
D
e n M> ]1/2 ) −
have that T − p(θbu − θ ∗ )/(σ[MΣ
→ N (0, Ipd×pd ), as desired.

Theorem 5.5 allows us to demonstrate the asymptotic validity of the confidence intervals we present in Corollary 5.6 as
follows:

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Proof of Corollary 5.6. By Theorem 5.5, the asymptotic normality of θbiu implies






∗
u
∗
u
∗
b
b
lim P θi ∈ Ii = lim P θi − θi ≤ δ(α, T − p) − lim P θi − θi ≤ −δ(α, T − p)
T −p→∞

T −p→∞

T −p→∞

√

T − p(θbiu − θi∗ )
−1
= lim P
≤ Φ (1 − α/2)
T −p→∞
e n M> ]1/2
σ[MΣ
i,i
√

T − p(θbiu − θi∗ )
−1
≤
−Φ
− lim P
(1
−
α/2)
T −p→∞
e n M> ]1/2
σ[MΣ
i,i
= 1 − α.

(A.2)

In a similar manner, Theorem 5.5 also permits us to prove the several desirable properties of hypothesis test ΨZ (α), which
we introduce in (4.7), that we present in Corollary 5.7.
Proof of Corollary 5.7. By (4.3), we have
ci | < zα/2 ) = α
P(ΨZ (α) = 1|H0i ) = P(−|Z
ci converges in distribution to the standard normal distribution. Similarly, for any u ∈ (0, 1),
where zα/2 = Φ−1 (α/2), since Z
we see that


u
c
c
P(Pi < u) = P(2(1 − Φ(|Zi |)) < u) = P Φ(|Zi |) > 1 −
2



(T −p)→∞
ci | > Φ−1 1 − u
−−−−−−−→ u
= P |Z
2
ci converges in distribution to the standard normal distribution.
since, again, Z

In Section 4.1, we claim that the Scaled Lasso noise estimator (Sun & Zhang, 2012) σ
b, as given by (4.4), is a consistent
estimator of the true noise level σ. We note that while Sun & Zhang (2012) prove the consistency in the i.i.d. case, σ
b is
nevertheless still consistent in our non-i.i.d. case as well. This result follows directly from Theorem 1 in Sun & Zhang
(2012), which we paraphrase in the following lemma.
p
b
Lemma A.3. Let (θ(λ),
σ
b(λ)) be the Scaled Lasso estimator from (4.4) and λ = 8Cσ log(pd)/(T − p). Furthermore,
let the assumptions of Theorem 5.5 hold. Then,




σ
b(λ)


P 
− 1 >  → 0,
σ
for all  > 0 as (T − p, pd) → ∞.
We present a proof of this lemma in Appendix B.3.
A.2. Proof of Theorem 5.9
We first present a property and three lemmas that will allow us to prove Theorem 5.9. For ease of presentation, let
G(t) = 2(1 − Φ(t)) and G−1 (t) = Φ−1 (1 − t/2).
e = [σi,j ] ∈ Rpd×pd is the true covariance matrix of our design matrix X.
e Now let ρi,j =
Property 1. Recall that Σ
√
−2−
σi,j / σi,i σj,j , and for δ,  > 0, let B(δ) = {(i, j)||ρi,j | ≥ δ, i 6= j} and A() = B([log(pd)]
). By a similar argument
b  √T − p/ log(pd) in Theorem 5.5, we have
to that made by Liu et al. (2013b), since we assume that supp(θ)
X
(pd)a1 = O((pd)2 /(log(pd)2 ),
(i,j)∈A()

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where a1 = 2|ρi,j |/(1 + |ρi,j |) + δ.
Property 1 amounts to a sparsity assumption on the true covariance matrix and allows us to cope with the correlation among
test statistics.
Lemma A.4. Suppose Assumptions 5.3, 5.4, the conditions of Theorem 5.9, and Property 1 are satisfied. Then we can
bound νb from (4.9) as follows:
P(0 ≤ νb ≤ xpd ) → 1
pd

where xpd = G−1 (ypd /(pd)), and ypd is a sequence such that ypd −→ ∞ and ypd = o(pd).
Lemma A.4 bounds νb with high probability. We use this bound in the following lemma:
Lemma A.5. Suppose Assumptions 5.3, 5.4, and Property 1 are satisfied. Then for xpd as defined in Lemma A.4, we have:
P

ci | ≥ ν)
 P

1(|Z
→ 0.
sup  i∈H0
− 1 −
|H0 |G(ν)
0≤ν≤xpd
Lemma A.6. Suppose Assumptions 5.3 and 5.4 are satisfied. Then we have:
max{

P

(pd)G(b
ν)
= α.
ci ≥ νb), 1}
1(|Z

1≤j≤pd

In the interest of clarity, we defer the proofs of Lemmas A.4, A.5, and A.6 to Appendices B.4, B.5, and B.6, respectively.
The proof of Theorem 5.9 proceeds in three parts. We first bound νb with high probability in Lemma A.4, and then prove in
Lemma A.5 that for any ν within those bounds
P
c
b) P
i∈H0 1(|Zi | ≥ ν
−
→ 1.
|H0 |G(b
ν)
The result of Theorem 5.9 then follows naturally by Lemma B.6 .
Proof of Theorem 5.9. By Lemma A.4, P(0 ≤ νb ≤ xpd ) → 1. Then by Lemma A.5, we have
P

P

ci | ≥ νb)
ci | ≥ ν)
 i∈H0 1(|Z

 i∈H0 1(|Z
 P



− 1 ≤ sup 
− 1 −
→ 0.

|H0 |G(b
ν)
|H0 |G(ν)
0≤ν≤xpd
Thus, we have
P

ci | ≥ νb) P
1(|Z
−
→ 1.
|H0 |G(b
ν)

i∈H0

From this result, we see that by the definition of FDP(ν),
P
ci | ≥ νb)
(pd) i∈H0 1(|Z
(pd)|H0 |G(b
ν)
FDP(b
ν)
P
=
−
→
.
P
P
c
ci ≥ νb), 1}
α|H0 |/(pd)
α|H0 | max{ 1≤j≤pd 1(|Zi ≥ νb), 1}
α|H0 | max{ 1≤j≤pd 1(|Z

(A.3)

By Lemma A.6,
max{

P

(pd)G(b
ν)
= α.
ci ≥ νb), 1}
1(|Z

1≤j≤pd

Thus, by (A.3) and (A.4)
FDP(b
ν) P
−
→ 1,
α|H0 |/(pd)
as (T − p, pd) →
− ∞. This result clearly then implies that
FDR(b
ν) P
−
→ 1,
α|H0 |/(pd)
as (T − p, pd) →
− ∞, as desired.

(A.4)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B. Proofs of Technical Lemmas in Appendix A
In this section we present the proofs of technical lemmas introduced in Appendix A.
B.1. Proof of Lemma A.1
We first present two auxiliary lemmas that we will use in the proof of Lemma A.1.
eΣ
e −1/2 are sub-Gaussian with
Lemma B.1. If Assumption 5.3 holds and we additionally assume that the rows of X
−1/2 f
e
sub-Gaussian norm of κ = kΣ
Xi kψ2 , then
s
e n − Ik∞ ≤ a
kMΣ

holds with probability at least 1 − 2(pd)−c2 , where c2 =

log(pd)
,
T −p

a2 Cmin
− 2 and a is some constant.
24e2 κ4 Cmax

The proof of Lemma B.1, which we defer to Appendix C.1, relies constructing a martingale difference sequence (see
Definition G.1 in Appendix G) and applying a Bernstein-type inequality (Lemma F.8) to this sequence.
p
Lemma B.2. Let λ = 8Cσ log(pd)/(T − p) for some constant C. Then
b 1 ≤ 12λs0 ,
kθ ∗ − θk
κ`
with probability at least
2

1 − b1 exp[−b2 σ log(pd)] −

2 exp(−c20 c21 c2 ω 2 (B))


(ω(A))2
− L exp − 4
,
α2


1
e 2 is the restricted minimum eigenvalue of Σ
e n restricted to A ⊆ S pd−1 (the
kXuk
2
T −p
e 1/2 u/kΣ
e 1/2 uk2 , u ∈ A} is the normalized set of A, α = diam(A) =
e = Σ
unit sphere in Rpd space), B = {e
u : u
supu,v∈A d(u, v) = supu,v∈A ku − vk2 , ω(A) is the Gaussian width of set A as defined in Definition F.5, and
κ` , b1 , b2 , c0 , c1 , c2 , L > 0 are constants.

e n |A) = inf u∈A
where λmin (Σ

b 1
The proof of Lemma B.2, which we present in Appendix C.2, relies on the restricted eigenvalue condition to bound kθ ∗ − θk
with high probability.
We now present a proof of Lemma A.1.
Proof of Lemma A.1. Hőlder’s inequality allows us to decompose k∆k1 as follows:
k∆k∞ ≤ k∆k1 ≤

p

b 1.
e n − Ik∞ · kθ ∗ − θk
T − pkMΣ

p
b 1 separately. By Lemma B.1, we find that kMΣ
e n − Ik∞ and kθ ∗ − θk
e n − Ik∞ ≤ a log(pd)/T − p
We now bound kMΣ
b 1 ≤ 12λs0 /κ` with high probability
with high probability . Additionally, by Lemma B.2, kθ ∗ − θk
Combining these two high-probability bounds yields the following result:
p
log(pd)
b 1 < as0 96σ · √
e n − Ik∞ · kθ ∗ − θk
T − pkMΣ
,
κ`
T −p

(B.1)

√
√
with high probability. Thus, by (B.1), k∆k∞ = o(s0 log(pd)/ T − p). Recall that by assumption, s0  T − p/ log(pd).
Therefore, k∆k∞ = o(1).

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B.2. Proof of Lemma A.2
We first present an auxiliary lemma that we will use in the proof of Lemma A.2.
Lemma B.3 (Lindeberg condition from Hall & Heyde (1980)). Denote the martingale difference sequence ζi,t =
2
f> mi )/(σ[m> Σ
e n mi ]1/2 ) and filtration Ft = σ(X
f1 , . . . , X
ft , 1 , . . . , t ). Then, PT
(t X
t
i
t=p+1 E[ζi,t 1(kζt | ≥ δ)|Ft−1 ] →
0.
In the interest of clarity, we defer the proof of Lemma B.3 to Appendix C.3.
Proof of Lemma A.2. To prove this lemma, we need to show that
ci
Z
e n M> ]1/2
σ[MΣ
i,i

=

e>
m>
D
i X 
−→ N (0, 1).
>
1/2
e
σ[m Σn mi ]
i

Note that
e >  = (m> X
e > )> = > Xm
e i=
ci = m> X
Z
i
i

T
X

f> mi .
t X
t

t=p+1

f1 , . . . , X
ft , 1 , . . . , t ). By (3.1), t is independent of Ft−1 and conditionally independent
Now define filtration Ft = σ(X
ft given Ft−1 . Furthermore, by (4.2), t is conditionally independent mi given Ft−1 . Therefore,
of X
f> mi |Ft−1 ] = E[t |Ft−1 ] · E[X
f> mi |Ft−1 ]
E[t X
t
t
f> mi |Ft−1 ]
= E[t ] · E[X
t
= 0,
where the last equality follows since t ∼N (0, σ 2 ). Thus,
{ζi,t }Tt=p+1 =



T
f> mi
t X
t
,
e n mi ]1/2 t=p+1
σ[m> Σ
i

is a martingale difference sequence by Definition G.1 in Appendix G.
e n mi ]1/2 ) = PT
ci /(σ[m> Σ
Since Z
i
t=p+1 ζi,t , if we can show that we can apply the Martingale Central Limit Theorem
(MCLT) (Hall & Heyde, 1980) to ζi,t , the result of this lemma will follow. To demonstrate that we can apply the MCLT to
ζi,t , we must prove that the Lindeberg condition holds for this sequence. By Lemma B.3, the Lindeberg condition holds for
ζi,t . Therefore, by the MCLT
T
X
t=p+1

ζi,t =

ci
Z
D
−→ N (0, 1),
>
1/2
e
σ[m Σn mi ]
i

as desired.

B.3. Proof of Lemma A.3
We present a variation on the argument made in the Proof of Theorem 1 in Sun & Zhang (2012). The proof of Lemma A.3
requires the following two supporting lemmas from Sun & Zhang (2012), which in turn necessitate introducing some new
e 2 + λkθk1 . We distinguish L(·)
notation. Denote the penalized least-squares loss function L(θ) = (2(T − p))−1 kY − Xθk
2
e 2 + σ/2 + λkθk1 .
from the Scaled Lasso loss function from (4.4), which we denote Lλ (θ, σ) = (2(T − p))−1 kY − Xθk
2
As Sun & Zhang (2012) note, θ is a critical point of L if and only if it satisfies:
(
f·,j (Y − Xθ)/(T
e
X
− p) = λsign(θj ), θj 6= 0
(B.2)
f
e
X·,j (Y − Xθ)/(T − p) ∈ [−λ, λ], θj 6= 0

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

f·,j is the j-th column of the design matrix X.
e Importantly, Sun & Zhang (2012) note that B.2 is the Karush-Kuhnwhere X
Tucker (KKT) condition for the minimization of L(·) when L(·) is convex in θ. This property will prove important in the
discussion of Lemma B.3 below. We now present the first of two supporting lemmas for the proof of Lemma A.3.
b
b
Lemma
p B.4 (Proposition 1 from Sun & Zhang (2012)). Let θ = θ(λ) be a solution path of B.2 and λ0 = λ/σ =
8C log(pd)/(T − p). Then the loss function Lλ (·, ·) is jointly convex in (θ, σ). Furthermore, the derivative of Lλ (·, ·)
with respect to σ is
2
e
∂
b 0 ), t) = 1 − kY − Xθ(tλ0 )k2 .
Lλ0 (θ(tλ
2
∂t
2
2(T − p)t

(B.3)

e and so we refer readers to the proof of Proposition 1 in Sun &
Lemma B.4 does not rely on the i.i.d.-ness of the rows of X,
Zhang (2012) for a proof of Lemma B.4.
The second supporting lemma requires additional notation from Sun & Zhang (2012). Let η(λ, ξ, w, Q) be a prediction
error bound for the estimation of θ ∗ via the Scaled Lasso. Let w ∈ Rpd Q ⊂ 1 . . . , pd, and
e ∗ − w)k2 /(T − p) + 2λkwQc k1 (2 − 1(w = θ, Q = ∅)) +
η(λ, ξ, w, Q) = kX(θ
2

4ξ 2 λ2 |Q|
,
(ξ + 1)2 κ(ξ, Q)

(B.4)

where vS = [vi ]i∈S ) ∈ R|S| and

e 2
|Q|1/2 kXuk
√
: u ∈ E(ξ, Q), u 6= 0 ,
(B.5)
kuQ k1 T − p
e ∗ k2 /√T − p, which Sun & Zhang (2012)
where E(ξ, Q) = {u : kuQc k1 ≤ ξkuQ k1 }. Now let σ ∗ = kY − Xθ
call the oracle estimator for σ. Based on these definitions, let the minimum prediction error bound be η∗ (λ, ξ) =
1/2 ∗
∗
inf w,Q
λ0 = λ/σ =
p η(λ, ξ, w, Q) and define the following related quantity τ0 = η∗ (σ λ0 , ξ)/σ , where recall that
∗
8C log(pd)/(T − p). As Sun & Zhang (2012) note, since in (3.2)  in a Gaussian random vector, σ is the maximum
likelihood estimator for σ when θ is known. Thus, in the proof of Lemma A.3, we bound the quantity σ
b/σ ∗ − 1 by τ0 in
order to prove the consistency of σ
b. However, we first require the following intermediate result.
∗
b
Lemma
Let θ(λ)
 B.5 (Theorem∗ 4 from Sun & ∗Zhang (2012)).
	 minimize L(·), ξ > 0, and define η (λ, ξ) =
2
2 1/2
c
minQ (1/2)(η(λ, ξ, θ , Q) + (η(λ, ξ, θ , Q) − 16λ kθQ k1 ) ) (not to be confused with η∗ (λ, ξ) defined above).
e top (Y − Xθ
e ∗ )k∞ /(T − p) ≤ λ(ξ − 1)/(ξ + 1), then
If kX

	
e θb − θ ∗ )k2 /(T − p) ≤ min η∗ (λ, ξ), η ∗ (λ, ξ) .
kX(
2


κ(ξ, Q) = min

The proof of Lemma B.3 follows directly from the proof of Theorem 4 from Sun & Zhang (2012). The only point of
contention in that proof is that B.2 must be the KKT condition for the minimization of L(·), which as we state above requires
that L(·) is convex in theta. In Lemma C.1 in Appendix C.2, we prove that the restricted eigenvalue condition holds with
e n . As we demonstrate in the proof of Lemma B.2 in Appendix C.2,
high probability for the sample covariance matrix Σ
e 2 is
the restricted eigenvalue condition implies that the unpenalized least-squares loss function (2(T − p))−1 kY − Xθk
2
∗
strictly convex for all θ such that the error vector θ − θ falls in the error cone E(3, S), where S is the support of θ ∗ . Since
E(3, S) actually encompasses all possible error vectors, a property we prove in the proof of Lemma B.2, we see that the
unpenalized loss function is convex with respect to θ. Since the derivative of penalty term λkθk1 with respect to θ is a
strictly positive vector, given that the unpenalized loss function is convex in θ, so is the pealized loss function. Thus, B.2 is
the KKT condition for the minimization of L(·), and Lemma follows immediately from the proof of Theorem 4 in Sun &
Zhang (2012). We refer the reader to that paper for the full proof.
We are now ready to present the proof of Lemma A.3.
Proof of Lemma A.3. We present an amended version of the Proof of Theorem 1 from Sun & Zhang (2012). Let z ∗ =
e > (Y − Xθ
e ∗ )/(T − p)k∞ /σ ∗ . Without loss of generality, assume τ0 < 1 and let t ≥ σ ∗ (1 − τ0 ) and let λ1 = tλ0 ,
kX
where λ0 is as defined above. Now note that
z ∗ σ ∗ ≤ σ ∗ (1 − τ0 )λ0

ξ−1
ξ−1
ξ−1
≤ tλ0
= λ1
.
ξ+1
ξ+1
ξ+1

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Then by this inequality, the definition of σ ∗ , the Cauchy-Schwarz inequality, and Lemma B.3, we have


b 1 )k2
b 1 ) − θ ∗ )k2
e θ(λ
e θ(λ
 kY − X
 kX(
1/2
∗

√
√
≤
−
σ
≤ η∗ (λ1 , ξ).


T −p
T −p

(B.6)

Now observe that B.3 in Lemma B.4 yields
2t2

2
eb
∂
b 0 ), t) = t2 − kY − Xθ(tλ0 )k2 ≤ t2 − (σ ∗ )2 (1 − τ0 )2 ,
Lλ0 (θ(tλ
∂t
(T − p)
1/2

where the last inequality follows from B.6 and the definition of τ0 , which implies η∗ (λ1 , ξ) ≤ σ ∗ τ0 for t < σ ∗ , since
λ1 = tλ0 . Note that when t = σ ∗ (1 − τ0 ), the expression on the right-hand side of the last inequality equals zero. Note
b 0 ), t) implies that σ
further that Lλ (·, ·) is strictly convex in σ. Then the negativity of 2t2 ∂/(∂t)Lλ0 (θ(tλ
b ≥ σ ∗ (1 − τ0 ).
∗
∗
On the other hand, at t = σ /(1 − τ0 ) > σ , we have
t2 −

b 0 )k2
e θ(tλ
kY − X
2
≥ t2 − (σ + tτ0 )2 ≥ 0,
(T − p)

1/2

since for t > σ ∗ , we have η∗ (λ1 , ξ) ≤ tτ0 . This result implies σ ∗ ≥ σ
b(1 − τ0 ) by the strict convexity of Lλ (·, ·) in σ.
Therefore,


σ
b
σ∗
max 1 − ∗ , 1 −
≤ τ0 .
(B.7)
σ
σ
b
1/2

As Sun & Zhang (2012) argue, η∗ (λ1 , ξ)/σ → 0 as (T − p, pd) → ∞. Thus,



σ

b(λ)


P 
− 1 >  → 0,
σ
for all  > 0 as (T − p, pd) → ∞.
B.4. Proof of Lemma A.4
We will need the following lemma, which is a modified version on Lemma 6.1 from Liu et al. (2013b) to prove Lemma A.4.
√
Lemma B.6. Let ξ1 , . . . , ξn ∈ Rp have mean zero. Suppose that p ≤ nr , log(p) = o( n), and E[kξi kbpr+2+
] ≤ ∞, for
2 P
n
r, b,  > 0. Furthermore, assume that k Cov(ξi ) − Ip×p k2 ≤ C(log(p))−2−γ , where Cov(ξi ) = E[(1/n) i=1 ξi ξi> ] and
γ > 0. Define k · kmin as kvkmin = min1≤i≤p {|vi |}. Then,


P
√

 P(k ni=1 ξi kmin ≥ t n)
 ≤ C(log(p))−1−γ1 ,

sup
−
1


p
√
(G(t))
0≤t≤b log(p)
where γ1 = min{γ, 1/2}.
√
Note that we make the assumptions p ≤ nr , r > 0 and log(p) = o( n), in the statement of Theorem 5.9. The former
assumption is clearly satisfied in our setting, as we can have r > 1. As noted by Liu & Luo (2014), given that p ≤ nr , for
e having bounded sub-Gaussian rows is equivalent to E[kξi kbpr+2+ ] ≤ ∞, for b,  > 0.
r > 0, our assumption 5.4 of X
2
Additionally, the assumption of k Cov(ξi ) − Ip×p k2 ≤ C(log(p)−2−γ is satisfied by Property 1, the sparsity property of the
covariance matrix. Whereas Liu et al. (2013b) prove Lemma B.6 for the i.i.d. case, we present a proof that draws on the
Bernstein inequality for martingale difference sequences (Lemma F.8) to prove this lemma when ξ1 , . . . , ξn ∈ Rp are not
independent. We defer the proof of Lemma B.6 to Appendix C.4. Having established Lemma B.6, we now present the proof
of Lemma A.4.
Proof of Lemma A.4. By Lemma B.6, we know that


ci ≥ ν)
 P(Z


 ≤ C(log(pd))−1−γ1 ,
max
sup
−
1


√
1≤i≤pd
G(ν)
0≤ν≤4 log(pd)

(B.8)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

p
ci | ≥
for positive constants C and γ1 . Then by (B.8), we have that P(|Z
2 log(pd)) → 1 for all i ∈ B =
p
e −1/2 ) ≥ c log(pd)/(T − p)}. Thus,
{i||θi∗ |/(σ Σ
i,i
p
P
c
2 log(pd)) P
i∈B P(|Zi | ≥
−
→ 1,
(B.9)
|B|
p
P
e −1/2 ) ≥ c log(pd)/(T − p)} →
since we assumed by Assumption 5.8 that |B| = i∈H1 1{|θi∗ |/(σ Σ
− ∞. If the number
i,i
of true alternatives were fixed, this convergence would clearly not occur. Now note that by Markov’s inequality


p
P
c
X
 E
2 log(pd))}
i∈B 1{|Zi | ≥
p
ci | ≥ 2 log(pd))} ≥ |B| ≤
P
1{|Z
|B|
i∈B
p
P
c
2 log(pd)))
i∈B P(|Zi | ≥
=
|B|
P

−
→ 1,
p
P
ci | ≥ 2 log(pd))} ≤ |B|, we have
where the convergence follows by (B.9). Therefore, since i∈B 1{|Z
p
P
c
2 log(pd))} P
i∈B 1{|Zi | ≥
−
→ 1.
|B|
This line implies that for 0 ≤ νb ≤ xpd , our FDR control procedure will correctly identify all true positives that meet a
certain minimum signal condition. The result of this lemma then follows from the definition of νb Section 4.2.
B.5. Proof of Lemma A.5
We will need the following lemma, which is a modified version on Lemma 6.2 from Liu et al. (2013b) to prove Lemma A.5.
√
Lemma B.7. Let η1 , . . . , ηn have mean zero, where ηt = (ηt,1 , ηt,2 )> . Suppose that p ≤ nr , log(p) = o( n), and
E[kξi k2bpr+2+ ] ≤ ∞, for r, b,  > 0. Furthermore, assume that V [ηt,1 ] = V [ηt,2 ] and |Cov(ηt,1 , ηt,2 )| ≤ δ, for some
0 ≤ δ ≤ 1. Then
 n



 X

 n

√
√ X




ηi,2  ≥ t n ≤ C(t + 1)−2 exp[−t2 /(1 + δ)],
P 
ηi,1  ≥ t n, 
i=1

i=1

for 0 ≤ t ≤ b log(2), where C depends only on b, r, , δ.
The proof of Lemma B.7 follows almost exactly the Proof of Lemma 6.2 in Liu et al. (2013b). The only difference is that
whereas Lemma 6.2 in Liu et al. (2013b) requires i.i.d. ηt vectors in order to cite the Proof of Lemma 6.1 in Liu et al.
(2013b), we do not require i.i.d. ηt vectors and instead appeal to the proof of Lemma B.6. We refer the reader to Liu et al.
(2013b) for more details. Having established Lemma B.7, we now present the proof of Lemma A.5.
2/3

δ

Proof of Lemma A.5. Let b0 < b1 < . . . < bk and νi = G−1 (bi ), where b0 = ypd /(pd), bi = ypd /(pd) + ypd ei /(pd),
2/3

k = [log((pd − ypd )/ypd )]1/δ , and 0 < δ < 1. Then we have G(νi )/G(νi+1 ) = 1 + o(1) for all 0 ≤ i ≤ k, and
p
ν0 / 2 log(pd/ypd ) = 1 + o(1). One can easily verify that 0 ≤ j ≤ k ↔ 0 ≤ ν ≤ ypd . So we see that to prove this lemma
it suffices to show that
P
ci | ≥ νj ) − G(νj )]  P
 i∈H0 [1(|Z

−
max
(B.10)
 → 0.
0≤j≤k 
|H0 |G(νj )
Observe that for all  > 0,
P
P




ci | ≥ νj ) − G(νj )] 
ci | ≥ νj ) − G(νj )] 
 i∈H0 [1(|Z
 i∈H0 [1(|Z





P max 
max 
 ≥  ≤ P 0≤j≤k
≥ 2
0≤j≤k
|H0 |G(νj )
|H0 |G(νj )
 P

k
ci | ≥ νj ) − G(νj )] 
X
 i∈H0 [1(|Z



≤
P 
≥ 2 .
|H0 |G(νj )
j=0

(B.11)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Now let
P
I(ν) =

i∈H0 [1(|Zi |

c ≥ ν) − P(|Z
ci | ≥ ν)]
.
|H0 |G(ν)

Note that
ci | ≥ νj ) − G(νj )]
c ≥ ν) − P(|Z
ci | ≥ ν)] P P
[1(|Z
−
→ i∈H0
,
|H0 |G(ν)
|H0 |G(νj )

P
I(ν) =

i∈H0 [1(|Zi |

by (B.8) in the proof of Lemma A.4 in Appendix B.4. Clearly, E[I(ν)] = 0, so if we can show that V [I(ν)] = E[I 2 (ν)] → 0,
P
we will have that I(ν) −
→ 0. By (B.11), this convergence will prove (B.10). We now decompose V [I(ν)] = E[I 2 (ν)] as
follows:
P
P
2 c
c
c
c
c
c
i,j∈H0 ,i6=j [P(|Zi | ≥ ν, |Zj | ≥ ν) − P(|Zi | ≥ ν)P(|Zj | ≥ ν)]
i∈H0 [P(|Zi | ≥ ν) − P (|Zi | ≥ ν)]
2
E[I (ν)] =
+
|H0 |2 G2 (ν)
|H0 |2 G2 (ν)
 c

cj | ≥ ν)
X
X
C|H0 |G(ν)
P(|Zi | ≥ ν, |Z
1
1
c
c
≤
+
P(|Zi | ≥ ν, |Zj | ≥ ν) +
−1 ,
(|H0 |G(ν))2 G2 (ν)|H0 |2
|H0 |2
G2 (ν)
T
c
(i,j)∈A()

i,j∈H0

A()

(B.12)
where the equality follows by direct computation, and the inequality holds by (B.8) in the proof of Lemma A.4 in Appendix
B.4. If we let
X
1
ci | ≥ ν, |Z
cj | ≥ ν),
I1,1 (ν) = 2
P(|Z
2
G (ν)|H0 |
(i,j)∈A()

and
I1,2 (ν) =

1
|H0 |2



X
i,j∈H0

T

A()c


ci | ≥ ν, |Z
cj | ≥ ν)
P(|Z
−
1
,
G2 (ν)

then (B.12) yields
C
+ I1,1 (ν) + I1,2 (ν).
|H0 |G(ν)
p
Applying Lemma B.6 to I1,2 (ν) yields the following result for all 0 ≤ t ≤ 2 log(pd) for some δ > 0:
E[I 2 (ν)] ≤

|I1,2 (ν)| ≤ C(log(pd))−1−δ .

(B.13)

(B.14)

Furthermore, Lemma B.7 yields

ci | ≥ ν, |Z
cj | ≥ ν) ≤ C exp
P(|Z


−ν 2
,
1 + |ρi,j | + δ1

(B.15)

for all (i, j) ∈ A() and i, j ∈ H0 , where δ1 , C > 0. Lastly, the proceeding result follows from (B.15) and Property 1
I1,1 (ν) ≤ C(log(pd))−2 .

(B.16)

Then by (F.2), (B.14), and (B.16), we have that
k
X

E[I(νj )]2 ≤ Ck[(log(pd))1−δ + (log(pd))−2 ] + C

j=0

k
X

(pdG(νj ))−1

j=0

≤C

k
X

1

j=0

ypd + ypd ej δ

2/3

+ o(1)

= o(1).
The second inequality holds by the definition of the sequence νi and since k = o(log(pd)). The third inequality follows
2/3 δ
since 1/(ypd + ypd ej ) = o(1/ypd ) = o(1/(pd)). Our desired result (B.10) follows naturally from this last set of
inequalities.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B.6. Proof of Lemma A.6
We present a variation on the argument made in the Proof of Theorem 3.1 in Liu et al. (2013b).
Proof of Lemma A.6. Since by the definition of νb in 4.9, νb is the infimum of all values ν > 0 such that FDP(ν) ≤ α, for
ν < νb
P
c
i∈H0 1(|Zi | ≥ ν)
> α.
P
ci ≥ ν), 1}
max{
1(|Z
1≤j≤pd

ci , which holds by Theorem 5.5, we can approximate P
c
Using the asymptotic normality of Z
i∈H0 1(|Zi | ≥ ν) by (pd)G(ν),
yielding

max{

P

(pd)G(ν)
> α,
ci ≥ ν), 1}
1(|Z

1≤j≤pd

P
ci ≥ ν), 1} is decreasing in ν. Then by letting ν approach νb, we obtain
for ν < νb. Note that (pd)G(ν)/ max{ 1≤j≤pd 1(|Z

max{

P

(pd)G(b
ν)
≥ α.
ci ≥ νb), 1}
1(|Z

(B.17)

1≤j≤pd

Now to prove the reverse bound, we note that the definition of infimum implies the existence of a sequence νk , where
k→∞
νk ≥ νb and νk −−−−→ νb. Since νk ≥ νb, we have
(pd)G(νk )
≤ α.
P
ci ≥ νk ), 1}
max{ 1≤j≤pd 1(|Z
Thus, by letting νk → νb, we see that
max{

P

(pd)G(b
ν)
≤ α.
ci ≥ νb), 1}
1(|Z

(B.18)

1≤j≤pd

Therefore, by (B.17) and (B.18)
(pd)G(b
ν)
= α,
P
ci ≥ νb), 1}
max{ 1≤j≤pd 1(|Z
as desired.

C. Proofs of Auxiliary Lemmas in Appendix B
In this section we present the proofs of auxiliary lemmas introduced in Appendix B.
C.1. Proof of Lemma B.1
Here we present a modified version of the proof for Theorem 7.(b) from Javanmard & Montanari (2014).
e n − Ik∞ ≤ kΣ
e −1 Σ
e n − Ik∞ . Let X t = Σ
e −1/2 X
ft , where X
ft is as defined in Section
Proof of Lemma B.1. Clearly kMΣ
pd×pd
3. Now define Z ∈ R
as follows:




T
T
X
X
e −1 Σ
en − I = 1
e −1 X
ft X
f> − I = 1
e −1/2 X t X > Σ
e 1/2 − I .
Z=Σ
Σ
Σ
t
t
T − p t=p+1
T − p t=p+1
(ij)
e −1/2 , X t i · hΣ
e 1/2 , X t i − δi,j , where p + 1 ≤ t ≤ T , and
For any given pair 1 ≤ i, j ≤ pd, denote γt
= hΣ
i,·
j,·
δi,j represents the Kronecker delta: δi,j = 1(i = j). Let Ft be the filtration Ft = σ(X 1 , . . . , X t ). Then note that

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference
(ij)

(ij)

E[γt |Ft−1 ] = 0, so by Definition G.1 in Appendix G, γt forms a martingale difference sequence. Note further that
PT
(ij)
e −1 Σ
e n − Ik∞ , and by transitivity kMΣ
e n − Ik∞ , we need only bound
Zi,j = (T − p)−1 t=p+1 γt . Thus, to bound kΣ
Zi,j .
We refer the reader to Definition G.2 in Appendix G for the definition of the sub-exponential norm. By Remark 5.18
(ij)
(Centering) from Vershynin (2012), we can bound the sub-exponential norm of γt as follows:
(ij)

kγt

−1/2

e
kψ1 ≤ 2khΣ
i,·

1/2

e , X t ikψ .
, X t i · hΣ
1
j,·

(C.1)

Now note that, as shown by Javanmard & Montanari (2014) we can bound the sub-exponential norm of the product of two
random variables X and Y by:
 
1/q
kXY kψ1 ≤ sup q −1 E |XY |q
q≥1

 
1/2q  
1/2q
≤ sup q −1 E |X|2q
E |Y |2q
q≥1


 
1/r 
 
1/r 
−1/2
r
−1/2
r
≤ 2 sup r
E |X|
sup r
E |Y |
r≥2

r≥2

≤ 2kXkψ2 · kY kψ2 .
Therefore, by (C.1)
(ij)

kγt

e −1/2 , X t i · hΣ
e 1/2 , X t ikψ ≤ 2khΣ
e −1/2 , X t ikψ · khΣ
e 1/2 , X t ikψ ,
kψ1 ≤ 2khΣ
1
2
2
i,·
j,·
i,·
j,·

and by assumption
s
−1/2

e
2khΣ
i,·

e 1/2 , X t ikψ ≤ 2kΣ
e −1/2 kψ · kΣ
e 1/2 kψ κ2 ≤ 2κ2
, X t ikψ2 · khΣ
2
2
2
j,·
i,·
j,·

Cmax
.
Cmin

p
(ij)
(ij)
Thus, if we let κ0 = 2κ2 Cmax /Cmin , then kγt kψ1 ≤ κ0 . Now, since γt is a martingale difference sequence, we can
apply the Bernstein inequality for martingale difference sequences (Lemma F.8 in Appendix F) to obtain:

P


 T


 2

T −p


1  X (ij) 
≥

≤
2
exp
−
γ
min
,
.
T − p t=p+1 t 
6
eκ0
eκ0

p
Let  = a log(pd)/(T − p), and assume that T − p ≥ (a/(eκ0 ))2 log(pd) so that (/(eκ0 ))2 ≤ (/(eκ0 )) ≤ 1. Then,

P

s
!
 T

1  X (ij) 
log(pd)
γ
≥a
=P
T − p  t=p+1 t 
T −p

s
!




log(pd)
Zi,j  ≥ a


T −p

≤ 2(pd)−a

2

/(6e2 κ02 )

2

= 2(pd)−(a

Cmin )/(24e2 κ4 Cmax )

.

Taking the union over all (pd)2 pairs and letting c2 = (a2 Cmin )/(24e2 κ4 Cmax ) − 2 yields the result:

e n − Ik∞ ≤ a
P kMΣ

s


log(pd)
 ≥ 1 − 2(pd)−c2 .
T −p

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

C.2. Proof of Lemma B.2
The proof of Lemma B.2 relies on two lemmas. First, the following lemma asserts that the restricted eigenvalue condition
e As we will see in the proof of Lemma B.2 below, the restricted eigenvalue
(RE condition) holds true for our design matrix X.
condition implies the restricted strong convexity condition when the loss function is the least squares loss function. This
property will prove instrumental in bounding kθb − θ ∗ k1 .
Lemma C.1. Under Assumptions 5.3 and 5.4, we have
inf
b ∗ ∈Er
θ−θ

1
e θb − θ ∗ )k2 > 0
kX(
2
T −p

with probability at least


(ω(A))2
,
1 − 2 exp(−c20 c21 c2 ω 2 (B)) − L exp − 4
α2
1
e 2 is the restricted minimum eigenvalue of Σ
e n restricted to A ⊆ S pd−1
kXuk
2
T −p
e 1/2 u/kΣ
e 1/2 uk2 , u ∈ A} is the normalized set of A, α = diam(A) =
e=Σ
(the unit sphere in Rpd space), B = {e
u:u
supu,v∈A d(u, v) = supu,v∈A ku − vk2 , and c0 , c1 , c2 , L > 0 are constants.

e n |A) = inf u∈A
where where λmin (Σ

The RE condition has been studied extensively in the setting where the rows of the design matrix are independent. However,
since the rows of the design matrix are dependent in this setting, we must appeal to martingale theory to prove the RE
e θb − θ ∗ )k2 , and then bound the minimum
condition. We construct a martingale difference sequence equal to 1/(T − p)kX(
2
restricted eigenvalue of that sequence using the results from Appendix F. We defer the proof of Lemma C.1 to Appendix
D.1.
Second, the following lemma establishes with high-probability a property of the regularization parameter λ.
−1
e 2
Lemma C.2.
p Denote the least-squares loss function L(θ) = (2(T − p)) kY − Xθk2 . If Assumption 5.4 holds and
λ = 8Cσ log(pd)/(T − p) for some constant C, then
λ ≥ 2k∇L(θ ∗ )k∞ ,
holds with probability at least 1 − b1 exp[−b2 σ 2 log(pd)], for constants b1 and b2 .
We defer the proof of Lemma C.2 to Appendix D.2.
We now present a proof of Lemma B.2.
Proof of Lemma B.2. To prove this lemma, we first recall the form of the biased Lasso Granger estimator from (3.3):
θb = arg min
θ

1
e 2 + λkθk1 .
kY − Xθk
2
2(T − p)

e 2 . We immediately realize that the optimality of θb
For brevity, we denote the loss function L(θ) = (2(T − p))−1 kY − Xθk
2
yields the following inequality:
b + λkθk
b 1 ≤ L(θ ∗ ) + λkθ ∗ k1 .
L(θ)

(C.2)

b we will appeal to the restricted strong convexity condition (RSC condition), which
To establish a lower bound on L(θ),
b
provides a lower bound on the first-degree Taylor approximation of L(θ):
b := L(θ)
b − L(θ ∗ ) − h∇L(θ ∗ ), θb − θ ∗ i ≥ κ` kθb − θ ∗ k2 > 0,
δL(θ)
2

(C.3)

for some constant κ0` . As noted by Negahban et al. (2012), when L(·) is the least-squares loss function, as it is in our setting,
we obtain
b =
δL(θ)

1
e θb − θ ∗ )k2 ≥ κ0 kθb − θ ∗ k2 .
kX(
2
`
2
T −p

(C.4)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Note that
inf
b ∗ ∈Er
θ−θ



1
1 e> e
e θb − θ ∗ )k2 = λmin
e n |Er ),
kX(
X
X|E
= λmin (Σ
r
2
T −p
T −p

(C.5)

e n |Er ) is the minimum restricted eigenvalue of the
where Er is the set the error vector θb − θ ∗ can fall in, and λmin (Σ
sample covariance matrix. So we see that when L(·) is the least-squares loss function, the restricted strong convexity
e with high probability by Lemma C.1. Thus,
condition collapses into the RE condition. The RE condition holds for X
e
λm in(Σn |Er ) > 0, and so the RSC condition holds. Then, rearranging (C.3), we see that
b ≥ L(θ ∗ ) + h∇L(θ ∗ ), θb − θ ∗ i + κ` kθb − θ ∗ k2 ,
L(θ)
2
2

(C.6)

where κ` = 2κ0` .
As a consequence of (C.2) and (C.6), we have
b 1 ≤ L(θ)
b + λkθk
b 1 ≤ L(θ ∗ ) + λkθ ∗ k1 .
L(θ ∗ ) + h∇L(θ ∗ ), θb − θ ∗ i + λkθk
Furthermore, since h∇L(θ ∗ ), θb − θ ∗ i ≤ k∇L(θ ∗ )k∞ · kθb − θ ∗ k1 by Holder’s inequality, we achieve the following result:
b 1 ≤ λkθ ∗ k1 .
−k∇L(θ ∗ )k∞ · kθb − θ ∗ k1 + λkθk

(C.7)

We apply Lemma C.2 to establish that λ ≥ 2k∇L(θ)k∞ with probability at least 1 − b1 exp[−b2 σ 2 log(pd)], for constants
b1 and b2 . Thus, since λ ≥ 2k∇L(θ ∗ )k∞ with high probability, (C.7) implies
1
b 1 ≤ λkθ ∗ k1 .
− λkθb − θ ∗ k1 + λkθk
2
Now denote S to be the support of θ ∗ , so that θ ∗ = θS∗ and θS∗ c = 0. Then, based on the previous inequality, we have
1
1
− λk(θb − θ ∗ )S k1 − λk(θb − θ ∗ )S c k1 + λkθbS k1 + λkθbS c k1 ≤ λkθS∗ k1 .
2
2
Rearranging these terms yields:
1
1
− λk(θb − θ ∗ )S k1 − λk(θb − θ ∗ )S c k1 + λk(θb − θ ∗ )S c k1 ≤ λkθS∗ k1 − λkθbS k1
2
2
≤ λk(θb − θ ∗ )S k1 ,
where the second inequality follows by the triangle inequality. Rearranging terms once more produces the following result:
λk(θb − θ ∗ )S c k1 ≤ 3λk(θb − θ ∗ )S k1 .

(C.8)

We use (C.8) to bound λkθb − θ ∗ k1 . Note that (C.2) and (C.6) together imply
1
b 1 + κ` kθb − θ ∗ k2 ≤ λkθ ∗ k1 ,
− λkθb − θ ∗ k1 + λkθk
2
2
2
and thus,
κ` b
b 1 + 1λkθb − θ ∗ k1 .
kθ − θ ∗ k22 ≤ λkθ ∗ k1 − λkθk
2
2
Applying the triangle inequality yields
κ` b
1
kθ − θ ∗ k22 ≤ λkθb − θ ∗ k1 + λkθb − θ ∗ k1
2
2
3
= λk(θb − θ ∗ )k1 .
2

(C.9)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Note that by (C.8), we have kθb − θ ∗ k1 ≤ 4k(θb − θ ∗ )S k1 . Substituting this result into (C.9) allows us to conclude that
κ` b
√
√
kθ − θ ∗ k22 ≤ 6λk(θb − θ ∗ )S k1 ≤ 6λ s0 k(θb − θ ∗ )S k2 ≤ 6λ s0 kθb − θ ∗ k2 .
2
√
Thus, kθb − θ ∗ k2 ≤ 12λ s0 /κ` , which offers us the result of this lemma:
kθb − θ ∗ k1 ≤

√

s0 kθb − θ ∗ k2 ≤

12λs0
.
κ`

We note that this result holds with high probability by Lemma C.2.

C.3. Proof of Lemma B.3
Here we verify that the Lindeberg condition (Hall & Heyde, 1980) holds for ζi,t .
f> mi )2 /(σ 2 [m> Σ
e n mi ])]−1/2 , some
Proof of Lemma B.3. Note that for some random variable Q ∼ N (0, 1), C = [(t X
t
i
positive, fixed constant δ > 0, and t ≥ p:
E[ζt2 1(kζt | ≥ δ)|Ft−1 ] =

f> mi )2 E[Q2 1(|Q| > δC)]
(t X
t
.
e n mi ]
σ 2 [m> Σ

(C.10)

i

By the properties of the truncated standard normal, we can see that E[Q2 |Q > c] = 1 + c(φ(c)/Φ(c)), where φ(c) is the
PDF of the standard normal. Thus,




φ(c)
c + c2
E[Q 1(Q > c)] = 2Φ(c) 1 + c
= 2(Φ(c) + cφ(c)) ≤ 2φ(c)
.
Φ(c)
c
2

Now the proceeding inequality follows from the union bound and a standard bound on the normal CDF:
max

p+1≤t≤T

f> mi | > v,
|t X
t

q
>e
e
with probability at most 2(T − p) exp[−v 2 /(2m>
i Σmi )]. If we let v = 2 log(T − p)mi Σmi , the we obtain the
following bound:
max

p+1≤t≤T

q
f> mi | ≤ 2 log(T − p)m> Σm
e i,
|t X
t
i

with probability at least 1 − 2/(T − p). Returning to (C.10), we now see that for D =
E[ζt2 1(k zetat k ≥ δ)|Ft−1 ] ≤

p
(T − p)/(4 log(T − p):

8 log(T − P )φ(δD)((δD)−1 + δD)
,
T −p

since φ(z)(z −1 + z) is a decreasing function. Therefore, if we sum over all t, we attain
T
X

E[ζt2 1(kζt | ≥ δ)|Ft−1 ] ≤ 8 log(T − P )φ(δD)((δD)−1 + δD) → 0,

t=p+1

which demonstrates that the Lindeberg condition does indeed hold for ζt .

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

C.4. Proof of Lemma B.6
We present a modified version of the proof of Lemma 6.1 from Liu et al. (2013b).
Proof of Lemma B.6. Define filtration Ft = σ(ξ1 , . . . , ξt ). For 1 ≤≤ p, let:
ξbt = ξt 1{kξt k2 ≤
ξet = ξt − ξbt .

√

√

n/(log(p))4 } − E[ξt 1{kξt k2 ≤

n/(log(p))4 }|Ft ],
(C.11)

Note that by Definition G.1, ξbi forms a martingale difference sequence (MDS). Now we have by the triangle inequality

 X
 n 

ξ
P 
t


√
≥t n



min

t=1


 X
 n

√
√
bt ≥ t n − n/(log(p))2 
ξ
≤P 





min

t=1


 X
 n

√
et ≥ n/(log(p))2 
ξ
+P 




,

(C.12)

min

t=1

and

 X
 n 

ξ
P 
t


√
≥t n



min

t=1


 X
 n

√
√
bt ≥ t n + n/(log(p))2 
≥P 
ξ





min

t=1


 X
 n

√
2
e

−P 
ξt ≥ n/(log(p)) 


.

(C.13)

min

t=1

√
By our assumption that log(p) = o( n), we have that,
n
X

E[ξt 1{kξt k2 ≤

√

√
n/(log(p))4 }|Ft ] = o( n/(log(p))2 ).

t=1

So by our assumption that E[kξt kbpr+2+
] ≤ ∞, for r, b,  > 0, the previous line implies that:
2

 X
 n

√
et ≥ n/(log(p))2 
P 
ξ




≤ nP( max kξt k ≥
1≤t≤n

min

t=1

√

n/(log(p))4 ) ≤ C(log(p))−3/2 (G(t))p ,

p
for 0 ≤ t ≤ b log(p). Thus, by (C.13) and (C.12), we obtain:

 X
 n 

P 
ξ
t


√
≥t n


 X
 n 

P 
ξ
t


√
≥t n

t=1



min


 X
 n

√
√
bt ≥ t n + n/(log(p))2 
≥P 
ξ






 X
 n

√
√
bt ≥ t n − n/(log(p))2 
≤P 
ξ





t=1

− C(log(p))−3/2 (G(t))p ,

min

and

t=1

min



t=1

+ C(log(p))−3/2 (G(t))p ,

min

for constant C > 0 Therefore, it suffices to prove
sup
√

0≤t≤b

log(p)

P
√


 P(k nt=1 ξbt kmin ≥ (t ± (log(p)−2 ) n)


− 1 ≤ C(log(p))−1−γ1

(G(t))p

(C.14)

in order to prove this lemma. To prove this line, we appeal to Theorem 1.1 from Zaitsev (1987). The original version of
Theorem 1.1 requires i.i.d. vectors only in order to leverage the Bernstein inequality. However, since in this application ξbt
form a MDS, the proof of Theorem 1.1 holds by our Bernstein inequality for MDS (Lemma F.8). In the interest of clarity,

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

since Theorem 1.1 requires introducing a significant amount of new material from probability theory, we refer the reader to
Zaitsev (1987) for more details. Thus, by application of Theorem 1.1 from Zaitsev (1987) to ξbt , we obtain

 X

 n 
√
−2
bt 
P 
ξ
≥
(t
+
(log(p)
)
n
≤ P(kW kmin ≥ t − 2(log(p))−2 ) + c1,p exp[−c2,d (log(p))2 ],


t=1

min

and

 X
 n 
bt 
ξ
P 


t=1


√
≥ (t − (log(p)−2 ) n ≤ P(kW kmin ≥ t + 2(log(p))−2 ) − c1,p exp[−c2,d (log(p))2 ],

min




Pn b √
where c1,p , c2,p > 0 are constants that depend only on p and W ∼ N 0, Cov
n
. By our assumption that
ξ
/
t=1 i
k Cov(ξi ) − Ip×p k2 ≤ C(log(p)−2−γ , one can easily show that,
P(kW kmin ≥ t − 2(log(p))−2 ) ≤ (1 + C(log(p))−1−γ1 )(G(t))p ,
and
P(kW kmin ≥ t + 2(log(p))−2 ) ≤ (1 − C(log(p))−1−γ1 )(G(t))p ,
p
p
for 0 ≤ t ≤ b log(p). Since for 0 ≤ t ≤ b log(p) we have c1,p exp[−c2,p (log(p))2 ] ≤ C(log(p))−1−γ1 (G(t))p , the
following holds:

 X

 n 
√
−2
b


P 
(C.15)
ξt 
≥ (t − (log(p) ) n ≤ (1 + C(log(p))−1−γ1 )(G(t))p ,
t=1

min

and

 X
 n 
b

P 
ξt 

t=1

−2

≥ (t + (log(p)

√

) n



≤ (1 − C(log(p))−1−γ1 )(G(t))p ,

(C.16)

min

p
for 0 ≤ t ≤ b log(p). Therefore, (C.15) and (C.16) imply (C.14), which concludes the proof.

D. Proofs of Supporting Lemmas in Appendix C
In this section we present proofs for the supporting Lemmas of Lemma B.2.
D.1. Proof of Lemma C.1
We present a variation of the proof of Theorem 5 from Johnson et al. (2016). This proof will rely on lower bounding
1
PT
2
e θb − θ ∗ )k2 by inf u∈A 1/(T − p) PT
f
f
inf θ−θ
kX(
b ∗ ∈Er
2
t=p+1 hXt − µt , ui − supu∈A 2/(T − p)
t=p+1 hXt − µt , ui.
T −p
PT
P
ft −µt , ui2 in Lemma D.1 and upper bound supu∈A 2/(T −p) T
f
We lower bound inf u∈A 1/(T −p) t=p+1 hX
t=p+1 hXt −
µt , ui in Lemma D.2 below.
ft − µt , for µt = E[X
ft |Ft−1 ], where Ft is the filtration Ft = σ(X
fp+1 , . . . , X
ft ), and Z =
Lemma D.1. Let Zt = X
−1/2

[Zp+1 , . . . , ZT ]> . Furthermore, let E[Z> Z/(T − p)] = ΣZ and kΣZ

Zt kψ2 ≤ κ0 . Then,




T
X
2c0 c1 κ02 ω(B)
1
2
f
√
P inf
hXt − µt , ui ≥ λmin (ΣZ |A) 1 −
> 0 ≥ 1 − 2 exp(−c20 c21 c2 ω 2 (B))
u∈A T − p
T
−
p
t=p+1


where λmin (Σ|A) = inf u∈A u> Σu is the restricted minimum eigenvalue of Σ restricted to A ⊆ S pd−1 (the unit sphere in
e = Σ1/2 u/kΣ1/2 uk2 , u ∈ A} is the normalized set of A.
Rpd space), and B = {e
u:u
The proof of Lemma D.1 relies on the restricted eigenvalue condition for martingale difference sequences from Appendix F.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

ft |Ft−1 ], where Ft is the filtration Ft = σ(X
fp+1 , . . . , X
ft ), A ⊆ S pd−1 (the unit sphere in
Lemma D.2. Let µt = E[X
pd
R space), and α = diam(A) = supu,v∈A d(u, v). Then,



T
X
(ω(A))2
2
f
.
hXt − µt , ui ≤ 0 ≥ 1 − L exp − 4
α2
u∈A T − p t=p+1


P sup

The proof of Lemma D.2 employs a generic chaining argument (Talagrand, 2006). We defer the proofs of Lemmas D.1 and
D.2 to Appendicies E.1 and E.2. We now present the proof of Lemma C.1.
Proof of Lemma C.1. We seek to prove that,
inf
b ∗ ∈Er
θ−θ

1
e θb − θ ∗ )k2 > 0.
kX(
2
T −p

(D.1)

From Negahban et al. (2012), we note that the error set Er is actually a cone, and that the magnitude of the error vector
θb − θ ∗ in (D.1) does not matter, only the direction does. Thus, we consider set A = S pd−1 ∩ Er and reformulate our
problem as
inf

u∈A

1
e 2 > 0.
kXuk
2
T −p

(D.2)

It suffices to prove (D.2) to prove the result of this lemma.
ft |Ft−1 ],
We now construct a martingale difference sequence that we will bound in order to prove (D.2). Let µt = E[X
f
f
f
where Ft is the filtration Ft = σ(Xp+1 , . . . , Xt ), so that by Definition G.1 in Appendix G, Xt − µt forms a martingale
difference sequence (MDS). Then we have,
T
X
1
1
2
e
ft , ui
kXuk2 =
hX
T −p
T − p t=p+1



T
X
1
>
2
>
>
>
>
>
2
>
2
f
f
f
=
(Xt u) − 2(Xt u)(µt u) + 2(Xt u)(µt u) − (µt u) + (µt u)
T − p t=p+1


T
X
1
>
>
2
>
2
>
>
f
f
=
(Xt u − µt u) − (µt u) + 2(Xt u)(µt u) .
T − p t=p+1
Distributing the summation in the last line then yields:
T
T
T
X
X
X
2
1
ft − µt , ui2 − 1
ft , uihµt , ui
e 2= 1
hX
hµt , ui2 +
hX
kXuk
2
T −p
T − p t=p+1
T − p t=p+1
T − p t=p+1

=

T
T
T
X
X
X
1
2
ft − µt , ui2 + 1
ft − µt , uihµt , ui
hX
hµt , ui2 +
hX
T − p t=p+1
T − p t=p+1
T − p t=p+1

≥

T
T
X
X
1
ft − µt , ui2 + 2
ft − µt , uihµt , ui.
hX
hX
T − p t=p+1
T − p t=p+1

Hence,
T
T
X
X
1
1
2
e 2 ≥ inf
ft − µt , ui2 + inf
ft − µt , uihµt , ui
kXuk
hX
hX
2
u∈A T − p
u∈A T − p
u∈A T − p
t=p+1
t=p+1

inf

T
T
X
X
1
ft − µt , ui2 − sup 2
ft − µt , ui,
hX
hX
u∈A T − p
T
−
p
u∈A
t=p+1
t=p+1

≥ inf

(D.3)

(D.4)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where the second inequality holds since we can scale the design matrix to fall in the L2 unit ball so that hµt , ui <= 1.
PT
ft − µt , ui2 and upper bound sup
Thus, to prove (D.2), we must lower bound inf u∈A 1/(T − p) t=p+1 hX
u∈A 2/(T −
P
PT
T
2
f
f
p) t=p+1 hXt − µt , ui. We bound inf u∈A 1/(T − p) t=p+1 hXt − µt , ui with Lemma D.1, and supu∈A 2/(T −
PT
ft − µt , ui with Lemma D.2. Thus, by application of Lemmas D.1 and D.2 to (D.3), we have that
p) t=p+1 hX
T
T
X
X
1
1
ft − µt , ui2 − sup 2
ft − µt , ui
e 2 ≥ inf
h
X
hX
kXuk
2
u∈A T − p
u∈A T − p
u∈A T − p t=p+1
t=p+1


2c0 c1 κ02 ω(B)
√
≥ λmin (ΣZ |A) 1 −
> 0,
T −p

inf

with probability at least
1−

2 exp(−c20 c21 c2 ω 2 (B))


(ω(A))2
.
− L exp − 4
α2


e satisfies that restricted eigenvalue condition with high probability.
Thus, X
D.2. Proof of Lemma C.2
Here we prove Lemma C.2.



e > 
Proof of Lemma C.2. To prove this lemma, we first note that 2k∇L(θ ∗ )k∞ = 4 (T − p)−1 X
 . Let  be the error
∞

e ∗ . Then Assumption 5.4 implies that  (3.2) is sub-Gaussian. Note that since Assumption
vector from (3.2), so  = Y − Xθ
f
f so that kX
e ·,j k2 /√T − p ≤ 1,
5.4 ensures that kXi kψ2 ≤ κ, for i ∈ {1, 2, . . . , T − p}, we can scale the columns of any X
for 1 ≤ k ≤ pd. Then the sub-Gaussian tails of  guarantee that for all t > 0
1
e ik2 < t,
khX,
T −p
with probability at least 1 − 2 exp[−(T − p)t2 /(2σ 2 )]. Bounding over all pd columns yields:


 1


>
e 
X

 < t,
T − p

∞

p
holds with probability at least 1 − 2 exp[−(T − p)t2 /(2σ 2 ) + log(pd)]. Setting t = 2σ log(pd)/(T − p) allows us to
conclude that
s


 1

log(pd)

∗
> 
e
λ = 8σ
≥ 2k∇L(θ )k∞ = 4 
X  ,
T − p

T −p
∞

holds with probability at least 1 − b1 exp[−b2 σ 2 log(pd)].

E. Proofs of Supporting Lemmas in Appendix D
In this section, we present proofs of the supporting lemmas for Lemma C.1.
E.1. Proof of Lemma D.1
PT
ft − µt , ui2 . Since {X
ft − µt } is a martingale difference sequence (MDS)
We seek to bound inf u∈A 1/(T − p) t=p+1 hX
by Definition G.1 in Appendix G, we will appeal to the restricted eigenvalue condition for MDS that we present in Theorem
F.6 in Appendix F. We reproduce Theorem F.6 here for the convenience of the reader:

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Theorem E.1. Let X = (X1 , · · · , Xn )> be a n × d design matrix whose anisotropic sub-Gaussian rows form a vector
valued martingale difference sequence. Let E[X> X/n] = Σ and kΣ−1/2 Xi kψ2 ≤ κ. Then for absolute constants
c0 , c1 , c2 > 0, with probability at least 1 − 2 exp(−c20 c21 c2 ω 2 (B)), we have


1
2c0 c1 κ2 ω(B)
√
≤ inf kXuk22 ,
λmin (Σ|A) 1 −
u∈A n
n
where λmin (Σ|A) = inf u∈A u> Σu is the restricted minimum eigenvalues of Σ restricted to A ⊆ S d−1 (the unit sphere in
e = Σ1/2 u/kΣ1/2 uk2 , u ∈ A} is the normalized set of A.
Rd space), and B = {e
u:u
ft − µt for µt = E[X
ft |Ft−1 ], where Ft is the filtration Ft = σ(X
fp+1 , . . . , X
ft ), as
Proof of Lemma D.1. Let Zt = X
>
defined in Appendix D.1. Then let Z = [Zp+1 , . . . , ZT ] . Clearly the rows of Z form a vector-values MDS, and by
e in Section 3 implies that the rows or Z are anisotropic.
Assumption 5.4, they are sub-Gaussian as well. The definition of X
−1/2
>
Let E[Z Z/(T − p)] = ΣZ be the true covariance matrix of Z, and kΣZ Zt kψ2 ≤ κ0 . Then by Theorem F.6 we have
that,


T
X
1
1
2c0 c1 κ02 ω(B)
2
2
f
√
hXt − µt , ui = inf kZuk2 ≥ λmin (ΣZ |A) 1 −
inf
,
u∈A n
u∈A T − p
T −p
t=p+1
with high probability. Thus, to prove Lemma D.1, it suffices to demonstrate that λmin (ΣZ |A), the restricted minimum
eigenvalue of ΣZ , is positive. In this endeavor, we will draw upon the argument made by Johnson et al. (2016) in a similar
context.
e so that its rows fall
Let B2pd (x, ) be a L2 ball centered at x with radius . Clearly, we can scale the orignial design matrix X
ft − µt },
in a L2 unit ball, in which case the rows of Z fall in a L2 unit ball centered at the origin. So then the set, A = {X
f
where Xt is drawn from the aforementioned L2 ball, is a subset of the L2 ball centered at the origin. Now note that by
definition λmin (ΣZ |A) ≥ λmin (ΣZ ), where λmin (ΣZ ) is the unrestricted minimum eigenvalue of ΣZ . So it suffices to
show that λmin (ΣZ ) > 0. By way of contradiction, assume that λmin (ΣZ ) = 0. Then let the eigendecomposition of ΣZ
be ΣZ = QΛQ−1 , where Q = [v1 , . . . , vpd ] has the eigenvectors of ΣZ for columns, and Λ = diag(λi )pd
i=1 is a diagonal
matrix of the eigenvalues of ΣZ in descending order. Observe that λpd = 0 implies
Ea∼A [ha, vpd i] = 0,

(E.1)

since vpd is the eigenvector corresponding to the minimum eigenvalue of 0. Let Avpd = {a ∈ A|ha, vpd i = 0}. Then
clearly, (E.1) implies,
P(a ∈ Avpd ) = 1.

(E.2)

Note that by (E.2), since there is no probability density outside Avpd , this density is thus concentrated on a subspace of Rpd .
Here we have a contradiction, since the span of Avpd is Rpd . Therefore, λmin (ΣZ |A) ≥ λmin (ΣZ ) > 0, and we have


T
X
1
c1 κ02 ω(B)
ft − µt , ui2 = inf 1 kZuk2 ≥ λmin (ΣZ |A) 1 − 2c0√
hX
> 0,
2
u∈A T − p
u∈A n
T −p
t=p+1
inf

with probability at least 1 − 2 exp(−c20 c21 c2 ω 2 (B)), as desired.
E.2. Proof of Lemma D.2
In this section we present a proof for Lemma D.2. This proof will make a generic chaining argument, and will thus rely on
the following standard lemmas from Talagrand (2006) and Talagrand (2014).
Lemma E.2 (Theorem 2.1.5 from Talagrand (2006)). Consider two processes {Xt }t∈A and {Yt }t∈A , indexed by the same
set A. Assume {Xt }t∈A is Gaussian, and {Yt }t∈A satisfies the condition:


− δ2
∀δ > 0, ∀u, v ∈ A, P(|Yu − Yv | > δ) ≤ 2 exp
,
d(u, v)2

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where d(u, v) is the distance metric associated with Xt (we assume d(u, v) = ku − vk2 ). Then, for some constant L,
E[ sup |Yu − Yv |] ≤ LE[sup Xv ]]
u,v∈A

v∈A

Lemma E.3 (Lemma 1.2.8 from Talagrand (2006)). If the process {Xt }t∈A is symmetric, then
E[ sup |Xu − Xv |] = 2E[sup Xu ]].
u,v∈A

u∈A

Lemma E.4 (Theorem 2.2.27 from Talagrand (2014)). Let {Xt }t∈A be a process that satisfies Lemma E.2. Then for any
δ > 0 and constant L,


P sup |Xu − Xv | ≥ L(γ2 (A, d(u, v)) + δα) ≤ L exp(−δ 2 ),
u,v∈A

where α = diam(A) = supu,v∈A d(u, v), and γ2 (·, ·) is the γ2 -functional defined in F.4.
Lemma E.5 (Theorem 2.2.27 from Talagrand (2014)). For constant L,
1
γ2 (A, d(u, v)) ≤ E sup Xu ≤ Lγ2 (A, d(u, v)).
L
u∈A
Having presented these lemmas, we now give the proof of Lemma D.2.
2 PT
ft − µt , ui. Recall that by Assumption 5.4, the subhX
T − p t=p+1
ft − µt kψ ≤ κ. Thus, X
ft is bounded by κ, which implies that kX
ft − µt forms a sub-Gaussian
Gaussian norm of each row X
2
bounded MDS, and so we can apply the Azuma-Hoeffding inequality to obtain
Proof of Lemma D.2. We seek to bound supu∈A


 T





− 2
1  X
f

hXt − µt , ui ≥  ≤ 2 exp
,
P √
2kuk22 κ2
T − p t=p+1

(E.3)

for any u ∈ A. Then for u, v ∈ A, we have that

 T





− 2
1  X
f
 ≥  ≤ 2 exp
P √
h
X
−
µ
,
u
−
vi
.
t
t

2ku − vk22 κ2
T − p t=p+1

(E.4)

PT
√
ft − µt , ui with high probability.
We now make a generic chaining argument to bound supu∈A 1/ T − p t=p+1 hX
fp+1 −µp+1 ), . . . , (X
fT −µT )]> . We first note that by (E.3), the process Zu = hZ, ui = 1/√T − phX
ft −µt , ui
Let Z = [(X
has sub-Gaussian concentration. Similarly, by (E.4), Zu − Zv is a sub-Gaussian process ∀u, v ∈ A. We now bound
PT
√
ft − µt , ui] in terms of the Gaussian width of set A (see Definition F.5), and then prove
E[supu∈A 1/ T − p t=p+1 hX
PT
√
ft − µt , ui concentrates around its expectation with high probability.
hX
that supu∈A 1/ T − p
t=p+1

PT
√
ft − µt , ui], we appeal to Lemma E.2. In our setting, E[supv∈A Xv ] = ω(A),
To bound E[supu∈A 1/ T − p t=p+1 hX
the Gaussian width of A. Thus, by Lemma E.2 and (E.4), we achieve the following bound for some constant L:
E[ sup |Zu − Zv |] ≤ Lκω(A).

(E.5)

u,v∈A

Note that by (E.3), the process Zu is symmetric, and so Lemma E.3 applies. Thus, by Lemma E.3 and (E.5), we have that,
E[ sup |Zu − Zv |] = 2E[sup Zu ]] ≤ Lκω(A).
u,v∈A

u∈A

(E.6)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

PT
√
ft − µt , ui] as follows:
Thus, the definition of Zu , we can bound E[supu∈A 2/ T − p t=p+1 hX

T
X
2
f
hXt − µt , ui = 2E[sup Zu ]] ≤ Lκω(A).
u∈A
u∈A T − p t=p+1


2E sup

PT
√
ft − µt , ui] in terms of the Gaussian width of A, we now
Having bound the expectation of E[supu∈A 1/ T − p t=p+1 hX
PT
√
f
seek to bound supu∈A 1/ T − p t=p+1 hXt − µt , ui around its expectation with high-probability. To do so, we appeal
to lemma E.4. In this setting, d(u, v) = ku − vk2 . Lemma E.4 motivates the following result:
P( sup |Zu − Zv | ≥ L(γ2 (A, d(u, v)) + δα) = P( sup |Zu − Zv | ≥ L(γ2 (A, d(u, v)) + ),
u,v∈A

(E.7)

u,v∈A

where the right-hand side of the inequality follows since, as Taylor et al. (2014) notes, γ2 (A, d(u, v)) ≥ α from the
definition of the γ2 functional. We now bound γ2 (A, d(u, v)) with Lemma E.5. So by Lemma E.5 and (E.5),
P( sup |Zu − Zv | ≥ L(γ2 (A, d(u, v)) + ) ≤ P( sup |Zu − Zv | ≥ E[ sup |Zu − Zv |] + )
u,v∈A

u,v∈A

u,v∈A

= P(sup |Zu | ≥ 2E[sup |Zu |] + ),
u∈A

u∈A

where the equality follows from Lemma E.3. Now substituting in the definition of Zu , and applying (E.6) and Lemma E.4,
we have:

 

 
T
X
 2
1
f
√
hXt − µt , ui ≥ 2Lκω(A) +  ≤ L exp −
P sup
.
Lκα
T − p t=p+1
u∈A
Dividing through by
desired quantity

√

T − p, multiplying by 2, and letting  = −2Lκαω(A), we achieve the following bound on the



T
X
2
(ω(A))2
f
hXt − µt , ui ≥ 0 ≤ L exp − 4
.
α2
u∈A T − p t=p+1


P sup

(E.8)

F. Restricted Eigenvalue Condition for Martingale Difference Sequences
In this section, we prove that under mild conditions, the restricted eigenvalue condition will hold for martingale difference
sequences (MDS), which we define in Definition G.1 in Appendix G. We first present the following definitions:
Definition F.1. An isotropic design matrix is one for which the covariance matrix of each row Σ = E[Xi XiT ] = I.
Definition F.2. An anisotropic design matrix has rows with a general covariance matrix Σ = E[Xi XiT ], but with corresponding isotropic rows X i = Xi Σ−1/2 .
Definition F.3. For a finite set A ⊂ T , denote the cardinality of A by |A|. An admissible sequence of T is a collection of
s
subsets of T, {Ts : s ≥ 0}, such that for every s ≥ 1, |Ts | = 22 and |T0 | = 1.
Definition F.4. (Talagrand, 2006) For a metric space (T, d) and k = 1, 2, define
γk (T, d) = inf sup

∞
X

2s/k d(t, Ts ),

t∈T s=0

where d(t, Ts ) is the distance between the set Ts and t, and the infimum is taken with respect to all admissible sequences of
T . In cases where the metric is clear from the context, we will denote the γk functional by γk (T ).
Definition F.5. (Gordon, 1988; Chandrasekaran et al., 2012) The Gaussian width of a set A ∈ Rp is
ω(A) = sup E[hg, ui],
u∈A

where we take the expectation over random vector g ∼ N (0, Ip×p ). The Gaussian width is a measure of the size of set A.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

We now present the restricted eigenvalue condition for MDS.
Theorem F.6. Let X = (X1 , · · · , Xn )> be a n × d design matrix whose anisotropic sub-Gaussian rows form a vector
valued martingale difference sequence. Let E[X> X/n] = Σ and kΣ−1/2 Xi kψ2 ≤ κ. Then for absolute constants
c0 , c1 , c2 > 0, with probability at least 1 − 2 exp(−c20 c21 c2 ω 2 (B)), we have


2c0 c1 κ2 ω(B)
1
√
λmin (Σ|A) 1 −
≤ inf kXuk22 ,
u∈A n
n
where λmin (Σ|A) = inf u∈A u> Σu is the restricted minimum eigenvalues of Σ restricted to A ⊆ S d−1 (the unit sphere in
e = Σ1/2 u/kΣ1/2 uk2 , u ∈ A} is the normalized set of A.
Rd space), and B = {e
u:u
In the proof of this theorem, we will use the following lemma, which is a MDS version of the sub-Gaussian concentration in
Mendelson et al. (2007).
Lemma F.7. (Mendelson et al., 2007) Let (Ω, µ) be a probability space, and F ⊂ SL2 be a set of functions, where
SL2 := {f : kf kL2 = 1} is the unit sphere in L2 (µ) space. Assume that diam(F, k · kψ2 ) = α. Then, for any θ > 0 and
n ≥ 1 satisfying
√
c1 αγ2 (F, k · kψ2 ) ≤ θ n,
we have with probability at least 1 − exp(−c2 nθ2 /α4 ) that
k
1 X



sup 
f 2 (Xi ) − E[f 2 ] ≤ θ,
f ∈F n i=1

where c1 , c2 are absolute constants.
The detailed proof of Lemma F.7 can be found in Mendelson et al. (2007). We outline the proof of this lemma in Appendix
F.1.
Proof of Theorem F.6. We will apply Lemma F.7 to this proof. First, we define the following class of functions:


1
h·, ui .
F = fu : u ∈ A, fu (·) = √
u>Σu
We need to verify that F ⊂ SL2 . In fact, for any fu ∈ F , we have
kfu (X)kL2 =

1
1
EX [hX, ui2 ] = >
EX [u> X> Xu] = 1.
u> Σu
u Σu

Note that
diam(F, k · kψ2 ) =

sup kfu − fv kψ2 ≤ 2 sup kfu kψ2 .

fu ,fv ∈F

fu ∈F

In order to bound the diameter of F according to k · kψ2 , we only need to get a bound on the following term




1/2




1
 = sup  Σ−1/2 X, Σ u
 .
√
hX,
ui
sup kfu kψ2 = sup 



1/2
kΣ uk2 ψ2
u∈A
u∈A
fu ∈F
u> Σu
ψ2
Thus, we have supfu ∈F kfu kψ2 ≤ kΣ−1/2 X |ψ2 ≤ κ. By similar argument, we have






 Σ1/2 u
Σ1/2 v
Σ1/2 v 
Σ1/2 u
−1/2

 .


kfu − fv kψ2 =  Σ
−
≤ κ 1/2
−
X,
kΣ1/2 uk2
kΣ1/2 vk2 ψ2
kΣ uk2
kΣ1/2 vk2 2
By definition, we also have
kfu − fv kL2


= E Σ−1/2 X,

Σ1/2 v
Σ1/2 u
−
kΣ1/2 uk2
kΣ1/2 vk2

2 



 Σ1/2 u
Σ1/2 v 

 .
=  1/2
−
kΣ uk2
kΣ1/2 vk2 2

(F.1)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

This equality immediately implies kfu − fv kψ2 ≤ κkfu − fv kL2 . Then the γ2 -functional in Lemma F.7 can be bounded as
γ2 (F ∩ SL2 , k · kψ2 ) ≤ κγ2 (F ∩ SL2 , k · kL2 ) ≤ κc0 ω(B),
e =
where the last inequality is due to the majorizing measure theorem in Talagrand (2006), B := {e
u : u
Σ1/2 u/kΣ1/2 uk2 , u ∈ A} is the normalized set of A and c0 > 0 is an absolute constant. Now we choose the parameter θ
in Theorem F.7 as
θ=

2c0 c1 κ2 ω(B)
2c1 κγ2 (F, k · kψ2 )
√
√
≥
.
n
n

Therefore, we have with probability at least 1 − exp(−c20 c21 c2 ω(B)2 /4) that


n
 2c0 c1 κ2 ω(B)
1 1 X
2
≤

√
sup 
hX
,
ui
−
1
,
i

>
n
u∈A n u Σu i=1
e = Σ1/2 u/kΣ1/2 uk2 , u ∈ A}. It follows that
where c0 , c1 , c2 are absolute constants and B := {e
u:u


2c0 c1 κ2 ω(B)
1 1
2
√
kXuk
≤
,
sup 1 −
2
n u> Σu
n
u∈A
and
1−

2c0 c1 κ2 ω(B)
1 1
1
1
√
≤ inf
kXuk22 ≤
inf kXuk22 .
>
u∈A n u Σu
λmin (Σ|A) u∈A n
n

Thus we obtain that


1
2c0 c1 κ2 ω(B)
√
≤ inf kXuk22
λmin (Σ|A) 1 −
u∈A n
n
holds with probability at least 1 − exp(−c20 c21 c2 ω(B)2 /4), with c0 , c1 , c2 being absolute constants.
F.1. Sketch of Proof of Lemma F.7
Here we lay the outline of the proof for Lemma F.7, and show that it can be extended to bounded martingale difference
sequence. The only difference in the proof for our MDS version of this lemma from the independent case is the Bernstein
inequality. Whereas the original result leveraged the canonical Bernstein inequality, we here use the following MDS version
of the Bernstein inequality:
Lemma F.8 (Bernstein-Type Inequality for Martingale Difference Sequences). Let X1 , . . . , Xn form a sub-exponential
Martingale Difference Sequence (MDS) such that max1≤i≤n kXi kψ1 ≤ κ. Here k · kψ1 is the sub-Exponential norm defined
in Definition G.2 in Appendix G. Then

 X




 n

t2
t


P 
ai Xi  ≥ t ≤ 2 exp − C min
,
,
κ2 kak2 κkak∞
i

where C is a constant.
We defer the proof of Lemma F.8 to Appendix F.2. With the Bernstein inequality for MDS, we can now outline the proof of
Lemma F.7.
Let X1 , . . . , Xn be a bounded martingale difference sequence. We first define the empircal processes
Zf = 1/n

n
X
i=1

2

2

f (Xi ) − E[f ]


Wf =

1/n

n
X

2

2

f (Xi )

i=1

The following lemma from Mendelson et al. (2007) can be easily obtained from Lemma F.8.

.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Lemma F.9. There exists an absolute constant c1 > 0 for which the following holds. Let F ⊂ SL2 , α = diam(F, k · kψ2 )
and set n ≥ 1. For every f, g ∈ F and every u ≥ 2 we have
P(Wf −g ≥ ukf − gkψ2 ) ≤ exp(−c1 nu2 ).
Also, for every u > 0,
P(|Zf − Zg | ≥ uαkf − gkψ2 ) ≤ exp(−c1 nu2 ),
and
P(|Zf | ≥ uα2 ) ≤ 2 exp(−c1 nu2 ).
The following two lemmas from Mendelson et al. (2007) hold in our setting since they do not require i.i.d. observations.
Lemma F.10. There exists an absolute constant C for which the following holds. Let F ⊂ SL2 , α = diam(F, k · kψ2 ) and
n ≥ 1. There is F 0 ⊂ F such that |F 0 | ≤ 4n and with probability at least 1 − exp(−n), we have, for every f ∈ F ,
Wf −πF 0 (f ) ≤

Cγ2 (F, k · kψ2 )
√
,
n

where πF 0 (f ) is a nearest point to f in F 0 with respect to the ψ2 metric.
Lemma F.11. There exist absolute constants C and c0 > 0 for which the following holds. Let F ⊂ SL2 and α =
diam(F, k · kψ2 ). Let n ≥ 1 and F 0 ⊂ F such that |F 0 | ≤ 4n . Then for every w > 0,
sup |Zf | ≤ Cα

f ∈F 0

γ2 (F, k · kψ2 )
√
+ α2 w,
n

with probability at least 1 − 3 exp(−c0 n min(w, w2 )).
Based on the above lemmas, the rest of proof of Theorem F.7 is stated as the proof of Theorem 1.4 in Mendelson et al.
(2007).
F.2. Proof of Berstein Inequality for MDS (Lemma F.8)
We first present the following lemma from Vershynin (2012).
Lemma F.12 (Lemma 5.15 from Vershynin (2012)). If X is a sub-exponential random variable such that E[X] = 0, then
for every t such that |t| ≤ c/kXkψ1 , we have
E[exp(tX)] ≤ exp(Ct2 kXkψ1 ],
where C, c > 0 are constants.
We now prove the Berstein Inequality for MDS.
Proof of Lemma F.8. We begin by bounding the moment-generating function of

 X

Y

n
n
E exp t ·
ai Xi
=E
exp[tai Xi ]
i

Pn
i

ai Xi as follows:

i


 

n−1
Y


= E E exp[tan Xn ]
exp[tai Xi ]X1 , . . . , Xn−1
i



 
  n−1

Y


= E E exp[tan Xn ]X1 , . . . , Xn−1 · E
exp[tai Xi ]X1 , . . . , Xn−1
i



 n−1

Y

2 2 2
≤ E exp(Ct an κ ) · E
exp[tai Xi ]X1 , . . . , Xn−1 ,
i

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where the second inequality holds by the Law of Iterated Expectations. Furthermore, the last inequality holds by Lemma
F.12 since E[Xn |X1 , . . . , Xn−1 ] = E[Xn |Fn−1 ] = 0 by the definition of a MDS (recall Definition G.1 in Appendix G). By
iteratively repeating this process, we obtain

 X

Y

n
n
2 2 2
E exp t ·
ai Xi
≤E
exp(Ct ai κ )
i

i



n
X
2
2 2
= exp Ct
ai κ
i



2
2 2
= exp Ct kak2 κ .

(F.2)

Now note that by the Chernoff bound, for all λ such that |λ| ≤ C/kak∞ we have
X


 X


n
n
P
ai Xi ≥ t = P exp λ
ai Xi ≥ exp(λt)
i

i

Pn

E[exp[λ i ai Xi ]]
exp[λt]
exp[Cλ2 kak22 κ2 ]
≤
exp[λt]
≤

= exp[Cλ2 kak22 κ2 − λt],
where the last inequality holds by F.2. Now if we let λ = min{t/(2Ckak22 κ2 ), c/(kak∞ κ)}, then we see that if
t/(2Ckak22 κ2 ) < c/(kak∞ κ), then


X



n
− t2
t2
t2
=
exp
P
ai Xi ≥ t ≤ exp
−
.
(F.3)
4Ckak22 κ2 2κ2 Ckak22
4Ckak22 κ2
i
Similarly, if c/(kak∞ κ) < t/(2Ckak22 κ2 ), then





X
n
ct
ct
Cckak22 κ
c
−
≤ exp −
,
ai Xi ≥ t ≤ exp
·
P
kak∞
kak∞ κ kak∞ κ
2kak∞ κ
i

(F.4)

where the second inequality holds since Cckak22 κ/kak∞ ≤ t/2. Combining F.3 and F.4 yields



X

n
− t2
− ct
ai Xi ≥ t ≤ exp min
P
,
.
4Ckak22 κ2 2kak∞ κ
i
Note that we can repeat this process and replace each Xi with −Xi to obtain this same bound for P(−
lemma then follows as a result of these two bounds.

Pn
i

ai Xi ≥ t). This

G. Auxiliary Definitions
In this section we present definitions used in the Appendix sections.
Definition G.1. A stochastic process {ξt } is a martingale difference sequence with respect to filtration Ft if:
1. ξt is Ft -measurable, and
2. E[ξt |Ft−1 ] = 0.
Definition G.2. The sub-Exponential norm of a random scalar variable X , kXkψ1 , is:
kXkψ1 = sup q
q≥1

−1

 
1/q
q
E |X|
.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

The sub-Exponential norm of a random vector X ∈ Rn is:
kXkψ1 = sup khX, uikψ1 ,
u∈S n−1

where S n−1 is the unit sphere in Rn space.

