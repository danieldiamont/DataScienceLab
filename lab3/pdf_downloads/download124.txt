Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

A. Proofs of Main Results
In this section we present proofs of the results from Section 5.
A.1. Proof of Theorem 5.5 and Corollaries
We first demonstrate how we decompose the estimation error in the unbiased Lasso Granger estimator Î¸bu into the sum of
two components:
1
b âˆ’ Î¸âˆ—
e > (XÎ¸
e âˆ—+âˆ’X
e Î¸)
MX
T âˆ’p
1
b âˆ’ (Î¸ âˆ— âˆ’ Î¸)
b
e >  + 1 MX
e > X(Î¸
e âˆ— âˆ’ Î¸)
=
MX
T âˆ’p
T âˆ’p
1
b
e >  + (MÎ£
e n âˆ’ I)(Î¸ âˆ— âˆ’ Î¸).
MX
=
T âˆ’p

Î¸bu âˆ’ Î¸ âˆ— = Î¸b +

b we have
e n âˆ’ I)(Î¸ âˆ— âˆ’ Î¸),
e > /âˆšT âˆ’ p and âˆ† = âˆšT âˆ’ p(MÎ£
Letting Z = MX
p

T âˆ’ p(Î¸bu âˆ’ Î¸ âˆ— ) = Z + âˆ†.

(A.1)

Note that, clearly, E[Z] = 0. Thus, âˆ† encapsulates the bias in Î¸bu . We divide this proof into two parts. We first establish in
Lemma A.1 that Î¸bu is an asymptotically unbiased estimator of Î¸ âˆ— by proving that kâˆ†kâˆž = o(1). We then proceed to prove
in Lemma A.2 that Z is asymptotically normally distributed.
b  (âˆšT âˆ’ p)/ log(pd) and Âµ 
Lemma
A.1.
Suppose
Assumptions
5.3
and
5.4
are
satisfied.
Let
s
=
supp(
Î¸)
0
p
âˆš
b
e n âˆ’ I)(Î¸ âˆ— âˆ’ Î¸).
log(pd)/(T âˆ’ p). Then kâˆ†kâˆž = o(1), where âˆ† = T âˆ’ p(MÎ£
The proof of Lemma A.1, presented in Appendix B.1, uses HoÌ‹lderâ€™s inequality to decompose kâˆ†k1 into the product
âˆš
b 1 . We bound kMÎ£
e n âˆ’ Ikâˆž Â· kÎ¸ âˆ— âˆ’ Î¸k
e n âˆ’ Ikâˆž by constructing a martingale difference sequence (see
T âˆ’ pkMÎ£
Definition G.1 in Appendix G) and applying a Bernstein-type inequality (Lemma F.8) to this sequence. We then bound
b 1 via a standard argument for Lasso-type estimators that relies on the restricted eigenvalue condition. We amend
kÎ¸ âˆ— âˆ’ Î¸k
this argument to work in our non-i.i.d. setting by appealing to martingale theory and present a restricted eigenvalue condition
for martingale difference sequences in Appendix F.
b  (âˆšT âˆ’ p)/ log(pd) and Âµ 
Lemma A.2. Suppose Assumptions 5.3 and 5.4 are satisfied. Let s0 = supp(Î¸)
p
D
e n M> ]1/2 ) = MX
e > /(Ïƒ âˆšT âˆ’ p[MÎ£
e n M> ]1/2 ) âˆ’
log(pd)/(T âˆ’ p). Then we have Z/(Ïƒ[MÎ£
â†’ N (0, IpdÃ—pd ).
The proof of Lemma A.2, deferred to Appendix B.2, relies on constructing a martingale difference sequence equal to
e n M> ]1/2 ), and applying the Martingale Central Limit Theorem (Hall & Heyde, 1980).
Z/(Ïƒ[MÎ£
Having established Lemmas A.1 and A.2, we are now ready to present a proof of Theorem 5.5.

Proof of Theorem 5.5. We write the estimation error of the unbiased Lasso Granger estimator as
p

T âˆ’ p(Î¸bu âˆ’ Î¸ âˆ— ) = Z + âˆ†.

P
b Then, by Lemma A.1 we have that âˆ† âˆ’
e > /âˆšT âˆ’ p and âˆ† = âˆšT âˆ’ p(MÎ£
e n âˆ’ I)(Î¸ âˆ— âˆ’ Î¸).
where Z = MX
â†’ 0. By
D
> 1/2
e
Lemma A.2, we have that Z/(Ïƒ[MÎ£n M ] ) âˆ’â†’ N (0, I). Therefore, by the Slutsky Theorem (Van der Vaart, 2000), we
âˆš
D
e n M> ]1/2 ) âˆ’
have that T âˆ’ p(Î¸bu âˆ’ Î¸ âˆ— )/(Ïƒ[MÎ£
â†’ N (0, IpdÃ—pd ), as desired.

Theorem 5.5 allows us to demonstrate the asymptotic validity of the confidence intervals we present in Corollary 5.6 as
follows:

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Proof of Corollary 5.6. By Theorem 5.5, the asymptotic normality of Î¸biu implies






âˆ—
u
âˆ—
u
âˆ—
b
b
lim P Î¸i âˆˆ Ii = lim P Î¸i âˆ’ Î¸i â‰¤ Î´(Î±, T âˆ’ p) âˆ’ lim P Î¸i âˆ’ Î¸i â‰¤ âˆ’Î´(Î±, T âˆ’ p)
T âˆ’pâ†’âˆž

T âˆ’pâ†’âˆž

T âˆ’pâ†’âˆž

âˆš

T âˆ’ p(Î¸biu âˆ’ Î¸iâˆ— )
âˆ’1
= lim P
â‰¤ Î¦ (1 âˆ’ Î±/2)
T âˆ’pâ†’âˆž
e n M> ]1/2
Ïƒ[MÎ£
i,i
âˆš

T âˆ’ p(Î¸biu âˆ’ Î¸iâˆ— )
âˆ’1
â‰¤
âˆ’Î¦
âˆ’ lim P
(1
âˆ’
Î±/2)
T âˆ’pâ†’âˆž
e n M> ]1/2
Ïƒ[MÎ£
i,i
= 1 âˆ’ Î±.

(A.2)

In a similar manner, Theorem 5.5 also permits us to prove the several desirable properties of hypothesis test Î¨Z (Î±), which
we introduce in (4.7), that we present in Corollary 5.7.
Proof of Corollary 5.7. By (4.3), we have
ci | < zÎ±/2 ) = Î±
P(Î¨Z (Î±) = 1|H0i ) = P(âˆ’|Z
ci converges in distribution to the standard normal distribution. Similarly, for any u âˆˆ (0, 1),
where zÎ±/2 = Î¦âˆ’1 (Î±/2), since Z
we see that


u
c
c
P(Pi < u) = P(2(1 âˆ’ Î¦(|Zi |)) < u) = P Î¦(|Zi |) > 1 âˆ’
2



(T âˆ’p)â†’âˆž
ci | > Î¦âˆ’1 1 âˆ’ u
âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ u
= P |Z
2
ci converges in distribution to the standard normal distribution.
since, again, Z

In Section 4.1, we claim that the Scaled Lasso noise estimator (Sun & Zhang, 2012) Ïƒ
b, as given by (4.4), is a consistent
estimator of the true noise level Ïƒ. We note that while Sun & Zhang (2012) prove the consistency in the i.i.d. case, Ïƒ
b is
nevertheless still consistent in our non-i.i.d. case as well. This result follows directly from Theorem 1 in Sun & Zhang
(2012), which we paraphrase in the following lemma.
p
b
Lemma A.3. Let (Î¸(Î»),
Ïƒ
b(Î»)) be the Scaled Lasso estimator from (4.4) and Î» = 8CÏƒ log(pd)/(T âˆ’ p). Furthermore,
let the assumptions of Theorem 5.5 hold. Then,




Ïƒ
b(Î»)


P 
âˆ’ 1 >  â†’ 0,
Ïƒ
for all  > 0 as (T âˆ’ p, pd) â†’ âˆž.
We present a proof of this lemma in Appendix B.3.
A.2. Proof of Theorem 5.9
We first present a property and three lemmas that will allow us to prove Theorem 5.9. For ease of presentation, let
G(t) = 2(1 âˆ’ Î¦(t)) and Gâˆ’1 (t) = Î¦âˆ’1 (1 âˆ’ t/2).
e = [Ïƒi,j ] âˆˆ RpdÃ—pd is the true covariance matrix of our design matrix X.
e Now let Ïi,j =
Property 1. Recall that Î£
âˆš
âˆ’2âˆ’
Ïƒi,j / Ïƒi,i Ïƒj,j , and for Î´,  > 0, let B(Î´) = {(i, j)||Ïi,j | â‰¥ Î´, i 6= j} and A() = B([log(pd)]
). By a similar argument
b  âˆšT âˆ’ p/ log(pd) in Theorem 5.5, we have
to that made by Liu et al. (2013b), since we assume that supp(Î¸)
X
(pd)a1 = O((pd)2 /(log(pd)2 ),
(i,j)âˆˆA()

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where a1 = 2|Ïi,j |/(1 + |Ïi,j |) + Î´.
Property 1 amounts to a sparsity assumption on the true covariance matrix and allows us to cope with the correlation among
test statistics.
Lemma A.4. Suppose Assumptions 5.3, 5.4, the conditions of Theorem 5.9, and Property 1 are satisfied. Then we can
bound Î½b from (4.9) as follows:
P(0 â‰¤ Î½b â‰¤ xpd ) â†’ 1
pd

where xpd = Gâˆ’1 (ypd /(pd)), and ypd is a sequence such that ypd âˆ’â†’ âˆž and ypd = o(pd).
Lemma A.4 bounds Î½b with high probability. We use this bound in the following lemma:
Lemma A.5. Suppose Assumptions 5.3, 5.4, and Property 1 are satisfied. Then for xpd as defined in Lemma A.4, we have:
P

ci | â‰¥ Î½)
 P

1(|Z
â†’ 0.
sup  iâˆˆH0
âˆ’ 1 âˆ’
|H0 |G(Î½)
0â‰¤Î½â‰¤xpd
Lemma A.6. Suppose Assumptions 5.3 and 5.4 are satisfied. Then we have:
max{

P

(pd)G(b
Î½)
= Î±.
ci â‰¥ Î½b), 1}
1(|Z

1â‰¤jâ‰¤pd

In the interest of clarity, we defer the proofs of Lemmas A.4, A.5, and A.6 to Appendices B.4, B.5, and B.6, respectively.
The proof of Theorem 5.9 proceeds in three parts. We first bound Î½b with high probability in Lemma A.4, and then prove in
Lemma A.5 that for any Î½ within those bounds
P
c
b) P
iâˆˆH0 1(|Zi | â‰¥ Î½
âˆ’
â†’ 1.
|H0 |G(b
Î½)
The result of Theorem 5.9 then follows naturally by Lemma B.6 .
Proof of Theorem 5.9. By Lemma A.4, P(0 â‰¤ Î½b â‰¤ xpd ) â†’ 1. Then by Lemma A.5, we have
P

P

ci | â‰¥ Î½b)
ci | â‰¥ Î½)
 iâˆˆH0 1(|Z

 iâˆˆH0 1(|Z
 P



âˆ’ 1 â‰¤ sup 
âˆ’ 1 âˆ’
â†’ 0.

|H0 |G(b
Î½)
|H0 |G(Î½)
0â‰¤Î½â‰¤xpd
Thus, we have
P

ci | â‰¥ Î½b) P
1(|Z
âˆ’
â†’ 1.
|H0 |G(b
Î½)

iâˆˆH0

From this result, we see that by the definition of FDP(Î½),
P
ci | â‰¥ Î½b)
(pd) iâˆˆH0 1(|Z
(pd)|H0 |G(b
Î½)
FDP(b
Î½)
P
=
âˆ’
â†’
.
P
P
c
ci â‰¥ Î½b), 1}
Î±|H0 |/(pd)
Î±|H0 | max{ 1â‰¤jâ‰¤pd 1(|Zi â‰¥ Î½b), 1}
Î±|H0 | max{ 1â‰¤jâ‰¤pd 1(|Z

(A.3)

By Lemma A.6,
max{

P

(pd)G(b
Î½)
= Î±.
ci â‰¥ Î½b), 1}
1(|Z

1â‰¤jâ‰¤pd

Thus, by (A.3) and (A.4)
FDP(b
Î½) P
âˆ’
â†’ 1,
Î±|H0 |/(pd)
as (T âˆ’ p, pd) â†’
âˆ’ âˆž. This result clearly then implies that
FDR(b
Î½) P
âˆ’
â†’ 1,
Î±|H0 |/(pd)
as (T âˆ’ p, pd) â†’
âˆ’ âˆž, as desired.

(A.4)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B. Proofs of Technical Lemmas in Appendix A
In this section we present the proofs of technical lemmas introduced in Appendix A.
B.1. Proof of Lemma A.1
We first present two auxiliary lemmas that we will use in the proof of Lemma A.1.
eÎ£
e âˆ’1/2 are sub-Gaussian with
Lemma B.1. If Assumption 5.3 holds and we additionally assume that the rows of X
âˆ’1/2 f
e
sub-Gaussian norm of Îº = kÎ£
Xi kÏˆ2 , then
s
e n âˆ’ Ikâˆž â‰¤ a
kMÎ£

holds with probability at least 1 âˆ’ 2(pd)âˆ’c2 , where c2 =

log(pd)
,
T âˆ’p

a2 Cmin
âˆ’ 2 and a is some constant.
24e2 Îº4 Cmax

The proof of Lemma B.1, which we defer to Appendix C.1, relies constructing a martingale difference sequence (see
Definition G.1 in Appendix G) and applying a Bernstein-type inequality (Lemma F.8) to this sequence.
p
Lemma B.2. Let Î» = 8CÏƒ log(pd)/(T âˆ’ p) for some constant C. Then
b 1 â‰¤ 12Î»s0 ,
kÎ¸ âˆ— âˆ’ Î¸k
Îº`
with probability at least
2

1 âˆ’ b1 exp[âˆ’b2 Ïƒ log(pd)] âˆ’

2 exp(âˆ’c20 c21 c2 Ï‰ 2 (B))


(Ï‰(A))2
âˆ’ L exp âˆ’ 4
,
Î±2


1
e 2 is the restricted minimum eigenvalue of Î£
e n restricted to A âŠ† S pdâˆ’1 (the
kXuk
2
T âˆ’p
e 1/2 u/kÎ£
e 1/2 uk2 , u âˆˆ A} is the normalized set of A, Î± = diam(A) =
e = Î£
unit sphere in Rpd space), B = {e
u : u
supu,vâˆˆA d(u, v) = supu,vâˆˆA ku âˆ’ vk2 , Ï‰(A) is the Gaussian width of set A as defined in Definition F.5, and
Îº` , b1 , b2 , c0 , c1 , c2 , L > 0 are constants.

e n |A) = inf uâˆˆA
where Î»min (Î£

b 1
The proof of Lemma B.2, which we present in Appendix C.2, relies on the restricted eigenvalue condition to bound kÎ¸ âˆ— âˆ’ Î¸k
with high probability.
We now present a proof of Lemma A.1.
Proof of Lemma A.1. HoÌ‹lderâ€™s inequality allows us to decompose kâˆ†k1 as follows:
kâˆ†kâˆž â‰¤ kâˆ†k1 â‰¤

p

b 1.
e n âˆ’ Ikâˆž Â· kÎ¸ âˆ— âˆ’ Î¸k
T âˆ’ pkMÎ£

p
b 1 separately. By Lemma B.1, we find that kMÎ£
e n âˆ’ Ikâˆž and kÎ¸ âˆ— âˆ’ Î¸k
e n âˆ’ Ikâˆž â‰¤ a log(pd)/T âˆ’ p
We now bound kMÎ£
b 1 â‰¤ 12Î»s0 /Îº` with high probability
with high probability . Additionally, by Lemma B.2, kÎ¸ âˆ— âˆ’ Î¸k
Combining these two high-probability bounds yields the following result:
p
log(pd)
b 1 < as0 96Ïƒ Â· âˆš
e n âˆ’ Ikâˆž Â· kÎ¸ âˆ— âˆ’ Î¸k
T âˆ’ pkMÎ£
,
Îº`
T âˆ’p

(B.1)

âˆš
âˆš
with high probability. Thus, by (B.1), kâˆ†kâˆž = o(s0 log(pd)/ T âˆ’ p). Recall that by assumption, s0  T âˆ’ p/ log(pd).
Therefore, kâˆ†kâˆž = o(1).

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B.2. Proof of Lemma A.2
We first present an auxiliary lemma that we will use in the proof of Lemma A.2.
Lemma B.3 (Lindeberg condition from Hall & Heyde (1980)). Denote the martingale difference sequence Î¶i,t =
2
f> mi )/(Ïƒ[m> Î£
e n mi ]1/2 ) and filtration Ft = Ïƒ(X
f1 , . . . , X
ft , 1 , . . . , t ). Then, PT
(t X
t
i
t=p+1 E[Î¶i,t 1(kÎ¶t | â‰¥ Î´)|Ftâˆ’1 ] â†’
0.
In the interest of clarity, we defer the proof of Lemma B.3 to Appendix C.3.
Proof of Lemma A.2. To prove this lemma, we need to show that
ci
Z
e n M> ]1/2
Ïƒ[MÎ£
i,i

=

e>
m>
D
i X 
âˆ’â†’ N (0, 1).
>
1/2
e
Ïƒ[m Î£n mi ]
i

Note that
e >  = (m> X
e > )> = > Xm
e i=
ci = m> X
Z
i
i

T
X

f> mi .
t X
t

t=p+1

f1 , . . . , X
ft , 1 , . . . , t ). By (3.1), t is independent of Ftâˆ’1 and conditionally independent
Now define filtration Ft = Ïƒ(X
ft given Ftâˆ’1 . Furthermore, by (4.2), t is conditionally independent mi given Ftâˆ’1 . Therefore,
of X
f> mi |Ftâˆ’1 ] = E[t |Ftâˆ’1 ] Â· E[X
f> mi |Ftâˆ’1 ]
E[t X
t
t
f> mi |Ftâˆ’1 ]
= E[t ] Â· E[X
t
= 0,
where the last equality follows since t âˆ¼N (0, Ïƒ 2 ). Thus,
{Î¶i,t }Tt=p+1 =



T
f> mi
t X
t
,
e n mi ]1/2 t=p+1
Ïƒ[m> Î£
i

is a martingale difference sequence by Definition G.1 in Appendix G.
e n mi ]1/2 ) = PT
ci /(Ïƒ[m> Î£
Since Z
i
t=p+1 Î¶i,t , if we can show that we can apply the Martingale Central Limit Theorem
(MCLT) (Hall & Heyde, 1980) to Î¶i,t , the result of this lemma will follow. To demonstrate that we can apply the MCLT to
Î¶i,t , we must prove that the Lindeberg condition holds for this sequence. By Lemma B.3, the Lindeberg condition holds for
Î¶i,t . Therefore, by the MCLT
T
X
t=p+1

Î¶i,t =

ci
Z
D
âˆ’â†’ N (0, 1),
>
1/2
e
Ïƒ[m Î£n mi ]
i

as desired.

B.3. Proof of Lemma A.3
We present a variation on the argument made in the Proof of Theorem 1 in Sun & Zhang (2012). The proof of Lemma A.3
requires the following two supporting lemmas from Sun & Zhang (2012), which in turn necessitate introducing some new
e 2 + Î»kÎ¸k1 . We distinguish L(Â·)
notation. Denote the penalized least-squares loss function L(Î¸) = (2(T âˆ’ p))âˆ’1 kY âˆ’ XÎ¸k
2
e 2 + Ïƒ/2 + Î»kÎ¸k1 .
from the Scaled Lasso loss function from (4.4), which we denote LÎ» (Î¸, Ïƒ) = (2(T âˆ’ p))âˆ’1 kY âˆ’ XÎ¸k
2
As Sun & Zhang (2012) note, Î¸ is a critical point of L if and only if it satisfies:
(
fÂ·,j (Y âˆ’ XÎ¸)/(T
e
X
âˆ’ p) = Î»sign(Î¸j ), Î¸j 6= 0
(B.2)
f
e
XÂ·,j (Y âˆ’ XÎ¸)/(T âˆ’ p) âˆˆ [âˆ’Î», Î»], Î¸j 6= 0

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

fÂ·,j is the j-th column of the design matrix X.
e Importantly, Sun & Zhang (2012) note that B.2 is the Karush-Kuhnwhere X
Tucker (KKT) condition for the minimization of L(Â·) when L(Â·) is convex in Î¸. This property will prove important in the
discussion of Lemma B.3 below. We now present the first of two supporting lemmas for the proof of Lemma A.3.
b
b
Lemma
p B.4 (Proposition 1 from Sun & Zhang (2012)). Let Î¸ = Î¸(Î») be a solution path of B.2 and Î»0 = Î»/Ïƒ =
8C log(pd)/(T âˆ’ p). Then the loss function LÎ» (Â·, Â·) is jointly convex in (Î¸, Ïƒ). Furthermore, the derivative of LÎ» (Â·, Â·)
with respect to Ïƒ is
2
e
âˆ‚
b 0 ), t) = 1 âˆ’ kY âˆ’ XÎ¸(tÎ»0 )k2 .
LÎ»0 (Î¸(tÎ»
2
âˆ‚t
2
2(T âˆ’ p)t

(B.3)

e and so we refer readers to the proof of Proposition 1 in Sun &
Lemma B.4 does not rely on the i.i.d.-ness of the rows of X,
Zhang (2012) for a proof of Lemma B.4.
The second supporting lemma requires additional notation from Sun & Zhang (2012). Let Î·(Î», Î¾, w, Q) be a prediction
error bound for the estimation of Î¸ âˆ— via the Scaled Lasso. Let w âˆˆ Rpd Q âŠ‚ 1 . . . , pd, and
e âˆ— âˆ’ w)k2 /(T âˆ’ p) + 2Î»kwQc k1 (2 âˆ’ 1(w = Î¸, Q = âˆ…)) +
Î·(Î», Î¾, w, Q) = kX(Î¸
2

4Î¾ 2 Î»2 |Q|
,
(Î¾ + 1)2 Îº(Î¾, Q)

(B.4)

where vS = [vi ]iâˆˆS ) âˆˆ R|S| and

e 2
|Q|1/2 kXuk
âˆš
: u âˆˆ E(Î¾, Q), u 6= 0 ,
(B.5)
kuQ k1 T âˆ’ p
e âˆ— k2 /âˆšT âˆ’ p, which Sun & Zhang (2012)
where E(Î¾, Q) = {u : kuQc k1 â‰¤ Î¾kuQ k1 }. Now let Ïƒ âˆ— = kY âˆ’ XÎ¸
call the oracle estimator for Ïƒ. Based on these definitions, let the minimum prediction error bound be Î·âˆ— (Î», Î¾) =
1/2 âˆ—
âˆ—
inf w,Q
Î»0 = Î»/Ïƒ =
p Î·(Î», Î¾, w, Q) and define the following related quantity Ï„0 = Î·âˆ— (Ïƒ Î»0 , Î¾)/Ïƒ , where recall that
âˆ—
8C log(pd)/(T âˆ’ p). As Sun & Zhang (2012) note, since in (3.2)  in a Gaussian random vector, Ïƒ is the maximum
likelihood estimator for Ïƒ when Î¸ is known. Thus, in the proof of Lemma A.3, we bound the quantity Ïƒ
b/Ïƒ âˆ— âˆ’ 1 by Ï„0 in
order to prove the consistency of Ïƒ
b. However, we first require the following intermediate result.
âˆ—
b
Lemma
Let Î¸(Î»)
 B.5 (Theoremâˆ— 4 from Sun & âˆ—Zhang (2012)).
	 minimize L(Â·), Î¾ > 0, and define Î· (Î», Î¾) =
2
2 1/2
c
minQ (1/2)(Î·(Î», Î¾, Î¸ , Q) + (Î·(Î», Î¾, Î¸ , Q) âˆ’ 16Î» kÎ¸Q k1 ) ) (not to be confused with Î·âˆ— (Î», Î¾) defined above).
e top (Y âˆ’ XÎ¸
e âˆ— )kâˆž /(T âˆ’ p) â‰¤ Î»(Î¾ âˆ’ 1)/(Î¾ + 1), then
If kX

	
e Î¸b âˆ’ Î¸ âˆ— )k2 /(T âˆ’ p) â‰¤ min Î·âˆ— (Î», Î¾), Î· âˆ— (Î», Î¾) .
kX(
2


Îº(Î¾, Q) = min

The proof of Lemma B.3 follows directly from the proof of Theorem 4 from Sun & Zhang (2012). The only point of
contention in that proof is that B.2 must be the KKT condition for the minimization of L(Â·), which as we state above requires
that L(Â·) is convex in theta. In Lemma C.1 in Appendix C.2, we prove that the restricted eigenvalue condition holds with
e n . As we demonstrate in the proof of Lemma B.2 in Appendix C.2,
high probability for the sample covariance matrix Î£
e 2 is
the restricted eigenvalue condition implies that the unpenalized least-squares loss function (2(T âˆ’ p))âˆ’1 kY âˆ’ XÎ¸k
2
âˆ—
strictly convex for all Î¸ such that the error vector Î¸ âˆ’ Î¸ falls in the error cone E(3, S), where S is the support of Î¸ âˆ— . Since
E(3, S) actually encompasses all possible error vectors, a property we prove in the proof of Lemma B.2, we see that the
unpenalized loss function is convex with respect to Î¸. Since the derivative of penalty term Î»kÎ¸k1 with respect to Î¸ is a
strictly positive vector, given that the unpenalized loss function is convex in Î¸, so is the pealized loss function. Thus, B.2 is
the KKT condition for the minimization of L(Â·), and Lemma follows immediately from the proof of Theorem 4 in Sun &
Zhang (2012). We refer the reader to that paper for the full proof.
We are now ready to present the proof of Lemma A.3.
Proof of Lemma A.3. We present an amended version of the Proof of Theorem 1 from Sun & Zhang (2012). Let z âˆ— =
e > (Y âˆ’ XÎ¸
e âˆ— )/(T âˆ’ p)kâˆž /Ïƒ âˆ— . Without loss of generality, assume Ï„0 < 1 and let t â‰¥ Ïƒ âˆ— (1 âˆ’ Ï„0 ) and let Î»1 = tÎ»0 ,
kX
where Î»0 is as defined above. Now note that
z âˆ— Ïƒ âˆ— â‰¤ Ïƒ âˆ— (1 âˆ’ Ï„0 )Î»0

Î¾âˆ’1
Î¾âˆ’1
Î¾âˆ’1
â‰¤ tÎ»0
= Î»1
.
Î¾+1
Î¾+1
Î¾+1

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Then by this inequality, the definition of Ïƒ âˆ— , the Cauchy-Schwarz inequality, and Lemma B.3, we have


b 1 )k2
b 1 ) âˆ’ Î¸ âˆ— )k2
e Î¸(Î»
e Î¸(Î»
 kY âˆ’ X
 kX(
1/2
âˆ—

âˆš
âˆš
â‰¤
âˆ’
Ïƒ
â‰¤ Î·âˆ— (Î»1 , Î¾).


T âˆ’p
T âˆ’p

(B.6)

Now observe that B.3 in Lemma B.4 yields
2t2

2
eb
âˆ‚
b 0 ), t) = t2 âˆ’ kY âˆ’ XÎ¸(tÎ»0 )k2 â‰¤ t2 âˆ’ (Ïƒ âˆ— )2 (1 âˆ’ Ï„0 )2 ,
LÎ»0 (Î¸(tÎ»
âˆ‚t
(T âˆ’ p)
1/2

where the last inequality follows from B.6 and the definition of Ï„0 , which implies Î·âˆ— (Î»1 , Î¾) â‰¤ Ïƒ âˆ— Ï„0 for t < Ïƒ âˆ— , since
Î»1 = tÎ»0 . Note that when t = Ïƒ âˆ— (1 âˆ’ Ï„0 ), the expression on the right-hand side of the last inequality equals zero. Note
b 0 ), t) implies that Ïƒ
further that LÎ» (Â·, Â·) is strictly convex in Ïƒ. Then the negativity of 2t2 âˆ‚/(âˆ‚t)LÎ»0 (Î¸(tÎ»
b â‰¥ Ïƒ âˆ— (1 âˆ’ Ï„0 ).
âˆ—
âˆ—
On the other hand, at t = Ïƒ /(1 âˆ’ Ï„0 ) > Ïƒ , we have
t2 âˆ’

b 0 )k2
e Î¸(tÎ»
kY âˆ’ X
2
â‰¥ t2 âˆ’ (Ïƒ + tÏ„0 )2 â‰¥ 0,
(T âˆ’ p)

1/2

since for t > Ïƒ âˆ— , we have Î·âˆ— (Î»1 , Î¾) â‰¤ tÏ„0 . This result implies Ïƒ âˆ— â‰¥ Ïƒ
b(1 âˆ’ Ï„0 ) by the strict convexity of LÎ» (Â·, Â·) in Ïƒ.
Therefore,


Ïƒ
b
Ïƒâˆ—
max 1 âˆ’ âˆ— , 1 âˆ’
â‰¤ Ï„0 .
(B.7)
Ïƒ
Ïƒ
b
1/2

As Sun & Zhang (2012) argue, Î·âˆ— (Î»1 , Î¾)/Ïƒ â†’ 0 as (T âˆ’ p, pd) â†’ âˆž. Thus,



Ïƒ

b(Î»)


P 
âˆ’ 1 >  â†’ 0,
Ïƒ
for all  > 0 as (T âˆ’ p, pd) â†’ âˆž.
B.4. Proof of Lemma A.4
We will need the following lemma, which is a modified version on Lemma 6.1 from Liu et al. (2013b) to prove Lemma A.4.
âˆš
Lemma B.6. Let Î¾1 , . . . , Î¾n âˆˆ Rp have mean zero. Suppose that p â‰¤ nr , log(p) = o( n), and E[kÎ¾i kbpr+2+
] â‰¤ âˆž, for
2 P
n
r, b,  > 0. Furthermore, assume that k Cov(Î¾i ) âˆ’ IpÃ—p k2 â‰¤ C(log(p))âˆ’2âˆ’Î³ , where Cov(Î¾i ) = E[(1/n) i=1 Î¾i Î¾i> ] and
Î³ > 0. Define k Â· kmin as kvkmin = min1â‰¤iâ‰¤p {|vi |}. Then,


P
âˆš

 P(k ni=1 Î¾i kmin â‰¥ t n)
 â‰¤ C(log(p))âˆ’1âˆ’Î³1 ,

sup
âˆ’
1


p
âˆš
(G(t))
0â‰¤tâ‰¤b log(p)
where Î³1 = min{Î³, 1/2}.
âˆš
Note that we make the assumptions p â‰¤ nr , r > 0 and log(p) = o( n), in the statement of Theorem 5.9. The former
assumption is clearly satisfied in our setting, as we can have r > 1. As noted by Liu & Luo (2014), given that p â‰¤ nr , for
e having bounded sub-Gaussian rows is equivalent to E[kÎ¾i kbpr+2+ ] â‰¤ âˆž, for b,  > 0.
r > 0, our assumption 5.4 of X
2
Additionally, the assumption of k Cov(Î¾i ) âˆ’ IpÃ—p k2 â‰¤ C(log(p)âˆ’2âˆ’Î³ is satisfied by Property 1, the sparsity property of the
covariance matrix. Whereas Liu et al. (2013b) prove Lemma B.6 for the i.i.d. case, we present a proof that draws on the
Bernstein inequality for martingale difference sequences (Lemma F.8) to prove this lemma when Î¾1 , . . . , Î¾n âˆˆ Rp are not
independent. We defer the proof of Lemma B.6 to Appendix C.4. Having established Lemma B.6, we now present the proof
of Lemma A.4.
Proof of Lemma A.4. By Lemma B.6, we know that


ci â‰¥ Î½)
 P(Z


 â‰¤ C(log(pd))âˆ’1âˆ’Î³1 ,
max
sup
âˆ’
1


âˆš
1â‰¤iâ‰¤pd
G(Î½)
0â‰¤Î½â‰¤4 log(pd)

(B.8)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

p
ci | â‰¥
for positive constants C and Î³1 . Then by (B.8), we have that P(|Z
2 log(pd)) â†’ 1 for all i âˆˆ B =
p
e âˆ’1/2 ) â‰¥ c log(pd)/(T âˆ’ p)}. Thus,
{i||Î¸iâˆ— |/(Ïƒ Î£
i,i
p
P
c
2 log(pd)) P
iâˆˆB P(|Zi | â‰¥
âˆ’
â†’ 1,
(B.9)
|B|
p
P
e âˆ’1/2 ) â‰¥ c log(pd)/(T âˆ’ p)} â†’
since we assumed by Assumption 5.8 that |B| = iâˆˆH1 1{|Î¸iâˆ— |/(Ïƒ Î£
âˆ’ âˆž. If the number
i,i
of true alternatives were fixed, this convergence would clearly not occur. Now note that by Markovâ€™s inequality


p
P
c
X
 E
2 log(pd))}
iâˆˆB 1{|Zi | â‰¥
p
ci | â‰¥ 2 log(pd))} â‰¥ |B| â‰¤
P
1{|Z
|B|
iâˆˆB
p
P
c
2 log(pd)))
iâˆˆB P(|Zi | â‰¥
=
|B|
P

âˆ’
â†’ 1,
p
P
ci | â‰¥ 2 log(pd))} â‰¤ |B|, we have
where the convergence follows by (B.9). Therefore, since iâˆˆB 1{|Z
p
P
c
2 log(pd))} P
iâˆˆB 1{|Zi | â‰¥
âˆ’
â†’ 1.
|B|
This line implies that for 0 â‰¤ Î½b â‰¤ xpd , our FDR control procedure will correctly identify all true positives that meet a
certain minimum signal condition. The result of this lemma then follows from the definition of Î½b Section 4.2.
B.5. Proof of Lemma A.5
We will need the following lemma, which is a modified version on Lemma 6.2 from Liu et al. (2013b) to prove Lemma A.5.
âˆš
Lemma B.7. Let Î·1 , . . . , Î·n have mean zero, where Î·t = (Î·t,1 , Î·t,2 )> . Suppose that p â‰¤ nr , log(p) = o( n), and
E[kÎ¾i k2bpr+2+ ] â‰¤ âˆž, for r, b,  > 0. Furthermore, assume that V [Î·t,1 ] = V [Î·t,2 ] and |Cov(Î·t,1 , Î·t,2 )| â‰¤ Î´, for some
0 â‰¤ Î´ â‰¤ 1. Then
 n



 X

 n

âˆš
âˆš X




Î·i,2  â‰¥ t n â‰¤ C(t + 1)âˆ’2 exp[âˆ’t2 /(1 + Î´)],
P 
Î·i,1  â‰¥ t n, 
i=1

i=1

for 0 â‰¤ t â‰¤ b log(2), where C depends only on b, r, , Î´.
The proof of Lemma B.7 follows almost exactly the Proof of Lemma 6.2 in Liu et al. (2013b). The only difference is that
whereas Lemma 6.2 in Liu et al. (2013b) requires i.i.d. Î·t vectors in order to cite the Proof of Lemma 6.1 in Liu et al.
(2013b), we do not require i.i.d. Î·t vectors and instead appeal to the proof of Lemma B.6. We refer the reader to Liu et al.
(2013b) for more details. Having established Lemma B.7, we now present the proof of Lemma A.5.
2/3

Î´

Proof of Lemma A.5. Let b0 < b1 < . . . < bk and Î½i = Gâˆ’1 (bi ), where b0 = ypd /(pd), bi = ypd /(pd) + ypd ei /(pd),
2/3

k = [log((pd âˆ’ ypd )/ypd )]1/Î´ , and 0 < Î´ < 1. Then we have G(Î½i )/G(Î½i+1 ) = 1 + o(1) for all 0 â‰¤ i â‰¤ k, and
p
Î½0 / 2 log(pd/ypd ) = 1 + o(1). One can easily verify that 0 â‰¤ j â‰¤ k â†” 0 â‰¤ Î½ â‰¤ ypd . So we see that to prove this lemma
it suffices to show that
P
ci | â‰¥ Î½j ) âˆ’ G(Î½j )]  P
 iâˆˆH0 [1(|Z

âˆ’
max
(B.10)
 â†’ 0.
0â‰¤jâ‰¤k 
|H0 |G(Î½j )
Observe that for all  > 0,
P
P




ci | â‰¥ Î½j ) âˆ’ G(Î½j )] 
ci | â‰¥ Î½j ) âˆ’ G(Î½j )] 
 iâˆˆH0 [1(|Z
 iâˆˆH0 [1(|Z





P max 
max 
 â‰¥  â‰¤ P 0â‰¤jâ‰¤k
â‰¥ 2
0â‰¤jâ‰¤k
|H0 |G(Î½j )
|H0 |G(Î½j )
 P

k
ci | â‰¥ Î½j ) âˆ’ G(Î½j )] 
X
 iâˆˆH0 [1(|Z



â‰¤
P 
â‰¥ 2 .
|H0 |G(Î½j )
j=0

(B.11)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Now let
P
I(Î½) =

iâˆˆH0 [1(|Zi |

c â‰¥ Î½) âˆ’ P(|Z
ci | â‰¥ Î½)]
.
|H0 |G(Î½)

Note that
ci | â‰¥ Î½j ) âˆ’ G(Î½j )]
c â‰¥ Î½) âˆ’ P(|Z
ci | â‰¥ Î½)] P P
[1(|Z
âˆ’
â†’ iâˆˆH0
,
|H0 |G(Î½)
|H0 |G(Î½j )

P
I(Î½) =

iâˆˆH0 [1(|Zi |

by (B.8) in the proof of Lemma A.4 in Appendix B.4. Clearly, E[I(Î½)] = 0, so if we can show that V [I(Î½)] = E[I 2 (Î½)] â†’ 0,
P
we will have that I(Î½) âˆ’
â†’ 0. By (B.11), this convergence will prove (B.10). We now decompose V [I(Î½)] = E[I 2 (Î½)] as
follows:
P
P
2 c
c
c
c
c
c
i,jâˆˆH0 ,i6=j [P(|Zi | â‰¥ Î½, |Zj | â‰¥ Î½) âˆ’ P(|Zi | â‰¥ Î½)P(|Zj | â‰¥ Î½)]
iâˆˆH0 [P(|Zi | â‰¥ Î½) âˆ’ P (|Zi | â‰¥ Î½)]
2
E[I (Î½)] =
+
|H0 |2 G2 (Î½)
|H0 |2 G2 (Î½)
 c

cj | â‰¥ Î½)
X
X
C|H0 |G(Î½)
P(|Zi | â‰¥ Î½, |Z
1
1
c
c
â‰¤
+
P(|Zi | â‰¥ Î½, |Zj | â‰¥ Î½) +
âˆ’1 ,
(|H0 |G(Î½))2 G2 (Î½)|H0 |2
|H0 |2
G2 (Î½)
T
c
(i,j)âˆˆA()

i,jâˆˆH0

A()

(B.12)
where the equality follows by direct computation, and the inequality holds by (B.8) in the proof of Lemma A.4 in Appendix
B.4. If we let
X
1
ci | â‰¥ Î½, |Z
cj | â‰¥ Î½),
I1,1 (Î½) = 2
P(|Z
2
G (Î½)|H0 |
(i,j)âˆˆA()

and
I1,2 (Î½) =

1
|H0 |2



X
i,jâˆˆH0

T

A()c


ci | â‰¥ Î½, |Z
cj | â‰¥ Î½)
P(|Z
âˆ’
1
,
G2 (Î½)

then (B.12) yields
C
+ I1,1 (Î½) + I1,2 (Î½).
|H0 |G(Î½)
p
Applying Lemma B.6 to I1,2 (Î½) yields the following result for all 0 â‰¤ t â‰¤ 2 log(pd) for some Î´ > 0:
E[I 2 (Î½)] â‰¤

|I1,2 (Î½)| â‰¤ C(log(pd))âˆ’1âˆ’Î´ .

(B.13)

(B.14)

Furthermore, Lemma B.7 yields

ci | â‰¥ Î½, |Z
cj | â‰¥ Î½) â‰¤ C exp
P(|Z


âˆ’Î½ 2
,
1 + |Ïi,j | + Î´1

(B.15)

for all (i, j) âˆˆ A() and i, j âˆˆ H0 , where Î´1 , C > 0. Lastly, the proceeding result follows from (B.15) and Property 1
I1,1 (Î½) â‰¤ C(log(pd))âˆ’2 .

(B.16)

Then by (F.2), (B.14), and (B.16), we have that
k
X

E[I(Î½j )]2 â‰¤ Ck[(log(pd))1âˆ’Î´ + (log(pd))âˆ’2 ] + C

j=0

k
X

(pdG(Î½j ))âˆ’1

j=0

â‰¤C

k
X

1

j=0

ypd + ypd ej Î´

2/3

+ o(1)

= o(1).
The second inequality holds by the definition of the sequence Î½i and since k = o(log(pd)). The third inequality follows
2/3 Î´
since 1/(ypd + ypd ej ) = o(1/ypd ) = o(1/(pd)). Our desired result (B.10) follows naturally from this last set of
inequalities.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

B.6. Proof of Lemma A.6
We present a variation on the argument made in the Proof of Theorem 3.1 in Liu et al. (2013b).
Proof of Lemma A.6. Since by the definition of Î½b in 4.9, Î½b is the infimum of all values Î½ > 0 such that FDP(Î½) â‰¤ Î±, for
Î½ < Î½b
P
c
iâˆˆH0 1(|Zi | â‰¥ Î½)
> Î±.
P
ci â‰¥ Î½), 1}
max{
1(|Z
1â‰¤jâ‰¤pd

ci , which holds by Theorem 5.5, we can approximate P
c
Using the asymptotic normality of Z
iâˆˆH0 1(|Zi | â‰¥ Î½) by (pd)G(Î½),
yielding

max{

P

(pd)G(Î½)
> Î±,
ci â‰¥ Î½), 1}
1(|Z

1â‰¤jâ‰¤pd

P
ci â‰¥ Î½), 1} is decreasing in Î½. Then by letting Î½ approach Î½b, we obtain
for Î½ < Î½b. Note that (pd)G(Î½)/ max{ 1â‰¤jâ‰¤pd 1(|Z

max{

P

(pd)G(b
Î½)
â‰¥ Î±.
ci â‰¥ Î½b), 1}
1(|Z

(B.17)

1â‰¤jâ‰¤pd

Now to prove the reverse bound, we note that the definition of infimum implies the existence of a sequence Î½k , where
kâ†’âˆž
Î½k â‰¥ Î½b and Î½k âˆ’âˆ’âˆ’âˆ’â†’ Î½b. Since Î½k â‰¥ Î½b, we have
(pd)G(Î½k )
â‰¤ Î±.
P
ci â‰¥ Î½k ), 1}
max{ 1â‰¤jâ‰¤pd 1(|Z
Thus, by letting Î½k â†’ Î½b, we see that
max{

P

(pd)G(b
Î½)
â‰¤ Î±.
ci â‰¥ Î½b), 1}
1(|Z

(B.18)

1â‰¤jâ‰¤pd

Therefore, by (B.17) and (B.18)
(pd)G(b
Î½)
= Î±,
P
ci â‰¥ Î½b), 1}
max{ 1â‰¤jâ‰¤pd 1(|Z
as desired.

C. Proofs of Auxiliary Lemmas in Appendix B
In this section we present the proofs of auxiliary lemmas introduced in Appendix B.
C.1. Proof of Lemma B.1
Here we present a modified version of the proof for Theorem 7.(b) from Javanmard & Montanari (2014).
e n âˆ’ Ikâˆž â‰¤ kÎ£
e âˆ’1 Î£
e n âˆ’ Ikâˆž . Let X t = Î£
e âˆ’1/2 X
ft , where X
ft is as defined in Section
Proof of Lemma B.1. Clearly kMÎ£
pdÃ—pd
3. Now define Z âˆˆ R
as follows:




T
T
X
X
e âˆ’1 Î£
en âˆ’ I = 1
e âˆ’1 X
ft X
f> âˆ’ I = 1
e âˆ’1/2 X t X > Î£
e 1/2 âˆ’ I .
Z=Î£
Î£
Î£
t
t
T âˆ’ p t=p+1
T âˆ’ p t=p+1
(ij)
e âˆ’1/2 , X t i Â· hÎ£
e 1/2 , X t i âˆ’ Î´i,j , where p + 1 â‰¤ t â‰¤ T , and
For any given pair 1 â‰¤ i, j â‰¤ pd, denote Î³t
= hÎ£
i,Â·
j,Â·
Î´i,j represents the Kronecker delta: Î´i,j = 1(i = j). Let Ft be the filtration Ft = Ïƒ(X 1 , . . . , X t ). Then note that

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference
(ij)

(ij)

E[Î³t |Ftâˆ’1 ] = 0, so by Definition G.1 in Appendix G, Î³t forms a martingale difference sequence. Note further that
PT
(ij)
e âˆ’1 Î£
e n âˆ’ Ikâˆž , and by transitivity kMÎ£
e n âˆ’ Ikâˆž , we need only bound
Zi,j = (T âˆ’ p)âˆ’1 t=p+1 Î³t . Thus, to bound kÎ£
Zi,j .
We refer the reader to Definition G.2 in Appendix G for the definition of the sub-exponential norm. By Remark 5.18
(ij)
(Centering) from Vershynin (2012), we can bound the sub-exponential norm of Î³t as follows:
(ij)

kÎ³t

âˆ’1/2

e
kÏˆ1 â‰¤ 2khÎ£
i,Â·

1/2

e , X t ikÏˆ .
, X t i Â· hÎ£
1
j,Â·

(C.1)

Now note that, as shown by Javanmard & Montanari (2014) we can bound the sub-exponential norm of the product of two
random variables X and Y by:
 
1/q
kXY kÏˆ1 â‰¤ sup q âˆ’1 E |XY |q
qâ‰¥1

 
1/2q  
1/2q
â‰¤ sup q âˆ’1 E |X|2q
E |Y |2q
qâ‰¥1


 
1/r 
 
1/r 
âˆ’1/2
r
âˆ’1/2
r
â‰¤ 2 sup r
E |X|
sup r
E |Y |
râ‰¥2

râ‰¥2

â‰¤ 2kXkÏˆ2 Â· kY kÏˆ2 .
Therefore, by (C.1)
(ij)

kÎ³t

e âˆ’1/2 , X t i Â· hÎ£
e 1/2 , X t ikÏˆ â‰¤ 2khÎ£
e âˆ’1/2 , X t ikÏˆ Â· khÎ£
e 1/2 , X t ikÏˆ ,
kÏˆ1 â‰¤ 2khÎ£
1
2
2
i,Â·
j,Â·
i,Â·
j,Â·

and by assumption
s
âˆ’1/2

e
2khÎ£
i,Â·

e 1/2 , X t ikÏˆ â‰¤ 2kÎ£
e âˆ’1/2 kÏˆ Â· kÎ£
e 1/2 kÏˆ Îº2 â‰¤ 2Îº2
, X t ikÏˆ2 Â· khÎ£
2
2
2
j,Â·
i,Â·
j,Â·

Cmax
.
Cmin

p
(ij)
(ij)
Thus, if we let Îº0 = 2Îº2 Cmax /Cmin , then kÎ³t kÏˆ1 â‰¤ Îº0 . Now, since Î³t is a martingale difference sequence, we can
apply the Bernstein inequality for martingale difference sequences (Lemma F.8 in Appendix F) to obtain:

P


 T


 2

T âˆ’p


1  X (ij) 
â‰¥

â‰¤
2
exp
âˆ’
Î³
min
,
.
T âˆ’ p t=p+1 t 
6
eÎº0
eÎº0

p
Let  = a log(pd)/(T âˆ’ p), and assume that T âˆ’ p â‰¥ (a/(eÎº0 ))2 log(pd) so that (/(eÎº0 ))2 â‰¤ (/(eÎº0 )) â‰¤ 1. Then,

P

s
!
 T

1  X (ij) 
log(pd)
Î³
â‰¥a
=P
T âˆ’ p  t=p+1 t 
T âˆ’p

s
!




log(pd)
Zi,j  â‰¥ a


T âˆ’p

â‰¤ 2(pd)âˆ’a

2

/(6e2 Îº02 )

2

= 2(pd)âˆ’(a

Cmin )/(24e2 Îº4 Cmax )

.

Taking the union over all (pd)2 pairs and letting c2 = (a2 Cmin )/(24e2 Îº4 Cmax ) âˆ’ 2 yields the result:
ï£«
e n âˆ’ Ikâˆž â‰¤ a
P ï£­kMÎ£

s

ï£¶
log(pd)
ï£¸ â‰¥ 1 âˆ’ 2(pd)âˆ’c2 .
T âˆ’p

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

C.2. Proof of Lemma B.2
The proof of Lemma B.2 relies on two lemmas. First, the following lemma asserts that the restricted eigenvalue condition
e As we will see in the proof of Lemma B.2 below, the restricted eigenvalue
(RE condition) holds true for our design matrix X.
condition implies the restricted strong convexity condition when the loss function is the least squares loss function. This
property will prove instrumental in bounding kÎ¸b âˆ’ Î¸ âˆ— k1 .
Lemma C.1. Under Assumptions 5.3 and 5.4, we have
inf
b âˆ— âˆˆEr
Î¸âˆ’Î¸

1
e Î¸b âˆ’ Î¸ âˆ— )k2 > 0
kX(
2
T âˆ’p

with probability at least


(Ï‰(A))2
,
1 âˆ’ 2 exp(âˆ’c20 c21 c2 Ï‰ 2 (B)) âˆ’ L exp âˆ’ 4
Î±2
1
e 2 is the restricted minimum eigenvalue of Î£
e n restricted to A âŠ† S pdâˆ’1
kXuk
2
T âˆ’p
e 1/2 u/kÎ£
e 1/2 uk2 , u âˆˆ A} is the normalized set of A, Î± = diam(A) =
e=Î£
(the unit sphere in Rpd space), B = {e
u:u
supu,vâˆˆA d(u, v) = supu,vâˆˆA ku âˆ’ vk2 , and c0 , c1 , c2 , L > 0 are constants.

e n |A) = inf uâˆˆA
where where Î»min (Î£

The RE condition has been studied extensively in the setting where the rows of the design matrix are independent. However,
since the rows of the design matrix are dependent in this setting, we must appeal to martingale theory to prove the RE
e Î¸b âˆ’ Î¸ âˆ— )k2 , and then bound the minimum
condition. We construct a martingale difference sequence equal to 1/(T âˆ’ p)kX(
2
restricted eigenvalue of that sequence using the results from Appendix F. We defer the proof of Lemma C.1 to Appendix
D.1.
Second, the following lemma establishes with high-probability a property of the regularization parameter Î».
âˆ’1
e 2
Lemma C.2.
p Denote the least-squares loss function L(Î¸) = (2(T âˆ’ p)) kY âˆ’ XÎ¸k2 . If Assumption 5.4 holds and
Î» = 8CÏƒ log(pd)/(T âˆ’ p) for some constant C, then
Î» â‰¥ 2kâˆ‡L(Î¸ âˆ— )kâˆž ,
holds with probability at least 1 âˆ’ b1 exp[âˆ’b2 Ïƒ 2 log(pd)], for constants b1 and b2 .
We defer the proof of Lemma C.2 to Appendix D.2.
We now present a proof of Lemma B.2.
Proof of Lemma B.2. To prove this lemma, we first recall the form of the biased Lasso Granger estimator from (3.3):
Î¸b = arg min
Î¸

1
e 2 + Î»kÎ¸k1 .
kY âˆ’ XÎ¸k
2
2(T âˆ’ p)

e 2 . We immediately realize that the optimality of Î¸b
For brevity, we denote the loss function L(Î¸) = (2(T âˆ’ p))âˆ’1 kY âˆ’ XÎ¸k
2
yields the following inequality:
b + Î»kÎ¸k
b 1 â‰¤ L(Î¸ âˆ— ) + Î»kÎ¸ âˆ— k1 .
L(Î¸)

(C.2)

b we will appeal to the restricted strong convexity condition (RSC condition), which
To establish a lower bound on L(Î¸),
b
provides a lower bound on the first-degree Taylor approximation of L(Î¸):
b := L(Î¸)
b âˆ’ L(Î¸ âˆ— ) âˆ’ hâˆ‡L(Î¸ âˆ— ), Î¸b âˆ’ Î¸ âˆ— i â‰¥ Îº` kÎ¸b âˆ’ Î¸ âˆ— k2 > 0,
Î´L(Î¸)
2

(C.3)

for some constant Îº0` . As noted by Negahban et al. (2012), when L(Â·) is the least-squares loss function, as it is in our setting,
we obtain
b =
Î´L(Î¸)

1
e Î¸b âˆ’ Î¸ âˆ— )k2 â‰¥ Îº0 kÎ¸b âˆ’ Î¸ âˆ— k2 .
kX(
2
`
2
T âˆ’p

(C.4)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Note that
inf
b âˆ— âˆˆEr
Î¸âˆ’Î¸



1
1 e> e
e Î¸b âˆ’ Î¸ âˆ— )k2 = Î»min
e n |Er ),
kX(
X
X|E
= Î»min (Î£
r
2
T âˆ’p
T âˆ’p

(C.5)

e n |Er ) is the minimum restricted eigenvalue of the
where Er is the set the error vector Î¸b âˆ’ Î¸ âˆ— can fall in, and Î»min (Î£
sample covariance matrix. So we see that when L(Â·) is the least-squares loss function, the restricted strong convexity
e with high probability by Lemma C.1. Thus,
condition collapses into the RE condition. The RE condition holds for X
e
Î»m in(Î£n |Er ) > 0, and so the RSC condition holds. Then, rearranging (C.3), we see that
b â‰¥ L(Î¸ âˆ— ) + hâˆ‡L(Î¸ âˆ— ), Î¸b âˆ’ Î¸ âˆ— i + Îº` kÎ¸b âˆ’ Î¸ âˆ— k2 ,
L(Î¸)
2
2

(C.6)

where Îº` = 2Îº0` .
As a consequence of (C.2) and (C.6), we have
b 1 â‰¤ L(Î¸)
b + Î»kÎ¸k
b 1 â‰¤ L(Î¸ âˆ— ) + Î»kÎ¸ âˆ— k1 .
L(Î¸ âˆ— ) + hâˆ‡L(Î¸ âˆ— ), Î¸b âˆ’ Î¸ âˆ— i + Î»kÎ¸k
Furthermore, since hâˆ‡L(Î¸ âˆ— ), Î¸b âˆ’ Î¸ âˆ— i â‰¤ kâˆ‡L(Î¸ âˆ— )kâˆž Â· kÎ¸b âˆ’ Î¸ âˆ— k1 by Holderâ€™s inequality, we achieve the following result:
b 1 â‰¤ Î»kÎ¸ âˆ— k1 .
âˆ’kâˆ‡L(Î¸ âˆ— )kâˆž Â· kÎ¸b âˆ’ Î¸ âˆ— k1 + Î»kÎ¸k

(C.7)

We apply Lemma C.2 to establish that Î» â‰¥ 2kâˆ‡L(Î¸)kâˆž with probability at least 1 âˆ’ b1 exp[âˆ’b2 Ïƒ 2 log(pd)], for constants
b1 and b2 . Thus, since Î» â‰¥ 2kâˆ‡L(Î¸ âˆ— )kâˆž with high probability, (C.7) implies
1
b 1 â‰¤ Î»kÎ¸ âˆ— k1 .
âˆ’ Î»kÎ¸b âˆ’ Î¸ âˆ— k1 + Î»kÎ¸k
2
Now denote S to be the support of Î¸ âˆ— , so that Î¸ âˆ— = Î¸Sâˆ— and Î¸Sâˆ— c = 0. Then, based on the previous inequality, we have
1
1
âˆ’ Î»k(Î¸b âˆ’ Î¸ âˆ— )S k1 âˆ’ Î»k(Î¸b âˆ’ Î¸ âˆ— )S c k1 + Î»kÎ¸bS k1 + Î»kÎ¸bS c k1 â‰¤ Î»kÎ¸Sâˆ— k1 .
2
2
Rearranging these terms yields:
1
1
âˆ’ Î»k(Î¸b âˆ’ Î¸ âˆ— )S k1 âˆ’ Î»k(Î¸b âˆ’ Î¸ âˆ— )S c k1 + Î»k(Î¸b âˆ’ Î¸ âˆ— )S c k1 â‰¤ Î»kÎ¸Sâˆ— k1 âˆ’ Î»kÎ¸bS k1
2
2
â‰¤ Î»k(Î¸b âˆ’ Î¸ âˆ— )S k1 ,
where the second inequality follows by the triangle inequality. Rearranging terms once more produces the following result:
Î»k(Î¸b âˆ’ Î¸ âˆ— )S c k1 â‰¤ 3Î»k(Î¸b âˆ’ Î¸ âˆ— )S k1 .

(C.8)

We use (C.8) to bound Î»kÎ¸b âˆ’ Î¸ âˆ— k1 . Note that (C.2) and (C.6) together imply
1
b 1 + Îº` kÎ¸b âˆ’ Î¸ âˆ— k2 â‰¤ Î»kÎ¸ âˆ— k1 ,
âˆ’ Î»kÎ¸b âˆ’ Î¸ âˆ— k1 + Î»kÎ¸k
2
2
2
and thus,
Îº` b
b 1 + 1Î»kÎ¸b âˆ’ Î¸ âˆ— k1 .
kÎ¸ âˆ’ Î¸ âˆ— k22 â‰¤ Î»kÎ¸ âˆ— k1 âˆ’ Î»kÎ¸k
2
2
Applying the triangle inequality yields
Îº` b
1
kÎ¸ âˆ’ Î¸ âˆ— k22 â‰¤ Î»kÎ¸b âˆ’ Î¸ âˆ— k1 + Î»kÎ¸b âˆ’ Î¸ âˆ— k1
2
2
3
= Î»k(Î¸b âˆ’ Î¸ âˆ— )k1 .
2

(C.9)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Note that by (C.8), we have kÎ¸b âˆ’ Î¸ âˆ— k1 â‰¤ 4k(Î¸b âˆ’ Î¸ âˆ— )S k1 . Substituting this result into (C.9) allows us to conclude that
Îº` b
âˆš
âˆš
kÎ¸ âˆ’ Î¸ âˆ— k22 â‰¤ 6Î»k(Î¸b âˆ’ Î¸ âˆ— )S k1 â‰¤ 6Î» s0 k(Î¸b âˆ’ Î¸ âˆ— )S k2 â‰¤ 6Î» s0 kÎ¸b âˆ’ Î¸ âˆ— k2 .
2
âˆš
Thus, kÎ¸b âˆ’ Î¸ âˆ— k2 â‰¤ 12Î» s0 /Îº` , which offers us the result of this lemma:
kÎ¸b âˆ’ Î¸ âˆ— k1 â‰¤

âˆš

s0 kÎ¸b âˆ’ Î¸ âˆ— k2 â‰¤

12Î»s0
.
Îº`

We note that this result holds with high probability by Lemma C.2.

C.3. Proof of Lemma B.3
Here we verify that the Lindeberg condition (Hall & Heyde, 1980) holds for Î¶i,t .
f> mi )2 /(Ïƒ 2 [m> Î£
e n mi ])]âˆ’1/2 , some
Proof of Lemma B.3. Note that for some random variable Q âˆ¼ N (0, 1), C = [(t X
t
i
positive, fixed constant Î´ > 0, and t â‰¥ p:
E[Î¶t2 1(kÎ¶t | â‰¥ Î´)|Ftâˆ’1 ] =

f> mi )2 E[Q2 1(|Q| > Î´C)]
(t X
t
.
e n mi ]
Ïƒ 2 [m> Î£

(C.10)

i

By the properties of the truncated standard normal, we can see that E[Q2 |Q > c] = 1 + c(Ï†(c)/Î¦(c)), where Ï†(c) is the
PDF of the standard normal. Thus,




Ï†(c)
c + c2
E[Q 1(Q > c)] = 2Î¦(c) 1 + c
= 2(Î¦(c) + cÏ†(c)) â‰¤ 2Ï†(c)
.
Î¦(c)
c
2

Now the proceeding inequality follows from the union bound and a standard bound on the normal CDF:
max

p+1â‰¤tâ‰¤T

f> mi | > v,
|t X
t

q
>e
e
with probability at most 2(T âˆ’ p) exp[âˆ’v 2 /(2m>
i Î£mi )]. If we let v = 2 log(T âˆ’ p)mi Î£mi , the we obtain the
following bound:
max

p+1â‰¤tâ‰¤T

q
f> mi | â‰¤ 2 log(T âˆ’ p)m> Î£m
e i,
|t X
t
i

with probability at least 1 âˆ’ 2/(T âˆ’ p). Returning to (C.10), we now see that for D =
E[Î¶t2 1(k zetat k â‰¥ Î´)|Ftâˆ’1 ] â‰¤

p
(T âˆ’ p)/(4 log(T âˆ’ p):

8 log(T âˆ’ P )Ï†(Î´D)((Î´D)âˆ’1 + Î´D)
,
T âˆ’p

since Ï†(z)(z âˆ’1 + z) is a decreasing function. Therefore, if we sum over all t, we attain
T
X

E[Î¶t2 1(kÎ¶t | â‰¥ Î´)|Ftâˆ’1 ] â‰¤ 8 log(T âˆ’ P )Ï†(Î´D)((Î´D)âˆ’1 + Î´D) â†’ 0,

t=p+1

which demonstrates that the Lindeberg condition does indeed hold for Î¶t .

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

C.4. Proof of Lemma B.6
We present a modified version of the proof of Lemma 6.1 from Liu et al. (2013b).
Proof of Lemma B.6. Define filtration Ft = Ïƒ(Î¾1 , . . . , Î¾t ). For 1 â‰¤â‰¤ p, let:
Î¾bt = Î¾t 1{kÎ¾t k2 â‰¤
Î¾et = Î¾t âˆ’ Î¾bt .

âˆš

âˆš

n/(log(p))4 } âˆ’ E[Î¾t 1{kÎ¾t k2 â‰¤

n/(log(p))4 }|Ft ],
(C.11)

Note that by Definition G.1, Î¾bi forms a martingale difference sequence (MDS). Now we have by the triangle inequality

 X
 n 

Î¾
P 
t


âˆš
â‰¥t n



min

t=1


 X
 n

âˆš
âˆš
bt â‰¥ t n âˆ’ n/(log(p))2 
Î¾
â‰¤P 





min

t=1


 X
 n

âˆš
et â‰¥ n/(log(p))2 
Î¾
+P 




,

(C.12)

min

t=1

and

 X
 n 

Î¾
P 
t


âˆš
â‰¥t n



min

t=1


 X
 n

âˆš
âˆš
bt â‰¥ t n + n/(log(p))2 
â‰¥P 
Î¾





min

t=1


 X
 n

âˆš
2
e

âˆ’P 
Î¾t â‰¥ n/(log(p)) 


.

(C.13)

min

t=1

âˆš
By our assumption that log(p) = o( n), we have that,
n
X

E[Î¾t 1{kÎ¾t k2 â‰¤

âˆš

âˆš
n/(log(p))4 }|Ft ] = o( n/(log(p))2 ).

t=1

So by our assumption that E[kÎ¾t kbpr+2+
] â‰¤ âˆž, for r, b,  > 0, the previous line implies that:
2

 X
 n

âˆš
et â‰¥ n/(log(p))2 
P 
Î¾




â‰¤ nP( max kÎ¾t k â‰¥
1â‰¤tâ‰¤n

min

t=1

âˆš

n/(log(p))4 ) â‰¤ C(log(p))âˆ’3/2 (G(t))p ,

p
for 0 â‰¤ t â‰¤ b log(p). Thus, by (C.13) and (C.12), we obtain:

 X
 n 

P 
Î¾
t


âˆš
â‰¥t n


 X
 n 

P 
Î¾
t


âˆš
â‰¥t n

t=1



min


 X
 n

âˆš
âˆš
bt â‰¥ t n + n/(log(p))2 
â‰¥P 
Î¾






 X
 n

âˆš
âˆš
bt â‰¥ t n âˆ’ n/(log(p))2 
â‰¤P 
Î¾





t=1

âˆ’ C(log(p))âˆ’3/2 (G(t))p ,

min

and

t=1

min



t=1

+ C(log(p))âˆ’3/2 (G(t))p ,

min

for constant C > 0 Therefore, it suffices to prove
sup
âˆš

0â‰¤tâ‰¤b

log(p)

P
âˆš


 P(k nt=1 Î¾bt kmin â‰¥ (t Â± (log(p)âˆ’2 ) n)


âˆ’ 1 â‰¤ C(log(p))âˆ’1âˆ’Î³1

(G(t))p

(C.14)

in order to prove this lemma. To prove this line, we appeal to Theorem 1.1 from Zaitsev (1987). The original version of
Theorem 1.1 requires i.i.d. vectors only in order to leverage the Bernstein inequality. However, since in this application Î¾bt
form a MDS, the proof of Theorem 1.1 holds by our Bernstein inequality for MDS (Lemma F.8). In the interest of clarity,

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

since Theorem 1.1 requires introducing a significant amount of new material from probability theory, we refer the reader to
Zaitsev (1987) for more details. Thus, by application of Theorem 1.1 from Zaitsev (1987) to Î¾bt , we obtain

 X

 n 
âˆš
âˆ’2
bt 
P 
Î¾
â‰¥
(t
+
(log(p)
)
n
â‰¤ P(kW kmin â‰¥ t âˆ’ 2(log(p))âˆ’2 ) + c1,p exp[âˆ’c2,d (log(p))2 ],


t=1

min

and

 X
 n 
bt 
Î¾
P 


t=1


âˆš
â‰¥ (t âˆ’ (log(p)âˆ’2 ) n â‰¤ P(kW kmin â‰¥ t + 2(log(p))âˆ’2 ) âˆ’ c1,p exp[âˆ’c2,d (log(p))2 ],

min




Pn b âˆš
where c1,p , c2,p > 0 are constants that depend only on p and W âˆ¼ N 0, Cov
n
. By our assumption that
Î¾
/
t=1 i
k Cov(Î¾i ) âˆ’ IpÃ—p k2 â‰¤ C(log(p)âˆ’2âˆ’Î³ , one can easily show that,
P(kW kmin â‰¥ t âˆ’ 2(log(p))âˆ’2 ) â‰¤ (1 + C(log(p))âˆ’1âˆ’Î³1 )(G(t))p ,
and
P(kW kmin â‰¥ t + 2(log(p))âˆ’2 ) â‰¤ (1 âˆ’ C(log(p))âˆ’1âˆ’Î³1 )(G(t))p ,
p
p
for 0 â‰¤ t â‰¤ b log(p). Since for 0 â‰¤ t â‰¤ b log(p) we have c1,p exp[âˆ’c2,p (log(p))2 ] â‰¤ C(log(p))âˆ’1âˆ’Î³1 (G(t))p , the
following holds:

 X

 n 
âˆš
âˆ’2
b


P 
(C.15)
Î¾t 
â‰¥ (t âˆ’ (log(p) ) n â‰¤ (1 + C(log(p))âˆ’1âˆ’Î³1 )(G(t))p ,
t=1

min

and

 X
 n 
b

P 
Î¾t 

t=1

âˆ’2

â‰¥ (t + (log(p)

âˆš

) n



â‰¤ (1 âˆ’ C(log(p))âˆ’1âˆ’Î³1 )(G(t))p ,

(C.16)

min

p
for 0 â‰¤ t â‰¤ b log(p). Therefore, (C.15) and (C.16) imply (C.14), which concludes the proof.

D. Proofs of Supporting Lemmas in Appendix C
In this section we present proofs for the supporting Lemmas of Lemma B.2.
D.1. Proof of Lemma C.1
We present a variation of the proof of Theorem 5 from Johnson et al. (2016). This proof will rely on lower bounding
1
PT
2
e Î¸b âˆ’ Î¸ âˆ— )k2 by inf uâˆˆA 1/(T âˆ’ p) PT
f
f
inf Î¸âˆ’Î¸
kX(
b âˆ— âˆˆEr
2
t=p+1 hXt âˆ’ Âµt , ui âˆ’ supuâˆˆA 2/(T âˆ’ p)
t=p+1 hXt âˆ’ Âµt , ui.
T âˆ’p
PT
P
ft âˆ’Âµt , ui2 in Lemma D.1 and upper bound supuâˆˆA 2/(T âˆ’p) T
f
We lower bound inf uâˆˆA 1/(T âˆ’p) t=p+1 hX
t=p+1 hXt âˆ’
Âµt , ui in Lemma D.2 below.
ft âˆ’ Âµt , for Âµt = E[X
ft |Ftâˆ’1 ], where Ft is the filtration Ft = Ïƒ(X
fp+1 , . . . , X
ft ), and Z =
Lemma D.1. Let Zt = X
âˆ’1/2

[Zp+1 , . . . , ZT ]> . Furthermore, let E[Z> Z/(T âˆ’ p)] = Î£Z and kÎ£Z

Zt kÏˆ2 â‰¤ Îº0 . Then,




T
X
2c0 c1 Îº02 Ï‰(B)
1
2
f
âˆš
P inf
hXt âˆ’ Âµt , ui â‰¥ Î»min (Î£Z |A) 1 âˆ’
> 0 â‰¥ 1 âˆ’ 2 exp(âˆ’c20 c21 c2 Ï‰ 2 (B))
uâˆˆA T âˆ’ p
T
âˆ’
p
t=p+1


where Î»min (Î£|A) = inf uâˆˆA u> Î£u is the restricted minimum eigenvalue of Î£ restricted to A âŠ† S pdâˆ’1 (the unit sphere in
e = Î£1/2 u/kÎ£1/2 uk2 , u âˆˆ A} is the normalized set of A.
Rpd space), and B = {e
u:u
The proof of Lemma D.1 relies on the restricted eigenvalue condition for martingale difference sequences from Appendix F.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

ft |Ftâˆ’1 ], where Ft is the filtration Ft = Ïƒ(X
fp+1 , . . . , X
ft ), A âŠ† S pdâˆ’1 (the unit sphere in
Lemma D.2. Let Âµt = E[X
pd
R space), and Î± = diam(A) = supu,vâˆˆA d(u, v). Then,



T
X
(Ï‰(A))2
2
f
.
hXt âˆ’ Âµt , ui â‰¤ 0 â‰¥ 1 âˆ’ L exp âˆ’ 4
Î±2
uâˆˆA T âˆ’ p t=p+1


P sup

The proof of Lemma D.2 employs a generic chaining argument (Talagrand, 2006). We defer the proofs of Lemmas D.1 and
D.2 to Appendicies E.1 and E.2. We now present the proof of Lemma C.1.
Proof of Lemma C.1. We seek to prove that,
inf
b âˆ— âˆˆEr
Î¸âˆ’Î¸

1
e Î¸b âˆ’ Î¸ âˆ— )k2 > 0.
kX(
2
T âˆ’p

(D.1)

From Negahban et al. (2012), we note that the error set Er is actually a cone, and that the magnitude of the error vector
Î¸b âˆ’ Î¸ âˆ— in (D.1) does not matter, only the direction does. Thus, we consider set A = S pdâˆ’1 âˆ© Er and reformulate our
problem as
inf

uâˆˆA

1
e 2 > 0.
kXuk
2
T âˆ’p

(D.2)

It suffices to prove (D.2) to prove the result of this lemma.
ft |Ftâˆ’1 ],
We now construct a martingale difference sequence that we will bound in order to prove (D.2). Let Âµt = E[X
f
f
f
where Ft is the filtration Ft = Ïƒ(Xp+1 , . . . , Xt ), so that by Definition G.1 in Appendix G, Xt âˆ’ Âµt forms a martingale
difference sequence (MDS). Then we have,
T
X
1
1
2
e
ft , ui
kXuk2 =
hX
T âˆ’p
T âˆ’ p t=p+1



T
X
1
>
2
>
>
>
>
>
2
>
2
f
f
f
=
(Xt u) âˆ’ 2(Xt u)(Âµt u) + 2(Xt u)(Âµt u) âˆ’ (Âµt u) + (Âµt u)
T âˆ’ p t=p+1


T
X
1
>
>
2
>
2
>
>
f
f
=
(Xt u âˆ’ Âµt u) âˆ’ (Âµt u) + 2(Xt u)(Âµt u) .
T âˆ’ p t=p+1
Distributing the summation in the last line then yields:
T
T
T
X
X
X
2
1
ft âˆ’ Âµt , ui2 âˆ’ 1
ft , uihÂµt , ui
e 2= 1
hX
hÂµt , ui2 +
hX
kXuk
2
T âˆ’p
T âˆ’ p t=p+1
T âˆ’ p t=p+1
T âˆ’ p t=p+1

=

T
T
T
X
X
X
1
2
ft âˆ’ Âµt , ui2 + 1
ft âˆ’ Âµt , uihÂµt , ui
hX
hÂµt , ui2 +
hX
T âˆ’ p t=p+1
T âˆ’ p t=p+1
T âˆ’ p t=p+1

â‰¥

T
T
X
X
1
ft âˆ’ Âµt , ui2 + 2
ft âˆ’ Âµt , uihÂµt , ui.
hX
hX
T âˆ’ p t=p+1
T âˆ’ p t=p+1

Hence,
T
T
X
X
1
1
2
e 2 â‰¥ inf
ft âˆ’ Âµt , ui2 + inf
ft âˆ’ Âµt , uihÂµt , ui
kXuk
hX
hX
2
uâˆˆA T âˆ’ p
uâˆˆA T âˆ’ p
uâˆˆA T âˆ’ p
t=p+1
t=p+1

inf

T
T
X
X
1
ft âˆ’ Âµt , ui2 âˆ’ sup 2
ft âˆ’ Âµt , ui,
hX
hX
uâˆˆA T âˆ’ p
T
âˆ’
p
uâˆˆA
t=p+1
t=p+1

â‰¥ inf

(D.3)

(D.4)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where the second inequality holds since we can scale the design matrix to fall in the L2 unit ball so that hÂµt , ui <= 1.
PT
ft âˆ’ Âµt , ui2 and upper bound sup
Thus, to prove (D.2), we must lower bound inf uâˆˆA 1/(T âˆ’ p) t=p+1 hX
uâˆˆA 2/(T âˆ’
P
PT
T
2
f
f
p) t=p+1 hXt âˆ’ Âµt , ui. We bound inf uâˆˆA 1/(T âˆ’ p) t=p+1 hXt âˆ’ Âµt , ui with Lemma D.1, and supuâˆˆA 2/(T âˆ’
PT
ft âˆ’ Âµt , ui with Lemma D.2. Thus, by application of Lemmas D.1 and D.2 to (D.3), we have that
p) t=p+1 hX
T
T
X
X
1
1
ft âˆ’ Âµt , ui2 âˆ’ sup 2
ft âˆ’ Âµt , ui
e 2 â‰¥ inf
h
X
hX
kXuk
2
uâˆˆA T âˆ’ p
uâˆˆA T âˆ’ p
uâˆˆA T âˆ’ p t=p+1
t=p+1


2c0 c1 Îº02 Ï‰(B)
âˆš
â‰¥ Î»min (Î£Z |A) 1 âˆ’
> 0,
T âˆ’p

inf

with probability at least
1âˆ’

2 exp(âˆ’c20 c21 c2 Ï‰ 2 (B))


(Ï‰(A))2
.
âˆ’ L exp âˆ’ 4
Î±2


e satisfies that restricted eigenvalue condition with high probability.
Thus, X
D.2. Proof of Lemma C.2
Here we prove Lemma C.2.



e > 
Proof of Lemma C.2. To prove this lemma, we first note that 2kâˆ‡L(Î¸ âˆ— )kâˆž = 4 (T âˆ’ p)âˆ’1 X
 . Let  be the error
âˆž

e âˆ— . Then Assumption 5.4 implies that  (3.2) is sub-Gaussian. Note that since Assumption
vector from (3.2), so  = Y âˆ’ XÎ¸
f
f so that kX
e Â·,j k2 /âˆšT âˆ’ p â‰¤ 1,
5.4 ensures that kXi kÏˆ2 â‰¤ Îº, for i âˆˆ {1, 2, . . . , T âˆ’ p}, we can scale the columns of any X
for 1 â‰¤ k â‰¤ pd. Then the sub-Gaussian tails of  guarantee that for all t > 0
1
e ik2 < t,
khX,
T âˆ’p
with probability at least 1 âˆ’ 2 exp[âˆ’(T âˆ’ p)t2 /(2Ïƒ 2 )]. Bounding over all pd columns yields:


 1


>
e 
X

 < t,
T âˆ’ p

âˆž

p
holds with probability at least 1 âˆ’ 2 exp[âˆ’(T âˆ’ p)t2 /(2Ïƒ 2 ) + log(pd)]. Setting t = 2Ïƒ log(pd)/(T âˆ’ p) allows us to
conclude that
s


 1

log(pd)

âˆ—
> 
e
Î» = 8Ïƒ
â‰¥ 2kâˆ‡L(Î¸ )kâˆž = 4 
X  ,
T âˆ’ p

T âˆ’p
âˆž

holds with probability at least 1 âˆ’ b1 exp[âˆ’b2 Ïƒ 2 log(pd)].

E. Proofs of Supporting Lemmas in Appendix D
In this section, we present proofs of the supporting lemmas for Lemma C.1.
E.1. Proof of Lemma D.1
PT
ft âˆ’ Âµt , ui2 . Since {X
ft âˆ’ Âµt } is a martingale difference sequence (MDS)
We seek to bound inf uâˆˆA 1/(T âˆ’ p) t=p+1 hX
by Definition G.1 in Appendix G, we will appeal to the restricted eigenvalue condition for MDS that we present in Theorem
F.6 in Appendix F. We reproduce Theorem F.6 here for the convenience of the reader:

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Theorem E.1. Let X = (X1 , Â· Â· Â· , Xn )> be a n Ã— d design matrix whose anisotropic sub-Gaussian rows form a vector
valued martingale difference sequence. Let E[X> X/n] = Î£ and kÎ£âˆ’1/2 Xi kÏˆ2 â‰¤ Îº. Then for absolute constants
c0 , c1 , c2 > 0, with probability at least 1 âˆ’ 2 exp(âˆ’c20 c21 c2 Ï‰ 2 (B)), we have


1
2c0 c1 Îº2 Ï‰(B)
âˆš
â‰¤ inf kXuk22 ,
Î»min (Î£|A) 1 âˆ’
uâˆˆA n
n
where Î»min (Î£|A) = inf uâˆˆA u> Î£u is the restricted minimum eigenvalues of Î£ restricted to A âŠ† S dâˆ’1 (the unit sphere in
e = Î£1/2 u/kÎ£1/2 uk2 , u âˆˆ A} is the normalized set of A.
Rd space), and B = {e
u:u
ft âˆ’ Âµt for Âµt = E[X
ft |Ftâˆ’1 ], where Ft is the filtration Ft = Ïƒ(X
fp+1 , . . . , X
ft ), as
Proof of Lemma D.1. Let Zt = X
>
defined in Appendix D.1. Then let Z = [Zp+1 , . . . , ZT ] . Clearly the rows of Z form a vector-values MDS, and by
e in Section 3 implies that the rows or Z are anisotropic.
Assumption 5.4, they are sub-Gaussian as well. The definition of X
âˆ’1/2
>
Let E[Z Z/(T âˆ’ p)] = Î£Z be the true covariance matrix of Z, and kÎ£Z Zt kÏˆ2 â‰¤ Îº0 . Then by Theorem F.6 we have
that,


T
X
1
1
2c0 c1 Îº02 Ï‰(B)
2
2
f
âˆš
hXt âˆ’ Âµt , ui = inf kZuk2 â‰¥ Î»min (Î£Z |A) 1 âˆ’
inf
,
uâˆˆA n
uâˆˆA T âˆ’ p
T âˆ’p
t=p+1
with high probability. Thus, to prove Lemma D.1, it suffices to demonstrate that Î»min (Î£Z |A), the restricted minimum
eigenvalue of Î£Z , is positive. In this endeavor, we will draw upon the argument made by Johnson et al. (2016) in a similar
context.
e so that its rows fall
Let B2pd (x, ) be a L2 ball centered at x with radius . Clearly, we can scale the orignial design matrix X
ft âˆ’ Âµt },
in a L2 unit ball, in which case the rows of Z fall in a L2 unit ball centered at the origin. So then the set, A = {X
f
where Xt is drawn from the aforementioned L2 ball, is a subset of the L2 ball centered at the origin. Now note that by
definition Î»min (Î£Z |A) â‰¥ Î»min (Î£Z ), where Î»min (Î£Z ) is the unrestricted minimum eigenvalue of Î£Z . So it suffices to
show that Î»min (Î£Z ) > 0. By way of contradiction, assume that Î»min (Î£Z ) = 0. Then let the eigendecomposition of Î£Z
be Î£Z = QÎ›Qâˆ’1 , where Q = [v1 , . . . , vpd ] has the eigenvectors of Î£Z for columns, and Î› = diag(Î»i )pd
i=1 is a diagonal
matrix of the eigenvalues of Î£Z in descending order. Observe that Î»pd = 0 implies
Eaâˆ¼A [ha, vpd i] = 0,

(E.1)

since vpd is the eigenvector corresponding to the minimum eigenvalue of 0. Let Avpd = {a âˆˆ A|ha, vpd i = 0}. Then
clearly, (E.1) implies,
P(a âˆˆ Avpd ) = 1.

(E.2)

Note that by (E.2), since there is no probability density outside Avpd , this density is thus concentrated on a subspace of Rpd .
Here we have a contradiction, since the span of Avpd is Rpd . Therefore, Î»min (Î£Z |A) â‰¥ Î»min (Î£Z ) > 0, and we have


T
X
1
c1 Îº02 Ï‰(B)
ft âˆ’ Âµt , ui2 = inf 1 kZuk2 â‰¥ Î»min (Î£Z |A) 1 âˆ’ 2c0âˆš
hX
> 0,
2
uâˆˆA T âˆ’ p
uâˆˆA n
T âˆ’p
t=p+1
inf

with probability at least 1 âˆ’ 2 exp(âˆ’c20 c21 c2 Ï‰ 2 (B)), as desired.
E.2. Proof of Lemma D.2
In this section we present a proof for Lemma D.2. This proof will make a generic chaining argument, and will thus rely on
the following standard lemmas from Talagrand (2006) and Talagrand (2014).
Lemma E.2 (Theorem 2.1.5 from Talagrand (2006)). Consider two processes {Xt }tâˆˆA and {Yt }tâˆˆA , indexed by the same
set A. Assume {Xt }tâˆˆA is Gaussian, and {Yt }tâˆˆA satisfies the condition:


âˆ’ Î´2
âˆ€Î´ > 0, âˆ€u, v âˆˆ A, P(|Yu âˆ’ Yv | > Î´) â‰¤ 2 exp
,
d(u, v)2

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where d(u, v) is the distance metric associated with Xt (we assume d(u, v) = ku âˆ’ vk2 ). Then, for some constant L,
E[ sup |Yu âˆ’ Yv |] â‰¤ LE[sup Xv ]]
u,vâˆˆA

vâˆˆA

Lemma E.3 (Lemma 1.2.8 from Talagrand (2006)). If the process {Xt }tâˆˆA is symmetric, then
E[ sup |Xu âˆ’ Xv |] = 2E[sup Xu ]].
u,vâˆˆA

uâˆˆA

Lemma E.4 (Theorem 2.2.27 from Talagrand (2014)). Let {Xt }tâˆˆA be a process that satisfies Lemma E.2. Then for any
Î´ > 0 and constant L,


P sup |Xu âˆ’ Xv | â‰¥ L(Î³2 (A, d(u, v)) + Î´Î±) â‰¤ L exp(âˆ’Î´ 2 ),
u,vâˆˆA

where Î± = diam(A) = supu,vâˆˆA d(u, v), and Î³2 (Â·, Â·) is the Î³2 -functional defined in F.4.
Lemma E.5 (Theorem 2.2.27 from Talagrand (2014)). For constant L,
1
Î³2 (A, d(u, v)) â‰¤ E sup Xu â‰¤ LÎ³2 (A, d(u, v)).
L
uâˆˆA
Having presented these lemmas, we now give the proof of Lemma D.2.
2 PT
ft âˆ’ Âµt , ui. Recall that by Assumption 5.4, the subhX
T âˆ’ p t=p+1
ft âˆ’ Âµt kÏˆ â‰¤ Îº. Thus, X
ft is bounded by Îº, which implies that kX
ft âˆ’ Âµt forms a sub-Gaussian
Gaussian norm of each row X
2
bounded MDS, and so we can apply the Azuma-Hoeffding inequality to obtain
Proof of Lemma D.2. We seek to bound supuâˆˆA


 T





âˆ’ 2
1  X
f

hXt âˆ’ Âµt , ui â‰¥  â‰¤ 2 exp
,
P âˆš
2kuk22 Îº2
T âˆ’ p t=p+1

(E.3)

for any u âˆˆ A. Then for u, v âˆˆ A, we have that

 T





âˆ’ 2
1  X
f
 â‰¥  â‰¤ 2 exp
P âˆš
h
X
âˆ’
Âµ
,
u
âˆ’
vi
.
t
t

2ku âˆ’ vk22 Îº2
T âˆ’ p t=p+1

(E.4)

PT
âˆš
ft âˆ’ Âµt , ui with high probability.
We now make a generic chaining argument to bound supuâˆˆA 1/ T âˆ’ p t=p+1 hX
fp+1 âˆ’Âµp+1 ), . . . , (X
fT âˆ’ÂµT )]> . We first note that by (E.3), the process Zu = hZ, ui = 1/âˆšT âˆ’ phX
ft âˆ’Âµt , ui
Let Z = [(X
has sub-Gaussian concentration. Similarly, by (E.4), Zu âˆ’ Zv is a sub-Gaussian process âˆ€u, v âˆˆ A. We now bound
PT
âˆš
ft âˆ’ Âµt , ui] in terms of the Gaussian width of set A (see Definition F.5), and then prove
E[supuâˆˆA 1/ T âˆ’ p t=p+1 hX
PT
âˆš
ft âˆ’ Âµt , ui concentrates around its expectation with high probability.
hX
that supuâˆˆA 1/ T âˆ’ p
t=p+1

PT
âˆš
ft âˆ’ Âµt , ui], we appeal to Lemma E.2. In our setting, E[supvâˆˆA Xv ] = Ï‰(A),
To bound E[supuâˆˆA 1/ T âˆ’ p t=p+1 hX
the Gaussian width of A. Thus, by Lemma E.2 and (E.4), we achieve the following bound for some constant L:
E[ sup |Zu âˆ’ Zv |] â‰¤ LÎºÏ‰(A).

(E.5)

u,vâˆˆA

Note that by (E.3), the process Zu is symmetric, and so Lemma E.3 applies. Thus, by Lemma E.3 and (E.5), we have that,
E[ sup |Zu âˆ’ Zv |] = 2E[sup Zu ]] â‰¤ LÎºÏ‰(A).
u,vâˆˆA

uâˆˆA

(E.6)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

PT
âˆš
ft âˆ’ Âµt , ui] as follows:
Thus, the definition of Zu , we can bound E[supuâˆˆA 2/ T âˆ’ p t=p+1 hX

T
X
2
f
hXt âˆ’ Âµt , ui = 2E[sup Zu ]] â‰¤ LÎºÏ‰(A).
uâˆˆA
uâˆˆA T âˆ’ p t=p+1


2E sup

PT
âˆš
ft âˆ’ Âµt , ui] in terms of the Gaussian width of A, we now
Having bound the expectation of E[supuâˆˆA 1/ T âˆ’ p t=p+1 hX
PT
âˆš
f
seek to bound supuâˆˆA 1/ T âˆ’ p t=p+1 hXt âˆ’ Âµt , ui around its expectation with high-probability. To do so, we appeal
to lemma E.4. In this setting, d(u, v) = ku âˆ’ vk2 . Lemma E.4 motivates the following result:
P( sup |Zu âˆ’ Zv | â‰¥ L(Î³2 (A, d(u, v)) + Î´Î±) = P( sup |Zu âˆ’ Zv | â‰¥ L(Î³2 (A, d(u, v)) + ),
u,vâˆˆA

(E.7)

u,vâˆˆA

where the right-hand side of the inequality follows since, as Taylor et al. (2014) notes, Î³2 (A, d(u, v)) â‰¥ Î± from the
definition of the Î³2 functional. We now bound Î³2 (A, d(u, v)) with Lemma E.5. So by Lemma E.5 and (E.5),
P( sup |Zu âˆ’ Zv | â‰¥ L(Î³2 (A, d(u, v)) + ) â‰¤ P( sup |Zu âˆ’ Zv | â‰¥ E[ sup |Zu âˆ’ Zv |] + )
u,vâˆˆA

u,vâˆˆA

u,vâˆˆA

= P(sup |Zu | â‰¥ 2E[sup |Zu |] + ),
uâˆˆA

uâˆˆA

where the equality follows from Lemma E.3. Now substituting in the definition of Zu , and applying (E.6) and Lemma E.4,
we have:

 

 
T
X
 2
1
f
âˆš
hXt âˆ’ Âµt , ui â‰¥ 2LÎºÏ‰(A) +  â‰¤ L exp âˆ’
P sup
.
LÎºÎ±
T âˆ’ p t=p+1
uâˆˆA
Dividing through by
desired quantity

âˆš

T âˆ’ p, multiplying by 2, and letting  = âˆ’2LÎºÎ±Ï‰(A), we achieve the following bound on the



T
X
2
(Ï‰(A))2
f
hXt âˆ’ Âµt , ui â‰¥ 0 â‰¤ L exp âˆ’ 4
.
Î±2
uâˆˆA T âˆ’ p t=p+1


P sup

(E.8)

F. Restricted Eigenvalue Condition for Martingale Difference Sequences
In this section, we prove that under mild conditions, the restricted eigenvalue condition will hold for martingale difference
sequences (MDS), which we define in Definition G.1 in Appendix G. We first present the following definitions:
Definition F.1. An isotropic design matrix is one for which the covariance matrix of each row Î£ = E[Xi XiT ] = I.
Definition F.2. An anisotropic design matrix has rows with a general covariance matrix Î£ = E[Xi XiT ], but with corresponding isotropic rows X i = Xi Î£âˆ’1/2 .
Definition F.3. For a finite set A âŠ‚ T , denote the cardinality of A by |A|. An admissible sequence of T is a collection of
s
subsets of T, {Ts : s â‰¥ 0}, such that for every s â‰¥ 1, |Ts | = 22 and |T0 | = 1.
Definition F.4. (Talagrand, 2006) For a metric space (T, d) and k = 1, 2, define
Î³k (T, d) = inf sup

âˆž
X

2s/k d(t, Ts ),

tâˆˆT s=0

where d(t, Ts ) is the distance between the set Ts and t, and the infimum is taken with respect to all admissible sequences of
T . In cases where the metric is clear from the context, we will denote the Î³k functional by Î³k (T ).
Definition F.5. (Gordon, 1988; Chandrasekaran et al., 2012) The Gaussian width of a set A âˆˆ Rp is
Ï‰(A) = sup E[hg, ui],
uâˆˆA

where we take the expectation over random vector g âˆ¼ N (0, IpÃ—p ). The Gaussian width is a measure of the size of set A.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

We now present the restricted eigenvalue condition for MDS.
Theorem F.6. Let X = (X1 , Â· Â· Â· , Xn )> be a n Ã— d design matrix whose anisotropic sub-Gaussian rows form a vector
valued martingale difference sequence. Let E[X> X/n] = Î£ and kÎ£âˆ’1/2 Xi kÏˆ2 â‰¤ Îº. Then for absolute constants
c0 , c1 , c2 > 0, with probability at least 1 âˆ’ 2 exp(âˆ’c20 c21 c2 Ï‰ 2 (B)), we have


2c0 c1 Îº2 Ï‰(B)
1
âˆš
Î»min (Î£|A) 1 âˆ’
â‰¤ inf kXuk22 ,
uâˆˆA n
n
where Î»min (Î£|A) = inf uâˆˆA u> Î£u is the restricted minimum eigenvalues of Î£ restricted to A âŠ† S dâˆ’1 (the unit sphere in
e = Î£1/2 u/kÎ£1/2 uk2 , u âˆˆ A} is the normalized set of A.
Rd space), and B = {e
u:u
In the proof of this theorem, we will use the following lemma, which is a MDS version of the sub-Gaussian concentration in
Mendelson et al. (2007).
Lemma F.7. (Mendelson et al., 2007) Let (â„¦, Âµ) be a probability space, and F âŠ‚ SL2 be a set of functions, where
SL2 := {f : kf kL2 = 1} is the unit sphere in L2 (Âµ) space. Assume that diam(F, k Â· kÏˆ2 ) = Î±. Then, for any Î¸ > 0 and
n â‰¥ 1 satisfying
âˆš
c1 Î±Î³2 (F, k Â· kÏˆ2 ) â‰¤ Î¸ n,
we have with probability at least 1 âˆ’ exp(âˆ’c2 nÎ¸2 /Î±4 ) that
k
1 X



sup 
f 2 (Xi ) âˆ’ E[f 2 ] â‰¤ Î¸,
f âˆˆF n i=1

where c1 , c2 are absolute constants.
The detailed proof of Lemma F.7 can be found in Mendelson et al. (2007). We outline the proof of this lemma in Appendix
F.1.
Proof of Theorem F.6. We will apply Lemma F.7 to this proof. First, we define the following class of functions:


1
hÂ·, ui .
F = fu : u âˆˆ A, fu (Â·) = âˆš
u>Î£u
We need to verify that F âŠ‚ SL2 . In fact, for any fu âˆˆ F , we have
kfu (X)kL2 =

1
1
EX [hX, ui2 ] = >
EX [u> X> Xu] = 1.
u> Î£u
u Î£u

Note that
diam(F, k Â· kÏˆ2 ) =

sup kfu âˆ’ fv kÏˆ2 â‰¤ 2 sup kfu kÏˆ2 .

fu ,fv âˆˆF

fu âˆˆF

In order to bound the diameter of F according to k Â· kÏˆ2 , we only need to get a bound on the following term




1/2




1
 = sup  Î£âˆ’1/2 X, Î£ u
 .
âˆš
hX,
ui
sup kfu kÏˆ2 = sup 



1/2
kÎ£ uk2 Ïˆ2
uâˆˆA
uâˆˆA
fu âˆˆF
u> Î£u
Ïˆ2
Thus, we have supfu âˆˆF kfu kÏˆ2 â‰¤ kÎ£âˆ’1/2 X |Ïˆ2 â‰¤ Îº. By similar argument, we have






 Î£1/2 u
Î£1/2 v
Î£1/2 v 
Î£1/2 u
âˆ’1/2

 .


kfu âˆ’ fv kÏˆ2 =  Î£
âˆ’
â‰¤ Îº 1/2
âˆ’
X,
kÎ£1/2 uk2
kÎ£1/2 vk2 Ïˆ2
kÎ£ uk2
kÎ£1/2 vk2 2
By definition, we also have
kfu âˆ’ fv kL2


= E Î£âˆ’1/2 X,

Î£1/2 v
Î£1/2 u
âˆ’
kÎ£1/2 uk2
kÎ£1/2 vk2

2 



 Î£1/2 u
Î£1/2 v 

 .
=  1/2
âˆ’
kÎ£ uk2
kÎ£1/2 vk2 2

(F.1)

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

This equality immediately implies kfu âˆ’ fv kÏˆ2 â‰¤ Îºkfu âˆ’ fv kL2 . Then the Î³2 -functional in Lemma F.7 can be bounded as
Î³2 (F âˆ© SL2 , k Â· kÏˆ2 ) â‰¤ ÎºÎ³2 (F âˆ© SL2 , k Â· kL2 ) â‰¤ Îºc0 Ï‰(B),
e =
where the last inequality is due to the majorizing measure theorem in Talagrand (2006), B := {e
u : u
Î£1/2 u/kÎ£1/2 uk2 , u âˆˆ A} is the normalized set of A and c0 > 0 is an absolute constant. Now we choose the parameter Î¸
in Theorem F.7 as
Î¸=

2c0 c1 Îº2 Ï‰(B)
2c1 ÎºÎ³2 (F, k Â· kÏˆ2 )
âˆš
âˆš
â‰¥
.
n
n

Therefore, we have with probability at least 1 âˆ’ exp(âˆ’c20 c21 c2 Ï‰(B)2 /4) that


n
 2c0 c1 Îº2 Ï‰(B)
1 1 X
2
â‰¤

âˆš
sup 
hX
,
ui
âˆ’
1
,
i

>
n
uâˆˆA n u Î£u i=1
e = Î£1/2 u/kÎ£1/2 uk2 , u âˆˆ A}. It follows that
where c0 , c1 , c2 are absolute constants and B := {e
u:u


2c0 c1 Îº2 Ï‰(B)
1 1
2
âˆš
kXuk
â‰¤
,
sup 1 âˆ’
2
n u> Î£u
n
uâˆˆA
and
1âˆ’

2c0 c1 Îº2 Ï‰(B)
1 1
1
1
âˆš
â‰¤ inf
kXuk22 â‰¤
inf kXuk22 .
>
uâˆˆA n u Î£u
Î»min (Î£|A) uâˆˆA n
n

Thus we obtain that


1
2c0 c1 Îº2 Ï‰(B)
âˆš
â‰¤ inf kXuk22
Î»min (Î£|A) 1 âˆ’
uâˆˆA n
n
holds with probability at least 1 âˆ’ exp(âˆ’c20 c21 c2 Ï‰(B)2 /4), with c0 , c1 , c2 being absolute constants.
F.1. Sketch of Proof of Lemma F.7
Here we lay the outline of the proof for Lemma F.7, and show that it can be extended to bounded martingale difference
sequence. The only difference in the proof for our MDS version of this lemma from the independent case is the Bernstein
inequality. Whereas the original result leveraged the canonical Bernstein inequality, we here use the following MDS version
of the Bernstein inequality:
Lemma F.8 (Bernstein-Type Inequality for Martingale Difference Sequences). Let X1 , . . . , Xn form a sub-exponential
Martingale Difference Sequence (MDS) such that max1â‰¤iâ‰¤n kXi kÏˆ1 â‰¤ Îº. Here k Â· kÏˆ1 is the sub-Exponential norm defined
in Definition G.2 in Appendix G. Then

 X




 n

t2
t


P 
ai Xi  â‰¥ t â‰¤ 2 exp âˆ’ C min
,
,
Îº2 kak2 Îºkakâˆž
i

where C is a constant.
We defer the proof of Lemma F.8 to Appendix F.2. With the Bernstein inequality for MDS, we can now outline the proof of
Lemma F.7.
Let X1 , . . . , Xn be a bounded martingale difference sequence. We first define the empircal processes
Zf = 1/n

n
X
i=1

2

2

f (Xi ) âˆ’ E[f ]


Wf =

1/n

n
X

2

2

f (Xi )

i=1

The following lemma from Mendelson et al. (2007) can be easily obtained from Lemma F.8.

.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

Lemma F.9. There exists an absolute constant c1 > 0 for which the following holds. Let F âŠ‚ SL2 , Î± = diam(F, k Â· kÏˆ2 )
and set n â‰¥ 1. For every f, g âˆˆ F and every u â‰¥ 2 we have
P(Wf âˆ’g â‰¥ ukf âˆ’ gkÏˆ2 ) â‰¤ exp(âˆ’c1 nu2 ).
Also, for every u > 0,
P(|Zf âˆ’ Zg | â‰¥ uÎ±kf âˆ’ gkÏˆ2 ) â‰¤ exp(âˆ’c1 nu2 ),
and
P(|Zf | â‰¥ uÎ±2 ) â‰¤ 2 exp(âˆ’c1 nu2 ).
The following two lemmas from Mendelson et al. (2007) hold in our setting since they do not require i.i.d. observations.
Lemma F.10. There exists an absolute constant C for which the following holds. Let F âŠ‚ SL2 , Î± = diam(F, k Â· kÏˆ2 ) and
n â‰¥ 1. There is F 0 âŠ‚ F such that |F 0 | â‰¤ 4n and with probability at least 1 âˆ’ exp(âˆ’n), we have, for every f âˆˆ F ,
Wf âˆ’Ï€F 0 (f ) â‰¤

CÎ³2 (F, k Â· kÏˆ2 )
âˆš
,
n

where Ï€F 0 (f ) is a nearest point to f in F 0 with respect to the Ïˆ2 metric.
Lemma F.11. There exist absolute constants C and c0 > 0 for which the following holds. Let F âŠ‚ SL2 and Î± =
diam(F, k Â· kÏˆ2 ). Let n â‰¥ 1 and F 0 âŠ‚ F such that |F 0 | â‰¤ 4n . Then for every w > 0,
sup |Zf | â‰¤ CÎ±

f âˆˆF 0

Î³2 (F, k Â· kÏˆ2 )
âˆš
+ Î±2 w,
n

with probability at least 1 âˆ’ 3 exp(âˆ’c0 n min(w, w2 )).
Based on the above lemmas, the rest of proof of Theorem F.7 is stated as the proof of Theorem 1.4 in Mendelson et al.
(2007).
F.2. Proof of Berstein Inequality for MDS (Lemma F.8)
We first present the following lemma from Vershynin (2012).
Lemma F.12 (Lemma 5.15 from Vershynin (2012)). If X is a sub-exponential random variable such that E[X] = 0, then
for every t such that |t| â‰¤ c/kXkÏˆ1 , we have
E[exp(tX)] â‰¤ exp(Ct2 kXkÏˆ1 ],
where C, c > 0 are constants.
We now prove the Berstein Inequality for MDS.
Proof of Lemma F.8. We begin by bounding the moment-generating function of

 X

Y

n
n
E exp t Â·
ai Xi
=E
exp[tai Xi ]
i

Pn
i

ai Xi as follows:

i


 

nâˆ’1
Y


= E E exp[tan Xn ]
exp[tai Xi ]X1 , . . . , Xnâˆ’1
i



 
  nâˆ’1

Y


= E E exp[tan Xn ]X1 , . . . , Xnâˆ’1 Â· E
exp[tai Xi ]X1 , . . . , Xnâˆ’1
i



 nâˆ’1

Y

2 2 2
â‰¤ E exp(Ct an Îº ) Â· E
exp[tai Xi ]X1 , . . . , Xnâˆ’1 ,
i

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

where the second inequality holds by the Law of Iterated Expectations. Furthermore, the last inequality holds by Lemma
F.12 since E[Xn |X1 , . . . , Xnâˆ’1 ] = E[Xn |Fnâˆ’1 ] = 0 by the definition of a MDS (recall Definition G.1 in Appendix G). By
iteratively repeating this process, we obtain

 X

Y

n
n
2 2 2
E exp t Â·
ai Xi
â‰¤E
exp(Ct ai Îº )
i

i



n
X
2
2 2
= exp Ct
ai Îº
i



2
2 2
= exp Ct kak2 Îº .

(F.2)

Now note that by the Chernoff bound, for all Î» such that |Î»| â‰¤ C/kakâˆž we have
X


 X


n
n
P
ai Xi â‰¥ t = P exp Î»
ai Xi â‰¥ exp(Î»t)
i

i

Pn

E[exp[Î» i ai Xi ]]
exp[Î»t]
exp[CÎ»2 kak22 Îº2 ]
â‰¤
exp[Î»t]
â‰¤

= exp[CÎ»2 kak22 Îº2 âˆ’ Î»t],
where the last inequality holds by F.2. Now if we let Î» = min{t/(2Ckak22 Îº2 ), c/(kakâˆž Îº)}, then we see that if
t/(2Ckak22 Îº2 ) < c/(kakâˆž Îº), then


X



n
âˆ’ t2
t2
t2
=
exp
P
ai Xi â‰¥ t â‰¤ exp
âˆ’
.
(F.3)
4Ckak22 Îº2 2Îº2 Ckak22
4Ckak22 Îº2
i
Similarly, if c/(kakâˆž Îº) < t/(2Ckak22 Îº2 ), then





X
n
ct
ct
Cckak22 Îº
c
âˆ’
â‰¤ exp âˆ’
,
ai Xi â‰¥ t â‰¤ exp
Â·
P
kakâˆž
kakâˆž Îº kakâˆž Îº
2kakâˆž Îº
i

(F.4)

where the second inequality holds since Cckak22 Îº/kakâˆž â‰¤ t/2. Combining F.3 and F.4 yields



X

n
âˆ’ t2
âˆ’ ct
ai Xi â‰¥ t â‰¤ exp min
P
,
.
4Ckak22 Îº2 2kakâˆž Îº
i
Note that we can repeat this process and replace each Xi with âˆ’Xi to obtain this same bound for P(âˆ’
lemma then follows as a result of these two bounds.

Pn
i

ai Xi â‰¥ t). This

G. Auxiliary Definitions
In this section we present definitions used in the Appendix sections.
Definition G.1. A stochastic process {Î¾t } is a martingale difference sequence with respect to filtration Ft if:
1. Î¾t is Ft -measurable, and
2. E[Î¾t |Ftâˆ’1 ] = 0.
Definition G.2. The sub-Exponential norm of a random scalar variable X , kXkÏˆ1 , is:
kXkÏˆ1 = sup q
qâ‰¥1

âˆ’1

 
1/q
q
E |X|
.

Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference

The sub-Exponential norm of a random vector X âˆˆ Rn is:
kXkÏˆ1 = sup khX, uikÏˆ1 ,
uâˆˆS nâˆ’1

where S nâˆ’1 is the unit sphere in Rn space.

