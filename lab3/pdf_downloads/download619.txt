Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

The supplementary materials is organized as follows. In Appendices A, B and C, we provide detailed proofs of the theoretical
results in the paper. In Appendix D, we provide additional figures for the experiments described in Section 5.

A. Proof of Theorem 1
In this appendix we prove the minimax bound of Theorem 1. The result is obtained by combining the following two lower
bounds:
Theorem 3 (Lower bound 1). For each problem instance such that EÂµ [Ï2 Ïƒ 2 ] < âˆ, we have
h

i ï£¹2
p
2 2
2
2 Ïƒ 2 ]/2
E
Ï
Ïƒ
1
ÏÏƒ
>
R
nE
[Ï
Âµ
max
Âµ
EÂµ [Ï Ïƒ ] ï£°
ï£» .
Rn (Ï€; Î», Âµ, Ïƒ, Rmax ) â‰¥
1âˆ’
2
2
32en
EÂµ [Ï Ïƒ ]
2 2

ï£®

2
Theorem 4 (Lower bound 2). Assume that EÂµ [Ï2 Rmax
] < âˆ, and we are given Î³ âˆˆ [0, 1] and Î´ âˆˆ (0, 1]. Write Î¾ := Î¾Î³
0 :=
and Î³
max{Î³, Î´}. Then there exist functions RÌ‚(x, a) and ÏÌ‚(x, a) such that
2
RÌ‚2 (x, a) â‰¤ Rmax
(x, a) â‰¤ (1 + Î´)RÌ‚2 (x, a) ,

ÏÌ‚2 (x, a) â‰¤ Ï2 (x, a) â‰¤ (1 + Î´)ÏÌ‚2 (x, a)

and the following lower bound holds:
Rn (Ï€; Î», Âµ, Ïƒ, Rmax )

q

 ï£¹2
2 2
2
2
EÂµ Î¾ ÏÌ‚ RÌ‚ 1 Î¾ ÏÌ‚RÌ‚ > nEÂµ [Î¾ ÏÌ‚ RÌ‚ ]/16 ï£º

EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ] ï£¯
ï£º âˆ’ Î³ 0 log 5/Î³ 0 (1 + Î´)EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ] .
ï£¯1 âˆ’
â‰¥
ï£»
ï£°
32en
EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]
ï£®

The reason for introducing Î³ 0 in Theorem 4 is to allow Î³ = 0, which is an important special case of the theorem. Otherwise,
we could just assume 0 < Î´ â‰¤ Î³. The first bound captures the intrinsic difficulty due to the variance of reward, and is
present even in a vanilla multi-armed bandit problem without contexts. The second result shows the additional dependence
2
on Rmax
, even when Ïƒ â‰¡ 0, whenever the distribution Î» is not too degenerate, and captures the additional difficulty of the
contextual bandit problem. We next show how these two lower bounds yield Theorem 1 and then return to their proofs.
Proof of Theorem 1. Throughout the theorem we write Î¾ := Î¾Î³ . We begin by simplifying the two lower bounds. Assume
that Assumption 1 holds with . This also means that EÂµ [Î¾(ÏRmax )2+ ] is finite as well as EÂµ [Î¾(ÏRmax )2 ] is finite and
either both of them are zero or both of them are non-zero. Similarly, EÂµ [(ÏÏƒ)2+ ] and EÂµ [(ÏÏƒ)2 ] are both finite and either
both of them are zero or both of them are non-zero, so CÎ³ is a finite constant. Let p = 1 + /2 and q = 1 + 2/, i.e.,
1/p + 1/q = 1. Further, let RÌ‚ and ÏÌ‚ be the functions from Theorem 4. Then the definition of CÎ³ means that
CÎ³1/(q) = CÎ³1/(2+)

ï£±
ï£¼
2
2
 2+
 2 2 2+  2+
ï£² E Î¾(Ï2 R2 ) 2+
ï£½
2
2
EÂµ (Ï Ïƒ )
Âµ
 max



= 2 Â· max
,
2
2
2
2
ï£¾
ï£³
EÂµ Î¾Ï Rmax
EÂµ Ï Ïƒ
( 
1/p

1/p )
2
EÂµ Î¾(Ï2 Rmax
)p
EÂµ (Ï2 Ïƒ 2 )p

 ,


= 2 Â· max
2
EÂµ Î¾Ï2 Rmax
EÂµ Ï2 Ïƒ 2
( 
1/p

1/p )
EÂµ Î¾(ÏÌ‚2 RÌ‚2 )p
EÂµ (Ï2 Ïƒ 2 )p



 ,
,
â‰¥ 2 Â· max
2
EÂµ Î¾Ï2 Rmax
EÂµ Ï2 Ïƒ 2

(10)

and recall that we assume that
n
o
2
n â‰¥ max 16CÎ³1/ , 2CÎ³2/ EÂµ [Ïƒ 2 /Rmax
] .

(11)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

First, we simplify the correction term in the lower bound of Theorem 3. Using HÃ¶lderâ€™s inequality and Eq. (10), we have



q
EÂµ Ï2 Ïƒ 2 1 ÏÏƒ 2 > Rmax nEÂµ [Ï2 Ïƒ 2 ]/2
q
i1/q
h
h
p i1/p
â‰¤ EÂµ Ï2 Ïƒ 2
Â· PÂµ ÏÏƒ 2 > Rmax nEÂµ [Ï2 Ïƒ 2 ]/2
q
i1/q
h
1
.
â‰¤ EÂµ [Ï2 Ïƒ 2 ] Â· CÎ³1/(q) Â· PÂµ ÏÏƒ 2 /Rmax > nEÂµ [Ï2 Ïƒ 2 ]/2
2
We further invoke Markovâ€™s inequality, Cauchy-Schwartz inequality, and Eq. (11) in the following three steps to simplify
this event as

 !1/q
EÂµ ÏÏƒ Â· (Ïƒ/Rmax )
p
nEÂµ [Ï2 Ïƒ 2 ]/2
!1/q
p
p
2
EÂµ [Ï2 Ïƒ 2 ] Â· EÂµ [Ïƒ 2 /Rmax
]
1
2 2
1/(q)
p
â‰¤ EÂµ [Ï Ïƒ ] Â· CÎ³
Â·
2
nEÂµ [Ï2 Ïƒ 2 ]/2

1/2q
2
]
2EÂµ [Ïƒ 2 /Rmax
1
1
â‰¤ EÂµ [Ï2 Ïƒ 2 ] .
= EÂµ [Ï2 Ïƒ 2 ] Â· CÎ³2/ Â·
2
n
2
1
â‰¤ EÂµ [Ï2 Ïƒ 2 ] Â· CÎ³1/(q) Â·
2

(12)

For the correction term in Theorem 4, we similarly have



q
EÂµ Î¾ ÏÌ‚2 RÌ‚2 1 Î¾ ÏÌ‚RÌ‚ > nEÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]/16
q
i1/q
h
h
p i1/p
â‰¤ EÂµ Î¾ ÏÌ‚2 RÌ‚2
Â· PÂµ Î¾ ÏÌ‚RÌ‚ > nEÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]/16
h
i1/q
1
2
] Â· CÎ³1/(q) Â· PÂµ Î¾ ÏÌ‚2 RÌ‚2 > nEÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]/16
â‰¤ EÂµ [Î¾Ï2 Rmax
,
2
so that Markovâ€™s inequality and Eq. (11) further yield
!1/q
EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]
1
2 2
1/(q)
Â·
â‰¤ EÂµ [Î¾Ï Rmax ] Â· CÎ³
2
nEÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]/16

1/q
1
1
(1 + Î´)2
2 2
1/ 16
2
= EÂµ [Î¾Ï Rmax ] Â· CÎ³ Â·
â‰¤ EÂµ [Î¾Ï2 Rmax
]â‰¤
EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ] .
2
n
2
2

(13)

Using Eq. (12), the bound of Theorem 3 simplifies as
Rn (Ï€; Î», Âµ, Ïƒ, Rmax )
h

i ï£¹2
ï£®
p
EÂµ Ï2 Ïƒ 2 1 ÏÏƒ 2 > Rmax nEÂµ [Ï2 Ïƒ 2 ]/2
EÂµ [Ï2 Ïƒ 2 ] ï£°
ï£»
â‰¥
1âˆ’
32en
EÂµ [Ï2 Ïƒ 2 ]
EÂµ [Ï2 Ïƒ 2 ]
â‰¥
32en



1
1âˆ’
2

2
=

EÂµ [Ï2 Ïƒ 2 ]
.
128en

(14)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Similarly, by Eq. (13), Theorem 4 simplifies as
Rn (Ï€; Î», Âµ, Ïƒ, Rmax )
ï£®


q

 ï£¹2
2 2
2 RÌ‚2 ]/16
E
Î¾
ÏÌ‚
RÌ‚
1
Î¾
ÏÌ‚
RÌ‚
>
nE
[Î¾
ÏÌ‚
Âµ
Âµ
ï£º
EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ] ï£¯
ï£¯1 âˆ’
ï£º âˆ’ Î³ 0 log(5/Î³ 0 )(1 + Î´)EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]
â‰¥
ï£»
32en ï£°
EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]


2
EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]
(1 + Î´)2
âˆ’ Î³ 0 log(5/Î³ 0 )(1 + Î´)EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]
1âˆ’
32en
2
2
EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]
=
1 âˆ’ 2Î´ âˆ’ Î´ 2 âˆ’ Î³ 0 log(5/Î³ 0 )(1 + Î´)EÂµ [Î¾ ÏÌ‚2 RÌ‚2 ]
128en
2
2
EÂµ [Î¾Ï2 Rmax
] 1 âˆ’ 2Î´ âˆ’ Î´ 2
2
âˆ’ Î³ 0 log(5/Î³ 0 )(1 + Î´)EÂµ [Î¾Ï2 Rmax
] .
â‰¥
128en
(1 + Î´)2
â‰¥

Since this bound is valid for all Î´ > 0, taking Î´ â†’ 0, we obtain
Rn (Ï€; Î», Âµ, Ïƒ, Rmax ) â‰¥

2
]
EÂµ [Î¾Ï2 Rmax
2
âˆ’ Î³ log(5/Î³)EÂµ [Î¾Ï2 Rmax
] .
128en

Combining this bound with Eq. (14) yields
Rn (Ï€; Î», Âµ, Ïƒ, Rmax )
â‰¥

2
] 1
1 EÂµ [Ï2 Ïƒ 2 ] 1 EÂµ [Î¾Ï2 Rmax
2
Â·
+ Â·
âˆ’ Â· Î³ log(5/Î³)EÂµ [Î¾Ï2 Rmax
]
2
128en
2
128en
2

2
EÂµ [Ï2 Ïƒ 2 ] EÂµ [Î¾Ï2 Rmax
] 1
2
+
âˆ’ Â· Î³ log(5/Î³)EÂµ [Î¾Ï2 Rmax
]
700n
700n
2

i
1 h
2
=
EÂµ [Ï2 Ïƒ 2 ] + EÂµ [Î¾Ï2 Rmax
] 1 âˆ’ 350nÎ³ log(5/Î³)
.
700n

â‰¥

It remains to prove Theorems 3 and 4. They are both proved by a reduction to hypothesis testing, and invoke Le Camâ€™s
argument to lower-bound the error in this testing problem. As in most arguments of this nature, the key contribution lies in
the construction of an appropriate testing problem that leads to the desired lower bounds. Before proving the theorems, we
recall the basic result of Le Cam which underlies our proofs. We point the reader to the excellent exposition of Lafferty et al.
(2008, Section 36.4) on more details about Le Camâ€™s argument.
Theorem 5 (Le Camâ€™s method, Lafferty et al., 2008, Theorem 36.8). Let P be a set of distributions, let X1 , . . . , Xn be an
i.i.d. sample from some P âˆˆ P, let Î¸(P ) be any function of P âˆˆ P, let Î¸Ì‚(X1 , . . . , Xn ) be an estimator, and d be a metric.
For any pair P0 , P1 âˆˆ P,
âˆ†
inf sup EP [d(Î¸Ì‚, Î¸(P ))] â‰¥ eâˆ’nDKL (P0 kP1 )
(15)
8
Î¸Ì‚ P âˆˆP
R
where âˆ† = d(Î¸(P0 ), Î¸(P1 )), and DKL (P0 kP1 ) = log(dP0 /dP1 )dP0 is the KL-divergence.
While the proofs of the two theorems share a lot of similarities, they have to use reductions to slightly different testing
problems given the different mean and variance constraints in the two results. We begin with the proof of Theorem 3, which
has a simpler construction.
A.1. Proof of Theorem 3
The basic idea of this proof is to reduce the problem of policy evaluation to that of Gaussian mean estimation where there is
a mean associated with each x, a pair. We now describe our construction.
Creating a family of problems Since we aim to show a lower bound on the hardness of policy evaluation in general,
it suffices to show a particular family of hard problem instances, such that every estimator requires the stated number of

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

samples on at least one of the problems in this family. Recall that our minimax setup assumes that Ï€, Âµ and Î» are fixed
and the only aspect of the problem which we can design is the conditional reward distribution D(r | x, a). For Theorem 3,
this choice is further constrained to satisfy E[r | x, a] â‰¤ Rmax (x, a) and Var(r | x, a) â‰¤ Ïƒ 2 (x, a). In order to describe our
construction, it will be convenient to define the shorthand E[r | x, a] = Î·(x, a). We will identify a problem in our family
with the function Î·(x, a) as that will be the only changing element in our problems. For a chosen Î·, the policy evaluation
question boils down to estimating vÎ·Ï€ = E[r(x, a)], where the contexts x are chosen according to Î», actions are drawn from
Ï€(x, a) and the reward distribution DÎ· (r | x, a) is a normal distribution with mean Î·(x, a) and variance Ïƒ 2 (x, a)
DÎ· (r | x, a) = N (Î·(x, a), Ïƒ 2 (x, a)).
Clearly this choice meets the variance constraint by construction, and satisfies the upper bound so long as Î·(x, a) â‰¤
Rmax (x, a) almost surely. Since the evaluation policy Ï€ is fixed throughout, we will drop the superscript and use vÎ· to
denote vÎ·Ï€ in the remainder of the proofs. With some abuse of notation, we also use EÎ· [Â·] to denote expectations where
contexts and actions are drawn based on the fixed choices Î» and Âµ corresponding to our data generating distribution, and the
rewards drawn from Î·. We further use PÎ· to denote this entire joint distribution over (x, a, r) triples.
Given this family of problem instances, it is easy to see that for any pair of Î·1 , Î·2 which are both pointwise upper bounded
by Rmax , we have the lower bound:
i
h
Rn (Î», Ï€, Âµ, Ïƒ 2 , Rmax ) â‰¥ inf max EÎ· (vÌ‚ âˆ’ vÎ· )2 ,
vÌ‚ Î·âˆˆÎ·1 ,Î·2
| {z }
`Î· (vÌ‚)
where we have introduced the shorthand `Î· (vÌ‚) to denote the squared error of vÌ‚ to vÎ· . For a parameter  > 0 to be chosen
later, we can further lower bound this risk for a fixed vÌ‚ as

Rn (vÌ‚) â‰¥ max EÎ· [`Î· (vÌ‚)] â‰¥ max PÎ· (`Î· â‰¥ )
Î·âˆˆÎ·1 ,Î·2
Î·âˆˆÎ·1 ,Î·2
i
h
â‰¥ PÎ·1 (`Î·1 (vÌ‚) â‰¥ ) + PÎ·2 (`Î·2 (vÌ‚) â‰¥ ) ,
2

(16)

where the last inequality lower bounds the maximum by the average. So far we have been working with an estimation
problem. We next describe how to reduce this to a hypothesis testing problem.
Reduction to hypothesis testing For turning our estimation problem into a testing problem, the idea is to identify a pair
Î·1 , Î·2 such that they are far enough from each other so that any estimator which gets a small estimation loss can essentially
identify whether the data generating distribution corresponds to PÎ·1 or PÎ·2 . In order to do this, we take any estimator vÌ‚
and identify a corresponding test statistic which maps vÌ‚ into one of Î·1 , Î·2 . The way to do this is essentially identified in
Eq. (16), and we describe it next.
Note that since we are constructing a hypothesis test for a specific pair of distributions PÎ·1 and PÎ·2 , it is reasonable to
consider test statistics which have knowledge of Î·1 and Î·2 , and hence the corresponding distributions. Consequently, these
tests also know the true policy values vÎ·1 and vÎ·2 and the only uncertainty is which of them gave rise to the observed data
samples. Therefore, for any estimator vÌ‚, we can a associate a statistic Ï†(vÌ‚) = argminÎ· {`Î·1 (vÌ‚), `Î·2 (vÌ‚)}.
Given this hypothesis test, we are interested in its error rate PÎ· (Ï†(vÌ‚) 6= Î·). We first relate the estimation error of vÌ‚ to the
error rate of the test. Suppose for now that
`Î·1 (vÌ‚) + `Î·1 (vÌ‚) â‰¥ 2,
(17)
so that at least one of the losses is at least . Suppose that the data comes from Î·1 . Then if `Î·1 (vÌ‚) < , we know that the test
is correct, because by Eq. (17) the other loss is greater than , and therefore Ï†(vÌ‚) = Î·1 . This means that the error under Î·1
can only occur if `Î·1 (vÌ‚) â‰¥ . Similarly, the error under Î·2 can only occur if `Î·2 (vÌ‚) â‰¥ , so the test error can be bounded as
max PÎ· (Ï†(vÌ‚) 6= Î·) â‰¤ PÎ·1 (Ï†(vÌ‚) 6= Î·1 ) + PÎ·2 (Ï†(vÌ‚) 6= Î·2 )

Î·âˆˆÎ·1 ,Î·2

â‰¤ PÎ·1 (`Î·1 (vÌ‚) â‰¥ ) + PÎ·2 (`Î·2 (vÌ‚) â‰¥ )
2
â‰¤ Rn (vÌ‚),


(18)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

where the final inequality uses our earlier lower bound in Eq. (16).
To finish connecting our the estimation problem to testing, it remains to establish our earlier supposition (17). Assume for
now that Î·1 and Î·2 are chosen such that
2
(vÎ·1 âˆ’ vÎ·2 ) â‰¥ 4.
(19)
Then an application of the inequality (a + b)2 â‰¤ 2a2 + 2b2 yields
2

4 â‰¤ (vÎ·1 âˆ’ vÎ·2 ) â‰¤ 2(vÌ‚ âˆ’ vÎ·1 )2 + 2(vÌ‚ âˆ’ vÎ·2 )2 = 2`Î·1 (vÌ‚) + 2`Î·2 (vÌ‚),
which yields the posited bound (16).
Invoking Le Camâ€™s argument So far we have identified a hypothesis testing problem and a test statistic whose error is
upper bounded in terms of the minimax risk of our problem. In order to complete the proof, we now place a lower bound on
the error of this test statistic. Recall the result of Le Cam (15), which places an upper bound on the attainable error in any
testing problem. In our setting, this translates to
1 âˆ’nDKL (PÎ· || PÎ· )
1
2 .
e
8
Since the distribution of the rewards is a spherical Gaussian, the KL-divergence is given by the squared distance between the
means, scaled by the variance, that is
max PÎ· (Ï†(vÌ‚) 6= Î·) â‰¥

Î·âˆˆÎ·1 ,Î·2


DKL (PÎ·1 || PÎ·2 ) = E


(Î·1 (x, a) âˆ’ Î·2 (x, a))2
,
2Ïƒ 2 (x, a)

where we recall that the contexts and actions are drawn from Î» and Âµ respectively. Since we would like the probability of
error in the test to be a constant, it suffices to choose Î·1 and Î·2 such that


(Î·1 (x, a) âˆ’ Î·2 (x, a))2
1
E
â‰¤ .
(20)
2Ïƒ 2 (x, a)
n
Picking the parameters So far, we have not made any concrete choices for Î·1 and Î·2 , apart from some constraints which
we have introduced along the way. Note that we have the constraints (19) and (20) which try to ensure that Î·1 and Î·2 are not
too close that an estimator does not have to identify the true parameter, or too far that the testing problem becomes trivial.
Additionally, we have the upper and lower bounds of 0 and Rmax on Î·1 and Î·2 . In order to reason about these constraints, it
is convenient to set Î·2 â‰¡ 0, and pick Î·1 (x, a) = Î·1 (x, a) âˆ’ Î·2 (x, a) = âˆ†(x, a). We now write all our constraints in terms
of âˆ†.
Note that vÎ·2 is now 0, so that the first constraint (19) is equivalent to

âˆš
vÎ·1 = EÎ·1 [Ï(x, a)r(x, a)] = Eâˆ† [Ï(x, a)r(x, a)] â‰¥ 2 ,

where the importance weighting function Ï is introduced since PÎ·1 is based on choosing actions according to Âµ and we seek
to evaluate Ï€. The second constraint (20) is also straightforward
 2
âˆ†
1
E
â‰¤ .
2Ïƒ 2
n
Finally, the bound Rmax and non-negativity of Î·1 and Î·2 are enforced by requiring 0 â‰¤ âˆ†(x, a) â‰¤ Rmax (x, a) almost
surely.
The minimax lower bound is then obtained by the largest  in the constraint (19) such that the other two constraints can be
satisfied. This gives rise to the following variational characterization of the minimax lower bound:
max
âˆ†

such that


âˆš
Eâˆ† [Ï(x, a)r(x, a)] â‰¥ 2 ,
 2
âˆ†
1
E
â‰¤ ,
2
2Ïƒ
n
0 â‰¤ âˆ†(x, a) â‰¤ Rmax (x, a).

(21)
(22)
(23)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Instead of finding the optimal solution, we just exhibit a feasible setting of âˆ† here. We set
r


Î±Ïƒ 2 Ï
2EÂµ [Ï2 Ïƒ 2 ]
âˆ† = min
, Rmax , where Î± =
.
2
2
EÂµ [Ï Ïƒ ]
n

(24)

This setting satisfies the bounds (23) by construction. A quick substitution also verifies that the constraint (22) is satisfied.
Consequently, it suffices to set  to the value attained in the constraint (21). Substituting the value of âˆ† in the constraint, we
see that
Eâˆ† [Ï(x, a)r(x, a)] = Exâˆ¼Î»,aâˆ¼Âµ [Ï(x, a)âˆ†(x, a)]


Î±Ïƒ 2 Ï  2
2 2
1 ÏÏƒ Î± â‰¤ Rmax EÂµ [Ï Ïƒ ]
â‰¥ Exâˆ¼Î»,aâˆ¼Âµ Ï
EÂµ [Ï2 Ïƒ 2 ]

 !
EÂµ Ï2 Ïƒ 2 1 ÏÏƒ 2 Î± > Rmax EÂµ [Ï2 Ïƒ 2 ]
=Î± 1âˆ’
EÂµ [Ï2 Ïƒ 2 ]
âˆš
=: 2 .
Putting all the foregoing bounds together, we obtain that for all estimators vÌ‚

 
Rn (vÌ‚) â‰¥ Â· max PÎ· (Ï†(vÌ‚) 6= Î·)
2 Î·âˆˆÎ·1 ,Î·2
 1
â‰¥ Â· eâˆ’nDKL (PÎ·1 || PÎ·2 )
2 8

 1
=
â‰¥ Â·
2 8e
16e

 !2
2
EÂµ Ï2 Ïƒ 2 1 ÏÏƒ 2 > Rmax EÂµ [Ï2 Ïƒ 2 ]/Î±
1 Î±
=
Â·
1âˆ’
16e 4
EÂµ [Ï2 Ïƒ 2 ]
h
ï£«
p
i ï£¶2
2 2
2
2 Ïƒ 2 ]/2
E
nE
[Ï
Ï
Ïƒ
1
ÏÏƒ
>
R
2 2
Âµ
Âµ
max
EÂµ [Ï Ïƒ ] ï£­
ï£¸ .
=
1âˆ’
32en
EÂµ [Ï2 Ïƒ 2 ]
A.2. Proof of Theorem 4
We now give the proof of Theorem 4. While it shares a lot of reasoning with the proof of Theorem 3, it has one crucial
difference. In Theorem 3, there is a non-trivial noise in the reward function, unlike in Theorem 4. This allowed the proof to
work with just two candidate mean-reward functions, since any realization in the data is corrupted with noise. However, in
the absence of added noise, the task of mean identification becomes rather trivial: an estimator can just check whether Î·1 or
Î·2 matches the observations exactly.
To prevent such a strategy, we instead construct a richer family of reward functions. Instead of merely two mean rewards,
our construction will involve a randomized design of the expected reward function from an appropriate prior distribution.
The draw of the mean reward from a prior will essentially generate noise even though any given problem is noiseless. The
construction will also highlight the crucial sources of difference between the contextual and multi-armed bandit problems,
since the arguments here rely on having access to a rich context distribution, by which we mean distribution that puts
non-trivial probability on many contexts. In the absence of this property, the bound of Theorem 4 becomes weaker.
Creating a family of problems Our family of problems will be parametrized by the two reals Î´ and Î³ from the statement
of the theorem. Our construction begins with a discretization step at the resolution Î´, whose goal is to create a countable
partition of the set of pairs X Ã— A. If sets X and A are countable or finite, this step is vacuous, but if the sets of contexts or
actions have continuous parts, this step is required.
First, let Âµ(x, a) denote the joint probability measure obtained by firstUdrawing x âˆ¼ Î» and then a âˆ¼ Âµ(Â· | x). In Lemma 1, we
show that X Ã— A can be split into countably many disjoint sets Bi , iâˆˆI Bi = X Ã— A, such that the following conditions
are satisfied:

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

â€¢ Each i âˆˆ I is associated with numbers Ri â‰¥ 0, Ïi â‰¥ 0 and Î¾i âˆˆ {0, 1} such that
2
Rmax
(x, a) âˆˆ [Ri2 , (1 + Î´)Ri2 ] ,

Ï2 (x, a) âˆˆ [Ï2i , (1 + Î´)Ï2i ] ,

Î¾Î³ (x, a) = Î¾i

for all (x, a) âˆˆ Bi .

â€¢ Each Bi either satisfies Âµ(Bi ) â‰¤ Î´ or consists of a single pair (xi , ai ).
The numbers Ri and Ïi will be exactly RÌ‚(x, a) and ÏÌ‚(x, a) from the theorem statement.
As before, we parametrize the family of reward distributions in terms of the mean reward function Î·(x, a). However, now
Î·(x, a) is itself a random variable, which is drawn from a prior distribution. The reward function Î·(x, a) will be constant on
each Bi , and its value on Bi , written as Î·(i), will be drawn from a scaled Bernoulli, parametrized by a prior function Î¸(i) as
follows:

Î¾i Ri
with probability Î¸(i),
Î·(i) =
(25)
0
with probability 1 âˆ’ Î¸(i).
We now set DÎ· (r | x, a) = Î·(i) whenever (x, a) âˆˆ Bi . This clearly satisfies the constraints on the mean since 0 â‰¤
E[r | x, a] â‰¤ Ri â‰¤ Rmax (x, a) from the property of the partition, and also Var(r | x, a) = 0 as per the setting of Theorem 4.
The goal of an estimator is to take n samples generated by drawing x âˆ¼ Î», a | x âˆ¼ Âµ and r | x, a âˆ¼ DÎ· , and output an
estimate vÌ‚ such that EÎ· [(vÌ‚ âˆ’ vÎ·Ï€ )2 ] is small. We recall our earlier shorthand vÎ· to denote the value of Ï€ under the reward
distribution generated by Î·. For showing a lower bound on this quantity, it is clearly
 sufficient to pick
 any prior distribution
governed by a parameter Î¸, as in Eq. (25), and lower bound the expectation EÎ¸ EÎ· [(vÌ‚ âˆ’ vÎ· )2 | Î·] . If this expectation is
large for some estimator vÌ‚, then there must be some realization Î·, which induces a large error least one function Î·(x, a)
which induces a large error EÎ· [(vÌ‚ âˆ’ vÎ· )2 | Î·], as desired. Consequently, we focus in the proof on lower bounding the
expectation EÎ¸ [Â·]. This expectation can be decomposed with the use of the inequality a2 â‰¥ (a + b)2 /2 âˆ’ b2 as follows:
i
h
i
h
i 1 h
EÎ¸ EÎ· [(vÌ‚ âˆ’ vÎ· )2 | Î·] â‰¥ EÎ¸ EÎ· [(vÌ‚ âˆ’ EÎ¸ [vÎ· ])2 | Î·] âˆ’ EÎ¸ (vÎ· âˆ’ EÎ¸ [vÎ· ])2 .
2
Taking the worst case over all problems in the above inequality, we obtain
h
i
sup EÎ· [(vÌ‚ âˆ’ vÎ· )2 ] â‰¥ sup EÎ¸ EÎ· [(vÌ‚ âˆ’ vÎ· )2 | Î·]
Î·

Î¸

i
h
i
1 h
â‰¥ sup EÎ¸ EÎ· [(vÌ‚ âˆ’ EÎ¸ [vÎ· ])2 | Î·] âˆ’ sup EÎ¸ (vÎ· âˆ’ EÎ¸ [vÎ· ])2 .
Î¸ 2
Î¸
{z
} |
{z
}
|
T1

(26)

T2

This decomposition says that the expected MSE of an estimator in estimating vÎ· can be related to the MSE of the same
estimator in estimating the quantity EÎ¸ [vÎ· ], as long as the variance of the quantity vÎ· under the distribution generated by Î¸ is
not too large. This is a very important observation, since we can now choose to instead study the MSE of an estimator in
estimating EÎ¸ [vÎ· ] as captured by T1 . Unlike the distribution DÎ· which is degenerate, this problem has a non-trivial noise
arising from the randomized draw of Î· according to Î¸. Thus we can use similar techniques as the proof of Theorem 3, albeit
where the reward distribution is a scaled Bernoulli instead of Gaussian. For now, we focus on controlling T1 , and T2 will be
handled later.
In order to bound T1 , we will consider two carefully designed choices Î¸1 and Î¸2 to induce two different problem instances
and show that T1 is large for any estimator under one of the two parameters. In doing this, it will be convenient to use the
additional shorthand `Î¸ (vÌ‚) = (vÌ‚ âˆ’ EÎ¸ [vÎ· ])2 . Proceeding as in the proof of Theorem 3, we have
h
i 1
h
i
1
sup EÎ¸ EÎ· [(vÌ‚ âˆ’ EÎ¸ [vÎ· ])2 | Î·] = sup EÎ¸ EÎ· [`Î¸ (vÌ‚) | Î·]
2 Î¸
2 Î¸


â‰¥ sup PÎ¸ (`Î¸ (vÌ‚) â‰¥ ) â‰¥
max PÎ¸ (`Î¸ (vÌ‚) â‰¥ )
2 Î¸
2 Î¸âˆˆÎ¸1 ,Î¸2
h
i

â‰¥ PÎ¸1 (`Î¸1 (vÌ‚) â‰¥ ) + PÎ¸2 (`Î¸2 (vÌ‚) â‰¥ ) .
4

T1 =

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Reduction to hypothesis testing As in the proof of Theorem 3, we now reduce the estimation problem into a hypothesis
test for whether the data is generated according to the parameter Î¸1 or Î¸2 . The arguments here are similar to the earlier proof,
so we will be terser in this presentation.
As before, our hypothesis test has entire knowledge of DÎ· as well as Î¸1 and Î¸2 . Consequently,
âˆšwe construct a test based on
picking Î¸1 whenever `Î¸1 (vÌ‚) â‰¤ `Î¸2 (vÌ‚). As before, we will ensure that |EÎ¸1 [vÎ· ] âˆ’ EÎ¸2 [vÎ· ]| â‰¥ 2  so that for any estimator vÌ‚,
we have
`Î¸1 (vÌ‚) + `Î¸2 (vÌ‚) â‰¥ 2.
Under this assumption, we can similarly conclude that the error of our hypothesis test is at most
PÎ¸1 (`Î¸1 (vÌ‚) â‰¥ ) + PÎ¸2 (`Î¸2 (vÌ‚) â‰¥ ) .
Invoking Le Camâ€™s argument Once again, we can lower bound the error rate of our test by invoking the result of Le
Cam. This requires an upper bound on the KL-divergence DKL (PÎ¸1 kPÎ¸2 ). The only difference from our earlier argument is
that these distributions are now Bernoulli instead of Gaussian, based on the construction in Eq. (25). More formally, we have


X
X
p(r; Î¸1 (i))
DKL (PÎ¸1 kPÎ¸2 ) =
log
p(r; Î¸1 (i))Âµ(Bi )
p(r; Î¸2 (i))
iâˆˆI râˆˆ{0,xii Ri }
h

i
= EÂµ Î¾i DKL Ber(Î¸1 (i))  Ber(Î¸2 (i)) ,
(27)
where i is treated as a random variable under Âµ, and Î¾i is included, because the two distributions assign r = 0 with
probability one if Î¾i = 0.
Picking the parameters It remains to carefully choose Î¸1 and Î¸2 . We define Î¸2 (i) â‰¡ 0.5, and let Î¸1 (i) = Î¸2 (i) + âˆ†i ,
where âˆ†i will be chosen to satisfy certain constraints as before. Then, by Lemma 3, the KL divergence in Eq. (27) can be
bounded as

1 
DKL (PÎ¸1 kPÎ¸2 ) â‰¤ EÂµ Î¾i âˆ†2i .
4
It remains to choose âˆ†i . Following a similar logic as before, we seek to find a good feasible solution of the maximization
problem
max
âˆ†

such that




âˆš
EÂµ Ï(x, a)Î¾i âˆ†i Ri â‰¥ 2 ,

1 
1
EÂµ Î¾i âˆ†2i â‰¤ ,
4
n
0 â‰¤ âˆ†i â‰¤ 0.5.

(28)
(29)
(30)

For some Î± > 0 to be determined shortly, we set

âˆ†i = min


Î¾i Ïi Ri Î±
, 0.5 .
EÂµ [Î¾i Ï2i Ri2 ]

p
The bound constraint (30) is satisfied by construction and we set Î± = 4EÂµ [Î¾i Ï2i Ri2 ]/n to satisfy the constraint (29). To
obtain a feasible choice of , we bound EÂµ [Ï(x, a)Î¾i âˆ†i Ri ] as follows:




EÂµ Ï(x, a)Î¾i âˆ†i Ri â‰¥ EÂµ Î¾i Ïi âˆ†i Ri
"
#
Î¾i Ï2i Ri2 Î±1 Î¾i Ïi Ri â‰¤ EÂµ [Î¾i Ï2i Ri2 ]/2Î±
â‰¥ EÂµ
EÂµ [Î¾i Ï2i Ri2 ]
 2 2
!
EÂµ Î¾i Ïi Ri 1(Î¾i Ïi Ri > EÂµ [Î¾i Ï2i Ri2 ]/2Î±)
=Î± 1âˆ’
EÂµ [Î¾i Ï2i Ri2 ]
âˆš
=: 2 .

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Collecting our arguments so far, we have established that

 
T1 â‰¥ Â· PÎ¸1 (`Î¸1 (vÌ‚) â‰¥ ) + PÎ¸2 (`Î¸2 (vÌ‚) â‰¥ )
4
 1
â‰¥ Â· eâˆ’nDKL (PÎ¸1 kPÎ¸2 )
4 8
 1

â‰¥ Â·
=
4 8e
32e

 !2
EÂµ Î¾i Ï2i Ri2 1(Î¾i Ïi Ri > EÂµ [Î¾i Ï2i Ri2 ]/2Î±)
1 Î±2
=
1âˆ’
Â·
32e 4
EÂµ [Î¾i Ï2i Ri2 ]

i ï£¶2
h
ï£«
p
EÂµ Î¾i Ï2i Ri2 1 Î¾i Ïi Ri > nEÂµ [Î¾i Ï2i Ri2 ]/16
EÂµ [Î¾i Ï2i Ri2 ] ï£­
ï£¸ .
=
1âˆ’
32en
EÂµ [Î¾i Ï2i Ri2 ]
In order to complete the proof, we need to further upper bound T2 in the decomposition (26).
Bounding T2 We need to bound the
 supremum over
 all priors Î¸. Consider an arbitrary prior Î¸ and assume that Î· is drawn
according to Eq. (25). To bound EÎ¸ (vÎ· âˆ’ EÎ¸ [vÎ· ])2 , we view (vÎ· âˆ’ EÎ¸ [vÎ· ])2 as a random variable under Î¸ and bound it
using Hoeffdingâ€™s inequality.
We begin by bounding its range. From the definition of Î· and vÎ· ,
0 â‰¤ vÎ· â‰¤ EÏ€ [Î¾i Ri ] = EÂµ [Ï(x, a)Î¾i Ri ] â‰¤ (1 + Î´)1/2 EÂµ [Î¾i Ïi Ri ] ,
so also 0 â‰¤ EÎ¸ [vÎ· ] â‰¤ (1 + Î´)1/2 EÂµ [Î¾i Ïi Ri ]. Hence, |vÎ· âˆ’ EÎ¸ [vÎ· ]| â‰¤ (1 + Î´)1/2 EÂµ [Î¾i Ïi Ri ], and we obtain the bound
(vÎ· âˆ’ EÎ¸ [vÎ· ])2 â‰¤ (1 + Î´)(EÂµ [Î¾i Ïi Ri ])2 â‰¤ (1 + Î´)EÂµ [Î¾i Ï2i Ri2 ].

(31)

The proof proceeds by applying Hoeffdingâ€™s inequality to control the probability that (vÎ· âˆ’ EÎ¸ [vÎ· ])2 â‰¥ t2 for a suitable t.
Then we can, with high probability, use the bound (vÎ· âˆ’ EÎ¸ [vÎ· ])2 â‰¥ t2 , and with the remaining small probability apply the
bound of Eq. (31).
To apply Hoeffdingâ€™s inequality, we write vÎ· explicitly as
X
X
vÎ· =
Âµ(Bi )Ï0i Î·i =:
Yi
iâˆˆI

iâˆˆI

Ï0i

where := EÂµ [Ï(x, a) | (x, a) âˆˆ Bi ]. Thus, vÎ· can be written as a sum of countably many independent variables, but we
can only apply Hoeffdingâ€™s inequality to their finite subset. Note that the variables Yi are non-negative and upper-bounded by
2 2
a summable series, namely Yi â‰¤ Âµ(Bi )Ï0i Ri , where the summability follows
P because EÂµ [ÏRmax ] â‰¤ 1 + EÂµ [Ï Rmax ] < âˆ.
This means that for any Î´0 > 0, we can choose a finite set I0 such that i6âˆˆI0 Yi â‰¤ Î´0 . We will determine the sufficiently
small value of Î´0 later; for now, consider the corresponding set I0 and define an auxiliary variable
X
vÎ·0 :=
Yi ,
iâˆˆI0

which by construction satisfies vÎ·0 â‰¤ vÎ· â‰¤ vÎ·0 + Î´0 . Note that the summands Yi can be bounded as
p
p
0 â‰¤ Yi â‰¤ Î¾i Ï0i Ri Âµ(Bi ) â‰¤ Î¾i (1 + Î´)1/2 Ïi Ri Âµ(Bi ) Î³ 0
p
âˆš
because Ï0i â‰¤ (1 + Î´)1/2 Ïi and Î¾i Âµ(Bi ) â‰¤ Î¾i Âµ(Bi ) Î³ 0 , because Î¾i = 0 whenever Âµ(Bi ) > max{Î³, Î´} = Î³ 0 . By
Hoeffdingâ€™s inequality, we thus have
(
)
2t2
0
0
P
P(|vÎ· âˆ’ EÎ¸ vÎ· | â‰¥ t) â‰¤ 2 exp âˆ’
(1 + Î´) iâˆˆI0 Î¾i Ï2i Ri2 Âµ(Bi )Î³ 0


2t2
â‰¤ 2 exp âˆ’
.
(1 + Î´)Î³ 0 EÂµ [Î¾i Ï2i Ri2 ]

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Now take t =

p
Î³ 0 log(4/Î³ 0 )(1 + Î´)EÂµ [Î¾i Ï2i Ri2 ]/2 in the above bound, which yields

h
i
h
i Î³0
P (vÎ·0 âˆ’ EÎ¸ vÎ·0 )2 â‰¥ t2 = P |vÎ·0 âˆ’ EÎ¸ vÎ·0 | â‰¥ t â‰¤
.
2
p
Now, we can go back to analyzing vÎ· . We set Î´0 sufficiently small, so t + Î´0 â‰¤ Î³ 0 log(5/Î³ 0 )(1 + Î´)EÂµ [Î¾i Ï2i Ri2 ]/2. Thus,
using Eq. (31), we have
h
i


EÎ¸ (vÎ· âˆ’ EÎ¸ vÎ· )2 â‰¤ (t + Î´0 )2 Â· P (vÎ· âˆ’ EÎ¸ vÎ· )2 < (t + Î´0 )2
h
i
+ (1 + Î´)EÂµ [Î¾i Ï2i Ri2 ] Â· P (vÎ· âˆ’ EÎ¸ vÎ· )2 â‰¥ (t + Î´0 )2
h
i
â‰¤ (t + Î´0 )2 + (1 + Î´)EÂµ [Î¾i Ï2i Ri2 ] Â· P (vÎ·0 âˆ’ EÎ¸ vÎ·0 )2 â‰¥ t2
Î³0
Î³ 0 log(5/Î³ 0 )(1 + Î´)EÂµ [Î¾i Ï2i Ri2 ]
+ (1 + Î´)EÂµ [Î¾i Ï2i Ri2 ] Â·
2
2
â‰¤ Î³ 0 log(5/Î³ 0 )(1 + Î´)EÂµ [Î¾i Ï2i Ri2 ] .
=

Combining this bound with the bound on T1 yields the theorem.
Lemma 1. Let Z := X Ã— A be a subset of Rd , let Âµ be a probability measure on Z and Rmax and Ï be non-negative
measurable functions on Z. Given Î³ âˆˆ [0, 1], define a random variable Î¾Î³ (z) := 1(Âµ(z) â‰¤ Î³). Then for any Î´ âˆˆ (0, 1],
there exists a countable index set I and disjoint sets Bi âŠ† Z alongside non-negative reals Ri , Ïi and Î¾i âˆˆ {0, 1} such that
the following conditions hold:
â€¢ Sets Bi form a partition of Z, i.e., Z = ]iâˆˆI Bi .
â€¢ Reals Ri and Ïi approximate Rmax and Ï, and Î¾i equals Î¾Î³ as follows:
2
Rmax
(z) âˆˆ [Ri2 , (1 + Î´)Ri2 ] ,

Ï2 (z) âˆˆ [Ï2i , (1 + Î´)Ï2i ] ,

Î¾Î³ (z) = Î¾i

for all z âˆˆ Bi .

â€¢ Each set Bi either satisfies Âµ(Bi ) â‰¤ Î´ or consists of a single z âˆˆ Z.
Proof. Let Z := X Ã— A. We begin our construction by separating out atoms, i.e., the elements z âˆˆ Z such that Âµ(z) > 0.
Specifically, we write Z = Z na ] Z a where Z a consists of atoms and Z na of all non-atoms. The set Z a is either finite or
countably infinite, so Z na is measurable.
By a theorem of SierpinÌski (1922), since Âµ does not have any atoms on Z na , it must be continuous on Z na in the sense that
if A is a measurable subset of Z na with Âµ(A) = a then for any b âˆˆ [0, a], there exists a measurable set B âŠ† A such that
na
Âµ(B) = b. This means that we can decompose Z na into N := d1/Î´e sets Z1na , Z2na , . . . , ZN
such that each has a measure at
UN
na
na
most Î´ and Z = j=1 Zj .
We next ensure the approximation properties for Rmax and Ï. We begin by a countable decomposition of non-negative reals.
We consider the countable index set J := Z âˆª {âˆ’âˆ} and define the sequence aj := (1 + Î´)j/2 , for j âˆˆ Z. Positive reals
can then be decomposed into the following intervals indexed by J :
Iâˆ’âˆ := {0} ,

Ij := (aj , aj+1 ] for j âˆˆ Z.

It will also be convenient to set aâˆ’âˆ := 0. Thus, the construction of Ij guarantees that for all j âˆˆ J and all t âˆˆ Ij we have
a2j â‰¤ t2 â‰¤ (1 + Î´)a2j .
The desired partition, with the index set I = Z a âˆª [N ] Ã— J 2 , is as follows:
for i = z âˆˆ Z a :

Bi := {z}, Ri := Rmax (z), Ïi := Ï(z), Î¾i := Î¾Î³ (z);
2

for i = (j, jR , jÏ ) âˆˆ [N ] Ã— J :

âˆ’1
Bi := Zjna âˆ© Rmax
(IjR ) âˆ© Ïâˆ’1 (IjÏ ),

Ri := ajR , Ïi := ajÏ , Î¾i := 1.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

B. Proof of Theorem 2
Let Ax := {a âˆˆ A : Ï(x, a) â‰¤ Ï„ }. For brevity, we write Ai := Axi . We decompose the mean squared error into the squared
bias and variance and control each term separately,

2
MSE(vÌ‚SWITCH ) = E[vÌ‚SWITCH ] âˆ’ v Ï€  + Var[vÌ‚SWITCH ].
We first calculate the bias. Note that bias is incurred only in the terms that fall in Acx , so
ï£®
ï£¹
ï£®
ï£¹
X
X
E[vÌ‚SWITCH ] âˆ’ v Ï€ = E ï£°
rÌ‚(x, a)Ï€(a|x)ï£» âˆ’ E ï£°
E[r|x, a] Ï€(a|x)ï£»
aâˆˆAcx

aâˆˆAcx

h

i

= EÏ€ rÌ‚(x, a) âˆ’ E[r|x, a] 1(a âˆˆ Acx )


= EÏ€ (x, a) 1(Ï > Ï„ )
where we recall that (x, a) = rÌ‚(x, a) âˆ’ E[r|x, a].
Next we upper bound the variance. Note that the variance contributions from the IPS part and the DM part are not
independent, since the indicators Ï(xi , a) > Ï„ and Ï(xi , a) â‰¤ Ï„ are mutually exclusive. To simplify the analysis, we use the
following inequality that holds for any random variable X and Y :
Var(X + Y ) â‰¤ 2Var(X) + 2Var(Y ).
This allows us to calculate the variance of each part separately.
" n
#
" n
#
1X
1 XX
Var[vÌ‚SWITCH ] â‰¤ 2 Var
[ri Ïi 1(ai âˆˆ Ai )] + 2 Var
rÌ‚(xi , a)Ï€(a|xi )1(a âˆˆ Aci )
n i=1
n i=1
aâˆˆA
ï£®
ï£¹
X

 2
2
= VarÂµ rÏ1(a âˆˆ Ax ) + Varï£°
rÌ‚(x, a)Ï€(a|x)ï£»
n
n
aâˆˆAcx
ï£®
ï£¹
X



 2

 2
2
= EÂµ Var rÏ1(a âˆˆ Ax )  x, a + VarÂµ E rÏ1(a âˆˆ Ax )  x, a + Varï£°
rÌ‚(x, a)Ï€(a|x)ï£»
n
n
n
aâˆˆAcx
"
#
h
i 2
 X
2


 2
2
2
rÌ‚(x, a)Ï€(a|x)
â‰¤ EÂµ Var rÏ1(a âˆˆ Ax )  x, a + EÂµ E[rÏ1(a âˆˆ Ax ) | x, a] + E
n
n
n
aâˆˆAcx
"
#
 X
2
 2 2
 2
 2
 2
2
2
â‰¤ EÂµ Ïƒ Ï 1(a âˆˆ Ax ) + EÂµ Rmax Ï 1(a âˆˆ Ax ) + E
rÌ‚(x, a)Ï€(a|x)
.
n
n
n
c
aâˆˆAx

To complete the proof, note that the last term is further upper bounded using Jensenâ€™s inequality as
"
#
"
#
 X
2
 X
2  X rÌ‚(x, a)Ï€(a|x) 2
P
E
rÌ‚(x, a)Ï€(a|x)
=E
Ï€(a|x)
aâˆˆAcx Ï€(a|x)
aâˆˆAcx
aâˆˆAcx
aâˆˆAcx
"
#
 X
 X
2
â‰¤E
Ï€(a|x)
rÌ‚(x, a) Ï€(a|x)
aâˆˆAcx

aâˆˆAcx

 2

â‰¤ EÏ€ Rmax
1(Ï > Ï„ ) ,
where the final inequality uses

P

aâˆˆAcx

Ï€(a|x) â‰¤ 1 and rÌ‚(x, a) âˆˆ [0, Rmax (x, a)] almost surely.

Combining the bias and variance bounds, we get the stated MSE upper bound.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

C. Utility Lemmas
Lemma 2 (Hoeffding, 1963, Theorem 2). Let Xi âˆˆ [ai , bi ] and X1 , ..., Xn are drawn independently. Then the empirical
mean XÌ„ = n1 (X1 + ... + Xn ) obeys
âˆ’ Pn

P(|XÌ„ âˆ’ E[XÌ„]| â‰¥ t) â‰¤ 2e

2n2 t2
(bi âˆ’ai )2

i=1

.

Lemma 3 (Bernoulli KL-divergence). For 0 < p, q < 1, we have
1
1
DKL (Ber(p)kBer(q)) â‰¤ (p âˆ’ q)2 ( +
).
q
1âˆ’q
Proof.
 


p
1âˆ’p
+ (1 âˆ’ p) log
q
1âˆ’q
qâˆ’p
(p âˆ’ q)2
(p âˆ’ q)2
pâˆ’q
+ (1 âˆ’ p)
=
+ (p âˆ’ q) +
+ (q âˆ’ p)
â‰¤p
q
1âˆ’q
q
1âˆ’q


1
1
= (p âˆ’ q)2
+
.
q
1âˆ’q

DKL (Ber(p)kBer(q)) = p log

D. Additional Figures from the Experiments

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits
IPS
10

DR

SWITCH-DR

oracle-SWITCH-DR

ecoli: n = 336, d = 7, k = 8

10

-1

10

-2

10

-3

oracle-Trim/TrunIPS

ecoli: n = 336, d = 7, k = 8

MSE
10

-2

10

-3

100

150

200

250

300

100

150

200

n

-1

10

-2

10

-3

100

glass: n = 214, d = 9, k = 6

120

140

160

180

200

10

-1

10

-2

10

-3

100

n

glass: n = 214, d = 9, k = 6

120

140

160

(d) glass / noisy reward

page-blocks: n = 5473, d = 10, k = 5

100

10-2

10-2

page-blocks: n = 5473, d = 10, k = 5

MSE

10-1

MSE

10-1

10-3

10-3

10-4

10-4

10-5 2
10

10-5 2
10

103

n

100

103

n

(e) page-blocks / deterministic reward

(f) page-blocks / noisy reward

satimage: n = 6435, d = 36, k = 6

100

satimage: n = 6435, d = 36, k = 6

MSE

10-1

MSE

180

n

(c) glass / deterministic reward
100

10-1

10-2

10-2 2
10

300

(b) ecoli / noisy reward

MSE

10

250

n

(a) ecoli / deterministic reward

MSE

SWITCH-DR-magic

-1

MSE

10

0

DM

103

n

(g) satimage / deterministic reward

10-3 2
10

103

n

(h) satimage / noisy reward

200

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits
IPS

DR

SWITCH-DR

oracle-SWITCH-DR

pendigits: n = 10992, d = 16, k = 10

100

MSE

MSE

100

DM

10-1

10-2 2
10

103

oracle-Trim/TrunIPS

pendigits: n = 10992, d = 16, k = 10

10-1

10-2 2
10

104

103

(a) pendigits / deterministic reward

(b) pendigits / noisy reward

letter: n = 20000, d = 16, k = 26

letter: n = 20000, d = 16, k = 26

10

-1

-1

MSE

MSE

104

n

n

10

SWITCH-DR-magic

10-2

10

-2

102

103

104

102

103

n

(c) letter / deterministic reward
10

(d) letter / noisy reward

vehicle: n = 846, d = 18, k = 4

10

-1

10

-2

10

-3

vehicle: n = 846, d = 18, k = 4

MSE

-1

MSE

10

0

10

-2

10

-3

100

200

300

400

500 600 700 800

100

200

300

n

10

0

(f) vehicle / noisy reward

wdbc: n = 569, d = 30, k = 2

10

-1

-2

100

500 600 700 800

0

10

-1

10

-2

10

-3

wdbc: n = 569, d = 30, k = 2

MSE

MSE

10

400

n

(e) vehicle / deterministic reward

10

104

n

200

300

400

n

(g) wdbc / deterministic reward

500

100

200

300

n

(h) wdbc / noisy reward

400

500

