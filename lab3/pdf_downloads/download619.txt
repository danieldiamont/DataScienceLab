Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

The supplementary materials is organized as follows. In Appendices A, B and C, we provide detailed proofs of the theoretical
results in the paper. In Appendix D, we provide additional figures for the experiments described in Section 5.

A. Proof of Theorem 1
In this appendix we prove the minimax bound of Theorem 1. The result is obtained by combining the following two lower
bounds:
Theorem 3 (Lower bound 1). For each problem instance such that Eµ [ρ2 σ 2 ] < ∞, we have
h

i 2
p
2 2
2
2 σ 2 ]/2
E
ρ
σ
1
ρσ
>
R
nE
[ρ
µ
max
µ
Eµ [ρ σ ] 
 .
Rn (π; λ, µ, σ, Rmax ) ≥
1−
2
2
32en
Eµ [ρ σ ]
2 2



2
Theorem 4 (Lower bound 2). Assume that Eµ [ρ2 Rmax
] < ∞, and we are given γ ∈ [0, 1] and δ ∈ (0, 1]. Write ξ := ξγ
0 :=
and γ
max{γ, δ}. Then there exist functions R̂(x, a) and ρ̂(x, a) such that
2
R̂2 (x, a) ≤ Rmax
(x, a) ≤ (1 + δ)R̂2 (x, a) ,

ρ̂2 (x, a) ≤ ρ2 (x, a) ≤ (1 + δ)ρ̂2 (x, a)

and the following lower bound holds:
Rn (π; λ, µ, σ, Rmax )

q

 2
2 2
2
2
Eµ ξ ρ̂ R̂ 1 ξ ρ̂R̂ > nEµ [ξ ρ̂ R̂ ]/16 

Eµ [ξ ρ̂2 R̂2 ] 
 − γ 0 log 5/γ 0 (1 + δ)Eµ [ξ ρ̂2 R̂2 ] .
1 −
≥


32en
Eµ [ξ ρ̂2 R̂2 ]


The reason for introducing γ 0 in Theorem 4 is to allow γ = 0, which is an important special case of the theorem. Otherwise,
we could just assume 0 < δ ≤ γ. The first bound captures the intrinsic difficulty due to the variance of reward, and is
present even in a vanilla multi-armed bandit problem without contexts. The second result shows the additional dependence
2
on Rmax
, even when σ ≡ 0, whenever the distribution λ is not too degenerate, and captures the additional difficulty of the
contextual bandit problem. We next show how these two lower bounds yield Theorem 1 and then return to their proofs.
Proof of Theorem 1. Throughout the theorem we write ξ := ξγ . We begin by simplifying the two lower bounds. Assume
that Assumption 1 holds with . This also means that Eµ [ξ(ρRmax )2+ ] is finite as well as Eµ [ξ(ρRmax )2 ] is finite and
either both of them are zero or both of them are non-zero. Similarly, Eµ [(ρσ)2+ ] and Eµ [(ρσ)2 ] are both finite and either
both of them are zero or both of them are non-zero, so Cγ is a finite constant. Let p = 1 + /2 and q = 1 + 2/, i.e.,
1/p + 1/q = 1. Further, let R̂ and ρ̂ be the functions from Theorem 4. Then the definition of Cγ means that
Cγ1/(q) = Cγ1/(2+)



2
2
 2+
 2 2 2+  2+
 E ξ(ρ2 R2 ) 2+

2
2
Eµ (ρ σ )
µ
 max



= 2 · max
,
2
2
2
2


Eµ ξρ Rmax
Eµ ρ σ
( 
1/p

1/p )
2
Eµ ξ(ρ2 Rmax
)p
Eµ (ρ2 σ 2 )p

 ,


= 2 · max
2
Eµ ξρ2 Rmax
Eµ ρ2 σ 2
( 
1/p

1/p )
Eµ ξ(ρ̂2 R̂2 )p
Eµ (ρ2 σ 2 )p



 ,
,
≥ 2 · max
2
Eµ ξρ2 Rmax
Eµ ρ2 σ 2

(10)

and recall that we assume that
n
o
2
n ≥ max 16Cγ1/ , 2Cγ2/ Eµ [σ 2 /Rmax
] .

(11)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

First, we simplify the correction term in the lower bound of Theorem 3. Using Hölder’s inequality and Eq. (10), we have



q
Eµ ρ2 σ 2 1 ρσ 2 > Rmax nEµ [ρ2 σ 2 ]/2
q
i1/q
h
h
p i1/p
≤ Eµ ρ2 σ 2
· Pµ ρσ 2 > Rmax nEµ [ρ2 σ 2 ]/2
q
i1/q
h
1
.
≤ Eµ [ρ2 σ 2 ] · Cγ1/(q) · Pµ ρσ 2 /Rmax > nEµ [ρ2 σ 2 ]/2
2
We further invoke Markov’s inequality, Cauchy-Schwartz inequality, and Eq. (11) in the following three steps to simplify
this event as

 !1/q
Eµ ρσ · (σ/Rmax )
p
nEµ [ρ2 σ 2 ]/2
!1/q
p
p
2
Eµ [ρ2 σ 2 ] · Eµ [σ 2 /Rmax
]
1
2 2
1/(q)
p
≤ Eµ [ρ σ ] · Cγ
·
2
nEµ [ρ2 σ 2 ]/2

1/2q
2
]
2Eµ [σ 2 /Rmax
1
1
≤ Eµ [ρ2 σ 2 ] .
= Eµ [ρ2 σ 2 ] · Cγ2/ ·
2
n
2
1
≤ Eµ [ρ2 σ 2 ] · Cγ1/(q) ·
2

(12)

For the correction term in Theorem 4, we similarly have



q
Eµ ξ ρ̂2 R̂2 1 ξ ρ̂R̂ > nEµ [ξ ρ̂2 R̂2 ]/16
q
i1/q
h
h
p i1/p
≤ Eµ ξ ρ̂2 R̂2
· Pµ ξ ρ̂R̂ > nEµ [ξ ρ̂2 R̂2 ]/16
h
i1/q
1
2
] · Cγ1/(q) · Pµ ξ ρ̂2 R̂2 > nEµ [ξ ρ̂2 R̂2 ]/16
≤ Eµ [ξρ2 Rmax
,
2
so that Markov’s inequality and Eq. (11) further yield
!1/q
Eµ [ξ ρ̂2 R̂2 ]
1
2 2
1/(q)
·
≤ Eµ [ξρ Rmax ] · Cγ
2
nEµ [ξ ρ̂2 R̂2 ]/16

1/q
1
1
(1 + δ)2
2 2
1/ 16
2
= Eµ [ξρ Rmax ] · Cγ ·
≤ Eµ [ξρ2 Rmax
]≤
Eµ [ξ ρ̂2 R̂2 ] .
2
n
2
2

(13)

Using Eq. (12), the bound of Theorem 3 simplifies as
Rn (π; λ, µ, σ, Rmax )
h

i 2

p
Eµ ρ2 σ 2 1 ρσ 2 > Rmax nEµ [ρ2 σ 2 ]/2
Eµ [ρ2 σ 2 ] 

≥
1−
32en
Eµ [ρ2 σ 2 ]
Eµ [ρ2 σ 2 ]
≥
32en



1
1−
2

2
=

Eµ [ρ2 σ 2 ]
.
128en

(14)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Similarly, by Eq. (13), Theorem 4 simplifies as
Rn (π; λ, µ, σ, Rmax )



q

 2
2 2
2 R̂2 ]/16
E
ξ
ρ̂
R̂
1
ξ
ρ̂
R̂
>
nE
[ξ
ρ̂
µ
µ

Eµ [ξ ρ̂2 R̂2 ] 
1 −
 − γ 0 log(5/γ 0 )(1 + δ)Eµ [ξ ρ̂2 R̂2 ]
≥

32en 
Eµ [ξ ρ̂2 R̂2 ]


2
Eµ [ξ ρ̂2 R̂2 ]
(1 + δ)2
− γ 0 log(5/γ 0 )(1 + δ)Eµ [ξ ρ̂2 R̂2 ]
1−
32en
2
2
Eµ [ξ ρ̂2 R̂2 ]
=
1 − 2δ − δ 2 − γ 0 log(5/γ 0 )(1 + δ)Eµ [ξ ρ̂2 R̂2 ]
128en
2
2
Eµ [ξρ2 Rmax
] 1 − 2δ − δ 2
2
− γ 0 log(5/γ 0 )(1 + δ)Eµ [ξρ2 Rmax
] .
≥
128en
(1 + δ)2
≥

Since this bound is valid for all δ > 0, taking δ → 0, we obtain
Rn (π; λ, µ, σ, Rmax ) ≥

2
]
Eµ [ξρ2 Rmax
2
− γ log(5/γ)Eµ [ξρ2 Rmax
] .
128en

Combining this bound with Eq. (14) yields
Rn (π; λ, µ, σ, Rmax )
≥

2
] 1
1 Eµ [ρ2 σ 2 ] 1 Eµ [ξρ2 Rmax
2
·
+ ·
− · γ log(5/γ)Eµ [ξρ2 Rmax
]
2
128en
2
128en
2

2
Eµ [ρ2 σ 2 ] Eµ [ξρ2 Rmax
] 1
2
+
− · γ log(5/γ)Eµ [ξρ2 Rmax
]
700n
700n
2

i
1 h
2
=
Eµ [ρ2 σ 2 ] + Eµ [ξρ2 Rmax
] 1 − 350nγ log(5/γ)
.
700n

≥

It remains to prove Theorems 3 and 4. They are both proved by a reduction to hypothesis testing, and invoke Le Cam’s
argument to lower-bound the error in this testing problem. As in most arguments of this nature, the key contribution lies in
the construction of an appropriate testing problem that leads to the desired lower bounds. Before proving the theorems, we
recall the basic result of Le Cam which underlies our proofs. We point the reader to the excellent exposition of Lafferty et al.
(2008, Section 36.4) on more details about Le Cam’s argument.
Theorem 5 (Le Cam’s method, Lafferty et al., 2008, Theorem 36.8). Let P be a set of distributions, let X1 , . . . , Xn be an
i.i.d. sample from some P ∈ P, let θ(P ) be any function of P ∈ P, let θ̂(X1 , . . . , Xn ) be an estimator, and d be a metric.
For any pair P0 , P1 ∈ P,
∆
inf sup EP [d(θ̂, θ(P ))] ≥ e−nDKL (P0 kP1 )
(15)
8
θ̂ P ∈P
R
where ∆ = d(θ(P0 ), θ(P1 )), and DKL (P0 kP1 ) = log(dP0 /dP1 )dP0 is the KL-divergence.
While the proofs of the two theorems share a lot of similarities, they have to use reductions to slightly different testing
problems given the different mean and variance constraints in the two results. We begin with the proof of Theorem 3, which
has a simpler construction.
A.1. Proof of Theorem 3
The basic idea of this proof is to reduce the problem of policy evaluation to that of Gaussian mean estimation where there is
a mean associated with each x, a pair. We now describe our construction.
Creating a family of problems Since we aim to show a lower bound on the hardness of policy evaluation in general,
it suffices to show a particular family of hard problem instances, such that every estimator requires the stated number of

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

samples on at least one of the problems in this family. Recall that our minimax setup assumes that π, µ and λ are fixed
and the only aspect of the problem which we can design is the conditional reward distribution D(r | x, a). For Theorem 3,
this choice is further constrained to satisfy E[r | x, a] ≤ Rmax (x, a) and Var(r | x, a) ≤ σ 2 (x, a). In order to describe our
construction, it will be convenient to define the shorthand E[r | x, a] = η(x, a). We will identify a problem in our family
with the function η(x, a) as that will be the only changing element in our problems. For a chosen η, the policy evaluation
question boils down to estimating vηπ = E[r(x, a)], where the contexts x are chosen according to λ, actions are drawn from
π(x, a) and the reward distribution Dη (r | x, a) is a normal distribution with mean η(x, a) and variance σ 2 (x, a)
Dη (r | x, a) = N (η(x, a), σ 2 (x, a)).
Clearly this choice meets the variance constraint by construction, and satisfies the upper bound so long as η(x, a) ≤
Rmax (x, a) almost surely. Since the evaluation policy π is fixed throughout, we will drop the superscript and use vη to
denote vηπ in the remainder of the proofs. With some abuse of notation, we also use Eη [·] to denote expectations where
contexts and actions are drawn based on the fixed choices λ and µ corresponding to our data generating distribution, and the
rewards drawn from η. We further use Pη to denote this entire joint distribution over (x, a, r) triples.
Given this family of problem instances, it is easy to see that for any pair of η1 , η2 which are both pointwise upper bounded
by Rmax , we have the lower bound:
i
h
Rn (λ, π, µ, σ 2 , Rmax ) ≥ inf max Eη (v̂ − vη )2 ,
v̂ η∈η1 ,η2
| {z }
`η (v̂)
where we have introduced the shorthand `η (v̂) to denote the squared error of v̂ to vη . For a parameter  > 0 to be chosen
later, we can further lower bound this risk for a fixed v̂ as

Rn (v̂) ≥ max Eη [`η (v̂)] ≥ max Pη (`η ≥ )
η∈η1 ,η2
η∈η1 ,η2
i
h
≥ Pη1 (`η1 (v̂) ≥ ) + Pη2 (`η2 (v̂) ≥ ) ,
2

(16)

where the last inequality lower bounds the maximum by the average. So far we have been working with an estimation
problem. We next describe how to reduce this to a hypothesis testing problem.
Reduction to hypothesis testing For turning our estimation problem into a testing problem, the idea is to identify a pair
η1 , η2 such that they are far enough from each other so that any estimator which gets a small estimation loss can essentially
identify whether the data generating distribution corresponds to Pη1 or Pη2 . In order to do this, we take any estimator v̂
and identify a corresponding test statistic which maps v̂ into one of η1 , η2 . The way to do this is essentially identified in
Eq. (16), and we describe it next.
Note that since we are constructing a hypothesis test for a specific pair of distributions Pη1 and Pη2 , it is reasonable to
consider test statistics which have knowledge of η1 and η2 , and hence the corresponding distributions. Consequently, these
tests also know the true policy values vη1 and vη2 and the only uncertainty is which of them gave rise to the observed data
samples. Therefore, for any estimator v̂, we can a associate a statistic φ(v̂) = argminη {`η1 (v̂), `η2 (v̂)}.
Given this hypothesis test, we are interested in its error rate Pη (φ(v̂) 6= η). We first relate the estimation error of v̂ to the
error rate of the test. Suppose for now that
`η1 (v̂) + `η1 (v̂) ≥ 2,
(17)
so that at least one of the losses is at least . Suppose that the data comes from η1 . Then if `η1 (v̂) < , we know that the test
is correct, because by Eq. (17) the other loss is greater than , and therefore φ(v̂) = η1 . This means that the error under η1
can only occur if `η1 (v̂) ≥ . Similarly, the error under η2 can only occur if `η2 (v̂) ≥ , so the test error can be bounded as
max Pη (φ(v̂) 6= η) ≤ Pη1 (φ(v̂) 6= η1 ) + Pη2 (φ(v̂) 6= η2 )

η∈η1 ,η2

≤ Pη1 (`η1 (v̂) ≥ ) + Pη2 (`η2 (v̂) ≥ )
2
≤ Rn (v̂),


(18)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

where the final inequality uses our earlier lower bound in Eq. (16).
To finish connecting our the estimation problem to testing, it remains to establish our earlier supposition (17). Assume for
now that η1 and η2 are chosen such that
2
(vη1 − vη2 ) ≥ 4.
(19)
Then an application of the inequality (a + b)2 ≤ 2a2 + 2b2 yields
2

4 ≤ (vη1 − vη2 ) ≤ 2(v̂ − vη1 )2 + 2(v̂ − vη2 )2 = 2`η1 (v̂) + 2`η2 (v̂),
which yields the posited bound (16).
Invoking Le Cam’s argument So far we have identified a hypothesis testing problem and a test statistic whose error is
upper bounded in terms of the minimax risk of our problem. In order to complete the proof, we now place a lower bound on
the error of this test statistic. Recall the result of Le Cam (15), which places an upper bound on the attainable error in any
testing problem. In our setting, this translates to
1 −nDKL (Pη || Pη )
1
2 .
e
8
Since the distribution of the rewards is a spherical Gaussian, the KL-divergence is given by the squared distance between the
means, scaled by the variance, that is
max Pη (φ(v̂) 6= η) ≥

η∈η1 ,η2


DKL (Pη1 || Pη2 ) = E


(η1 (x, a) − η2 (x, a))2
,
2σ 2 (x, a)

where we recall that the contexts and actions are drawn from λ and µ respectively. Since we would like the probability of
error in the test to be a constant, it suffices to choose η1 and η2 such that


(η1 (x, a) − η2 (x, a))2
1
E
≤ .
(20)
2σ 2 (x, a)
n
Picking the parameters So far, we have not made any concrete choices for η1 and η2 , apart from some constraints which
we have introduced along the way. Note that we have the constraints (19) and (20) which try to ensure that η1 and η2 are not
too close that an estimator does not have to identify the true parameter, or too far that the testing problem becomes trivial.
Additionally, we have the upper and lower bounds of 0 and Rmax on η1 and η2 . In order to reason about these constraints, it
is convenient to set η2 ≡ 0, and pick η1 (x, a) = η1 (x, a) − η2 (x, a) = ∆(x, a). We now write all our constraints in terms
of ∆.
Note that vη2 is now 0, so that the first constraint (19) is equivalent to

√
vη1 = Eη1 [ρ(x, a)r(x, a)] = E∆ [ρ(x, a)r(x, a)] ≥ 2 ,

where the importance weighting function ρ is introduced since Pη1 is based on choosing actions according to µ and we seek
to evaluate π. The second constraint (20) is also straightforward
 2
∆
1
E
≤ .
2σ 2
n
Finally, the bound Rmax and non-negativity of η1 and η2 are enforced by requiring 0 ≤ ∆(x, a) ≤ Rmax (x, a) almost
surely.
The minimax lower bound is then obtained by the largest  in the constraint (19) such that the other two constraints can be
satisfied. This gives rise to the following variational characterization of the minimax lower bound:
max
∆

such that


√
E∆ [ρ(x, a)r(x, a)] ≥ 2 ,
 2
∆
1
E
≤ ,
2
2σ
n
0 ≤ ∆(x, a) ≤ Rmax (x, a).

(21)
(22)
(23)

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Instead of finding the optimal solution, we just exhibit a feasible setting of ∆ here. We set
r


ασ 2 ρ
2Eµ [ρ2 σ 2 ]
∆ = min
, Rmax , where α =
.
2
2
Eµ [ρ σ ]
n

(24)

This setting satisfies the bounds (23) by construction. A quick substitution also verifies that the constraint (22) is satisfied.
Consequently, it suffices to set  to the value attained in the constraint (21). Substituting the value of ∆ in the constraint, we
see that
E∆ [ρ(x, a)r(x, a)] = Ex∼λ,a∼µ [ρ(x, a)∆(x, a)]


ασ 2 ρ  2
2 2
1 ρσ α ≤ Rmax Eµ [ρ σ ]
≥ Ex∼λ,a∼µ ρ
Eµ [ρ2 σ 2 ]

 !
Eµ ρ2 σ 2 1 ρσ 2 α > Rmax Eµ [ρ2 σ 2 ]
=α 1−
Eµ [ρ2 σ 2 ]
√
=: 2 .
Putting all the foregoing bounds together, we obtain that for all estimators v̂

 
Rn (v̂) ≥ · max Pη (φ(v̂) 6= η)
2 η∈η1 ,η2
 1
≥ · e−nDKL (Pη1 || Pη2 )
2 8

 1
=
≥ ·
2 8e
16e

 !2
2
Eµ ρ2 σ 2 1 ρσ 2 > Rmax Eµ [ρ2 σ 2 ]/α
1 α
=
·
1−
16e 4
Eµ [ρ2 σ 2 ]
h

p
i 2
2 2
2
2 σ 2 ]/2
E
nE
[ρ
ρ
σ
1
ρσ
>
R
2 2
µ
µ
max
Eµ [ρ σ ] 
 .
=
1−
32en
Eµ [ρ2 σ 2 ]
A.2. Proof of Theorem 4
We now give the proof of Theorem 4. While it shares a lot of reasoning with the proof of Theorem 3, it has one crucial
difference. In Theorem 3, there is a non-trivial noise in the reward function, unlike in Theorem 4. This allowed the proof to
work with just two candidate mean-reward functions, since any realization in the data is corrupted with noise. However, in
the absence of added noise, the task of mean identification becomes rather trivial: an estimator can just check whether η1 or
η2 matches the observations exactly.
To prevent such a strategy, we instead construct a richer family of reward functions. Instead of merely two mean rewards,
our construction will involve a randomized design of the expected reward function from an appropriate prior distribution.
The draw of the mean reward from a prior will essentially generate noise even though any given problem is noiseless. The
construction will also highlight the crucial sources of difference between the contextual and multi-armed bandit problems,
since the arguments here rely on having access to a rich context distribution, by which we mean distribution that puts
non-trivial probability on many contexts. In the absence of this property, the bound of Theorem 4 becomes weaker.
Creating a family of problems Our family of problems will be parametrized by the two reals δ and γ from the statement
of the theorem. Our construction begins with a discretization step at the resolution δ, whose goal is to create a countable
partition of the set of pairs X × A. If sets X and A are countable or finite, this step is vacuous, but if the sets of contexts or
actions have continuous parts, this step is required.
First, let µ(x, a) denote the joint probability measure obtained by firstUdrawing x ∼ λ and then a ∼ µ(· | x). In Lemma 1, we
show that X × A can be split into countably many disjoint sets Bi , i∈I Bi = X × A, such that the following conditions
are satisfied:

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

• Each i ∈ I is associated with numbers Ri ≥ 0, ρi ≥ 0 and ξi ∈ {0, 1} such that
2
Rmax
(x, a) ∈ [Ri2 , (1 + δ)Ri2 ] ,

ρ2 (x, a) ∈ [ρ2i , (1 + δ)ρ2i ] ,

ξγ (x, a) = ξi

for all (x, a) ∈ Bi .

• Each Bi either satisfies µ(Bi ) ≤ δ or consists of a single pair (xi , ai ).
The numbers Ri and ρi will be exactly R̂(x, a) and ρ̂(x, a) from the theorem statement.
As before, we parametrize the family of reward distributions in terms of the mean reward function η(x, a). However, now
η(x, a) is itself a random variable, which is drawn from a prior distribution. The reward function η(x, a) will be constant on
each Bi , and its value on Bi , written as η(i), will be drawn from a scaled Bernoulli, parametrized by a prior function θ(i) as
follows:

ξi Ri
with probability θ(i),
η(i) =
(25)
0
with probability 1 − θ(i).
We now set Dη (r | x, a) = η(i) whenever (x, a) ∈ Bi . This clearly satisfies the constraints on the mean since 0 ≤
E[r | x, a] ≤ Ri ≤ Rmax (x, a) from the property of the partition, and also Var(r | x, a) = 0 as per the setting of Theorem 4.
The goal of an estimator is to take n samples generated by drawing x ∼ λ, a | x ∼ µ and r | x, a ∼ Dη , and output an
estimate v̂ such that Eη [(v̂ − vηπ )2 ] is small. We recall our earlier shorthand vη to denote the value of π under the reward
distribution generated by η. For showing a lower bound on this quantity, it is clearly
 sufficient to pick
 any prior distribution
governed by a parameter θ, as in Eq. (25), and lower bound the expectation Eθ Eη [(v̂ − vη )2 | η] . If this expectation is
large for some estimator v̂, then there must be some realization η, which induces a large error least one function η(x, a)
which induces a large error Eη [(v̂ − vη )2 | η], as desired. Consequently, we focus in the proof on lower bounding the
expectation Eθ [·]. This expectation can be decomposed with the use of the inequality a2 ≥ (a + b)2 /2 − b2 as follows:
i
h
i
h
i 1 h
Eθ Eη [(v̂ − vη )2 | η] ≥ Eθ Eη [(v̂ − Eθ [vη ])2 | η] − Eθ (vη − Eθ [vη ])2 .
2
Taking the worst case over all problems in the above inequality, we obtain
h
i
sup Eη [(v̂ − vη )2 ] ≥ sup Eθ Eη [(v̂ − vη )2 | η]
η

θ

i
h
i
1 h
≥ sup Eθ Eη [(v̂ − Eθ [vη ])2 | η] − sup Eθ (vη − Eθ [vη ])2 .
θ 2
θ
{z
} |
{z
}
|
T1

(26)

T2

This decomposition says that the expected MSE of an estimator in estimating vη can be related to the MSE of the same
estimator in estimating the quantity Eθ [vη ], as long as the variance of the quantity vη under the distribution generated by θ is
not too large. This is a very important observation, since we can now choose to instead study the MSE of an estimator in
estimating Eθ [vη ] as captured by T1 . Unlike the distribution Dη which is degenerate, this problem has a non-trivial noise
arising from the randomized draw of η according to θ. Thus we can use similar techniques as the proof of Theorem 3, albeit
where the reward distribution is a scaled Bernoulli instead of Gaussian. For now, we focus on controlling T1 , and T2 will be
handled later.
In order to bound T1 , we will consider two carefully designed choices θ1 and θ2 to induce two different problem instances
and show that T1 is large for any estimator under one of the two parameters. In doing this, it will be convenient to use the
additional shorthand `θ (v̂) = (v̂ − Eθ [vη ])2 . Proceeding as in the proof of Theorem 3, we have
h
i 1
h
i
1
sup Eθ Eη [(v̂ − Eθ [vη ])2 | η] = sup Eθ Eη [`θ (v̂) | η]
2 θ
2 θ


≥ sup Pθ (`θ (v̂) ≥ ) ≥
max Pθ (`θ (v̂) ≥ )
2 θ
2 θ∈θ1 ,θ2
h
i

≥ Pθ1 (`θ1 (v̂) ≥ ) + Pθ2 (`θ2 (v̂) ≥ ) .
4

T1 =

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Reduction to hypothesis testing As in the proof of Theorem 3, we now reduce the estimation problem into a hypothesis
test for whether the data is generated according to the parameter θ1 or θ2 . The arguments here are similar to the earlier proof,
so we will be terser in this presentation.
As before, our hypothesis test has entire knowledge of Dη as well as θ1 and θ2 . Consequently,
√we construct a test based on
picking θ1 whenever `θ1 (v̂) ≤ `θ2 (v̂). As before, we will ensure that |Eθ1 [vη ] − Eθ2 [vη ]| ≥ 2  so that for any estimator v̂,
we have
`θ1 (v̂) + `θ2 (v̂) ≥ 2.
Under this assumption, we can similarly conclude that the error of our hypothesis test is at most
Pθ1 (`θ1 (v̂) ≥ ) + Pθ2 (`θ2 (v̂) ≥ ) .
Invoking Le Cam’s argument Once again, we can lower bound the error rate of our test by invoking the result of Le
Cam. This requires an upper bound on the KL-divergence DKL (Pθ1 kPθ2 ). The only difference from our earlier argument is
that these distributions are now Bernoulli instead of Gaussian, based on the construction in Eq. (25). More formally, we have


X
X
p(r; θ1 (i))
DKL (Pθ1 kPθ2 ) =
log
p(r; θ1 (i))µ(Bi )
p(r; θ2 (i))
i∈I r∈{0,xii Ri }
h

i
= Eµ ξi DKL Ber(θ1 (i))  Ber(θ2 (i)) ,
(27)
where i is treated as a random variable under µ, and ξi is included, because the two distributions assign r = 0 with
probability one if ξi = 0.
Picking the parameters It remains to carefully choose θ1 and θ2 . We define θ2 (i) ≡ 0.5, and let θ1 (i) = θ2 (i) + ∆i ,
where ∆i will be chosen to satisfy certain constraints as before. Then, by Lemma 3, the KL divergence in Eq. (27) can be
bounded as

1 
DKL (Pθ1 kPθ2 ) ≤ Eµ ξi ∆2i .
4
It remains to choose ∆i . Following a similar logic as before, we seek to find a good feasible solution of the maximization
problem
max
∆

such that




√
Eµ ρ(x, a)ξi ∆i Ri ≥ 2 ,

1 
1
Eµ ξi ∆2i ≤ ,
4
n
0 ≤ ∆i ≤ 0.5.

(28)
(29)
(30)

For some α > 0 to be determined shortly, we set

∆i = min


ξi ρi Ri α
, 0.5 .
Eµ [ξi ρ2i Ri2 ]

p
The bound constraint (30) is satisfied by construction and we set α = 4Eµ [ξi ρ2i Ri2 ]/n to satisfy the constraint (29). To
obtain a feasible choice of , we bound Eµ [ρ(x, a)ξi ∆i Ri ] as follows:




Eµ ρ(x, a)ξi ∆i Ri ≥ Eµ ξi ρi ∆i Ri
"
#
ξi ρ2i Ri2 α1 ξi ρi Ri ≤ Eµ [ξi ρ2i Ri2 ]/2α
≥ Eµ
Eµ [ξi ρ2i Ri2 ]
 2 2
!
Eµ ξi ρi Ri 1(ξi ρi Ri > Eµ [ξi ρ2i Ri2 ]/2α)
=α 1−
Eµ [ξi ρ2i Ri2 ]
√
=: 2 .

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Collecting our arguments so far, we have established that

 
T1 ≥ · Pθ1 (`θ1 (v̂) ≥ ) + Pθ2 (`θ2 (v̂) ≥ )
4
 1
≥ · e−nDKL (Pθ1 kPθ2 )
4 8
 1

≥ ·
=
4 8e
32e

 !2
Eµ ξi ρ2i Ri2 1(ξi ρi Ri > Eµ [ξi ρ2i Ri2 ]/2α)
1 α2
=
1−
·
32e 4
Eµ [ξi ρ2i Ri2 ]

i 2
h

p
Eµ ξi ρ2i Ri2 1 ξi ρi Ri > nEµ [ξi ρ2i Ri2 ]/16
Eµ [ξi ρ2i Ri2 ] 
 .
=
1−
32en
Eµ [ξi ρ2i Ri2 ]
In order to complete the proof, we need to further upper bound T2 in the decomposition (26).
Bounding T2 We need to bound the
 supremum over
 all priors θ. Consider an arbitrary prior θ and assume that η is drawn
according to Eq. (25). To bound Eθ (vη − Eθ [vη ])2 , we view (vη − Eθ [vη ])2 as a random variable under θ and bound it
using Hoeffding’s inequality.
We begin by bounding its range. From the definition of η and vη ,
0 ≤ vη ≤ Eπ [ξi Ri ] = Eµ [ρ(x, a)ξi Ri ] ≤ (1 + δ)1/2 Eµ [ξi ρi Ri ] ,
so also 0 ≤ Eθ [vη ] ≤ (1 + δ)1/2 Eµ [ξi ρi Ri ]. Hence, |vη − Eθ [vη ]| ≤ (1 + δ)1/2 Eµ [ξi ρi Ri ], and we obtain the bound
(vη − Eθ [vη ])2 ≤ (1 + δ)(Eµ [ξi ρi Ri ])2 ≤ (1 + δ)Eµ [ξi ρ2i Ri2 ].

(31)

The proof proceeds by applying Hoeffding’s inequality to control the probability that (vη − Eθ [vη ])2 ≥ t2 for a suitable t.
Then we can, with high probability, use the bound (vη − Eθ [vη ])2 ≥ t2 , and with the remaining small probability apply the
bound of Eq. (31).
To apply Hoeffding’s inequality, we write vη explicitly as
X
X
vη =
µ(Bi )ρ0i ηi =:
Yi
i∈I

i∈I

ρ0i

where := Eµ [ρ(x, a) | (x, a) ∈ Bi ]. Thus, vη can be written as a sum of countably many independent variables, but we
can only apply Hoeffding’s inequality to their finite subset. Note that the variables Yi are non-negative and upper-bounded by
2 2
a summable series, namely Yi ≤ µ(Bi )ρ0i Ri , where the summability follows
P because Eµ [ρRmax ] ≤ 1 + Eµ [ρ Rmax ] < ∞.
This means that for any δ0 > 0, we can choose a finite set I0 such that i6∈I0 Yi ≤ δ0 . We will determine the sufficiently
small value of δ0 later; for now, consider the corresponding set I0 and define an auxiliary variable
X
vη0 :=
Yi ,
i∈I0

which by construction satisfies vη0 ≤ vη ≤ vη0 + δ0 . Note that the summands Yi can be bounded as
p
p
0 ≤ Yi ≤ ξi ρ0i Ri µ(Bi ) ≤ ξi (1 + δ)1/2 ρi Ri µ(Bi ) γ 0
p
√
because ρ0i ≤ (1 + δ)1/2 ρi and ξi µ(Bi ) ≤ ξi µ(Bi ) γ 0 , because ξi = 0 whenever µ(Bi ) > max{γ, δ} = γ 0 . By
Hoeffding’s inequality, we thus have
(
)
2t2
0
0
P
P(|vη − Eθ vη | ≥ t) ≤ 2 exp −
(1 + δ) i∈I0 ξi ρ2i Ri2 µ(Bi )γ 0


2t2
≤ 2 exp −
.
(1 + δ)γ 0 Eµ [ξi ρ2i Ri2 ]

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

Now take t =

p
γ 0 log(4/γ 0 )(1 + δ)Eµ [ξi ρ2i Ri2 ]/2 in the above bound, which yields

h
i
h
i γ0
P (vη0 − Eθ vη0 )2 ≥ t2 = P |vη0 − Eθ vη0 | ≥ t ≤
.
2
p
Now, we can go back to analyzing vη . We set δ0 sufficiently small, so t + δ0 ≤ γ 0 log(5/γ 0 )(1 + δ)Eµ [ξi ρ2i Ri2 ]/2. Thus,
using Eq. (31), we have
h
i


Eθ (vη − Eθ vη )2 ≤ (t + δ0 )2 · P (vη − Eθ vη )2 < (t + δ0 )2
h
i
+ (1 + δ)Eµ [ξi ρ2i Ri2 ] · P (vη − Eθ vη )2 ≥ (t + δ0 )2
h
i
≤ (t + δ0 )2 + (1 + δ)Eµ [ξi ρ2i Ri2 ] · P (vη0 − Eθ vη0 )2 ≥ t2
γ0
γ 0 log(5/γ 0 )(1 + δ)Eµ [ξi ρ2i Ri2 ]
+ (1 + δ)Eµ [ξi ρ2i Ri2 ] ·
2
2
≤ γ 0 log(5/γ 0 )(1 + δ)Eµ [ξi ρ2i Ri2 ] .
=

Combining this bound with the bound on T1 yields the theorem.
Lemma 1. Let Z := X × A be a subset of Rd , let µ be a probability measure on Z and Rmax and ρ be non-negative
measurable functions on Z. Given γ ∈ [0, 1], define a random variable ξγ (z) := 1(µ(z) ≤ γ). Then for any δ ∈ (0, 1],
there exists a countable index set I and disjoint sets Bi ⊆ Z alongside non-negative reals Ri , ρi and ξi ∈ {0, 1} such that
the following conditions hold:
• Sets Bi form a partition of Z, i.e., Z = ]i∈I Bi .
• Reals Ri and ρi approximate Rmax and ρ, and ξi equals ξγ as follows:
2
Rmax
(z) ∈ [Ri2 , (1 + δ)Ri2 ] ,

ρ2 (z) ∈ [ρ2i , (1 + δ)ρ2i ] ,

ξγ (z) = ξi

for all z ∈ Bi .

• Each set Bi either satisfies µ(Bi ) ≤ δ or consists of a single z ∈ Z.
Proof. Let Z := X × A. We begin our construction by separating out atoms, i.e., the elements z ∈ Z such that µ(z) > 0.
Specifically, we write Z = Z na ] Z a where Z a consists of atoms and Z na of all non-atoms. The set Z a is either finite or
countably infinite, so Z na is measurable.
By a theorem of Sierpiński (1922), since µ does not have any atoms on Z na , it must be continuous on Z na in the sense that
if A is a measurable subset of Z na with µ(A) = a then for any b ∈ [0, a], there exists a measurable set B ⊆ A such that
na
µ(B) = b. This means that we can decompose Z na into N := d1/δe sets Z1na , Z2na , . . . , ZN
such that each has a measure at
UN
na
na
most δ and Z = j=1 Zj .
We next ensure the approximation properties for Rmax and ρ. We begin by a countable decomposition of non-negative reals.
We consider the countable index set J := Z ∪ {−∞} and define the sequence aj := (1 + δ)j/2 , for j ∈ Z. Positive reals
can then be decomposed into the following intervals indexed by J :
I−∞ := {0} ,

Ij := (aj , aj+1 ] for j ∈ Z.

It will also be convenient to set a−∞ := 0. Thus, the construction of Ij guarantees that for all j ∈ J and all t ∈ Ij we have
a2j ≤ t2 ≤ (1 + δ)a2j .
The desired partition, with the index set I = Z a ∪ [N ] × J 2 , is as follows:
for i = z ∈ Z a :

Bi := {z}, Ri := Rmax (z), ρi := ρ(z), ξi := ξγ (z);
2

for i = (j, jR , jρ ) ∈ [N ] × J :

−1
Bi := Zjna ∩ Rmax
(IjR ) ∩ ρ−1 (Ijρ ),

Ri := ajR , ρi := ajρ , ξi := 1.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

B. Proof of Theorem 2
Let Ax := {a ∈ A : ρ(x, a) ≤ τ }. For brevity, we write Ai := Axi . We decompose the mean squared error into the squared
bias and variance and control each term separately,

2
MSE(v̂SWITCH ) = E[v̂SWITCH ] − v π  + Var[v̂SWITCH ].
We first calculate the bias. Note that bias is incurred only in the terms that fall in Acx , so




X
X
E[v̂SWITCH ] − v π = E 
r̂(x, a)π(a|x) − E 
E[r|x, a] π(a|x)
a∈Acx

a∈Acx

h

i

= Eπ r̂(x, a) − E[r|x, a] 1(a ∈ Acx )


= Eπ (x, a) 1(ρ > τ )
where we recall that (x, a) = r̂(x, a) − E[r|x, a].
Next we upper bound the variance. Note that the variance contributions from the IPS part and the DM part are not
independent, since the indicators ρ(xi , a) > τ and ρ(xi , a) ≤ τ are mutually exclusive. To simplify the analysis, we use the
following inequality that holds for any random variable X and Y :
Var(X + Y ) ≤ 2Var(X) + 2Var(Y ).
This allows us to calculate the variance of each part separately.
" n
#
" n
#
1X
1 XX
Var[v̂SWITCH ] ≤ 2 Var
[ri ρi 1(ai ∈ Ai )] + 2 Var
r̂(xi , a)π(a|xi )1(a ∈ Aci )
n i=1
n i=1
a∈A


X

 2
2
= Varµ rρ1(a ∈ Ax ) + Var
r̂(x, a)π(a|x)
n
n
a∈Acx


X



 2

 2
2
= Eµ Var rρ1(a ∈ Ax )  x, a + Varµ E rρ1(a ∈ Ax )  x, a + Var
r̂(x, a)π(a|x)
n
n
n
a∈Acx
"
#
h
i 2
 X
2


 2
2
2
r̂(x, a)π(a|x)
≤ Eµ Var rρ1(a ∈ Ax )  x, a + Eµ E[rρ1(a ∈ Ax ) | x, a] + E
n
n
n
a∈Acx
"
#
 X
2
 2 2
 2
 2
 2
2
2
≤ Eµ σ ρ 1(a ∈ Ax ) + Eµ Rmax ρ 1(a ∈ Ax ) + E
r̂(x, a)π(a|x)
.
n
n
n
c
a∈Ax

To complete the proof, note that the last term is further upper bounded using Jensen’s inequality as
"
#
"
#
 X
2
 X
2  X r̂(x, a)π(a|x) 2
P
E
r̂(x, a)π(a|x)
=E
π(a|x)
a∈Acx π(a|x)
a∈Acx
a∈Acx
a∈Acx
"
#
 X
 X
2
≤E
π(a|x)
r̂(x, a) π(a|x)
a∈Acx

a∈Acx

 2

≤ Eπ Rmax
1(ρ > τ ) ,
where the final inequality uses

P

a∈Acx

π(a|x) ≤ 1 and r̂(x, a) ∈ [0, Rmax (x, a)] almost surely.

Combining the bias and variance bounds, we get the stated MSE upper bound.

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits

C. Utility Lemmas
Lemma 2 (Hoeffding, 1963, Theorem 2). Let Xi ∈ [ai , bi ] and X1 , ..., Xn are drawn independently. Then the empirical
mean X̄ = n1 (X1 + ... + Xn ) obeys
− Pn

P(|X̄ − E[X̄]| ≥ t) ≤ 2e

2n2 t2
(bi −ai )2

i=1

.

Lemma 3 (Bernoulli KL-divergence). For 0 < p, q < 1, we have
1
1
DKL (Ber(p)kBer(q)) ≤ (p − q)2 ( +
).
q
1−q
Proof.
 


p
1−p
+ (1 − p) log
q
1−q
q−p
(p − q)2
(p − q)2
p−q
+ (1 − p)
=
+ (p − q) +
+ (q − p)
≤p
q
1−q
q
1−q


1
1
= (p − q)2
+
.
q
1−q

DKL (Ber(p)kBer(q)) = p log

D. Additional Figures from the Experiments

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits
IPS
10

DR

SWITCH-DR

oracle-SWITCH-DR

ecoli: n = 336, d = 7, k = 8

10

-1

10

-2

10

-3

oracle-Trim/TrunIPS

ecoli: n = 336, d = 7, k = 8

MSE
10

-2

10

-3

100

150

200

250

300

100

150

200

n

-1

10

-2

10

-3

100

glass: n = 214, d = 9, k = 6

120

140

160

180

200

10

-1

10

-2

10

-3

100

n

glass: n = 214, d = 9, k = 6

120

140

160

(d) glass / noisy reward

page-blocks: n = 5473, d = 10, k = 5

100

10-2

10-2

page-blocks: n = 5473, d = 10, k = 5

MSE

10-1

MSE

10-1

10-3

10-3

10-4

10-4

10-5 2
10

10-5 2
10

103

n

100

103

n

(e) page-blocks / deterministic reward

(f) page-blocks / noisy reward

satimage: n = 6435, d = 36, k = 6

100

satimage: n = 6435, d = 36, k = 6

MSE

10-1

MSE

180

n

(c) glass / deterministic reward
100

10-1

10-2

10-2 2
10

300

(b) ecoli / noisy reward

MSE

10

250

n

(a) ecoli / deterministic reward

MSE

SWITCH-DR-magic

-1

MSE

10

0

DM

103

n

(g) satimage / deterministic reward

10-3 2
10

103

n

(h) satimage / noisy reward

200

Optimal and Adaptive Off-policy Evaluation in Contextual Bandits
IPS

DR

SWITCH-DR

oracle-SWITCH-DR

pendigits: n = 10992, d = 16, k = 10

100

MSE

MSE

100

DM

10-1

10-2 2
10

103

oracle-Trim/TrunIPS

pendigits: n = 10992, d = 16, k = 10

10-1

10-2 2
10

104

103

(a) pendigits / deterministic reward

(b) pendigits / noisy reward

letter: n = 20000, d = 16, k = 26

letter: n = 20000, d = 16, k = 26

10

-1

-1

MSE

MSE

104

n

n

10

SWITCH-DR-magic

10-2

10

-2

102

103

104

102

103

n

(c) letter / deterministic reward
10

(d) letter / noisy reward

vehicle: n = 846, d = 18, k = 4

10

-1

10

-2

10

-3

vehicle: n = 846, d = 18, k = 4

MSE

-1

MSE

10

0

10

-2

10

-3

100

200

300

400

500 600 700 800

100

200

300

n

10

0

(f) vehicle / noisy reward

wdbc: n = 569, d = 30, k = 2

10

-1

-2

100

500 600 700 800

0

10

-1

10

-2

10

-3

wdbc: n = 569, d = 30, k = 2

MSE

MSE

10

400

n

(e) vehicle / deterministic reward

10

104

n

200

300

400

n

(g) wdbc / deterministic reward

500

100

200

300

n

(h) wdbc / noisy reward

400

500

