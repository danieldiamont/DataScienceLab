Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

Simultaneous Learning of Trees and Representations for Extreme Classification
with Application to Language Modeling
(Supplementary material)

(n)

9. Geometric interpretation of probabilities pj

h
K1
K2
100

Discrete:
(n)
6
p1 = 12
= 0.5
(n)
(n)
3
p1|1 = 3 = 1,
p1|2 =

100
70

100
100
70

70 100
100

100

3
3

= 1,

(n)

0
3

p1|3 =

= 0,

(n)

p1|4 =

0
3

=0

Continuous:
(n)
1
(σ(100) + σ(70) + . . . + σ(−70) + σ(−100)) ≈ 0.5
p1 = 12
(n)
1
p1|1 = 3 (σ(100) + σ(70) + σ(100)) ≈ 1

K3

70 100

(n)

and pj|i

(n)

p1|2 = 13 (σ(100) + σ(70) + σ(100)) ≈ 1
(n)

p1|3 = 13 (σ(−100) + σ(−70) + σ(−100)) ≈ 0
(n)

p1|4 = 31 (σ(−100) + σ(−70) + σ(−100)) ≈ 0

K4

(n)

(n)

Figure 3. The comparison of discrete and continuous definitions of probabilities pj and pj|i on a simple example with K = 4 classes
and binary tree (M = 2). n is an exemplary node, e.g. root. σ denotes sigmoid function. Color circles denote data points.
(n)

Remark 3. One could define pj

as the ratio of the number of examples that reach node n and are sent to its j th child
(n)

to the total the number of examples that reach node n and pj|i as the ratio of the number of examples that reach node n,
correspond to label i, and are sent to the j th child of node n to the total the number of examples that reach node n and
correspond to label i. We instead look at the continuous counter-parts of these discrete definitions as given by Equations 8
and 9 and illustrated in Figure 3 (note that continuous definitions have elegant geometric interpretation based on margins),
which simplifies the optimization problem.

10. Theoretical proofs
Proof of Lemma 1. Recall the form of the objective defined in 6:

Jn

=
=

K
M

2 X (n)  X (n)
(n)
|pj − pj|i |
qi
M i=1
j=1
h
i
2
(n)
Ei∼q(n) fnJ (i, p·|· , q (n) )
M

Where:
(n)

fnJ (i, p·|· , q (n) )

=

M 
M 
K
 X

X
 (n)
 (n) X (n) (n) 
(n) 
qi0 pj|i0 
pj − pj|i  =
pj|i −
j=1

=

j=1

M X
K

X

(n) (n) 
(1i=i0 − qi0 )pj|i0 

j=1 i0 =1

i0 =1

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

Hence:

(n)

∂fnJ (i, p·|· , q (n) )

(n)

(n)

(n)

= (1 − qi ) sign(pj|i − pj )

(n)

∂pj|i
And:

(n)

(n)

∂fnJ (i, p·|· , q (n) )
(n)

(n)

(n)

(n)

(1 − qi ) sign(pj|i − pj )

=

∂ log pj|i

(n)

(n)

(n)

∂pj|i

(n)

∂ log pj|i
(n)

(1 − qi ) sign(pj|i − pj )pj|i

=

By assigning each label j to a specific child i under the constraint that no child has more than L labels, we take a step in
the direction ∂E ∈ {0, 1}M ×K , where:
∀i ∈ [1, K],

PM

∂Ej,i = 1
and
PK
∀j ∈ [1, M ],
i=1 ∂Ej,i ≤ L
j=1

Thus:
∂Jn
(n)

∂E

=

∂p·|·

·|·

=
And:

i
h
(n) (n)
J
E
(i,
p
,
q
)
f
(n)
n
·|·
2 i∼q
∂E
(n)
M
∂p
2
M

K
X

(n)

(n)

qi (1 − qi )

i=1

M 
X

(n)

(n)

sign(pj|i − pj )∂Ej,i



(13)

j=1

M 
K

X
2 X (n)
(n)
(n)
(n) (n)
q
(1
−
q
)
sign(p
−
p
)p
∂E
∂E
=
j,i
i
i
j
j|i
j|i
(n)
M i=1
∂ log p
j=1

∂Jn

(14)

·|·

If there exists such an assignment for which 13 is positive, then the greedy method proposed in 2 finds it. Indeed, suppose
that Algorithm 2 assigns label i to child j and i0 to j 0 . Suppose now that another assignment ∂E 0 sends i to j 0 and i to j 0 .
Then:
  ∂J
∂Jn 
∂Jn   ∂Jn
∂Jn 
n
∂E − ∂E 0 =
(15)
+ (n) −
+ (n)
(n)
(n)
(n)
∂p·|·
∂pj|i
∂pj 0 |i0
∂pj|i0
∂pj 0 |i
Since the algorithm assigns children by descending order of
∂Jn
(n)
∂pj|i

Hence:

≥

∂Jn
(n)
∂pj|i0

∂Jn 
(n)

∂Jn
(n)
∂pj|i

and

until a child j is full, we have:
∂Jn
(n)
∂p0 j|i0

≥

∂Jn
(n)

∂pj 0 |i


∂E − ∂E 0 ≥ 0

∂p·|·

Thus, the greedy algorithm finds the assignment that most increases Jn most under the children size constraints.
Moreover,

∂Jn
(n)
∂p·|·

is always positive for L ≤ M or L ≥ 2M (M − 2).

Proof of Lemma 2. Both Jn and JT are defined as the sum of non-negative values which gives the lower-bound. We next
derive the upper-bound on Jn . Recall:


M K
M K
K

2 X X (n) (n)
2 X X (n) X (n) (n)
(n)
(n) 
Jn =
qi |pj − pj|i | =
qi 
ql pj|l − pj|i 


M
M
j=1 i=1

j=1 i=1

l=1

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

PK (n) (n)
(n)
since pj =
l=1 ql pj|l . The objective Jn is maximized on the extremes of the [0, 1] interval. Thus, define the
following two sets of indices:
(n)

Oj = {i : i ∈ {1, 2, . . . , K}, pj|i = 1}

and

(n)

Zj = {i : i ∈ {1, 2, . . . , K}, pj|i = 0}.

We omit indexing these sets with n for the ease of notation. We continue as follows




M
X (n)
X (n) X (n)
2 X  X (n) 
ql  +
qi
ql 
qi
1−
Jn ≤
M j=1
i∈Zj
i∈Oj
l∈Oj
l∈Oj


2 
M
4 X  X (n)  X (n)  
qi −
qi
=


M j=1
i∈Oj


=

M
X

i∈Oj



4 

1 −
M
j=1

X

2 

(n)
qi   ,

i∈Oj

PM (n)
PK (n) (n) P
(n)
(n)
where the last inequality is the consequence of the following: j=1 pj = 1 and pj = l=1 ql pj|l = i∈Oj qi ,
PM P
(n)
thus j=1 i∈Oj qi = 1. Apllying Jensen’s ineqality to the last inequality obtained gives

2

M
X
X (n)
1
4

− 4
qi 
M
M
j=1
i∈Oj


4
1
1−
M
M

≤

Jn

=
That ends the proof.

(n)

Proof of Lemma 3. We start from proving that if the split in node n is perfectly balanced, i.e. ∀j={1,2,...,M } pj
perfectly pure, i.e.

(n)
∀j={1,2,...,M } min(pj|i , 1
i={1,2,...,K}

−

(n)
pj|i )

= 0, then Jn admits the highest value Jn =

4
M

1−

1
M



=

1
M,

and

. Since the

split is maximally balanced we write:


M K
2 X X (n)  1
(n) 
Jn =
q
− pj|i  .
M j=1 i=1 i  M
(n)

Since the split is maximally pure, each pj|i can only take value 0 or 1. As in the proof of previous lemma, define two sets
of indices:
(n)
(n)
Oj = {i : i ∈ {1, 2, . . . , K}, pj|i = 1}
and
Zj = {i : i ∈ {1, 2, . . . , K}, pj|i = 0}.
We omit indexing these sets with n for the ease of notation. Thus



 X
M
X
X
2
1
(n)
(n) 1 

Jn =
qi
1−
+
qi
M j=1
M
M
i∈Oj
i∈Zj





M
X
X
X
2
1
1 
(n)
(n)

=
qi
1−
+
1−
qi 
M j=1
M
M
i∈Oj

 M
2
2 X X (n)
2
1−
qi +
M
M j=1
M
i∈Oj


4
1
1−
,
M
M


=
=

i∈Oj

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

where the last equality comes from the fact that
PM P
(n)
= 1.
j=1
i∈Oj qi

PM

j=1

(n)

pj

(n)

= 1 and pj

=

(n) (n)
l=1 ql pj|l

PK

=

P

i∈Oj

(n)

qi , thus

Thus we are done with proving one induction direction. Next we prove that if Jn admits the highest value Jn =

(n)
4
1
1
= M
, and perfectly pure, i.e.
M 1 − M , then the split in node n is perfectly balanced, i.e. ∀j={1,2,...,M } pj
(n)

(n)

∀j={1,2,...,M } min(pj|i , 1 − pj|i ) = 0.
i={1,2,...,K}
(n)

Without loss of generality assume each qi ∈ (0, 1). The objective Jn is certainly maximized in the extremes of the
(n)
(n)
interval [0, 1], where each pj|i is either 0 or 1. Also, at maximum it cannot be that for any given j, all pj|i ’s are 0 or all
(n)

pj|i ’s are 1. The function J(h) is differentiable in these extremes. Next, define three sets of indices:

Aj = {i :

K
X

(n) (n)

(n)

qi pj|l ≥ pj|i }

Bj = {i :

and

l=1

K
X

(n) (n)

(n)

qi pj|l < pj|i }

Cj = {i :

and

l=1

K
X

(n) (n)

(n)

qi pj|l > pj|i }.

l=1

We omit indexing these sets with n for the ease of notation. Objective Jn can then be re-written as


M
2 X  X (n)
qi
Jn =
M j=1
i∈Aj

K
X

!
(n) (n)

(n)

qi pj|l − pj|i

+2

X

(n)

qi

(n)

pj|i −

i∈Bj

l=1

K
X

!
(n) (n)

qi pj|l

,

l=1

(n)

We next compute the derivatives of Jn with respect to pj|z , where z = {1, 2, . . . , K}, everywhere where the function is
differentiable and obtain

∂Jn
(n)
∂pj|z

(
=

(n)
(n) P
2qz ( i∈Cj qi − 1)
P
(n)
(n)
2qz (1 − i∈Bj qi )

if z ∈ Cj
if z ∈ Bj

,

P
P
(n)
(n)
Note that in the extremes of the interval [0, 1] where Jn is maximized, it cannot be that i∈Cj qi = 1 or i∈Bj qi = 1
thus the gradient is non-zero. This fact and the fact that Jn is convex imply that Jn can only be maximized at the extremes
of the [0, 1] interval. Thus if Jn admits the highest value, then the node split is perfectly pure. We still need to show that
if Jn admits the highest value, then the node split is also perfectly balanced. We give a proof by contradiction, thus we
(n)
(n)
(n)
1
1
assume that at least for one value of j, pj 6= M
, or in other words if we decompose each pj as pj = M
+ xj , then at
least for one value of j, xj 6= 0. Lets once again define two sets of indices (we omit indexing xj and these sets with n for
the ease of notation):

(n)

Oj = {i : i ∈ {1, 2, . . . , K}, pj|i = 1}

and

(n)

Zj = {i : i ∈ {1, 2, . . . , K}, pj|i = 0},

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation
(n)

and recall that pj

=

(n) (n)
l=1 ql pj|l

PK
4
M

=

P

(n)

qi . We proceed as follows


M
X (n) (n)
2 X  X (n)
(n)
=
qi (1 − pj ) +
qi pj 
M j=1

i∈Oj



1
1−
= Jn
M

i∈Oj

=

=

=

=

=

<

i∈Zj

M h
X

2
M

i
(n)
(n)
(n)
(n)
pj (1 − pj ) + pj (1 − pj )

j=1

M
i
4 X h (n)
(n)
pj − (pj )2
M j=1


M
X
4 
(n) 2 
1−
(pj )
M
j=1


2
M 
X
4 
1
1−
+ xj 
M
M
j=1


M
M
X
4 
1
2 X
1−
−
xj −
x2j 
M
M
M j=1
j=1


4
1
1−
M
M

Thus we obtain the contradiction which ends the proof.
(n)

Proof of Lemma 4. Since we node that the split is perfectly pure, then each pj|i is either 0 or 1. Thus we define two sets
(n)

Oj = {i : i ∈ {1, 2, . . . , K}, pj|i = 1}
and thus

(n)

Zj = {i : i ∈ {1, 2, . . . , K}, pj|i = 0}.

and



M
X (n)
2 X  X (n)
qi (1 − pj ) +
qi pj 
Jn =
M j=1
i∈Oj

Note that pj =

(n)
i∈Oj qi .

P

i∈Zj

Then



M
M
M
X
X
X
4
4 
2
p2j 
[pj (1 − pj ) + (1 − pj )pj ] =
pj (1 − pj ) =
1−
Jn =
M j=1
M j=1
M
j=1
and thus

M
X

p2j = 1 −

j=1

Lets express pj as pj =

1
M

1
+ j , where j ∈ [− M
,1 −

M
X
j=1

since

2
M

PM

j=1 j

p2j

1
M ].

M Jn
.
4

Then

2
M 
M
M
M
X
X
X
1
1
2 X
1
=
+ j
=
+
j +
2j =
+
2j ,
M
M
M
M
j=1
j=1
j=1
j=1

= 0. Thus combining Equation 16 and 17
M

(16)

X
1
M Jn
+
2j = 1 −
M j=1
4

(17)

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

and thus

M
X

1
M Jn
−
.
M
4

2j = 1 −

j=1

The last statement implies that
r
max

j=1,2,...,M

1−

j ≤

1
M Jn
−
,
M
4

which is equivalent to
1
1
min pj =
− max j ≥
−
j
j=1,2,...,M
M
M

r

1
M Jn
1
1−
−
=
−
M
4
M

p

M (J ∗ − Jn )
.
2

Proof of Lemma 5. Since the split is perfectly balanced we have the following:




M K
K M
2 X X (n)  1
2 X X (n)  1
(n) 
(n) 
Jn =
− pj|i  =
− pj|i 
qi 
qi 
M j=1 i=1
M
M i=1 j=1
M
Define two sets
(n)

Ai = {j : j ∈ {1, 2, . . . , K}, pj|i <

1
}
M

and

(n)

Bi = {j : j ∈ {1, 2, . . . , K}, pj|i ≥

1
}.
M

Then
Jn

=

=

=




 X


K
2 X  X (n) 1
1 
(n)
(n)
(n)
qi
− pj|i +
qi
pj|i −
M i=1
M
M
j∈Ai
j∈Bi



 X

K
1
1 
2 X (n)  X
(n)
(n)
q
− pj|i +
pj|i −
M i=1 i
M
M
j∈Ai
j∈Bi



 X

K
X
X
1
1
2
(n)
(n)
(n)
q 
− pj|i +
(1 −
) − (1 − pj|i ) 
M i=1 i
M
M
j∈Ai

j∈Bi

Recall that the optimal value of Jn is:
J∗ =

4
M


1−

1
M







N
N
X
X
X
1
2
1
2
(n)
(n)
+ 1−
=
qi
(M − 1)
=
qi 
M i=1
M
M
M i=1

j∈Ai ∪Bi

(n)

(n)






1
1 
1 
−
+ 1−
M
M
M

Note Ai can have at most M − 1 elements. Furthermore, ∀j ∈ Ai , pj|i < 1 − pj|i . Then, we have:





K
X
X (n) X 
1
1
1
2
1
(n)
(n)

q 
pj|i +
− (1 −
) −
+ 1−
J∗ − Jn =
(1 − pj|i ) +
M i=1 i
M
M
M
M
j∈Ai

j∈Bi

Hence, since Bi has at least one element:
J∗ − Jn

≥

≥
≥



K


X
X
X
2
(n)
(n)
(n)
q 
pj|i +
1 − pj|i 
M i=1 i
j∈Ai
j∈Bi


K
M
X
X
2
(n)
(n)
(n)
q 
min(pj|i , 1 − pj|i )
M i=1 i
j=1
2α

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

Proof of Theorem 1. Let the weight of the tree leaf be defined as the probability that a randomly chosen data point x drawn
from some fixed target distribution P reaches this leaf. Suppose at time step t, n is the heaviest leaf and has weight w.
Consider splitting this leaf to M children n1 , n2 , . . . , nM . Let the weight of the j th child be denoted as wj . Also for the
Pm
(n)
(n)
ease of notation let pj refer to pj (recall that j=1 pj = 1) and pj|i refer to pj|i , and furthermore let qi be the shorthand
P
P
(n)
K
K
for qi . Recall that pj =
j =wpj . Let q
i=1 qi pj|i and
i=1 qi = 1. Notice that for any j = {1, 2, . . . , M }, w
PK
th
e
be the k-element vector with i entry equal to qi . Define the following function: G̃ (q) = i=1 qi ln q1i . Recall the
 
P
PK (l)
1
e
, where L is a set of all tree leaves. Before
expression for the entropy of tree leaves: G = l∈L wl i=1 qi ln (l)
qi

e

(nj )

e

the split the contribution of node n to G was equal to wG̃ (q). Note that for any j = {1, 2, . . . , M }, qi

qi pj|i
is
pj
nj
let qi be

=

the probability that a randomly chosen x drawn from P has label i given that x reaches node nj . For brevity,
PM
denoted as qj,i . Let qj be the k-element vector with ith entry equal to qj,i . Notice that q = j=1 pj qj . After the split
PM
the contribution of the same, now internal, node n changes to w j=1 pj G̃e (qj ). We denote the difference between the
contribution of node n to the value of the entropy-based objectives in times t and t + 1 as


M
X
∆et := Get − Get+1 = w G̃e (q) −
pj G̃e (qj ) .
(18)
j=1

The entropy function G̃e is strongly concave with respect to l1 -norm with modulus 1, thus we extend the inequality given
by Equation 7 in (Choromanska et al., 2016) by applying Theorem 5.2. from (Azocar et al., 2011) and obtain the following
bound


M
X
∆et = w G̃e (q) −
pj G̃e (qj )
j=1
M

M

X
1X
pl ql k21
pj kqj −
2 j=1
l=1

!2
M
K
M
X  qi pj|i X
qi pl|i 
1X
w
pj
−
pl


 pj
2 j=1
pl 
i=1
l=1
!2

M
K
M

p
X
1X
 j|i X 
pj
qi 
−
pl|i 
w

 pj
2 j=1
i=1
l=1
!

 2
M
K
X
 pj|i

1X

w
pj
qi 
− 1
2 j=1
pj
i=1
!2
M
K

1 X 1 X 
w
qi pj|i − pj  .
2 j=1 pj i=1

≥ w

=

=

=

=

Before proceeding, we will bound each pj . Note that by the Weak Hypothesis Assumption we have


M
M
γ∈
min pj , 1 −
min pj ,
2 j=1,2,...,M
2 j=1,2,...,M
thus
min

j=1,2,...,M

thus all pj s are such that pj ≥

2γ
M.

pj ≥

2γ
,
M

Thus
max

j=1,2,...,M

pj ≤ 1 −

2γ
M (1 − 2γ) + 2γ
(M − 1) =
.
M
M

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation
M (1−2γ)+2γ
.
M

Thus all pj s are such that pj ≤
∆et

≥

≥

!2
M
K
X

1 X 
M2

qi pj|i − pj
w
2[(M (1 − 2γ) + 2γ] j=1 M i=1

2
M
K
X
X


M2
1

w
qi pj|i − pj 
2[(M (1 − 2γ) + 2γ] j=1 M i=1

=


2
M X
K
X


M2
2

w
qi pj|i − pj 
8[(M (1 − 2γ) + 2γ] M j=1 i=1

=

M2
wJn2
,
[(M (1 − 2γ) + 2γ] 8

where the last inequality is a consequence of Jensen’s inequality. w can further be lower-bounded by noticing the following
!
K
X
X
X X
1
(l)
≤
w
ln
K
≤
w
ln
K
1 = [t(M − 1) + 1]w ln K ≤ (t + 1)(M − 1)w ln K,
Get =
wl
qi ln
l
(l)
qi
i=1
l∈L
l∈L
l∈L
where the first inequality results from the fact that uniform distribution maximizes the entropy.
This gives the lower-bound on ∆et of the following form:
∆et ≥

M 2 Get Jn2
,
8(t + 1)[M (1 − 2γ) + 2γ](M − 1) ln K

and by using Weak Hypothesis Assumption we get
∆et ≥≥

M 2 Get γ 2
8(t + 1)[M (1 − 2γ) + 2γ](M − 1) ln K

Following the recursion of the proof in Section 3.2 in (Choromanska et al., 2016) (note that in our case Ge1 ≤ 2(M −
1) ln K), we obtain that under the Weak Hypothesis Assumption, for any κ ∈ [0, 2(M − 1) ln K], to obtain Get ≤ κ it
suffices to make
−1) ln K

 16[M (1−2γ)+2γ](M
M 2 log2 eγ 2
2(M − 1) ln K
t≥
κ
splits. We next proceed to directly proving the error bound. Denote w(l) to be the probability that a data point x reached
(l)
(l)
leaf l. Recall that qi is the probability that the data point x corresponds to label i given that x reached l, i.e. qi =
P (y(x) = i|x reached l). Let the label assigned to the leaf be the majority label and thus lets assume that the leaf is
(l)
(l)
assigned to label i if and only if the following is true ∀z={1,2,...,k} qi ≥ qz . Therefore we can write that
z6=i

(T ) =

K
X

P (t(x) = i, y(x) 6= i)

(19)

i=1

=

X

w(l)

X
l∈L

=

X

P (t(x) = i, y(x) 6= i|x reached l)

i=1

l∈L

=

K
X

w(l)

K
X

P (y(x) 6= i|t(x) = i, x reached l)P (t(x) = i|x reached l)

i=1
(l)

(l)

(l)

w(l)(1 − max(q1 , q2 , . . . , qK ))

X
l∈L

P (t(x) = i|x reached l)

i=1

l∈L

=

K
X

(l)

(l)

(l)

w(l)(1 − max(q1 , q2 , . . . , qK ))

(20)

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

Consider again the Shannon entropy G(T ) of the leaves of tree T that is defined as
Ge (T ) =

X

w(l)

K
X

(l)

qi log2

i=1

l∈L

1
(l)

.

(21)

qi

(l)

Let il = arg maxi={1,2,...,K} qi . Note that
Ge (T )

X

=

w(l)

X

w(l)

X

w(l)

K
X

(l)

qi

1

(l)

qi log2

i=1
i6=il

l∈L

≥

K
X

1

(l)

qi log2

i=1

l∈L

≥

K
X

(l)

qi

(l)

qi

i=1
i6=il

l∈L

(l)

=

X

=

(T ),

(l)

(l)

w(l)(1 − max(q1 , q2 , . . . , qK ))

l∈L

(22)
(l)

where the last inequality comes from the fact that ∀i={1,2,...,K} qi
consequently ∀i={1,2,...,K} log2
i6=il

1
(l)
qi

≤ 0.5 and thus ∀i={1,2,...,K}

i6=il

i6=il

1
(l)
qi

∈ [2; +∞] and

∈ [1; +∞].

We next use the proof of Theorem 6 in (Choromanska et al., 2016). The proof modifies only slightly for our purposes and
thus we only list these modifications below.
• Since we define the Shannon entropy through logarithm with base 2 instead of the natural logarithm, the right hand
side of inequality (2.6) in (Shalev-Shwartz, 2012) should have an additional multiplicative factor equal to ln12 and
thus the right-hand side of the inequality stated in Lemma 14 has to have the same multiplicative factor.
• For the same reason as above, the right-hand side of the inequality in Lemma 9 should take logarithm with base 2 of
k instead of the natural logarithm of k.
Propagating these changes in the proof of Theorem 6 results in the statement of Theorem 1.

Proof of Corollary 1. Note that the lower-bound on ∆et from the previous prove could be made tighter as follows:
!2
M
K

1 X 1 X 
e

∆t ≥ w
qi pj|i − pj
2 j=1 pj i=1
!2
M
K

M 2 X 1 X 
qi pj|i − pj 
= w
2 j=1 M i=1

2
M
K

M 2 X 1 X 
qi pj|i − pj 
≥ w
2
M
j=1
i=1
2
M X
K
X


M  2
= w
qi pj|i − pj 
8
M j=1 i=1
2

=



M 2 wJn2
,
8

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

d

Model

Arity

Prec

Rec

Train

Test

TagSpace

-

30.1

-

3h8

6h

FastText

2

27.2

4.17

8m

1m

Huffman Tree

5
20

28.3
29.9

4.33
4.58

8m
10m

1m
3m

Learned Tree

5
20

31.6
32.1

4.85
4.92

18m
30m

1m
3m

TagSpace

-

35.6

-

5h32

15h

FastText

2

35.2

5.4

12m

1m

Huffman Tree

5
20

35.8
36.4

5.5
5.59

13m
18m

2m
3m

Learned Tree

5
20

36.1
36.6

5.53
5.61

35m
45m

3m
8m

50

200

Table 3. Classification performance on the YFCC100M dataset.
Model

perp.

train ms/batch

test ms/batch

Random Tree

172

5.1

2.7

Flat soft-max

151

11.5

5.1

Learned Tree

159

6.3

2.6

Table 4. Comparison of a flat soft-max to a 25-ary hierarchical soft-max (learned, random and heuristic-based tree).

where the first inequality was taken from the proof of Theorem 1 and the following equality follows from the fact that each
node is balanced. By next following exactly the same steps as shown in the proof of Theorem 1 we obtain the corollary.

11. Experimental Setting
11.1. Classification
For the YFCC100M experiments, we learned our models with SGD with a linearly decreasing rate for five epochs. We run
a hyper-parameter search on the learning rate (in {0.01, 0.02, 0.05, 0.1, 0.25, 0.5}). In the learned tree settings, the learning
rate stays constant for the first half of training, during which the AssignLabels() routine is called 50 times. We run the
experiments in a Hogwild data-parallel setting using 12 threads on an Intel Xeon E5-2690v4 2.6GHz CPU. At prediction
time, we perform a truncated depth first search to find the most likely label (using the same idea as in a branch-and-bound
algorithm: if a node score is less than that of the best current label, then all of its descendants are out).
11.2. Density Estimation
In our experiments, we use a context window size of 4. We optimize the objectives with Adagrad, run a hyper-parameter
search on the batch size (in {32, 64, 128}) and learning rate (in {0.01, 0.02, 0.05, 0.1, 0.25, 0.5}). The hidden representation dimension is 200. In the learned tree settings, the AssignLabels() routine is called 50 times per epoch. We used a
12GB NVIDIA GeForce GTX TITAN GPU and all tree-based models are 65-ary for the Gutenberg data and 25-ary for
Pen TreeBank. Table 4 provides the perplexity and speed results on the PTB text.
For the Cluster Tree, we learn dimension 50 word embeddings with FastTree for 5 epochs using a hierarchical softmax loss,
then obtain 45 = 652 centroids using the ScikitLearn implementation of MiniBatchKmeans, and greedily assign words to
clusters until full (when a cluster has 65 words).

Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation

Algorithm 3 Label Assignment Algorithm under Depth Constraint
Input Node statistics, max depth D
Paths from root to labels: P = (ci )K
i=1
node ID n and depth d
List of labels currently reaching the node
Ouput Updated paths
Lists of labels now assigned to each of n’s
children under depth constraints
procedure AssignLabels (labels, n, d)
(n)
(n)
// first, compute pj and pj|i .  is the element-wise
// multiplication
pavg
←0
0
count ← 0
for i in labels do
pavg
← pavg
+ SumProbasn,i
0
0
count ← count + Countsn,i
pavg
← SumProbasn,i /Countsn,i
i
avg
p0 ← pavg
0 /count

Leaf 229
suggested
watched
created
violated
introduced
discovered
carried
described
accepted
listed
...

Leaf 230
vegas
&
calif.
park
n.j.
conn.
pa.
pa.
ii
d.
...

// then, assign each label to a child of n under depth
// constraints
unassigned ← labels
full ← ∅
for j = 1 to M do
assignedj ← ∅
while
..unassigned 6= ∅ do
∂Jn
(n) is given in Equation 10
∂pj|i


∂Jn
(i∗ , j ∗ ) ←
argmax
(n)
∗

i∈unassigned,j6∈full

∂pj|i

cid ← (n, j ∗ )
assignedj ∗ ← assignedj ∗ ∪ {i∗ }
unassigned ← unassigned \ {i∗ }
if |assignedj ∗ | = M D−d then
full ← full ∪ {j ∗ }
for j = 1 to M do
AssignLabels (assignedj , childn,j , d + 1)
return assigned

Leaf 300
payments
buy-outs
swings
gains
taxes
operations
profits
penalties
relations
liabilities
...

Leaf 231
operates
includes
intends
makes
means
helps
seeks
reduces
continues
fails
...

Table 5. Example of labels reaching leaf nodes in the final tree. We can identify a leaf for 3rd person verbs, one for past participates, one
for plural nouns, and one (loosely) for places.

