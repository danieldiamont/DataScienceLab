Supplementary Materials for
Understanding Synthetic Gradients and Decoupled Neural Interfaces

A. Relation to critic methods
Instead of estimating the gradient directly, one could estimate loss instead (thus use some trainable φ(h|θ) ≈
E[L|h]) and then use its gradient wrt. to its inputs (∂φ/∂h)
as a surrogate for the synthetic gradient. These kind
of approaches are known in Reinforcement Learning as
critic methods (Fairbank, 2014; Heess et al., 2015), but
in terms of gradient approximation they do not guarantee
any alignment between these signals if only critic is a nonlinear function. As an example lets consider a function
φ(h(xi )|θ) = L(h(xi )) for every xi , such that it is constant (in terms of its output) in some  balls around each
xi . As a consequence gradients of φ are 0 everywhere, yet
as a critic it receives no learning signal (since loss is approximated perfectly). This example shows that in general
alignment between critic gradient and true gradient can be
arbitrary, and completely independent from the loss error
itself.

B. Additional examples
Critical points
We can show an example of SG introducing new critical
points. Consider a small one-dimensional training dataset
{−2, −1, 1, 2} ⊂ R, and let us consider a simple system
where the model f : R → R is parametrised with two
scalars, a and b and
P4 produces ax + b. We train it to minimise L(a, b) = i=1 |axi + b|. This has a unique minimum which is obtained for a = b = 0, and standard gradient based methods will converge to this solution. Let us
now attach a SG module betweenf and L. This module
produces a (trainable) scalar value c ∈ R (thus it produces
a single number, independent from the input). Regardless
of the value of a, we have a critical point of the SG module
when b = 0 and c = 0. However, solutions with a = 1
and c = 0 are clearly not critical points of the original system. Figure 6 shows the loss surface and the fitting of SG
module when it introduces new critical point.

Figure 6. Left: The loss surface with a white marker represents
critical point of the original optimisation and white line a set of
critical points of SG based one. Right: A situation when SG finds
a solution d = 0 which introduces new critical point, which is not
a critical point of the original problem.

loss. If one chooses the learning rate of the SG module
using line search, then in every iteration there exists small
enough, positive learning rate of the main network such
that it converges to the global solution.
Proof. Let X = {xs }Ss=1 ∈ Rd×S be the data, let
{ys }Ss=1 ∈ R1×S be the labels. Throughout the proof k
will be the iteration of training.
We denote by 1 ∈ R1×S a row vector in which every element is 1. We also follow the standard convention of including the bias in the weight matrix by augmenting the
data X with one extra coordinate always equal to 1. Thus,
we denote X̄ = (XT |1T )T , X̄ ∈ R(d+1)×S and x̄s -the
columns of X̄. Using that convention, the weight matrix is
Wk ∈ R1×(d+1) . We have
psk := Wk x̄s ,
S

L=

n

1X s
1X s
2
2
(y − psk ) =
(y − Wk x̄s ) .
2 s=1
2 i=1

Our aim is to find
arg min L.
W,b

C. Proofs
Theorem 1 Let us consider linear regression trained with
a linear SG module attached between its output and the

We use
S

X ∂L ∂ps
∂L
∂L ∂p
=
=
=
∂W
∂p ∂W s=1 ∂ps ∂W

Understanding Synthetic Gradients and DNIs
S
S
X
∂L s X s
x̄
=
(y − Wk x̄s ) (x̄s )T
s
∂p
s=1
s=1


∂L
= p1 − y 1 , . . . , pS − y S
∂p
We will use the following parametrization of the synthetic
g k = (αk +1)pk −(βk +1)y+γk 1. The reason
gradient ∇L
for using this form instead of simply ak pk + bk y + ck 1 is
that we are going to show that under DNI this synthetic gradient will converge to the “real gradient” ∂L
∂p , which means
showing that lim (αk , βk , γk ) = (0, 0, 0). Thanks to this
k→∞

choice of parameters αk , βk , γk we have the simple expression for the error
2


∂L 
g
 =

Ek = ∇Lk −
∂p 2
k(αk + 1)pk − (βk + 1)y + γk 1−
2
p1k − y 1 , . . . , pSk − y S 2 =


 αk p1k − βk y 1 + γk , . . . , αk pSk − βk y S + γk 2
2
Parameters αk , βk , γk will be updated using the gradient
descent minimizing the error E. We have
S

X
∂E
(αk psk − βk y s + γk )psk
=
∂α
s=1
S
X
∂E
(αk psk − βk y s + γk )y s
=−
∂β
s=1
S

X
∂E
(αk psk − βk y s + γk ).
=
∂γ
s=1
As prescribed in Jaderberg et al. (2016), we start our iterative procedure from the synthetic gradient being equal to
zero and we update the parameters by adding the (negative)
gradient multiplied by a learning rate ν. This means that we
apply the iterative procedure:
α0 = −1, β0 = −1, γ0 = 0
Wk+1 =Wk − µ

S
X

((αk + 1)psk −

s=1

(βk + 1)ys + γk ) (x̄s )T
αk+1 =αk − ν

S
X
(αk psk − βk y s + γk )psk
s=1

βk+1

S
X
=βk + ν
(αk psk − βk y s + γk )y s
s=1

γk+1 =γk − ν

S
X
s=1

(αk psk

s

− βk y + γk ).

Using matrix notation
Wk+1 = Wk − µ((αk + 1)pk − (βk + 1)y + γk 1)X̄T

αk+1 = αk − ν αk kpk k22 − βk hy, pk i + γk h1, pk i

βk+1 = βk + ν αk hpk , yi − βk kyk22 + γk h1, yi
γk+1 = γk − ν (αk h1, pk i − βk h1, yi + Sγk )
Note, that the subspace given by α = β = γ = 0 is invariant under this mapping. As noted before, this corresponds
to the synthetic gradient being equal to the real gradient.
Proving the convergence of SG means showing, that a trajectory starting from α0 = −1, β0 = −1, γ0 = 0 converges to W = W0 , α = β = γ = 0, where W0 are
the “true” weigts of the linear regression. We are actually going to prove more, we will show that W = W0 ,
α = β = γ = 0 is in fact a global attractor, i.e. that any
trajectory converges to that point. Denoting ω = (α, β, γ)t
we get
Wk+1 = Wk − µ((αk + 1)pk − (βk + 1)y + γk 1)X̄T

T  T

ωk+1 = ωk − ν pTk | − yT |1T
pk | − yT |1T ωk

T
Wk+1 = Wk − µ(pk − y)X̄T − µωkT pTk | − yT |1T X̄T

T  T

ωk+1 = ωk − ν pTk | − yT |1T
pk | − yT |1T ωk .


Denoting by Ak = pTk | − yT |1T we get
Wk+1 = Wk − µ(pk − y)X̄T − µω T ATk X̄T
ωk+1 = ωk − νATk Ak ωk .
Multiplying both sides of the first equation by X̄ we obtain
Wk+1 X̄ = Wk X̄ − µ(pk − y)X̄T X̄ − µω T ATk X̄T X̄
ωk+1 = ωk − νATk Ak ωk .
Denote B = X̄T X̄. We get
pk+1

=

pk − µpk B + µyB − µωkT ATk B

ωk+1

=

ωk − νATk Ak ωk .

Denoting ek = (y − pk )T we get
ek+1

= ek − µBek + µBAk ωk

ωk+1

= ωk − νATk Ak ωk .

We will use the symbol ξ = Ak ωk . Then
ek+1
ξk+1

= ek − µBek + µBξk
= ξk − νAk ATk ξk .

(1)

Every vector v can be uniquely expressed as a sum v =
v ⊥ + v k with X̄v ⊥ = 0 and v k = X̄T θ for some θ (v k
is a projection of v onto the linear subspace spanned by

Understanding Synthetic Gradients and DNIs

s

the columns of X̄). Applying this decomposition to ek =
k
e⊥
k + ek we get
e⊥
k+1
k

⊥
⊥
= e⊥
k − µ(Bek ) + µ(Bξk )
k

ek+1

= ek − µ(Bek )k + µ(Bξk )k

ξk+1

= ξk − νAk ATk ξk .

Note now, that as B = X̄T X̄, for any vector v there is
(Bv)⊥ = 0, and (Bv)k = Bv (because the operator v 7→
v k is a projection). Moreover, Bv = Bv k . Therefore
e⊥
k+1

=

e⊥
k

ek+1

=

ek − µ(Bek ) + µ(Bξk )k

ξk+1

=

ξk − νAk ATk ξk .

k

k

k

The value e⊥
k does not change. Thus, we will be omitting
the first equation. Note, that e⊥
k is “the residue”, the smallest error that can be obtained by a linear regression.
k
For the sake of visual appeal we will denote f = ek
fk+1

=

fk − µBfk + µBξk

ξk+1

=

ξk − νAk ATk ξk .

Taking norms and using ku + vk ≤ kuk + kvk we obtain
kfk+1 k2

≤

kfk − µBfk k2 + µkBξk k2

kξk+1 k22

=

kξk k22 − 2νkATk ξk k22 + ν 2 kAk ATk ξk k22 .

1−
√

kATk ξk k22 kATk ξk k22
kξk k2 ≤
kAk ATk ξk k22 kξk k22

1 − h ≤ 1 − 21 h we get
p
k{fk+1 } ⊕ {ξk+1 }k⊕ ≤ 1 − 2µb + µ2 kBk2 kfk k2 +


kATk ξk k22 kATk ξk k22
1−
+ µ kξk k2
2kAk ATk ξk k22 kξk k22
p
b
Note, that 1 − 2µb + µ2 kBk2 < 1 for 0 < µ ≤ kBk
2.
Thus, for


b
kATk ξk k22 kATk ξk k22
µ < min
,
1
−
,
kBk2
2kAk ATk ξk k22 kξk k22

Using

for every pair {fk+1 } ⊕ {ξk+1 } 6= {0} ⊕ {0} (and if they
are zeros then we already converged) there is
k{fk+1 } ⊕ {ξk+1 }k⊕ < k{fk } ⊕ {ξk }k⊕ .
Therefore, by Theorem 2, the error pair {fk+1 } ⊕ {ξk+1 }
has to converge to 0, which ends the proof in the case
Ak ATk ξk 6= 0. It remains to investigate what happens if
Ak ATk ξk = 0.
We start by observing that either ξk = 0 or ATk ξk 6= 0 and
Ak ATk ξk 6= 0. This follows directly from the definition
ξk = Ak ωk . Indeed, if ξk 6= 0 there is 0 < kAk ωk k22 =
ωkT ATk ξk and analogously 0 < kATk ξk k = ξkT Ak ATk ξk .

Observe that kfk − µBfk k22 = kfk k22 − 2µfk Bfk +
µ2 kBfk k22 . As B is a constant matrix, there exists a constant b > 0 such that v T Bv ≥ bkvk22 for any v satisfying
v k = v. Therefore kfk − µBfk k22 ≤ kfk k22 − 2µbkfk k22 +
µ2 kBk2 kfk k22 . Using that and kBξk k2 ≤ kBkkξk k2 we
get
p
1 − 2µb + µ2 kBk2 kfk k2 + µkBkkξk k2
kfk+1 k2 ≤

In case ξk = 0 there
p is k{fk+1 } ⊕ {ξk+1 }k⊕ =
kp fk+1 k2
<
1 − 2µb + µ2 kBk2 kfk k2
=
2
2
1 − 2µb + µ kBk k{fk } ⊕ {ξk }k⊕ and the theorem follows.

kξk+1 k22

n→∞

=

kξk k22 − 2νkATk ξk k22 + ν 2 kAk ATk ξk k22 .

Let us assume that Ak ATk ξk 6= 0. In that case the righthand side of the second equation is a quadratic function is
2
kAT
k ξk k 2
ν, whose minimum value is attained for ν = kAk A
T ξ k2 .
k k 2
For so-chosen ν we have
p
kfk+1 k2 ≤
1 − 2µb + µ2 kBk2 kfk k2 + µkBkkξk k2


kATk ξk k22 kATk ξk k22
2
kξk+1 k2 =
1−
kξk k22 .
kAk ATk ξk k22 kξk k22
Consider a space {f } ⊕ {ξ} (concatenation of vectors) with
a norm k{f } ⊕ {ξ}k⊕ = kf k2 + kξk2 .

Theorem 2. Let B be a finite-dimensional Banach space.
Let f : B → B be a continuous map such that for every
x ∈ B there is kf (x)k < kxk. Then for every x there is
lim f n (x) = 0.
Proof. Let ω(x) = {y : ∃i1 <i2 <... lim f in (x) = y}.
n→∞

Because kf (x)k < kxk, the sequence x, f (x), f 2 (x), . . .
is contained in a ball of a radius kxk, which due to a finite dimensionality of B is a compact set. Thus, ω(x)
is nonempty. Moreover, from the definition, ω(x) is a
closed set, and therefore it is a compact set. Let y0 =
inf y∈ω(x) kyk – which we know exists, due to the compactness of ω(x) and the continuity of k · k (Weierstraß
theorem). But for every y ∈ ω(x) there is f (y) ∈ ω(x),
thus there must be y0 = 0. By definition, for every ε, there
exists n0 such that kf n0 (x)k < ε. Therefore, for n > n0
kf n (x)k < ε. Therefore, f n (x) must converge to 0.

k{fk+1 } ⊕ {ξk+1 }k⊕ ≤
p

1 − 2µb + µ2 kBk2 kfk k2 + µkBkkξk k2 +

Proposition 2. Let us assume that a SG module is trained
in each iteration in such a way that it -tracks true gradient,

Understanding Synthetic Gradients and DNIs

i.e. that kSG(h, y) − ∂L/∂hk ≤ . If k∂h/∂θ<h k is upper
bounded by some K and there exists a constant δ ∈ (0, 1)
such that in every iteration K ≤ k∂L/∂θ<h k 1−δ
1+δ , then
the whole training process converges to the solution of the
original problem.
Proof. Directly from construction we get that k∂L/∂θ<h −
ˆ ∂θ
ˆ <h k = k(∂L/∂h−SG(h, y))∂h/∂θ<h k ≤ K thus
∂L/
in each iteration there exists such a vector e, that kek ≤ K
ˆ ∂θ
ˆ <h = ∂L/∂θ<h + e. Consequently, we get
and ∂L/
a model trained with noisy gradients, where the noise of
the gradient is bounded in norm by K so, directly from
assumptions, it is also upper bounded by k∂L/∂θ<h k 1−δ
1+δ
and we we get that the direction followed is sufficient for
convergence as this means that cosine between true gradient and synthetic gradient is uniformly bounded away (by
δ) from zero (Zoutendijk, 1970; Gratton et al., 2011). At
the same time, due to Proposition 1, we know that the assumptions do not form an empty set as the SG module can
stay in an  neighborhood of the gradient, and both norm
of the synthetic gradient and k∂h/∂θ<h k can go to zero
around the true critical point.

the noise is equal to e∂h/∂θ<h we get that
ke∂h/∂θ<h k ≤ kekk∂h/∂θ<h k < 1/3k∂L/∂θ<h k,
which is equivalent to error for θ<h being upper bounded
by (1 − δ)/(1 + δ)k∂L/∂hk for δ = 0.5 which matches
assumptions of Proposition 2, thus leading to the convergence of the model considered. If at any moment we lose
track of the gradient again – the same mechanism kicks in µ goes down for as long as the inequality (2) does not hold
again (and it has to at some point, given ν is positive and
small enough).

D. Technical details
All experiments were performed using TensorFlow (Abadi
et al., 2016). In all the experiments SG loss is the MSE
between synthetic and true gradients. Since all SGs considered were linear, weights were initialized to zeros so
initially SG produces zero gradients, and it does not affect
convergence (since linear regression is convex).
Datasets

Corollary 1. For a deep linear model and an MSE objective, trained with a linear SG module attached between
two of its hidden layers, there exist learning rates in each
iteration such that it converges to the critical point of the
original problem.

Each of the artificial datasets is a classification problem,
consisting of X sampled from k-dimensional Gaussian distribution with zero mean and unit standard deviation. For
k = 2 we sample 100 points and for k = 100 we sample
1000. Labels y are generated in a way depending on the
dataset name:

Proof. Denote the learning rate of the main model by µ
and learning rate of the SG module by ν > 0 and put µ =
 max(0, kek − 1/(3k∂h/∂θ<h k)k∂L/∂θ<h k), where  is
a small learning rate (for example found using line search)
and e is the error SG will make in the next iteration. The
constant 1/3 appears here as it is equal to (1 − δ)/(1 + δ)
for δ = 0.5 which is a constant from Proposition 2, which
we will need later on. Norm of e consists of the error fitting
term LSG which we know, and the term depending on the
previous µ value, since this is how much the solution for the
SG problem evolved over last iteration. In such a setting,
the main model changes iff

• lineark - we randomly sample an origin-crossing hyperplane (by sampling its parameters from standard
Gaussians) and label points accordingly,

kekk∂h/∂θ<h k < 1/3k∂L/∂θ<h k.

(2)

First of all, this takes place as long as ν is small enough
since the linear SG is enough to represent ∂L/∂h with arbitrary precision (Proposition 1) and it is trained to do so
in a way that always converges (as it is a linear regression
fitted to a linear function). So in the worst case scenario
for a few first iterations we choose very small µ (it always
exists since in the worst case scenario µ = 0 agrees with
the inequality). Furthermore, once this happens we follow
true gradient on θ>h and a noisy gradient on θ<h . Since

• noisyk - we label points according to lineark and then
randomly swap labels of 10% of samples,
• randomk - points are labeled completely randomly.
We used one-hot encoding of binary labels to retain compatibility with softmax-based models, which is consistent
with the rest of experiments. However we also tested
the same things with a single output neuron and regular
sigmoid-based network and obtained analogous results.
Optimisation
Optimisation is performed using the Adam optimiser (Kingma & Ba, 2014) with a learning rate of
3e − 5. This applies to both main model and to SG module.
Artificial datasets
Table 2 shows results for training linear regression (shallow
MSE), 10 hidden layer deep linear regression (deep MSE),

Understanding Synthetic Gradients and DNIs

Figure 7. (top) Representation Dissimilarity Matrices for a label ordered sample from MNIST dataset pushed through 20-hidden layer
deep relu networks trained with backpropagation (top row), a single SG attached between layers 11 and 12 (2nd row), SG between
every pair of layers (3rd row), and the DFA model (4th row). Notice the moment of appearance of dark blue squares on a diagonal in
each learning method, which shows when a clear inner-class representation has been learned. For visual confidence off block diagonal
elements are semi transparent. (bottom) L2 distance between diagonal elements at a given layer and the same elements at layer 20.
Dotted lines show where SGs are inserted. With a single SG module we can see that there is the representation is qualitatively different
for the first part of the network (up to layer 11) and the rest. For fully unlocked model the representation constantly evolves through
all the layers, as opposed to backprop which has a nearly constant representation correlation from layer 9 forward. Also due to DFA
mathematical formulation it tries to solve the task as early as possible thus leading to nearly non-evolving representation correlation after
the very first layer.

logistic regression (shallow log loss) and 10 hidden layer
deep linear classifier (deep log loss). Since all these problems (after proper initialisation) converge to the global optima, we report the difference between final loss obtained
for SG enriched models and the true global optimum.
MNIST experiments
Networks used are simple feed forward networks with h
layers of 512 hidden relu units followed by batch normalisation layers. The final layer is a regular 10-class softmax
layer. Inputs were scaled to [0, 1] interval, besides that there
was no preprocessing applied.
Representational Dissimilarity Matrices
In order to build RDMs for a layer h we sample 400 points
(sorted according to their label) from the MNIST dataset,
{xi }400
i=1 and record activations on each of these points,
hi = h(xi ). Then we compute a matrix RDM such that
RDMij = 1 − corr(hi , hj ). Consequently a perfect RDM
is a block diagonal matrix, thus elements of the same class
have a representation with high correlation and the representations of points from two distinct classes are not correlated. Figure 7 is the extended version of the analogous
Figure 3 from the main paper where we show RDMs for

backpropagation, a single SG, SG in-between every two
layers, and also the DFA model, when training 20 hidden
layer deep relu network.
Linear classifier/regression probes
One way of checking the degree to which the actual classification problem is solved at every layer of a feedforward
network is to attach linear classifiers to every hidden layer
and train them on the main task without backpropagating
through the rest of the network. This way we can make a
plot of train accuracy obtained from the representation at
each layer. As seen in Figure 8 (left) there is not much of
the difference between such analysis for backpropagation
and a single SG module, confirming our claim in the paper
that despite different representations in both sections of SG
based module - they are both good enough to solve the main
problem. We can also that DFA tries to solve the classification problem bottom-up as opposed to up-bottom – notice
that for DFA we can have 100% accuracy after the very first
hidden layer, which is not true even for backpropagation.
We also introduced a new kind of linear probe, which tries
to capture how much computation (non-linear transformations) are being used in each layer. To achieve this, we attach a linear regressor module after each hidden layer and

Understanding Synthetic Gradients and DNIs

dataset

model

MSE

log loss

linear2
linear100
noisy2
noisy100
random2
random100
noisy2
noisy100
random2
random100

shallow
shallow
shallow
shallow
shallow
shallow
deep
deep
deep
deep

0.00000
0.00002
0.00000
0.00002
0.00000
0.00004
0.00000
0.00001
0.00000
0.00001

0.03842
0.08554
0.00036
0.00442
0.00000
0.00003
0.00000
0.00293
0.00000
0.00004

Table 2. Differences in final losses obtained for various models/datasets when trained with SG as compared to model trained
with backpropagation. Bolded entries denote experiments which
converged to a different solution. lineark is k dimensional, linearly separable dataset, noisy is linearly separable up to 10% label noise, and random has completely random labeling. Shallow
models means linear ones, while deep means 10 hidden layer deep
linear models. Reported differences are averaged across 10 different datasets from the same distributions.

Figure 8. Left: Training accuracy at each linear classifier probe.
Right: MSE for each linear regressor probe.

regress it (with MSE) to the input of the network. This
is obviously label agnostic approach, but measures how
non-linear the transformations are up to the given hidden
layer. Figure 8 (right) again confirms that with a single SG
we have two parts of the network (thus results are similar
to RDM experiments) which do have slightly different behaviour, and again show clearly that DFA performs lots of
non-linear transformations very early on compared to all
other methods.
Loss estimation
In the main paper we show how SG modules using both activations and labels are able to implicitly describe the loss
surface reasonably well for most of the training, with different datasets and losses. For completeness, we also include the same experiment for SG modules which do not
use label information (Figure 9 (a) - (d)) as well as a module which does not use activations at all6 (Figure 9 (e) (h))). There are two important observations here: Firstly,
6

This is more similar to a per-label stale gradient model.

none of these two approaches provide a loss estimation fidelity comparable with the full SG (conditioned on both
activations and labels). This gives another empirical confirmation for correct conditioning of the module. Secondly,
models which used only labels did not converge to a good
solutions after 100k iterations, while without the label SG
was able to do so (however it took much longer and was far
noisier).

References
Abadi, Martı́n, Agarwal, Ashish, Barham, Paul, Brevdo,
Eugene, Chen, Zhifeng, Citro, Craig, Corrado, Greg S,
Davis, Andy, Dean, Jeffrey, Devin, Matthieu, et al. Tensorflow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467,
2016.
Fairbank, M. Value-gradient learning. PhD thesis, City
University London, UK, 2014.
Gratton, Serge, Toint, Philippe L, and Tröltzsch, Anke.
How much gradient noise does a gradient-based linesearch method tolerate. Technical report, Citeseer, 2011.
Heess, N, Wayne, G, Silver, D, Lillicrap, T P, Erez, T,
and Tassa, Y. Learning continuous control policies by
stochastic value gradients. In Advances in Neural Information Processing Systems 28: Annual Conference on
Neural Information Processing Systems 2015, December
7-12, 2015, Montreal, Quebec, Canada, pp. 2944–2952,
2015.
Jaderberg, Max, Czarnecki, Wojciech Marian, Osindero,
Simon, Vinyals, Oriol, Graves, Alex, and Kavukcuoglu,
Koray. Decoupled neural interfaces using synthetic gradients. arXiv preprint arXiv:1608.05343, 2016.
Kingma, Diederik and Ba, Jimmy.
method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam: A
arXiv preprint

Zoutendijk, G. Nonlinear programming, computational
methods. Integer and nonlinear programming, 143(1):
37–86, 1970.

Understanding Synthetic Gradients and DNIs

Every layer SG

Single SG

Every layer SG

Train iteration

Single SG

b) log loss, noisy linear data, no label conditioning

c) MSE, randomly labeled data, no label conditioning

d) log loss, randomly labeled data, no label conditioning

e) MSE, noisy linear data, only label conditioning

f) log loss, noisy linear data, only label conditioning

g) MSE, randomly labeled data, only label conditioning

h) log loss, randomly labeled data, only label conditioning

Train iteration

Train iteration

Train iteration

a) MSE, noisy linear data, no label conditioning

Figure 9. Visualisation of the true loss and the loss extracted from the SG module. In each block left plot shows an experiment with
a single SG attached and the right one with a SG after each hidden layer. Note, that in this experiment the final loss is actually big,
thus even though the loss reassembles some part of the noise surface, the bright artifact lines are actually keeping it away from the true
solution.

