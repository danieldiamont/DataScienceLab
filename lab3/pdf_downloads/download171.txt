Distributed Batch Gaussian Process Optimization

A. Derivation of I[fD ; yDt |yD1:t-1 ] Term in (2)
By the definition of conditional mutual information,

I[fD ; yDt |yD1:t-1 ]
= H[yDt |yD1:t-1 ] H[yDt |fD , yD1:t-1 ]
= H[yDt |yD1:t-1 ] H[yDt |fDt ]
= 0.5|Dt | log(2⇡e) + 0.5 log | n2 I + ⌃Dt Dt |
= 0.5 log(| n2 I + ⌃Dt Dt || n2 I| 1 )
= 0.5 log(| n2 I + ⌃Dt Dt || n 2 I|)
= 0.5 log |I + n 2 ⌃Dt Dt |

0.5|Dt | log(2⇡e)

0.5 log |

2
n I|

where the third equality is due to the definition of Gaussian entropy, that is, H[yDt |yD1:t-1 ] , 0.5|Dt | log(2⇡e) +
0.5 log | n2 I +⌃Dt Dt | and H[yDt |fDt ] , 0.5|Dt | log(2⇡e)+0.5 log | n2 I|, the latter of which follows from ✏ = yx f (x) ⇠
N (0, n2 ) for all x 2 Dt and hence p(yDt |fDt ) = N (0, n2 I).

B. Proof of Proposition 1

log |
=
=
=
=
=

D t Dt |

1

log | Dt Dt |
log |U > U |
log |U > ||U |
log |U |2
N
Y
2 log
|Unn |
n=1

N
X

=

2

=

n=1
N
X

=
=

n=1
N
X
n=1
N
X
n=1

=
=

N
X

n=1
N
X
n=1

log |Unn |

log |Unn |2
>
log |Unn
||Unn |
>
log |Unn
Unn |

>
log |Unn
Unn |
>
log |(Unn
Unn )

1

1

|

where the first, third, fourth, eighth, ninth, and last equalities follow from the properties of the determinant, the second
1
equality is due to the Cholesky factorization of Dt Dt , and the fifth equality follows from the property that the determinant
QN
of an upper triangular block matrix is a product of determinants of its diagonal blocks (i.e., |U | = n=1 |Unn |).

Distributed Batch Gaussian Process Optimization

C. Proof of Proposition 4
From the definition of DKL (

Dt Dt ,

Dt Dt ),

DKL ( ⇣Dt Dt , Dt Dt )
1
= 0.5 tr( Dt Dt Dt Dt ) log |
⇣
⌘
1
= 0.5
log | Dt Dt Dt Dt |

Dt Dt

1
Dt Dt |

|Dt |

⌘

= 0.5
log | Dt Dt | + log | Dt Dt |
= 0.5 log | Dt Dt | 0.5 log | Dt Dt |
= Ĩ[fD ; yDt |yD1:t-1 ] I[fD ; yDt |yD1:t-1 ] .
The second equality is due to tr(

Dt Dt

1
Dt Dt )

= tr(

1
D t Dt )

Dt Dt

= tr(I) = |Dt |, which follows from the observations
1
Dt Dt

that the blocks within the B-block bands of Dt Dt and Dt Dt coincide and
follows that
DKL ( Dt Dt , Dt Dt )
= Ĩ[fD ; yDt |yD1:t-1 ] I[fD ; yDt |yD1:t-1 ]
|Dt |
⇣
⌘
X
= 0.5 log | Dt Dt |
0.5 log 1 + n 2 ⌃b{xb1}{xb }
b=1

 0.5 log
=

|Dt |
X

Y

1+

n

x2Dt

0.5 log 1 +

2

n

b=1



|Dt |
X

⇣

0.5 log 1 +

b=1

 exp(2C)

|Dt |
X

n

2

⌃{x}{x}

2

|Dt |
X

X
b=1

⇣

n

⇣

2

⌃b{xb1}{xb }

1)

0.5 log 1 +

= (exp(2C)

1) I[fD ; yDt |yD1:t-1 ] .

n

2

⇣
0.5 log 1 +

⇣
0.5 log 1 +

exp(2C)⌃b{xb1}{xb }

= (exp(2C)

b=1

|Dt |
X

b=1
|Dt |

⌃{xb }{xb }

0.5 log 1 +

b=1

!

⌘
⌘

|Dt |
X
b=1

|Dt |
X
b=1

⌃b{xb1}{xb }

⌘

n

n

2

2

is B-block-banded (Proposition 3). It

⌃b{xb1}{xb }

⌃b{xb1}{xb }

⇣
0.5 log 1 +
⇣
0.5 log 1 +

n

n

⌘

⌘

2

⌃b{xb1}{xb }

2

⌃b{xb1}{xb }

⌘
⌘

The second and last equalities are due to Lemma 4 in Appendix F and ⌃b{xb1}{xb } is defined in Definition 1 in Appendix F.
The first inequality is due to Hadamard’s inequality and the observation that the blocks within the B-block bands of Dt Dt
and Dt Dt (and thus their diagonal elements) coincide. The second inequality is due to Lemma 2 in Appendix F. The third
inequality is due to Bernoulli’s inequality.
Remark. The first inequality can also be interpreted as bounding the approximated information gain for an arbitrary Dt Dt
by the approximated information gain for the Dt Dt with the highest possible degree of our proposed Markov approximation, i.e., for N = |Dt | and B = 0. In this case, all inputs of the batch are assumed to have conditionally independent
corresponding outputs such that the determinant of the approximated matrix reduces toQthe product of its diagonal elements which are equal to the diagonal elements of the original matrix. Thus, | Dt Dt |  x2Dt 1 + n 2 ⌃{x}{x} which
interestingly coincides with Hadamard’s inequality. Note that we only consider B 1 for our proposed algorithm (Proposition 2) since the case of B = 0 entails an issue similar to that discussed at the beginning of Section 3 of selecting the
same input |Dt | times within a batch.

D. Minimal KL Distance of Approximated Matrix
For the approximation quality of Dt Dt (4), the following result shows that the Kullback-Leibler (KL) distance of Dt Dt
from Dt Dt is the least among all |Dt | ⇥ |Dt | matrices with a B-block-banded inverse:
Proposition 5. Let KL distance DKL ( , e ) , 0.5(tr( e 1 ) log | e 1 | |Dt |) between two |Dt | ⇥ |Dt | symmetric positive definite matrices
and e measure the error of approximating
with e . Then, DKL ( D D , D D ) 
t

t

t

t

Distributed Batch Gaussian Process Optimization

DKL (
Proof.

Dt Dt ,

e ) for any matrix e with a B-block-banded inverse.

DKL ( ⇣Dt Dt ,

= 0.5 tr(
⇣
= 0.5 tr(
⇣
= 0.5 tr(
= DKL (

Dt Dt ) + DKL ( Dt Dt ,
1
Dt Dt Dt Dt ) log | Dt Dt
Dt Dt
Dt Dt

Dt Dt

e
e

, e) .

1

)

1

)

The second equality is due to tr(

e)

⌘
⇣
|Dt | + 0.5 tr(
⌘
log | Dt Dt | log | e 1 | |Dt |
⌘
log | Dt Dt e 1 | |Dt |
Dt Dt

1
D t Dt )

1
D t Dt |

= tr(

Dt Dt

1
Dt Dt )

D t Dt

e

1

) log |

Dt Dt

e

1

| |Dt |

⌘

= tr(I) = |Dt |, which follows from the observa1

tions that the blocks within the B-block bands of Dt Dt and Dt Dt coincide and Dt Dt is B-block-banded (Proposition 3). The third equality follows from the first observation above and the definition that e 1 is B-block-banded. Since
DKL ( Dt Dt , e ) 0, DKL ( Dt Dt , Dt Dt )  DKL ( Dt Dt , e ).

E. Pseudocode for DB-GP-UCB

Algorithm 1 DB-GP-UCB
Input: Objective function f , input domain D, batch size |Dt |, time horizon T , prior mean mx and kernel kxx0 ,
approximation parameters B and N
for t = 1, . . . , T do
8
q
>
< 1> µDt + ↵t I[fD ; yDt |y
(2) if B = N 1 ,
D 1:t-1 ]
Select acquisition function a(Dt ) ,
q
P
N
>
>
:
B |
0.5↵t log | Dtn Dtn |Dtn
(5) otherwise
n=1 1 µDtn +
Select batch Dt , arg maxDt ⇢D a(Dt )
Query batch Dt to obtain yDt , (f (x) + ✏)>
x2D t
end for
e , arg maxx2D µ{x}
Output: Recommendation x

F. Proof of Theorem 1

We first define a different notion of posterior variance:
Definition 1 (Updated Posterior Variance). Let Dt , {x1 , . . . , x|Dt | } be the batch selected in iteration t. Assume an
arbitrary ordering of the inputs in Dt . Then, for 0  b 1 < |Dt |, ⌃b{xb1}{xb } is defined as the updated posterior variance
at input xb that is obtained by applying (1) conditioned on the previous inputs in the batch Dtb 1 , {x1 , . . . , xb 1 }. Note
that performing this update is possible without querying Dtb 1 since ⌃b{xb1}{xb } is independent of the outputs yDb 1 . For
b

1 = 0, ⌃b{xb1}{xb } reduces to ⌃{xb }{xb } .

t

The following lemmas are necessary for proving our main result here:
P1
Lemma 1. Let 2 (0, 1) be given and t , 2 log(|D|⇡t / ) where t=1 ⇡t 1 = 1 and ⇡t > 0. Then,
⇣
⌘
Pr 8x 2 D 8t 2 N |f (x) µ{x} |  t1/2 ⌃1/2
1
.
{x}{x}

Lemma 1Pabove corresponds to Lemma 5.1 in (Srinivas et al., 2010); see its proof therein. For example, ⇡t = t2 ⇡ 2 /6 > 0
1
satisfies t=1 ⇡t 1 = 1.

Lemma 2. For f sampled from a known GP prior with known noise variance n2 , the ratio of ⌃{xb }{xb } to ⌃b{xb1}{xb } for
all xb 2 Dt is bounded by
⇣
⌘
⌃{xb }{xb }
=
exp
2
I[f
;
y
|y
]
 exp(2C)
b
1
D1:t-1
{xb }
Dt
⌃b{xb1}{xb }

Distributed Batch Gaussian Process Optimization

where ⌃b{xb1}{xb } and Dtb

1

are previously defined in Definition 1, and for all x 2 D and t 2 N,
C

I[f{x} ; yDb 1 |yD1:t-1 ]
t

is a suitable constant.
Lemma 2 above is a combination of Proposition 1 and equation 9 in (Desautels et al., 2014); see their proofs therein. The
only difference is that we equivalently bound the ratio of variances instead of the ratio of standard deviations, thus leading
to an additional factor of 2 in the argument of exp.
Remark. Since the upper bound exp(2C) will appear in our regret bounds, we need to choose C suitably. A straightforward
choice C , |Dt | 1 = maxA⇢D,|A||Dt | 1 I[fD ; yA ] maxA⇢D,|A||Dt | 1 I[fD ; yA |yD1:t-1 ] I[fD ; yDb 1 |yD1:t-1 ]
t
I[f{x} ; yDb 1 |yD1:t-1 ] (see equations 11, 12, and 13 in (Desautels et al., 2014)) is unfortunately unsatisfying from the
t
perspective of asymptotic scaling since it grows at least as ⌦(log |Dt |), thus implying that exp(2C) grows at least linearly
in |Dt | and yielding a regret bound that is also at least linear in |Dt |. The work of Desautels et al. (2014) shows that when
initializing an algorithm suitably, one can obtain a constant C independent of the batch size |Dt |. Refer to Section 4 in
(Desautels et al., 2014) for a more detailed discussion.
Lemma 3. For all t 2 N and xb 2 Dt ,

where C0 , 2/ log(1 +

n

2

⇣
⌃b{xb1}{xb }  0.5C0 log 1 +

n

2

⌃b{xb1}{xb }

⌘

).

Lemma 3 above corresponds to an intermediate step of Lemma 5.4 in (Srinivas et al., 2010); see its proof therein.
Lemma 4. The information gain for a batch Dt chosen in any iteration t can be expressed in terms of the updated posterior
variances of the individual inputs xb 2 Dt , b 2 {1, . . . , |Dt |} of the batch Dt . That is, for all t 2 N,
I[fD ; yDt |yD1:t-1 ] = 0.5

|Dt |
X
b=1

⇣
log 1 +

n

2

⌘
⌃b{xb1}{xb } .

Lemma 4 above corresponds to Lemma 5.3 in (Srinivas et al., 2010) (the only difference being that we equivalently sum
over 1, . . . , |Dt | instead of 1, . . . , T ); see its proof therein.

Lemma 5. Let 2 (0, 1) be given, C0 , 2/ log(1 + n 2 ), and ↵t , C0 |Dt | exp(2C) t where t and exp(2C) are
previously defined in Lemmas 1 and 2, respectively. Then,
!
X
p
Pr 8Dt ⇢ D 8t 2 N
|f (x) µ{x} |  ↵t I[fD ; yDt |yD1:t-1 ]
1
.
x2Dt

Proof. For all Dt ⇢ D and t 2 N,

X

x2Dt

=


t

t

⌃{x}{x}

|Dt |
X
b=1
|Dt |

t

X
b=1

⌃{xb }{xb }
exp(2C) ⌃b{xb1}{xb }

 0.5C0 exp(2C)

t

|Dt |
X
b=1

⇣
log 1 +

= C0 exp(2C) t I[fD ; yDt |yD1:t-1 ]
= |Dt | 1 ↵t I[fD ; yDt |yD1:t-1 ]

n

2

⌃b{xb1}{xb }

⌘

Distributed Batch Gaussian Process Optimization

where the first inequality is due to Lemma 2, the second inequality is due to Lemma 3, and the second equality is due to
Lemma 4. Thus,
X

1/2

t

x2Dt

1/2

⌃{x}{x}

s
X
 |Dt |

t ⌃{x}{x}

x2Dt



p
↵t I[fD ; yDt |yD1:t-1 ]

where the first inequality is due to the Cauchy-Schwarz inequality. It follows that
X

p
Pr 8Dt ⇢ D 8t 2 N
|f (x) µ{x} |  ↵t I[fD ; yDt |yD1:t-1 ]
x2Dt
!
X
X 1/2 1/2
Pr 8Dt ⇢ D 8t 2 N
|f (x) µ{x} | 
t ⌃{x}{x}
x2D
x2D
t
t⌘
⇣
Pr 8x 2 D 8t 2 N |f (x) µ{x} |  t1/2 ⌃1/2
{x}{x}
1

!

where the first two inequalities are due to the property that for logical propositions A and B, [A =) B] =) [Pr(A) 
Pr(B)], and the last inequality is due to Lemma 1.
Lemma 6. Let ⌫t
for all t 2 N,

Ĩ[fD ; yDt |yD1:t-1 ]

I[fD ; yDt |yD1:t-1 ] be an upper bound on the approximation error of

N q
X
0.5 log |

n=1

B |
Dtn Dtn |Dtn

p



Dt D t .

Then,

N (I[fD ; yDt |yD1:t-1 ] + ⌫t ) .

Proof.
N q
X

n=1
v

0.5 log |

B |
Dtn Dtn |Dtn

u N
u X
 tN
0.5 log |

B |
Dtn Dtn |Dtn

q n=1
= N Ĩ[fD ; yDt |yD1:t-1 ]
q
= N (I[fD ; yDt |yD1:t-1 ] + Ĩ[fD ; yDt |yD1:t-1 ]
p
 N (I[fD ; yDt |yD1:t-1 ] + ⌫t )

I[fD ; yDt |yD1:t-1 ])

where the first inequality is due to the Cauchy-Schwarz inequality.
Lemma 7. Let t 2 N be given. If
X

x2Dt

|f (x)

P
for all Dt
⇢
D, then
x2D t rx
q
2 |Dt | 2 ↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t ).

µ{x} | 


p

(6)

↵t I[fD ; yDt |yD1:t-1 ]

q
2 ↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t )

and

minx2Dt rx



Distributed Batch Gaussian Process Optimization

Proof.

X

rx

x2D
t
X

=
=

x2D
Xt

x2D t
0

@

(f (x⇤ )

f (x))

f (x⇤ )

X

X

x2D t

=
=


q
q
q

f (x)
1

x2D t

µ{x} +

q

↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t )A
0

↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t ) + @

X

x2D t

X

↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t ) +

x2D
q t

↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t ) +
q
= 2 ↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t ) .

µ{x}

µ{x}

X

x2D t

X

x2D t

f (x)

f (x)
1

(7)

f (x)A

↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t )

The first equality in (7) is by definition (Section 2). The first inequality in (7) is due to
X

f (x⇤ )

x2DX
t

=

f (x)

x2Dt⇤



=

X

x2Dt⇤

X

x2Dt⇤

X

x2Dt⇤





X

x2Dt⇤

X

x2D t

X

x2D t

µ{x} +
µ{x} +

q
↵t I[fD ; yDt⇤ |yD1:t-1 ]
q
↵t Ĩ[fD ; yDt⇤ |yD1:t-1 ]

v
u
N
u X
µ{x} + t↵t
0.5 log |
n=1

N
p Xq
µ{x} + ↵t
0.5 log |

µ{x} +
µ{x} +

p

↵t

n=1
N q
X

n=1

0.5 log |

(8)

⇤ D ⇤ |D ⇤B |
Dtn
tn
tn

⇤ D ⇤ |D ⇤B |
Dtn
tn
tn

B

D tn D tn |D tn

q
↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t )

|

where, in (8), the first inequality is due to (6), the second inequality is due to Proposition 4 (see the
paragraph after this
qP
PN p
N
proposition in particular), the third inequality is due to the simple observation that n=1 an
n=1 an , the fourth
⇤
inequality follows from the definition of Dt in (5) and, with a slight abuse of notation, Dt is defined as a batch of |Dt |
inputs x⇤ , and the last inequality is due to Lemma 6. The last inequality in (7) follows from (6) and an argument equivalent
to the one in (8) (i.e., by substituting Dt⇤ by Dt ).
From (7),

min rx 

x2D t

q
1 X
rx  2 |Dt |
|Dt |
x2D t

2↵

t

N (I[fD ; yDt |yD1:t-1 ] + ⌫t ) .

Distributed Batch Gaussian Process Optimization

Main Proof.
RT0
T X
X
=
rx
t=1 x2D t

T
q
X

2 ↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t )
t=1
v
u T
u X
 2t T
↵t N (I[fD ; yDt |yD1:t-1 ] + ⌫t )

v t=1
u
u
 2t T ↵ N
T

T
X
t=1

I[fD ; yDt |yD1:t-1 ] +

q
= 2 T ↵T N I[fD ; yD1:T ] + ⌫¯T
p
2
q T ↵T N ( T + ⌫¯T )
= C2 T |DT | exp(2C) T N ( T + ⌫¯T )

T
X

⌫t

t=1

!

holds with probability 1
where the first equality is by definition (Section 2), the first inequality follows from Lemmas 5
and 7, the second inequality is due to the Cauchy-Schwarz inequality, the third inequality is due to the non-decreasing
↵
Pt Twith increasing t, the second equality follows from the chain rule for mutual information and the definition of ⌫¯T ,
t=1 ⌫t , the fourth inequality is by definition (Theorem 1), and the third equality is due to the definition of ↵t in Lemma 5,
|D1 | = . . . = |DT | and the definition that C2 , 4C0 = 8/ log(1 + n 2 ).
Analogous reasoning leads to the result that

RT =

T
X
t=1

min rx  2

x2D t

holds with probability 1

q
T |DT |

2↵

T

N(

T

+ ⌫¯T ) =

q
C2 T |DT |

1

exp(2C)

T

N(

T

+ ⌫¯T )

, where the first equality is by definition (Section 2).

G. Comparison of Regret Bounds

Table 1. Bounds on RT ( TP, 2 log(|D|T 2 ⇡ 2 /(6 )), C1 , 4/ log(1 + n 2 ), C2 , 2C1 , C3 , 9C1 ). Note that |DT | = 1 in T
T
for GP-UCB and HDPP ,
1)-DPP with kernel K (see
t=1 H(DP P (Kt )) with H(DP P (K)) denoting the entropy of a (|Dt |
(Kathuria et al., 2016) for details on their proposed kernels). Also, note that for DB-GP-UCB andp
GB-BUCB, we assume the use of the
initialization strategy proposed by Desautels et al. (2014); otherwise, the factor C 0 is replaced by exp(2C).

BO Algorithm

DB-GP-UCB (5)
GP-UCB-PE (Contal et al., 2013)
GP-BUCB (Desautels et al., 2014)
GP-UCB (Srinivas et al., 2010)
UCB-DPP-SAMPLE (Kathuria et al., 2016)

Bound on RT
p
C C2 T |DT | 1 T N ( T + ⌫¯T )
p
C1 T |DT | 1 T T
p
C 0 C2 T |DT | 1 T |DT | T
p
C2 T T T
0

p
2C3 T |DT |

T

[

T

HDPP + |DT | log(|D|)]

Distributed Batch Gaussian Process Optimization
Table 2. Bounds on maximum mutual information T (Srinivas et al., 2010; Kathuria et al., 2016) and values of C 0 (Desautels et al.,
2014) for different commonly-used kernels (↵ , d(d + 1)/(2⌫ + d(d + 1))  1 with ⌫ being the Matérn parameter).

Kernel

Linear
RBF
Matérn

T

C0

d log(T |DT |)
d
(log(T |DT |))
(T |DT |)↵ log(T |DT |)

exp(2/e)
exp((2d/e)d )
e

H. Synthetic Benchmark Objective Functions and Real-World pH Field

Table 3. Synthetic benchmark objective functions.

Name

Function

Branin-Hoo

f (x) = a(x2 bx21 + cx1 r)2 + s(1 t) cos(x1 ) + s
where a = 1, b = 5.1/(4⇡ 2 ), c = 5/⇡, r = 6, s = 10, and t = 1/(8⇡).
d
Y
|4xi

2| + ai
1 + ai
i=1
where d = 2 and ai = 1 for i = 1, . . . , d.
P2
f (x) = 1
r(xi ))
i=1 (g(xi )

gSobol

f (x) =

Mixture of cosines

where g(xi ) = (1.6xi

D

[ 5, 15]2

[ 5, 5]2

[ 1, 1]2

0.5)2 and r(xi ) = 0.3 cos(3⇡(1.6xi

0.5)).

18
8.5

16
8

14

12

7.5

10
7

8
6.5

6

4

6

2
5.5

5

10

15

20

25

Figure 3. Real-world pH field of Broom’s Barn farm (Webster & Oliver, 2007).

30

Distributed Batch Gaussian Process Optimization

I. Details on the Implementations of Batch BO Algorithms
Table 4. Details on the available implementations of the batch BO algorithms for comparison with DB-GP-UCB in our experiments.

BO Algorithm

Language

URL of Source Code

GP-BUCB
SM-UCB
GP-UCB-PE
q-EI
BBO-LP

MATLAB
MATLAB
MATLAB
R
Python

http://www.gatsby.ucl.ac.uk/˜tdesautels/
http://www.gatsby.ucl.ac.uk/˜tdesautels/
http://econtal.perso.math.cnrs.fr/software/
http://cran.r-project.org/web/packages/DiceOptim/
http://sheffieldml.github.io/GPyOpt/

J. Analysis of the Trade-Off between the Approximation Quality vs. Time Efficiency of
DB-GP-UCB
We now analyze the trade-off between the approximation quality vs. time efficiency of DB-GP-UCB by varying the Markov
order B and number N of functions in DCOP. The mixture of cosines function (Anderson et al., 2000) is used as the objective function f and modeled as a sample of a GP. A large batch size |DT | = 16 is used as it allows us to compare a multitude
of different configurations of [N, B] 2 {[16, 14], [16, 12], . . . , [16, 0], [8, 6], [8, 4], [8, 2], [8, 0], [4, 2], [4, 0], [2, 0]}. The acquisition function in our batch variant of GP-UCB (2) is used as the performance metric to evaluate the approximation
quality of the batch DT (i.e., by plugging DT into (2) to compute the value of the acquisition function) produced by our
DB-GP-UCB algorithm (5) for each configuration of [N, B].
Fig. 4 (top) shows results of the normalized values of the acquisition function in (2) achieved by plugging in the batch DT
produced by DP-GP-UCB (5) for different configurations of [N, B] such that the optimal value of (2) (i.e., achieved in the
case of N = 1) is normalized to 1. Fig. 4 (bottom) shows the corresponding time complexity of DP-GP-UCB plotted in
log|D| -scale, thus displaying the values of (B + 1)|DT |/N . It can be observed that the approximation quality improves
near-linearly with an increasing Markov order B at the expense of higher computational cost (i.e., exponential in B).
Value of acq. function

1.02
1
0.98
0.96
0.94
0.92
B=N-1
N=1 [16,14] [16,12] [16,10] [16,8] [16,6] [16,4] [16,2] [16,0]

[8,6]

[8,4]

[8,2]

[8,0]

[4,2]

[4,0]

[2,0]

B=N-1
N=1 [16,14] [16,12] [16,10] [16,8] [16,6] [16,4] [16,2] [16,0]

[8,6]

[8,4]

[8,2]

[8,0]

[4,2]

[4,0]

[2,0]

Time complexity

20
15
10
5
0

Configuration of [N,B]

Figure 4. (Top) Mean of the normalized value of the acquisition function in (2) (over 64 experiments of randomly selected noisy observations of size 5) achieved by plugging in the batch DT (of size 16) produced by our DP-GP-UCB algorithm (5) for different configurations
of [N, B] (including the case of N = 1 yielding the optimal value of (2)); note that the horizontal line is set at the optimal baseline of
y = 1 for easy comparison and the y-axis starts at y = 0.915. (Bottom) Time complexity of DP-GP-UCB for different configurations
of [N, B] plotted in log|D| -scale.

Distributed Batch Gaussian Process Optimization

K. Replication of Regret Graphs including Error Bars
6

6
DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP
qEI

4

Regret

Regret

4

5

3

12
10

3

2

2

1

1

0

0

DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP
qEI

14

Regret

5

16
DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP
qEI

8
6
4

5

10

15

20

25

30

2
5

10

15

T
3

25

0

30

1

0.5

0.5

4

6

8

10

12

14

2

4

6

8

10

12

14

1.2

2

4

6

8

10

12

14

16

6

7

8

3.5

4

T
4

DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

1.2
1

DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

3.5
3
2.5

0.8

0.8

Regret

Regret

1

Regret

3

0

16

1.4

1.4

30

4

T

DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

25

1

T
1.6

20

2

0

16

5

1.5

1

2

15

DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

6

Regret

2

1.5

0

10

7
DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

2.5

Regret

2

5

T

3
DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

2.5

Regret

20

T

0.6

0.6

2
1.5

0.4

0.4
0.2

0.2

0

0

1

2

3

4

5

6

7

8

1
0.5
1

2

3

4

T

5

6

7

0

8

0.9

0.7

2

3

4

5

T
2

0.8
DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

0.8

1

T

DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

0.7
0.6

DB-GP-UCB
GP-UCB-PE
GP-BUCB
SM-UCB
BBO-LP

1.5

0.5

0.5

Regret

Regret

Regret

0.6

0.4

1

0.4
0.3

0.3

0.1

0.5

0.2

0.2
1

1.5

2

2.5

T

Branin-Hoo

3

3.5

4

0.1

1

1.5

2

2.5

T

gSobol

3

3.5

4

0

1

1.5

2

2.5

3

T

pH field

Figure 5. Cumulative regret incurred by tested algorithms with varying batch sizes |DT | = 2, 4, 8, 16 (rows from top to bottom) using a
fixed budget of T |DT | = 64 function evaluations for the Branin-Hoo function, gSobol function, and real-world pH field. The error bars
denote the standard error.

