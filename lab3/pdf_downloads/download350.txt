Coordinated Multi-Agent Imitation Learning

A. Variational Inference Derivation for
Hidden Markov Models
In this section, we provide the mathematical derivation for
the structured variational inference procedure. We focus
on the training for Bayesian Hidden Markov Model, in particular the Forward-Backward procedure to complete the
description of Algorithm 3. The mathematical details for
other types of graphical models depend on the family of
such models and should follow similar derivations. Further
relevant details on stochastic variational inference can be
found in (Hoffman et al., 2013; Johnson & Willsky, 2014;
Beal, 2003). d
Settings. Given an arbitrarily ordered set of trajectories U “ tU1 , . . . , UK , Cu, let the coordination mechanism underlying each such U be governed by a true unknown model p, with global parameters θ. We suppress
the agent/policy subscript and consider a generic featurized
trajectory xt “ rut , ct s @t. Let the latent role sequence for
the same agent be z “ z1:T .
At any time t, each agent is acting according to a latent role
zt „ Categoricalt1̄, 2̄, . . . , K̄u, which are the local
parameters to the structured model.
Ideally, role and index asignment can be obtained by calculating the posterior ppz|x, θq, which is often intractable.
One way to infer the role assignment is via approximating the intractable posterior ppz|x, θq using Bayesian inference, typically via MCMC or mean-field variational methods. Since sampling-based MCMC methods are often slow,
we instead aim to learn to approximate ppz|x, θq by a simpler distribution q via Bayesian inference. In particular,
we employ techniques from stochastic variational inference
(Hoffman et al., 2013), which allows for efficient stochastic
training on mini-batches that can naturally integrate with
our imitation learning subroutine.
Structured Variational Inference for Unsupervised Role
Learning. Consider a full probabilistic model:
ppθ, z, xq “ ppθq

T
ź

ppzt |θqppxt |zt , θq

For unsupervised structured prediction problem over a family of graphical model, we focus on the structured meanfield variational family, which factorizes q as qpz, θq “
qpzqqpθq (Hoffman & Blei, 2014) and decomposes the
ELBO objective:
L “ Eq rlog ppθs ´ Eq rlog qpθs
` Eq rlogpppz, x|θqs ´ Eq rlogpqpzqqs.

(3)

This factorization breaks the dependency between θ and z,
but not between single latent states zt , unlike variational
inference for i.i.d data (Kingma & Welling, 2013).
Variational inference optimizes the objective L typically
using natural gradient ascent over global factors qpθq and
local factors qpzq. (Under mean-field assumption, optimization typically proceeds via alternating updates of θ
and z.) Stochastic variational inference performs such updates efficiently in mini-batches. For graphical models,
structured stochastic variational inference optimizes L using natural gradient ascent over global factors qpθq and
message-passing scheme over local factors qpzq. We assume the prior ppθq and complete conditionals ppzt , xt |θq
are conjugate pairs of exponential family, which gives natural gradient of L with respect to qpθq convenient forms
(Johnson & Willsky, 2014). Denote the exponential family
forms of ppθq and ppzt , yt |θq by:
ln ppθq “ xηθ , tθ pθqy ´ Aθ pηθ q
ln ppzt , xt |θq “ xηzx pθq, tzx pzt , xt qy ´ Azx pηzx pθqq
where ηθ and ηzx are functions indicating natural parameters, tθ and tzx are sufficient statistics and Ap¨q are lognormalizers ((Blei et al., 2017)). Note that in general, different subscripts corresponding to η, t, A indicate different
function parameterization (not simply a change in variable
value assignment). Conjugacy in the exponential family
yields that (Blei et al., 2017):
tθ pθq “ rηzx pθq, ´Azx pηzx pθqqs
and that

t“1

ppθ|zt , xt q9 exptxηθ ` rtzx pzt , xt q, 1s , tθ pθqyu
with global latent variables θ, local latent variables z “
tzt uTt“1 . Posterior approximation is often cast as optimizing over a simpler model class Q, via searching for global
parameters θ and local latent variables z that maximize the
evidence lower bound (ELBO) L:
log ppxq ě Eq rlog ppz, θ, xqs ´ Eq rlog qpz, θqs
fi L pqpz, θqq .
Maximizing L is equivalent to finding q P Q to minimize
the KL divergence KL pqpz, θ|xq||ppz, θ|xqq.

(4)

Conjugacy in the exponential family also implies that the
optimal qpθq is in the same family (Blei et al., 2017), i.e.
qpθq “ exptxr
ηθ , tθ pθqy ´ Aθ pr
ηθ qu
for some natural parameters ηrθ of qpθq.
To optimize over global parameters qpθq, conjugacy in
the exponential family allows obtaining convenient expression for the gradient of L with respect to natural parameters ηrθ . The derivation is shown similarly to (Johnson

Coordinated Multi-Agent Imitation Learning

& Willsky, 2014) and (Blei et al., 2017) - we use simplified notations ηr fi ηrθ , η fi ηθ , A fi Aθ , and tpz, xq fi
řT
t“1 rtzx pzt , xt q, 1s. Taking advantage of the exponential
η q, the objective L can
family identity Eqpθq rtθ pθqs “ ∇Apr
be re-written as:
L “ Eqpθqqpzq rln ppθ|z, xq ´ ln qpθqs
“ xη ` Eqpzq rtpz, xqs, ∇Apr
η qy ´ pxr
η , ∇Apr
η qy ´ Apr
η qq
Differentiating with respect to ηr, we have that
`
˘`
˘
∇ηr L “ ∇2 Apr
η q η ` Eqpzq rtpz, xqs ´ ηr

»

r ηr , is defined as ∇
r ηr fi
The natural gradient of L, denoted ∇
` 2
˘´1
∇ Apr
ηq
∇ηr . And so the natural gradient of L can be
compactly described as:
r ηr L “ η `
∇

T
ÿ

Eqpzt q trtzx pzt , xt q, 1su ´ ηr

(5)

fi
p1
— ffi
P “ – ... fl
pK
The Bayesian hierarchical model over the parameters, hidden state sequence z1:T , and observation sequence y1:T is
iid

φi „ ppφq, pi „ Dirpαi q

t“1

Thus a stochastic natural descent update on the global parameters ηrθ proceeds at step n by sampling a mini-batch xt
and taking the global update with step size ρn :
ηrθ Ð p1 ´ ρn qr
ηθ ` ρn pηθ ` bJ Eq˚ pzt q rtpzt , xt qsq

Consider a Bayesian HMM on K latent states. Priors on
the model parameters include the initial state distribution
p0 , transition matrix P with rows denoted p1 , . . . , pK , and
the emission parameters φ “ tφi uK
i“1 . In this case we have
the global parameters θ “ pp0 , P, φq. For Hidden Markov
Model with observation x1:T and latent sequence z1:T , the
generative model over the parameters is given by φi „
ppφq (i.i.d from prior), pi „ Dirpαi q, z1 „ p0 , zt`1 „ pzt ,
and xt „ ppxt |φzt q (conditional distribution given parameters φ). We can also write the transition matrix:

(6)

where b is a vector of scaling factors adjusting for the relative size of the mini-batches. Here the global update assumes optimal local update q ˚ pzq has been computed. In
each step however, the local factors q ˚ pzt q are computed
with mean field updates and the current value of qpθq (analogous to coordinate ascent). In what follows, we provide
the derivation for the update rules for Hidden Markov Models, which are the particular instantiation of the graphical
model we use to represent the role transition for our multiagent settings.
Variational factor updates via message passing for Hidden Markov Models. For HMMs, we can view global parameters θ as the parameters of the underlying HMMs such
as transition matrix and emission probabilities, while local
parameters z govern hidden state assignment at each time
step.
Fixing the global parameters, the local updates are based on
message passing over the graphical model. The exact mathematical derivation depends on the specific graph structure.
The simplest scenario is to assume independence among
zt ’s, which resembles naive Bayes. We instead focus on
Hidden Markov Models to capture first-order dependencies in role transitions over play sequences. In this case,
K
global parameters θ “ pp0 , P, φq where P “ rPij si,j“1
is the transition matrix with Pij “ ppzt “ j|zt´1 “ iq,
φ “ tφi uK
i“1 are the emission parameters, and p0 is the
initial distribution.

z1 „ p0 , zt`1 „ pzt , xt „ ppxt |φzt q
For HMMs, we have a full probabilistic model: ppz, x|θq “
śT
p0 pz1 q t“1 ppzt |zt´1 , P qppxt |zt , φq. Define the likelihood potential Lt,i “ ppxt |φi q, the likelihood of the latent
sequence, given observation and model parameters, is as
follows:
ppz1:T |x1:T , P, φq “
˜
¸
T
T
ÿ
ÿ
exp log p0 pz1 q `
log Pzt´1 ,zt `
log Lt,zt ´ Z
t“2

t“1

(7)
where Z is the normalizing constant. Following the notation and derivation from (Johnson & Willsky, 2014),
we denote ppz1:T |x1:T ,P,φ q “ HMMpp0 , P, Lq. Under
mean field assumption, we approximate the true posterior ppP, φ, z1:T |x1:T q with a mean field variational family
qpP qqpφqqpz1:T q and update each variational factor in turn
while fixing the others.
Fixing the global parameters θ, taking expectation of log
of (7), we derive the update rule for qpzq as qpz1:T q “
r where:
HMMpPr, pr0 , Lq
Prj,k “ exptEqpP q lnpPj,k qu
pr0,k “ exptln Eqpp0 q p0,k u
r t,k “ exptEqpφ q lnpppxt |zt “ kqqu
L
k
To calculate the expectation with respect to qpz1:T q,
which is necessary for updating other factors, the
Forward-Backward recursion of HMMs is defined by

Coordinated Multi-Agent Imitation Learning

Algorithm 5 Coordinated Structure Learning
LearnStructure tU1 , . . . , UK , C, θ, ρu ÞÑ qpθ, zq

forward messages F and backward messages B:
Ft,i “

K
ÿ

r t,i
Ft´1,j Prj,i L

(8)

r t`1,j Bt`1,j
Pri,j L

(9)

j“1

Bt,i “

K
ÿ
j“1

F1,i “ p0 piq
BT,i “ 1
As a summary, calculating the gradient w.r.t z yields the
following optimal variational distribution over the latent sequence:
T
´
ÿ
q ˚ pzq9 exp EqpP q rln p0 pz1 qs `
EqpP q rlog Pzt´1 ,zt s

Input: Set of trajectories U “ tUk uK
k“1 . Context C
Previous parameters θ “ pp0 , θP , θφ q, stepsize ρ
1: Xk “ txt,k uTt“1 “ trut,k , ct su @t, k.X “ tXk uK
k“1
2: Local update: Compute Pr and pr per equation 11 and 12
and compute qpzq “ Forward-BackwardpX, Pr , prq
3: Global update of θ, per equations 16, 17, and 18.
output Updated model qpθ, zq “ qpθqqpzq

Using message passing scheme as per equations (8) and (9),
we define the intermediate quantities:
p
tx,i fi Eqpz1:T q

`

T
ÿ

“

¯
Eqpφq lnrppxt |zt qs ,

(10)

Ft,i Bt,i rtx,i pxt q, 1s{Z

which gives the local updates for q pzq, given current estimates of P and φ:
“
‰
Prj,k “ exp EqpP q lnpPj,k q
(11)
“
‰
prpxt |zt “ kq “ exp Eqpφq ln ppxt |xt “ kq ,
(12)
for k “ 1, . . . , K, t “ 1, . . . , T , and then use p0 , Pr, pr
to run the forward-backward algorithm to compute the update q ˚ pzt “ kq and q ˚ pzt´1 “ j, zt “ kq. The forwardbackward algorithm in the local update step takes OpK 2 T q
time for a chain of length T and K hidden states.
Training to learn model parameters for HMMs. Combining natural gradient step with message-passing scheme
for HMMs yield specific update rules for learning the
model parameters. Again for HMMs, the global parameters
are θ “ pp0 , P, φq and local variables z “ z1:T . Assuming
the priors on observation parameter ppφi q and likelihoods
ppxt |φi q are conjugate pairs of exponential family distribution for all i, the conditionals ppφi |xq have the form as seen
from equation 4:
ppφi |xq9 exptxηφi ` rtx,i pxq, 1s, tφi pφi qyu
For structured mean field inference, the approximation qpθq
factorizes as qpP qqpp0 qqpφq. At each iteration, stochastic
variational inference sample a sequence x1:T from the data
set (e.g. trajectory from any randomly sampled player) and
perform stochastic gradient step on qpP qqpp0 qqpφq. In order to compute the gradient, we need to calculate expected
sufficient statistics w.r.t the optimal factor for qpz1:T q,
which in turns depends on current value of qpP qqpp0 qqpφq.
Following the notation from (Johnson & Willsky, 2014),
we write the prior and mean field factors as
pppi q “ Dirpαi q, ppφi q9 exptxηφi , tφi pφi qyu
qppi q “ Dirpr
αi q, qpφi q9 exptxr
ηφi , tφi pφi qyu

(13)

t“1

t“1
˚

Irzt “ istx,i pxt q

t“1

t“2
T
ÿ

T
ÿ

pp
ttrans,i qj fi Eqpz1:T q

Tÿ
´1

Irzt “ i, zt`1 “ js

t“1
Tÿ
´1

“

r t`1,j Bt`1,j {Z
Ft,i Pri,j L

(14)

t“1

pp
tinit qi fi Eqpz1:T q Irz1 “ is “ pr0 B1,i {Z

(15)

řK
where Z fi i“1 FT,i is the normalizing constant, and I is
the indicator function.
Given these expected sufficient statistics, the specific update rules corresponding to the natural gradient step in the
natural parameters of qpP q, qpp0 q, and qpφq become:
ηrφ,i Ð p1 ´ ρqr
ηφ,i ` ρpηφ,i ` bJ p
tx,i q
ri Ð p1 ´ ρqr
α
αi ` ρpαi ` bJ p
ttrans,i q
α
r0 Ð p1 ´ ρqr
α0 ` ρpα0 `

bJ p
tinit,i q

(16)
(17)
(18)

B. Experimental Evaluation
B.1. Batch-Version of Algorithm 2 for Predator-Prey
B.2. Visualizing Role Assignment for Soccer
The Gaussian components of latent structure in figure 7
give interesting insight about the latent structure of the
demonstration data, which correspond to a popular formation arrangement in professional soccer. Unlike the
predator-prey domain, however, the players are sometimes
expected to switch and swap roles. Figure 8 displays the
tendency that each learning policy k would takes on other
roles outside of its dominant mode. Policies indexed 0 ´ 3
tend to stay most consistent with the prescribed latent roles.
We observe that these also correspond to players with the
least variance in their action trajectories. Imitation loss is

Coordinated Multi-Agent Imitation Learning

Algorithm 6 Multi-Agent Data Aggregation Imitation
Learning
LearnpA1 , A2 , . . . , AK , C|Dq
Input: Ordered actions Ak “ tat,k uTt“1 @k, context
tct uTt“1
Input: Aggregating data set D1 , .., DK for each policy
Input: base routine TrainpS, Aq mapping state to actions
1: for t “ 0, 1, 2, . . . , T do
2:
Roll-out ât`1,k “ πk pŝt,k q @ agent k
3:
Cross-update for each policy k P t1, . . . , Ku
ŝt`1,k “ ϕk prât`1,1 , . . . , ât`1,k , . . . , ât`1,K , ct`1 sq
4:
Collect expert action a˚t`1,k given state ŝt`1,k @k
´1
5:
Aggregate data set Dk “ Dk Y tŝt`1,k , a˚t`1,k uTt“0
6: end for
7: πk Ð TrainpDk q
output K new policies π1 , π2 , . . . , πK

Figure 8. Role frequency assigned to policy, according to the maximum likelihood estimate of the latent structured model

generally higher for less consistent roles (e.g. policies indexed 8 ´ 9). Intuitively, entropy regularization encourages
a decomposition of roles that result in learning policies as
decoupled as possible, in order to minimize the imitation
loss.

