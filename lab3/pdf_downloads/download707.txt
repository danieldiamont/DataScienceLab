Supplementary Material: Asynchronous Stochastic Gradient Descent with
Delay Compensation

A. Theorem 3.1 and Its Proof
Theorem 3.1:
Assume the loss function is L1 -Lipschitz. If λ ∈ [0, 1] make the following inequality holds,
K
∑
k=1



(K
)2
∑
′
1
1
2
≥ 2 Cij
+ Cij L1 |ϵt | ,
σk3 (x, wt )
σk (x, wt )

(1)

k=1

′

u u β

1
1
where Cij = 1+λ
( li lij √j α )2 , Cij = (1+λ)α(l
2 , and the model converges to the optimal model, then the MSE of λG(wt )
i lj )
is smaller than the MSE of G(wt ) in approximating Hessian H(wt ).

Proof:
For simplicity, we abbreviate E(Y |x,w∗ ) as E, Gt as G(wt ) and Ht as H(wt ). First, we calculate the MSE of Gt , λGt
to approximate Ht for each element of Gt . We denote the element in the i-th row and j-th column of G(wt ) as Gtij and
H(wt ) as Hij (t).
The MSE of Gtij :
t 2
t
E(Gtij − EHij
) = E(Gtij − EGtij )2 + (EHij
− EGtij )2 = E(Gtij )2 − (EGtij )2 + ϵ2t

(2)

The MSE of λgij :
t 2
t
E(λGtij − EHij
) = λ2 E(Gtij − EGtij )2 + (EHij
− λEGtij )2

= λ2 E(Gtij )2 − λ2 (EGtij )2 + (1 − λ)2 (EGtij )2 + ϵ2t + 2(λ − 1)EGtij ϵt

(3)

t 2
t 2
The condition for E(Gtij − EHij
) ≥ E(λGtij − EHij
) is

(1 − λ2 )(E(Gtij )2 − (EGtij )2 ) ≥ 2(1 − λ)(EGtij )2 + 2(λ − 1)EGtij ϵt

(4)

Inequality (4) is equivalent to
(1 + λ)E(Gtij )2 ≥ 2[(EGtij )2 − EGtij ϵt ]

(5)

Next we calculate E(Gtij )2 , and (EGtij )2 which appear in Eqn.(5). For simplicity, we denote σk (x, wt ) as σk , and I[Y =k]

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

as zk . Then we can get:
(

)2 (
)2
∂
∂
log P (Y |x, wt )
log P (Y |x, wt )
∂wi
∂wj
(K (
)4
)
∑
zk
≥ E(Y |x,w∗ )
−
(li lj )2
σk
k=1
(K
)
∑
1
2
= α (li lj )
σ 3 (x, wt )
k=1 k
(
(
) ∑
(
))2
K
K
∑
∂σk
zk
∂σk
zk
2
(Ehij ) = E(Y |x,w∗ )
−
·
−
∂wi
σk
∂wj
σk
k=1
k=1
(K
)2
∑
1
≤ β 2 (ui uj )2
.
σk (x, wt )
E(gij )2 = E(Y |x,wt )

(6)

(7)

(8)

k=1

By substituting Ineq.(7) and Ineq.(8) into Ineq.(5), a sufficient condition for Ineq.(5) to be satisfied is
[

2 Cij

(∑
K

1
k=1 σk (x,wt )

)2

]

′

∑K

≥

1
k=1 σ 3 (x,wt )
k

+ Cij L21 |ϵt | because Gtij ≤ L21 . 

B. Corollary 3.2 and Its Proof
Corollary 3.2:
[
K−1
1−
′
2

A sufficient
condition for inequality (1) is λ ∈ [0, 1] and ∃k0
]

2(Cij K +Cij L2
1 ϵt )

∈ [K] such that σk0

,1 .

Proof:
(∑
∑K
K
1
Denote ∆ = 2CK−1
3 (x,w ) − 2Cij
2 and F (σ1 , ..., σK ) =
k=1
k=1 σk
ij K
t
that σk1 ∈ [1 − ∆, 1], we have for k ̸= k1 σk ∈ [0, ∆]. Therefore

F (σ1 , ..., σK ) ≥
≥
≥
=
≥
≥

1
σk (x,wt )

)2

′

− 2Cij L21 |ϵt |. If ∃k1 ∈ [K] such

(
)2
′
K −1
1
K −1
+
− 2Cij
+
− 2Cij L21 |ϵt |
(σk1 )3
∆3
σk1
∆
((
)
)2
′
K −1
K −1
1
2(K − 1)
− 2Cij
+ 2 +
− 2Cij L21 |ϵt |
3
∆
∆
σk1
σk1 ∆
(
)
2
′
K −1
(K − 1)
2K − 1
− 2Cij
+
− 2Cij L21 |ϵt |
3
2
∆
∆
σk1 ∆
(
(
))
′
1 K −1
(K − 1)2
2K − 1
−
2C
+
− 2Cij L21 |ϵt |
ij
2
∆
∆
∆
σk1
(
(
))
2
′
1 K −1
(K − 1) + 2K − 1
−
2C
− 2Cij L21 |ϵt |
ij
2
∆
∆
∆
(
)
′
1
K −1
2
2
−
2C
K
−
2C
L
|ϵ
|
ij
ij 1 t
∆2
∆
1

=0

where Ineq.(11) and (13) is established since σk1 > ∆; and Eqn.(15) is established by putting ∆ =
Eqn.(14). 

∈

(9)
(10)
(11)
(12)
(13)
(14)
(15)

K−1
′
2(Cij K 2 +Cij L21 |ϵt |)

in

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

C. Uniform upper bound of MSE
Lemma C.1 Assume the loss function is L1 -Lipschitz, and the diagonalization error of Hessian is upper bounded by ϵD ,
i.e., ||Diag(H(wt )) − H(wt )|| ≤ ϵD , 1 then we have, for ∀t,
mset (Diag(λG)) ≤ 4λ2 V1 + 4(1 − λ)2 L41 + 4ϵ2t + 4ϵD ,

(16)

where V1 is the upper bound of the variance of G(wt ).
Proof:
mset (Diag(λG))

(17)

≤E∥Diag(λG(wt )) − H(wt )∥

2

(18)

≤4E∥Diag(λG(wt )) − E(Diag(λG(wt )))∥ + 4∥E(Diag(λG(wt ))) − E(Diag(G(wt )))∥
2

2

+ 4∥E(Diag(G(wt ))) − E(Diag(H(wt )))∥ + 4∥E(Diag(H(wt ))) − EH(wt )∥
2

≤4λ V1 + 4(1 − λ)
2

2

L41

+

4ϵ2t

2

+ 4ϵD

(19)
(20)
(21)

D. Convergence Rate for DC-ASGD: Convex Case
DC-ASGD is a general method to compensate delay in ASGD. We first show the convergence rate for convex loss function.
If the loss function f (w) is convex about w, we can add a regularization term ρ2 ∥w∥2 to make the objective function
F (w) + ρ2 ∥w∥2 strongly convex. Thus, we assume that the objective function is µ-strongly convex.
Theorem 4.1: (Strongly Convex) If f (w) is L2 -smooth and µ-strongly convex about w, ∇f (w) is L3 -smooth about w
and the expectation of the ∥ · ∥22 norm of the delay compensated gradient is upper bounded by a constant G. By setting the
1
learning rate ηt = µt
, DC-ASGD has convergence rate as
EF (wt ) − F (w∗ ) ≤

√
2L22 G2
2G2 L22 θ τ
L3 L32 τ 2 G3
√
(1
+
4τ
C
)
+
+
,
λ
4
tµ
µ 6 t2
µ4 t t

√
L2
τ GL3
2
where θ = 2HKLG
µ
µ (1 + µL2 ) and Cλ = (1 − λ)L1 + ϵD , and the expectation is taking with respect to the random
sampling of DC-ASGD and E(y|x,w∗ ) .
Proof:
We denote g dc (wt ) = g(wt ) + λg(wt ) ⊙ g(wt ) ⊙ (wt+τ − wt ), g h (wt ) = g(wt ) + Hit (wt )(wt+τ − wt ) and ∇F h (wt ) =
∇F (wt ) + Eit Hit (wt )(wt+τ − wt ). Obviously, we have Eg h (wt ) = ∇F h (wt ). By the smoothness condition, we have
EF (wt+τ +1 ) − F (w∗ )
≤
≤
=

(22)

L2
F (wt+τ ) − F (w ) − ⟨∇F (wt+τ ), wt+τ +1 − wt+τ ⟩ +
∥wt+τ +1 − wt+τ ∥2
2
2
L2 ηt+τ
G2
F (wt+τ ) − F (w∗ ) − ηt+τ ⟨∇F (wt+τ ), g dc (wt )⟩ +
2
F (wt+τ ) − F (w∗ ) − ηt+τ ⟨∇F (wt+τ ), ∇F (wt+τ )⟩ + ηt+τ ⟨∇F (wt+τ ), ∇F (wt+τ ) − ∇F h (wt )⟩
∗

2
L2 ηt+τ
G2
+ηt+τ ⟨∇F (wt+τ ), Eg h (wt ) − g dc (wt )⟩ +
2

(23)
(24)
(25)
(26)

Since f (w) is L2 -smooth and µ strongly convex, we have
−⟨∇F (wt+τ ), ∇F (wt+τ )⟩ ≤ −µ2 ∥wt+τ − w∗ ∥2 ≤ −

2µ2
(F (wt+τ ) − F (w∗ )).
L2

(27)

1
(LeCun, 1987) demonstrated that the diagonal approximation to Hessian for neural networks is an efficient method with no much
drop on accuracy

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

For the term ηt+τ ⟨∇F (wt+τ ), ∇F (wt+τ ) − ∇F h (wt )⟩, we have
ηt+τ ⟨∇F (wt+τ ), ∇F (wt+τ ) − ∇F h (wt )⟩

(28)

≤

ηt+τ ∥∇F (wt+τ )∥∥∇F (wt+τ ) − ∇F (wt )∥

(29)

≤

ηt+τ G∥∇F (wt+τ ) − ∇F h (wt )∥

(30)

h

By the smoothness condition for ∇F (w), we have
∥∇F (wt+τ ) − ∇F h (wt )∥ ≤

Let ηt =

L2
µ2 t ,

we can get

∑τ
j=1

2
ηt+j
≤

L22
µ4

·

≤

τ
t(t+τ )

τ −1
L3
L3 τ G2 ∑ 2
∥wt+τ − wt ∥2 ≤
ηt+j
2
2
j=0

(31)

2L22 τ
µ4 (t+τ )2 .

For the term ηt+τ ⟨∇F (wt+τ ), Eg h (wt ) − g dc (wt )⟩, we have
⟨∇F (wt+τ ), E(g h (wt ) − g dc (wt ))⟩
≤ ∥∇F (wt+τ )∥∥E(λg(wt ) ⊙ g(wt ) − H(wt ))(wt+τ − wt )∥
≤ G2 τ

τ
−1
∑

(32)
(33)

ηt+j (∥E(λg(wt ) ⊙ g(wt ) − g(wt ) ⊙ g(wt )∥ + ∥g(wt ) ⊙ g(wt ) − Diag(H(wt ))∥ + ∥Diag(H(wt )) − H(wt )∥)

j=0

(34)
2G2 L2 τ
≤
(Cλ + ϵt ),
(t + τ )µ2

(35)

where Cλ = (1 − λ)L21 + ϵD .
√
√
τ
Using Lemma F.1, ϵt ≤ θ 1t ≤ θ t+τ
. Putting inequality 27 and 31 in inequality 26, we have
(

)

L3 L3 τ 2 G3
(EF (wt ) − F (w∗ )) + 6 2
µ (t + τ )3
(
)
√
2 2
2G L2 τ
τ
L22 G2
+ 4
Cλ + θ
+
2
µ (t + τ )
t+τ
2(t + τ )2 µ4

EF (wt+τ +1 ) − F (w∗ ) ≤

1−

2
t+τ

(36)
(37)

We can get
EF (wt ) − F (w∗ ) ≤

√
2L22 G2
L3 L32 τ 2 G3
2G2 L22 θ τ
√
+
(1
+
4τ
C
)
+
.
λ
4
4
tµ
µ6 t2
µ t t

(38)

by induction. 
Discussion:
(1). Following the above proof steps and using ∥∇F (wt+τ ) − ∇F (wt )∥ ≤ L2 ∥wt+τ − wt ∥, we can get the convergence
rate of ASGD is
EF (wt ) − F (w∗ ) ≤

2L22 G2
(1 + 4τ L2 ) .
tµ4

Compared the convergence rate of DC-ASGD with ASGD, the extra term

√
2G2 L22 θ τ
√
µ4 t t

(39)

+

L3 L32 τ 2 G3
µ 6 t2

converge to zero faster

2L22 G2
tµ4

than
(1 + 4τ Cλ ) in terms of the order of t. Thus, when t is large, the extra term has smaller value. We assume that
t is large and the term can be neglected. Then the condition for DC-ASGD outperforming ASGD is L2 > Cλ .

E. Convergence Rate for DC-ASGD: Nonconvex Case
Theorem 5.1: (Nonconvex Case) Assume that Assumptions 1-4 hold. Set the learning rate
√
ηt =

2(F (w1 ) − F (w∗ )
,
bT V 2 L2

(40)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

where b is the mini-batch size, and V is the upper bound of the variance of the delay-compensated gradient. If T ≥
max{O(1/r4 ), 2D0 bL2 /V 2 } and delay τ is upper-bounded as below,
{
τ ≤ min

L2 V
Cλ

√

L2 T V
,
2D0 b Cλ

√

L2 T T V
,
2D0 b C̃

√

L2 V L2 T
,
bD0 4C̃

√

}
T L2
2D0 b

.

(41)

then DC-ASGD has the following ergodic convergence rate,
√
min

t={1,··· ,T }

E(∥∇F (wt )∥ ) ≤ V
2

2D0 L2
,
bT

(42)

where the expectation is taken with respect to the random sampling in SGD and the data distribution P (Y |x, w∗ ).
Proof:
dc
We denote gm (wt ) + λgm (wt ) ⊙ gm (wt ) ⊙ (wt+τ − wt ) as gm
(wt ) where m ∈ {1, · · · , b} is the index of instances in the
minibatch. From the proof the Theorem 1 in ASGD (Lian et al., 2015), we can get

EF (wt+τ +1 ) − F (wt+τ )
≤
≤

≤

(43)

L2
⟨∇F (wt+τ ), wt+τ − wt ⟩ +
∥wt+τ +1 − wt+τ ∥2
2

2 
b
b


2
∑
∑
η L2


dc
dc
−ηt+τ ⟨∇F (wt+τ ),
Egm
(wt )⟩ + t+τ E 
gm
(wt ) 


2
m=1
m=1

2 
2 
 b
b



∑
∑
bηt+τ 




dc
dc
2
Egm (wt ) 
Egm (wt ) − ∇F (wt+τ ) −
−
∥∇F (wt+τ )∥ + 




2
m=1
m=1



b
2
∑
η 2 L2


dc
+ t+τ E 
gm
(wt ) 


2

(44)
(45)

(46)

m=1


2
∑b


dc
For the term T1 = ∇F (wt+τ ) − m=1 Egm
(wt ) , by using the smooth condition of g, we have

T1

=

≤
≤


2
b


∑


dc
Egm (wt )
∇F (wt+τ ) −


m=1

2
b


∑


h
h
dc
Egm (wt )
∇F (wt+τ ) − ∇F (wt ) + ∇F (wt ) −


m=1

2

2
b


∑
 L3



2
h
dc

2
∇F (wt ) −
Egm (wt )
 2 ∥wt+τ − wt ∥  + 2 



(47)

(48)

(49)

m=1

≤

(L23 π 2 /2 + 2(((1 − λ)L21 + ϵD )2 + ϵ2t ))∥wt+τ − wt ∥2

(50)

Thus by following the proof of ASGD, we have
(

2 )


2
2
dc
E(T1 ) ≤ 4(L23 π 2 /4 + ((1 − λ)L21 + ϵD )2 + ϵ2t ) bτ ηt+τ
V 2 + τ 2 ηt+τ
bEgm (wt ) .

(51)

(
2 )
∑b

dc
For the term T2 = E  m=1 gm (wt ) , it has

2


dc
E(T2 ) ≤ bV 2 + bEgm
(wt ) .

(52)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

By putting Ineq.(51) and Ineq.(52) in Ineq.(46), we can get
E(F (wt+τ +1 ) − F (wt+τ )

≤

(53)

) (
( 2
2 )
ηt+τ L2
bηt+τ
ηt+τ


dc
−
E∥∇F (wt+τ )∥2 +
−
E bEgm
(wt )
2
2
2b
)
( 2
ηt+τ bL2
3
V2
+
+ (L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + ϵ2t )b2 τ ηt+τ
2
(
2 )


dc
2 2
2
2
2
2 3
+(L3 π /2 + 2((1 − λ)L1 + ϵD ) + ϵt )bτ ηt+τ E bEgm (wt )

(54)
(55)

Summarizing the Ineq.(55) from t = 1 to t + τ = T , we have
EF (wT +1 ) − F (w1 )
T
∑

(56)
T (
∑

)

2
bL2
ηt+τ
3
V2
+ (L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + ϵ2t )b2 τ ηt+τ
2
t=1
t=1
) 
T ( 2
2
∑
ηt
ηt L2


dc
+
+ (L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + ϵ2t )bτ 2 ηt3 −
(wmax{t−τ,1} ) .
E bEgm
2
2b
t=1

≤−

b
2

ηt E∥∇F (wt )∥2 +

(57)

(58)

By Lemma F.1 and under our assumptions, we
√ have when t > T0 , wt will goes into a strongly convex neighbourhood of
some local optimal wloc . Thus, ϵt ≤ ϵnc + θ 1/(t − T0 ), when t > T0 and ϵt < maxs∈1,··· ,T0 ϵs when t < T0 .
√
∗
1 )−F (w )
Let ηt = 2(F (w
. It follows that
bT V 2 L2
T
∑
ηt L2
+ (L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + ϵ2t )bτ 2 ηt2
2
t=1
}
T {
∑
ηt L2
≤
+ (L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + 2ϵ2nc )bτ 2 ηt2 + 2bτ 2 ηt2 (4T0 max (ϵs )2 + 4θ2 log(T − T0 ))
s∈1,··· ,T0
2
t=1

(59)

(60)

We ignore the log(T − T0 ) term and regards C̃ 2 = 4T0 maxs∈1,··· ,T0 (ϵs )2 + 4θ2 log(T − T0 ) as a constant, which yields
T
∑
ηt L2
+ (L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + ϵ2t )bτ 2 ηt2
2
t=1
}
T {
∑
ηt L2
≤
+ (L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + 2ϵ2nc )bτ 2 ηt2 + 2τ 2 ηt2 bC̃ 2
2
t=1

(61)

(62)

ηt should be set to make
(
)
T
∑
2τ 2 ηt3 bC̃ 2
ηt
ηt2 L2
2 2
2
2
2
2 3
+ (L3 π /2 + 2((1 − λ)L1 + ϵD ) + 2ϵnc )bτ ηt +
−
≤ 0.
2
T
2b
t=1

(63)

Then we can get
T
1 ∑
E∥∇F (wt )∥2
T t=1

(64)
ηt3 C̃ 2 4bτ
T

≤

2(F (w1 ) − F (w∗ ) + T b(ηt2 L2 + 2(L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + 2ϵ2nc )bτ ηt3 )V 2 +
bT ηt

≤

2(F (w1 ) − F (w∗ )
η 2 C̃ 2 4bτ V 2
+ (ηt L2 + 2(L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + 2ϵ2nc )bτ ηt2 )V 2 + t
bT ηt
T

V2

(65)
(66)
(67)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

We set ηt to make
(2(L23 π 2 /2 + 2((1 − λ)L21 + ϵD )2 + 2ϵ2nc )bτ ηt2 ) +

√
Thus let ηt =

ηt2 C̃ 2 4bτ
≤ ηt L2
T

(68)

2(F (w1 )−F (w∗ )
,
bT V 2 L2
T
1 ∑
E∥∇F (wt )∥2 ≤ V
T t=1

√

2D0 L2
.
bT

(69)

And we can get the condition for T by putting η in ineq.63 and ineq.68, we can get that
{
τ ≤ min

L2 V
Cλ

√

L2 T V
,
2D0 b Cλ

√

L2 T T V
,
2D0 b C̃

√

L2 V L2 T
,
bD0 4C̃

√

}
T L2
2D0 b

.

(70)

F. Decreasing rate of the approximation error ϵt
Since ϵt is contained the proof of the convergence rate for DC-ASGD , in this section we will introduce a lemma which
describes the approximation error ϵt the for both convex and nonconvex cases.
Lemma F.1 Assume
that the true label y is generated according to the distribution P(Y = k|x, w∗ ) = σk (x, w∗ ) and
∑K
f (x, y, w) = − k=1 (I[y=k] log σk (x; w)). If we assume that the loss function is µ-strongly convex about w. We denote wt is
the output of DC-ASGD by using the outerproduct approximation of Hessian, we have
√
(
) (
)

∂2
∂
∂
1


ϵt = E(x,y|w∗ ) 2 f (x, y, wt ) − E(x,y|w∗ )
f (x, y, wt ) ⊗
f (x, y, wt )  ≤ θ
,
∂w
∂w
∂w
t

where θ =

2HKLV L2
µ2

√

1
µ (1

+

L2 +λL21
τ ).
L2

If
 2we assume that the loss
 function is µ-strongly convex in a neighborhood of each local optimal d(wloc , r),
 ∂ P(Y =k|x,w)

1
× P (Y =k|x,w)  ≤ H, ∀k, x, w, each σk (w) is L-Lipschitz continuous about w. We denote wt is the out
∂2w
put of DC-ASGD by using the outerproduct approximation of Hessian, we have
√
(
) (
)

∂
∂
1
∂2


f (x, y, wt ) ⊗
f (x, y, wt )  ≤ θ
+ ϵnc .
ϵt = E(x,y|w∗ ) 2 f (x, y, wt ) − E(x,y|w∗ )
∂w
∂w
∂w
t − T0

where t > T0 ≥ O( r18 ).
Proof:
∂2
∂2
E(y|x,w∗ )
f (x, Y, wt ) = −E(y|x,w∗ )
2
∂w
∂w2
2

(

K
∑
(I[y=k] log σk (x; wt ))
k=1

∂
= −E(y|x,w∗ )
log
∂w2
= −E(y|x,w∗ )
= −E(y|x,w∗ )
= −E(y|x,w∗ )
= −E(y|x,w∗ )

(

K
∏

)

)
I[y=k]

σk (x, wt )

k=1

∂2
log P(y|x, wt )
∂w2
∂2
P(y|x, wt )
∂ω 2

P(y|x, wt )

(

+ E(y|x,w∗ )

∂
P(y|x, wt )
∂ω

)2

P(y|x, wt )

)2
∂
log P(y|x, wt ) .
P(y|x, wt )
∂ω
2
(
)2
∂
P(y|x, wt )
∂
∂ω 2
+ E(y|x,w∗ )
f (x, Y, wt ) .
P(y|x, wt )
∂ω
∂2
P(y|x, wt )
∂ω 2

(

+ E(y|x,w∗ )

(71)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation
∂2

P(y|x,wt )

2
Since E(y|x,wt ) ∂ωP(y|x,w
t)
2001), we have




E(y|x,w∗ )


= 0 by the two equivalent methods to calculating fisher information matrix (Friedman et al.,




∂2
∂2

P(y|x, wt )
P(y|x, wt ) 

∂ω 2
∂ω 2
− E(y|x,wt )
 = E(y|x,w∗ )

P(y|x, wt )  
P(y|x, wt )
P(y|x, wt ) 


K
∑

P(Y = k|x, w∗ ) − P(Y = k|x, wt ) 
∂2

=
P(Y
=
k|X
=
x,
w
)
×

t


∂ω 2
P(Y = k|x, wt )

∂2
P(y|x, wt ) 
∂ω 2

(72)

k=1

≤H·

K
∑

|P(Y = k|x, w∗ ) − P(Y = k|x, wt )|

k=1

≤ HKL∥wt − wloc ∥ + HK

max

k=1,··· ,K

|P(Y = k|x, wloc ) − P(Y = k|x, w∗ )|

≤ HKL∥wt − wloc ∥ + ϵnc .

(73)
(74)

For strongly convex objective functions, ϵnc = 0 and wloc = w∗ . The only thing we need is to prove the convergence of
DC-ASGD without using the information of ϵt like before. By the smoothness condition, we have
EF (wt+τ +1 ) − F (w∗ )
≤
=

(75)

2
V2
L2 ηt+τ
F (wt+τ ) − F (w∗ ) − ηt+τ ⟨∇F (wt+τ ), Eg dc (wt )⟩ +
2
F (wt+τ ) − F (w∗ ) − ηt+τ ⟨∇F (wt+τ ), ∇F (wt+τ )⟩

+ηt+τ ⟨∇F (wt+τ ), ∇F (wt+τ ) − Eg dc (wt )⟩ +
≤
≤
≤

2
L2 ηt+τ
V2

(76)
(77)
(78)

2

2
L2 ηt+τ
V2
2ηt+τ µ2
)(F (wt+τ ) − F (w∗ )) + ηt+τ ∥∇F (wt+τ )∥∥∇F (wt+τ ) − Eg dc (wt )∥ +
L2
2
2
L2 ηt+τ
V2
2ηt+τ µ2
(1 −
)(F (wt+τ ) − F (w∗ )) + ηt+τ V · (L2 + λL21 )∥wt+τ − wt ∥ +
L2
2
τ
2
∑
L2 ηt+τ
V2
2ηt+τ µ2
(1 −
)(F (wt+τ ) − F (w∗ )) + ηt+τ V · (L2 + λL21 )∥
ηt+τ −j g dc (wt )∥ +
L2
2
j=1

(1 −

(79)
(80)
(81)

Taking expectation to the above inequality, we can get
EF (wt+τ +1 ) − F (w∗ )

≤
≤

Let ηt =

L2
µ2 t ,

2
η 2 (L2 + λL21 )V 2 τ
L2 ηt+τ
V2
2ηt+τ µ2
)(EF (wt+τ ) − F (w∗ )) + t+τ
+
L2
2
2
2
ηt+τ
V 2 L2
2ηt+τ µ2
L2 + λL21
∗
(1 −
)(EF (wt+τ ) − F (w )) +
(1 +
τ ).
L2
2
L2

(1 −

(82)
(83)

we have
EF (wt+1 ) − F (w∗ ) ≤

(
1−

2
t

)

(EF (wt ) − F (w∗ )) +

V 2 L22
2µ4 t2

(
)
L2 + λL21
1+
τ .
L2

(84)

We can get
EF (wt ) − F (w∗ ) ≤

2L22 V 2
tµ4

(
)
L2 + λL21
1+
τ .
L2

(85)

∥wt − w∗ ∥2 ≤

4L22 V 2
tµ5

(
)
L2 + λL21
1+
τ .
L2

(86)

by induction. Then we can get

By putting Ineq.86 into Ineq.73, we can get the result in the theorem.
For nonconvex case, if wt ∈ B(wloc , r), we have E(wt −wloc ) ≤ µ1 E∇F (wt ) under the assumptions. Next we will prove that,
∂
F (x, y, wt )∥2 =
for nonconvex loss function f (x, y, wt ), DC-ASGD has ergodic convergence rate. mint=1,··· ,T E∥ ∂w
t
√
O(1/ T ), where the expectation is taking with respect to the stochastic sampling.

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

M=8

0.15

M=8

0.130

SGD
Async SGD
DC-ASGD:¸0 = 0: 5
DC-ASGD:¸0 = 1
DC-ASGD:¸0 = 2

0.125
0.120
Test error

Training error

0.20

0.10
0.05

0.115
0.110
0.105
0.100
0.095
0.090

0.00
0

20

40

60 80 100 120 140 160
Epochs

0.085
80

90 100 110 120 130 140 150 160
Epochs

Figure 1. Error rates of the global model with Different λ0 w.r.t. number of effective passes on CIFAR-10

Compared with the proof of ASGD (Lian et al., 2015), DC-ASGD with Hessian approximation has
T1

=

∥∇F (wt+τ ) − Eg dc (wt )∥2

=

∥∇F (wt+τ ) − ∇F (wt ) − λEg(wt ) ⊙ g(wt ) · (wt+τ − wt )∥2

≤

2∥∇F (wt+τ ) − ∇F (wt )∥ + 2∥λEg(wt ) ⊙ g(wt ) · (wt+τ − wt )∥

≤

2(L22

(87)

2

+λ

2

L41 )∥wt+τ

(88)
2

− wt ∥ ,

(90)

since L1 is the upper bound of ∇f (w) and L2 is the smooth coefficient of f (w). Suppose that η =
upper bounded as Theorem 5.1,
min E∥∇F (wt )∥2 ≤

t=1,··· ,T

(89)

2

T
1 ∑
1
E∥∇F (wt )∥2 ≤ O( 1/2 ).
T t=1
T

√

2D0
bT V 2 L2

and τ is

(91)

Referring to a recent work of Lee et.al (Lee et al., 2016), GD with a random initialization and sufficiently small constant
step size converges to a local minimizer almost surely under the assumptions in Theorem 1.2. Thus, the assumption that
F (w) is µ-strongly convex in the r-neighborhood of arbitrary local minimum wloc is easily to be satistied with probability
one. By the L1 -Lipschitz assumption, we have P (Y = k|x, wt ) − P (Y = k|x, wloc ) ≤ L1 ∥wt − wloc ∥. By the L2 -smooth
assumption, we have L2 ∥wt − wloc ∥2 ≥ ⟨∇F (wt ), wt − wloc ⟩. Thus for wt ∈ B(wloc , r), we have ∥∇F (wt )∥ ≤ L2 ∥wt −
wloc ∥ ≤ L2 r. By the continuously twice differential assumption, we can assume that ∥∇F (wt )∥ ≤ L2 ∥wt − wloc ∥ ≤ L2 r
for wt ∈ B(wloc , r) and ∥∇F (wt )∥ ≤ L2 ∥wt − wloc ∥ > L2 r for wt ∈
/ B(wloc , r) without loss of generality 2 . Therefore
mint=1,··· ,T E∥∇F (wt )∥2 ≤ L22 r2 is a sufficient condition for E∥wT − wloc ∥ ≤ r.
min

t=1,··· ,T0

We have T0 ≥ O

(

1
r4

E∥∇F (wt )∥2 ≤ O(

1
1/2
T0

) ≤ r2 .

(92)

)
.

Thus we have finished the proof for nonconvex case.

G. Experimental Results on the Influence of λ
In this section, we show how the parameter λ affect our DC-ASGD algorithm. We compare the performance of respectively
sequential SGD, ASGD and DC-ASGD-a with different value of initial λ0 3 . The results are given in Figure 1. This
experiment reflects to the discussion in Section 5, too large value of this parameter (λ0 > 2 in this setting) will introduce
large variance and lead to a wrong gradient direction, meanwhile too small will make the compensation influence nearly
disappear. As λ decreasing, DC-ASGD will gradually degrade to ASGD. A proper λ will lead to significant better accuracy.
2
3

We can choose r small enough to make it satisfied.
We also compare different λ0 for DC-ASGD-c and the results are very similar to DC-ASGD-a.

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

H. Large Mini-batch Synchronous SGD with Delay-Compensated Gradient
In this section, we discuss how delay-compensated gradient can be used in synchronous SGD. The effective mini-batch
size in SSGD is usually enlarged M times comparing with sequential SGD. A learning rate scaling trick is commonly used
to overcome the influence of large mini-batch size in SSGD (Goyal et al., 2017): when the mini-batch size is multiplied by
M , multiply the learning rate by M . For sequential mini-batch SGD with learning rate η we have:
wt+M = wt − η

M
−1
∑

g(wt+j , zt+j ),

(93)

j=0

where zt+j is the t + j-th minibatch.
On the other hand, taking one step with M times large mini-batch size and learning rate η̂ = M η in synchronous SGD
yields:
ŵt+1 = wt − η̂

M −1
1 ∑
g(wt , ztj ),
M j=0

(94)

where ztj is the t-th minibatch on local machine j.
Assume that zt+j = ztj . The assumption g(wt+j , zt+j ) ≈ g(wt , ztj ) was made in synchronous SGD(Goyal et al., 2017).
However, it often may not hold.
∑
1
i
If we denote w̃jt+1 = wt − η̂ M
i<j g(wt , zt ), we can unfold the summation in Eq.94 to
j
w̃j+1
t+1 = w̃t+1 − η̂

1
g(wt , ztj ), j < M,
M

(95)

then we have ŵt+1 = w̃M
t+1 . We propose to use Eq.(5) in the main paper to compensate this assumption and apply
delay-compensated gradient to update Eq.95 with:
)
g(wt+j , zt+j ) ≈ g̃(w̃jt+1 , ztj ) := g(wt , ztj ) + λg(wt , ztj ) ⊙ g(wt , ztj ) ⊙ (w̃jt+1 − wt ) ,
(96)
1
j
(97)
w̃j+1
g̃(w̃jt+1 , ztj ), j < M.
t+1 = w̃t+1 − η̂
M
j
Please note that we redefine the previous w̃j+1
t+1 in Eq.97. For j > 1, we need to design an order to make w̃t+1 ≈ wt+j .
Choosing w̃jt+1 according to the increasing order of ∥w̃jt+1 − wt ∥2 can be used since the smaller distance with wt will
induce more accurate approximation by using Taylor expansion.

References
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. The elements of statistical learning, volume 1. Springer series in statistics
Springer, Berlin, 2001.
Goyal, Priya, Dollar, Piotr, Girshick, Ross, Noordhuis, Pieter, Wesolowski, Lukasz, Kyrola, Aapo, Tulloch, Andrew, Jia, Yangqing, and
He, Kaiming. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
LeCun, Yann. Modèles connexionnistes de lapprentissage. PhD thesis, These de Doctorat, Universite Paris 6, 1987.
Lee, Jason D, Simchowitz, Max, Jordan, Michael I, and Recht, Benjamin. Gradient descent converges to minimizers. University of
California, Berkeley, 1050:16, 2016.
Lian, Xiangru, Huang, Yijun, Li, Yuncheng, and Liu, Ji. Asynchronous parallel stochastic gradient for nonconvex optimization. In
Advances in Neural Information Processing Systems, pp. 2737–2745, 2015.

