Approximate Steepest Coordinate Descent

Sebastian U. Stich 1 Anant Raj 2 Martin Jaggi 1

Abstract
We propose a new selection rule for the coordinate selection in coordinate descent methods
for huge-scale optimization. The efficiency of
this novel scheme is provably better than the efficiency of uniformly random selection, and can
reach the efficiency of steepest coordinate descent (SCD), enabling an acceleration of a factor
of up to n, the number of coordinates. In many
practical applications, our scheme can be implemented at no extra cost and computational efficiency very close to the faster uniform selection.
Numerical experiments with Lasso and Ridge regression show promising improvements, in line
with our theoretical guarantees.

1. Introduction
Coordinate descent (CD) methods have attracted a substantial interest the optimization community in the last few
years (Nesterov, 2012; RichtaÌrik & TakaÌcÌŒ, 2016). Due to
their computational efficiency, scalability, as well as their
ease of implementation, these methods are the state-of-theart for a wide selection of machine learning and signal processing applications (Fu, 1998; Hsieh et al., 2008; Wright,
2015). This is also theoretically well justified: The complexity estimates for CD methods are in general better than
the estimates for methods that compute the full gradient in
one batch pass (Nesterov, 2012; Nesterov & Stich, 2017).
In many CD methods, the active coordinate is picked
at random, according to a probability distribution. For
smooth functions it is theoretically well understood how
the sampling procedure is related to the efficiency of the
scheme and which distributions give the best complexity
estimates (Nesterov, 2012; Zhao & Zhang, 2015; AllenZhu et al., 2016; Qu & RichtaÌrik, 2016; Nesterov & Stich,
2017). For nonsmooth and composite functions â€” that
appear in many machine learning applications â€” the pic1
EPFL 2 Max Planck Institute for Intelligent Systems. Correspondence to: Sebastian U. Stich <sebastian.stich@epfl.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

ture is less clear. For instance in (Shalev-Shwartz &
Zhang, 2013; Friedman et al., 2007; 2010; Shalev-Shwartz
& Tewari, 2011) uniform sampling (UCD) is used, whereas
other papers propose adaptive sampling strategies that
change over time (Papa et al., 2015; Csiba et al., 2015; Osokin et al., 2016; Perekrestenko et al., 2017).
A very simple deterministic strategy is to move along the
direction corresponding to the component of the gradient
with the maximal absolute value (steepest coordinate descent, SCD) (Boyd & Vandenberghe, 2004; Tseng & Yun,
2009). For smooth functions this strategy yields always
better progress than UCD, and the speedup can reach a factor of the dimension (Nutini et al., 2015). However, SCD
requires the computation of the whole gradient vector in
each iteration which is prohibitive (except for special applications, cf. Dhillon et al. (2011); Shrivastava & Li (2014)).
In this paper we propose approximate steepest coordinate
descent (ASCD), a novel scheme which combines the best
parts of the aforementioned strategies: (i) ASCD maintains
an approximation of the full gradient in each iteration and
selects the active coordinate among the components of this
vector that have large absolute values â€” similar to SCD;
and (ii) in many situations the gradient approximation can
be updated cheaply at no extra cost â€” similar to UCD. We
show that regardless of the errors in the gradient approximation (even if they are infinite), ASCD performs always
better than UCD.
Similar to the methods proposed in (Tseng & Yun, 2009)
we also present variants of ASCD for composite problems.
We confirm our theoretical findings by numerical experiments for Lasso and Ridge regression on a synthetic dataset
as well as on the RCV1 (binary) dataset.
Structure of the Paper and Contributions. In Sec. 2 we
review the existing theory for SCD and (i) extend it to the
setting of smooth functions. We present (ii) a novel lower
bound, showing that the complexity estimates for SCD and
UCD can be equal in general. We (iii) introduce ASCD
and the save selection rules for both smooth (Sec. 3) and
to composite functions (Sec. 5). We prove that (iv) ASCD
performs always better than UCD (Sec. 3) and (v) it can
reach the performance of SCD (Sec. 6). In Sec. 4 we discuss important applications where the gradient estimate can
efficiently be maintained. Our theory is supported by nu-

Approximate Steepest Coordinate Descent (ASCD)

merical evidence in Sec. 7, which reveals that (vi) ASCD
performs extremely well on real data.
Notation. Define [x]i := hx, ei i with ei the standard
unit vectors in Rn . We abbreviate âˆ‡i f := [âˆ‡f ]i . A
convex function f : Rn â†’ R with coordinate-wise Li Lipschitz continuous gradients1 for constants Li > 0,
i âˆˆ [n] := {1, . . . , n}, satisfies by the standard reasoning
f (x + Î·ei ) â‰¤ f (x) + Î·âˆ‡i f (x) +

Li 2
2 Î·

(1)

for all x âˆˆ Rn and Î· âˆˆ R. A function is coordinate-wise
L-smooth if Li â‰¤ L for i = 1, . . . , n. For an optimization
problem minxâˆˆRn f (x) define X ? := arg minxâˆˆRn f (x)
and denote by x? âˆˆ Rn an arbitrary element x? âˆˆ X ? .

2. Steepest Coordinate Descent
In this section we present SCD and discuss its theoretical
properties. The functions of interest are composite convex
functions F : Rn â†’ R of the form
F (x) := f (x) + Î¨(x)

Coordinate descent methods with constant step size generate a sequence {xt }tâ‰¥0 of iterates that satisfy the relation
1
L âˆ‡it f (x)eit

.

it = arg max âˆ‡i |f (xt )| .

(4)

iâˆˆ[n]

2.1. Convergence analysis
With the quadratic upper bound (1) one can easily get a
lower bound on the one step progress
i
h
2
1
|âˆ‡it f (xt )| . (5)
E [f (xt ) âˆ’ f (xt+1 ) | xt ] â‰¥ Eit 2L
For UCD and SCD the expression on the right hand side
evaluates to

Ï„SCD (xt ) :=

Ï„SCD (xt ) â‰¥

1

(f (xt ) âˆ’ f (x? )) ,

(8)

Smooth Objectives. When the objective function f is
just smooth (but not necessarily strongly convex), then the
analysis mentioned above is not applicable. We here extend
the analysis from (Nutini et al., 2015) to smooth functions.
Theorem 2.1. Let f : Rn â†’ R be convex and coordinatewise L-smooth. Then for the sequence {xt }tâ‰¥0 generated
by SCD it holds:

2
1
2nL kâˆ‡f (xt )k2
2
1
2L kâˆ‡f (xt )kâˆ

(6)

f (xt ) âˆ’ f (x? ) â‰¤

for R1 := max
?
?
x âˆˆX

2LR12
,
t

(9)


maxn [kx âˆ’ x k1 | f (x) â‰¤ f (x0 )] .
?

xâˆˆR

Proof. In the proof we first derive a lower bound on the one
step progress (Lemma A.1), similar to the analysis in (Nesterov, 2012). The lower bound for the one step progress of
SCD can in each iteration differ up to a factor of n from the
analogous bound derived for UCD (similar as in (7)). All
details are given in Section A.1 in the appendix.
Note that the R1 is essentially the diameter of the level set
at f (x0 ) measured in the 1-norm. In the complexity estimate of UCD, R12 in (9) is replaced by nR22 , where R2 is
the diameter of the level at f (x0 ) measured in the 2-norm
(cf. Nesterov (2012); Wright (2015)). As in (7) we observe
with Cauchy-Schwarz
1 2
n R1

With Cauchy-Schwarz we find
1
n Ï„SCD (xt )

Âµ1
L

where Âµ1 denotes the strong convexity parameter. By this,
they get a uniform upper bound on the convergence that
does not directly depend on local properties of the function,
like for instance Ï„SCD (xt ), but just on Âµ1 . It always holds
Âµ1 â‰¤ Âµ2 , and for functions where both quantities are equal,
SCD enjoys a linear speedup over UCD.

(3)

In UCD the active coordinate it is chosen uniformly at random from the set [n], it âˆˆu.a.r. [n]. SCD chooses the coordinate according to the Gauss-Southwell (GS) rule:

Ï„UCD (xt ) :=

Strongly Convex Objectives. Nutini et al. (2015)
present an elegant solution of this problem for Âµ2 -strongly
convex functions2 . They propose to measure the strong
convexity of the objective function in the 1-norm instead
of the 2-norm. This gives rise to the lower bound

(2)

where f is coordinate-wise L-smooth
Pn and Î¨ convex and
separable, that is that is Î¨(x) =
i=1 Î¨i ([x]i ). In the
first part of this section we focus on smooth problems, i.e.
we assume that Î¨ â‰¡ 0.

xt+1 = xt âˆ’

Hence, the lower bound on the one step progress of SCD is
always at least as large as the lower bound on the one step
progress of UCD. Moreover, the one step progress could
be even lager by a factor of n. However, it is very difficult
to formally prove that this linear speed-up holds for more
than one iteration, as the expressions in (7) depend on the
(a priori unknown) sequence of iterates {xt }tâ‰¥0 .

â‰¤ R22 â‰¤ R12 ,

(10)

i.e. SCD can accelerate up to a factor of n over to UCD.
â‰¤ Ï„UCD (xt ) â‰¤ Ï„SCD (xt ) .

|âˆ‡i f (x + Î·ei ) âˆ’ âˆ‡i f (x)| â‰¤ Li |Î·| ,

n

(7)

âˆ€x âˆˆ R , Î· âˆˆ R.

2

A function is Âµp -strongly convex in the p-norm, p â‰¥ 1, if
Âµ
f (y) â‰¥ f (x) + hâˆ‡f (x), y âˆ’ xi + 2p ky âˆ’ xk2p , âˆ€x, y âˆˆ Rn .

Approximate Steepest Coordinate Descent (ASCD)

2.2. Lower bounds
In the previous section we provided complexity estimates
for the methods SCD and UCD and showed that SCD can
converge up to a factor of the dimension n faster than UCD.
In this section we show that this analysis is tight. In Theorem 2.2 below we give a function q : Rn â†’ R, for which
the one step progress Ï„SCD (xt ) â‰ˆ Ï„UCD (xt ) up to a constant factor, for all iterates {xt }tâ‰¥0 generated by SCD.
By a simple technique we can also construct functions for
which the speedup is exactly equal to an arbitrary factor
Î» âˆˆ [1, n]. For instance we can consider functions with
a (separable) low dimensional structure. Fix integers s, n
such that ns â‰ˆ Î», define the function f : Rn â†’ R as
f (x) := q(Ï€s (x))

(11)

where Ï€s denotes the projection to Rs (being the first s
out of n coordinates) and q : Rs â†’ R is the function from
Theorem 2.2. Then
Ï„SCD (xt ) â‰ˆ Î» Â· Ï„UCD (xt ) ,

(12)

for all iterates {xt }tâ‰¥0 generated by SCD.
Theorem 2.2. Consider the function q(x) = 12 hQx, xi for
99
Q := In âˆ’ 100n
Jn , where Jn = 1n 1Tn , n > 2. Then
there exists x0 âˆˆ Rn such that for the sequence {xt }tâ‰¥0
generated by SCD it holds
2

kâˆ‡q(xt )kâˆ â‰¤

4
n

mod n)

2

kâˆ‡q(xt )k2 .

= cnÎ± Â· [xtâˆ’1 ]1+(tâˆ’1

Vi (x, y, s) := sy +

L 2
2y

(13)

mod n)

. (14)

We verify that for this sequence property (13) holds.
2.3. Composite Functions
The generalization of the GS rule (4) to composite problems (2) with nontrival Î¨ is not straight forward. The
â€˜steepestâ€™ direction is not always meaningful in this setting;
consider for instance a constrained problem where this rule
could yield no progress at all when stuck at the boundary.
Nutini et al. (2015) discuss several generalizations of the
Gauss-Southwell rule for composite functions. The GSs rule is defined to choose the coordinate with the most
negative directional derivative (Wu & Lange, 2008). This
rule is identical to (4) but requires the calculation of subgradients of Î¨i . However, the length of a step could be

+ Î¨i ([x]i + y) ,

(15)

for i âˆˆ [n]. The GS-q rule is formally defined as
iGSâˆ’q = arg min min Vi (x, y, âˆ‡i f (x)) .
iâˆˆ[n]

yâˆˆR

(16)

2.4. The Complexity of the GS rule
So far we only studied the iteration complexity of SCD,
but we have disregarded the fact that the computation of
the GS rule (4) can be as expensive as the computation
of the whole gradient. The application of coordinate descent methods is only justified if the complexity to compute
one directional derivative is approximately n times cheaper
than the computation of the full gradient vector (cf. Nesterov (2012)). By Theorem 2.2 this reasoning also applies
to SCD. A class of function with this property is given by
functions F : Rn â†’ R
F (x) := f (Ax) +

Proof. In the appendix we discuss a family of functions
defined by matrices Q := (Î± âˆ’ 1) n1 Jn + In and define
corresponding parameters 0 < cÎ± < 1 such that for x0 defor i = 1, . . . , n, SCD cycles through
fined as [x0 ]i = ciâˆ’1
Î±
the coordinates, that is, the sequence {xt }tâ‰¥0 generated by
SCD satisfies
[xt ]1+(tâˆ’1

arbitrarily small. In contrast, the GS-r rule was defined to
pick the coordinate direction that yields the longest step
(Tseng & Yun, 2009). The rule that enjoys the best theoretical properties (cf. Nutini et al. (2015)) is the GS-q rule,
which is defined as to maximize the progress assuming a
quadratic upper bound on f (Tseng & Yun, 2009). Consider the coordinate-wise models

n
X

Î¨i ([x]i )

(17)

i=1

where A is a d Ã— n matrix, and where f : Rd â†’ R,
and Î¨i : Rn â†’ R are convex and simple, that is the
time complexity T for computing their gradients is linear:
T (âˆ‡y f (y), âˆ‡x Î¨(x) = O(d + n). This class of functions includes least squares, logistic regression, Lasso, and
SVMs (when solved in dual form).
Assuming the matrix is dense, the complexity to compute
the full gradient of F is T (âˆ‡x F (x)) = O(dn). If the value
w = Ax is already computed, one directional derivative
can be computed in time T (âˆ‡i F (x)) = O(d). The recursive update of w after one step needs the addition of one
column of matrix A with some factors and can be done in
time O(d). However, we note that recursively updating the
full gradient vector takes time O(dn) and consequently the
computation of the GS rule cannot be done efficiently.
Nutini et al. (2015) consider sparse matrices, for which the
computation of the Gauss-Southwell rule becomes traceable. In this paper, we propose an alternative approach. Instead of updating the exact gradient vector, we keep track
of an approximation of the gradient vector and recursively
update this approximation in time O(n log n). With these
updates, the use of coordinate descent is still justified in
case d = â„¦(n).

Approximate Steepest Coordinate Descent (ASCD)

Algorithm 1 Approximate SCD (ASCD)
Input: f , x0 , T , Î´-gradient oracle g, method M
Initialize [gÌƒ0 ]i = 0, [r0 ]i = âˆ for i âˆˆ [n].
for t = 0 to T do
For i âˆˆ [n] define
compute u.-and l.-bounds
[ut ]i := max{|[gÌƒt ]i âˆ’ [rt ]i | , |[gÌƒt ]i + [rt ]i |}
[`t ]i := minyâˆˆR {|y| | [gÌƒt ]i âˆ’[rt ]i â‰¤ y â‰¤ [gÌƒt ]i +[rt ]i }
P
1
[` ]2
compute active set
av(I) := |I|
iâˆˆI
 t i
	
It := arg minI  I âŠ† [n] | [ut ]2 < av(I), âˆ€i âˆˆ
/I 
i

active coordinate
Pick it âˆˆu.a.r. arg maxiâˆˆIt {[`]i }
(xt+1 , [gÌƒt+1 ]it , [rt+1 ]it ) := M(xt , âˆ‡it f (xt ))

Î³t := [xt+1 ]it âˆ’ [xt ]it
update âˆ‡f (xt+1 ) estimate
Update [gÌƒt+1 ]j := [gÌƒt ]j + Î³t git j (xt ), j 6= it
Update [rt+1 ]j := [rt ]j + Î³t Î´it j , j 6= it
end for

3. Algorithm
Is it possible to get the significantly improved convergence
speed from SCD, when one is only willing to pay the computational cost of only the much simpler UCD? In this section, we give a formal definition of our proposed approximate SCD method which we denote ASCD.
The core idea of the algorithm is the following: While performing coordinate updates, ideally we would like to efficiently track the evolution of all elements of the gradient,
not only the one coordinate which is updated in the current step. The formal definition of the method is given in
Algorithm 1 for smooth objective functions. In each iteration, only one coordinate is modified according to some
arbitrary update rule M. The coordinate update rule M
provides two things: First the new iterate xt+1 , and secondly also an estimate gÌƒ of the it -th entry of the gradient at
the new iterate3 . Formally,
(xt+1 , gÌƒ, r) := M(xt , âˆ‡it f (xt ))

(18)

such that the quality of the new gradient estimate gÌƒ satisfies
|âˆ‡it f (xt+1 ) âˆ’ gÌƒ| â‰¤ r .

(19)

The non-active coordinates are updated with the help of
gradient oracles with accuracy Î´ â‰¥ 0 (see next subsection
for details). The scenario of exact updates of all gradient
entries is obtained for accuracy parameters Î´ = r = 0 and
in this case ASCD is identical to SCD.
3.1. Safe bounds for gradient evolution
ASCD maintains lower and upper bounds for the absolute values of each component of the gradient ([`]i â‰¤
3
For instance, for updates by exact coordinate optimization
(line-search), we have gÌƒ = r = 0.

|âˆ‡i f (x)| â‰¤ [u]i ). These bounds allow to identify the coordinates on which the absolute values of the gradient are
small (and hence cannot be the steepest one). More precisely, the algorithm maintains a set It of active coordinates
(similar in spirit as in active set methods, see e.g. Kim &
Park (2008); Wen et al. (2012)). A coordinate j is excluded
from It if the estimated progress in this direction (cf. (5))
is lower than the average of the estimated
Pprogress along
coordinate directions in It , [ut ]2j < |I1t | iâˆˆIt [`t ]2i . The
active set It can be computed in O(n log n) time by sorting. All other operations take linear O(n) time.
Gradient Oracle. The selection mechanism in ASCD
crucially relies on the following definition of a Î´-gradient
oracle. While the update M delivers the estimated active
entry of the new gradient, the additional gradient oracle is
used to update all other coordinates j 6= it of the gradient;
as in the last two lines of Algorithm 1.
Definition 3.1 (Î´-gradient oracle). For a function
f : Rn â†’ R and indices i, j âˆˆ [n], a (i, j)-gradient oracle
with error Î´ij â‰¥ 0 is a function gij : Rn â†’ R satisfying
âˆ€x âˆˆ Rn , âˆ€Î³ âˆˆ R:
|âˆ‡j f (x + Î³ei ) âˆ’ Î³gij (x)| â‰¤ |Î³| Î´ij .

(20)

We denote by a Î´-gradient oracle a family {gij }i,jâˆˆ[n] of
Î´ij -gradient oracles.
We discuss the availability of good gradient oracles for
many problem classes in more detail in Section 4. For example for least squares problems and general linear models,
a Î´-gradient oracle is for instance given by a scalar product
estimator as in (24) below. Note that ASCD can also handle
very bad estimates, as long as the property (20) is satisfied
(possibly even with accuracy Î´ij = âˆ).
Initialization. In ASCD the initial estimate gÌƒ0 of the gradient is just arbitrarily set to 0, with uncertainty r0 = âˆ.
Hence in the worst case it takes Î˜(n log n) iterations until each coordinate gets picked at least once (cf. Dawkins
(1991)) and until corresponding gradient estimates are set
to a realistic value. If better estimates of the initial gradient
are known, they can be used for the initialization as long
as a strong error bound as in (19) is known as well. For
instance the initialization can be done with âˆ‡f (x0 ) if one
is willing to compute this vector in one batch pass.
Convergence Rate Guarantee. We present our first
main result showing that the performance of ASCD is provably between UCD and SCD. First observe that if in Algorithm 1 the gradient oracle is always exact, i.e. Î´ij â‰¡ 0,
and if gÌƒ0 is initialized with âˆ‡f (x0 ), then in each iteration
|âˆ‡it f (xt )| = kâˆ‡f (xt )kâˆ and ASCD identical to SCD.
Lemma 3.1. Let imax := arg maxiâˆˆ[n] |âˆ‡i f (xt )|. Then
imax âˆˆ It , for It as in Algorithm 1.

Approximate Steepest Coordinate Descent (ASCD)

Proof. This is immediate from the definitions of It and the
upper and lower bounds. Suppose imax âˆˆ
/ It , then there exists j 6= imax such that [`t ]j > [ut ]imax , and consequently
|âˆ‡j f (xt )| > |âˆ‡imax f (xt )|.
Theorem 3.2. Let f : Rn â†’ R be convex and coordinatewise L-smooth, let Ï„UCD , Ï„SCD , Ï„ASCD denote the expected
one step progress (6) of UCD, SCD and ASCD, respectively, and suppose all methods use the same step-size
rule M. Then
Ï„UCD (x) â‰¤ Ï„ASCD (x) â‰¤ Ï„SCD (x) âˆ€x âˆˆ Rn .

(21)

P
2
1
Proof. By (5) we get Ï„ASCD (x) = 2L|I|
iâˆˆI |âˆ‡i f (x)| ,
where I denotes the corresponding index set of ASCD
when at iterate x. Note P
that for j âˆˆ
/ I it
Pmust hold that2
2
1
1
2
[`]
â‰¤
|âˆ‡j f (x)| â‰¤ [u]2j < |I|
i
iâˆˆI
iâˆˆI |âˆ‡i f (x)|
|I|
by definition of I.
Observe that the above theorem holds for all gradient oracles and coordinate update variants, as long as they are
used with corresponding quality parameters r (as in (19))
and Î´ij (as in (20)) as part of the algorithm.
Heuristic variants. Below also propose three heuristic
variants of ASCD. For all these variants the active set It
can be computed O(n), but the statement of Theorem 3.2
does not apply. These variants only differ from ASCD in
the choice of the active set in Algorithm 1:
u-ASCD: It := arg maxiâˆˆ[n] [ut ]i
`-ASCD: It := arg maxiâˆˆ[n] [`t ]i

	
a-ASCD: It := i âˆˆ [n] | [ut ]i â‰¥ maxiâˆˆ[n] [`t ]i

4. Approximate Gradient Update
In this section we argue that for a large class of objective
functions of interest in machine learning, the change in the
gradient along every coordinate direction can be estimated
efficiently.
Lemma 4.1. Consider F : Rn â†’ R as in (17) with
twice-differentiable f : Rd â†’ R. Then for two iterates
xt , xt+1 âˆˆ Rn of a coordinate descent algorithm, i.e.
xt+1 = xt + Î³t eit , there exists a xÌƒ âˆˆ Rn on the line
segment between xt and xt+1 , xÌƒ âˆˆ [xt , xt+1 ] with
âˆ‡i F (xt+1 ) âˆ’ âˆ‡i F (xt ) = Î³t hai , âˆ‡2 f (AxÌƒ)ait i
where ai denotes the i-th column of the matrix A.

âˆ€i 6= it
(22)

Proof. For coordinates i 6= it the gradient (or subgradient
set) of Î¨i ([xt ]i ) does not change. Hence it suffices to calculate the change âˆ‡f (xt+1 ) âˆ’ âˆ‡f (xt ). This is detailed in
the appendix.

Least-Squares with Arbitrary Regularizers. The least
squares problem is defined as problem (17) with f (Ax) =
2
1
d
2 kAx âˆ’ bk2 for a b âˆˆ R . This function is twice differ2
entiable with âˆ‡ f (Ax) = In . Hence (22) reduces to
âˆ‡i F (xt+1 ) âˆ’ âˆ‡i F (xt ) = Î³t hai , ait i

âˆ€i 6= it .

(23)

This formulation gives rise to various gradient oracles (20)
for the least square problems. For for i 6= it we easily
verify that the condition (20) is satisfied:
1
1. gij
:= hai , ait i; Î´ij = 0,
2
2. gij := max {âˆ’ kai k kaj k , min {S(i, j), kai k kaj k}};
Î´ij =  kai k kaj k, where S : [n]Ã—[n] denotes a function
with the property
|S(i, j) âˆ’ hai , aj i| â‰¤  kai k kaj k , âˆ€i, j âˆˆ [n] (24)
3
3. gij
:= 0; Î´ij = kai k kaj k,
4
4. gij
âˆˆu.a.r. [âˆ’ kai k kaj k , kai k kaj k]; Î´ij = kai k kaj k.

Oracle g 1 can be used in the rare cases where the dot product matrix is accessible to the optimization algorithm without any extra cost. In this case the updates will all be exact.
If this matrix is not available, then the computation of each
scalar product takes time O(d). Hence, they cannot be recomputed on the fly, as argued in Section 2.4. In contrast,
the oracles g 3 and g 4 are extremely cheap to compute, but
the error bounds are worse. In the numerical experiments
in Section 7 we demonstrate that these oracles perform surprisingly well.
The oracle g 2 can for instance be realized by lowdimensional embeddings, such as given by the JohnsonLindenstrauss lemma (cf. Achlioptas (2003); MatousÌŒek
(2008)). By embedding each vector
 in a lower-dimensional
space of dimension O âˆ’2 log n and computing the scalar
products of the embedding in time O(log n), relation (24)
is satisfied.
Updating the gradient of the active coordinate. So far
we only discussed the update of the passive coordinates.
For the active coordinate the best strategy depends on the
update rule M from (18). If exact line search is used, then
0 âˆˆ âˆ‡it f (xt+1 ). For other update rules we can update the
gradient âˆ‡it f (xt+1 ) with the same gradient oracles as for
the other coordinates, however we need also to take into
account the change of the gradient of Î¨i ([xt ]i ). If Î¨i is
simple, like for instance in ridge or lasso, the subgradients
at the new point can be computed efficiently.
Bounded variation. In many applications the Hessian
âˆ‡2 f (AxÌƒ) is not so simple as in the case of square loss.
If we assume that the Hessian of f is bounded, i.e.
âˆ‡2 f (Ax)  M Â· In for a constant M â‰¥ 0, âˆ€x âˆˆ Rn ,
then it is easy to see that the following holds :
âˆ’M kai kkaj k â‰¤ hai , âˆ‡2 f (AxÌƒ)ait i â‰¤ M kai kkaj k .

Approximate Steepest Coordinate Descent (ASCD)

Using this relation, we can define gradient oracles for more
general functions, by taking the additional approximation
factor M into account. The quality can be improved, if we
have access to local bounds on âˆ‡2 f (Ax).
Heuristic variants. By design, ASCD is robust to high
errors in the gradient estimations â€“ the steepest descent direction is always contained in the active set. However, instead of using only the very crude oracle g 4 to approximate
all scalar products, it might be advantageous to compute
some scalar products with higher precision. We propose to
use a caching technique to compute the scalar products with
high precision for all vectors in the active set (and storing a
matrix of size O(It Ã— n)). This presumably works well if
the active set does not change much over time.

5. Extension to Composite Functions
The key ingredients of ASCD are the coordinate-wise upper and lower bounds on the gradient and the definition of
the active set It which ensures that the steepest descent direction is always kept and that only provably bad directions
are removed from the active set. These ideas can also be
generalized to the setting of composite functions (2). We
already discussed some popular GS-âˆ— update rules in the
introduction in Section 2.3.
Implementing ASCD for the GS-s rule is straight forward,
and we comment on the GS-r in the appendix in Sec. D.2.
Here we exemplary detail the modification for the GS-q
rule (16), which turns out to be the most evolved (the same
reasoning also applies to the GSL-q rule from (Nutini et al.,
2015)). In Algo. 2 we show the construction â€” based just
on approximations of the gradient of the smooth part f â€”
of the active set I. For this we compute upper and lower
bounds v, w on minyâˆˆR V (x, y, âˆ‡i f (x)), such that
[v]i â‰¤ min V (x, y, âˆ‡i f (x) â‰¤ [w]i
yâˆˆR

âˆ€i âˆˆ [n] .

(25)

The selection of the active coordinate is then based on these
bounds. Similar as in Lemma 3.1 and Theorem 3.2 this set
has the property iGSâˆ’q âˆˆ I, and directions are only discarded in such a way that the efficiency of ASCD-q cannot
drop below the efficiency of UCD. The proof can be found
in the appendix in Section D.1.

6. Analysis of Competitive Ratio
In Section 3 we derived in Thm. 3.2 that the one step
progress of ASCD is between the bounds on the onestep
progress of UCD and SCD. However, we know that the efficiency of the latter two methods can differ much, up to
a factor of n. In this section we will argue that in certain
cases where SCD performs much better than UCD, ASCD
will accelerate as well. To measure this effect, we could for

Algorithm 2 Adaptation of ASCD for GS-q rule
Input: Gradient estimate gÌƒ, error bounds r.
For i âˆˆ [n] define:
compute u.-and l.-bounds
[u]i := [gÌƒ]i + [r]i , [`]i := [gÌƒ]i âˆ’ [r]i
[u? ]i := arg minyâˆˆR V (x, y, [u]i )
[`? ]i := arg minyâˆˆR V (x, y, [`]i )

minimize the model

compute u.-and l. bounds on minyâˆˆR V (x, y, âˆ‡i f (x))

[Ï‰u ]i := V (x, [u? ]i , [u]i )+max{0, [u? ]i ([`]i âˆ’ [u]i )}
[Ï‰` ]i := V (x, [`? ]i , [`]i ) + max{0, [`? ]i ([u]i âˆ’ [`]i )}
[v]i := min {V (x, [u? ]i , [u]i ), V (x, [`? ]i , [`]i )}
[w]i := min {[Ï‰u ]i , [Ï‰` ]i , Î¨i ([x]i )}
P
1
compute active set
av(I) := |I|
iâˆˆI [w]i
It := arg minI |{I âŠ† [n] | [v]i > av(I), âˆ€i âˆˆ
/ I}|
instance consider the ratio:

 i âˆˆ It | |âˆ‡i f (xt )| â‰¥
%t :=
|It |

1
2

	
kâˆ‡f (xt )kâˆ 

,

(26)

For general functions this expression is a bit cumbersome
to study, therefore we restrict our discussion to the class
of objective functions (11) as introduced in Sec. 2.2. Of
course not all real-world objective functions will fall into
this class, however this problem class is still very interesting in our study, as we will see in the following, because it
will highlight the ability (or disability) of the algorithms to
eventually identify the right set of â€˜activeâ€™ coordinates.
For the functions with the structure (11) (and q as in
Thm. 2.2), the active set falls into the first s coordinates.
Hence it is reasonable to approximate %t by the competitive ratio
|It âˆ© [s]|
Ït :=
.
(27)
|It |
It is also reasonable to assume that in the limit, (t â†’ âˆ),
a constant fraction of the [s] will be contained in the active
set It (it might not hold [s] âŠ† It âˆ€t, as for instance with
exact line search the directional derivative vanishes just after the update). In the following theorem we calculate Ït
for (t â†’ âˆ), the proof is given in the appendix.
Theorem 6.1. Let f : Rn â†’ R be of the form (11).
For indices i âˆˆ
/ [s] define Ki := {t | i âˆˆ
/ It , i âˆˆ Itâˆ’1 }.
For j âˆˆ Ki define Tji := min {t âˆ’ j | i âˆˆ Ij+t }, i.e.
i
the number of iterations outside
the active set, Tâˆ
:=

i
limtâ†’âˆ EjâˆˆKi Tj | j > k , and the average Tâˆ :=
 i 
Eiâˆˆ[s]
Tâˆ . If there exists a constant c > 0 such that
/
limtâ†’âˆ |[s] âˆ© It | = cs, then (with the notation Ïâˆ :=
limtâ†’âˆ E [Ït ]),
2cs
âˆš ,
(28)
Ïâˆ â‰¥
cs + n âˆ’ s âˆ’ Tâˆ + Î¸
where Î¸ â‰¡ Î¸ := n2 + (c âˆ’ 1)2 s2 + 2n((c âˆ’ 1)s âˆ’ Tâˆ ) +
2
2(1 + c)sTâˆ + Tâˆ
. Especially, Ïâˆ â‰¥ 1 âˆ’ nâˆ’s
Tâˆ .

Approximate Steepest Coordinate Descent (ASCD)
T

= n/2

T

1
measured
exact
lower bound

0.8

=n

T

1

1

0.8

0.8

0.6

0.6

0.6

0.4

0.4

0.4

0.2

0.2

0.2

0

0
0

50

100

= 4n

0
0

epochs

50

100

epochs

0

50

100

epochs

Figure 1. Competitive ratio Ït (blue) in comparison with Ïâˆ (28)
(red) and the lower bound Ïâˆ â‰¥ 1 âˆ’ nâˆ’s
(black). Simulation for
Tâˆ
parameters n = 100, s = 10, c = 1 and Tâˆ âˆˆ {50, 100, 400}.

In Figure 1 we compare the lower bound (28) of the competitive ratio in the limit (t â†’ âˆ) with actual measurements of Ït for simulated example with parameters n =
100, s = 10, c = 1 and various Tâˆ âˆˆ {50, 100, 400}.
We initialized the active set I0 = [s], but we see that the
equilibrium is reached quickly.

Based on this Thm. 6.1, we can now estimate the competitive ratio in various scenarios. On the class (11) it holds
c â‰ˆ 1 as we argued before. Hence the competitive ratio (28) just depends on Tâˆ . This quantity measures how
many iterations a coordinate j âˆˆ
/ [s] is in average outside
of the active set It . From the lower bound we see that the
competitive ratio Ït approaches a constant for (t â†’ âˆ) if
Tâˆ = Î˜ (n), for instance Ïâˆ â‰¥ 0.8 if Tâˆ â‰¥ 5n.
As an approximation to Tâˆ , we estimate the quantities Ttj0
defined in Thm. 6.1. Ttj0 denotes the number of iterations
it takes until coordinate j enters the active set again, assuming it left the active set at iteration t0 âˆ’ 1. We estimate
Ttj0 â‰¥ TÌ‚ , where TÌ‚ denotes maximum number of iterations
such that

t=t0

s

Î³ t Î´ ii j



1 X 

â‰¤
âˆ‡k f xt0 +TÌ‚ 
s

7. Empirical Observations
In this section we evaluate the empirical performance of
ASCD on synthetic and real datasets. We consider the following regularized general linear models:
min 1 kAx âˆ’ bk22 + Î»2 kxk22
xâˆˆRn 2
min 1 kAx âˆ’ bk22 + Î»kxk1
xâˆˆRn 2

,

(30)

,

(31)

that is, l2 -regularized least squares (30) as well as l1 regularized linear regression (Lasso) in (31), respectively.

6.1. Estimates of the competitive ratio

tX
0 +TÌ‚

justified if s is large, for instance s â‰¥ 14 n. Otherwise the
convergence on q is too fast, and the gradient approximations are too weak. However, notice that we assumed Î´ to
be an uniform bound on all errors. If the errors have large
discrepancy the estimates become much better (this holds
for instance on datasets where the norm data vectors differs
much, or when caching techniques as mentioned in Sec. 4
are employed).

âˆ€j âˆˆ
/ [s].

(29)

k=1

For smooth functions, the steps Î³t = Î˜ (|âˆ‡it f (xt )|) and if
we additionally assume that the errors of the gradient oracle
are uniformly bounded Î´ij â‰¤ Î´, the sum in (29) simplifies
Pt0 +TÌ‚
to Î´ t=t
|âˆ‡it f (xt )|.
0
For smooth, but not strongly convex function q, the norms
of the gradient changes very slowly,
 with a rate independent
of s or n, and we get TÌ‚ = Î˜  1Î´ . Hence, the competitive
ratio is constant for Î´ = Î˜ n1 .
For strongly convex function q, the norm of the gradient
2
decreases linearly, say kâˆ‡f (xt )k2 âˆ eÎºt for Îº â‰ˆ 1s . I.e.
it decreases by half after each Î˜ (s) iterations. Therefore
n
to guarantee TÌ‚ = Î˜ (n) it needs to hold Î´ = eâˆ’Î˜( s ) .
This result seems to indicate that the use of ACDM is only

Datasets. The datasets A âˆˆ RdÃ—n in problems (30)
and (31) were chosen as follows for our experiments. For
the synthetic data, we follow the same generation procedure as described in (Nutini et al., 2015), which generates very sparse data matrices. For completeness, full details of the data generation process are also provided in
the appendix in Sec. E. For the synthetic data we choose
n = 5000 for problem (31) and n = 1000 for problem (30).
Dimension d = 1000 is fixed for both cases.
For real datasets, we perform the experimental evaluation
on RCV1 (binary,training), which consists of 20, 242 samples, each of dimension 47, 236 (Lewis et al., 2004). We
use the un-normalized version with all non-zeros values set
to 1 (bag-of-words features).
Gradient oracles and implementation details. On the
RCV1 dataset, we approximate the scalar products with
the oracle g 4 that was introduced in Sec. 4. This oracle
is extremely cheap to compute, as the norms kai k of the
columns of A only need to be computed once.
On the synthetic data, we simulate the oracle g 2 for various
precisions values . For this, we sample a value uniformly
at random from the allowed error interval (24). Figs. 2d
and 3d show the convergence for different accuracies.
For the l1 -regularized problems, we used ASCD with the
GS-s rule (the experiments in (Nutini et al., 2015) revealed
almost identical performance of the different GS-âˆ— rules).
We compare the performance of UCD, SCD and ASCD.
We also implement the heuristic version a-ASCD that was
introduced in Sec. 3. All algorithm variants use the same
step size rule (i.e. the method M in Algorithm 1). We use
exact line search for the experiment in Fig. 3c, for all others we used a fixed step size rule (the convergence is slower

Approximate Steepest Coordinate Descent (ASCD)

(a) Convergence for l2

(b) Convergence for l1

(c) True vs No Initialization for l2

(d) Error Variation (ASCD)

Figure 2. Experimental results on synthetically generated datasets

(a) Convergence for l2

(b) Convergence for l1

(c) Line search for l1

(d) Error Variation (ASCD)

Figure 3. Experimental results on the RCV1-binary dataset

for all algorithms, but the different effects of the selection
of the active coordinate is more distinctly visible).
ASCD is either initialized with the true gradient (Figs. 2a,
2b, 2d, 3c, 3d) or arbitrarely (with error bounds Î´ = âˆ) in
Figs. 3a and 3b (Fig. 2c compares both initializations).
Fig. 2 shows results on the synthetic data, Fig. 3 on the
RCV1 dataset. All plots show also the size of the active
set It . The plots 3c and 3d are generated on a subspace
of RCV1, with 10000 and 5000 randomly chosen columns,
respectively.
Here are the highlights of our experimental study:
1. No initialization needed.
We observe (see e.g.
Figs. 2c,3a, 3b) that initialization with the true gradient
values is not needed at beginning of the optimization
process (the cost of the initialization being as expensive
as one epoch of ASCD). Instead, the algorithm performs
strong in terms of learning the active set on its own, and
the set converges very fast after just one epoch.
2. High errors toleration. The gradient oracle g 4 gives
very crude approximations, however the convergence of
ASCD is excellent on RCV1 (Fig. 3). Here the size of
the true active set is very small (in the order of 0.1% on
RCV1) and ASCD is able to identify this set. Fig. 3d
shows that almost nothing can be gained from more precise (and more expensive) oracles.
3. Heuristic a-ASCD performs well. The convergence
behavior of ASCD follows theory. For the heuristic version a-ASCD (which computes the active set slightly

faster, but Thm. 3.2 does not hold) performs identical
to ASCD in practice (cf. Figs. 2, 3), and sometimes
slightly better. This is explained by the active set used
in ASCD typically being larger than the active set of aASCD (Figs. 2a,2b, 3a, 3b).

8. Concluding Remarks
We proposed ASCD, a novel selection mechanism for the
active coordinate in CD methods. Our scheme enjoys
three favorable properties: (i) its performance can reach
the performance steepest CD â€” both in theory and practice, (ii) the performance is never worse than uniform CD,
(iii) in many important applications, the scheme it can be
implemented at no extra cost per iteration.
ASCD calculates the active set in a safe manner, and picks
the active coordinate uniformly at random from this smaller
set. It seems possible that an adaptive sampling strategy
on the active set could boost the performance even further.
Here we only study CD methods where a single coordinate
gets updated in each iteration. ASCD can immediately also
be generalized to block-coordinate descent methods. However, the exact implementation in a distributed setting can
be challenging.
Finally, it is an interesting direction to extend ASCD also
to the stochastic gradient descent setting (not only heuristically, but with the same strong guarantees as derived in this
paper).

Approximate Steepest Coordinate Descent (ASCD)

References
Achlioptas, Dimitris. Database-friendly random projections:
Johnson-lindenstrauss with binary coins. Journal of Computer
and System Sciences, 66(4):671 â€“ 687, 2003.
Allen-Zhu, Z, Qu, Z, Richtarik, P, and Yuan, Y. Even faster accelerated coordinate descent using non-uniform sampling. 2016.
Boyd, Stephen P and Vandenberghe, Lieven. Convex optimization. Cambridge University Press, 2004.
Csiba, Dominik, Qu, Zheng, and RichtaÌrik, Peter. Stochastic Dual
Coordinate Ascent with Adaptive Probabilities. In ICML 2015
- Proceedings of the 32th International Conference on Machine
Learning, 2015.
Dawkins, Brian. Siobhanâ€™s problem: The coupon collector revisited. The American Statistician, 45(1):76â€“82, 1991.

Nutini, Julie, Schmidt, Mark W, Laradji, Issam H, Friedlander,
Michael P, and Koepke, Hoyt A. Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random
Selection. In ICML, pp. 1632â€“1641, 2015.
Osokin, Anton, Alayrac, Jean-Baptiste, Lukasewitz, Isabella,
Dokania, Puneet K., and Lacoste-Julien, Simon. Minding the
gaps for block frank-wolfe optimization of structured svms.
In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48,
ICMLâ€™16, pp. 593â€“602. PMLR, 2016.
Papa, Guillaume, Bianchi, Pascal, and CleÌmencÌ§on, SteÌphan.
Adaptive Sampling for Incremental Optimization Using
Stochastic Gradient Descent. ALT 2015 - 26th International
Conference on Algorithmic Learning Theory, pp. 317â€“331,
2015.

Dhillon, Inderjit S, Ravikumar, Pradeep, and Tewari, Ambuj.
Nearest Neighbor based Greedy Coordinate Descent. In NIPS
2014 - Advances in Neural Information Processing Systems 27,
2011.

Perekrestenko, Dmytro, Cevher, Volkan, and Jaggi, Martin. Faster
Coordinate Descent via Adaptive Importance Sampling. In
Proceedings of the 20th International Conference on Artificial
Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research, pp. 869â€“877, Fort Lauderdale, FL,
USA, 20â€“22 Apr 2017. PMLR.

Friedman, Jerome, Hastie, Trevor, HoÌˆfling, Holger, and Tibshirani, Robert. Pathwise coordinate optimization. The Annals of
Applied Statistics, 1(2):302â€“332, 2007.

Qu, Zheng and RichtaÌrik, Peter. Coordinate descent with arbitrary
sampling i: algorithms and complexity. Optimization Methods
and Software, 31(5):829â€“857, 2016.

Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. Regularization Paths for Generalized Linear Models via Coordinate
Descent. Journal of Statistical Software, 33(1):1â€“22, 2010.

RichtaÌrik, Peter and TakaÌcÌŒ, Martin. Parallel coordinate descent
methods for big data optimization. Mathematical Programming, 156(1):433â€“484, 2016.

Fu, Wenjiang J. Penalized regressions: The bridge versus the
lasso. Journal of Computational and Graphical Statistics, 7
(3):397â€“416, 1998.

Shalev-Shwartz, Shai and Tewari, Ambuj. Stochastic Methods
for l1 -regularized Loss Minimization. JMLR, 12:1865â€“1892,
2011.

Hsieh, Cho-Jui, Chang, Kai-Wei, Lin, Chih-Jen, Keerthi,
S Sathiya, and Sundararajan, S. A Dual Coordinate Descent
Method for Large-scale Linear SVM. In the 25th International
Conference on Machine Learning, pp. 408â€“415, New York,
USA, 2008.

Shalev-Shwartz, Shai and Zhang, Tong. Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization.
JMLR, 14:567â€“599, 2013.

Kim, Hyunsoo and Park, Haesun. Nonnegative matrix factorization based on alternating nonnegativity constrained least
squares and active set method. SIAM Journal on Matrix Analysis and Applications, 30(2):713â€“730, 2008.
Lee, Daniel D and Seung, H Sebastian. Learning the parts of objects by non-negative matrix factorization. Nature, 401(6755):
788â€“791, 1999.
Lewis, David D., Yang, Yiming, Rose, Tony G., and Li, Fan.
Rcv1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5:361â€“397, 2004.
MatousÌŒek, JirÌŒÄ±Ì. On variants of the johnsonlindenstrauss lemma.
Random Structures & Algorithms, 33(2):142â€“156, 2008.
Nesterov, Yu. Efficiency of coordinate descent methods on hugescale optimization problems. SIAM Journal on Optimization,
22(2):341â€“362, 2012.
Nesterov, Yurii and Stich, Sebastian U. Efficiency of the accelerated coordinate descent method on structured optimization problems. SIAM Journal on Optimization, 27(1):110â€“123,
2017.

Shrivastava, Anshumali and Li, Ping. Asymmetric LSH (ALSH)
for sublinear time maximum inner product search (MIPS). In
NIPS 2014 - Advances in Neural Information Processing Systems 27, pp. 2321â€“2329, 2014.
Tseng, Paul and Yun, Sangwoon. A coordinate gradient descent
method for nonsmooth separable minimization. Mathematical
Programming, 117(1):387â€“423, 2009.
Wen, Zaiwen, Yin, Wotao, Zhang, Hongchao, and Goldfarb, Donald. On the convergence of an active-set method for 1 minimization. Optimization Methods and Software, 27(6):1127â€“
1146, 2012.
Wright, Stephen J. Coordinate descent algorithms. Mathematical
Programming, 151(1):3â€“34, 2015.
Wu, Tong Tong and Lange, Kenneth. Coordinate descent algorithms for lasso penalized regression. Ann. Appl. Stat., 2(1):
224â€“244, 2008.
Zhao, Peilin and Zhang, Tong. Stochastic optimization with importance sampling for regularized loss minimization. In Proceedings of the 32nd International Conference on Machine
Learning, volume 37 of PMLR, pp. 1â€“9, Lille, France, 2015.
PMLR.

Approximate Steepest Coordinate Descent (ASCD)

Appendix

A. On Steepest Coordinate Descent
A.1. Convergence on Smooth Functions
Lemma A.1 (Lower bound on the one step progress on smooth functions). Let f : Rn â†’ R be convex and coordinate-wise
L-smooth. For a sequence of iterates {xt }tâ‰¥0 define the progress measure
âˆ†(xt ) :=

1
1
âˆ’
.
E [f (xt+1 ) âˆ’ f (x? ) | xt ] f (xt ) âˆ’ f (x? )

(32)

For sequences {xt }tâ‰¥0 generated by SCD it holds:
âˆ†SCD (xt ) â‰¥

1
2

2L kxt âˆ’ x? k1

,

t â‰¥ 0,

(33)

t â‰¥ 0.

(34)

and for a sequences generated by UCD:
âˆ†UCD (xt ) â‰¥

1
2

2nL kxt âˆ’ x? k2

,

It is important to note that the lower bounds presented in Equations (33) and (34) are quite tight and equality is almost
achievable under special conditions. When comparing the per-step progress of these two methods, we find â€” similarly as
in (7) â€” the relation
1
âˆ†SCD (xt ) â‰¤ âˆ†UCD (xt ) â‰¤ âˆ†SCD (xt ) ,
n

(35)

that is, SCD can boost the performance over the random coordinate descent up to the factor of n. This also holds for a
sequence of consecutive updates, as show in Theorem 2.1.
Proof of Lemma A.1. Define f ? := f (x? ). From the smoothness assumption (1), we get
(5)

1
kâˆ‡f (xt )k2âˆ
2L


1
â‡’ f (xt+1 ) âˆ’ f ? â‰¤ f (xt ) âˆ’ f ? âˆ’
kâˆ‡f (xt )k2âˆ
2L
f (xt+1 ) â‰¤ f (xt ) âˆ’

(36)

Now from the property of a convex function and HoÌˆlderâ€™s inequality:
f (xt ) âˆ’ f ? â‰¤ hâˆ‡f (xt ), xt âˆ’ x? i â‰¤ kâˆ‡f (xt )kâˆ kxt âˆ’ x? k1

(37)

Hence,
f (xt ) âˆ’ f ?
â‡’ kâˆ‡f (xt )k2âˆ

2

â‰¤ kâˆ‡f (xt )k2âˆ kxt âˆ’ x? k21
2
f (xt ) âˆ’ f ?
â‰¥
kxt âˆ’ x? k21

(38)

From Equations (36) and (38),
1
1
1
âˆ’
â‰¥
2Lkxt âˆ’ x? k21
f (xt+1 ) âˆ’ f ?
f (xt ) âˆ’ f ?
Which concludes the proof.

(39)

Approximate Steepest Coordinate Descent (ASCD)

We like to remark, that the one step progress for UCD can be written as (Nesterov, 2012; Wright, 2015):
1
1
1
âˆ’
â‰¥
2Lnkxt âˆ’ x? k22
E[f (xt+1 )|xt ] âˆ’ f ?
f (xt ) âˆ’ f ?

(40)

Proof of Theorem 2.1. From Lemma A.1,
1
1
1
âˆ’
â‰¥
2Lkxt âˆ’ x? k21
f (xt+1 ) âˆ’ f ?
f (xt ) âˆ’ f ?
Now summing up the above equation for t = 0 till t âˆ’ 1, we get:
tâˆ’1

1
1
1 X
1
âˆ’
â‰¥
?
?
2L i=0 kxt âˆ’ x? k21
f (xt ) âˆ’ f
f (x0 ) âˆ’ f
tâˆ’1

â‡’

1
1 X
1
â‰¥
2L i=0 kx0 âˆ’ x? k21
f (xt ) âˆ’ f ?

â‡’

1
t
â‰¥
?
2LR12
f (xt ) âˆ’ f

â‡’ f (xt ) âˆ’ f ? â‰¤

2LR12
t

Which concludes the proof.
A.2. Lower bounds
In this section we provide the proof of Theorem 2.2. Our result is slightly more general, we will proof the following (and
Theorem 2.2 follows by the choice Î± = 0.01 < 13 ).
Theorem A.2. Consider the function q(x) = 21 hQx, xi for Q := (Î± âˆ’ 1) n1 Jn + In , where Jn = 1n 1Tn and 0 < Î± < 12 ,
n > 2. Then there exists x0 âˆˆ Rn such that for the sequence {xt }tâ‰¥0 generated by SCD it holds
2

kâˆ‡q(xt )kâˆ â‰¤

3 + 3Î±
2
kâˆ‡q(xt )k2 .
n

(41)

In the proof below we will construct a special x0 âˆˆ Rn that has the claimed property. However, we would like to remark
that this is not very crucial. We observe that for functions as in Theorem A.2 almost any initial iterate (x not aligned with
the coordinate axes) the sequence {xt }tâ‰¥0 of iterates generated by SCD suffers from the same issue, i.e. relation (41)
holds for iteration counter t sufficiently large. We do not prove this formally, but demonstrate this behavior in Figure 4.
We see that the steady state is almost reached after 2n iterations.
Proof of Theorem A.2. Define the parameter cÎ± by the equation




Î± âˆ’ 1 nâˆ’1
1âˆ’Î±
1+
cÎ± =
Snâˆ’1 (cÎ± )
n
n


1âˆ’Î±
cnâˆ’1
=
Sn (cÎ± )
Î±
n
where Sn (cÎ± ) =
cÎ± â‰¥ 1 âˆ’ n3 Î±.

Pnâˆ’1
i=0

(42)
(43)

iâˆ’1
cnÎ± ; and define x0 as [x0 ]i = cÎ±
for i = 1, . . . , n. In Lemma A.3 below we show that

We now show that SCD cycles through the coordinates, i.e. the sequence {xt }tâ‰¥0 generated by SCD satisfies
[xt ]1+(tâˆ’1

mod n)

= cnÎ± Â· [xtâˆ’1 ]1+(tâˆ’1

mod n)

.

(44)

Approximate Steepest Coordinate Descent (ASCD)
Ratio (
7

f(xk ))

Normalized entries of
1.5

f(xk )

1.4

6

1.3
5

1.2
1.1

4

1

3

0.9
2

0.8

1

0.7
0

20

40

60

80

100

0

20

40

iterations

60

80

100

iterations

Figure 4. SCD on the function from Theorem A.2 in dimension n = 20 with x0 = 1n (i.e. not the worst starting point constructed in
the proof of Theorem A.2). On the right the (normalized and sorted) components of âˆ‡f (xt ).

Observe âˆ‡f (x0 ) = Qx0 . Hence the GS rule picks i1 = 1 in the first iteration. The iterate is updated as follows:
[Qx0 ]1
Q11
(Î± âˆ’ 1) n1 Sn (cÎ± ) + 1
=1âˆ’
(Î± âˆ’ 1) n1 + 1
(3)

[x1 ]1 = [x0 ]1 âˆ’

(45)
(46)

=

(Î± âˆ’ 1) n1 (1 âˆ’ Sn (cÎ± ))
(Î± âˆ’ 1) n1 + 1

(47)

=

(Î± âˆ’ 1) n1 (cnÎ± âˆ’ cÎ± Sn (cÎ± ))
(Î± âˆ’ 1) n1 + 1

(48)

(Î± âˆ’ 1) n1 cnÎ± + cnÎ±
= cnÎ±
(Î± âˆ’ 1) n1 + 1

(49)

(42)

=

The relation (44) can now easily be checked by the same reasoning and induction.
It remains to verify that for this sequence property (41) holds. This is done in Lemma A.4. Note that âˆ‡f (x0 ) = Qx0 = g,
where g is defined as in the lemma, and that all gradients âˆ‡f (xt ) are up to scaling and reordering of the coordinates
equivalent to the vector g.
Pnâˆ’1
Lemma A.3. Let 0 < Î± < 12 and 0 < cÎ± < 1 defined by equation (42), where Sn (cÎ± ) = i=0 cnÎ± . Then cÎ± â‰¥ 1 âˆ’ n4 Î±
for Î± âˆˆ [0, 12 ].
Proof. Using the summation formula for geometric series, Sn (cÎ± ) =
(42)

Î± = 1âˆ’

1âˆ’cn
Î±
1âˆ’cÎ±

we derive

ncnâˆ’1
n(1 âˆ’ cÎ± )cnâˆ’1
Î±
Î±
=1âˆ’
.
Sn (cÎ± )
1 âˆ’ cnÎ±
|
{z
}

(50)

:=Î¨(cÎ± )

With Taylor expansion we observe that


3Î±
Î¨ 1âˆ’
â‰¥ Î±,
n



2Î±
Î¨ 1âˆ’
â‰¤Î±
n

(51)

where the first inequality only hold for n > 2 and Î± â‰¤âˆˆ [0, 21 ]. Hence any solution to (50) must satisfy cÎ± â‰¥ 1 âˆ’ n3 Î±.
Lemma A.4. Let cÎ± as in (42). Let g âˆˆ Rn be defined as
[g]i =

(Î± âˆ’ 1) n1 Sn (cÎ± ) + ciâˆ’1
Î±
1 + Î±âˆ’1
n

(52)

Approximate Steepest Coordinate Descent (ASCD)

Then
2

max

kgkâˆ

iâˆˆ[n] 1
n

2

kgk2

â‰¤ 3 + 3Î± .

(53)

Proof. Observe

(Î± âˆ’ 1) n1 Snâˆ’1 (cÎ± ) + cnâˆ’1
+ cnâˆ’1
+ (ciâˆ’1
âˆ’ cnâˆ’1
Î±
Î±
Î±
Î± )
[g]i =
Î±âˆ’1
1+ n
(42)

=

ciâˆ’1
âˆ’ cnâˆ’1
Î±
Î±
1 + Î±âˆ’1
n

(54)
(55)

Thus [g]1 > [g]2 > Â· Â· Â· > [g]n and the maximum is attained at
Ï‰(g) :=

1
n

[g]2
Pn 1

2
i=1 [g]i

=


2
n
c2Î± c2Î± âˆ’ 1 1 âˆ’ cnâˆ’1
Î±
2cn+1
+ 2cn+2
âˆ’ 2c2n+1
+ (n âˆ’ 1)c2n+2
âˆ’ c2Î± âˆ’ nc2n
Î±
Î±
Î±
Î±
Î±

(56)

For cÎ± â‰¥ 1 âˆ’ n3 Î± and Î± â‰¤ 12 , this latter expression can be estimated as
Ï‰(g) â‰¤ 3 + 3Î±
especially Ï‰(g) â‰¤ 4 for Î± â‰¤

(57)

1
3.

B. Approximate Gradient Update
In this section we will prove Lemma 4.1. Consider first the following simpler case, where we assume f is given as in least
2
squares, i.e. f (x) := 12 kAx âˆ’ bk .
In the tth iteration, we choose coordinate it to optimize upon and the update from xt+1 to xt can be written as xt+1 =
xt + Î³t eit . Now for any coordinate i other than it , it is fairly easy to compute the change in the gradient of the other
coordinates. We already observed that [xt ]j does not change, hence the sub-gradient set of Î¨j ([xt ]j ) and Î¨j ([xt+1 ]j ) are
equal. For the change in âˆ‡f , consider the analysis below:
>
âˆ‡i F (xt+1 ) âˆ’ âˆ‡i F (xt ) = a>
i (Axt+1 âˆ’ b) âˆ’ ai (Axt âˆ’ b)

= a>
i A(xt+1 âˆ’ xt )

= a>
i A(xt + Î³t eit âˆ’ xt )

>
= a>
i Î³Aeit = Î³t ai ait

(58)
(59)
(60)
(61)

Equation (60) comes from the update of xt to xt+1 .
By the same reasoning, we can now derive the general proof.
Proof of Lemma 4.1. Consider a composite function F as given in Lemma 4.1. By the same reasoning as above, the two
sub-gradient sets of Î¨j ([xt ]j ) and Î¨j ([xt+1 ]j ) are identical, for every passive coordinate j 6= it . The gradient of F can
be written as:
âˆ‡i F (Î±t ) = a>
i âˆ‡f (AÎ±t )
For any arbitrary passive coordinate j 6= it the change of the gradient can be computed as follows:
>
âˆ‡j F (xt+1 ) âˆ’ âˆ‡j F (xt ) = a>
j âˆ‡f (Axt+1 ) âˆ’ aj âˆ‡f (Axt )

= a>
j (âˆ‡f (Axt+1 ) âˆ’ âˆ‡f (Axt ))



= a>
j âˆ‡f A(xt + Î³t eit ) âˆ’ âˆ‡f Axt

âˆ— 

= A> âˆ‡2 f (AxÌƒ)aj , xt+1 âˆ’ xt



= Î³t âˆ‡2 f (AxÌƒ)aj , A(xt+1 âˆ’ xt )

(62)

2
= Î³t a>
j âˆ‡ f (AxÌƒ)ait

(63)

Here xÌƒ is a point on the line segment between [xt ]it and [xt+1 ]it which can be found by the Mean Value Theorem.

Approximate Steepest Coordinate Descent (ASCD)

C. Algorithm and Stability
Proof of Theorem 6.1. As we are interested to study the expected competitive ration E [Ït ] for t â†’ âˆ, we can assume
mixing and consider only the steady state.
Define Î±t âˆˆ [0, 1] s.t. Î±t (n âˆ’ s) = |{i âˆˆ It | i > s}|. I.e. Î±t (n âˆ’ s) denotes the number of indices in |It | which do not
belong to the set [s].
Denote Î±âˆ := limtâ†’âˆ Î±t . By equilibrium considerations, the probability that an index i âˆˆ
/ [s] gets picked (and removed
from the active set), i.e. 1 âˆ’ Ïâˆ , must be equal to the probability that an index j âˆˆ
/ [s] enters the active set. Hence
Î±âˆ (n âˆ’ s)
(1 âˆ’ Î±âˆ )(n âˆ’ s)
= 1 âˆ’ Ïâˆ =
.
Tâˆ
Î±âˆ (n âˆ’ s) + cs

(64)

We deduce the quadratic relation Î±âˆ Tâˆ = (1 âˆ’ Î±âˆ ) (Î±âˆ (n âˆ’ s) + cs) with solution
p
2
n âˆ’ (1 + c)s âˆ’ Tâˆ + n2 + (c âˆ’ 1)2 s2 + 2n((c âˆ’ 1)s âˆ’ Tâˆ ) + 2(1 + c)sTâˆ + Tâˆ
Î±âˆ =
.
2(n âˆ’ s)

(65)

2
Denote Î¸ := n2 + (c âˆ’ 1)2 s2 + 2n((c âˆ’ 1)s âˆ’ Tâˆ ) + 2(1 + c)sT + Tâˆ
. Hence,
(64)

Ïâˆ =

cs
2cs
(65)
âˆš .
=
Î±âˆ (n âˆ’ s) + cs
cs + n âˆ’ s âˆ’ Tâˆ + Î¸

(66)

We now verify the provided lower bound on Ïâˆ :
(64)

Ïâˆ = 1 âˆ’

(1 âˆ’ Î±âˆ )(n âˆ’ s)
nâˆ’s
â‰¥1âˆ’
.
Tâˆ
Tâˆ

(67)

This bound is sharp for large values of Tâˆ , (Tâˆ > 2n, say), but trivial for Tâˆ â‰¤ n âˆ’ s.

D. GS rule for Composite Functions
D.1. GS-q rule
In this section we show how ASCD can be implemented for the GS-q rule. Define the coordinate-wise model
Vi (x, y, s) := sy +

L 2
y + Î¨i (xi + y)
2

(68)

The GS-q rule is defined as (cf. Nutini et al. (2015))
i = arg min min V (x, y, âˆ‡i f (x))
iâˆˆ[n]

(69)

yâˆˆR

First we show that the vectors v and w defined in Algorithm 2 gives valid upper and lower bounds on the value of
minyâˆˆR V (x, y, âˆ‡i f (x)). We start with the lower bound v:
Suppose we have upper and lower bounds, ` â‰¤ âˆ‡i f (x) â‰¤ u on one component of the gradient. Define Î± âˆˆ [0, 1] such that
âˆ‡i f (x) = (1 âˆ’ Î±)` + Î±u. Note that
(1 âˆ’ Î±)Vi (x, y, `) + Î±Vi (x, y, u) = Vi (x, y, âˆ‡i f (x))

(70)

Hence,




min min Vi (x, y, u), min Vi (x, y, `)
y

y

â‰¤ min Vi (x, y, âˆ‡i f (x)) .

(71)

y

Define `? := arg minyâˆˆR Vi (x, y, `), u? :=

The derivation of the upper bounds w is a bit more cumbersome.
arg minyâˆˆR Vi (x, y, u) and observe:

Vi (x, u? , âˆ‡i f (x)) = Vi (x, u? , u) âˆ’ (u âˆ’ âˆ‡i f (x))u? â‰¤ Vi (x, u? , u) âˆ’ uu? + max{uu? , `u? } =: Ï‰u
?

?

?

?

?

?

?

Vi (x, ` , âˆ‡i f (x)) = Vi (x, ` , `) âˆ’ (` âˆ’ âˆ‡i f (x))` â‰¤ Vi (x, ` , `) âˆ’ `` + max{u` , `` } =: Ï‰`
Vi (x, 0, âˆ‡i f (x)) = Î¨i ([x]i )

(72)
(73)
(74)

Approximate Steepest Coordinate Descent (ASCD)

Hence miny Vi (x, y, âˆ‡i f (x)) â‰¤ min{Ï‰` , Ï‰u , Î¨i ([x]i )}.
Note
Ï‰u = Vi (x, u? , u) + max{0, (` âˆ’ u)u? }

(75)

Ï‰` = Vi (x, `? , `) + max{0, (u âˆ’ `)`? }

(76)

which coincides with the formulas in Algorithm 2.
It remains to show that the computation of the active set is save, i.e. that the progress achieved by ASCD as defined in
Algorithm 2 is always better than the progress achieved by UCD. Let I be defined as in Algorithm 2. Then
1 X
1 X
min Vi (x, y, âˆ‡i f (x)) â‰¤
min Vi (x, y, âˆ‡i f (x))
yâˆˆR
yâˆˆR
|I|
n
iâˆˆI

(77)

iâˆˆ[n]

=

X
1
minn
Vi (x, y, âˆ‡i f (x)) .
n yâˆˆR

(78)

iâˆˆ[n]

Using this observation, and the same lines of reasoning as given in (Lee & Seung, 1999, Section H.3), it follows immediately that the one step progress of ASCD is at least as good as the for UCD.
D.2. GS-r rule
With the notation [y ? ]i := arg minyâˆˆR Vi (x, y, âˆ‡i f (x)), the GS-r rule is defined as (cf. Lee & Seung (1999))
i = arg max |[y ? ]i | .

(79)

iâˆˆ[n]

In order to implement ASCD for GS-r, we need therefore to maintain lower and upper bounds on the values |[y ? ]i |.
Suppose we have upper and lower bounds, ` â‰¤ âˆ‡i f (x) â‰¤ u on one component of the gradient. Define `? :=
arg minyâˆˆR Vi (x, y, `), u? := arg minyâˆˆR Vi (x, y, u), then y ? is contained in the line segment between `? and u? . Hence
as in Algorithm 1, the lower and upper bounds can be defined as
[ut ]i := max{`? â‰¤ y â‰¤ u? }

(80)

[`t ]i := min{`? â‰¤ y â‰¤ u? }

(81)

yâˆˆR

yâˆˆR

However, note that in (Nutini et al., 2015) it is established that GS-r rule can be worse than UCD in general. Hence we
cannot expect that ASCD for the GS-r rule is better than UCD in general. However, the by the choice of the active set, the
index chosen by the GS-r rule is always contained in the active set, and ASCD approaches GS-r for small errors.

E. Experimental Details
We generate a matrix A âˆˆ RmÃ—n from the standard normal N (0, 1) distribution. m is kept fixed at 1000 but n is chosen
1000 for the l2 regularized least squares regression and 5000 for l1 regularized counterpart. 1 is added to each entry (to
induce a dependency between columns), multiplied each column by a sample from N (0, 1) multiplied by ten (to induce
different Lipschitz constants across the coordinates), and only kept each entry of A non-zero with probability 10 log(n)
n .
This is exactly the same procedure which has been discussed in (Nutini et al., 2015).

