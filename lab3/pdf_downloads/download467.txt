Why is Posterior Sampling Better than Optimism for Reinforcement Learning?

APPENDICES
A. Proof of Lemma 3
This section centers around the proof of Lemma 3, which we reproduce below for completeness. In the main paper we
present a simple sketch for the special case of S = 2. We now extend this argument to general MDPs with S > 2. The main
strategy for this proof is to proceed via an inductive argument and consider the contribution of each component of Pk in
turn. We will see that, for any choice of component, the resultant random variable is dominated by a matched Gaussian
random variable just as in (12).
Lemma 3 (Transition concentration). For any independent prior over rewards with r 2 [0,1], additive sub-Gaussian noise
and an independent Dirichlet prior over transitions at state-action pair xkh , then
s
2log(2/ )
whP (xkh )  2H
(11)
max(nk (xkh ) 2,1)
with probability at least 1

.

Our analysis of Lemma 3 will rely heavily upon the technical analysis of Osband & Van Roy (2017). We first reproduce
Lemma 2 from Osband & Van Roy (2017) in terms of stochastic optimism, rather than second order stochastic dominance.
Lemma 4 (Beta vs Dirichlet dominance).
Let X = P > v for the random variable P ⇠ Dirichlet(↵) and constants v 2 RS and ↵ 2 RS+ . Without loss of generality,
Ps
Pd
assume v1  v2  ···  vS . Let ↵
˜ = i=1 ↵i (vi v1 )/(vd v1 ) and ˜ = i=1 ↵i (vd vi )/(vd v1 ). Then, there exists a
random variable P̃ ⇠ Beta(↵
˜ , ˜) such that, for X̃ = P̃ vd + (1 P̃ )v1 , E[X̃|X] = X and X̃ <so X.
Pd
Proof. Let i = Gamma(↵,1) be independent and identically distributed and let = i=1 i , so that P ⌘D / . Let
↵i0 = ↵i (vi v1 )/(vd v1 ) and ↵i1 = ↵i (vd vi )/(vd v1 ) so that ↵ = ↵0 + ↵1 . Define independent random variables
0
⇠ Gamma(↵i0 ,1) and 1 ⇠ Gamma(↵i1 ,1) so that ⌘D 0 + 1 .
Pd
Take 0 and 1 to be independent, and couple these variables with so that = 0 + 1 . Note that ˜ = i=1 ↵i0 and
Pd
P
P
d
d
↵
˜ = i=1 ↵i1 . Let 0 = i=1 i0 and 1 = i=1 i1 , so that 1 P̃ ⌘D 0 / and P̃ ⌘D 1 / . Couple these variables so that
0
1
1 P̃ = / and P̃ = / . We can now say,

v1 0 vd 1
E[X̃|X] = E[(1 P̃ )v1 + P̃ vd |X] = E
+
X
 

v1 0 + vd 1
v1 E[ 0 | ] + vd E[ 1 | ]
,X X = E
X
= E E
" P
#
Pd
d
v1 i=1 E[ i0 | i ] + vd i=1 xp[ i1 | i ]
= E
X
(a)

=
=
=

E
E

E

"
"

v1
v1

Pd

0
i=1 i ↵i /↵i + vd

Pd

"P

Pd

1
i=1 i ↵i /↵i

X

#

#
Pd
v1 ) + vd i=1 i (vd vi )
X
(vd v1 )
#
" d
#
X
i vi
X =E
pi vi X = X,

i=1 i (vi

d
i=1

i=1

where (a) follows from elementary properties of Gamma distribution (Osband & Van Roy, 2017). Therefore, X̃ is a
mean-preserving spread of X and so by definition of stochastic optimism X̃ <so X.

Why is Posterior Sampling Better than Optimism for Reinforcement Learning?

Next, consider any fixed Pk (xkh ) and let Rk and Pk (x 6= xkh ) vary in any arbitrary way to maximize the variation from
k
k
transition wkP (xkh ) = (Pk (xkh ) P̂ (xkh ))T Vkh+1
through their effects on the future value Vkh+1
2 [0,H]S . We can then
S
upper bound the deviation from transitions by the deviation under the worst possible v 2 [0,H] .
whP (xkh ) 

max

Rk ,Pk (x6=xkh )

(Pk (xkh )

k
P̂k (xkh ))T Vkh+1
 max (Pk (xkh )
v2[0,H]S

P̂k (xkh ))T v.

(16)

We can then apply Lemma 4 to (16): for any possible value of v 2 [0,H]S there is a matched Beta random variable that
P
is stochastically
⇣
⌘ optimistic for wh (xkh ). This means that we can then apply Lemma 2 to show that there is a matched
2

X ⇠ 0, ↵HT 1 <so whP (xkh ). To complete the proof of Lemma 3 we apply the Gaussian tail concentration Lemma 1.

p
B. Conjecture of Õ( HSAT ) bounds

PH
P
The key remaining loose piece of our analysis concerns the
q summation h=1 wh (xkh ). Our current proof of
q Theorem
H
H
P
2 bounds each wh (xkh ) independently. Each term is Õ( nk (xkh ) ) and we bound the resulting sum Õ(H nk (x
).
kh )
However, this approach is very loose and pre-supposes that each timestep could be maximally bad during a single episode.
To repeat our geometric intuition, we have assumed a worst-case hyper-rectangle
over all timesteps H when the actual
p
geometry should be an ellipse. We therefore suffer an additional term of Õ( H) in exactly the style of Figure 3.
In fact, it is not even possible to sequentially get the “worst-case” transitions O(H) at each and every timestep during an
episode, since once your sample gets one such transition then there will be no more future
p value to deplete. Rather than just
being independent per timestep, which would be enough for us to end up with an Õ( H) saving, they actually have some
kind of anti-correlation property through the law of total variance. A very similar observation is used by recent analyses in
the sample complexity setting (Lattimore & Hutter, 2012) and also finite horizon MDPs (Dann & Brunskill, 2015). This
seems to suggest that it should be possible
to combine
the insights of Lemma 3 with, for example, Lemma 4 of (Dann &
p
p
Brunskill, 2015) to remove both the S and the H from our bounds to prove Conjecture 1.
We note that this informal argument would not apply Gaussian PSRL, since it generates wP from some Gaussian posterior
which does not satisfy the Bellman operators. Therefore, we should be able to find some evidence for this conjecture if we
find domains where UCRL, Gaussian PSRL and PSRL all demonstrate their (unique) predicted scalings. We present some
evidence of this effect in Section 5.3 and find that that our empirical results are consistent with this conjecture.

C. Estimation experiments
In this section we expand upon the simple examples given by Section 4.1 to a full decision problem with two actions. We
define an MDP similar to Figures 1 and 2 but now with two actions. The first action is identical to Figure 1, but the second
action modifies the transition probabilities to favor the rewarding states with probability 0.6/N and assigning only 0.4/N
to the non-rewarding states.
We now investigate the regret of several learning algorithms which we adapt to this setting. These algorithms are based
upon BEB (Kolter & Ng, 2009), BOLT (Araya et al., 2012), ✏-greedy with ✏ = 0.1, Gaussian PSRL (see Algorithm 3),
Optimistic PSRL (which takes K = 10 samples and takes the maximum over sampled Q-values similar to BOSS (Asmuth
et al., 2009)), PSRL (Strens, 2000), UCFH (Dann & Brunskill, 2015) and UCRL2 (Jaksch et al., 2010). We link to the full
code for implementation in Appendix D.
We see that the loose estimates in OFU algorithms from Figures 4 and 5 lead to bad performance in a decision problem.
This poor scaling with the number of successor states N occurs when either the rewards or the transition function is
unknown. We note that in stochastic environments the PAC-Bayes algorithm BOLT, which relies upon optimistic fake
prior data, can sometimes concentrate too quickly and so incur the maximum linear regret. In general, although BOLT is
PAC-Bayes, it concentrates too fast to be PAC-MDP just like BEB (Kolter & Ng, 2009).
In Figure 12 we see a similar effect as we increase the episode length H. We note the second order UCFH modification
improves upon UCRL2’s miscalibration with H, as is reflected in their bounds (Dann & Brunskill, 2015). We note that
both BEB and BOLT scale poorly with the horizon H.

Why is Posterior Sampling Better than Optimism for Reinforcement Learning?

Figure 10. Known rewards R and unknown transitions P , similar to Figure 4.

Figure 11. Unknown rewards R and known transitions P , similar to Figure 5.

Figure 12. Unknown rewards R and transitions P , similar to Figure 6.

D. Chain experiments
All of the code and experiments used in this paper are available in full on github. As per the review request we have removed
the link to this code, but instead include an anonymized excerpt of the some of the code in our submission file. We hope
that researchers will find this simple codebase useful for quickly prototyping and experimenting in tabular reinforcement
learning simulations.

Why is Posterior Sampling Better than Optimism for Reinforcement Learning?

In addition to the results already presented we also investigate the scaling of similar Bayesian learning algorithms BEB
(Kolter & Ng, 2009) and BOLT (Araya et al., 2012). We see that neither algorithms scale as gracefully as PSRL, although
BOLT comes close. However, as observed in Appendix C, BOLT can perform poorly in highly stochastic environments.
BOLT also requires S-times more computational cost than PSRL or BEB. We include these algorithms in Figure 13.

Figure 13. Scaling of more learning algorithms.

D.1. Rescaling confidence sets
It is well known that provably-efficient OFU algorithms can perform poorly in practice. In response to this observation,
many practitioners suggest rescaling confidence sets to obtain better empirical performance (Szita & Szepesvári, 2010;
Araya et al., 2012; Kolter & Ng, 2009). In Figure 14 we present the performance of several algorithms with confidence sets
rescaled 2 {0.01,0.03,0.1,0.3,1}. We can see that rescaling for tighter confidence sets can sometimes give better empirical
performance. However, it does not change the fundamental scaling of the algorithm. Also, for aggressive scalings some
seeds may not converge at all.

Figure 14. Rescaled proposed algorithms for more aggressive learning.

Why is Posterior Sampling Better than Optimism for Reinforcement Learning?

D.2. Prior sensitivities
We ran all of our Bayesian algorithms with uninformative independent priors for rewards and transitions. For rewards, we
use r(s,a) ⇠ N (0,1) and updated as if the observed noise were Gaussian with precision ⌧ = 12 = 1. For transitions, we use
a uniform Dirichlet prior P (s,a) ⇠ Dirchlet(↵). In Figures 15 and 16 we examine the performance of Gaussian PSRL and
PSRL on a chain of length N = 10 as we vary ⌧ and ↵ = ↵0 1.

Figure 15. Prior sensitivity in Gaussian PSRL.

Figure 16. Prior sensitivity in PSRL.

We find that both of the algorithms are extremely robust over several orders of magnitude. Only large values of ⌧ (which
means that the agent updates it reward prior too quickly) caused problems for some seeds in this environment. Developing
a more clear frequentist analysis of these Bayesian algorithms is a direction for important future research.
D.3. Optimistic posterior sampling
We compare our implementation of PSRL with a similar optimistic variant which samples K 1 samples from the posterior
and forms the optimistic Q-value over the envelope of sampled Q-values. This algorithm is sometimes called “optimistic
posterior sampling” (Fonteneau et al., 2013). We experiment with this algorithm over several values of K but find that the
resultant algorithm performs very similarly to PSRL, but at an increased computational cost. We display this effect over
several magnitudes of K in Figures 17 and 18.

Why is Posterior Sampling Better than Optimism for Reinforcement Learning?

Figure 17. PSRL with multiple samples is almost indistinguishable.

Figure 18. PSRL with multiple samples is almost indistinguishable.

This algorithm “Optimistic PSRL” is spiritually very similar to BOSS (Asmuth et al., 2009) and previous work had suggested that K > 1 could lead to improved performance. We believe that an important difference is that PSRL, unlike
Thompson sampling, should not resample every timestep but previous implementations had compared to this faulty benchmark (Fonteneau et al., 2013).

