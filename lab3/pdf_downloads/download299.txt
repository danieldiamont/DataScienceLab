Contextual Decision Processes with low Bellman rank are PAC-Learnable

Appendix
This appendix is organized as follows. We begin with describing the formal statements for the extensions of Theorem 1
along with necessary algorithmic modifications in Appendix A. We then provide formal statements and proofs for several
models with low Bellman rank introduced in Section 3 in Appendix B. In Appendix C, we provide the proofs of our main
results, including a more general version of Theorem 1 which also covers the aforementioned extensions. In Appendix D
we give the various technical lemmas required for our proofs and we present the main lower bound statements and proofs
in Appendix F.

A. Extensions
We introduce four important extensions to the algorithm and analysis.
A.1. Unknown Bellman Rank
The first extension eliminates the need to know M in advance (note that Algorithm 1 requires M as an input parameter). A
simple procedure, as described in Algorithm 2, can guess the value of M by a doubling schedule and handle this situation
with no consequences to asymptotic sample complexity.8
Algorithm 2 G UESS M(F, ζ, , δ)
1: for i = 1, 2, . . . do
2:
M 0 ← 2i .
3:
Call O LIVE(F, M 0 , ,

δ
i(i+1) )

with parameters specified on Theorem 1.
 √ 0 
Terminate the subroutine when t > HM 0 log 6H M ζ / log(5/3) in Line 4 (the for-loop).
if a policy π is returned from O LIVE then
Return π.
end if
end for

4:
5:
6:
7:
8:

Theorem 2. For any , δ ∈ (0, 1), any Contextual Decision Process and function class F that admit a Bellman factorization with parameters M, ζ, if we run G UESS M(F, , δ), then with probability at least 1 − δ, G UESS M halts and returns a
policy which satisfies V π̂ ≥ VF? − , and the number of episodes required is at most

Õ


M 2H 3K
log(N
ζ/δ)
.
2

We give some intuition about the proof here, with details in Appendix E.1. In Algorithm 2, M 0 is a guess for M which
δ
grows exponentially. When M 0 ≥ M , analysis of the main algorithm shows that O LIVE(F, M 0 , , i(i+1)
) terminates
and returns a near-optimal policy with high probability. The doubling schedule implies that the largest guess is at most
2M , which has negligible effect on the sample complexity. On the other hand, O LIVE may not explore effectively when
M 0 < M , because not enough samples (chosen according to M 0 ) are used to estimate the average Bellman errors in
Line 13 of O LIVE. This worse accuracy does not guarantee sufficient progress in learning.
However, the high-probability guarantee that f ? is not eliminated is unaffected, because the threshold φ on Line 14 of
O LIVE is set in accordance with the sample size n specified in Theorem 1, regardless of M . Consequently, if the algorithm
ever terminates when M 0 < M , we still get a near-optimal policy. When M 0 < M the O LIVE subroutine may not
terminate, which the explicit termination on line 4 in Algorithm 2 addresses. Finally, by splitting the failure probability δ
appropriately among all guesses of M 0 , we obtain the same order of sample complexity as in Theorem 1.
8

In Algorithm
2 we assume that ζ is fixed. In the examples
provided in Proposition 1, 2, and 3, however, ζ grows with M in the form
√
√
of ζ = 2 M . In this case, we can compute ζ 0 = 2 M 0 and call O LIVE with ζ 0 instead of ζ. As long as ζ is a polynomial term and
non-decreasing in M the same analysis applies and Theorem 2 holds.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

A.2. Separation of Policy Class and V-value Class
So far, we have assumed that the agent has access to a class of Q-value functions F ⊂ X × A → [0, 1]. In this section, we
show the algorithm allows separate representations of policies and V-value functions.
For every f ∈ F, and any x ∈ X , a 6= πf (x), we note that the value of f (x, a) is not used by Algorithm 1, and changing
it to arbitrary values does not affect the execution of the algorithm as long as f (x, a) ≤ f (x, πf (x)) (so that πf does not
change). In other words, the algorithm only interacts with f in two forms:
1. f ’s greedy policy πf .
2. A mapping gf : x 7→ f (x, πf (x)). We call such mappings V-value functions to contrast the previous use of Q-value
functions.9
Hence, supplying F is equivalent to supplying the following space of (policy, V-value function) pairs:

{ πf , gf : f ∈ F}.
This observation provides further evidence that Definition 3 is significantly less restrictive than standard realizability assumptions. Validity of f means that (πf , gf ) obeys the Bellman Equations for Policy Evaluation (i.e., gf predicts the
long-term value of following πf ), as opposed to the more common Bellman Optimality Equations. In MDPs, there are
many ways to satisfy the policy evaluation equations at every state simultaneously, while Q? is the only function that
satisfies all optimality equations.
More generally, instead of using a Q-value function class, we can run O LIVE with a policy space Π ⊂ X → A and a
V-value function class G ⊂ X → [0, 1] where we assemble (policy,V-value function) pairs by taking the Cartesian product
of Π and G. O LIVE can be run here with the understanding that each Q-value function f in O LIVE is associated with a
(π, g) pair, and the algorithm uses π instead of πf and g(x) instead of f (x, πf (x)). All the analysis applies directly with
this transformation, and the log |F| dependence in sample complexity is replaced by log |Π| + log |G|. Note also that the
definition of Bellman factorization also extends naturally to this case, where f in Equation 3 corresponds to a (π, g) pair
and f 0 corresponds to a roll-in policy, π 0 .
A.3. Infinite Hypothesis Classes
The arguments in Section 4 assume that |F| = N < ∞. However, almost all commonly used function approximators are
infinite classes, which restricts the applicability of the algorithm. On the other hand, the size of the function class appears
in the analysis only through deviation bounds, so techniques from empirical process theory can be used to generalize the
results to infinite classes. This section establishes parallel versions of those deviation bounds for function classes with
finite combinatorial dimensions, and together with the rest of the original analysis we can show the algorithm enjoys
similar guarantees when working with infinite hypothesis classes.
Specifically, we consider the setting where Π and G are given (see Appendix A.2), and they are infinite classes with
finite combinatorial dimensions. We assume that Π has finite Natarajan dimension (Definition 6), and G has finite pseudo
dimension (Definition 7). These two dimensions are standard extensions of VC-dimension to multi-class classification and
regression respectively.
Definition 6 (Natarajan dimension (Natarajan, 1989)). Suppose X is a feature space and Y is a finite label space. Given
hypothesis class H ⊂ X → Y, its Natarajan dimension Ndim(H) is defined as the maximum cardinality of a set A ⊆ X
that satisfies the following: there exists h1 , h2 : A → Y such that (1) ∀x ∈ A, h1 (x) 6= h2 (x), and (2) ∀B ⊆ A, ∃h ∈ H
such that ∀x ∈ B, h(x) = h1 (x) and ∀x ∈ A \ B, h(x) = h2 (x).
Definition 7 (Pseudo dimension (Haussler, 1992)). Suppose X is a feature space. Given hypothesis class H ⊂ X → R,
its pseudo dimension Pdim(H) is defined as Pdim(H) = VC-dim(H+ ), where H+ = {(x, ξ) 7→ 1[h(x) > ξ] : h ∈ H} ⊂
X × R → {0, 1}.
The definition of pseudo dimension relies on that of VC-dimension, whose definition and basic properties are recalled
in Appendix E.2. We state the final sample complexity result here. Since the algorithm parameters are somewhat com9

In the MDP setting, such functions are also known as state-value functions.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

plex expressions, we omit them in the theorem statement and provide specification in the proof, which is deferred to
Appendix E.2.
Theorem 3. Let Π ⊂ X → A with Ndim(Π) ≤ dΠ < ∞ and G ⊂ X → [0, 1] with Pdim(G) ≤ dG < ∞. For
any , δ ∈ (0, 1), any Contextual Decision Process with policy space Π and function space G that admits a Bellman
factorization with parameters M, ζ, if we run O LIVE with appropriate parameters, then with probability at least 1 − δ,
O LIVE halts and returns a policy π̂ that satisfies V π̂ ≥ VF? − , and the number of episodes required is at most
 2 3 2

M H K
d
+
d
+
log(ζ/δ)
.
(7)
Õ
Π
G
2
Compared to Theorem 1, the sample complexity we get for infinite hypothesis classes has two differences: (1) log N is
replaced by dΠ +dG , which is expected, based on the discussion in Appendix A.2, and (2) the dependence on K is quadratic
as opposed to linear. In fact, in the proof of Theorem 1, we exploited the low-variance property of importance weights in
Line 13 of O LIVE, and applied Bernstein’s inequality to avoid a factor of K. With infinite hypothesis classes, the same
approach does not apply directly. However, this may only be a technical issue, and a more refined analysis might recover
the linear dependence (e.g., using tools from Panchenko (2002)).
A.4. Approximate Validity and Approximate Bellman Rank
Recall that the sample-efficiency guarantee of O LIVE relies on two major assumptions:
• We assumed that F contains valid functions (Definition 3). In practice, however, it is hard to specify a function
class that contains strictly valid functions, as the notion of validity depends on the environment dynamics, which are
unknown. A much more realistic situation is that some functions in F satisfy validity only approximately.
• We assumed that the average Bellman errors have an exact low-rank factorization (Definition 5). While this is true
for a number of RL models (Section 3), it is worth keeping in mind that these are only models of the environments,
which are different from and only approximations to the real environments themselves. Therefore, it is more realistic
to assume that an approximate factorization exists when defining Bellman factorization.
In this section, we show that the algorithmic ideas of O LIVE are indeed robust against both types of approximation errors,
and degrades gracefully as the two assumptions are violated. Below we introduce the approximate versions of Definition 3
and 5, give a slightly extended version of the algorithm, O LIVER (for Optimism-Led Iterative Value-function Elimination
with Robustness, see Algorithm 3), and state its sample complexity guarantee in Theorem 4.
Definition 8 (Approximate validity of f ). Given any CDP and function class F, we say f ∈ F is θ-valid if for any f 0 ∈ F
and any h ∈ [H], |E(f, πf 0 , h)| ≤ θ.
The approximation error θ introduced in Definition 8 allows the algorithm to compete against a broader range of functions;
hence the notions of optimal function and value need to be re-defined accordingly.
Definition 9. For a fixed θ, define fθ? = argmaxf ∈F : f is θ-valid V πf , and VF?,θ = V

πf ?
θ

.

By definition, VF?,θ is non-decreasing in θ with Definition 3 being a special case where θ = 0. When θ > 0, we compete
against some functions that do not obey Bellman equations, breaking an essential element of value-based RL. As a consequence, returning a policy with value close to VF?,θ in a sample-efficient manner is very challenging, so the value that
O LIVER can guarantee is suboptimal to VF?,θ by a term that is proportional to θ and does not diminish with more data.
Definition 10 (Approximate Bellman rank). We say that a CDP (X , A, H, P ) and F ⊂ X × A → [0, 1], admits a Bellman
factorization with Bellman rank M , norm parameter ζ, and approximation error η, if there exists νh : F → RM , ξh : F →
RM for each h ∈ [H], such that for any f, f 0 ∈ F, h ∈ [H],
|E(f, πf 0 , h) − hνh (f 0 ), ξh (f )i| ≤ η,

(8)

and kνh (f 0 )k2 · kξh (f )k2 ≤ ζ < ∞.
A modified version of O LIVE that deals with these approximation errors, O LIVER, is specified in Algorithm 3. Here, we
use  to denote the component of the suboptimality that diminishes as more data is collected, and the total suboptimality

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Algorithm 3 O LIVER (F, θ, M, ζ, η, , δ)
√
1: Let 0 =  + 2H(3 M (θ + η) + η).
(i) nest
2: Collect nest trajectories with actions taken in an arbitrary manner; save initial contexts {x1 }i=1
.
P
(i)
(i)
nest
1
3: Estimate the predicted value for each f ∈ F: V̂f = nest
i=1 f (x1 , πf (x1 )).
4: F0 ← F.
5: for t = 1, 2, . . . do
6:
Choose policy ft = argmaxf ∈Ft−1 V̂f , πt = πft .
7:
8:

(i)

(i)

(i)

(i)

(i)

(i)

(i)

eval
Collect neval trajectories {(x1 , a1 , r1i , . . . , xH , aH , rH )}ni=1
by following πt (i.e. ah = πt (xh ) for all h, i).
Estimate ∀h ∈ [H],

Ẽ(ft , πt , h) =

neval h
i
1 X
(i) (i)
(i)
(i)
(i)
ft (xh , ah ) − rh − ft (xh+1 , ah+1 ) .

neval

(9)

i=1

PH

Ẽ(ft , πt , h) ≤ 50 /8 then
Terminate and ouptut πt .
end if
Pick any ht ∈ [H] for which Ẽ(ft , πt , ht ) ≥ 50 /8H (One is guaranteed to exist).
(i) (i) (i)
(i) (i) (i)
(i)
(i)
(i)
13:
Collect trajectories {(x1 , a1 , r1 , . . . , xH , aH , rH )}ni=1 where ah = πt (xh ) for all h 6= ht and aht is drawn
uniformly at random.
14:
Estimate

9:
10:
11:
12:

if

h=1

(i)
(i)
n

X
1[aht = πf (xht )] 
(i) (i)
(i)
(i)
(i)
b πt , ht ) = 1
f (xht , aht ) − rht − f (xht +1 , πf (xht +1 )) .
E(f,
n i=1
1/K

15:

(10)

Learn
n
Ft = f ∈ Ft−1



o

b
πt , ht ) ≤ φ + θ .
: E(f,

(11)

16: end for

that we can guarantee is  plus a term proportional to θ and η (see Eq. (12) in Theorem 4). The algorithm is almost identical
to O LIVE except in two places: (1) it uses 0 (defined on Line 1) in the termination condition (Line 9) as opposed to , and
(2) it uses a higher threshold that depends on θ in Eq. (11) to avoid eliminating θ-valid functions. The approximation and
sample complexity guarantees of O LIVER are stated in Theorem 4, with the proof deferred to Appendix D.
Theorem 4. For any , δ ∈ (0, 1), any Contextual Decision Process and function class F that admit a Bellman factorization with parameters M , ζ, and η, suppose we run O LIVER with any θ ∈ [0, 1], and nest , neval , n, φ as specified in
Theorem 1. Then with probability at least 1 − δ, O LIVER halts and returns a policy π̂ which is at most
√
 + 8H M (θ + η)
(12)
suboptimal compared to VF?,θ defined in Definition 9, and the number of episodes required is at most

 2 3
M H K
log(N
ζ/δ)
.
Õ
2

(13)

B. Models with Low Bellman Rank
B.1. Proof of Proposition 1
Let M = |S| and each element of νh (·) and ξh (·) be indexed by
 s ∈ S. We explicitly construct νh and ξh as follows: let
[νh(f 0 )]s = Pr [xh = (s, h) | a1:h−1 ∼ πf 0 ], and [ξh (f )]s = E f (xh , ah ) − rh − f (xh+1 , ah+1 )  xh = (s, h), ah:h+1 ∼
πf . In other words, νh (f 0 ) is the distribution over states induced by πf 0 at time step h, and the s-th element of ξh is
the traditional notion of Bellman error for state s. It is easy to verify that Eq. (3) holds. For the norm constraint, since

Contextual Decision Processes with low Bellman rank are PAC-Learnable

√
√
kνh (·)k1 = 1 and kξh (·)k∞ ≤ 2, we have kνh (·)k2 ≤ 1 and kξh (·)k2 ≤ 2 M , hence ζ = 2 M is a valid upper bound
on the product of vector norms.
B.2. Generalization of Li (2009)’s Setting
Li (2009, Section 8.2.3) considers the setting where the learner is given an abstraction φ that maps the large state space
S in an MDP to some finite abstract state space S̄. |S̄| is potentially much smaller than |S|, and it is guaranteed that Q?
can be expressed as a function of (φ(s), a). Li shows that when delayed Q-learning is applied to this setting, the sample
complexity has polynomial dependence on |S̄| with no direct dependence on |S|.
In the next proposition, we show that a similar setting for finite-horizon problems admits Bellman factorization with low
Bellman rank. In particular, we subsume Li’s setting by viewing it as a POMDP, where φ is a deterministic emission
process that maps hidden state s ∈ S to discrete observations φ(s) ∈ S̄ = O, and the candidate value functions are
reactive so they depend on φ(s) but not directly on s or any previous state. More generally, Proposition 6 claims that for
POMDPs with large hidden-state spaces and finite observation spaces, the Bellman rank is polynomial in the number of
observations if the function class is reactive.
Proposition 6 (A generalization of (Li, 2009)’s setting). Consider a POMDP introduced in Example 2 with |O| < ∞,
and assume that rewards can only take CR different discrete values.10 The CDP (X , A, H, P ) induced by letting X =
2
O × [H] and
√ xh = (oh , h), with any F : X × A → [0, 1], admits a Bellman factorization with M = |O| CR K and
ζ = 2|O|K CR .
Proof. For any f, f 0 ∈ F, h ∈ [H], let νh (f 0 ) and ξh (f ) be vectors of length |O|2 CR K. Let the entry of νh (f 0 ) indexed
by (oh , ah , rh , oh+1 ) be
P [oh , rh , oh+1 | a1:h−1 ∼ πf 0 , ah ],
interpreted as the following: conditioned on the fact that the first h − 1 actions are chosen according to πf 0 , what is
the probability of seeing a particular tuple of (oh , rh , oh+1 ) when taking a particular action for ah ? For ξh (f ), let the
corresponding entry be (with xh = (oh , h) and xh+1 = (oh+1 , h + 1) as the corresponding contexts in the CDP)

1[ah = πf (xh )] f (xh , ah ) − rh − f (xh+1 , πf (xh+1 )) .
It is not hard to verify that E(f, πf 0 , h) = hνh (f 0 ), ξh (f )i. Since fixing ah to any non-adaptive choice of action induces a
0
valid distribution over (oh , rh , oh+1 ), we have kνh (f 0 )k1 = K and kν
√h (f )k2 ≤ K. On the other hand, kξh (f )k∞ ≤ 2
2
but the vector only has |O| CR non-zero entries, so kξh (f )k2 ≤ 2|O| CR . Together the norm bound follows.
B.3. POMDP-like Models
Here we first state the formal version of Proposition 2, and prove Propositions 2 and 3 together by studying a slightly more
general model (See Figure 1).
Proposition 7 (Formal version of Proposition 2). Consider an MDP introduced in Example 1. With a slight abuse of
notation let Γ denote its transition matrix of size |S × A| × |S|, whose element indexed by ((s, a), s0 ) is Γ(s0 |s, a).
Assume that there are two row-stochastic matrices Γ(1) and Γ(2) with sizes |S × A| × M and M × |S| respectively,
xh = (sh , h). For any
such that Γ = Γ(1) Γ(2) . Recall that we convert an MDP into a CDP by letting X = S × [H], √
F ⊂ X × A → [0, 1], this model admits a Bellman factorization with Bellman rank M and ζ = 2 M .
The model in Figure 1 that we use to prove Propositions 2 and 3 simultaneously behaves like a POMDP except that
both the transition function and the reward depends also on the observation, that is Γ : S × O × A → ∆(S) and
R : S × O × A → ∆([0, 1]). Clearly this model generalizes standard POMDPs, where the transition and reward are
both assumed to be independent of the current observation.
This model also generalizes the MDP with low-rank dynamics described in Proposition 2: if the future hidden-state is
independent of the current hidden-state conditioned on the observation (i.e., Γ(s0 |s, o, a) does not depend on s), the observations themselves become Markovian, and we can treat o as the observed state s in Proposition 7, and the hidden-state s
10
The discrete reward assumption is made to simplify presentation and can be relaxed. For arbitrary rewards, we can always discretize
the reward distribution onto a grid of resolution CR , which incurs η = O(1/CR ) approximation error in Definition 10.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

as the low-rank factor in Proposition 7 (see Figure 1). Hence, Proposition 2 follows as a special case of the analysis for
this more general model.
As in Proposition 3, we consider a class F reactive value functions. Observe that for the MDP with low rank dynamics,
this provides essentially no loss of generality, since the optimal value function is reactive.
Proposition 8. Let (X , A, H, P ) be the CDP induced by the model in Figure ?? which generalizes
pPOMDPs, with X =
O × [H] and xh = (oh , h). Given any F : X × A → [0, 1], the Bellman rank M ≤ |S| with ζ = 2 |S|.
Proof. For any f, f 0 ∈ F, h ∈ [H], consider
a1:h−1 ∼ πf 0 , ah:h+1 ∼ πf ,
which is how actions are chosen in the definition of E(f, πf 0 , h) (see Definition 2). Such a decision-making strategy
induces a distribution over the following set of variables
(sh , oh , ah , rh , oh+1 , ah+1 ).
We use µf,f 0 to denote this distribution, and the subscript emphasizes its dependence on both f and f 0 . Note that the
marginal distribution of sh only depends on f 0 and has no dependence on f , which we denote as µf 0 . Then, sampling from
µf,f 0 is equivalent to the following sampling procedure: (recall that xh = (oh , h))
sh ∼ µf 0 , oh ∼ Dsh , ah = πf (xh ), rh ∼ R(sh , oh , ah ),
sh+1 ∼ Γ(sh , oh , ah ), oh+1 ∼ Dsh+1 , ah+1 = πf (xh+1 ).
That is, we first sample sh from the marginal µf 0 , and then sample the remaining variables conditioned on sh . Notice
that once we condition on sh , the sampling of the remaining variable has no dependence on f 0 , so we denote the joint
distribution over the remaining variables (conditioned on the value of sh ) µf |sh .
Finally, we express the factorization of E(f, πf 0 , h) as follows:
E(f, πf 0 , h) = Eµf,f 0 [f (xh , ah ) − rh − f (xh+1 , ah+1 )]
= Esh ∼µf 0 Eµf |sh [f (xh , ah ) − rh − f (xh+1 , ah+1 )]
X
=
µf 0 (s) · Eµf |s [f (xh , ah ) − rh − f (xh+1 , ah+1 )].
s∈S

We define νh (·) and ξh (·) explicitly with dimension M = |S|: given f and f 0 , we index the elements of νh (f√0 ) and those
of ξh (f ) by s ∈ S, and let [νh (f 0 )]s = µf 0 (s), [ξh (f )]s = Eµf |s [f (xh , ah ) − rh − f (xh+1 , ah+1 )]. ζ = 2 M follows
from the fact that kνh (f 0 )k1 = 1 and kξh (f )k∞ ≤ 2.
B.4. Predictive State Representations
In this subsection we state and prove the formal version of Proposition 4. We first recall the definitions and some basic
properties of PSRs, which can be found in Singh et al. (2004); Boots et al. (2011). Consider dynamical systems with
discrete and finite observation space O and action space A. Such systems can be fully specified by moment matrices
PT |H , where H is a set of histories (past events) and T is a set of tests (future events). Elements of T and H are sequences
of alternating actions and observations, and the entry of PT |H indexed by t ∈ T on the row and τ ∈ H on the column is
Pt|τ , the probability that the test t succeeds conditioned on a particular past τ . For example, if t = aoa0 o0 , success of t
means seeing o and o0 in the next two steps after τ is observed, if interventions a and a0 were to be taken.
Among all such systems, we are concerned about those that have finite linear dimension, defined as supT ,H rank(PT |H ).
As an example, the linear dimension of a POMDP is bounded by the number of hidden-states. Systems with finite linear
dimension have many nice properties, which allow them to be expressed by compact models, namely PSRs. In particular,
fixing any T and H such that rank(PT |H ) is equal to the linear dimension (such (H, T ) are called core histories and core
tests), we have:
1. For any history τ ∈ (A × O)∗ , the conditional predictions of core tests PT |{τ } (we also write PT |τ ) is always a state,
that is, a sufficient statistics of history. This gives rise to the name “predictive state representation”.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

2. Based on PT |τ , the conditional prediction of any test t can be computed from a PSR model, parameterized by square
matrices {Bao } and a vector b∞ with dimension |T |. Letting t(i) be the i-th (action, observation) pair in t, and |t| be
the number of such pairs, the prediction rule is
Pt|τ = b>
∞ Bt(|t|) · · · Bt(1) PT |τ .

(14)

And these parameters can be computed as
Bao = PT ,ao,H PT† ,H ,

> †
b>
∞ = PH PT ,H

(15)

where
• PT ,H is a matrix whose element indexed by (t ∈ T , τ ∈ H) is Pτ t|∅ , where τ t is the concatenation of τ and t
and ∅ is the null history.
• PH = P{∅},H .
• PT ,ao,H = PT ,Hao , where Hao = {τ ao : τ ∈ H}.
Now we are ready to state and prove the formal version of Proposition 4.
Proposition 9 (Formal version of Proposition 4). Consider a partially observable system with observation space O, and
the induced CDP (X , A, H, P ) with xh = (oh , h). To handle some subtleties, we assume that
1. |O| < ∞ (classical PSR results assume discrete observations).
2. o1 is deterministic (PSR trajectories always start with an action), and rh is a deterministic function of oh+1 (reward
is usually omitted or assumed to be part of the observation).
If the linear dimension of the original system is at most L, then with any F : X × A → [0, 1], this model admits a Bellman
factorization with M = LK. Assuming further that the PSR’s parameters√are non-negative under some choice of core
3
, where σmin is the minimal non-zero
histories and tests (H, T ) of size |H| = |T | = L, then we have ζ ≤ 2K 2 L3 L/σmin
singular value of PT ,H .
Proof. For any f, f 0 ∈ F, h ∈ [H], define
1. µf 0 ,h as the distribution vector over (a1 , o2 , . . . , oh−1 , ah−1 ) ∈ (A × O)h−2 × A induced by a1:h−1 ∼ πf 0 . (Recall
that o1 is deterministic.)
2. P2|h−1 as a moment matrix whose element with column index (oh , ah , oh+1 ) ∈ O × A × O and
row index (a1 , o2 , . . . , oh−1 , ah−1 ) ∈ (A × O)h−2 × A is
P [oh , oh+1 k ah−1 , ah | a1 , o2 , . . . , oh−1 ].11
3. Ff,h as a vector whose element indexed by (oh , ah , oh+1 ) ∈ O × A × O is (recall that xh = (oh , h) and rh is function
of oh+1 )

1[ah 6= πf (xh )] f (xh , ah ) − rh − f (xh+1 , πf (xh+1 )) .
First we verify that
E(f, πf 0 , h) = µ>
f 0 ,h P2|h−1 Ff,h .
To show this, first observe that µ>
f 0 ,h P2|h−1 is a row vector whose element indexed by (oh , ah , oh+1 ) is
P [oh , oh+1 k ah | a1:h−1 ∼ πf 0 ].
Multiplied by Ff,h , we further get
E[f (xh , ah ) − rh − f (xh+1 , πf (xh+1 )) | a1:h−1 ∼ πf 0 , ah ∼ πf ] = E(f, πf 0 , h).
11
PSR literature often emphasizes the intervention aspect of the actions in tests via the uses “k” symbol; mathematically they can be
treated as the conditioning operator in most cases.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Next, we explicitly construct ξh (f ) and νh (f 0 ) by factorizing P2|h−1 = P1 ×P2 , where both P1 and P2 have no dependence
on either f or f 0 . Recall that for PSRs, any history (a1 , o2 , . . . , oh−1 ) has sufficient statistics PT |a1 ,o2 ,...,oh−1 , that is a
vector of predictions over the selected core tests T conditioned on the observed history. P1 consists of row vectors of
length LK, and for the row indexed by (a1 , o2 , . . . , oh−1 , ah−1 ) the vector is

Padah−1 PT>|a1 ,o2 ,...,oh−1 ,
where Pada (·) is a function that takes a L-dimensional vector, puts it in the a-th block of a vector of length LK, and fills
the remaining entries with 0.
We construct P2 to be a matrix whose column vector indexed by (oh , ah , oh+1 ) is

Ba>(1) ,oh Ba>h ,oh+1 b∞

,
...
Ba>(K) ,oh Ba>h ,oh+1 b∞


where A = {a(1) , . . . , a(K) }. It is easy to verify that P2|h−1 = P1 × P2 by recalling the prediction rules of PSRs in
Eq. (14):
P [oh , oh+1 k ah−1 , ah | a1 , o2 , . . . , oh−1 ] = b>
∞ Bah ,oh+1 Bah−1 ,oh PT |a1 ,o2 ,...,oh−1
= PT>|a1 ,o2 ,...,oh−1 (Ba>h−1 ,oh Ba>h ,oh+1 b∞ ).
Given this factorization, we can write
E(f, πf 0 , h) = (µ>
f 0 ,h P1 ) × (P2 Ff,h ).
So we let νh (f 0 ) = P1> µf 0 ,h and ξh (f ) = P2 Ff,h . It remains to be shown that we can bound their norms. Notice that the
entries of a state vector PT |(·) are predictions of probabilities, so kP1 k∞ ≤ 1. Since µf 0 ,h is a probability vector, its dot
√
product with every column in P1 is bounded by 1, hence kνh (f 0 )k2 ≤ LK.
At last, we consider bounding the norm of P2 Ff,h . We upper bound each entry of P2 Ff,h by providing an `1 bound on
the row vectors of P2 , and then applying the Hölder’s inequality with kFf,h k∞ ≤ 2. Since we assumed that all model
parameters of the PSRs are non-negative, P2 is a non-negative matrix, and bounding the `1 norm of its row vectors is
equivalent to bounding each entry of the vector P2 1, where 1 is an all-1 vector. This vector is equal to
 P
 P


P

>
>
>
>
B
B
b
B
B
b
∞
(1)
a
,o
oh a ,oh
(ah ,oh+1 )
h h+1
(oh ,ah ,oh+1 ) a(1) ,oh ah ,oh+1 ∞



=
...
.
.
.
P2 1 = P
(16)
P
 P
 .
>
>
>
>
(oh ,ah ,oh+1 ) Ba(K) ,oh Bah ,oh+1 b∞
B (K)
B
b∞
oh

a

,oh

(ah ,oh+1 )

ah ,oh+1

Since we care about the `∞ norm of this vector, we can bound the `∞ norm of each component vector. Using the PSR
learning equations, we have
!
X
X
X
†
Bao =
PT ,ao,H PT ,H =
PT ,ao,H PT† ,H .
a,o

a,o

a,o

P
Note that for any fixed a = a(i) , every entry of o PT ,ao,H is the probability that the event t ∈ T happens after h ∈ H
happens with a one step delay in the middle, where a is intervened
in that delayed time step. Such entries are predicted
P
probabilities of events, hence
lie
in
[0,
1].
Consequently,
k
P
a,o T ,ao,H k∞ ≤ K, and we can upper bound the matrix `2
P
P
norm by Frobenius norm: k a,o PT ,ao,H k2 ≤ k a,o PT ,ao,H kF ≤ KL. Hence,




X

X
 


 



Bao  ≤ 
PT ,ao,H  · PT† ,H  ≤ KL/σmin .





2
a,o

2

a,o

2

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Using a similar argument, for any fixed a = a(i) , k
norm similarly:

P

o

Bao k2 ≤ L/σmin . We also recall the definition of b∞ and bound its



√
 > † 
PT ,H  ≤ L/σmin .
kb∞ k2 = PH
2

Finally, we have
kP2 1k∞


 
!

 X
X


>
>



B
B
b
≤ max 
∞
a,oh
ah ,oh+1

a∈A 

oh
(ah ,oh+1 )


 ∞
!

 X
X


>
>



B
b
B
≤ max 
∞
ah ,oh+1
a,oh

a∈A 

oh
(ah ,oh+1 )
2

 !


 X
X
√

 

3
Bao  kb∞ k2 ≤ KL2 L/σmin
Bao  
≤ max 
.
a∈A 



a,o
o
2

(Eq. (16))

2

So each row of P2 has `1 norm
bounded by the above expression. Applying Hölder’s
inequality we have each entry of
√
√
3
3
P2 Ff,h bounded by 2KL2 L/σmin
, hence kξh (f )k2 = kP2 Ff,h k2 ≤ 2L3 K K/σmin
. Combined with the bound on
kνh (f 0 )k2 the proposition follows.
B.5. Linear Quadratic Regulators
In this subsection we prove that Linear Quadratic Regulators (LQR) (See e.g., Anderson & Moore (2007) for a standard
reference) admit Bellman factorization with low Bellman rank. We study a finite-horizon, discrete-time LQR, governed by
the equations:
x 1 = 0 ,

xh+1 = Axh + Bah + h ,

and

>
ch = x>
h Qxh + ah ah + τh ,

2
2
where xh ∈ Rd , ah ∈ RK and the noise variables are centered with E[h >
h ] = Σ, and Eτh = σ . We operate with costs
2
ch , and the goal is to minimize the cumulative cost. We assume that all parameters A, B, Σ, Q, σ are bounded in spectral
norm by some Θ ≥ 1, that λmin (B > B) ≥ κ > 0, and that Q is strictly positive definite. Other formulations of LQR
>
replace a>
h ah in the cost with ah Rah for a positive definite matrix R, which can be accounted for by a change of variables.
Generalization to non-stationary parameters is straightforward.

This model describes an MDP with continuous state and action spaces, and the corresponding CDP has context space
Rd × [H], although we always explicitly write both parts of the context in this section. It is well known that in a discrete
time LQR, the optimal policy is a non-stationary linear policy π ? (x, h) = P?,h x (Anderson & Moore, 2007), where
P?,h ∈ RK×d is an h-dependent control matrix. Moreover, if all of the parameters are known to have spectral norm
bounded by Θ then the optimal policy has matrices with bounded spectral norm as well, as we will see in the proof.
The arguments for LQR use decoupled policy and value function classes as in Appendix A.2. We use a policy class and
value function class defined below for parameters B1 , B2 , B3 that we set in the proof.
Π = {πP~ : πP~ (x, h) = Ph x, P~ ∈

H
Y

RK×d , kPh k2 ≤ B1 }

i=1
>
~
G = {fΛ,
~ O
~ : fΛ,
~ O
~ (x, h) = x Λh x + Oh , Λ ∈

H
Y

~ ∈ RH , |Oh | ≤ B3 }
Rd×d , kΛh k2 ≤ B2 , O

i=1

The policy class consists of linear non-stationary policies, while the value functions are nonstationary quadratics with
constant offset.
Proposition 10 (Formal version of Proposition 5). Consider an LQR under the assumptions above.
Let G be a class of non-stationary quadratic value functions with offsets and let Π be a class of linear non-stationary
policies, defined above. Then, at level h, for any (π, g) pair and any roll-in policy π 0 ∈ Π, the average Bellman error can

Contextual Decision Processes with low Bellman rank are PAC-Learnable

be written as
E(g, π, π 0 , h) = hξh (π, g), νh (π 0 )i,
2

where ν, ξ ∈ Rd +1 . If Π, G are defined as above with bounds B1 , B2 , B3 and if all problem parameters have spectral
norm at most Θ, then

kξh (π, g)k22 ≤ d B2 + Θ + B12 − (Θ + ΘB1 )2 B2 + 4B32 + d2 Θ2 B22
kνh (π 0 )k22 ≤ dH+1 Θ(ΘB1 )2H + 1.
Hence, the problem admits Bellman factorization with Bellman rank at most d2 + 1 and ζ that is exponential in H but
polynomial in all other parameters. Moreover, if we set B1 , B2 , B3 as,
2



B1 = Θ /κ, B2 =

6Θ6
κ2

H


Θ, B3 =

6Θ6
κ2

H

dHΘ2 ,

then the optimal policy and value function belong to Π, G respectively.
We prove the proposition in several components. First, we study the relationship between policies and value functions,
showing that linear policies induce quadratic value functions. Then, we turn to the structure of the optimal policy, showing
that it is linear. Next, we derive bounds on the parameters B1 , B2 , B3 , which ensure that the optimal policy and value
function belong to Π, G. Lastly, we demonstrate the Bellman factorization.
The next lemma derives a relationship between linear policies and quadratic value functions.
Lemma 2. If π is a linear non-stationary policy, πh (x) = Pπ,h x, then V π (x, h) = x> Λπ,h x + Oπ,h where Λπ,h ∈ Rd×d
depends only on π and h and Oπ,h ∈ R. These parameters are defined inductively by,
>
Λπ,H = Q + Pπ,H
Pπ,H ,

Λπ,h = Q +

>
Pπ,h
Pπ,h

Oπ,H = 0

+ (A + BPπ,h )> Λπ,h+1 (A + BPπ,h )

Oπ,h = tr(Λπ,h+1 Σ) + Oπ,h+1 ,
where we recall that Σ is the covariance matrix of the h random variables.
Proof. The proof is by backward induction on h, starting from level H. Clearly,
>
V π (x, H) = x> Qx + πH (x)> πH (x) = x> Qx + x> Pπ,H
Pπ,H x , x> Λπ,H x,

so V π (·, H) is a quadratic function.
For the inductive step, consider level h and assume that for all x, V π (x, h + 1) = x> Λπ,h+1 x + Oπ,h+1 . Then, expanding
definitions
V π (x, h) = x> Qx + πh (x)> πh (x) + Ex0 ∼(x,πh (x)) V π (x0 , h + 1)


>
= x> Qx + x> Pπ,h
Pπ,h x + Ex0 ∼(x,πh (x)) (x0 )> Λπ,h+1 (x0 ) + Oπ,h+1


>
= x> Qx + x> Pπ,h
Pπ,h x + Eh (Ax + Bπh (x) + h )> Λπ,h+1 (Ax + Bπh (x) + h ) + Oπ,h+1


>
= x> Qx + x> Pπ,h
Pπ,h x + Eh (Ax + BPπ,h x + h )> Λπ,h+1 (Ax + BPπ,h x + h ) + Oπ,h+1
>
= x> Qx + x> Pπ,h
Pπ,h x + x> (A + BPπ,h )> Λπ,h+1 (A + BPπ,h )x + Eh >
h Λπ,h+1 h + Oπ,h+1
>
= x> Qx + x> Pπ,h
Pπ,h x + x> (A + BPπ,h )> Λπ,h+1 (A + BPπ,h )x + tr(Λπ,h+1 Σ) + Oπ,h+1 .

Thus, setting
>
Λπ,h = Q + Pπ,h
Pπ,h + (A + BPπ,h )> Λπ,h+1 (A + BPπ,h )

Oπ,h = tr(Λπ,h+1 Σ) + Oπ,h+1 ,
we have shown that V π (x, h) is a quadratic function of x.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

The next lemma shows that the optimal policy is linear.
Lemma 3. In an LQR, the optimal policy π ? is a non-stationary linear policy given by π ? (x, h) = P?,h x, with parameter
matrices P?,h ∈ RK×d at each level h. The optimal value function V ? is a non-stationary quadratic function given by
V ? (x, h) = x> Λ?,h x + O?,h with parameter matrix Λ?,h ∈ Rd×d and offset O?,h ∈ R. The optimal parameters are
defined recursively by,
P?,H = 0

Λ?,H = Q

O?,H = 0

>

P?,h = (I + B Λ?,h+1 B)

−1

B > Λ?,h+1 A

>
Λ?,h = Q + P?,h
P?,h + (A + BP?,h )> Λ?,h+1 (A + BP?,h )

O?,h = tr(Λ?,h+1 Σ) + O?,h+1 .
Proof. We explicitly calculate the optimal policy π? and demonstrate that it is linear. Then we instantiate these matrices in
Lemma 2 to compute the optimal value function.
For the optimal policy, we use backward induction on H. At the last level, we have,
π ? (x, H) = argmin{x> Qx + a> a} = 0.
a

Recall that we are working with costs, so the optimal policy minimizes the expected cost. Thus P?,H = 0 ∈ RK×d and
π ? (x, H) is a linear function of x.
Plugging into Lemma 2 the value function has parameters
Λ?,H = Q,

O?,H = 0.

For the induction step, assume that π ? (x, h + 1) = P?,h+1 x is linear and V ? (x, h + 1) is quadratic with parameter
Λ?,h+1  0 and O?,h+1 . We then have,
π ? (x, h) = argmin x> Qx + a> a + Ex0 ∼(x,a) V ? (x0 , h + 1)
a

= argmin x> Qx + a> a + Eh (Ax + Ba + h )> Λ?,h+1 (Ax + Ba + h ) + O?,h+1
a

= argmin a> (I + B > Λ?,h+1 B)a + 2hΛ?,h+1 Ax, Bai.
a

This follows by applying definitions and eliminating terms that are independent of a. Since R, Λ?,h+1  0 by assumption
and using the inductive hypothesis we can analytically minimize. Setting the derivative equal to zero gives,
a = (I + B > Λ?,h+1 B)−1 B > Λ?,h+1 Ax.
Thus P?,h = (I + B > Λ?,h+1 B)−1 B > Λ?,h+1 A.
As a consequence, we can now derive bounds on the policy and value function parameters. Recall that we assume that all
system parameters are bounded in spectral norm by Θ ≥ 1 and that (B > B)−1 has minimum eigenvalue at least κ.
Corollary 1. With Θ and κ defined above, we have
 6 H−h
Θ2
6Θ
kP?,h kf ≤
,
kΛ?,h k ≤
Θ,
κ
κ2


|O?,h | ≤ (H − h)

6Θ6
κ2

H−h

dΘ2 .

Proof. Again we proceed by backward induction, using Lemma 3. Clearly kP?,H kF = 0, kΛ?,H kF ≤ Θ, |O?,H | = 0.
For the inductive step we can actually compute P?,h without any assumption on Λ?,h+1 , except for the fact that it is
symmetric positive definite, which follows from Lemma 3. First, we consider just the matrix B > Λ?,h+1 A. Diagonalizing
Λ?,h+1 = U > DU where U is orthonormal and D is diagonal, gives,
B > Λ?,h+1 A = (U B)> D(U A) = (U B)> D(U B)(B > U > U B)−1 (U B)> (U A)
= (U B)> D(U B)(B > B)−1 B > A = B > Λ?,h+1 ΠB A.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Here ΠB = B(B > B)−1 B > is an orthogonal projection operator. This derivation uses the fact that since (U B)> D has
rows in the column space of U B, we can right multiply by the projector onto U B. We also use that U > U = I since U has
orthonormal rows and columns.
Thus, by the submultiplicative property of the spectral norm, we obtain
k(I + B > Λ?,h+1 B)−1 B > Λ?,h+1 Ak2 ≤ k(I + B > Λ?,h+1 B)−1 B > Λ?,h+1 Bk2 k(B > B)−1 B > Ak2
≤ k(B > B)−1 B > Ak2 ≤ Θ2 /κ.
Here κ is a lower bound on the minimum eigenvalue of B > B.
Using this bound on kP?,h k, we can now bound the optimal value function as
kΛ?,h k ≤ Θ + Θ4 /κ2 + (Θ + Θ3 /κ)2 kΛ?,h+1 k ≤ 6Θ6 /κ2 kΛ?,h+1 k.
The last bound uses the fact we apply a bound for kΛ?,h+1 k2 that is larger than one, so the last term dominates. We also
use the inequalities Θ2 /κ ≥ 1 and Θ ≥ 1. This recurrence yields

kΛ?,h k2 ≤

6Θ6
κ2

H−h
Θ.

A naive upper bound on O?,h gives,

O?,h ≤ kΛ?,h+1 k tr(Σ) + |O?,h+1 | ≤ (H − h)

6Θ6
κ2

H−h

dΘ2 .

The final component of the proposition is to demonstrate the Bellman factorization.
Proof of Proposition 10. Fix h and a value function g parametrized by matrices Λ and offset O at time h and Λ0 , O0 at time
h + 1. Also fix π which uses operator Pπ at time h.
E(π, g, π 0 , h) = Ex∼(π0 ,h) x> Λx + O − x> Qx − x> Pπ> Pπ x − Ex0 ∼(x,π(x)) (x0 )> Λ0 x0 + O0
= Ex∼(π0 ,h) x> Λx + O − x> Qx − x> Pπ> Pπ x − E (Ax + BPπ x + )> Λ0 (Ax + BPπ x + ) + O0



= tr Λ − Q − Pπ> Pπ − (A + BPπ )> Λ0 (A + BPπ ) Ex∼(π0 ,h) xx> + O − O0 − tr(ΛΣ).
Thus we may write ξh (π, g) = vec(Λ − Q − Pπ> Pπ − (A + BPπ )> Λ0 (A + BPπ )) in the first d2 coordinates and
O − O0 − tr(ΛΣ) in the last coordinate. We also write νh (π 0 ) = vec(Ex∼(π0 ,h) xx> ) in the first d2 coordinates and 1 in
the last coordinate.
The norm bound on ξ is straightforward, since all terms in its decomposition have an exponential in H bound.
For ν, since the distribution is based on applying a bounded policy π 0 at level h − 1 iteration, we can write x = Ax̃ +
BPπ0 x̃ +  where x̃ is obtained by rolling in with π 0 for h − 1 steps. If (π 0 , h − 1) denotes the distribution at the previous
level, this gives

kEx∼(π0 ,h) xx> kF ≤ kΣkF + tr (A + BP )> (A + BP )Ex̃∼(π0 ,h−1) x̃x̃>
≤ kΣkF + d(Θ + ΘB1 )2 kEx̃∼(π0 ,h−1) x̃x̃> kF .
Since at level one we have that the norm is at most kΣkF , we obtain a recurrence which produces a bound at level h of
kEx∼(π0 ,h) xx> kF ≤ kΣkF

h
X
i=1

if Θ, B1 ≥ 1, which is the regime of interest.

di−1 (Θ + ΘB1 )2(i−1) ≤ kΣkF HdH (ΘB1 )2H ,

Contextual Decision Processes with low Bellman rank are PAC-Learnable

C. Proofs of Main Results
In this section, we provide the main ideas as well as the key lemmas involved in proving Theorem 1. We also show how
the lemmas are assembled to prove the theorem. Detailed proofs of the lemmas are in Appendix D.
The proof follows an explore-or-terminate argument common to existing sample-efficient RL algorithms. We argue that
the optimistic policy chosen in Line 5 of Algorithm 1 is either approximately optimal, or visits a context distribution under
which its associated value function has a large Bellman error. This implies that using this policy for exploration leads to
learning on a new context distribution. For sample efficiency, we then need to establish that this event cannot happen too
many times. This is done by leveraging the Bellman factorization of the process and arguing that the number of times an
 sub-optimal policy is found can be no larger than Õ(M H). Combining with the number of samples collected for every
sub-optimal policy, this immediately yields the PAC learning guarantee.
C.1. Key Lemmas for Theorem 1
We begin by recalling Lemma 1.
Lemma (Restatement of Lemma 1 from main text for convenience) With Vf = E[f (x1 , πf (x1 ))], we have
Vf − V πf =

H
X

E(f, πf , h).

(17)

h=1

The structure of this lemma is similar to many existing results in RL that upper-bound the loss of following an approximate
value function greedily using the function’s Bellman errors (e.g., Singh & Yee, 1994). However, most existing results are
inequalities that use max-norm relaxations to deal with mismatch in distributions; hence, they are likely to be loose. This
lemma, on the other hand, is an equality, thanks to the fact that we are comparing V πf to Vf , not V ? . As the remaining
analysis shows, this simple equation allows us to relate policy loss (the LHS) with the average Bellman error (the RHS)
that we use to drive exploration. In particular, this lemma implies an explore-or-terminate behavior for the algorithm.
Lemma 4 (Optimism drives exploration). Suppose the estimates V̂f and Ẽ(ft , πt , h) in Line 2 and 7 always satisfy

(18)
|V̂f − Vf | ≤ /8,
and
|Ẽ(ft , πt , h) − E(ft , πt , h)| ≤
8H
throughout the execution of the algorithm. Assume further that f ? is never eliminated. Then in any iteration t, one of the
following two statements holds:
(i) the algorithm does not terminate and
E(ft , πt , ht ) ≥


,
2H

(19)

(ii) the algorithm terminates and the output policy πt satisfies V πt ≥ VF? − .
The lemma guarantees that the policy πt used at iteration t in O LIVE has sufficiently large Bellman error on at least one of
the levels, provided that the two conditions in Equation (18) are met. These conditions require that (1) we have reasonably
accurate value function estimates from Line 1, and (2) we collect enough samples in Line 6 to form reliable Bellman error
estimates under ft at each level h. The result of Theorem 1 can then be obtained using two further ingredients. First,
we need to make sure that the first case in Lemma 4 does not happen too many times. Second, we need to collect enough
samples in Lines 1 and 6 to ensure the preconditions in Equation (18). We first establish a bound on the number of iterations
using the Bellman rank of the problem, before moving on to sample complexity questions.
b πt , ht ) in Line 13 always satisfies
Lemma 5 (Iteration complexity). If E(f,
b πt , ht ) − E(f, πt , ht )| ≤ φ
|E(f,

(20)

throughout the execution of the algorithm (φ is the threshold in the elimination criterion), then f ? is never eliminated.
Furthermore, for any particular level h, if whenever ht = h we have
√
(21)
|E(ft , πt , ht )| ≥ 6 M φ,
 
ζ
then the number of iterations that ht = h is at most M log 2φ
/ log 53 .

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Precondition (20) simply posits that we collect enough samples for reliable Bellman error estimation in Line 12. Intuitively,
since f ? has no Bellman error, this is sufficient to ensure that it is never eliminated. Precondition (21) is naturally satisfied
by the exploration policies πt given Lemma 4, when φ is chosen appropriately according to , M , and H. Given this, the
above lemma bounds the number of iterations at which we can find a large Bellman error at any particular level.
The intuition behind this claim is most clear in the POMDP setting of Proposition 3. In this case, νh (f 0 ) in Definition 5
corresponds to the distribution over hidden states induced by πf 0 at level h. At iteration t, the exploration policy πft induces
such a hidden-state distribution p = νh (ft ) at the chosen level h = ht , which results in the elimination of all functions
that have large Bellman error on p. Thanks to the Bellman factorization, this corresponds to the elimination of all f with a
large |p> ξh (f )|, where ξh (f ) is also defined in Definition 5. In this case, it can be easily shown that ξh (f ) ∈ [−2, 2]M , so
the space of all such vectors {ξh (f ) : f ∈ F } at each level h is originally contained in an `∞ ball in M dimensions with
radius 2, and, whenever ht = h, we intersect this set with two parallel halfspaces. Via a geometric argument adapted from
Todd (1982), we show that each such intersection reduces the volume of the space by a multiplicative factor of 3/5. We
also show that the volume is bounded from below, hence volume reduction cannot occur indefinitely. Together, these two
facts lead to the iteration complexity in Lemma 5. The mathematical techniques used here are analogous to the analysis of
the Ellipsoid method in linear programming (see e.g. Bland et al. (1981)).
Finally, we need to ensure that the number of samples collected in each of Lines 1, 6, and 12 of O LIVE can be upper
bounded, which yields the overall PAC learning result in Theorem 1. The next three lemmas present precisely the deviation
bounds required for this argument. The first two follow from simple applications of Hoeffding’s inequality.
Lemma 6 (Deviation bound for V̂f ). With probability at least 1 − δ,
r
1
2N
log
|V̂f − Vf | ≤
2nest
δ
32
2

holds for all f ∈ F simultaneously. Hence, we can set nest ≥

log

2N
δ

to guarantee that |V̂f − Vf | ≤ /8.

This controls the number of samples required in Line 1.
Lemma 7 (Deviation bound for Ẽ(ft , πt , h)). For any fixed ft , with probability at least 1 − δ,
r
1
2H
b
|E(ft , πt , h) − E(ft , πt , h)| ≤ 3
log
2neval
δ
288H 2
2

holds for all h ∈ [H] simultaneously. Hence, we can set neval ≥

.
E(ft , πt , h)| ≤ 8H

log

2H
δ

to guarantee that |Ẽ(ft , πt , h) −

This lemma can be seen as the sample complexity at each iteration in Line 6. Note that no union bound over F is needed
here, since Line 6 only estimates the average Bellman error for a single function, which is fixed before data is collected.
Finally, we bound the sample complexity of the learning step.
b πt , ht )). For any fixed πt and ht , with probability at least 1 − δ,
Lemma 8 (Deviation bound for E(f,
s
b πt , ht ) − E(f, πt , ht )| ≤
|E(f,
holds for all f ∈ F simultaneously. Hence, we can set n ≥
as long as φ ≤ 4.

32K
φ2

8K log
n

log

2N
δ

2N
δ

+

2K log
n

2N
δ

b πt , ht ) − E(f, πt , ht )| ≤ φ
to guarantee that |E(f,

This lemma uses Bernstein’s inequality to exploit the small variance of the importance weighted estimates.
C.2. Proof of Theorem 1
Suppose the preconditions of Lemma 4 (Eq. (18)) and Lemma 5 (Eq. (20)) hold; we show them via concentration inequalities later. Applying Lemma 4, in every iteration t before the algorithm terminates,
E(ft , πt , ht ) ≥

√

= 6 M φ,
2H

Contextual Decision Processes with low Bellman rank are PAC-Learnable

due to the
of φ. For level h = ht , Eq. (21) is satisfied. According to Lemma 5, the event ht = h can happen at most
 choice

ζ
M log 2φ
/ log 53 times for every h ∈ [H]. Hence, the total number of iterations in the algorithm is at most

HM log

ζ
2φ



5
/ log = HM log
3

!
√
5
6H M ζ
/ log .

3

Now we are ready to apply the concentration inequalities to show that Eq. (18) and (20) hold with high probability. We
split the total failure probability δ among the following estimation events:
1. Estimation of V̂f (Lemma 6; only once): δ/3.


 √ 
2. Estimation of Ẽ(ft , πt , h) (Lemma 7; every iteration): δ/ 3HM log 6H  M ζ / log 35 .
b πt , ht ) (Lemma 8; every iteration): same as above.
3. Estimation of E(f,
Since these events happen in a particular sequence, the proof actually bounds the probability of these failure events conditioned on all previous events succeeding. This imposes no technical challenge as fresh data is collected for every event, so
it effectively reduces to a standard union bound.
Applying Lemmas 6, 7, and 8 with the above failure probabilities, we can verify that the choices of nest , neval , and n in the
algorithm statement satisfy the preconditions of Lemmas 4 and 5. Finally, we upper bound the total number of episodes as
!
!
√
√
6H M ζ
5
6H M ζ
5
nest + neval · HM log
/ log + n · HM log
/ log

3

3


 2 3

3
2 3
log(N/δ) M H
M H K
M H K
= Õ
+
log(ζ/δ)
+
log(N
ζ/δ)
=
Õ
log(N
ζ/δ)
.
2
2
2
2

D. Auxiliary Proofs of the Main Lemmas
In this appendix we give the full proofs of the lemmas sketched in Appendix C. Note that O LIVER (Algorithm 3) with
parameters θ = 0 and η = 0 is precisely O LIVE, and the two analyses are identical. To avoid repetition, in this appendix
we analyze O LIVER (Algorithm 3) and prove the versions of the lemmas that can be used for Theorem 4. Readers can easily
recover the detailed proofs of the lemmas in Appendix C for O LIVE by letting θ = 0, η = 0, 0 = , fθ? = f ? , VF?,θ = VF? .
To facilitate understanding we break up the proofs into 3 parts. The main proofs appear in D.1, and two types of technical
lemmas are invoked from there: (1) a series of lemmas that adapt the work of Todd (1982) for the purpose, which are given
in D.2; (2) deviation bounds, which are given in D.3.
D.1. Main Proofs
Proof of Lemma 1. Recall from Definition 2 that the average Bellman errors are defined as



E(f, π, h) = E f (xh , ah ) − rh − f (xh+1 , ah+1 )  a1:h−1 ∼ π, ah:h+1 ∼ πf .
Expanding RHS of Eq. (17), we get
H
X
h=1




E f (xh , ah ) − rh − f (xh+1 , ah+1 )  a1:h−1 ∼ πf , ah:h+1 ∼ πf .

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Since all H expected values share the same distribution over trajectories, which is the one induced by a1:H ∼ πf , the
above expression is equal to
H
X




E f (xh , ah ) − rh − f (xh+1 , ah+1 )  a1:H ∼ πf

h=1

"
=E

H 
X


f (xh , ah ) − rh − f (xh+1 , ah+1 )  a1:H ∼ πf

#

h=1



 

= E f (x1 , πf (x1 )) − E rh  a1:H ∼ πf = Vf − V πf .
Lemma 9 (Optimism drives exploration, analog of Lemma 4). If the estimates V̂f and Ẽ(ft , πt , h) in Line 3 and 8 of
Algorithm 3 always satisfy
|V̂f − Vf | ≤ 0 /8,

|Ẽ(ft , πt , h) − E(ft , πt , h)| ≤

0
8H

(22)

throughout the execution of the algorithm (recall that 0 is defined on Line 1), and fθ? is never eliminated, then in any
iteration t, either the algorithm does not terminate and
E(ft , πt , ht ) ≥

0
,
2H

(23)

or the algorithm terminates and the output policy πt satisfies V πt ≥ VF?,θ − 0 − Hθ.
Proof. Eq. (23) follows directly from the termination criterion and Eq. (22). Suppose the algorithm terminates in iteration
t. Let fmax := argmaxf ∈Ft−1 Vf , and we have
V

πt

= Vft −

H
X

E(ft , πt , h)

(Lemma 1)

h=1

≥ V̂ft −

H
X

Ẽ(ft , πt , h) − 0 /4

(Eq. (22))

h=1
0

≥ V̂ft − 7 /8

(termination criterion)

0

≥ V̂fmax − 7 /8
0

≥ Vfmax −  ≥ Vfθ? − 

(ft is the maximizer of V̂f )
0

≥ VF?,θ − Hθ − 0 .

(fθ? is not eliminated)
(Lemma 1)

The last inequality uses Lemma 1 on Vfθ? and the definition of VF?,θ , which is the value of policy πfθ? . Lemma 1 relates
these two quantities to the average Bellman errors, which are upper bounded by θ at each level since fθ? is θ-valid.
b πt , ht ) in Eq. (10) always satisfies
Lemma 10 (Volumetric argument, analog of Lemma 5). If E(f,
b πt , ht ) − E(f, πt , ht )| ≤ φ
|E(f,

(24)

throughout the execution of the algorithm (φ is the threshold in the elimination criterion), then fθ? is never eliminated.
Furthermore, for any particular level h, if whenever ht = h, we have
√
(25)
|E(ft , πt , ht )| ≥ 3 M (2φ + θ + η) + η, ,
then the number of iterations that ht = h is at most
M log

ζ
5
/ log .
2φ
3

(26)

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof. The first claim that fθ? is never eliminated follows directly from the fact |E(fθ? , πt , ht )| ≤ θ (Definition 8), Eq. (24),
and the elimination threshold φ + θ. Below we prove the second claim.
For any particular level h, suppose i1 < · · · < iτ < · · · < iTh are the iteration indices with ht = h, {t : ht = h} ordered
from first to last, and Th = |{t : ht = h}|. For convenience define i0 = 0. The goal is to prove an upper bound on Th .
Define notations:
• p1 , . . . , pTh . pτ := νh (fiτ ) where νh (·) is given in Definition 10. Recall that fiτ is the optimistic function used for
exploration in iteration t = iτ .
• U(Fi0 ), U(Fi1 ), . . . , U(FiTh ). U(Fiτ ) = {ξh (f ) : f ∈ Fiτ } where ξh (f ) ∈ RM is given in Definition 10.
• Ψ = supf ∈F kνh (f )k2 , and Φ = supf ∈F kξh (f )k2 . By Definition 10, Ψ · Φ ≤ ζ.
• V0 , V1 , . . . , VTh . V0 := {v : kvk2 ≤ Φ}, and Vτ := {v ∈ Vτ −1 : |p>
τ v| ≤ 2φ + θ + η}.
• B0 , B1 , . . . , BTh . Bτ is a minimum volume enclosing ellipsoid (MVEE) of Vτ .
For every τ = 0, . . . , Th , we first show that U(Fiτ ) ⊆ Vτ . When τ = 0 this is obvious. For τ ≥ 1, we have ∀f ∈ Fiτ ,
|E(f, πfiτ , h)| ≤ 2φ + θ.
by the elimination criterion and Eq. (24). By Definition 10, this implies that, ∀v ∈ U(Fiτ ),
|p>
τ v| ≤ 2φ + θ + η,
so U(Fiτ ) ⊆ Vτ .

√
Next we show that ∃v ∈ Vτ −1 such that |p>
τ v| ≥ 3 M (2φ + θ + η). In fact, Eq. (25) and the fact that fit was chosen
(implying that it survived) implies that this v can be chosen as
v = ξh (fiτ ) ∈ U(Fiτ −1 ) ⊆ U(Fiτ −1 ) ⊆ Vτ −1 .
(The first “⊆” follows from the fact that Ft shrinks monotonically in Algorithm 3, since the learning steps between
t = iτ −1 + 1 and t = iτ − 1 on other levels can only eliminate functions.) We verify that this v satisfies the desired
property, given by Definition 10 and Eq. (25):
√
|p>
τ v| = |hνh (fiτ ), ξh (fiτ )i| ≥ |E(fiτ , πiτ , h)| − η ≥ 3 M (2φ + θ + η).
Observing that Vt is centrally symmetric and consequently√so is Bt (Todd & Yıldırım, 2007), we apply Lemma 11 and
Fact 4 with the variables set to d := M, B := Bτ −1 , κ := 3 M (2φ + θ + η), γ := 2φ + θ + η. We obtain that
vol(B+ )
≤ 0.6,
vol(Bt−1 )
>
where B+ is the MVEE of Vτ0 := {v ∈ Bτ −1 : |p>
τ v| ≤ 2φ + θ + η}. Note that Vτ = {v ∈ Vτ −1 : |pτ v| ≤
0
0
2φ + θ + η} ⊆ Vτ given that Vτ −1 ⊆ Bτ −1 . Since B+ is an enclosing ellipsoid of Vτ , and Bτ is the MVEE of Vτ , we
have vol(Bτ ) ≤ vol(B+ ). Altogether we claim that

vol(Bτ )
≤ 0.6.
vol(Bτ −1 )
This result shows that the volume of Bτ shrinks exponentially with τ . To prove that Th is small, it suffices to show that the
volume of B0 is not too large, and that of BTh is not too small. Let cM be the volume of Euclidean sphere with unit radius
in RM . By definition, vol(B0 ) = cM (Φ)M .

Contextual Decision Processes with low Bellman rank are PAC-Learnable

For vol(BTh ), since kpτ k2 ≤ Ψ always holds, we can guarantee that




\
VT ⊇ q ∈ RM :
|hp, qi| ≤ 2φ + θ + η


p∈RM :kpk2 ≤Ψ

	
⊇ q ∈ RM : kqk2 ≤ (2φ + θ + η)/Ψ

	
⊇ q ∈ RM : kqk2 ≤ 2φ/Ψ .

(Hölder’s inequality)

M

Hence, vol(BTh ) ≥ cM (2φ/Ψ) , and
T

h
vol(BTh ) Y
vol(Bt )
cM (2φ/Ψ)M
≤
=
≤ 0.6Th .
M
cM (Φ)
vol(B0 )
vol(B
)
t−1
t=1

Algebraic manipulations give

M log

ΨΦ
2φ



5
≥ Th log .
3

The second claim of the lemma statement follows by recalling that ΨΦ ≤ ζ.
D.2. Lemmas for the Volumetric Argument
We adapt the work of Todd (1982) to derive lemmas that we use in D.1. The main result of this section is Lemma 11. As
this section focuses on generic geometric results, we adopt notation more standard for these arguments unlike the notation
used in the rest of the paper.
Theorem 5 (Theorem 2 of Todd (1982)). Define E = {w ∈ Rd : w> w ≤ 1} and Eβ = {w ∈ E : |e>
1 w| ≤ β} for
0 < β ≤ d−1/2 . The ellipsoid,
−1
E+ = {w ∈ Rd | w> (ρ(I − σe1 e>
w ≤ 1},
1 ))

(27)

is a minimum volume enclosing ellipsoid (MVEE) for Eβ if
σ=

1 − dβ 2
1 − β2

and

ρ=

d(1 − β 2 )
.
d−1

Fact 3. With E, E+ , σ, ρ as in Theorem 5, we have
Vol(E+ ) √
= dβ
Vol(E)



d
d−1

(d−1)/2

1 − β2

(d−1)/2

.

(28)

d
> −1
Proof. For convenience, let us define G = ρ(I − σe1 e>
w ≤ 1}. Notice that E can be
1 ) so that E+ = {w ∈ R : w G
−1/2
obtained from E+ by the affine transformation v = G
w, which means that if w ∈ E+ then v = G−1/2 w ∈ E. Via
change of variables this implies that

Vol(E+ )
= det(G1/2 ).
Vol(E)
The determinant is simply the product of the eigenvalues, which is easy to calculate since G is diagonal,
det(G1/2 ) = ρ(d−1)/2 (ρ(1 − σ))1/2 .
Plugging in the definitions of ρ, σ from Theorem 5 proves the statement.
Lemma 11. Consider a closed and bounded set V ⊂ Rd and a vector p ∈ Rd . Let B be any enclosing ellipsoid of V that
is centered at the origin, and we abuse the same symbol for the symmetric positive definite matrix that defines the ellipsoid,
>
i.e., B = {v ∈ Rd : v > B −1 v ≤ 1}. Suppose there exists
√v ∈ V with |p v| ≥ κ and define B+ as the minimum volume
>
enclosing ellipsoid of {v ∈ B : |p v| ≤ γ}. If γ/κ ≤ 1/ d, we have

(d−1)/2 
(d−1)/2
vol(B+ ) √ γ
d
γ2
≤ d
1− 2
.
(29)
vol(B)
κ d−1
κ

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof. The first claim is to prove a bound on p> Bp.
κ ≤ |p> v| = |p> B 1/2 B −1/2 v| ≤

p
p
√
p> Bp v > B −1 v ≤ p> Bp.

The last inequality applies since v ∈ T
B so that v > B −1 v ≤ 1. Now we proceed to work with the ellipsoids, let L = {v :
>
|v p| ≤ γ}. Set B+ = M V EE(B L). We apply two translations of the coordinate system so that B gets mapped to
the unit ball and so that p gets mapped to αe1 (i.e. a scaled multiple of the first standard basis vector). The first translation
is done by setting w = B −1/2 v where w is in the new coordinate system and v is in the old coordinate system. Let
p1 = B 1/2 p so that we can equivalently write L = {w : |w> p1 | ≤ γ}. The second translation maps p1 to αe1 via a
rotation matrix R such that RB 1/2 p = Rp1 = αe1 . We also translate w to Rw but this doesn’t affect the now spherically
symmetric ellipsoid, so we do not change the variable names.
T
To summarize, after applying the scaling and the rotation, we are interested in M V EE(I {w : |w> e1 | ≤ γ/α}) and
specifically, since volume ratios are invariant under affine transformation, we have
T
Vol(M V EE(I {w : |w> e1 | ≤ γ/α}))
Vol(B+ )
=
.
Vol(B)
Vol(I)
Here I is the unit ball (i.e. the ellipsoid with identity matrix). Further applying Fact 3, we obtain

(d−1)/2 
(d−1)/2
Vol(B+ ) √ γ
d
γ2
= d
1− 2
.
Vol(B)
α d−1
α
It remains to lower bound α, which is immediate since
α = kRB 1/2 pk2 = kB 1/2 pk2 ≥ κ.
Substituting this lower bound on α completes the proof.
Fact 4. When γ/κ =

1
√
,
3 d

the RHS of Eq. (29) is less than 0.6.

Proof. Plugging in the numbers, we have the RHS of Eq. (29) equal to

(d−1)/2

9(d−1)/8 · 4/9
1
d 9d − 1
1
8
1
=
1+
≤ exp(4/9) ≤ 0.52.
3 d − 1 9d
3
9(d − 1)
3
Here we used the fact that (1 + x1 )x is monotonically increasing towards e on x ∈ [1, ∞).
D.3. Deviation Bounds
In this section we prove the deviation bounds. Note that the statement of the lemmas in this section, which are for O LIVER,
coincide with those stated in Appendix C for O LIVE. This is not surprising as the two algorithms draw data and estimate
quantities in the same way.
Lemma 12 (Deviation Bound for V̂f ). With probability at least 1 − δ,
r
1
2N
log
|V̂f − Vf | ≤
2nest
δ
holds for all f ∈ F simultaneously. Hence, we can set nest ≥

32
2

log

2N
δ

to guarantee that |V̂f − Vf | ≤ /8.

Proof. The bound follows from a straight-forward application of Hoeffding’s inequality and the union bound, and we only
need to verify that the Vf is the expected value of the V̂f , and the range of the random variables is [0, 1].
Lemma 13 (Deviation Bound for Ẽ(ft , πt , h)). For any fixed ft , with probability at least 1 − δ,
r
1
2H
|Ẽ(ft , πt , h) − E(ft , πt , h)| ≤ 3
log
2neval
δ
holds for all h ∈ [H] simultaneously. Hence, for any neval ≥

|Ẽ(ft , πt , h) − E(ft , πt , h)| ≤ 8H
.

288H 2
2

log

2H
δ ,

with probability at least 1 − δ we have

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof. This bound is another straight-forward application of Hoeffding’s inequality and the union bound, except that the
random variables that go into the average have range [−1, 2], and we have to realize that Ẽ(ft , πt , h) is an unbiased estimate
of E(ft , πt , h).
b πt , ht )). For any fixed πt and ht , with probability at least 1 − δ,
Lemma 14 (Deviation Bound for E(f,
s
b πt , ht ) − E(f, πt , ht )| ≤
|E(f,
holds for all f ∈ F simultaneously. Hence, for any n ≥
b πt , ht ) − E(f, πt , ht )| ≤ φ.
|E(f,

32K
φ2

8K log
n

log

2N
δ

2N
δ

+

2K log
n

2N
δ

and φ ≤ 4, with probability at least 1 − δ we have

b πt , ht ) is an average of i.i.d. random variables with mean E(f, πt , ht ). We use µ as a
Proof. We first show that E(f,
shorthand for the distribution over trajectories induced by a1 , . . . , aht −1 ∼ πt , ah ∼ unif(A), which is the distribub πt , ht ). On the other hand, let µ0 denote the distribution over trajectories induced by
tion of data used to estimate E(f,
a1 , . . . , aht −1 ∼ πt , ah ∼ πf . The importance weight used in Eq. (10) essentially converts the distribution from µ to µ0 ,
b πt , ht ) can be written as
hence the expected value of E(f,
Eµ [K1[ah = πf (xh )] (f (xh , ah ) − rh − f (xh+1 , πf (xh+1 )))]
= Eµ0 [f (xh , ah ) − rh − f (xh+1 , πf (xh+1 ))] = E(f, πt , ht ).
Now, we apply Bernstein’s inequality. We first analyze the 2nd-moment of the random variable.
y(xh , ah , rh , xh+1 ) = f (xh , ah ) − rh − f (xh+1 , πf (xh+1 )) ∈ [−2, 1], the 2nd-moment is

Defining

h
i
2
Eµ (K1[ah = πf (xh )]y(xh , ah , rh , xh+1 ))
h
i
2 
= Pr[ah = πf (xh )] · Eµ (Ky(xh , ah , rh , xh+1 ))  ah = πf (xh ) + Pr[ah 6= πf (xh )] · 0
µ

≤

µ

1
Eµ
K





K 2 · 4  ah = πf (xh ) = 4K.

Next we check the range of the centered random variable. The uncentered variable lies in [−2K, K], and the expected
value is in [−2, 1], so the centered variable lies in [−2K − 1, K + 2] ⊆ [−3K, 3K]. Applying Bernstein’s inequality, we
have with probability at least 1 − δ,
s
2N
2N
b πt , ht ) − E(f, πt , ht )| ≤ 2 Var [K1[ah = πf (xh )]y(xh , ah , rh , xh+1 )] log δ + 6K log δ
|E(f,
n
3n
s
8K log 2N
2K log 2N
δ
δ
≤
+
.
(variance is bounded by 2nd-moment)
n
n
As long as

2K log
n

2N
δ

≤ 1, the above is bounded by 2

for n, which indeed guarantees that

2K log
n

2N
δ

q

8K log
n

2N
δ

q
8K log 2N
δ
. The choice of n follows from solving 2
=φ
n

≤ 1 as φ ≤ 4.

E. Proofs of Extensions
E.1. Proof for Unknown Bellman Rank (Theorem 2)
Since we assign δ 0 =

δ
i(i+1)

failure probability to the i-th call of Algorithm 2, the total failure probability is at most
∞
X
i=1


∞ 
X
δ
1
1
=δ
−
= δ.
i(i + 1)
i
i+1
i=1

Contextual Decision Processes with low Bellman rank are PAC-Learnable

So with probability at least 1 − δ, all high probability events in the analysis of O LIVE occur for every i = 1, 2, . . .. Note
that regardless of whether M 0 < M , we never eliminate f ? according to Lemma 5. Hence Lemma 4 holds and whenever
the algorithm returns a policy it is near-optimal.
While the algorithm returns a near-optimal policy if it terminates, we still must prove that the algorithm terminates. Since
when M 0 < M Eq. (21) and Lemma 10 do not apply, we cannot naively use arguments from the analysis of O LIVE.
However, we monitor the number of iterations that have passed in each execution to O LIVE and stop the subroutine when
the actual number of iterations exceeds the iteration complexity bound (Lemma 5) to prevent wasting more samples on the
wrong M 0 .
O LIVE is guaranteed to terminate within the sample complexity bound and output near-optimal policy when M ≤ M 0 .
Since M 0 grows on a doubling schedule, for the first M 0 that satisfies M ≤ M 0 , we have M 0 ≤ 2M and i ≤ log2 M + 1.
Hence, the total number of calls is bounded by log2 M + 1.
Finally, since the sample complexity bound in Theorem 1 is monotonically increasing in M and 1/δ and the schedule for
δ 0 is decreasing, we can bound the total sample complexity by that of the last call to O LIVE multiplied by the number of
2 M +1)
≤ (log2 M +2)(log
, so the sample complexity bound is
calls. The last call to O LIVE has M 0 ≤ 2M , and δ 0 = i(i+1)
δ
δ
only affected by factors that are at most logarithmic in the relevant parameters.
E.2. Proofs for Infinite Hypothesis Classes
In this section we prove sample complexity guarantee for using infinite hypothesis classes in Appendix A.3. Recall that
we are working with separated policy class Π and V-value function class G, and when running O LIVE any occurrence of
f ∈ F is replaced appropriately by (π, g) ∈ Π × G. For clarity, we use (π, g) instead of f in the derivations in this section.
We assume that the two function classes have finite Natarajan dimension and pseudo dimension respectively.
The key technical step for the sample complexity guarantee is to establish the necessary deviation bounds for infinite
classes. Among these deviation bounds, the bound on Ẽ((πt , gt ), πt , h) (Lemma 7) does not involve union bound over F,
so it can be reused without modification. The other two bounds need to be replaced by Lemma 15 and 16, stated below.
With these lemmas, Theorem 3 immediately follows simply by replacing the deviation bounds.
Definition 11. Define dΠ = max(Ndim(Π), 6), dG = max(Pdim(G), 6), and d = dΠ + dG .
Lemma 15. If

nest

8192
≥ 2




1
128e
+ log(8e(dG + 1)) + log
dG log

δ


,

(30)

then with probability at least 1 − δ, |V̂(π,g) − V(π,g) | ≤ /8, ∀(π, g) ∈ Π × G.
We remark that both the estimate V̂(π,g) and population quantity V(π,g) are independent of π in the separable case, and
hence the sample complexity is independent of dΠ .
Lemma 16. If
1152K 2
n≥
φ2





3
48eK
+ log 8e(6d log (2eKd) + 1) + log
6d log (2eKd) log
φ
δ

then for any fixed πt and ht , with probability at least 1 − δ,

b
|E((π,
g), πt , ht ) − E((π, g), πt , ht )| ≤ φ, ∀(π, g) ∈ Π × G.


,

(31)

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof of Theorem 3. Set the algorithm parameters to:

√ ,
12H M



128e
3
+ log(8e(dG + 1)) + log

δ
!
√
2
2
12H M log(6H M ζ/)
288H
log
,
neval =
2
δ




1152K 2 
48eK
n=
6d
log
2eK
log
+
log
8e(6d
log
(2eKd)
+
1)
φ2
φ
√
18HM log(6H M ζ/) 
.
+ log
δ
φ=

nest =

8192
2

dG log


,

The rest of the proof is essentially the same as the proof of Theorem 1, and the sample complexity follows by noticing that
) and n = Õ(K 2 (dΠ + dG + log(1/δ))/φ2 ).
nest = Õ( dG +log(1/δ)
2
Lemma 15 is a straight-forward application of Corollary 2 introduced in E.2.1 and are not proved separately. In the
remainder of this section we prove Lemma 16. Before that, we review some standard definitions and results from statistical
learning theory.
E.2.1. D EFINITIONS AND BASIC L EMMAS
Notations X , x, n, d, ξ in this section are used according to conventions in the literature and may not share semantics with
the same symbols used elsewhere in this paper.
Definition 12 (VC-Dimension). Given hypothesis class H ⊂ X → {0, 1}, its VC-dimension VC-dim(H) is defined as the
maximal cardinality of a set X = {x1 , . . . , x|X| } ⊂ X that satisfies |HX | = 2|X| (or X is shattered by H), where HX is
the restriction of H to X, namely {(h(x1 ), . . . , h(x|X| )) : h ∈ H}.
Lemma 17 (Sauer’s Lemma). Given hypothesis class H ⊂ X → {0, 1} with d = VC-dim(H) < ∞, we have ∀X =
(x1 , . . . , xn ) ∈ X n ,
|HX | ≤ (n + 1)d .
Lemma 18 (Sauer’s Lemma for Natarajan dimension (Ben-David et al., 1992; Haussler & Long, 1995)). Given hypothesis
class H ⊂ X → Y with Ndim(H) ≤ d, we have ∀X = (x1 , . . . , xn ) ∈ X n ,

|HX | ≤

ne(K + 1)2
2d

d
,

where K = |Y|.
Definition 13 (Covering number). Given hypothesis class H ⊂ X → R,  > 0, X = (x1 , . . . , xn ) ∈ X n , the covering
n
number N1 (α, H, X) is defined
Pnas the minimal cardinality of a set C ⊂ R , such that for any h ∈ H there exists
1
c = (c1 , . . . , cn ) ∈ C where n i=1 |h(xi ) − ci | ≤ α.
Lemma 19 (Bounding covering number by pseudo dimension (Haussler, 1995)). Given hypothesis class H ⊂ X → R
with Pdim(H) ≤ d, we have for any X ∈ X n ,

N1 (α, H, X) ≤ e(d + 1)

2e
α

d
.

Lemma 20 (Uniform deviation bound using covering number (Pollard, 2012); also see Devroye et al. (1996), Theorem
29.1). Let H ⊂ X → [0, b] be a hypothesis class, and (x1 , . . . , xn ) be i.i.d. samples drawn from some distribution
supported on X . For any α > 0,


(
)


n
1 X



nα2


Pr sup 
h(xi ) − E[h(x1 )] > α ≤ 8 E N1 α/8, H, (x1 , . . . , xn ) exp −
.

128b2
h∈H  n
i=1

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Corollary 2 (Uniform deviation bound using pseudo dimension). Suppose Pdim(H) ≤ d, then


(
)



d
n
1 X

nα2
16e


exp −
.
Pr sup 
h(xi ) − E[h(x1 )] > α ≤ 8e(d + 1)

α
128b2
h∈H  n
i=1

To guarantee that this probability is upper bounded by δ, it suffices to have


128
16e
1
n ≥ 2 d log
+ log(8e(d + 1)) + log
.
α
α
δ
E.2.2. P ROOF OF L EMMA 16
b
The idea is to establish deviation bounds for each of the three terms in the definition of E((π,
g), πt , ht ) (Line 13). Each
term takes the form of an importance weight multiplied by a real-valued function, and we first show that the function
space formed by these products has bounded pseudo dimension. We state this supporting lemma in terms of an arbitrary
value-function class V which might operate on an input space X 0 different from the context space X . In the sequel, we
instantiate V and X 0 in the the lemma with specific choices to prove the desired results.
Lemma 21. Let Y be a label space with |Y| = K, let Π ⊆ X → Y be a function class with Natarajan dimension at most
dΠ ∈ [6, ∞), and let V ⊆ X 0 → [0, 1] be a class with pseudo dimension at most dV ∈ [6, ∞). The hypothesis class H =
{(x, a, x0 ) 7→ 1[a = π(x)]g(x0 ) : π ∈ Π, g ∈ V} has pseudo dimension Pdim(H) ≤ 6(dΠ + dV ) log (2eK(dΠ + dV )).
Proof. Recall that Pdim(H) = VC-dim(H+ ), so it suffices to show that for any
+
X = {(x1 , a1 , x01 , ξ1 ), . . . , (xd , ad , x0d , ξd )} ∈ (X ×A×X 0 ×R)d where d = 6(dΠ +dV ) log (2eK(dΠ + dV )), |HX
| < 2d .
Note that since g(x) ∈ [0, 1] for all g, x
h
i
H+ = {(x, a, x0 , ξ) 7→ 1 1[a = π(x)]g(x0 ) > ξ }
= {(x, a, x0 , ξ) 7→ 1[ξ < 0] + 1[ξ ≥ 0] · 1[a = π(x)] · 1[g(x0 ) > ξ]}
For points where ξi < 0, all hypotheses in H+ produce label 1, so without loss of generality we can assume that ξi ≥
0, i = 1, . . . , d.
With a slight abuse of notation, let ΠX denote the restriction of Π to the set of contexts {x1 , . . . , xd } (actions and future
+
denote the restriction of V + to
contexts (a1 , x01 ), . . . , (ad , x0d ) are ignored since Π does not operate on them), and VX
+
+
0
0
{(x1 , ξ1 ), . . . , (xd , ξd )}. HX can be produced by the Cartesian product of ΠX and VX as follows:
+
+
HX
= {(1[a1 = α1 ]β1 , . . . , 1[ad = αd ]βd ) : (α1 , . . . , αd ) ∈ ΠX , (β1 , . . . , βd ) ∈ VX
}.
+
+
Therefore, |HX
| ≤ |ΠX | |VX
|. Recall that Ndim(Π) ≤ dΠ and VC-dim(V + ) = Pdim(V) ≤ dV . Applying Lemma 18 and
17:
d

de(K + 1)2 Π
+
(d + 1)dV .
|HX
|≤
2dΠ

The logarithm of the RHS is


de(K + 1)2
+ dV log(d + 1) < dΠ log(de(K + 1)2 ) + dV log(d + 1)
dΠ log
2dΠ
≤ dΠ log d + 2dΠ log(2eK) + dV log(d + 1) ≤ 2(dΠ + dV ) log(2eK) + (dΠ + dV ) log(2d).
It remains to be shown that this is less than log(2d ) = d log 2. Note that
d log 2 > 3(dΠ + dV )(log(2eK) + log(dΠ + dV )),
so we only need to show that (dΠ + dV ) log(2d) ≤ (dΠ + dV ) log(2eK) + 3(dΠ + dV ) log(dΠ + dV ). Now


(dΠ + dV ) log(2d) = (dΠ + dV ) log(12(dΠ + dV )) + log log(2eK(dΠ + dV ))


≤ 2(dΠ + dV ) log(dΠ + dV ) + (dΠ + dV ) log log(2eK) + log(dΠ + dV )
(dΠ + dV ≥ 12)

≤ 2(dΠ + dV ) log(dΠ + dV ) + (dΠ + dV ) log(2eK) + log(dΠ + dV ) .

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Proof of Lemma 16. Recall that when we are given a policy class Π and separate V-value function class G, for every
π ∈ Π, g ∈ G, we instead estimate average Bellman error with
(i)
(i)
n

1 X 1[aht = π(xht )]  (i)
(i)
(i)
b
g(xht ) − rht − g(xht +1 ) .
E((π, g), πt , ht ) =
n i=1
1/K
(i)

(i)

(i)

(i)

(i)

(i)

(i)

(i)

(i)

So it suffices to show that the averages of 1[aht = π(xht )]g(xht ), 1[aht = π(xht )]rht , 1[aht = π(xht )]g(xht +1 ) are
φ
3K -close to their expectations with probability at least 1 − δ/3, respectively. It turns out that, we can use Lemma 21 for
all the three terms. For the first and the third terms, we apply Lemma 21 with V = G, X 0 = X , and obtain the necessary
sample size directly from Corollary 2. For the second term, we apply Lemma 21 with V = {x 7→ x}, X 0 = R. Note that
in this case V is a singleton with the only element being the identity function over R, so it is clear that Pdim(V) < 6 ≤ dG ,
hence the sample size for the other two terms is also adequate for this term.
E.3. Proofs for OLIVER
Recall that the main lemmas for analyzing O LIVER have been proved in Appendix D.1, so below we directly prove Theorem 4.
Proof of Theorem 4. Suppose the preconditions of Lemma 9 (Eq. (22)) and Lemma 10 (Eq. (24)) hold; we show them by
invoking the deviation bounds later. By Lemma 9, when the algorithm terminates, the value of the output policy is at least
VF?,θ − 0 − Hθ.
√
Recall that 0 =  + 2H(3 M (θ + η) + η) (Line 1), so the suboptimality compared to VF?,θ is at most
√
√
 + 2H(3 M (θ + η) + η) + Hθ ≤  + 8H M (θ + η),
which establishes the suboptimality claim.
It remains to show the sample complexity bound. Applying Lemma 9, in every iteration t before the algorithm terminates,
E(ft , πt , ht ) ≥

√
√
0

=
+ 3 M (θ + η) + η = 3 M (2φ + θ + η) + η,
2H
2H

0
thanks to the choice ofφ and
  . For level h = ht , Eq. (25) is satisfied. According to Lemma 10, the event ht = h can
ζ
happen at most M log 2φ
/ log 53 times for every h ∈ [H]. Hence, the total number of iterations in the algorithm is at
most
!
√
 
ζ
5
5
6H M ζ
HM log
/ log .
/ log = HM log
2φ
3

3

Now we are ready to apply the deviation bounds to show that Eq. (22) and 20 hold with high probability. We split the total
failure probability δ among the following events:
1. Estimation of V̂f (Lemma 12; only once): δ/3.


 √ 
2. Estimation of Ẽ(ft , πt , h) (Lemma 13; every iteration): δ/ 3HM log 6H  M ζ / log 53 .
b πt , ht ) (Lemma 14; every iteration): same as above.
3. Estimation of E(f,
Applying Lemma 12, 13, 14 with the above failure probabilities, the choices of nest , neval , n in the algorithm statement
satisfy the preconditions of Lemmas 9 and 10. In particular, the choice of nest and neval guarantee that |V̂f − Vf | ≤ /8
and |Ẽ(ft , πt , h) − E(ft , πt , h)| ≤ /(8H), which are tighter than needed as  ≤ 0 (only 0 /8 and 0 /(8H) are needed
respectively, but tightening these bounds does not improve the sample complexity significantly, so we keep them the same
as in Theorem 1 for simplicity). The remaining calculation of sample complexity is exactly the same as in the proof of
Theorem 1.

Contextual Decision Processes with low Bellman rank are PAC-Learnable

F. Lower Bounds
F.1. An Exponential Lower Bound
We include a result from Krishnamurthy et al. (2016) to formally show that, without making additional assumptions, the
sample complexity of value-based RL for CDPs as introduced in Section 2 has a lower bound of order K H .
Proposition
p 11 (Restatement of Proposition 2 in Krishnamurthy et al. (2016)). For any H, K ∈ N with K ≥ 2, and any
 ∈ (0, 1/8), there exists a family of finite-horizon MDPs with horizon H and |A| = K, and a function space F with
|F| = K H and a universal constant c, such that Q? ∈ F for all MDP instances in the family, yet for any algorithm and
any T ≤ cK H /2 , the probability that the algorithm outputs a policy π̂ with V π̂ ≥ V ? −  after collecting T trajectories
is at most 2/3 when the problem instance is chosen from the family by an adversary.
Proof. The proof relies on the fact that CDPs include MDPs where the state space is arbitrarily large. Each instance of
the MDP family is a complete tree with branching factor K and depth H. Transition dynamics are deterministic, and only
leaf nodes have non-zero rewards. All leaves give Ber(1/2) rewards, except for one that gives Ber(1/2 + ). Changing the
position of the most rewarding leaf node yields a family of K H MDP instances so collecting optimal Q-value functions
forms the desired function class F. Since F provides no information other than the fact that the true MDP lies in this
family, the problem is equivalent to identifying the best arm in a multi-arm bandit with K H arms, and the remaining
analysis follows exactly as in Krishnamurthy et al. (2016).
F.2. A Polynomial Lower Bound that Depends on Bellman Rank
In this section, we prove a new lower bound for layered episodic MDPs that meet the assumptions we make in this paper.
We first recall some definitions. A layered episodic MDP is defined by a time horizon H, a state space S, partitioned
into sets S1 , . . . , SH , each of size at most M , and an action space A of size K. The system descriptor is replaced with a
transition function Γ that associates a distribution over states with each state action pair. More formally, for any sh ∈ Sh ,
and a ∈ A, Γ(sh , a) ∈ ∆(Sh+1 ). The starting state is drawn from Γ1 ∈ ∆(S1 ), and all transitions from SH are terminal.
There is also a reward distribution R that associates a random reward with each state-action pair. We use r ∼ R(s, a) to
PH
denote the random instantaneous reward for taking action a at state s. We assume that the cumulative reward h=1 rh ∈
[0, 1], where rh is the reward received at level h as in Assumption 1.
Observe that this process is a special case of the finite-horizon Contextual Decision Process and moreover, with the set of
all value functions F = (S ×A → [0, 1]), admits a Bellman factorization with Bellman rank at most M (by Proposition 1).
Thus the upper bounds for PAC learning apply directly to this setting.
We now state the lower bound.
Theorem 6. Fix M ≥ 4, H, K ≥ 2 and  ∈ (0, 481√8 ). For any algorithm and any n ≤ cM KH/2 , there exists a layered
episodic MDP with H layers, M states per layer, and K actions, such that the probability that the algorithm outputs a
policy π̂ with V (π̂) ≥ V ? −  after collecting n trajectories is at most 11/12. Here c > 0 is a universal constant.
The result precludes a o(M KH/2 ) PAC-learning sample complexity bound since in this case the algorithm must fail
with constant probability. The result is similar in spirit to other lower bounds for PAC-learning MDPs (Dann & Brunskill,
2015; Krishnamurthy et al., 2016), but we are not aware of any lower bound that applies directly to the setting. There are
two main differences between this bound and the lower bound due to Dann & Brunskill (2015) for episodic MDPs. First,
that bound assumes that the total reward is in [0, H], so the H 2 dependence in the sample complexity is a consequence of
scaling the rewards. Second, that MDP is not layered, but instead has M total states shared across all layers. In contrast,
our process is layered with M distinct states per layer and total reward bounded in [0, 1]. Intuitively, the additional H
dependence arises simply from having M H total states.
At a high level, the proof is based on embedding Θ(M H) independent multi-arm bandit instances into a MDP and requiring
that the algorithm identify the best action in Ω(M H) of them to produce a near-optimal policy. By appealing to a sample
complexity lower bound for best arm identification, this implies that the algorithm requires Ω(M HK/2 ) samples to
identify a near-optimal policy.
We rely on a fairly standard lower bound for best arm identification. We reproduce the formal statement from Krishnamurthy et al. (2016), although the proof is based on earlier lower bounds due to Auer et al. (2002).

Contextual Decision Processes with low Bellman rank are PAC-Learnable

p
Proposition 12. For any K ≥ 2 and τ ≤ 1/8 and any best arm identification algorithm that produces an estimate â,
there exists a multi-arm bandit problem for which the best arm a? is τ better than all others, but P[â 6= a? ] ≥ 1/3 unless
K
the number of samples T is at least 72τ
2.
In particular, the problem instance used in this lower bound is one where the best arm a? has reward Ber(1/2 + ), while
all other arms have reward Ber(1/2). Our construction embeds precisely these instances into the MDP.
Proof. We construct an MDP with M states per level, H levels, and K actions per state. At each level, we allocate
three special states, wh , gh , and bh , for “waiting”, “good”, and “bad.” The remaining M − 3 “bandit” states are denoted
sh,i , i ∈ [M − 3]. Each bandit state has an unknown optimal action a?h,i .
The dynamics are as follows.
• For waiting states wh , all actions are equivalent and with probability 1 − 1/H, they transition to the next waiting
state wh+1 . With the remaining 1/H probability, they transition randomly to one of the bandit state sh+1,i so each
subsequent bandit state is visited with probability H(M1 −3) .
• For bandit states sh,i , the optimal action a?h,i transitions to the good state gh+1 with probability 1/2 + τ and otherwise
to the bad state bh+1 . All other actions transition to gh+1 and bh+1 with probability 1/2. Here τ is a parameter we set
toward the end of the proof.
• Good states always transition to the next good state and bad states always transition to bad states.
• The starting state is w1 with probability 1 − 1/H and s1,i with probability

1
H(M −3)

for each i ∈ [M − 3].

The reward at all states except gH is zero, and the reward at gH is one. Clearly the optimal policy takes actions a?h,i for
each bandit state, and takes arbitrary actions at the waiting, good, and bad states.
This construction embeds H(M − 3) best arm identification problems that are identical to the one used in Proposition 12
into the MDP. Moreover, these problems are independent in the sense that samples collected from one provides no inforK
mation about any others. Appealing to Proposition 12, for each bandit state (h, i), unless 72τ
2 samples are collected from
?
that state, the learning algorithm fails to identify the optimal action ah,i with probability at least 1/3.
After the execution of the algorithm, let B be the set of (h, i) pairs for which the algorithm identifies the correct action.
K
C
Let C be the set of (h, i) pairs for which the algorithm collects fewer than 72τ
to denote
2 samples. For a set S, we use S
the complement.


X
1[ah,i = a?h,i ]
E[|B|] = E 
(h,i)

≤ ((M − 3)H − |C|) +

X

E1[ah,i = a?h,i ]

(h,i)∈C

2
≤ ((M − 3)H − |C|) + |C| = (M − 3)H − |C|/3
3
K
The second inequality is based on Proposition 12. Now, by the pigeonhole principle, if n ≤ (M −3)H
× 72τ
2 , then the
2
K
algorithm can collect 72τ 2 samples from at most half of the bandit problems. Thus |C| > (M − 3)H/2, which implies,

E[|B|] <

5
(M − 3)H
6

By Markov’s inequality,


11
P |B| ≥
(M − 3)H ≤
12

E[|B|]
5/6
<
= 10/11
11/12
− 3)H

11
12 (M

Contextual Decision Processes with low Bellman rank are PAC-Learnable

Thus with probability at least 1/11 we know that |B| ≤ 11
12 (M − 3)H, so the algorithm failed to identify the optimal action
on 1/12 fraction of the bandit problems. Under this event, the suboptimality of the policy produced by the algorithm is,
[
X
V ? − V (π̂) = P[visit B C ] × τ = P[
visit (h, i)] × τ =
P[visit (h, i)] × τ
(h,i)∈B C

=

X
(h,i)∈B C

≥

X
(h,i)∈B C

1
(1 − 1/H)h−1 τ ≥
H(M − 3)

(h,i)∈B C

X
(h,i)∈B C

1
(1 − 1/H)H τ
H(M − 3)

1
1
H(M − 3)
1
1
τ
τ≥
τ=
.
H(M − 3) 4
12
H(M − 3) 4
48

Here we use the fact that the probability of visiting a bandit state is independent of the policy and that the policy can only
visit one bandit state per episode, so the events are disjoint. Moreover, if we visit a bandit state for which the algorithm
failed to identify the optimal action, the difference in value is τ , since the optimal action visits the good state with τ more
probability than a suboptimal one. The remainder of the calculation uses the transition model, the fact that H ≥ 2, and
11
(M − 3)H. Setting τ = 48 and using the requirement on τ gives a stricter requirement on 
finally the fact that |B| ≤ 12
and proves the result.

