Supplementary Material for Multilevel Clustering via Wasserstein Means

Nhat Ho 1 XuanLong Nguyen 1 Mikhail Yurochkin 1 Hung Hai Bui 2 Viet Huynh 3 Dinh Phung 3

Appendix A
In this appendix, we collect relevant information on the
Wasserstein metric and Wasserstein barycenter problem,
which were introduced in Section 2 in the paper. For any
Borel map g : Î˜ â†’ Î˜ and probability measure G on
Î˜, the push-forward measure of G through
g, denoted by
Â´
g#G, is defined by the condition that f (y)d(g#G)(y) =
Î˜
Â´
f (g(x))dG(x) for every continuous bounded function f
on Î˜.
Wasserstein metric

When G =

k
P

pi Î´Î¸i and G0 =

i=1
0

i=1

Wasserstein barycenter As introduced in Section 2.2 in
the paper, for any probability measures P1 , P2 , . . . , PN âˆˆ
P2 (Î˜), their Wasserstein barycenter P N,Î» is such that
P N,Î» = arg min

N
X

P âˆˆP2 (Î˜) i=1

Î˜

k
P

computation, throughout the paper we shall utilize Cuturiâ€™s
algorithm to compute the Wasserstein distance between G
and G0 as well as their optimal transport in (1).

p0i Î´Î¸i0 are discrete measures with finite support, i.e., k

and k 0 are finite, the Wasserstein distance of order r between G and G0 can be represented as
Wrr (G, G0 ) =

min

T âˆˆÎ (G,G0 )

hT, MG,G0 i

where Î» âˆˆ âˆ†N denote weights associated with
P1 , . . . , PN . According to (Agueh and Carlier, 2011),
PN,Î» can be obtained as a solution to so-called multimarginal optimal transporation problem. In fact, if we denote Tk1 as the measure preseving map from P1 to Pk , i.e.,
Pk = Tk1 #P1 , for any 1 â‰¤ k â‰¤ N , then
P N,Î» =

(1)

where we have
n
o
0
Î (G, G0 ) = T âˆˆ RkÃ—k
: T 1k0 = p, T 1k = p0
+
such that p = (p1 , . . . , pk )T and p0 = (p01 , . . . , p0k0 )T ,

	
0
MG,G0 = kÎ¸i âˆ’ Î¸j0 k i,j âˆˆ RkÃ—k
is the cost matrix, i.e.
+
matrix of pairwise distances of elements between G and G0 ,
and hA, Bi = tr(AT B) is the Frobenius dot-product of matrices. The optimal T âˆˆ Î (G, G0 ) in optimization problem
(1) is called the optimal coupling of G and G0 , representing
the optimal transport between these two measures. When
k = k 0 , the complexity of best algorithms for finding the
optimal transport is O(k 3 log k). Currently, (Cuturi, 2013)
proposed a regularized version of (1) based on Sinkhorn
distance where the complexity of finding an approximation
of the optimal transport is O(k 2 ). Due to its favorably fast
1
Department of Statistics, University of Michigan, Ann Arbor, USA. 2 Adobe Research. 3 Center for Pattern Recognition and
Data Analytics (PRaDA), Deakin University, Australia. Correspondence to: Nhat Ho <minhnhat@umich.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Î»i W22 (P, Pi )

X
N

Î»k Tk1


#P1 .

k=1

Unfortunately, the forms of the maps Tk1 are analytically intractable, especially if no special constraints on P1 , . . . , PN
are imposed.
Recently, (Anderes et al., 2015) studied the Wasserstein
barycenters P N,Î» when
 P1 , P2 , . . . ,PN are finite discrete
measures and Î» = 1/N, . . . , 1/N . They demonstrate
the following sharp result (cf. Theorem 2 in (Anderes et al.,
2015)) regarding the number of atoms of P N,Î»
Theorem A.1. There exists a Wasserstein barycenter P N,Î»
N
P
such that supp(P N,Î» ) â‰¤
si âˆ’ N + 1.
i=1

Therefore, when P1 , . . . , PN are indeed finite discrete measures and the weights are uniform, the problem of finding Wasserstein barycenter P N,Î» over the (computationally
large) space P2 (Î˜) is reduced to a search over a smaller
N
P
space Ol (Î˜) where l =
si âˆ’ N + 1.
i=1

Appendix B
In this appendix, we provide proofs for the remaining results in the paper. We start by giving a proof for the tran-

Supplementary Material for Multilevel Clustering via Wasserstein Means

sition from multilevel Wasserstein means objective function to objective function (4) in Section 3.1 in the paper.
All the notations in this appendix are similar to those in
the main text. For each closed subset S âŠ‚ P2 (Î˜), denote
the Voronoi region generated by S on the space P2 (Î˜) by
the collection of subsets {VP }P âˆˆS , where VP := {Q âˆˆ
P2 (Î˜) : W22 (Q, P ) = min W22 (Q, G)}. We define the

S. Now, from the definition of Ï€S
Ë†
2
E(W2 (X, Ï€S (X))) =
W22 (P, Ï€S (P ))dQ(P )
Ë†
=
d2W2 (P, S)dQ(P )
= E(d2W2 (X, S))

GâˆˆS

projection mapping Ï€S as: Ï€S : P2 (Î˜) â†’ S where
Ï€S (Q) = P as Q âˆˆ VP . Note that, for any P1 , P2 âˆˆ S
such that VP1 and VP2 share the boundary, the values of Ï€S
at the elements in that boundary can be chosen to be either
P1 or P2 . Now, we start with the following useful lemmas.
Lemma B.1. For any closed subset S on P2 (Î˜), if Q âˆˆ
P2 (P2 (Î˜)), then EXâˆ¼Q (d2W2 (X, S)) = W22 (Q, Ï€S #Q)
where d2W2 (X, S) = inf W22 (X, P ).
P âˆˆS

Ë†
W22 (P, G)dÏ€(P, G)

â‰¥
Ë†

EXâˆ¼Q (d2W2 (X, S)) â‰¥ W22 (Q, Ï€S #Q).

(5)

From (2) and (5), it is straightforward that
EXâˆ¼Q (d(X, S)2 ) = W22 (Q, Ï€S #Q). Therefore, we
achieve the conclusion of the lemma.

d2W2 (P, S)dÏ€(P, G)
Proof. SinceË† supp(Âµ) âŠ† S, it is clear that W22 (Q, Âµ) =
inf
W22 (P, G)dÏ€(P, G).

d2W2 (P, S)dQ(P )

=

where the integrations in the above equations range over
P2 (Î˜). By combining (3) and (4), we would obtain that

Lemma B.2. For any closed subset S âŠ‚ P2 (Î˜) and Âµ âˆˆ
P2 (P2 (Î˜)) with supp(Âµ) âŠ† S, there holds W22 (Q, Âµ) â‰¥
W22 (Q, Ï€S #Q) for any Q âˆˆ P2 (P2 (Î˜)).

Proof. For any element Ï€ âˆˆ Î (Q, Ï€S #Q):
Ë†

(4)

Ï€âˆˆÎ (Q,Âµ)
P2 (Î˜)Ã—S

= EXâˆ¼Q (d2W2 (X, S))

Additionally, we have
where the integrations in the first two terms range over
P2 (Î˜) Ã— S while that in the final term ranges over P2 (Î˜).
Therefore, we obtain

Ë†

Ë†
W22 (P, G)dÏ€(P, G) â‰¥

Ë†
W22 (Q, Ï€S #Q)

=

=
W22 (P, G)dÏ€(P, G)

inf
P2 (Î˜)Ã—S

â‰¥ EXâˆ¼Q (d2W2 (X, S))

(2)

where the infimum in the first equality ranges over all Ï€ âˆˆ
Î (Q, Ï€S #Q).
On the other hand, let g : P2 (Î˜) â†’ P2 (Î˜) Ã— S such that
g(P ) = (P, Ï€S (P )) for all P âˆˆ P2 (Î˜). Additionally, let
ÂµÏ€S = g#Q, the push-forward measure of Q under mapping g. It is clear that ÂµÏ€S is a coupling between Q and
Ï€S #Q. Under this construction, we obtain for any X âˆ¼ Q
that
Ë†

E W22 (X, Ï€S (X))

=
â‰¥
=

W22 (P, G)dÂµÏ€S (P, G)
Ë†
inf W22 (P, G)dÏ€(P, G)
W22 (Q, Ï€S #Q)

d2W2 (P, S)dQ(P )

=

EXâˆ¼Q (d2W2 (X, S))

=

W22 (Q, Ï€S #Q)

where the last inequality is due to Lemma B.1 and the integrations in the first two terms range over P2 (Î˜) Ã— S while
that in the final term ranges over P2 (Î˜). Therefore, we
achieve the conclusion of the lemma.
Equipped with Lemma B.1 and Lemma B.2, we are ready
to establish the equivalence between multilevel Wasserstein
means objective function (5) and objective function (4) in
Section 3.1 in the main text.
Lemma B.3. For any given positive integers m and M , we
have
m

A :=

(3)

where the infimum in the second inequality ranges over all
Ï€ âˆˆ Î (Q, Ï€S #Q) and the integrations range over P2 (Î˜)Ã—

Ë†

d2W2 (P, S)dÏ€(P, G)

=

inf

HâˆˆEM (P2 (Î˜))

W22 (H,

1 X
Î´G )
m j=1 j

m
X
1
inf
d2 (Gj , H) := B.
m H=(H1 ,...,HM ) j=1 W2

Supplementary Material for Multilevel Clustering via Wasserstein Means
m
1 P
Î´G . From the definition of B,
m j=1 j
for any  > 0, we can find H such that

Proof. Write Q =

m

B

â‰¥

1 X 2
d (Gj , H) âˆ’ 
m j=1 W2

=

EXâˆ¼Q (d2W2 (X, H)) âˆ’ 

=

W22 (Q, Ï€H #Q) âˆ’ 

â‰¥

Aâˆ’

where the second equality in the above display is due to
Lemma B.1 while the last inequality is from the fact that
Ï€H #Q is a discrete probability measure in P2 (P2 (Î˜))
with exactly M support points. Since the inequality in the
above display holds for any , it implies that B â‰¥ A. On
the other hand, from the formation of A, for any  > 0, we
also can find H0 âˆˆ EM (P2 (Î˜)) such that
A â‰¥
â‰¥
=
â‰¥

W22 (H0 , Q) âˆ’ 
W22 (Q, Ï€H 0 #Q) âˆ’ 
m
1 X 2
d (Gj , H 0 ) âˆ’ 
m j=1 W2

PROOF OF THEOREM 3.1 The proof of this theorem
is straightforward from the formulation of Algorithm 1. In
fact, for any Gj âˆˆ Ekj (Î˜) and H = (H1 , . . . , HM ), we
denote the function
m
X
d2 (Gj , H)
f (G, H) =
W22 (Gj , Pnj ) + W2
m
j=1
where G = (G1 , . . . , Gm ). To obtain the conclusion of
this theorem, it is sufficient to demonstrate for any t â‰¥ 0
that
f (G(t+1) , H (t+1) ) â‰¤ f (G(t) , H (t) ).
This inequality comes directly from f (G(t+1) , H (t) ) â‰¤
f (G(t) , H (t) ), which is due to the Wasserstein barycen(t+1)
ter problems to obtain Gj
for 1 â‰¤ j â‰¤ m, and
(t+1)
(t+1)
(t+1)
f (G
,H
) â‰¤ f (G
, H (t) ), which is due
(t+1)
to the optimization steps to achieve elements Hu
of
(t+1)
H
as 1 â‰¤ u â‰¤ M . As a consequence, we achieve
the conclusion of the theorem.
PROOF OF THEOREM 4.1 To simplify notation, write
Ln =

Bâˆ’

where H 0 = supp(H0 ), the second inequality is due to
Lemma B.2, and the third equality is due to Lemma B.1.
Therefore, it means that A â‰¥ B. We achieve the conclusion of the lemma.
Proposition B.4. For any positive integer numbers m, M
and kj as 1 â‰¤ j â‰¤ m, we denote
C

:=

m
X

inf

Gj âˆˆOkj (Î˜) âˆ€1â‰¤jâ‰¤m,
i=1
HâˆˆEM (P2 (Î˜))

W22 (Gj , Pnjj )

D

+
:=

inf

m
X

Gj âˆˆOkj (Î˜) âˆ€1â‰¤jâ‰¤m,
j=1
H=(H1 ,...,HM )

L0 =

f (G, H).

1/2

+ .

Therefore, we would have
1/2
â‰¤ L1/2
+
n âˆ’ f (G, H)

â‰¤ fn (G, H)1/2 âˆ’ f (G, H)1/2 + 
fn (G, H) âˆ’ f (G, H)
+
=
fn (G, H)1/2 + f (G, H)1/2
m
X
|W22 (Gj , Pnjj ) âˆ’ W22 (Gj , P j )|
â‰¤
+
j
j
j=1 W2 (Gj , Pnj ) + W2 (Gj , P )

W22 (Gj , Pnjj )

d2W2 (Gj , H)
.
m
Then, we have C = D.
+

In the remainder of the Supplement, we present the proofs
for all remaining theorems stated in the main text.

inf

Gj âˆˆOkj (Î˜),
HâˆˆEM (P2 (Î˜))

f (G, H)1/2 â‰¤ L0
1/2

Proof. The proof of this proposition is a straightforward application of Lemma B.3. Indeed, for each fixed
(G1 , . . . , Gm ) the infimum w.r.t to H in C leads to the
same infimum w.r.t to H in D, according to Lemma B.3.
Now, by taking the infimum w.r.t to (G1 , . . . , Gm ) on both
sides, we achieve the conclusion of the proposition.

fn (G, H),

For any  > 0, from the definition of L0 , we can find Gj âˆˆ
Okj (Î˜) and H âˆˆ EM (P(Î˜)) such that

L1/2
n âˆ’ L0

m

1 X
W22 (H,
Î´G )
m i=1 i

inf

Gj âˆˆOkj (Î˜),
HâˆˆEM (P2 (Î˜))

â‰¤

m
X

W2 (Pnjj , P j ) + .

j=1

By reversing the direction, we also obtain the inequality
m
P
1/2
1/2
1/2
W2 (Pnjj , P j ) âˆ’ . Hence, |Ln âˆ’
Ln âˆ’ L0 â‰¥
j=1

1/2
L0

âˆ’

m
P
j=1

W2 (Pnjj , P j )| â‰¤  for any  > 0. Since P j âˆˆ

P2 (Î˜) for all 1 â‰¤ j â‰¤ m, we obtain that W2 (Pnjj , P j ) â†’
0 almost surely as nj â†’ âˆž (see for example Theorem
6.9 in (Villani, 2009)). As a consequence, we obtain the
conclusion of the theorem.

Supplementary Material for Multilevel Clustering via Wasserstein Means

PROOF OF THEOREM 4.2 For any  > 0, we denote

A() = Gi âˆˆ Oki (Î˜), H âˆˆ EM (P(Î˜)) :

d(G, H, F) â‰¥  .
Since Î˜ is a compact set, we also have Okj (Î˜) and
EM (P2 (Î˜)) are compact for any 1 â‰¤ i â‰¤ m. As a consequence, A() is also a compact set. For any (G, H) âˆˆ
A(), by the definition of F we would have f (G, H) >
f (G0 , H0 ) for any (G0 , H0 ) âˆˆ F. Since A() is compact,
it leads to
f (G, H) > f (G0 , H0 ).

inf
(G,H)âˆˆA()

for any (G0 , H0 ) âˆˆ F. From the formulation of fn
as in the proof of Theorem 4.1, we can verify that
b n, H
b n, H
b n ) = lim f (G
b n ) almost surely as
lim fn (G
nâ†’âˆž
nâ†’âˆž
n â†’ âˆž. Combining this result with that of Theorem 4.1,
b n, H
b n ) â†’ f (G0 , H0 ) as n â†’ âˆž for any
we obtain f (G
0
0
(G , H ) âˆˆ F. Therefore, for any  > 0, as n is large
b n, H
b n , F) < . As a consequence,
enough, we have d(G
we achieve the conclusion regarding the consistency of the
mixing measures.

Appendix C
In this appendix, we provide details on the algorithm for
the Multilevel Wasserstein means with sharing (MWMS)
formulation (Algorithm 2). Recall the MWMS objective
function as follows
m
X

inf

SK ,Gj ,HâˆˆBM,SK

W22 (Gj , Pnjj ) +

j=1

d2W2 (Gj , H)
m

Algorithm 2 Multilevel Wasserstein Means with Sharing
(MWMS)
Input: Data Xj,i , K, M .
Output: global set SK , local measures Gj , and elements
Hi of H.
n
o
(0)
(0)
(0)
(0)
Initialize SK = a1 , . . . , aK , elements Hi of
H (0) , and t = 0.
(t)
(t)
(t)
while SK , Gj , Hi have not converged do
(t)

1. Update global set SK :
for j = 1 to m do
(t)
(t)
ij â† arg min W22 (Gj , Hu ).
1â‰¤uâ‰¤M

(t)

j

T â† optimal coupling of Gj , Pnj (cf. Appendix
A).
(t)
(t)
U j â† optimal coupling of Gj , Hij .
end for
for i = 1 to M do
(t)
(t)
(t)
hi â† atoms of Hi with hi,v as v-th column.
end for
for i = 1 to K do
ni
m P
m P
P
P
u
u
mD â† m
Ti,v
+
Ui,v
.
u=1 v=1
u=1 v6=i
 m n
P Pi u
(t+1)
ai
â† m
Ti,v Xu,v +
u=1
v=1

m P
P
u (t)
Ui,v
hju ,v /mD.
u=1 v


where BM,SK = Gj âˆˆ OK (Î˜), H = (H1 , . . . , HM ) :

supp(Gj ) âŠ† SK âˆ€1 â‰¤ j â‰¤ m .

end for
(t)
2. Update Gj for 1 â‰¤ j â‰¤ m:
for j = 1 to m do
(t+1)
Gj
â†
arg min
W22 (Gj , Pnjj )
(t+1)

We make the following remarks regarding the initializations and updates of Algorithm 2:
(0)

(i) An
 efficient way
 to initialize global set SK =
(0)
(0)
a1 , . . . , aK âˆˆ RdÃ—K is to perform K-means on
the whole data set Xj,i for 1 â‰¤ j â‰¤ m, 1 â‰¤ i â‰¤ nj ;
(t+1)

(ii) The updates aj
are indeed the solutions of the following optimization problems

inf

X
m

(t)

aj

l=1

m
P
(t)

W22 (Gl , Pnl ) +

l=1

(t)

(t)

W22 (Gl , Hil ) 
m

,

Gj :supp(Gj )â‰¡SK
(t)
2
+W2 (Gj , Hij )/m.

end for
(t)
3. Update Hi for 1 â‰¤ i â‰¤ M as Algorithm 1.
4. t â† t + 1.
end while

Supplementary Material for Multilevel Clustering via Wasserstein Means
(t)

which is equivalent to find aj to optimize

m
+

nj
m X
X

u=1 v=1
m X
X

(t)
u
Tj,v
kaj

(t+1)

Therefore, the update of ai
E

2

âˆ’ Xu,v k

(t+1)

u
mTj,v
kaj

(t+1)

u
+ Uj,v
kaj
(t)

(t)

â‰¥ m

u=1 v

(t)

where T j is an optimal coupling of Gj , Pnj and U j
(t)

â‰¥ m

(t)

is an optimal coupling of Gj , Hij . By taking the
first order derivative of the above function with respect
(t)
(t+1)
to aj , we quickly achieve aj
as the closed form
minimum of that function;
(t+1)

(iii) Updating the local weights of Gj
the atoms of

is equivalent to

(t+1)
Gj

âˆ’ Xu,v k2

u=1 j,v

u
Uj,v
kaj âˆ’ hij ,v ||2 .

(t+1)
updating Gj
as
(t+1)
stem from SK .

m X
X

â‰¥

from Algorithm 2 leads to

are known to

m
X
j=1
m
X

(t)

âˆ’ hiu ,v k2

(t)0
W22 (Gj , Pnj )

+

(t)0

W22 (Gj , Pnj ) +

j=1

= mf (G
where G0

(t)

Theorem C.1. Algorithm 2 monotonically decreases the
objective function of the MWMS formulation.

j=1
m
X

(t)0

(t)

W22 (Gj , Hij )
(t)0

d2W2 (Gj , H (t) )

j=1
0 (t)

,H

(t)

)

(t)0

(t)0

(t)0

= (G1 , . . . , Gm ), Gj

are formed by re-

(t+1)
(t)
placing the atoms of Gj by the elements of SK , noting
0
(t)
(t+1)
that supp(Gj ) âŠ† SK
as 1 â‰¤ j â‰¤ m, and the second

inequality comes directly from the definition of Wasserstein distance. Hence, we obtain
f (G(t) , H (t) ) â‰¥ f (G0

Now, similar to Theorem 3.1 in the main text, we also have
the following theoretical guarantee regarding the behavior
of Algorithm 2 as follows

m
X

(t+1)

From the formation of Gj
m
X

(t+1)

d2W2 (Gj

j=1

, H (t) ) â‰¤

(t)

, H (t) ).

(6)

as 1 â‰¤ j â‰¤ m, we get

m
X

(t)0

d2W2 (Gj , H (t) ).

j=1

Thus, it leads to
f (G0
Proof. The proof is quite similar to the proof of Theorem
3.1. In fact, recall from the proof of Theorem 3.1 that for
any Gj âˆˆ Ekj (Î˜) and H = (H1 , . . . , HM ) we denote the
function

f (G, H) =

m
X
j=1

W22 (Gj , Pnj ) +

d2W2 (Gj , H)
m

(t)

, H (t) ) â‰¥ f (G(t+1) , H (t) ).
(t+1)

Finally, from the definition of H1
have

(7)
(t+1)

, . . . , HM

, we

f (G(t+1) , H (t) ) â‰¥ f (G(t+1) , H (t+1) ).

(8)

By combining (6), (7), and (8), we arrive at the conclusion
of the theorem.

Appendix D
where G = (G1 , . . . , Gm ). Now it is sufficient to demonstrate for any t â‰¥ 0 that
f (G(t+1) , H (t+1) ) â‰¤ f (G(t) , H (t) ).
where the formulation of f is similar as in the proof of
Theorem 3.1. Indeed, by the definition of Wasserstein distances, we have
E = mf (G(t) , H (t) ) =
m X
X
u=1 j,v

In this appendix, we offer details on the data generation
processes utilized in the simulation studies presented in
Section 5 in the main text. The notions of m, n, d, M are
given in the main text. Let Ki be the number of supporting
atoms of Hi and kj the number of atoms of Gj . For any
d â‰¥ 1, we denote 1d to be d dimensional vector with all
components to be 1. Furthermore, Id is an identity matrix
with d dimensions.
Comparison metric (Wasserstein distance to truth)
m

(t)

(t)

(t)

u
u
mTj,v
kaj âˆ’ Xu,v k2 + Uj,v
kaj âˆ’ hiu ,v k2 .

W :=

1 X
W2 (GÌ‚j , Gj ) + dM (HÌ‚, H)
m j=1

Supplementary Material for Multilevel Clustering via Wasserstein Means

where HÌ‚ := {HÌ‚1 , . . . , HÌ‚M }, H := {H1 , . . . , HM } and
dM (HÌ‚, H) is a minimum-matching distance (Tang et al.,
2014; Nguyen, 2015):
dM (HÌ‚, H) := max{d(HÌ‚, H), d(H, HÌ‚)}

For each group j = 1, . . . , m generate local measures and
data as follows:
pick cluster label zÌƒj âˆ¼ Unif({1, . . . , M }).
select shared atoms sj = {k : zk = zÌƒj }.
weights of atoms psj âˆ¼ Dir(1|sj | );

where

Gj :=

X

pi Î´Î¸i .

iâˆˆsj

d(HÌ‚, H) := max

min

1â‰¤iâ‰¤M 1â‰¤jâ‰¤M

W2 (Hi , HÌ‚j ).

Multilevel Wasserstein means setting The global clusters are generated as follows:
means for atoms Âµi := 5(i âˆ’ 1), i = 1, . . . , M.
atoms of Hi : Ï†ij âˆ¼ N (Âµi 1d , Id ), j = 1, . . . , Ki .
weights of atoms: Ï€i âˆ¼ Dir(1Ki ).
Let Hi :=

Ki
X

Ï€ij Î´Ï†ij .

j=1

For each group j = 1, . . . , m, generate local measures and
data as follows:
pick cluster label zj âˆ¼ Unif({1, . . . , M }).
mean for atoms : Ï„ji âˆ¼ Hzj , i = 1, . . . , kj .
atoms of Gj : Î¸ji âˆ¼ N (Ï„ji , Id ), i = 1, . . . , kj .
weights of atoms pj âˆ¼ Dir(1kj ).
Let Gj :=

kj
X

pji Î´Î¸ji .

i=1

data mean Âµi âˆ¼ Gj , i = 1, . . . , nj .
observation Xj,i âˆ¼ N (Âµi , Id ).
For the case of non-constrained variances, the variance to
generate atoms Î¸ji of Gj is set to be proportional to global
cluster label zj assigned to Gj .
Multilevel Wasserstein means with sharing setting
The global clusters are generated as follows:
means for atoms Âµi := 5(i âˆ’ 1), i = 1, . . . , M.
atoms of Hi : Ï†ij âˆ¼ N (Âµi 1d , Id ), j = 1, . . . , Ki .
weights of atoms Ï€i âˆ¼ Dir(1Ki ).
Let Hi :=

Ki
X

Ï€ij Î´Ï†ij .

j=1

For each shared atom k = 1, . . . , K:
pick cluster label zk âˆ¼ Unif({1, . . . , M }).
mean for atoms : Ï„k âˆ¼ Hzk .
atoms of SK : Î¸k âˆ¼ N (Ï„k , Id ).

data mean Âµi âˆ¼ Gj , i = 1, . . . , nj .
observation Xj,i âˆ¼ N (Âµi , Id ).
For the case of non-constrained variances, the variance to
generate atoms Î¸i of Gj where i âˆˆ sj is set to be proportional to global cluster label zÌƒj assigned to Gj .
Three-stage K-means First, we estimate Gj for each
group 1 â‰¤ j â‰¤ m by using K-means algorithm with kj
clusters. Then, we cluster labels using K-means algorithm
with M clusters based on the collection of all atoms of Gj s.
Finally, we estimate the atoms of each Hi via K-means
algorithm with exactly L clusters for each group of local
atoms. Here, L is some given threshold being used in Algorithm 1 in Section 3.1 in the main text to speed up the computation (see final remark regarding Algorithm 1 in Section
3.1). The three-stage K-means algorithm is summarized in
Algorithm 3.
Algorithm 3 Three-stage K-means
Input: Data Xj,i , kj , M , L.
Output: local measures Gj and global elements Hi of
H.
Stage 1
for j = 1 to m do
Gj â† kj clusters of group j with K-means (atoms as
centroids and weights as label frequencies).
end for
C â† collection of all atoms of Gj .
Stage 2
{D1 , . . . , DM } â† M clusters from K-means on C.
Stage 3
for i = 1 to M do
Hi â† L clusters of Di with K-means (atoms as centroids and weights as label frequencies).
end for

Supplementary Material for Multilevel Clustering via Wasserstein Means

References
M. Agueh and G. Carlier. Barycenters in the wasserstein
space. SIAM Journal on Mathematical Analysis, 43:
904â€“924, 2011.
E. Anderes, S. Borgwardt, and J. Miller. Discrete wasserstein barycenters: optimal transport for discrete data.
http://arxiv.org/abs/1507.07218, 2015.
M. Cuturi. Sinkhorn distances: lightspeed computation of
optimal transport. Advances in Neural Information Processing Systems 26, 2013.
X. Nguyen. Posterior contraction of the population poly-

tope in finite admixture models. Bernoulli, 21:618â€“646,
2015.
Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, Qiaozhu
Mei, and Ming Zhang. Understanding the limiting factors of topic modeling via posterior contraction analysis.
In Proceedings of The 31st International Conference on
Machine Learning, pages 190â€“198. ACM, 2014.
C. Villani.
Optimal Transport: Old and New.
Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathemtical Sciences]. Springer,
Berlin, 2009.

