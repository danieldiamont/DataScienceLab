Online Partial Least Square Optimization

A. Proof Detailed Proofs in Section 3
A.1. Proof of Proposition 3.4
Proof. Before we proceed, we first introduce the following lemma.
Lemma A.1. For |x|  59 , we have
1

(1 + x) 2

1+

x
 3x2 .
2

Proof of Lemma A.1. By the Taylor Expansion with Lagrange remainder, we have
(1 + x0 )

1
2

=1

1
2

1
1 @ 2 (1 + x0 )
x0 +
2
2
@x2

x=✓x0

x20

for some ✓ 2 (0, 1). Then for |x|  59 , we have
(1 + x)

1
3
1
3
1
36 2
2
2
1+ x =
x

x
=
x  3x2 .
2
8 (1 + ✓x) 52
8 (1 59 ) 52
28

1
2

We then proceed with the main proof. Since the optimization problem is symmetric about u and v, we only prove the claim
for u. Specifically, we first compute uk+1 uk . By (2.2) and (2.3), we have
uk+1 =

uk + ⌘Xk Yk> vk
.
kuk + ⌘Xk Yk> vk k 2

Since ⌘Bd  14 , by Cauchy-Schwarz inequality, we have
|x| = 2⌘uk > Xk Yk> vk + ⌘ 2 vk> Yk Xk> Xk Yk> vk

 2⌘kuk k2 kXk k2 kYk k2 kvk k2 + ⌘ 2 kvk k2 kYk k2 kXk k22 kYk k2 kvk k2
5
= 2⌘Bd + ⌘ 2 B 2 d2 < ,
9

which satisfies the condition of Lemma A.1. We denote
1

T1 :=(1 + 2⌘uk > Xk Yk> vk + ⌘ 2 vk> Yk Xk> Xk Yk> vk ) 2 1 + ⌘uk > Xk Yk> vk
1
1 2 >
+ ⌘ 2 vk> Yk Xk> Xk Yk> vk
⌘ vk Yk Xk> Xk Yk> vk .
2
2
Then by Lemma A.1, we have |T1 |  3 2⌘uk > Xk Yk> vk + ⌘ 2 vk> Yk Xk> Xk Yk> vk
uk+1

uk = kuk + ⌘Xk Yk> vk k2 1 (uk + ⌘Xk Yk> vk )
>
⌘u>
k Xk Yk vk )(uk
>
⌘u>
k Xk Yk vk )(uk

= (1
= (1

+
+

⌘Xk Yk> vk )
⌘Xk Yk> vk )



✓

✓

3(2⌘uk

>

Xk Yk> vk

+

uk + T1 (uk + ⌘Xk Yk> vk )
uk + Rk = ⌘(Xk Yk> vk

⌘ 2 vk> Yk Xk> Xk Yk> vk )2

2

2

>
u>
k X k Y k v k u k ) + Rk ,

◆
1 2 2 2
(i)
+ ⌘ B d (uk + ⌘Xk Yk> vk )
2

◆
1 2
2
 3⌘ (dB) (2 + ⌘(dB)) + ⌘ (dB) (1 + ⌘dB)
2
✓
◆
2
9
1
4
 3⌘ 2 (dB)2 + ⌘ 2 (dB)2
 20⌘ 2 (dB)2 .
4
2
5
2

+ 12 ⌘ 2 B 2 d2 . Therefore, we have

uk

⇣
⌘>
(1)
(2)
(d)
where Rk = Rk , Rk , ... , Rk
with
(i)
Rk

2

Online Partial Least Square Optimization

A.2. Proof of Theorem 3.5
Proof. We first bound the infinitesimal conditional variance. Since the optimization problem is symmetric about u and v,
we only prove the claim for u.
⌘2
d ⇣ (j)
E U⌘ (t) U⌘(j) (0)
t=0
dt
⇣ h
⌘
i
>
1
 ⌘ tr E (U⌘ (⌘) U⌘ (0)) (U⌘ (⌘) U⌘ (0))
U⌘ (0) = uk , V⌘ (0) = vk
h
i
>
>
>
= ⌘ 1 E ⌘ Xk Yk> uk u>
⌘ Xk Yk> uk u>
k Xk Yk vk uk + fk (uk , vk )
k Xk Yk vk uk + fk (uk , vk )
>
>
= ⌘E u>
k Y k Xk Xk Y k uk

>
>
>
>
>
>
2
2u>
+ O(⌘ 2 ).
k Y Xk uk uk Xk Yk vk + uk uk (uk Xk Yk vk )

Furthermore, by Cauchy-Schwarz inequality, we have
⌘2
d ⇣ (j)
E U⌘ (t) U⌘(j) (0)
 ⌘E (dB)2 + 2(dB)2 + (dB)2 + O(⌘ 2 )  ⌘4(dB)2 + O(⌘ 2 ) = O(⌘).
t=0
dt

By Section 4 of Chapter 7 in (Ethier and Kurtz, 2009), we know that, as ⌘ ! 0+ , U⌘ (t) and V⌘ (t) weakly converge to the
solution of (3.1) and (3.2) with the same initial. By definition of U⌘ (t) and V⌘ (t), we complete the proof.
A.3. Proof of Theorem 3.6
Proof. Since P is an orthonormal matrix, kHj k2 = kWj k2 = 1 for all j = 1, ..., d. Thus, we have
d (i)
H =
dt
=

iH

2d
X

(i)

j (H

(j) 2

) H (i)

j=1

i

2d
X

(H (j) )2 H (i)

j=1

= H (i)

2d
X

j (H

(j) 2

) H (i)

j=1

2d
X

(

j ) (H

i

(j) 2

) .

j=1

We then verify (3.8) satisfies (3.7). By (Evans, 1988), we know that since Hj (t) is continuously differentiable in t, the
solution to the ODE is unique. For notational simplicity, we denote
S (j) (t) = H (j) (0) exp(

j t).

Then we have
S (i) (t)
H (i) (t) = qP
2d
(j) (t)
j=1 S

Now we only need to verify

d (i)
H (t) =
dt
=

=

iS

(i)

(t)

qP

P2d

i qP
iH

(i)

S (i) (t)
2d
j=1

(t)

S (j) (t)
2d
X

j

2

⇣

2d
X

2

S (j) (t)

j

j=1

H (j) (t)

.

⇣ P
⌘
2
(j)
2 2d
(t)) S (i) (t)
j=1 j (S
qP
2d
(j) (t) 2
2
)
j=1 (S

S (j) (t)

j=1

j=1

which completes the proof.

2
S (j) (t)

2d
j=1

2

⌘2

P2d

j=1

2

S (i) (t)
q
2
P2d
(j) (t)
S (j) (t)
j=1 S

H (i) (t),

2

Online Partial Least Square Optimization

B. Proof Detailed Proofs in Section 4
B.1. Proof of Theorem 4.1
(i)

Proof. We calculate the infinitesimal conditional expectation and variance for Z⌘ , i 6= j.
h
i
d
EZ⌘(i) (t) t=0 = ⌘ 1 E Z⌘(i) (⌘) Z⌘(i) (0) H⌘ (0) = h
dt
h
⇣
⌘
i
= ⌘ 1 E ⌘ 1/2 H⌘(i) (⌘) H⌘(i) (0) H⌘ (0) = h
=⌘

1/2 (i)

h

2d
X

(

l ) (h

i

(l) 2

) + O(⌘) = Z⌘(i) (

i

j)

+ o(1),

(B.1)

l=1

where the last equality comes from the assumption that the algorithm starts near j th column of P, j 6= 1, i.e., h ⇡ ej . To
b
compute variance, we first compute ⇤,
!
>
>
>
>
1
Y
X
+
X
Y
Y
X
X
Y
>
b = P QP =
⇤
,
>
>
>
>
2
Y X +X Y
Y X
XY
b
where Q is defined in (3.3). Then we analyze e>
i ⇤ej by cases:
8 ⇣
⌘
(i) (j)
(j) (i)
1
>
X
Y
+
X
Y
>
> 2⇣
>
⌘
>
(j) (i d)
(i d) (j)
>
< 1
X
Y
+
X
Y
2
b
⇣ (j d) (i)
⌘
e>
i ⇤ej =
(i) (j d)
1
>
X
Y
X
Y
>
> 2⇣
>
>
(i d) (j d)
(j d) (i
>
: 1
X
Y
X
Y
2

if max(i, j)  d,
if j  d < i,
d)

which further implies

d
E(Z⌘(i) (t)
dt

Z⌘(i) (0))2

t=0

=⌘

1

⇥
E Z⌘(i) (⌘)

Z⌘(i) (0)

2

b
b
b
E[⌘ 2 (⇤h
h> ⇤hh)(
⇤h
b > b>
= E(e>
i ⇤ej ej ⇤ ei ) + o(1)
=⌘

=

1
4

2

i !j

+

j !i

+ 2 sign(i

⌘

if i  d < j,

if min(i, j) > d,

H⌘ (0) = h

⇤

>
b
h> ⇤hh)
]i,i + O(⌘)

d

1/2) · sign(j

1/2

d) · ↵ij .

(B.2)

By (B.1) and (B.2), we get the limit stochastic differential equation,
dZ (i) (t) =

(

i )Z

j

(i)

(t)dt +

ij dB(t).

B.2. Proof of Proposition 4.2
(1)

(1)

Proof. Our analysis is based on approximating z⌘,k by its continuous approximation Z⌘ (t), which is normal distributed
at time t. By simple manipulation, we have
⇣
⌘
⇣
⌘
1
(2)
(2)
(1)
2
2
P (h⌘,N1 )2  1
= P (z⌘,N1 )2  ⌘ 1 (1
)
P(|z⌘,N1 | ⌘ 2 ).
We then prove P
variance

2
12

2(

1

2)

⇣
⇥

(1)

z⌘,N1

exp 2(
0

⌘

1
2

2(

1

1

P @q

⌘

(1)

⌫. At time t, z⌘,k approximates to a normal distribution with mean 0 and
⇤
1 . Therefore, let (x) be the CDF of N (0, 1), we have
2 )⌘N1
1
✓
◆
(1)
z⌘,N1
1
+
⌫
1
A ⇡ 1 ⌫,
2
2
12
· [exp (2(
)⌘N ) 1]
2)

1

1

2

1

Online Partial Least Square Optimization

which requires
1
2

⌘

1



Solving the above inequality, we get

✓

◆ s
·

1+⌫
2

N1 =

2(

2)

1

2)

1

1

⌘
2(

2
12

2⌘

log

· [exp (2(

1 2

(

2)

1

1+⌫ 2
2

1

2 )⌘N1 )

1

2
12

1].

!

+1 .

B.3. Proof of Proposition 4.3
(1)

(1)

Proof. After Phase I, we restart our counter, i.e., h⌘,0 = . By (3.8) and h⌘,N2 approximating to the process H (1) (⌘N2 ),
we obtain
0
1 1
◆
2d ✓⇣
⇣
⌘2 ⇣
⌘2
⌘2
⇣
⌘2
X
(1)
h⌘,N2 (t) = H (1) (⌘N2 ) = @
H (j) (0) exp (2 j ⌘N2 ) A
H (1) (0) exp (2 1 ⌘N2 )
j=1

2

exp(2

1 ⌘N2 )

2

+ (1

) exp(2

which requires
2

exp(2

1 ⌘N2 )

2

+ (1

) exp(2

1 2

2 ⌘N2 )

1 2

2 ⌘N2 )

exp(2

exp(2

1 ⌘N2 ),

1 ⌘N2 )

⌘

1

2

(1

).

Solving the above inequality, we get
N2 =

⌘
2(

1

1

2)

log

2

1

.

2

B.4. Proof of Theorem 4.4
Proof. For i = 2, ..., 2d, we compute the infinitesimal conditional expectation and variance,
h
i
d
EZ⌘(i) (t) t=t = ⌘ 1 E Z⌘(i) (t0 + ⌘) Z⌘(i) (t0 ) H ⌘ (t0 ) = h
0
dt
2d
X
2
(i)
= ⌘ 1/2 hi
( i
( i
j ) hj + O(⌘) = Z
1 ) + o(1),
j=1

d ⇣ (i)
E Z⌘ (t)
dt

Z⌘(i) (t0 )

⌘2

t=t0

=⌘

1

=⌘

2

E

⇣

Z⌘(i) (t0 + ⌘)

h
b
E ⌘ 2 (⇤h

Z⌘(i) (t0 )

b
b
h> ⇤hh)(
⇤h

⌘2

H ⌘ (t0 ) = h
i
>
b
h> ⇤hh)
+ O(⌘)
i,i

1
b > b>
= E(e>
( i !1 +
i ⇤e1 e1 ⇤ ei ) + o(1) =
4

1 !i

2 sign(i

d

1/2)↵i1 ) + o(1).

Following similar lines to the proof of Theorem 4.1, by Section 4 of Chapter 7 in (Ethier and Kurtz, 2009), we have for
(i)
(k)
each k = 2, ..., 2d, if Z (i) (0) = ⌘ 1/2 h⌘,0 as ⌘ ! 0+ , then the stochastic process ⌘ 1/2 h⌘,bt⌘ 1 c weakly converges to the
solution of the stochastic differential equation (4.3).
B.5. Proof of Proposition 4.5
P2d (i)
Proof. Since we restart our counter, we have i=2 (z⌘,0 )2 = ⌘
moment:
✓⇣
⇣
⌘2
⌘2
2
i1
(i)
E Z (t) =
+
Z (i) (0)
2( 1
2(
i)

1 2

(i)

. Since z⌘,k approximates to Z (i) (⌘k) and its second

2
i1
1

i)

◆

exp [ 2(

1

i )t] ,

for i 6= 1,

Online Partial Least Square Optimization

we use the Markov inequality:

P

2d ⇣
X

(i)

h⌘,N3

i=2

⌘2

>✏

!

E

=

✓

P2d ⇣

1

2d
X

i=2

✏

1✏

⌘

(i)
h⌘,N3

i=2

⌘2 ◆

2
i1

2(

i)

1

⇣

=

✓

1

exp

E

P2d ⇣
i=2

1✏

⌘

2(

d max ( 2 )
1 ⇣ 2id i1 ⇣

1 exp
2(
⌘ 1 ✏ 2( 1
2)
2
d max ( i1
)⇣
d+1i2d
+
1 exp
2( 1 + d )
0
d max ( 2 )
1 @ 1id i1

+ 2 exp [ 2(
⌘ 1✏
( 1
2)
To guarantee

1
⌘

1✏

d max (
1id

(

2
i1 )

2)

1

+

2

exp [ 2(

⌘

N3

2(

1
2)

1

2 )⌘N3 ]

1

0

log @

(

(i)
z⌘,N3

!

⌘

i )⌘N3

1

d )⌘N3

1

4

1 ⌘N3

⌘

2 )⌘N3 ]

1

⇣
⌘2
(i)
+ z⌘,0 exp [ 2(

1

⌘
2

+
1

exp [ 2(

1

2 )⌘N3 ]

i )⌘N3 ]

⌘

A.

 14 , we get:
4( 1
2 )✏⌘

1

⌘2 ◆

2)
1

2
2
i1

4d max

1id

1

A.

B.6. Proof of Corollary 4.6
Proof. First, we prove that ku⌘,k

P2d ⇣ (i) ⌘2
u
bk22 + kv⌘,k vbk22 can be bounded by 3 i=2 h⌘,k , when it is near the optima.
)> and e1 = b
h = p1 P (b
u> vb> )> . Our analysis has shown that when k is large enough,

>
Recall that h⌘,k = p12 P > (u>
⌘,k v⌘,k
the SGD iterates near the optima. Then we have

ku⌘,k

u
bk22 +kv⌘,k
=4

2

vbk22 = 4 2hu⌘,k , u
bi 2hv⌘,k , vbi = 4 4h1⌘,k
r
P2d
(i) 2
X2d
16 i=2 h⌘,k
(i) 2
q
4 1
h⌘,k =
P2d
i=2
(i)
4+4 1
i=2 h⌘,k

where the last inequality holds since k is large enough such that
tions 4.2, 4.3, and 4.5, the total iteration number is

P2d

i=2

(i)

h⌘,k

2

2

3

X2d

i=2

(i)

2

h⌘,k ,

(B.3)

is sufficiently small. By Proposi(B.4)

N = N1 + N2 + N3 .
To explicitily bound N in (B.4) in terms of sample size n, we consider
N1 =
N2 =
N3 =

⌘
2(

2)

1

1
2)

log

1 2

2⌘

log

1

1

⌘
2(

2)

1

⌘
2(

1

1

1+⌫ 2
2

2
12

!

(B.5)

+1 ,

(

(B.6)

,

2

log @

2)

1

2

1
0

(

1

4( 1
2 )✏⌘

2)
1

2

4d max

1id

2
i1

1

A.

(B.7)

Online Partial Least Square Optimization

Given a small enough ✏, we choose ⌘ as follow:
⌘⇣

✏( 1
2)
2 .
d max1id i1

(B.8)

Combining the above sample complexities (B.5), (B.6), (B.7), and (B.8), we get

✓ ◆
d
d
N =O
log
.
2
✏( 1
✏
2)
By Proposition 4.5 with (B.3), given ⌫ < 1/9, after at most N iterations, we have
ku⌘,n
with probability at least 23 .

u
bk22 + kv⌘,n

vbk22  3kh⌘,n

b
hk22  3✏,

(B.9)

