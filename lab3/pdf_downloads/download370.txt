Supplementary materials for
Stochastic modified equations and the dynamics
of stochastic gradient algorithms

A

Modified equations in the numerical analysis of PDEs

The method of modified equations is widely applied in finite difference methods in
numerical solution of PDEs Hirt (1968); Noh & Protter (1960); Daly (1963); Warming
& Hyett (1974). In this section, we briefly demonstrate this classical method. Consider
the one dimensional transport equation
∂u
∂u
=c
∂t
∂x

(1)

where u : [0, T ] × [0, L] → R represents a density of some material in [0, L] and
c > 0 is the transport velocity. It is well-known that the simple forward-time-centralspace differencing leads to instability for all discretization step-sizes (LeVeque, 2002).
Instead, more sophisticated differencing schemes must be used.
We set time and space discretization steps to ∆t and ∆x and denote u(n∆t, j∆x) =
Un,j for 1 ≤ n ≤ N and 1 ≤ j ≤ J. The simplest scheme that can exhibit stability is
the upwind scheme (Courant et al., 1952), where we approximate (1) by the difference
equation


+ Un,j+1 − Un,j
− Un,j − Un,j−1
Un+1,j = Un,j + ∆t c
+c
,
(2)
∆x
∆x
where c+ = max(c, 0) + c− = min(c, 0). The idea is to now approximate this difference scheme by another continuous PDE, that is not equal to the original equation (1)
for non-zero ∆x, ∆t. This can be done by Taylor expanding each term in (2) around
u(t, x) = Un,j . Simplifying and truncating to leading term in ∆t, ∆x, we obtain the
modified equation
∂u
∂u
1
∂2u
−c
= c∆x(1 − r) 2 ,
(3)
∂t
∂x
2
∂x
where r = c∆t/∆x is the Courant-Friedrichs-Lewy (CFL) number (Courant et al.,
1952). Notice that in the limit ∆t, ∆x → 0 with r fixed, one recovers the original
1

transport equation, but for finite step sizes, the upwind scheme is really described by
the modified equation (3). In other words, this truncated equation describes the leading
order, non-trivial behavior of the finite difference scheme.
From the modified equation (3), one can immediately deduce a number of interesting properties. First, the error from the upwind scheme is diffusive in nature, due to
the presence of the second order spatial derivative on the right hand side. Second, we
observe that if the CFL number r is greater than 1, then the coefficient for the diffusive term becomes negative and this results in instability. This is the well-known CFL
condition. This places a fundamental limit on the spatial resolution for fixed temporal resolution with regards to the stability of the algorithm. Lastly, the error term is
proportional to ∆x for fixed r, thus it may be considered a first order method.
Now, another possible proposal for discretizing (1) is the Lax-Wendroff (LW) scheme (Lax
& Wendroff, 1960):


Un,j+1 − 2Un,j + Un,j−1
Un,j+1 − Un,j−1
− c∆t
, (4)
Un+1,j = Un,j + ∆t c
2∆x
2∆x2
whose modified equation is
∂u
1
∂3u
∂u
−c
= c∆x2 (r2 − 1) 3 .
∂t
∂x
6
∂x

(5)

Comparing with (3), we observe that the LW scheme error is of higher order (∆x2 ), but
at the cost of introducing dispersive, instead of diffusive errors due to the presence of
the third derivative. These findings are in excellent agreement with the actual behavior
of their respective discrete numerical schemes (Warming & Hyett, 1974).
We stress here that if we simply took the trivial leading order, the right hand sides of
(3) and (5) disappear and vital information, including stability, accuracy and the nature
of the error term will be lost. The ability to capture the effective dynamical behavior of
finite difference schemes is the key strength of the modified equations approach, which
has become the primary tool in analyzing and improving finite difference algorithms.
The goal of our work is to extend this approach to analyze stochastic algorithms.

B

Summary of SDE terminologies and results

Here, we summarize various SDE terminologies and results we have used throughout
the main paper and also subsequent derivations. A particular important result is the
Itô formula (Sec. B.2), which is used throughout this work for deriving moment equations. For a thorough reference on the subject of stochastic calculus and SDEs, we
suggest Oksendal (2013).

B.1

Stochastic differential equations

Let T > 0. An Itô stochastic differential equation on the interval [0, T ] is an equation
of the form
dXt = b(Xt , t)dt + σ(Xt , t)dWt ,
X0 = x0 ,
(6)
2

where Xt ∈ Rd , b : Rd × [0, T ] → Rd , σ : Rd × [0, T ] → Rd×l and Wt is a ldimensional Wiener process, or Brownian motion. This is a mathematically sound way
of expressing the intuitive notion of SDEs being ODEs plus noise:
Ẋt = b(Xt , t) + “noise”

(7)

The equation (6) is really a “shorthand” for the integral equation
Z t
Z t
σ(Xs , s)dWs .
b(Xs , s)ds +
Xt − x0 =
The last integral is defined in the Itô sense, i.e.
Z t
X
Fsi−1 (Wsi − Wsi−1 ),
Fs dWs := lim
n→∞

0

(8)

0

0

(9)

[si−1 ,si ]∈πn

where πn is a sequence of n-partitions of [0, t] and the limit represents convergence in
probability. In (6), b is known as the drift, and σ is known as the diffusion matrix. When
they satisfy Lipschitz conditions, one can show that (6) (or (8)) has a unique strong
solution (Oksendal (2013), Chapter 5). For our purposes in this paper, we consider the
special case where b, σ do not depend on time and we set d = l so that σ is a square
matrix.
To perform calculus, we need an important result that generalizes the notion of
chain rule to the stochastic setting.

B.2

Itô formula

Itô formula, also known as Itô’s lemma, is the extension of the chain rule of ordinary
calculus to the stochastic setting. Let φ ∈ C 2,1 (Rd × [0, T ]) and let Xt be a stochastic
process satisfying the SDE (6), and thus (8). Then, the stochastic process φ(Xt , t) is
again an Itô process satisfying



1
dφ(Xt , t) = ∂t φ(Xt , t) + ∇φ(Xt , t)T b(Xt , t) + Tr[σ(t, Xt )T Hφ(t, Xt )σ(t, Xt )] dt
2


T
+ ∇φ(Xt , t) σ(Xt , t) dWt ,
(10)
where ∇ denotes gradient with respect to the first argument and Hφ denotes the Hessian, i.e. Hφ(ij) = ∂ 2 φ/∂x(i) ∂x(j) . The formula (10) is the Itô formula. If φ is not
a scalar but a vector, then each of its component satisfy (10). Note that if σ = 0, this
reduces to the chain rule of ordinary calculus.

B.3

The Ornstein-Uhlenbeck process

An important solvable SDE is the Ornstein-Uhlenbeck (OU) process Uhlenbeck &
Ornstein (1930). Consider d = 1, b(x, t) = θ(ξ − x) and σ(x, t) = σ > 0, with θ > 0,
σ > 0 and ξ ∈ R. Then we have the SDE
dXt = θ(ξ − Xt )dt + σdWt ,
3

X0 = x0 .

(11)

To solve this equation, we change variables x 7→ φ(x, t) = xeθt . Applying Itô formula,
we have
dφ(Xt , t) = θξeθt dt + σeθt dWt ,
(12)
which we can integrate from 0 to T to get
Xt = x0 e

−θt

−θt

+ ξ(1 − e

Z
)+σ

t

e−θ(t−s) dWs .

(13)

0

This is a path-wise solution to the SDE (11). To infer distributional properties, we
do not require such precise solutions. In fact, we only need the distribution of the
random variable Xt at any fixed time t ∈ [0, T ]. Observe that Xt is really a Gaussian
process, since the integrand in the Wiener integral is deterministic. Hence, we need
only calculate its moments. Taking expectation on (13), we get
EXt = x0 e−θt + ξ(1 − e−θt ).

(14)

To obtain the covariance function, we see that
Z t

Z t
E(Xt − EXt )(Xs − EXs ) = σ 2 E
eθ(u−s) dWu
eθ(v−s) dWv .
0

(15)

0

This can be evaluated by using Itô’s isometry, which says that for any Wt adapted
process φt , ψt , we have
Z t

Z t

Z t
E
φu dWu
ψv dWv = E
φs ψs ds .
(16)
0

0

0

We get, for s ≤ t
cov(Xs , Xt ) =


σ 2  −θ|t−s|
e
+ e−θ|t+s| ,
2θ

(17)

and in particular, for fixed t ∈ [0, T ], we have
Var(Xt ) =

σ2
(1 − e−2θt ).
2θ

(18)

Hence, we have
Xt ∼ N



σ2
x0 e−θt + ξ(1 − e−θt ), (1 − e−2θt ) .
2θ

(19)

In Sec. 3.1 in the main paper, the solution of the SME is the OU process with θ =
√
2(1 + η), ξ = 0, σ = 2 η. Making these substitutions, we obtain


η 
Xt ∼ N x0 e−2(1+η)t ,
1 − e−4(1+η)t
.
1+η

4

B.4

Numerical solution of SDEs

Unfortunately, most SDEs are not amenable to exact solutions. Often, we resort to
numerical methods. The simplest method is the Euler-Maruyama method. This extends
the Euler method for ODEs to SDEs. Fix a time discretization size δ > 0 and define
X̃k = Xkδ , then we can iterate the finite difference equation
X̃k+1 = X̃k + δb(X̃k , kδ) + σ(X̃k , kδ)(W(k+1)δ − Wkδ ).

(20)

By definition, W(k+1)δ − Wkδ ∼ N (0, δI), and are independent for each k. Here, I is
the identity matrix. Hence, we have the Euler-Maruyama scheme
√
(21)
X̃k+1 = X̃k + δb(X̃k , kδ) + δσ(X̃k , kδ)Zk ,
i.i.d.

where Zk ∼ N (0, I).
One can show that the Euler-Maruyama method (21) is a first order weak approximation (c.f. Def. 1 in main paper) to the SDE (6). However, it is only a order 1/2
scheme in the strong sense (Kloeden & Platen, 2011), i.e.
1

E|Xkδ − X̃kδ | < Cδ 2 .

(22)

With more sophisticated methods, one can design higher order schemes (both in the
strong and weak sense), see Milstein (1986).

B.5

Stochastic asymptotic expansion

Besides numerics, if there exists small parameters in the SDE, we can proceed with
stochastic asymptotic expansions Freidlin et al. (2012). This is the case for the SME,
which has a small η 1/2 multiplied to the noise term. Let us consider a time-homogeneous
SDE of the form
dXt = b(Xt )dt + σ(Xt )dWt
(23)
where   1. The idea is to follow standard asymptotic analysis and write Xt as an
asymptotic series
Xt = X0,t + X1,t + 2 X2,t + . . . .
(24)
We substitute (24) into (23) and assuming smoothness of b and σ, we expand
b (Xt ) = b(X0,t ) + ∇b(X0,t )X1,t + O(2 )
σ(Xt ) = σ(X0,t ) + ∇σ(X0,t )X1,t + O(2 )

(25)

to get
dX0,t = b(X0,t )dt,
dX1,t = ∇b(X0,t )X1,t dt + σ(X0,t )dWt ,
..
.

5

(26)

and X0,0 = x0 , X1,0 = 0. In general, the equation for Xi,t are linear stochastic differential equations with time-dependent coefficients depending on {X0,t , X1,t , . . . , Xi−1,t }
and the initial conditions are X0,0 = x0 , Xi,0 = 0 for all i ≥ 1. Hence, the asymptotic
equations can be solved sequentially to obtain an estimate of Xt to arbitrary order in .
The equations for higher order terms become messy quickly, but they are always linear
in the unknown, as long as all the previous equations are solved. For more details on
stochastic asymptotic expansions, the reader is referred to Freidlin et al. (2012).

B.6

Asymptotics of the SME

√
We now derive the first two asymptotic equations of the SME. we take  = η,
b = −∇f (O(η) term can be ignored for first two terms) and σ = Σ1/2 . Then, (26)
becomes
dX0,t = −∇f (X0,t )dt,

(27)
1
2

dX1,t = −Hf (X0,t )X1,t dt + Σ(X0,t ) dWt ,

(28)

where Hf(ij) = ∂(i) ∂(j) f is the Hessian of f .
In the following analysis, we shall assume that the truncated series approximation
√
(29)
X̂t = X0,t + ηX1,t ,
where X0,t , X1,t satisfy (27) and (28), describes the leading order stochastic dynamics
of the SGD. Now, let us analyze the asymptotic equations in detail. First, we assume
that the ODE (27) has a unique solution X0,t , t ≥ 0 with X0,0 = x0 . This is true if for
example, ∇f is locally Lipschitz. Next, let us define the non-random functions
Ht = Hf (X0,t ),
Σt = Σ(X0,t ).

(30)

Both H and σ are d×d matrices for each t. Then, (28) becomes the time-inhomogeneous
linear SDE
1
(31)
dX1,t = −Ht X1,t + Σt2 dWt ,
with X1,0 = 0. Since the drift is linear and the diffusion matrix is constant (i.e. independent of X1,t ), X1,t is a Gaussian process. Hence we need only calculate its mean
and covariance using Itô formula (see B.2). We have
EX1,t = 0,

(32)

and the covariance matrix St = Cov(X1,t ) satisfies the differential equation
d
St = −St Ht − Ht St + Σt ,
(33)
dt
with S0 = 0. This equation is a linearized version of the Riccati equation and there are
simple closed-form solutions under special conditions, e.g. d = 1 or Ht is constant.
Hence, we conclude that the asymptotic approximation X̂t is a Gaussian process
with distribution
X̂t ∼ N (X0,t , ηSt ),
(34)
where X0,t solves the ODE (27) and St solves the ODE (33), with Ht , Σt given by (30).
6

Remark 1. At this point, it is important to discuss the validity of the asymptotic approximation (34), and the SME approximation (35) in general. What we prove in Sec. C
and is shown in Freidlin et al. (2012) is that for fixed T , we can take η = η(T ) small
enough so that the SME and its asymptotic expansion is a good approximation of the
distribution of the SGD iterates. What we did not prove is that for fixed η, the approximations hold for arbitrary T . In particular, it is not hard to construct systems where
for fixed η, both the SME and the asymptotic expansion fails when T is large enough.
To prove the second general statement requires further assumptions, particularly on
the distribution of fi ’s. This is out of the scope of the current work.

C

Formal Statement and proof of Thm. 1

Theorem 1 (Stochastic modified equations). Let α ∈ {1, 2}, 0 < η < 1, T > 0 and
set N = bT /ηc. Let xk ∈ R, 0 ≤ k ≤ N denote a sequence of SGD iterations defined
by (2). Define Xt ∈ Rd as the stochastic process satisfying the SDE
1
1
(35)
dXt = −∇(f (Xt ) + (α − 1)η|∇f (Xt )|2 )dt + (ηΣ(Xt )) 2 dWt
4
Pn
X0 = x0 and Σ(x) = n1 i=1 (∇f (x) − ∇fi (x))(∇f (x) − ∇fi (x))T .
Fix some test function g ∈ G (c.f. Def. 1 in main paper). Suppose further that the
following conditions are met:

(i) ∇f, ∇fi satisfy a Lipschitz condition: there exists L > 0 such that
|∇f (x) − ∇f (y)| +

n
X

|∇fi (x) − ∇fi (y)| ≤ L|x − y|.

i=1

(ii) f, fi and its partial derivatives up to order 7 belong to G.
(iii) ∇f, ∇fi satisfy a growth condition: there exists M > 0 such that
|∇f (x)| +

n
X

|∇fi (x)| ≤ M (1 + |x|).

i=1

(iv) g and its partial derivatives up to order 6 belong to G.
Then, there exists a constant C > 0 independent of η such that for all k = 0, 1, . . . , N ,
we have
|Eg(Xkη ) − Eg(xk )| ≤ Cη α .
That is, the equation (35) is an order α weak approximation of the SGD iterations.
The basic idea of the proof is similar to the classical approach in proving weak convergence of discretization schemes of SDEs outlined in the seminal papers by Milstein
(Milstein (1975, 1979, 1986, 1995)). The main difference is that we wish to establish
that the continuous SME is an approximation of the discrete SGD, instead of the other
7

way round, which is the case dealt by classical approximation theorems of SDEs with
finite difference schemes. In the following, we first show that a one-step approximation
has order η α+1 error, and then deduce, using the general result in Milstein (1986), that
the overall global error is of order η α .
It is well known that a second order weak convergence discretization scheme for
a SDE is not trivial. The classical Euler-Maruyama scheme, as well as the Milstein
scheme are both first order weak approximations. However, in our case the problem
simplifies significantly. This is because the noise we are trying to model is small, so
that from the outset, we may assume that b(x) = O(1) but σ(x) = O(η 1/2 ), i.e. we
set σ(x) = η 1/2 σ̃(x) where σ̃ = O(1) and deduce the appropriate expansions. For
brevity, in the following we will drop the tilde and simply denote the noise term of the
SDE by η 1/2 σ.
In the subsequent proofs we will make repeated use of Taylor expansions in powers
of η. To simplify presentation, we introduce the shorthand that whenever we write
O(η α ), we mean that there exists a function K(x) ∈ G (c.f. Def. 1 in main text) such
that the error terms are bounded by K(x)η α . For example, we write
b(x + η) = b0 (x) + ηb1 (x) + O(η 2 )

(36)

to mean: there exists K ∈ G such that
|b(x + η) − b0 (x) − ηb1 (x)| ≤ K(x)η 2 .

(37)

These results can be deduced easily using Taylor’s theorem with a variety of forms of
the remainder, e.g. Lagrange form. We omit such routine calculations. We also denote
the partial derivative with respect to x(i) by ∂(i) .
First, let us prove a lemma regarding moments of SDEs with small noise.
Lemma 1. Let 0 < η < 1. Consider a stochastic process Xt , t ≥ 0 satisfying the SDE
1

dXt = b(Xt ) + η 2 σ(Xt )dWt

(38)

with X0 = x ∈ Rd and b, σ together with their derivatives belong to G. Define the
one-step difference ∆ = Xη − x, then we have
Pd
(i) E∆(i) = b(i) η + 21 [ j=1 b(j) ∂(j) b(i) ]η 2 + O(η 3 ).
T
(ii) E∆(i) ∆(j) = [b(i) b(j) + σσ(ij)
]η 2 + O(η 3 ).
Qs
(iii) E j=1 ∆(ij ) = O(η 3 ) for all s ≥ 3, ij = 1, . . . , d.

All functions above are evaluated at x.
Proof. One way to establish (i)-(iii) is to employ the Ito-Taylor expansion (see Kloeden
& Platen (2011), Chapter 5) on the random variable Xη around x and calculating the
moments. Here, we will employ instead the method of semigroup expansions (see Hille
& Phillips (1996), Chapter XI), which works directly on expectation functions. The

8

generator of the stochastic process (38) is the operator L acting on sufficiently smooth
functions φ : Rd → R, and is defined by
Lφ =

d
X

d
1 X
T
∂(i) ∂(j) φ,
b(i) ∂(i) φ + η 2
σσ(ij)
2
i=1
i,j=1

(39)

A classical result on semigroup expansions (Hille & Phillips (1996), Chapter XI) states
that if φ and its derivatives up to order 6 belong to G, then
1
Eφ(Xη ) = φ(x) + Lφ(x)η + L2 φ(x)η 2 + O(η 3 ).
2

(40)

Now, let t ∈ Rd and consider the moment-generating function (MGF)
M (t) = Eet·∆ .

(41)

To ensure its existence we may instead set t to be purely imaginary, i.e. t = is where s
is real. Then, (41) is known as the characteristic function (CF). The important property
we make use of is that the moments of ∆ are found by differentiating the MGF (or CF)
with respect to t. In fact, we have

s
Y
∂ s M (t) 
(42)
E
∆(ij ) = Qs
 ,
j=1 ∂t(ij ) t=0
j=1
where ij = 1, . . . , d. We now expand M (t) in powers of η using formula (40). We get,


d
d
X
X
1
M (t) =1 + 
b(i) t(i) η +
b(i) t(j) ∂(i) b(j) η 2 
2
i=1
i,j=1


d
d
X
X
1
1
T
(43)
b(i) t(i) )2 +
σσ(ij)
t(i) t(j)  + O(η 3 ).
+  η2 (
2
2
i=1
i,j=1
All functions are again evaluated at x. Finally, we apply formula (42) to deduce (i)(iii).
Next, we have an equivalent result for one SGD iteration.
Lemma 2. Let 0 < η < 1. Consider xk , k ≥ 0 satisfying the SGD iterations
xk+1 = xk − η∇fγk (xk )
¯ = x1 − x, then we have
with x0 = x ∈ Rd . Define the one-step difference ∆
¯ (i) = −∂(i) f η
(i) E∆
¯ (i) ∆
¯ (j) = ∂(i) f ∂(j) f η 2 + Σ(ij) η 2 .
(ii) E∆
Qs ¯
3
(iii) E j=1 ∆a
(ij ) = O(η ) for all s ≥ 3, ij = 1, . . . , d.
9

(44)

where Σ =

1
n

Pn

− ∇fi )(∇f − ∇fi )T . All functions above are evaluated at x.

i=1 (∇f

Proof. From definition (44) and the definition of Σ, the results are immediate.
Now, we will need a key result linking one step approximations to global approximations due to Milstein. We reproduce the theorem, tailored to our problem, below.
The more general statement can be found in Milstein (1986).
Theorem 2 (Milstein, 1986). Let α be a positive integer and let the assumptions in
Theorem 1 hold. If in addition there exists K1 , K2 ∈ G so that
|E

s
Y

∆(ij ) − E

j=1

s
Y

¯ (i ) | ≤ K1 (x)η α+1 ,
∆
j

j=1

for s = 1, 2, . . . , 2α + 1 and
E

2α+2
Y

¯ (i ) | ≤ K2 (x)η α+1 .
|∆
j

j=1

Then, there exists a constant C so that for all k = 0, 1, . . . , N we have
|Eg(Xkη ) − Eg(xk )| ≤ Cη α
Proof. See Milstein (1986), Theorem 2 and Lemma 5.

Proof of Theorem 1
We are now ready to prove theorem 1 by checking the conditions in theorem 2 with
α = 1, 2. The second condition is implied by Lemma 2. The first condition is implied
by Lemma 1 and Lemma 2 with the choice
1
b(x) = −∇(f (x) + η(α − 1)|∇f (x))|2 ,
4
1
σ(x) = Σ(x) 2 .

To illustrate our approximation result, let us calculate, using Monte-Carlo simulations, the weak error of the SME approximation
Ew = |Eg(XN η ) − Eg(xN )|,

(45)

for α = 1, 2 v.s. η for different f, fi and generic polynomial test functions g. The
results are shown in Fig. 1. We see that we have order α weak convergence, even when
some conditions of the above theorem are not satisfied (Fig. 1(b)).

10

α=1
α=2

10 -1

10 1

Slope=1
Slope=2

Ew

Ew

10 1
10 -3
10 -5 10 0

10 -1
η

α=1
α=2

10 -1
10 -3

-5
10 -2 10 10 0

(a)

Slope=1
Slope=2

10 -1
η

10 -2

(b)

Figure 1: Weak error Ew , as defined in (45) with α = 1, 2, v.s. learning rate η for two
different choices of f, fi . All errors are averaged over 1012 samples of SGD trajectories
up to T = 1.0. The initial condition is x0 = 1. The SMEs moments are solved exactly
since they involve linear drifts. (a) Quadratic objective with n = 2, fi = (x − γi )2
where γi ∈ {±1}. The total objective is f (x) = x2 + 1. The test function is g(x) =
x + x2 + x3 . (b) Non-convex fi ’s with n = 2, fi (x) = (x − γi )2 + γi x3 where
γi ∈ {±1}. The total objective is the same f (x) = x2 + 1. We chose g(x) = x so that
Eg(XT ) has closed form solution. Note that for this choice of fi , the condition (iii) of
Theorem 1 is not satisfied. Nevertheless, in both cases, we observe that the weak error
decreases with η like Ew ∼ η α .
Remark 2. From above, we also observe that if we pick b(x) = −∇f (x) and σ(x)
to be any function in G (and its sufficiently high derivatives are also in G), then we
have matching moments up to order η 2 and hence we can conclude that for this choice,
the resulting SDE is a first order weak approximation of the SGD, with |Eg(Xkη ) −
Eg(xk )| ≤ Cη, k = 0, 1, . . . , N . In particular, the deterministic gradient flow is a first
order weak approximation of the SGD. Hence, just like traditional modified equations,
our SME (35) (α = 2) is the next order approximation of the underlying algorithm.
However, we stress that for our first order SME with α = 1, i.e. the choice
1
b = −∇f and σ = Σ 2 , the fact that we did not the improve the order of weak convergence from the deterministic gradient flow does not mean that this is a equally bad
approximation. The constant C in the weak error depends on the choice of Σ and in
fact, it can be shown empirically that with this choice, we do have lower weak error
|Eg(Xkη ) − Eg(xk )|, but the order of convergence of the weak error as η → 0 is the
same. An analytical justification must then rely on using the Itô-Taylor expansion to
obtain precise estimates for the factor C (see e.g. Talay & Tubaro (1990)). This is
beyond the scope of the current paper.
Remark 3. The Lipschitz condition (i) is to ensure that the SME has a unique strong
solution with uniformly bounded moments Milstein (1986). If we allow weak solutions
and establish uniform boundedness of moments by other means (more assumptions on
the growth and direction of ∇f for large x), then condition (i) is expected to be relaxed
although the technical details will be tedious.
Condition (iii) in Theorem 1 appears to be the most stringent one and in fact it may
limit applications to problems with objectives that have more than quadratic growth.
11

However, closer inspection tells us it can also be relaxed. For example, if there exists
an invariant distribution that concentrates on a compact subset of Rd then as η → 0,
xk ’s would be bounded with high probability, and hence for large x we may replace
f, fi with a version that satisfies the growth condition in (iii). Further work is needed
to make this precise but we can already see in Fig. 1(b) that we have quadratic weak
convergence even when (iii) is not satisfied.
Remark 4. The regularity conditions on f and g in Theorem 1 are inherited from Theorem 2 in Milstein (1986). For smooth objectives, polynomial growth conditions are
usually not restrictive. Still, with care, these should be relaxed since in our case the
small noise helps to reduce the number of terms containing higher derivatives in various Taylor and Itô-Taylor expansions. Proving a more general version of Theorem 1
will be left as future work.

D

Derivation of SMEs

In this section, we include more detailed derivations of the SMEs used in the main
paper. For brevity, we do not include rigorous proofs of approximation statements for
SGD variants in Sec. D.2 and D.3, but only heuristic justifications. Proving rigorous
statements for these approximations can be done by modifying the proof of Thm. 1.

D.1

SME for the simple quadratic example

We start with the example in Sec. 3.1 of the main paper. Let n = 2, d = 1 and set
f (x) = x2 + 1 with f1 (x) = (x − 1)2 and f2 (x) = (x + 1)2 . The SGD iterations picks
at random between f1 and f2 and performs descent with their respective gradients.
Recall that the (second order) SME is given by
1
1
dXt = −(f 0 (Xt ) + ηf 0 (Xt )f 00 (Xt ))dt + (ηΣ(Xt )) 2 dWt .
2

(46)

Now, f 0 (x) = 2x, f 00 (x) = 2 and
2

1X 0
(f (x) − fi (x))2 = 4
2 i=1 i

(47)

√
dXt = −2(1 + η)Xt dt + 2 ηdWt .

(48)

Σ(x) =
and hence the SME is

D.2

SME for learning rate adjustment

The SGD iterations with learning rate adjustment is
xk+1 = xk − ηuk f 0 (xk ),

(49)

where uk ∈ [0, 1] is the learning rate adjustment factor. η is the maximum allowed
learning rate. There are two reasons we introduce this hyper-parameter. First, gradients
12

cannot be arbitrarily large since that will cause instabilities. Second, the SME is only
an approximation of the SGD for small learning rates, and so it is hard to justify the
approximation for large η.
In this case, deriving the corresponding SME is extremely simple. Notice that we
can define gi,k (xk ) = uk fi (xk ), gk = uk f (xk ). Then, the iterations above is simply
xk+1 = xk − ηgγ0 k ,k (xk ),

(50)

whose (first order) SME is by Thm. 1
1

dXt = −g 0 (Xt )dt + (ηu2t Σ(Xt )) 2 dWt .

(51)

And hence the SME for SGD with learning rate adjustments is
1

dXt = −ut f 0 (Xt )dt + ut (ηΣ(Xt )) 2 dWt .

D.3

(52)

SME for SGD with momentum

First let us consider the constant momentum parameter case. The SGD with momentum
is the paired update
vk+1 = µvk − ηfγ0 k (xk ),
xk+1 = xk + vk+1 .
To derive and SME, notice that we can write the above as



1−µ
0
vk+1 = vk + η −
vk − f (xk ) + η f 0 (xk ) − fγ0 k (xk ) ,
η


vk+1
.
xk+1 = xk + η
η

(53)

(54)

Recall that since we are looking at first order weak approximations, it is sufficient to
compare to the Euler-Maruyama discretization (Sec. B.4). We observe that the above
can be seen as an Euler-Maruyama discretization of the coupled SDE
dVt = (−
dXt =

1
1−µ
Vt − f 0 (Xt ))dt + (ηΣ(Xt )) 2 dWt ,
η

1
Vt dt,
η

(55)

with the usual choice of Σ(x). Hence, this is the first order SME for the SGD with
momentum having a constant momentum parameter µ. For time-varying momentum
parameter, we just replace µ by µt to get
dVt = (−
dXt =

1
1 − µt
Vt − f 0 (Xt ))dt + (ηΣ(Xt )) 2 dWt ,
η

1
Vt dt.
η

(56)
13

E
E.1

Solution of optimal control problems
Brief introduction to optimal control

We first introduce some basic terminologies and results on optimal control theory to
pave way for our solutions to optimal control problems for the learning rate and momentum parameter. For simplicity, we restrict to one state dimension (d = 1), but
similar equations hold for multiple dimensions. For a more thorough introduction
to optimal control theory and calculus of variations, we refer the reader to Liberzon
(2012).
Let t ∈ [0, T ] and consider the ODE
d
zt = Φ(zt , ut ),
dt

(57)

where zt , ut ∈ R and Φ : R × R → R. The variable zt describes the evolution of some
state and ut is the control variable, which can affect the state dynamics. Consider the
control problem of minimizing the cost functional
Z T
C(u) =
L(zs , us )ds + G(zT )
(58)
0

with respect to ut , subject to zt satisfying the ODE (57) with prescribed initial condition z0 ∈ R. The function L is known as the running cost and G is the terminal
cost. Usually, we also specify some control set U ⊂ R so that we only consider
u : [0, T ] → U . The full control problem reads
min
u:[0,T ]→U

C(u) subject to (57).

(59)

Note that additional path constraints can also be added and (57) can also be made timeinhomogeneous, but for our purposes it is sufficient to consider the above form.
There are two principal ways of solving optimal control problems: either dynamic
programming through the Hamilton-Jacobi-Bellman (HJB) equation (Bellman, 1956),
or using the Pontryagin’s maximum principle (PMP) (Pontryagin, 1987). In this section, we will only discuss the HJB method as this is the one we employ to solve the
relevant control problems in this paper.

E.2

Dynamic programming and the HJB equation

The first way to solve (59) is through the dynamic programming principle. For t ∈
[0, T ] and z ∈ R, define the value function
Z T
V (z, t) = min
L(zs , us )ds + G(zT ),
u:[t,T ]→U

subject to
d
zs = Φ(zs , us ),
ds
z(t) = z.

t

s ∈ [t, T ],
(60)
14

Notice that if there exists a solution to (59), then the value of the minimum cost is
V (z0 , 0). The dynamic programming principle allows us to derive a recursion on the
function V , in the form of a partial differential equation (PDE)
∂t V (z, t) + min{∂z V (z, t)Φ(z, u) + L(z, u)} = 0,
u∈U

V (T, z) = G(z).

(61)

This is known as the Hamilton-Jacobi-Bellman equation (HJB). Note that this PDE is
solved backwards in time. The derivation of this PDE can be found in most references
on optimal control, e.g. in Liberzon (2012). The main idea is the dynamic programming principle: for any t the [t, T ]-portion of the optimal trajectory must again be
optimal.
After solving the HJB (61), we can then obtain the optimal control u∗t as function
of the state process zt and t, given by
u∗t = arg min{∂z V (zt , t)Φ(zt , u) + L(zt , u)}.

(62)

u∈U

In some cases, we find that the optimal control is independent of time and is strictly of
a feed-back control law, i.e.
u∗t = u∗ (zt )
(63)
for some function u∗ : R → U . This is the case for the problems considered in this
paper. With the optimal control found in (62), we can then substitute ut = u∗t in (57)
to obtain the optimally controlled process zt∗ .
In summary, to solve the optimal control problem (59), we first solve the HJB
PDE (61), and then solve for the optimal control (62), and lastly (if necessary) solve
the optimally controlled state process by substituting the solution of (62) into (57).
Sometimes, the optimal control (62) can be solved without fully solving the HJB for
V , e.g. when L = 0 and one can infer the sign of ∂V . This is the case for the two
control problems we encounter in this paper. The solution to (62) is the most important
for all practical purposes since it gives a way to adjust the control parameters on-the-fly,
especially when we have a feed back control law.

E.3

Solution of the learning rate control problem

Now, let us apply the HJB equations (Sec. E.2) to solve the learning rate control problem. Recall from Sec. 4.1.2 that we wish to solve
min
u:[0,T ]→[0,1]

mT subject to

d
1
mt = −2aut mt + aηΣu2t ,
dt
2
1
2
m0 = a(x0 − b) .
2

15

(64)

This is of the form (59) with Φ(m, u) = −2aum + aηΣu2 /2, L(m, u) = 0 and
G(m) = m. Thus, we write the HJB equation
1
∂t V (m, t) + min {∂m V (m, t)[−2aum + aηΣu2 ]} = 0,
2
u∈[0,1]
V (m, T ) = m.

(65)

First, it’s not hard to see that for a > 0, ∂m V ≥ 0 for all m, t. This is because, the
lower the m, the closer we are to the optimum and hence the minimum cost achievable
in the same time interval [t, T ] should be less. Similarly,∂m V ≥ 0 holds for a < 0
if one reverses all previous statements (in this case m is negative). Hence, we can
calculate the minimum
1
u∗ = arg min{−2aum + aηΣu2 },
2
u∈[0,1]
(
1
a ≤ 0,
=
2m
a > 0.
min(1, ηΣ )

(66)

Notice that this solution is a feed-back control policy. We can now substitute ut = u∗t
where
(
1
a ≤ 0,
u∗t =
(67)
t
min(1, 2m
)
a > 0.
ηΣ
into the ODE in (64) to obtain
(
m0 e−2at + 41 ηΣ(1 − e−2at )
∗
mt =
ηΣ
2+2a(t−t∗ )

a ≤ 0 or t < t∗ ,
a > 0 and t ≥ t∗ .

(68)

where



4m0
1
log
−1
2a
ηΣ
And therefore, we get from (67) the effective annealing schedule
(
1
a ≤ 0 or t ≥ t∗ ,
u∗t =
1
a > 0 and t > t∗ ,
1+a(t−t∗ )
t∗ =

E.4

(69)

(70)

Solution of the momentum parameter control problem

We shall consider the case a > 0, since for a ≤ 0 the optimal control is trivially µt = 1.
The momentum parameter control problem is
min

mT subject to

µ:[0,T ]→[0,1]

d
mt = Rλ(µt )(mt − m∞ (µt )),
dt
1
m0 = a(x0 − b)2 ,
2
16

(71)

where
λ(µ) = −

(1 − µ) −

p

(1 − µ)2 − 4aη
,
η

m∞ (µ) =

ηΣ
.
4(1 − µ)

(72)

This is of the form (59) with Φ(m, µ) = Rλ(µ)(m − m∞ (µ)), L(m, u) = 0 and
G(m) = m. The HJB equation is
∂t V (m, t) + min {∂m V (m, t)[Rλ(µ)(m − m∞ (µ))]} = 0,
µ∈[0,1]

V (m, T ) = m.

(73)

Again, it is easy to see that ∂m V (m, t) ≥ 0 for all m, t and so
µ∗ = arg min{Rλ(µ)(m − m∞ (µ))}

(74)

µ∈[0,1]

This minimization problem has no closed form solution. However, observe that Rλ(µ) ≤
√
0 and is minimized at µ = µopt = max(0, 1 − 2 aη). Now, if µ > µopt , we have
Rλ(µ) = −(1 − µ)/η and so Rλ(µ)(m − m∞ (µ)) is increasing in µ for µ > µopt
(one can check this by differentiation and showing that the derivative is always positive). Hence, µ∗ ≤ µopt and it is sufficient to consider µ ∈ [0, µopt ] in the minimization
problem (74).
Next, observe that m − m∞ (µ) is decreasing in µ and negative if
m<

ηΣ
.
4(1 − µ)

(75)

or

ηΣ
.
(76)
4m
At the same time, Rλ(µ) is negative and decreasing for µ ∈ [0, µopt ]. Thus, the product
ηΣ
Rλ(µ)(m − m∞ (µ)) is positive and increasing for 1 − 4m
< µ < µopt and hence we
must have
ηΣ
µ∗ ≤ 1 −
.
(77)
4m
Note that this is only a bound, but for small η, we can take this as an approximation of
µ∗ , so long as it is less than µopt . Hence, we arrive at
(
1
a ≤ 0,
∗
µt =
(78)
ηΣ
min(µopt , max(0, 1 − 4m
))
a > 0.
t
µ>1−

One can of course follow the steps in Sec. E.3 to calculate m∗t and hence µ∗t in the
form of an annealing schedule. We omit these calculations since they are not relevant
to applications.

F

Numerical experiments

In this section, we provide model and algorithmic details for the various numerical experiments considered in the main paper, as well as a brief description of the commonly
applied adaptive learning rate methods that we compare the cSGD algorithm with.
17

F.1

Model details

In Sec. 4 from the main paper, we consider three separate models for two datasets.
M0: fully connected NN on MNIST
The first dataset we consider the MNIST dataset (LeCun et al., 1998), which involves
computer recognition of 60000 28 × 28 pixel pictures of handwritten digits. We split
it into 55000 training samples and 5000 test samples. Our inference model is a fully
connected neural network with one hidden layer. For a input batch K of pixel data
(flattened into a vector) z ∈ R784×K , we define the model
y = softmax(W2 hR (W1 z + b1 ) + b2),

(79)

where the activation function hR is the commonly used Rectified Linear Unit (ReLU)
hR (z)(ij) = max(z(ij) , 0).

(80)

The first layer weights and biases are W1 ∈ R784×10 and b1 ∈ R10 and the second
layer weights and biases are W2 ∈ R10×10 and b2 ∈ R10 . These constitute the trainable
parameters. The softmax function is defined as
exp(−z(ij) )
softmax(z)(ij) = P
.
k exp(−z(kj) )

(81)

The output tensors y ∈ R10×K is compared to a batch of one-hot target labels ŷ with
the cross-entropy loss
C(y, ŷ) = −

1 X
ŷ(ij) log y(ij) .
10K i,j

(82)

Lastly, we use `2 regularization so that the minimization problem is
min

W1 ,b1 ,W2 ,b2

C(y, ŷ) +

2
X

λW,i kWi k22 +

i=1

2
X

λb,i kbi k22 ,

(83)

i=1

Each regularization strength λ is set to be 1 divided by the dimension of the trainable
parameter.
C0: fully connected NN on CIFAR-10
The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) consists of 60000 small 32 × 32
pixels of RGB natural images belonging to ten separate classes. We split the dataset
into 50000 training samples and 10000 test samples. Our first model for this dataset is
a deeper fully connected neural network
y = softmax(W3 hT (W2 hT (W1 z + b1 ) + b2 ) + b3 ),

18

(84)

where we use a tanh activation function between the hidden layers
hT (z)(ij) = tanh(z(ij) ).

(85)

The layers have width 3071,500,300,10. That is, the trainable parameters have dimensions W1 ∈ R3071×500 , b1 ∈ R500 , W2 ∈ R500×300 , b2 ∈ R300 , W3 ∈ R300×10 , b3 ∈
R10 . We use the same soft-max output, cross-entropy loss and `2 regularization as as
before.
C1: convolutional NN on CIFAR-10
Our last experiment is a convolutional neural network on the same CIFAR-10 dataset.
We use four convolution layers consisting of convolution,batch-normalization,ReLU,maxpooling. Convolution filter size is 5 × 5, with uniform stride 1 and padding 2. Output
channels of convolution layers are {96,128,256,64}. The pooling size is 2 × 2 with
stride 2. The output layers consist of two fully connected layers of width {1024,256}
and drop-out rate 0.5. `2 regularization is introduced as a weight decay with decay
parameter 5e-3.

F.2

Adagrad and Adam

Here, we write down for completeness the iteration rules of Adagrad (Duchi et al.,
2011), and Adam (Kingma & Ba, 2015) optimizers, which are commonly applied tools
to tune the learning rate. For more details and background, the reader should consult
the respective references.
Adagrad. The Adagrad modification to the SGD reads
x(i),k+1 = xk,(i) − p

η
∂(i) fγk (xk ),
Gk,(i)

(86)

where Gk,(i) is the running sum of gradients ∂(i) fγl (xl ) for l = 0, . . . , k − 1. The
tunable hyper-parameters are the learning rate η and the initial accumulator value G0 .
In this paper we consider only the learning rate hyper-parameter as this is equivalent to
setting the initial accumulator to a common constant across all dimensions.
Adam. The Adam method has similar ideas to momentum. It keeps the exponential
moving averages
m(i),k+1 = β1 mk,(i) + (1 − β1 )∂(i) fγk (xk ),
v(i),k+1 = β2 vk,(i) + (1 − β2 )[∂(i) fγk (xk )]2 .

(87)

Next, set,
mk,(i)
,
1 − β1k
vk,(i)
=
.
1 − β2k

m̂k,(i) =
v̂k,(i)

19

(88)

Finally, the Adam update is
x(i),k+1 = xk,(i) − p

η
m̂k,(i) .
v̂k,(i)

(89)

The hyper-parameters are the learning rate η and the EMA decay parameters β1 , β2 .
Note that for both methods above, one can also introduce a regularization term  to
the denominator to prevent numerical instabilities.

F.3

Implementation of cSGD

Recall from Sec. 4.1 that the optimal control solution for learning rate control of the
quadratic objective f (x) = 12 a(x − b)2 is given by
(
1
a ≤ 0,
∗
(90)
ut =
t
)
a > 0.
min(1, 2m
ηΣ
The idea is to perform a local quadratic approximation
d

f (x) ≈

1X
a(i) (x(i) − b(i) )2 .
2 i=1

(91)

This is equivalent to a local linear approximation of the gradient, i.e. for i = 1, 2, . . . , d
∂(i) f (x) ≈ a(i) (x(i) − b(i) ).

(92)

This effectively decouples the control problems of d identical one-dimensional control
problems, so that we may apply (90) element-wise. We note that this approximation
is only assumed to hold locally and the parameters must be updated. There are many
ways to do this. Our approach uses linear regression on-the-fly via exponential moving
averages (EMA). For each trainable dimension i, we maintain the following exponential averages
g k+1,(i) = βk,(i) g k,(i) + (1 − βk,(i) )fγ0 k (xk,(i) ),
g 2 k+1,(i) = βk,(i) g 2 k,(i) + (1 − βk,(i) )fγ0 k (xk,(i) )2 ,
xk+1,(i) = βk,(i) xk,(i) + (1 − βk,(i) )xk,(i) ,
x2 k+1,(i) = βk,(i) x2 k,(i) + (1 − βk,(i) )x2k,(i) ,
gxk+1,(i) = βk,(i) gxk,(i) + (1 − βk,(i) )xk,(i) fγ0 k (xk,(i) ).

(93)

The decay parameter βk,(i) controls the effective averaging window size. In practice,
we should adjust βk,(i) so that it is small when variations are large, and vice versa. This
ensures that our local approximations adapts to the changing landscapes. Since local
variations is related to the gradient, we use the following heuristic
βk+1,(i) = βmin + (βmax − βmin )

20

g 2 k,(i) − g 2k,(i)
g 2 k,(i)

.

(94)

Algorithm 1 controlled SGD (cSGD)
Hyper-parameters: η, u0
Initialize x0 ; β0,(i) = 0.9 ∀i
for k = 0 to (#iterations − 1) do
Compute sample gradient ∇fγk (xk )
for i = 1 to d do
Update EMA using (93)
Compute ak,(i) , bk,(i) , Σk,(i) using (95)
Compute u∗k,(i) using (96)
βk+1,(i) = (g 2 k,(i) − g 2k,(i) )/g 2 k,(i) and clip
uk+1,(i) = βk,(i) uk,(i) + (1 − βk,(i) )u∗k,(i)
xk+1,(i) = xk,(i) − ηuk,(i) ∇fγk (xk )(i)
end for
end for

which is similar to the one employed in Schaul et al. (2013) for maintaining EMAs.
The additional clipping to the range [βmin , βmax ] is to make sure that there are enough
samples to calculate meaningful regressions, and at the same time prevent too large decay values where the contribution of new samples vanish. In the applications presented
in this paper, we usually set βmin = 0.9 and βmax = 0.999, but results are generally
insensitive to these values.
With the EMAs (93), we compute ak,(i) by the ordinary-least-squares formula and
Σk,(i) as the variance of the gradients:
ak,(i) =

gxk,(i) − g k,(i) xk,(i)
x2 k,(i) − x2k,(i)

bk,(i) = xk,(i) −

,

g k,(i)
,
ak,(i)

Σk,(i) = g 2 k,(i) − g 2k,(i) ,
This allows us to estimate the policy (90) as
(
1
∗
uk,(i) =
a
(xk,(i) −bk,(i) )2
min(1, k,(i) ηΣ
)
k,(i)

(95)

ak,(i) ≤ 0,
ak,(i) > 0.

(96)

for i = 1, 2, . . . , d. Since our averages are from exponentially averaged sources, we
should also update our learning rate policy in the same way:
uk+1,(i) = βk,(i) uk,(i) + (1 − βk,(i) )u∗k,(i)
The algorithm is summarized in Alg. 1

21

(97)

Algorithm 2 controlled momentum SGD (cMSGD)
Hyper-parameters: η, µ0
Initialize x0 , v0 ; β0,(i) = 0.9 ∀i
for k = 0 to (#iterations − 1) do
Compute sample gradient ∇fγk (xk )
for i = 1 to d do
Update EMA using (93)
Compute ak,(i) , bk,(i) , Σk,(i) using (95)
Compute µ∗k,(i) using (99)
βk+1,(i) = (g 2 k,(i) − g 2k,(i) )/g 2 k,(i) and clip
µk+1,(i) = βk,(i) µk,(i) + (1 − βk,(i) )µ∗k,(i)
vk+1,(i) = µk,(i) vk,(i) − η∇fγk (xk )(i)
xk+1,(i) = xk,(i) + vk+1,(i)
end for
end for

F.4

Implementation of cMSGD

We wish to apply the momentum parameter control
(
1
∗
µt =
ηΣ
))
min(µopt , max(0, 1 − 4m
t

a ≤ 0,
a > 0,

(98)

√
where µopt = max{0, 1 − 2 aη}. We proceed in the same way as in Sec. F.3 by
keeping the relevant EMA averages and performing linear regression on the fly. The
only difference is the application of the momentum parameter adjustment, which is

ak,(i) ≤ 0,

1
√
∗
µk,(i) = min[max(0, 1 − 2 ak,(i) η),
(99)

ηΣk,(i)
max(0, 1 −
)]
a
>
0,
k,(i)
2ak,(i) (xk,(i) −bk,(i) )2
The algorithm is summarized in Alg. 2.

F.5

Training accuracy for C1

For completeness we also provide in Fig. 2 the training accuracies of C1 with various
hyper-parameter choices and methods tested in this work. These complements the
plots of test accuracies in Fig. 3,5,6 in the main paper. We see that cSGD and cMSGD
display the same robustness in terms of test and training accuracies.

References
Bellman, Richard. Dynamic programming and Lagrange multipliers. Proceedings of
the National Academy of Sciences, 42(10):767–769, 1956.
22

Courant, Richard, Isaacson, Eugene, and Rees, Mina. On the solution of nonlinear
hyperbolic differential equations by finite differences. Communications on Pure and
Applied Mathematics, 5(3):243–255, 1952.
Daly, Bart J. The stability properties of a coupled pair of non-linear partial difference
equations. Mathematics of Computation, 17(84):346–360, 1963.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of Machine Learning Research, 12
(Jul):2121–2159, 2011.
Freidlin, Mark I, Szücs, Joseph, and Wentzell, Alexander D. Random perturbations of
dynamical systems, volume 260. Springer Science & Business Media, 2012.
Hille, Einar and Phillips, Ralph Saul. Functional analysis and semi-groups, volume 31.
American Mathematical Soc., 1996.
Hirt, CW. Heuristic stability theory for finite-difference equations. Journal of Computational Physics, 2(4):339–355, 1968.
Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. ICLR,
2015.
Kloeden, P. E. and Platen, E. Numerical Solution of Stochastic Differential Equations.
Springer, New York, corrected edition, June 2011.
Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny
images. 2009.
Lax, Peter and Wendroff, Burton. Systems of conservation laws. Communications on
Pure and Applied mathematics, 13(2):217–237, 1960.
LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The mnist dataset of
handwritten digits. URL http://yann. lecun. com/exdb/mnist, 1998.
LeVeque, Randall J. Finite volume methods for hyperbolic problems, volume 31. Cambridge university press, 2002.
Liberzon, Daniel. Calculus of variations and optimal control theory: a concise introduction. Princeton University Press, 2012.
Milstein, GN. Approximate integration of stochastic differential equations. Theory of
Probability & Its Applications, 19(3):557–562, 1975.
Milstein, GN. A method of second-order accuracy integration of stochastic differential
equations. Theory of Probability & Its Applications, 23(2):396–401, 1979.
Milstein, GN. Weak approximation of solutions of systems of stochastic differential
equations. Theory of Probability & Its Applications, 30(4):750–766, 1986.
Milstein, GN. Numerical integration of stochastic differential equations, volume 313.
Springer Science & Business Media, 1995.
23

Noh, WF and Protter, MH. Difference methods and the equations of hydrodynamics.
Technical report, California. Univ., Livermore. Lawrence Radiation Lab., 1960.
Oksendal, Bernt. Stochastic differential equations: an introduction with applications.
Springer Science & Business Media, 2013.
Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC Press,
1987.
Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. In ICML
(3), volume 28, pp. 343–351, 2013.
Talay, Denis and Tubaro, Luciano. Expansion of the global error for numerical schemes
solving stochastic differential equations. Stochastic analysis and applications, 8(4):
483–509, 1990.
Uhlenbeck, George E and Ornstein, Leonard S. On the theory of the Brownian motion.
Physical review, 36(5):823, 1930.
Warming, RF and Hyett, BJ. The modified equation approach to the stability and
accuracy analysis of finite-difference methods. Journal of computational physics,
14(2):159–179, 1974.

24

Train acc

1.0
0.8
0.6
0.4
0.2
0.00

cSGD

Adagrad
ηu0

η

1e-2
1e-1
5e-1

50 100 150
Epoch

Adam
η

5e-1
5e-2
1e-2

0

50 100 150
Epoch

5e-2
1e-2
5e-4

0

50 100 150
Epoch

Train acc

(a) C1, Learning rate adjustments (c.f. main paper Fig . 3)

1.0
0.8
0.6
0.4
0.2
0.00

cMSGD

MSGD

µ0

µ

0.95
0.9
0.999

50 100 150
Epoch

MSGD-A
µmax

0.999
0.99
0.95

0

50 100 150
Epoch

0.999
0.99
0.9

0

50 100 150
Epoch

Train acc

(b) C1, Momentum adjustments (c.f. main paper Fig . 5)

1.0
0.8
0.6
0.4
0.2
0.00

cMSGD

MSGD
η

η

2e-1
1e-1
2e-2

50 100 150
Epoch

MSGD-A
η

1e-1
1e-2
1e-3

0

50 100 150
Epoch

1e-1
1e-2
1e-3

0

50 100 150
Epoch

(c) C1, Learning rate sensitivity (c.f. main paper Fig . 6)

Figure 2: Training accuracies for various methods and hyper-parameter choices. The
set-up is the same as in the main paper, Fig. 3,5,6 except that we plot training accuracy
instead of test accuracy. The qualitative observation is the same: cSGD and cMSGD
are generally robust to changing parameters and models.

25

