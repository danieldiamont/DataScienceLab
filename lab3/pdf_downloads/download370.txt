Supplementary materials for
Stochastic modified equations and the dynamics
of stochastic gradient algorithms

A

Modified equations in the numerical analysis of PDEs

The method of modified equations is widely applied in finite difference methods in
numerical solution of PDEs Hirt (1968); Noh & Protter (1960); Daly (1963); Warming
& Hyett (1974). In this section, we briefly demonstrate this classical method. Consider
the one dimensional transport equation
âˆ‚u
âˆ‚u
=c
âˆ‚t
âˆ‚x

(1)

where u : [0, T ] Ã— [0, L] â†’ R represents a density of some material in [0, L] and
c > 0 is the transport velocity. It is well-known that the simple forward-time-centralspace differencing leads to instability for all discretization step-sizes (LeVeque, 2002).
Instead, more sophisticated differencing schemes must be used.
We set time and space discretization steps to âˆ†t and âˆ†x and denote u(nâˆ†t, jâˆ†x) =
Un,j for 1 â‰¤ n â‰¤ N and 1 â‰¤ j â‰¤ J. The simplest scheme that can exhibit stability is
the upwind scheme (Courant et al., 1952), where we approximate (1) by the difference
equation


+ Un,j+1 âˆ’ Un,j
âˆ’ Un,j âˆ’ Un,jâˆ’1
Un+1,j = Un,j + âˆ†t c
+c
,
(2)
âˆ†x
âˆ†x
where c+ = max(c, 0) + câˆ’ = min(c, 0). The idea is to now approximate this difference scheme by another continuous PDE, that is not equal to the original equation (1)
for non-zero âˆ†x, âˆ†t. This can be done by Taylor expanding each term in (2) around
u(t, x) = Un,j . Simplifying and truncating to leading term in âˆ†t, âˆ†x, we obtain the
modified equation
âˆ‚u
âˆ‚u
1
âˆ‚2u
âˆ’c
= câˆ†x(1 âˆ’ r) 2 ,
(3)
âˆ‚t
âˆ‚x
2
âˆ‚x
where r = câˆ†t/âˆ†x is the Courant-Friedrichs-Lewy (CFL) number (Courant et al.,
1952). Notice that in the limit âˆ†t, âˆ†x â†’ 0 with r fixed, one recovers the original
1

transport equation, but for finite step sizes, the upwind scheme is really described by
the modified equation (3). In other words, this truncated equation describes the leading
order, non-trivial behavior of the finite difference scheme.
From the modified equation (3), one can immediately deduce a number of interesting properties. First, the error from the upwind scheme is diffusive in nature, due to
the presence of the second order spatial derivative on the right hand side. Second, we
observe that if the CFL number r is greater than 1, then the coefficient for the diffusive term becomes negative and this results in instability. This is the well-known CFL
condition. This places a fundamental limit on the spatial resolution for fixed temporal resolution with regards to the stability of the algorithm. Lastly, the error term is
proportional to âˆ†x for fixed r, thus it may be considered a first order method.
Now, another possible proposal for discretizing (1) is the Lax-Wendroff (LW) scheme (Lax
& Wendroff, 1960):


Un,j+1 âˆ’ 2Un,j + Un,jâˆ’1
Un,j+1 âˆ’ Un,jâˆ’1
âˆ’ câˆ†t
, (4)
Un+1,j = Un,j + âˆ†t c
2âˆ†x
2âˆ†x2
whose modified equation is
âˆ‚u
1
âˆ‚3u
âˆ‚u
âˆ’c
= câˆ†x2 (r2 âˆ’ 1) 3 .
âˆ‚t
âˆ‚x
6
âˆ‚x

(5)

Comparing with (3), we observe that the LW scheme error is of higher order (âˆ†x2 ), but
at the cost of introducing dispersive, instead of diffusive errors due to the presence of
the third derivative. These findings are in excellent agreement with the actual behavior
of their respective discrete numerical schemes (Warming & Hyett, 1974).
We stress here that if we simply took the trivial leading order, the right hand sides of
(3) and (5) disappear and vital information, including stability, accuracy and the nature
of the error term will be lost. The ability to capture the effective dynamical behavior of
finite difference schemes is the key strength of the modified equations approach, which
has become the primary tool in analyzing and improving finite difference algorithms.
The goal of our work is to extend this approach to analyze stochastic algorithms.

B

Summary of SDE terminologies and results

Here, we summarize various SDE terminologies and results we have used throughout
the main paper and also subsequent derivations. A particular important result is the
ItoÌ‚ formula (Sec. B.2), which is used throughout this work for deriving moment equations. For a thorough reference on the subject of stochastic calculus and SDEs, we
suggest Oksendal (2013).

B.1

Stochastic differential equations

Let T > 0. An ItoÌ‚ stochastic differential equation on the interval [0, T ] is an equation
of the form
dXt = b(Xt , t)dt + Ïƒ(Xt , t)dWt ,
X0 = x0 ,
(6)
2

where Xt âˆˆ Rd , b : Rd Ã— [0, T ] â†’ Rd , Ïƒ : Rd Ã— [0, T ] â†’ RdÃ—l and Wt is a ldimensional Wiener process, or Brownian motion. This is a mathematically sound way
of expressing the intuitive notion of SDEs being ODEs plus noise:
XÌ‡t = b(Xt , t) + â€œnoiseâ€

(7)

The equation (6) is really a â€œshorthandâ€ for the integral equation
Z t
Z t
Ïƒ(Xs , s)dWs .
b(Xs , s)ds +
Xt âˆ’ x0 =
The last integral is defined in the ItoÌ‚ sense, i.e.
Z t
X
Fsiâˆ’1 (Wsi âˆ’ Wsiâˆ’1 ),
Fs dWs := lim
nâ†’âˆž

0

(8)

0

0

(9)

[siâˆ’1 ,si ]âˆˆÏ€n

where Ï€n is a sequence of n-partitions of [0, t] and the limit represents convergence in
probability. In (6), b is known as the drift, and Ïƒ is known as the diffusion matrix. When
they satisfy Lipschitz conditions, one can show that (6) (or (8)) has a unique strong
solution (Oksendal (2013), Chapter 5). For our purposes in this paper, we consider the
special case where b, Ïƒ do not depend on time and we set d = l so that Ïƒ is a square
matrix.
To perform calculus, we need an important result that generalizes the notion of
chain rule to the stochastic setting.

B.2

ItoÌ‚ formula

ItoÌ‚ formula, also known as ItoÌ‚â€™s lemma, is the extension of the chain rule of ordinary
calculus to the stochastic setting. Let Ï† âˆˆ C 2,1 (Rd Ã— [0, T ]) and let Xt be a stochastic
process satisfying the SDE (6), and thus (8). Then, the stochastic process Ï†(Xt , t) is
again an ItoÌ‚ process satisfying



1
dÏ†(Xt , t) = âˆ‚t Ï†(Xt , t) + âˆ‡Ï†(Xt , t)T b(Xt , t) + Tr[Ïƒ(t, Xt )T HÏ†(t, Xt )Ïƒ(t, Xt )] dt
2


T
+ âˆ‡Ï†(Xt , t) Ïƒ(Xt , t) dWt ,
(10)
where âˆ‡ denotes gradient with respect to the first argument and HÏ† denotes the Hessian, i.e. HÏ†(ij) = âˆ‚ 2 Ï†/âˆ‚x(i) âˆ‚x(j) . The formula (10) is the ItoÌ‚ formula. If Ï† is not
a scalar but a vector, then each of its component satisfy (10). Note that if Ïƒ = 0, this
reduces to the chain rule of ordinary calculus.

B.3

The Ornstein-Uhlenbeck process

An important solvable SDE is the Ornstein-Uhlenbeck (OU) process Uhlenbeck &
Ornstein (1930). Consider d = 1, b(x, t) = Î¸(Î¾ âˆ’ x) and Ïƒ(x, t) = Ïƒ > 0, with Î¸ > 0,
Ïƒ > 0 and Î¾ âˆˆ R. Then we have the SDE
dXt = Î¸(Î¾ âˆ’ Xt )dt + ÏƒdWt ,
3

X0 = x0 .

(11)

To solve this equation, we change variables x 7â†’ Ï†(x, t) = xeÎ¸t . Applying ItoÌ‚ formula,
we have
dÏ†(Xt , t) = Î¸Î¾eÎ¸t dt + ÏƒeÎ¸t dWt ,
(12)
which we can integrate from 0 to T to get
Xt = x0 e

âˆ’Î¸t

âˆ’Î¸t

+ Î¾(1 âˆ’ e

Z
)+Ïƒ

t

eâˆ’Î¸(tâˆ’s) dWs .

(13)

0

This is a path-wise solution to the SDE (11). To infer distributional properties, we
do not require such precise solutions. In fact, we only need the distribution of the
random variable Xt at any fixed time t âˆˆ [0, T ]. Observe that Xt is really a Gaussian
process, since the integrand in the Wiener integral is deterministic. Hence, we need
only calculate its moments. Taking expectation on (13), we get
EXt = x0 eâˆ’Î¸t + Î¾(1 âˆ’ eâˆ’Î¸t ).

(14)

To obtain the covariance function, we see that
Z t

Z t
E(Xt âˆ’ EXt )(Xs âˆ’ EXs ) = Ïƒ 2 E
eÎ¸(uâˆ’s) dWu
eÎ¸(vâˆ’s) dWv .
0

(15)

0

This can be evaluated by using ItoÌ‚â€™s isometry, which says that for any Wt adapted
process Ï†t , Ïˆt , we have
Z t

Z t

Z t
E
Ï†u dWu
Ïˆv dWv = E
Ï†s Ïˆs ds .
(16)
0

0

0

We get, for s â‰¤ t
cov(Xs , Xt ) =


Ïƒ 2  âˆ’Î¸|tâˆ’s|
e
+ eâˆ’Î¸|t+s| ,
2Î¸

(17)

and in particular, for fixed t âˆˆ [0, T ], we have
Var(Xt ) =

Ïƒ2
(1 âˆ’ eâˆ’2Î¸t ).
2Î¸

(18)

Hence, we have
Xt âˆ¼ N



Ïƒ2
x0 eâˆ’Î¸t + Î¾(1 âˆ’ eâˆ’Î¸t ), (1 âˆ’ eâˆ’2Î¸t ) .
2Î¸

(19)

In Sec. 3.1 in the main paper, the solution of the SME is the OU process with Î¸ =
âˆš
2(1 + Î·), Î¾ = 0, Ïƒ = 2 Î·. Making these substitutions, we obtain


Î· 
Xt âˆ¼ N x0 eâˆ’2(1+Î·)t ,
1 âˆ’ eâˆ’4(1+Î·)t
.
1+Î·

4

B.4

Numerical solution of SDEs

Unfortunately, most SDEs are not amenable to exact solutions. Often, we resort to
numerical methods. The simplest method is the Euler-Maruyama method. This extends
the Euler method for ODEs to SDEs. Fix a time discretization size Î´ > 0 and define
XÌƒk = XkÎ´ , then we can iterate the finite difference equation
XÌƒk+1 = XÌƒk + Î´b(XÌƒk , kÎ´) + Ïƒ(XÌƒk , kÎ´)(W(k+1)Î´ âˆ’ WkÎ´ ).

(20)

By definition, W(k+1)Î´ âˆ’ WkÎ´ âˆ¼ N (0, Î´I), and are independent for each k. Here, I is
the identity matrix. Hence, we have the Euler-Maruyama scheme
âˆš
(21)
XÌƒk+1 = XÌƒk + Î´b(XÌƒk , kÎ´) + Î´Ïƒ(XÌƒk , kÎ´)Zk ,
i.i.d.

where Zk âˆ¼ N (0, I).
One can show that the Euler-Maruyama method (21) is a first order weak approximation (c.f. Def. 1 in main paper) to the SDE (6). However, it is only a order 1/2
scheme in the strong sense (Kloeden & Platen, 2011), i.e.
1

E|XkÎ´ âˆ’ XÌƒkÎ´ | < CÎ´ 2 .

(22)

With more sophisticated methods, one can design higher order schemes (both in the
strong and weak sense), see Milstein (1986).

B.5

Stochastic asymptotic expansion

Besides numerics, if there exists small parameters in the SDE, we can proceed with
stochastic asymptotic expansions Freidlin et al. (2012). This is the case for the SME,
which has a small Î· 1/2 multiplied to the noise term. Let us consider a time-homogeneous
SDE of the form
dXt = b(Xt )dt + Ïƒ(Xt )dWt
(23)
where   1. The idea is to follow standard asymptotic analysis and write Xt as an
asymptotic series
Xt = X0,t + X1,t + 2 X2,t + . . . .
(24)
We substitute (24) into (23) and assuming smoothness of b and Ïƒ, we expand
b (Xt ) = b(X0,t ) + âˆ‡b(X0,t )X1,t + O(2 )
Ïƒ(Xt ) = Ïƒ(X0,t ) + âˆ‡Ïƒ(X0,t )X1,t + O(2 )

(25)

to get
dX0,t = b(X0,t )dt,
dX1,t = âˆ‡b(X0,t )X1,t dt + Ïƒ(X0,t )dWt ,
..
.

5

(26)

and X0,0 = x0 , X1,0 = 0. In general, the equation for Xi,t are linear stochastic differential equations with time-dependent coefficients depending on {X0,t , X1,t , . . . , Xiâˆ’1,t }
and the initial conditions are X0,0 = x0 , Xi,0 = 0 for all i â‰¥ 1. Hence, the asymptotic
equations can be solved sequentially to obtain an estimate of Xt to arbitrary order in .
The equations for higher order terms become messy quickly, but they are always linear
in the unknown, as long as all the previous equations are solved. For more details on
stochastic asymptotic expansions, the reader is referred to Freidlin et al. (2012).

B.6

Asymptotics of the SME

âˆš
We now derive the first two asymptotic equations of the SME. we take  = Î·,
b = âˆ’âˆ‡f (O(Î·) term can be ignored for first two terms) and Ïƒ = Î£1/2 . Then, (26)
becomes
dX0,t = âˆ’âˆ‡f (X0,t )dt,

(27)
1
2

dX1,t = âˆ’Hf (X0,t )X1,t dt + Î£(X0,t ) dWt ,

(28)

where Hf(ij) = âˆ‚(i) âˆ‚(j) f is the Hessian of f .
In the following analysis, we shall assume that the truncated series approximation
âˆš
(29)
XÌ‚t = X0,t + Î·X1,t ,
where X0,t , X1,t satisfy (27) and (28), describes the leading order stochastic dynamics
of the SGD. Now, let us analyze the asymptotic equations in detail. First, we assume
that the ODE (27) has a unique solution X0,t , t â‰¥ 0 with X0,0 = x0 . This is true if for
example, âˆ‡f is locally Lipschitz. Next, let us define the non-random functions
Ht = Hf (X0,t ),
Î£t = Î£(X0,t ).

(30)

Both H and Ïƒ are dÃ—d matrices for each t. Then, (28) becomes the time-inhomogeneous
linear SDE
1
(31)
dX1,t = âˆ’Ht X1,t + Î£t2 dWt ,
with X1,0 = 0. Since the drift is linear and the diffusion matrix is constant (i.e. independent of X1,t ), X1,t is a Gaussian process. Hence we need only calculate its mean
and covariance using ItoÌ‚ formula (see B.2). We have
EX1,t = 0,

(32)

and the covariance matrix St = Cov(X1,t ) satisfies the differential equation
d
St = âˆ’St Ht âˆ’ Ht St + Î£t ,
(33)
dt
with S0 = 0. This equation is a linearized version of the Riccati equation and there are
simple closed-form solutions under special conditions, e.g. d = 1 or Ht is constant.
Hence, we conclude that the asymptotic approximation XÌ‚t is a Gaussian process
with distribution
XÌ‚t âˆ¼ N (X0,t , Î·St ),
(34)
where X0,t solves the ODE (27) and St solves the ODE (33), with Ht , Î£t given by (30).
6

Remark 1. At this point, it is important to discuss the validity of the asymptotic approximation (34), and the SME approximation (35) in general. What we prove in Sec. C
and is shown in Freidlin et al. (2012) is that for fixed T , we can take Î· = Î·(T ) small
enough so that the SME and its asymptotic expansion is a good approximation of the
distribution of the SGD iterates. What we did not prove is that for fixed Î·, the approximations hold for arbitrary T . In particular, it is not hard to construct systems where
for fixed Î·, both the SME and the asymptotic expansion fails when T is large enough.
To prove the second general statement requires further assumptions, particularly on
the distribution of fi â€™s. This is out of the scope of the current work.

C

Formal Statement and proof of Thm. 1

Theorem 1 (Stochastic modified equations). Let Î± âˆˆ {1, 2}, 0 < Î· < 1, T > 0 and
set N = bT /Î·c. Let xk âˆˆ R, 0 â‰¤ k â‰¤ N denote a sequence of SGD iterations defined
by (2). Define Xt âˆˆ Rd as the stochastic process satisfying the SDE
1
1
(35)
dXt = âˆ’âˆ‡(f (Xt ) + (Î± âˆ’ 1)Î·|âˆ‡f (Xt )|2 )dt + (Î·Î£(Xt )) 2 dWt
4
Pn
X0 = x0 and Î£(x) = n1 i=1 (âˆ‡f (x) âˆ’ âˆ‡fi (x))(âˆ‡f (x) âˆ’ âˆ‡fi (x))T .
Fix some test function g âˆˆ G (c.f. Def. 1 in main paper). Suppose further that the
following conditions are met:

(i) âˆ‡f, âˆ‡fi satisfy a Lipschitz condition: there exists L > 0 such that
|âˆ‡f (x) âˆ’ âˆ‡f (y)| +

n
X

|âˆ‡fi (x) âˆ’ âˆ‡fi (y)| â‰¤ L|x âˆ’ y|.

i=1

(ii) f, fi and its partial derivatives up to order 7 belong to G.
(iii) âˆ‡f, âˆ‡fi satisfy a growth condition: there exists M > 0 such that
|âˆ‡f (x)| +

n
X

|âˆ‡fi (x)| â‰¤ M (1 + |x|).

i=1

(iv) g and its partial derivatives up to order 6 belong to G.
Then, there exists a constant C > 0 independent of Î· such that for all k = 0, 1, . . . , N ,
we have
|Eg(XkÎ· ) âˆ’ Eg(xk )| â‰¤ CÎ· Î± .
That is, the equation (35) is an order Î± weak approximation of the SGD iterations.
The basic idea of the proof is similar to the classical approach in proving weak convergence of discretization schemes of SDEs outlined in the seminal papers by Milstein
(Milstein (1975, 1979, 1986, 1995)). The main difference is that we wish to establish
that the continuous SME is an approximation of the discrete SGD, instead of the other
7

way round, which is the case dealt by classical approximation theorems of SDEs with
finite difference schemes. In the following, we first show that a one-step approximation
has order Î· Î±+1 error, and then deduce, using the general result in Milstein (1986), that
the overall global error is of order Î· Î± .
It is well known that a second order weak convergence discretization scheme for
a SDE is not trivial. The classical Euler-Maruyama scheme, as well as the Milstein
scheme are both first order weak approximations. However, in our case the problem
simplifies significantly. This is because the noise we are trying to model is small, so
that from the outset, we may assume that b(x) = O(1) but Ïƒ(x) = O(Î· 1/2 ), i.e. we
set Ïƒ(x) = Î· 1/2 ÏƒÌƒ(x) where ÏƒÌƒ = O(1) and deduce the appropriate expansions. For
brevity, in the following we will drop the tilde and simply denote the noise term of the
SDE by Î· 1/2 Ïƒ.
In the subsequent proofs we will make repeated use of Taylor expansions in powers
of Î·. To simplify presentation, we introduce the shorthand that whenever we write
O(Î· Î± ), we mean that there exists a function K(x) âˆˆ G (c.f. Def. 1 in main text) such
that the error terms are bounded by K(x)Î· Î± . For example, we write
b(x + Î·) = b0 (x) + Î·b1 (x) + O(Î· 2 )

(36)

to mean: there exists K âˆˆ G such that
|b(x + Î·) âˆ’ b0 (x) âˆ’ Î·b1 (x)| â‰¤ K(x)Î· 2 .

(37)

These results can be deduced easily using Taylorâ€™s theorem with a variety of forms of
the remainder, e.g. Lagrange form. We omit such routine calculations. We also denote
the partial derivative with respect to x(i) by âˆ‚(i) .
First, let us prove a lemma regarding moments of SDEs with small noise.
Lemma 1. Let 0 < Î· < 1. Consider a stochastic process Xt , t â‰¥ 0 satisfying the SDE
1

dXt = b(Xt ) + Î· 2 Ïƒ(Xt )dWt

(38)

with X0 = x âˆˆ Rd and b, Ïƒ together with their derivatives belong to G. Define the
one-step difference âˆ† = XÎ· âˆ’ x, then we have
Pd
(i) Eâˆ†(i) = b(i) Î· + 21 [ j=1 b(j) âˆ‚(j) b(i) ]Î· 2 + O(Î· 3 ).
T
(ii) Eâˆ†(i) âˆ†(j) = [b(i) b(j) + ÏƒÏƒ(ij)
]Î· 2 + O(Î· 3 ).
Qs
(iii) E j=1 âˆ†(ij ) = O(Î· 3 ) for all s â‰¥ 3, ij = 1, . . . , d.

All functions above are evaluated at x.
Proof. One way to establish (i)-(iii) is to employ the Ito-Taylor expansion (see Kloeden
& Platen (2011), Chapter 5) on the random variable XÎ· around x and calculating the
moments. Here, we will employ instead the method of semigroup expansions (see Hille
& Phillips (1996), Chapter XI), which works directly on expectation functions. The

8

generator of the stochastic process (38) is the operator L acting on sufficiently smooth
functions Ï† : Rd â†’ R, and is defined by
LÏ† =

d
X

d
1 X
T
âˆ‚(i) âˆ‚(j) Ï†,
b(i) âˆ‚(i) Ï† + Î· 2
ÏƒÏƒ(ij)
2
i=1
i,j=1

(39)

A classical result on semigroup expansions (Hille & Phillips (1996), Chapter XI) states
that if Ï† and its derivatives up to order 6 belong to G, then
1
EÏ†(XÎ· ) = Ï†(x) + LÏ†(x)Î· + L2 Ï†(x)Î· 2 + O(Î· 3 ).
2

(40)

Now, let t âˆˆ Rd and consider the moment-generating function (MGF)
M (t) = EetÂ·âˆ† .

(41)

To ensure its existence we may instead set t to be purely imaginary, i.e. t = is where s
is real. Then, (41) is known as the characteristic function (CF). The important property
we make use of is that the moments of âˆ† are found by differentiating the MGF (or CF)
with respect to t. In fact, we have

s
Y
âˆ‚ s M (t) 
(42)
E
âˆ†(ij ) = Qs
 ,
j=1 âˆ‚t(ij ) t=0
j=1
where ij = 1, . . . , d. We now expand M (t) in powers of Î· using formula (40). We get,
ï£®
ï£¹
d
d
X
X
1
M (t) =1 + ï£°
b(i) t(i) Î· +
b(i) t(j) âˆ‚(i) b(j) Î· 2 ï£»
2
i=1
i,j=1
ï£¹
ï£®
d
d
X
X
1
1
T
(43)
b(i) t(i) )2 +
ÏƒÏƒ(ij)
t(i) t(j) ï£» + O(Î· 3 ).
+ ï£° Î·2 (
2
2
i=1
i,j=1
All functions are again evaluated at x. Finally, we apply formula (42) to deduce (i)(iii).
Next, we have an equivalent result for one SGD iteration.
Lemma 2. Let 0 < Î· < 1. Consider xk , k â‰¥ 0 satisfying the SGD iterations
xk+1 = xk âˆ’ Î·âˆ‡fÎ³k (xk )
Â¯ = x1 âˆ’ x, then we have
with x0 = x âˆˆ Rd . Define the one-step difference âˆ†
Â¯ (i) = âˆ’âˆ‚(i) f Î·
(i) Eâˆ†
Â¯ (i) âˆ†
Â¯ (j) = âˆ‚(i) f âˆ‚(j) f Î· 2 + Î£(ij) Î· 2 .
(ii) Eâˆ†
Qs Â¯
3
(iii) E j=1 âˆ†a
(ij ) = O(Î· ) for all s â‰¥ 3, ij = 1, . . . , d.
9

(44)

where Î£ =

1
n

Pn

âˆ’ âˆ‡fi )(âˆ‡f âˆ’ âˆ‡fi )T . All functions above are evaluated at x.

i=1 (âˆ‡f

Proof. From definition (44) and the definition of Î£, the results are immediate.
Now, we will need a key result linking one step approximations to global approximations due to Milstein. We reproduce the theorem, tailored to our problem, below.
The more general statement can be found in Milstein (1986).
Theorem 2 (Milstein, 1986). Let Î± be a positive integer and let the assumptions in
Theorem 1 hold. If in addition there exists K1 , K2 âˆˆ G so that
|E

s
Y

âˆ†(ij ) âˆ’ E

j=1

s
Y

Â¯ (i ) | â‰¤ K1 (x)Î· Î±+1 ,
âˆ†
j

j=1

for s = 1, 2, . . . , 2Î± + 1 and
E

2Î±+2
Y

Â¯ (i ) | â‰¤ K2 (x)Î· Î±+1 .
|âˆ†
j

j=1

Then, there exists a constant C so that for all k = 0, 1, . . . , N we have
|Eg(XkÎ· ) âˆ’ Eg(xk )| â‰¤ CÎ· Î±
Proof. See Milstein (1986), Theorem 2 and Lemma 5.

Proof of Theorem 1
We are now ready to prove theorem 1 by checking the conditions in theorem 2 with
Î± = 1, 2. The second condition is implied by Lemma 2. The first condition is implied
by Lemma 1 and Lemma 2 with the choice
1
b(x) = âˆ’âˆ‡(f (x) + Î·(Î± âˆ’ 1)|âˆ‡f (x))|2 ,
4
1
Ïƒ(x) = Î£(x) 2 .

To illustrate our approximation result, let us calculate, using Monte-Carlo simulations, the weak error of the SME approximation
Ew = |Eg(XN Î· ) âˆ’ Eg(xN )|,

(45)

for Î± = 1, 2 v.s. Î· for different f, fi and generic polynomial test functions g. The
results are shown in Fig. 1. We see that we have order Î± weak convergence, even when
some conditions of the above theorem are not satisfied (Fig. 1(b)).

10

Î±=1
Î±=2

10 -1

10 1

Slope=1
Slope=2

Ew

Ew

10 1
10 -3
10 -5 10 0

10 -1
Î·

Î±=1
Î±=2

10 -1
10 -3

-5
10 -2 10 10 0

(a)

Slope=1
Slope=2

10 -1
Î·

10 -2

(b)

Figure 1: Weak error Ew , as defined in (45) with Î± = 1, 2, v.s. learning rate Î· for two
different choices of f, fi . All errors are averaged over 1012 samples of SGD trajectories
up to T = 1.0. The initial condition is x0 = 1. The SMEs moments are solved exactly
since they involve linear drifts. (a) Quadratic objective with n = 2, fi = (x âˆ’ Î³i )2
where Î³i âˆˆ {Â±1}. The total objective is f (x) = x2 + 1. The test function is g(x) =
x + x2 + x3 . (b) Non-convex fi â€™s with n = 2, fi (x) = (x âˆ’ Î³i )2 + Î³i x3 where
Î³i âˆˆ {Â±1}. The total objective is the same f (x) = x2 + 1. We chose g(x) = x so that
Eg(XT ) has closed form solution. Note that for this choice of fi , the condition (iii) of
Theorem 1 is not satisfied. Nevertheless, in both cases, we observe that the weak error
decreases with Î· like Ew âˆ¼ Î· Î± .
Remark 2. From above, we also observe that if we pick b(x) = âˆ’âˆ‡f (x) and Ïƒ(x)
to be any function in G (and its sufficiently high derivatives are also in G), then we
have matching moments up to order Î· 2 and hence we can conclude that for this choice,
the resulting SDE is a first order weak approximation of the SGD, with |Eg(XkÎ· ) âˆ’
Eg(xk )| â‰¤ CÎ·, k = 0, 1, . . . , N . In particular, the deterministic gradient flow is a first
order weak approximation of the SGD. Hence, just like traditional modified equations,
our SME (35) (Î± = 2) is the next order approximation of the underlying algorithm.
However, we stress that for our first order SME with Î± = 1, i.e. the choice
1
b = âˆ’âˆ‡f and Ïƒ = Î£ 2 , the fact that we did not the improve the order of weak convergence from the deterministic gradient flow does not mean that this is a equally bad
approximation. The constant C in the weak error depends on the choice of Î£ and in
fact, it can be shown empirically that with this choice, we do have lower weak error
|Eg(XkÎ· ) âˆ’ Eg(xk )|, but the order of convergence of the weak error as Î· â†’ 0 is the
same. An analytical justification must then rely on using the ItoÌ‚-Taylor expansion to
obtain precise estimates for the factor C (see e.g. Talay & Tubaro (1990)). This is
beyond the scope of the current paper.
Remark 3. The Lipschitz condition (i) is to ensure that the SME has a unique strong
solution with uniformly bounded moments Milstein (1986). If we allow weak solutions
and establish uniform boundedness of moments by other means (more assumptions on
the growth and direction of âˆ‡f for large x), then condition (i) is expected to be relaxed
although the technical details will be tedious.
Condition (iii) in Theorem 1 appears to be the most stringent one and in fact it may
limit applications to problems with objectives that have more than quadratic growth.
11

However, closer inspection tells us it can also be relaxed. For example, if there exists
an invariant distribution that concentrates on a compact subset of Rd then as Î· â†’ 0,
xk â€™s would be bounded with high probability, and hence for large x we may replace
f, fi with a version that satisfies the growth condition in (iii). Further work is needed
to make this precise but we can already see in Fig. 1(b) that we have quadratic weak
convergence even when (iii) is not satisfied.
Remark 4. The regularity conditions on f and g in Theorem 1 are inherited from Theorem 2 in Milstein (1986). For smooth objectives, polynomial growth conditions are
usually not restrictive. Still, with care, these should be relaxed since in our case the
small noise helps to reduce the number of terms containing higher derivatives in various Taylor and ItoÌ‚-Taylor expansions. Proving a more general version of Theorem 1
will be left as future work.

D

Derivation of SMEs

In this section, we include more detailed derivations of the SMEs used in the main
paper. For brevity, we do not include rigorous proofs of approximation statements for
SGD variants in Sec. D.2 and D.3, but only heuristic justifications. Proving rigorous
statements for these approximations can be done by modifying the proof of Thm. 1.

D.1

SME for the simple quadratic example

We start with the example in Sec. 3.1 of the main paper. Let n = 2, d = 1 and set
f (x) = x2 + 1 with f1 (x) = (x âˆ’ 1)2 and f2 (x) = (x + 1)2 . The SGD iterations picks
at random between f1 and f2 and performs descent with their respective gradients.
Recall that the (second order) SME is given by
1
1
dXt = âˆ’(f 0 (Xt ) + Î·f 0 (Xt )f 00 (Xt ))dt + (Î·Î£(Xt )) 2 dWt .
2

(46)

Now, f 0 (x) = 2x, f 00 (x) = 2 and
2

1X 0
(f (x) âˆ’ fi (x))2 = 4
2 i=1 i

(47)

âˆš
dXt = âˆ’2(1 + Î·)Xt dt + 2 Î·dWt .

(48)

Î£(x) =
and hence the SME is

D.2

SME for learning rate adjustment

The SGD iterations with learning rate adjustment is
xk+1 = xk âˆ’ Î·uk f 0 (xk ),

(49)

where uk âˆˆ [0, 1] is the learning rate adjustment factor. Î· is the maximum allowed
learning rate. There are two reasons we introduce this hyper-parameter. First, gradients
12

cannot be arbitrarily large since that will cause instabilities. Second, the SME is only
an approximation of the SGD for small learning rates, and so it is hard to justify the
approximation for large Î·.
In this case, deriving the corresponding SME is extremely simple. Notice that we
can define gi,k (xk ) = uk fi (xk ), gk = uk f (xk ). Then, the iterations above is simply
xk+1 = xk âˆ’ Î·gÎ³0 k ,k (xk ),

(50)

whose (first order) SME is by Thm. 1
1

dXt = âˆ’g 0 (Xt )dt + (Î·u2t Î£(Xt )) 2 dWt .

(51)

And hence the SME for SGD with learning rate adjustments is
1

dXt = âˆ’ut f 0 (Xt )dt + ut (Î·Î£(Xt )) 2 dWt .

D.3

(52)

SME for SGD with momentum

First let us consider the constant momentum parameter case. The SGD with momentum
is the paired update
vk+1 = Âµvk âˆ’ Î·fÎ³0 k (xk ),
xk+1 = xk + vk+1 .
To derive and SME, notice that we can write the above as



1âˆ’Âµ
0
vk+1 = vk + Î· âˆ’
vk âˆ’ f (xk ) + Î· f 0 (xk ) âˆ’ fÎ³0 k (xk ) ,
Î·


vk+1
.
xk+1 = xk + Î·
Î·

(53)

(54)

Recall that since we are looking at first order weak approximations, it is sufficient to
compare to the Euler-Maruyama discretization (Sec. B.4). We observe that the above
can be seen as an Euler-Maruyama discretization of the coupled SDE
dVt = (âˆ’
dXt =

1
1âˆ’Âµ
Vt âˆ’ f 0 (Xt ))dt + (Î·Î£(Xt )) 2 dWt ,
Î·

1
Vt dt,
Î·

(55)

with the usual choice of Î£(x). Hence, this is the first order SME for the SGD with
momentum having a constant momentum parameter Âµ. For time-varying momentum
parameter, we just replace Âµ by Âµt to get
dVt = (âˆ’
dXt =

1
1 âˆ’ Âµt
Vt âˆ’ f 0 (Xt ))dt + (Î·Î£(Xt )) 2 dWt ,
Î·

1
Vt dt.
Î·

(56)
13

E
E.1

Solution of optimal control problems
Brief introduction to optimal control

We first introduce some basic terminologies and results on optimal control theory to
pave way for our solutions to optimal control problems for the learning rate and momentum parameter. For simplicity, we restrict to one state dimension (d = 1), but
similar equations hold for multiple dimensions. For a more thorough introduction
to optimal control theory and calculus of variations, we refer the reader to Liberzon
(2012).
Let t âˆˆ [0, T ] and consider the ODE
d
zt = Î¦(zt , ut ),
dt

(57)

where zt , ut âˆˆ R and Î¦ : R Ã— R â†’ R. The variable zt describes the evolution of some
state and ut is the control variable, which can affect the state dynamics. Consider the
control problem of minimizing the cost functional
Z T
C(u) =
L(zs , us )ds + G(zT )
(58)
0

with respect to ut , subject to zt satisfying the ODE (57) with prescribed initial condition z0 âˆˆ R. The function L is known as the running cost and G is the terminal
cost. Usually, we also specify some control set U âŠ‚ R so that we only consider
u : [0, T ] â†’ U . The full control problem reads
min
u:[0,T ]â†’U

C(u) subject to (57).

(59)

Note that additional path constraints can also be added and (57) can also be made timeinhomogeneous, but for our purposes it is sufficient to consider the above form.
There are two principal ways of solving optimal control problems: either dynamic
programming through the Hamilton-Jacobi-Bellman (HJB) equation (Bellman, 1956),
or using the Pontryaginâ€™s maximum principle (PMP) (Pontryagin, 1987). In this section, we will only discuss the HJB method as this is the one we employ to solve the
relevant control problems in this paper.

E.2

Dynamic programming and the HJB equation

The first way to solve (59) is through the dynamic programming principle. For t âˆˆ
[0, T ] and z âˆˆ R, define the value function
Z T
V (z, t) = min
L(zs , us )ds + G(zT ),
u:[t,T ]â†’U

subject to
d
zs = Î¦(zs , us ),
ds
z(t) = z.

t

s âˆˆ [t, T ],
(60)
14

Notice that if there exists a solution to (59), then the value of the minimum cost is
V (z0 , 0). The dynamic programming principle allows us to derive a recursion on the
function V , in the form of a partial differential equation (PDE)
âˆ‚t V (z, t) + min{âˆ‚z V (z, t)Î¦(z, u) + L(z, u)} = 0,
uâˆˆU

V (T, z) = G(z).

(61)

This is known as the Hamilton-Jacobi-Bellman equation (HJB). Note that this PDE is
solved backwards in time. The derivation of this PDE can be found in most references
on optimal control, e.g. in Liberzon (2012). The main idea is the dynamic programming principle: for any t the [t, T ]-portion of the optimal trajectory must again be
optimal.
After solving the HJB (61), we can then obtain the optimal control uâˆ—t as function
of the state process zt and t, given by
uâˆ—t = arg min{âˆ‚z V (zt , t)Î¦(zt , u) + L(zt , u)}.

(62)

uâˆˆU

In some cases, we find that the optimal control is independent of time and is strictly of
a feed-back control law, i.e.
uâˆ—t = uâˆ— (zt )
(63)
for some function uâˆ— : R â†’ U . This is the case for the problems considered in this
paper. With the optimal control found in (62), we can then substitute ut = uâˆ—t in (57)
to obtain the optimally controlled process ztâˆ— .
In summary, to solve the optimal control problem (59), we first solve the HJB
PDE (61), and then solve for the optimal control (62), and lastly (if necessary) solve
the optimally controlled state process by substituting the solution of (62) into (57).
Sometimes, the optimal control (62) can be solved without fully solving the HJB for
V , e.g. when L = 0 and one can infer the sign of âˆ‚V . This is the case for the two
control problems we encounter in this paper. The solution to (62) is the most important
for all practical purposes since it gives a way to adjust the control parameters on-the-fly,
especially when we have a feed back control law.

E.3

Solution of the learning rate control problem

Now, let us apply the HJB equations (Sec. E.2) to solve the learning rate control problem. Recall from Sec. 4.1.2 that we wish to solve
min
u:[0,T ]â†’[0,1]

mT subject to

d
1
mt = âˆ’2aut mt + aÎ·Î£u2t ,
dt
2
1
2
m0 = a(x0 âˆ’ b) .
2

15

(64)

This is of the form (59) with Î¦(m, u) = âˆ’2aum + aÎ·Î£u2 /2, L(m, u) = 0 and
G(m) = m. Thus, we write the HJB equation
1
âˆ‚t V (m, t) + min {âˆ‚m V (m, t)[âˆ’2aum + aÎ·Î£u2 ]} = 0,
2
uâˆˆ[0,1]
V (m, T ) = m.

(65)

First, itâ€™s not hard to see that for a > 0, âˆ‚m V â‰¥ 0 for all m, t. This is because, the
lower the m, the closer we are to the optimum and hence the minimum cost achievable
in the same time interval [t, T ] should be less. Similarly,âˆ‚m V â‰¥ 0 holds for a < 0
if one reverses all previous statements (in this case m is negative). Hence, we can
calculate the minimum
1
uâˆ— = arg min{âˆ’2aum + aÎ·Î£u2 },
2
uâˆˆ[0,1]
(
1
a â‰¤ 0,
=
2m
a > 0.
min(1, Î·Î£ )

(66)

Notice that this solution is a feed-back control policy. We can now substitute ut = uâˆ—t
where
(
1
a â‰¤ 0,
uâˆ—t =
(67)
t
min(1, 2m
)
a > 0.
Î·Î£
into the ODE in (64) to obtain
(
m0 eâˆ’2at + 41 Î·Î£(1 âˆ’ eâˆ’2at )
âˆ—
mt =
Î·Î£
2+2a(tâˆ’tâˆ— )

a â‰¤ 0 or t < tâˆ— ,
a > 0 and t â‰¥ tâˆ— .

(68)

where



4m0
1
log
âˆ’1
2a
Î·Î£
And therefore, we get from (67) the effective annealing schedule
(
1
a â‰¤ 0 or t â‰¥ tâˆ— ,
uâˆ—t =
1
a > 0 and t > tâˆ— ,
1+a(tâˆ’tâˆ— )
tâˆ— =

E.4

(69)

(70)

Solution of the momentum parameter control problem

We shall consider the case a > 0, since for a â‰¤ 0 the optimal control is trivially Âµt = 1.
The momentum parameter control problem is
min

mT subject to

Âµ:[0,T ]â†’[0,1]

d
mt = RÎ»(Âµt )(mt âˆ’ mâˆž (Âµt )),
dt
1
m0 = a(x0 âˆ’ b)2 ,
2
16

(71)

where
Î»(Âµ) = âˆ’

(1 âˆ’ Âµ) âˆ’

p

(1 âˆ’ Âµ)2 âˆ’ 4aÎ·
,
Î·

mâˆž (Âµ) =

Î·Î£
.
4(1 âˆ’ Âµ)

(72)

This is of the form (59) with Î¦(m, Âµ) = RÎ»(Âµ)(m âˆ’ mâˆž (Âµ)), L(m, u) = 0 and
G(m) = m. The HJB equation is
âˆ‚t V (m, t) + min {âˆ‚m V (m, t)[RÎ»(Âµ)(m âˆ’ mâˆž (Âµ))]} = 0,
Âµâˆˆ[0,1]

V (m, T ) = m.

(73)

Again, it is easy to see that âˆ‚m V (m, t) â‰¥ 0 for all m, t and so
Âµâˆ— = arg min{RÎ»(Âµ)(m âˆ’ mâˆž (Âµ))}

(74)

Âµâˆˆ[0,1]

This minimization problem has no closed form solution. However, observe that RÎ»(Âµ) â‰¤
âˆš
0 and is minimized at Âµ = Âµopt = max(0, 1 âˆ’ 2 aÎ·). Now, if Âµ > Âµopt , we have
RÎ»(Âµ) = âˆ’(1 âˆ’ Âµ)/Î· and so RÎ»(Âµ)(m âˆ’ mâˆž (Âµ)) is increasing in Âµ for Âµ > Âµopt
(one can check this by differentiation and showing that the derivative is always positive). Hence, Âµâˆ— â‰¤ Âµopt and it is sufficient to consider Âµ âˆˆ [0, Âµopt ] in the minimization
problem (74).
Next, observe that m âˆ’ mâˆž (Âµ) is decreasing in Âµ and negative if
m<

Î·Î£
.
4(1 âˆ’ Âµ)

(75)

or

Î·Î£
.
(76)
4m
At the same time, RÎ»(Âµ) is negative and decreasing for Âµ âˆˆ [0, Âµopt ]. Thus, the product
Î·Î£
RÎ»(Âµ)(m âˆ’ mâˆž (Âµ)) is positive and increasing for 1 âˆ’ 4m
< Âµ < Âµopt and hence we
must have
Î·Î£
Âµâˆ— â‰¤ 1 âˆ’
.
(77)
4m
Note that this is only a bound, but for small Î·, we can take this as an approximation of
Âµâˆ— , so long as it is less than Âµopt . Hence, we arrive at
(
1
a â‰¤ 0,
âˆ—
Âµt =
(78)
Î·Î£
min(Âµopt , max(0, 1 âˆ’ 4m
))
a > 0.
t
Âµ>1âˆ’

One can of course follow the steps in Sec. E.3 to calculate mâˆ—t and hence Âµâˆ—t in the
form of an annealing schedule. We omit these calculations since they are not relevant
to applications.

F

Numerical experiments

In this section, we provide model and algorithmic details for the various numerical experiments considered in the main paper, as well as a brief description of the commonly
applied adaptive learning rate methods that we compare the cSGD algorithm with.
17

F.1

Model details

In Sec. 4 from the main paper, we consider three separate models for two datasets.
M0: fully connected NN on MNIST
The first dataset we consider the MNIST dataset (LeCun et al., 1998), which involves
computer recognition of 60000 28 Ã— 28 pixel pictures of handwritten digits. We split
it into 55000 training samples and 5000 test samples. Our inference model is a fully
connected neural network with one hidden layer. For a input batch K of pixel data
(flattened into a vector) z âˆˆ R784Ã—K , we define the model
y = softmax(W2 hR (W1 z + b1 ) + b2),

(79)

where the activation function hR is the commonly used Rectified Linear Unit (ReLU)
hR (z)(ij) = max(z(ij) , 0).

(80)

The first layer weights and biases are W1 âˆˆ R784Ã—10 and b1 âˆˆ R10 and the second
layer weights and biases are W2 âˆˆ R10Ã—10 and b2 âˆˆ R10 . These constitute the trainable
parameters. The softmax function is defined as
exp(âˆ’z(ij) )
softmax(z)(ij) = P
.
k exp(âˆ’z(kj) )

(81)

The output tensors y âˆˆ R10Ã—K is compared to a batch of one-hot target labels yÌ‚ with
the cross-entropy loss
C(y, yÌ‚) = âˆ’

1 X
yÌ‚(ij) log y(ij) .
10K i,j

(82)

Lastly, we use `2 regularization so that the minimization problem is
min

W1 ,b1 ,W2 ,b2

C(y, yÌ‚) +

2
X

Î»W,i kWi k22 +

i=1

2
X

Î»b,i kbi k22 ,

(83)

i=1

Each regularization strength Î» is set to be 1 divided by the dimension of the trainable
parameter.
C0: fully connected NN on CIFAR-10
The CIFAR-10 dataset (Krizhevsky & Hinton, 2009) consists of 60000 small 32 Ã— 32
pixels of RGB natural images belonging to ten separate classes. We split the dataset
into 50000 training samples and 10000 test samples. Our first model for this dataset is
a deeper fully connected neural network
y = softmax(W3 hT (W2 hT (W1 z + b1 ) + b2 ) + b3 ),

18

(84)

where we use a tanh activation function between the hidden layers
hT (z)(ij) = tanh(z(ij) ).

(85)

The layers have width 3071,500,300,10. That is, the trainable parameters have dimensions W1 âˆˆ R3071Ã—500 , b1 âˆˆ R500 , W2 âˆˆ R500Ã—300 , b2 âˆˆ R300 , W3 âˆˆ R300Ã—10 , b3 âˆˆ
R10 . We use the same soft-max output, cross-entropy loss and `2 regularization as as
before.
C1: convolutional NN on CIFAR-10
Our last experiment is a convolutional neural network on the same CIFAR-10 dataset.
We use four convolution layers consisting of convolution,batch-normalization,ReLU,maxpooling. Convolution filter size is 5 Ã— 5, with uniform stride 1 and padding 2. Output
channels of convolution layers are {96,128,256,64}. The pooling size is 2 Ã— 2 with
stride 2. The output layers consist of two fully connected layers of width {1024,256}
and drop-out rate 0.5. `2 regularization is introduced as a weight decay with decay
parameter 5e-3.

F.2

Adagrad and Adam

Here, we write down for completeness the iteration rules of Adagrad (Duchi et al.,
2011), and Adam (Kingma & Ba, 2015) optimizers, which are commonly applied tools
to tune the learning rate. For more details and background, the reader should consult
the respective references.
Adagrad. The Adagrad modification to the SGD reads
x(i),k+1 = xk,(i) âˆ’ p

Î·
âˆ‚(i) fÎ³k (xk ),
Gk,(i)

(86)

where Gk,(i) is the running sum of gradients âˆ‚(i) fÎ³l (xl ) for l = 0, . . . , k âˆ’ 1. The
tunable hyper-parameters are the learning rate Î· and the initial accumulator value G0 .
In this paper we consider only the learning rate hyper-parameter as this is equivalent to
setting the initial accumulator to a common constant across all dimensions.
Adam. The Adam method has similar ideas to momentum. It keeps the exponential
moving averages
m(i),k+1 = Î²1 mk,(i) + (1 âˆ’ Î²1 )âˆ‚(i) fÎ³k (xk ),
v(i),k+1 = Î²2 vk,(i) + (1 âˆ’ Î²2 )[âˆ‚(i) fÎ³k (xk )]2 .

(87)

Next, set,
mk,(i)
,
1 âˆ’ Î²1k
vk,(i)
=
.
1 âˆ’ Î²2k

mÌ‚k,(i) =
vÌ‚k,(i)

19

(88)

Finally, the Adam update is
x(i),k+1 = xk,(i) âˆ’ p

Î·
mÌ‚k,(i) .
vÌ‚k,(i)

(89)

The hyper-parameters are the learning rate Î· and the EMA decay parameters Î²1 , Î²2 .
Note that for both methods above, one can also introduce a regularization term  to
the denominator to prevent numerical instabilities.

F.3

Implementation of cSGD

Recall from Sec. 4.1 that the optimal control solution for learning rate control of the
quadratic objective f (x) = 12 a(x âˆ’ b)2 is given by
(
1
a â‰¤ 0,
âˆ—
(90)
ut =
t
)
a > 0.
min(1, 2m
Î·Î£
The idea is to perform a local quadratic approximation
d

f (x) â‰ˆ

1X
a(i) (x(i) âˆ’ b(i) )2 .
2 i=1

(91)

This is equivalent to a local linear approximation of the gradient, i.e. for i = 1, 2, . . . , d
âˆ‚(i) f (x) â‰ˆ a(i) (x(i) âˆ’ b(i) ).

(92)

This effectively decouples the control problems of d identical one-dimensional control
problems, so that we may apply (90) element-wise. We note that this approximation
is only assumed to hold locally and the parameters must be updated. There are many
ways to do this. Our approach uses linear regression on-the-fly via exponential moving
averages (EMA). For each trainable dimension i, we maintain the following exponential averages
g k+1,(i) = Î²k,(i) g k,(i) + (1 âˆ’ Î²k,(i) )fÎ³0 k (xk,(i) ),
g 2 k+1,(i) = Î²k,(i) g 2 k,(i) + (1 âˆ’ Î²k,(i) )fÎ³0 k (xk,(i) )2 ,
xk+1,(i) = Î²k,(i) xk,(i) + (1 âˆ’ Î²k,(i) )xk,(i) ,
x2 k+1,(i) = Î²k,(i) x2 k,(i) + (1 âˆ’ Î²k,(i) )x2k,(i) ,
gxk+1,(i) = Î²k,(i) gxk,(i) + (1 âˆ’ Î²k,(i) )xk,(i) fÎ³0 k (xk,(i) ).

(93)

The decay parameter Î²k,(i) controls the effective averaging window size. In practice,
we should adjust Î²k,(i) so that it is small when variations are large, and vice versa. This
ensures that our local approximations adapts to the changing landscapes. Since local
variations is related to the gradient, we use the following heuristic
Î²k+1,(i) = Î²min + (Î²max âˆ’ Î²min )

20

g 2 k,(i) âˆ’ g 2k,(i)
g 2 k,(i)

.

(94)

Algorithm 1 controlled SGD (cSGD)
Hyper-parameters: Î·, u0
Initialize x0 ; Î²0,(i) = 0.9 âˆ€i
for k = 0 to (#iterations âˆ’ 1) do
Compute sample gradient âˆ‡fÎ³k (xk )
for i = 1 to d do
Update EMA using (93)
Compute ak,(i) , bk,(i) , Î£k,(i) using (95)
Compute uâˆ—k,(i) using (96)
Î²k+1,(i) = (g 2 k,(i) âˆ’ g 2k,(i) )/g 2 k,(i) and clip
uk+1,(i) = Î²k,(i) uk,(i) + (1 âˆ’ Î²k,(i) )uâˆ—k,(i)
xk+1,(i) = xk,(i) âˆ’ Î·uk,(i) âˆ‡fÎ³k (xk )(i)
end for
end for

which is similar to the one employed in Schaul et al. (2013) for maintaining EMAs.
The additional clipping to the range [Î²min , Î²max ] is to make sure that there are enough
samples to calculate meaningful regressions, and at the same time prevent too large decay values where the contribution of new samples vanish. In the applications presented
in this paper, we usually set Î²min = 0.9 and Î²max = 0.999, but results are generally
insensitive to these values.
With the EMAs (93), we compute ak,(i) by the ordinary-least-squares formula and
Î£k,(i) as the variance of the gradients:
ak,(i) =

gxk,(i) âˆ’ g k,(i) xk,(i)
x2 k,(i) âˆ’ x2k,(i)

bk,(i) = xk,(i) âˆ’

,

g k,(i)
,
ak,(i)

Î£k,(i) = g 2 k,(i) âˆ’ g 2k,(i) ,
This allows us to estimate the policy (90) as
(
1
âˆ—
uk,(i) =
a
(xk,(i) âˆ’bk,(i) )2
min(1, k,(i) Î·Î£
)
k,(i)

(95)

ak,(i) â‰¤ 0,
ak,(i) > 0.

(96)

for i = 1, 2, . . . , d. Since our averages are from exponentially averaged sources, we
should also update our learning rate policy in the same way:
uk+1,(i) = Î²k,(i) uk,(i) + (1 âˆ’ Î²k,(i) )uâˆ—k,(i)
The algorithm is summarized in Alg. 1

21

(97)

Algorithm 2 controlled momentum SGD (cMSGD)
Hyper-parameters: Î·, Âµ0
Initialize x0 , v0 ; Î²0,(i) = 0.9 âˆ€i
for k = 0 to (#iterations âˆ’ 1) do
Compute sample gradient âˆ‡fÎ³k (xk )
for i = 1 to d do
Update EMA using (93)
Compute ak,(i) , bk,(i) , Î£k,(i) using (95)
Compute Âµâˆ—k,(i) using (99)
Î²k+1,(i) = (g 2 k,(i) âˆ’ g 2k,(i) )/g 2 k,(i) and clip
Âµk+1,(i) = Î²k,(i) Âµk,(i) + (1 âˆ’ Î²k,(i) )Âµâˆ—k,(i)
vk+1,(i) = Âµk,(i) vk,(i) âˆ’ Î·âˆ‡fÎ³k (xk )(i)
xk+1,(i) = xk,(i) + vk+1,(i)
end for
end for

F.4

Implementation of cMSGD

We wish to apply the momentum parameter control
(
1
âˆ—
Âµt =
Î·Î£
))
min(Âµopt , max(0, 1 âˆ’ 4m
t

a â‰¤ 0,
a > 0,

(98)

âˆš
where Âµopt = max{0, 1 âˆ’ 2 aÎ·}. We proceed in the same way as in Sec. F.3 by
keeping the relevant EMA averages and performing linear regression on the fly. The
only difference is the application of the momentum parameter adjustment, which is
ï£±
ak,(i) â‰¤ 0,
ï£´
ï£²1
âˆš
âˆ—
Âµk,(i) = min[max(0, 1 âˆ’ 2 ak,(i) Î·),
(99)
ï£´
Î·Î£k,(i)
ï£³max(0, 1 âˆ’
)]
a
>
0,
k,(i)
2ak,(i) (xk,(i) âˆ’bk,(i) )2
The algorithm is summarized in Alg. 2.

F.5

Training accuracy for C1

For completeness we also provide in Fig. 2 the training accuracies of C1 with various
hyper-parameter choices and methods tested in this work. These complements the
plots of test accuracies in Fig. 3,5,6 in the main paper. We see that cSGD and cMSGD
display the same robustness in terms of test and training accuracies.

References
Bellman, Richard. Dynamic programming and Lagrange multipliers. Proceedings of
the National Academy of Sciences, 42(10):767â€“769, 1956.
22

Courant, Richard, Isaacson, Eugene, and Rees, Mina. On the solution of nonlinear
hyperbolic differential equations by finite differences. Communications on Pure and
Applied Mathematics, 5(3):243â€“255, 1952.
Daly, Bart J. The stability properties of a coupled pair of non-linear partial difference
equations. Mathematics of Computation, 17(84):346â€“360, 1963.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online
learning and stochastic optimization. Journal of Machine Learning Research, 12
(Jul):2121â€“2159, 2011.
Freidlin, Mark I, SzuÌˆcs, Joseph, and Wentzell, Alexander D. Random perturbations of
dynamical systems, volume 260. Springer Science & Business Media, 2012.
Hille, Einar and Phillips, Ralph Saul. Functional analysis and semi-groups, volume 31.
American Mathematical Soc., 1996.
Hirt, CW. Heuristic stability theory for finite-difference equations. Journal of Computational Physics, 2(4):339â€“355, 1968.
Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. ICLR,
2015.
Kloeden, P. E. and Platen, E. Numerical Solution of Stochastic Differential Equations.
Springer, New York, corrected edition, June 2011.
Krizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny
images. 2009.
Lax, Peter and Wendroff, Burton. Systems of conservation laws. Communications on
Pure and Applied mathematics, 13(2):217â€“237, 1960.
LeCun, Yann, Cortes, Corinna, and Burges, Christopher JC. The mnist dataset of
handwritten digits. URL http://yann. lecun. com/exdb/mnist, 1998.
LeVeque, Randall J. Finite volume methods for hyperbolic problems, volume 31. Cambridge university press, 2002.
Liberzon, Daniel. Calculus of variations and optimal control theory: a concise introduction. Princeton University Press, 2012.
Milstein, GN. Approximate integration of stochastic differential equations. Theory of
Probability & Its Applications, 19(3):557â€“562, 1975.
Milstein, GN. A method of second-order accuracy integration of stochastic differential
equations. Theory of Probability & Its Applications, 23(2):396â€“401, 1979.
Milstein, GN. Weak approximation of solutions of systems of stochastic differential
equations. Theory of Probability & Its Applications, 30(4):750â€“766, 1986.
Milstein, GN. Numerical integration of stochastic differential equations, volume 313.
Springer Science & Business Media, 1995.
23

Noh, WF and Protter, MH. Difference methods and the equations of hydrodynamics.
Technical report, California. Univ., Livermore. Lawrence Radiation Lab., 1960.
Oksendal, Bernt. Stochastic differential equations: an introduction with applications.
Springer Science & Business Media, 2013.
Pontryagin, Lev Semenovich. Mathematical theory of optimal processes. CRC Press,
1987.
Schaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. In ICML
(3), volume 28, pp. 343â€“351, 2013.
Talay, Denis and Tubaro, Luciano. Expansion of the global error for numerical schemes
solving stochastic differential equations. Stochastic analysis and applications, 8(4):
483â€“509, 1990.
Uhlenbeck, George E and Ornstein, Leonard S. On the theory of the Brownian motion.
Physical review, 36(5):823, 1930.
Warming, RF and Hyett, BJ. The modified equation approach to the stability and
accuracy analysis of finite-difference methods. Journal of computational physics,
14(2):159â€“179, 1974.

24

Train acc

1.0
0.8
0.6
0.4
0.2
0.00

cSGD

Adagrad
Î·u0

Î·

1e-2
1e-1
5e-1

50 100 150
Epoch

Adam
Î·

5e-1
5e-2
1e-2

0

50 100 150
Epoch

5e-2
1e-2
5e-4

0

50 100 150
Epoch

Train acc

(a) C1, Learning rate adjustments (c.f. main paper Fig . 3)

1.0
0.8
0.6
0.4
0.2
0.00

cMSGD

MSGD

Âµ0

Âµ

0.95
0.9
0.999

50 100 150
Epoch

MSGD-A
Âµmax

0.999
0.99
0.95

0

50 100 150
Epoch

0.999
0.99
0.9

0

50 100 150
Epoch

Train acc

(b) C1, Momentum adjustments (c.f. main paper Fig . 5)

1.0
0.8
0.6
0.4
0.2
0.00

cMSGD

MSGD
Î·

Î·

2e-1
1e-1
2e-2

50 100 150
Epoch

MSGD-A
Î·

1e-1
1e-2
1e-3

0

50 100 150
Epoch

1e-1
1e-2
1e-3

0

50 100 150
Epoch

(c) C1, Learning rate sensitivity (c.f. main paper Fig . 6)

Figure 2: Training accuracies for various methods and hyper-parameter choices. The
set-up is the same as in the main paper, Fig. 3,5,6 except that we plot training accuracy
instead of test accuracy. The qualitative observation is the same: cSGD and cMSGD
are generally robust to changing parameters and models.

25

