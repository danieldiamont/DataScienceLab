When can Multi-Site Datasets be Pooled for Regression?
Hypothesis Tests, `2 -consistency and Neuroscience Applications
(Supplementary Material)

Hao Henry Zhou 1 Yilin Zhang 1 Vamsi K. Ithapu 1 Sterling C. Johnson 1 2 Grace Wahba 1 Vikas Singh 1
In this supplementary document we first present the proofs of all the technical results in Section 2 and 3 of the main paper.
We then expand upon the Section 4 and present extra experiments to strengthen the evaluations from the main paper.
Remarks on transformations in pre-processing step: For all i ‚àà {1, ..., k}, after applying the transformation (shift
correction), we pool (Xi , yi ) together to estimate Œ≤ ‚àó . Note that in general the transformation (shift correction) should not
depend on the responses yi , otherwise we get a dependence on the noise. To see this, notice that yi = Xi Œ≤i + i where
Xi is the transformed set of features. But when the transformation depends on yi , then Xi will also depend on i , which
causes a poor estimation of Œ≤ ‚àó (and Œ≤i ). In situations where the transformations must involve yi , a sensible strategy is to
separate each site‚Äôs dataset into two parts, where one part from each site is used to learn the transformation, and the other
part (after applying the learned transformation) is used for pooling towards Œ≤ ‚àó estimation and conducting our hypothesis
test.

1. Proof of Section 2
We now provide the proofs of the results presented in the main paper.
Theorem 2.1. œÑi =

œÉ1
œÉi

achieve the smallest variance in Œ≤ÃÇ.

Proof. The choice of œÑi leads to weighted least squares, which is known to be the best linear unbiased estimator (BLUE)
under uncorrelated heteroscedastic errors. The variance of Œ≤ÃÇ is equivalent to the case when ‚àÜŒ≤i = 0. In the latter case,
BLUE condition holds and setting œÑi to the above value achieves lowest variance. The equivalence between variances
under two cases completes the proof.

Lemma 2.2. For multi-site model, we have
kBiasŒ≤ k22
‚â§ k(Œ£ÃÇk1 )‚àí2 (Œ£ÃÇk2 (n1 Œ£ÃÇ1 )‚àí1 Œ£ÃÇk2 + Œ£ÃÇk2 )k‚àó ,
kG‚àí1/2 ‚àÜŒ≤k22




V arŒ≤ = œÉ12 (n1 Œ£ÃÇ1 )‚àí1 ‚àí (n1 Œ£ÃÇ1 + Œ£ÃÇk2 )‚àí1  .
‚àó

(1)
(2)

Proof. The estimation from single site model is unbiased, and it has the following variance.
V ar1 = tr((X1T X1 )‚àí1 )œÉ12 = tr((n1 Œ£ÃÇ1 )‚àí1 )œÉ12

(3)

1

University of Wisconsin-Madison 2 William S. Middleton Memorial Veteran‚Äôs Affairs Hospital. Correspondence to: Hao Zhou
<hzhou@stat.wisc.edu>, Vikas Singh <vsingh@biostat.wisc.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

When can Multi-Site Datasets be Pooled for Regression?

The estimation error from multi-sites model has the following closed form expression
Ô£´Ô£´

X1
Ô£¨Ô£¨ œÑ2 X2
‚àó
Ô£¨
Ô£¨
Œ≤ÃÇ ‚àí Œ≤ = Ô£≠Ô£≠
..
œÑk Xk
Ô£∂T Ô£´
Ô£´Ô£´
X1
Ô£¨Ô£¨ œÑ2 X2 Ô£∑ Ô£¨
Ô£¨
Ô£∑ Ô£¨
+Ô£¨
Ô£≠Ô£≠ .. Ô£∏ Ô£≠
œÑk Xk

Ô£∂Ô£∂‚àí1
Ô£´
X1
œÑ2 X2
Ô£∑ Ô£¨ œÑ2 X2 Ô£∑Ô£∑
Ô£∑ Ô£¨
Ô£∑Ô£∑ Ô£≠ ..
Ô£∏ Ô£≠ .. Ô£∏Ô£∏
œÑk Xk
œÑk Xk
Ô£∂T Ô£´
Ô£∂Ô£∂‚àí1 Ô£´
X1
X1
Ô£∑ Ô£¨
Ô£∑ Ô£¨
œÑ2 X2 Ô£∑
Ô£∑Ô£∑ Ô£¨ œÑ2 X2 Ô£∑ Ô£¨
Ô£∏
Ô£≠
Ô£∏
.. Ô£∏ Ô£≠
..
œÑk Xk
œÑk Xk
Ô£∂T Ô£´

Ô£∂T Ô£´

Ô£∂
œÑ2 X2 (‚àÜŒ≤2 )
Ô£∏ Ô£≠
Ô£∏
..
œÑk Xk (‚àÜŒ≤k )
(4)
Ô£∂
1
œÑ2 2 Ô£∑
Ô£∑
.. Ô£∏
œÑk k

First term in the summation from (4) is bias, while second term is variance. We can see that our choice of œÑi =
heteroscedastic errors issue among sites. We further simplify bias and variance terms, and obtain
V ar2 = tr((n1 Œ£ÃÇ1 +

k
X

œÉ1
œÉi

resolves

ni œÑi2 Œ£ÃÇi )‚àí1 )œÉ12

(5)

i=2

The reduced variance statement is proved. For the bias term, it is equivalent as shown below.
Ô£∂T Ô£´
X1
X1
Ô£¨Ô£¨ œÑ2 X2 Ô£∑ Ô£¨ œÑ2 X2
Ô£¨Ô£¨
Ô£∑ Ô£¨
Ô£≠Ô£≠ .. Ô£∏ Ô£≠ ..
œÑk Xk
œÑk Xk
Ô£´Ô£´

Ô£∂Ô£∂‚àí1
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∏Ô£∏

Ô£∂T Ô£´
œÑ2 X2
œÑ2 X2
Ô£≠ .. Ô£∏ Ô£≠ 0
0
œÑk Xk
Ô£´

0
œÑ3 X3
0

...
...
...

Ô£±
Ô£∂
Ô£´
Ô£∂Ô£º
‚àÜŒ≤2 Ô£Ω
0
Ô£≤
0 Ô£∏ G1/2 G‚àí1/2 Ô£≠ .. Ô£∏
Ô£≥
Ô£æ
œÑk Xk
‚àÜŒ≤k

(6)

A one step Cauchy Schwartz inequality is then applied. Then our final proof is to show k..k2F on
Ô£∂T Ô£´
X1
X1
Ô£¨Ô£¨ œÑ2 X2 Ô£∑ Ô£¨ œÑ2 X2
Ô£¨Ô£¨
Ô£∑ Ô£¨
Ô£≠Ô£≠ .. Ô£∏ Ô£≠ ..
œÑk Xk
œÑk Xk
Ô£´Ô£´

Ô£∂Ô£∂‚àí1
Ô£∑Ô£∑
Ô£∑Ô£∑
Ô£∏Ô£∏

Ô£∂T Ô£´
œÑ2 X2
œÑ2 X2
Ô£≠ .. Ô£∏ Ô£≠ 0
œÑk Xk
0
Ô£´

0
œÑ3 X3
0

...
...
...

Ô£∂
0
0 Ô£∏ G1/2
œÑk Xk

(7)

is equal to right side of the bias relaxation in (1).
It is easy to see that kAk2F = kAT Ak‚àó . Based on this, we can see the first term of matrix inverse contributes the (Œ£ÃÇk1 )‚àí2
in (1). Let the other part in (7) be L. We have
Ô£∂T Ô£´ 2 T
Ô£∂ Ô£´
Ô£∂T Ô£´
Ô£∂
œÑ22 X2T X2
œÑ2 X2 X2
n2 œÑ22 Œ£ÃÇ2
n2 œÑ22 Œ£ÃÇ2
Ô£∏ GÔ£≠
Ô£∏=Ô£≠
Ô£∏ GÔ£≠
Ô£∏
..
..
..
..
LLT = Ô£≠
œÑk2 XkT Xk
œÑk2 XkT Xk
nk œÑk2 Œ£ÃÇk
nk œÑk2 Œ£ÃÇk
Ô£´

(8)

After some manipulations, this becomes (Œ£ÃÇk2 (n1 Œ£ÃÇ1 )‚àí1 Œ£ÃÇk2 + Œ£ÃÇk2 ). The bias part is proved.
Theorem 2.3. a): The multi-sites model has smaller MSE of Œ≤ÃÇ than single-site model whenever

2


H0 : G‚àí1/2 ‚àÜŒ≤  ‚â§ œÉ12 .

(9)

2

b): Further, we have the following test statistic,


 G‚àí1/2 ‚àÜŒ≤ÃÇ 2



 ‚àº œá2(k‚àí1)‚àóp


œÉ1
2

where kG‚àí1/2 ‚àÜŒ≤/œÉ1 k2 is called a ‚Äúcondition value‚Äù.

 ‚àí1/2
2 !
G

‚àÜŒ≤

 ,


œÉ1
2

(10)

When can Multi-Site Datasets be Pooled for Regression?

Proof. (a): Based on Lemma 2.2, the theorem is proved when right side in (9) is replaced by




œÉ12 (n1 Œ£ÃÇ1 )‚àí1 ‚àí (n1 Œ£ÃÇ1 + Œ£ÃÇk2 )‚àí1 
‚àó

k(Œ£ÃÇk1 )‚àí2 (Œ£ÃÇk2 (n1 Œ£ÃÇ1 )‚àí1 Œ£ÃÇk2 + Œ£ÃÇk2 )k‚àó
We first calculate the numerator

h


i




œÉ12 (n1 Œ£ÃÇ1 )‚àí1 ‚àí (n1 Œ£ÃÇ1 + Œ£ÃÇk2 )‚àí1  = œÉ12  (n1 Œ£ÃÇ1 )‚àí1 (n1 Œ£ÃÇ1 + Œ£ÃÇk2 ) ‚àí I (n1 Œ£ÃÇ1 + Œ£ÃÇk2 )‚àí1 
‚àó
‚àó


2
‚àí1 k
k ‚àí1 
= œÉ1 (n1 Œ£ÃÇ1 ) Œ£ÃÇ2 (n1 Œ£ÃÇ1 + Œ£ÃÇ2 ) 

(11)

(12)

‚àó

The denominator is then given by
k(Œ£ÃÇk1 )‚àí2 (Œ£ÃÇk2 (n1 Œ£ÃÇ1 )‚àí1 Œ£ÃÇk2 + Œ£ÃÇk2 )k‚àó = k(Œ£ÃÇk1 )‚àí2 ((Œ£ÃÇk2 + n1 Œ£ÃÇ1 )(n1 Œ£ÃÇ1 )‚àí1 Œ£ÃÇk2 )k‚àó
Remember
=

k((Œ£ÃÇk2

Œ£ÃÇk1

+

= Œ£ÃÇk2 + n1 Œ£ÃÇ1 , , we continue
n1 Œ£ÃÇ1 )‚àí1 (n1 Œ£ÃÇ1 )‚àí1 Œ£ÃÇk2 )k‚àó = k((n1 Œ£ÃÇ1 )‚àí1 Œ£ÃÇk2 (n1 Œ£ÃÇ1

(13)
(14)

+

Œ£ÃÇk2 )‚àí1 )k‚àó

(15)

The last step uses the property of k..k‚àó norm. The proof is completed by noticing the simplified form of numerator and
denominator. It is clear now that the right side in (9) is exactly œÉ12 .
(b): First, we show œÉ12 G is the covariance matrix of ‚àÜŒ≤ÃÇ. We have
cov(‚àÜŒ≤ÃÇi , ‚àÜŒ≤ÃÇj ) = cov(Œ≤ÃÇi , Œ≤ÃÇj ) ‚àí cov(Œ≤ÃÇi , Œ≤ÃÇ1 ) ‚àí cov(Œ≤ÃÇ1 , Œ≤ÃÇj ) + cov(Œ≤ÃÇ1 , Œ≤ÃÇ1 )

(16)

Since each site is independent from other site, we have

(17)

cov(‚àÜŒ≤ÃÇi , ‚àÜŒ≤ÃÇj ) = cov(Œ≤ÃÇ1 , Œ≤ÃÇ1 ) =

œÉ12 (n1 Œ£ÃÇ1 )‚àí1 for

i 6= j

(18)

cov(‚àÜŒ≤ÃÇi , ‚àÜŒ≤ÃÇi ) = cov(Œ≤ÃÇi , Œ≤ÃÇi ) + cov(Œ≤ÃÇ1 , Œ≤ÃÇ1 ) = œÉ12 ((n1 Œ£ÃÇ1 )‚àí1 + (ni (œÉ12 /œÉi2 )Œ£ÃÇi )‚àí1 ) = œÉ12 ((n1 Œ£ÃÇ1 )‚àí1 + (ni œÑi2 Œ£ÃÇi )‚àí1 )
(19)
‚àÜŒ≤ÃÇ follows Gaussian distribution since it is a linear transformation of Gaussian distribution. It‚Äôs expectation is ‚àÜŒ≤ since
each Œ≤ÃÇi is an unbiased estimator. Hence, we have
‚àÜŒ≤ÃÇ ‚àº N (‚àÜŒ≤, œÉ12 G)

(20)

This distribution result, and noticing the connection between Gaussian and non-central œá2 distributions completes the
proof.
Corollary 2.4. For the case where we have two participating sites, the condition (9) from Theorem 2.3 reduces to
H0 : ‚àÜŒ≤ T ((n1 Œ£ÃÇ1 )‚àí1 + (n2 œÑ22 Œ£ÃÇ2 )‚àí1 )‚àí1 ‚àÜŒ≤ ‚â§ œÉ12 .

(21)

Proof. The proof is follows by noticing the form of G when k = 2.
Theorem 2.5. Analysis in Section 2.1 holds for Œ≤ in model with Z confounding features, when we replace Œ£ÃÇi with
Œ£ÃÉi = Œ£ÃÇxxi ‚àí Œ£ÃÇxzi (Œ£ÃÇzzi )‚àí1 Œ£ÃÇzxi .

T
Proof. Define Œ≥ T = (Œ≥1T , ..., Œ≥kT ), Xall
= X1T , œÑ2 X2T , ..., œÑk XkT , Zall = Diag (Z1 , œÑ2 Z2 , ..., œÑk Zk ). We have
Ô£´
Ô£∂
0

  ‚àó   T



‚àí1
T
T
Ô£¨ œÑ2 X2 (‚àÜŒ≤2 ) Ô£∑
Xall Xall Xall
Œ≤
Zall
Xall
Œ≤ÃÇ
Ô£¨
Ô£∑+
=
‚àí
‚àó
T
T
T
Ô£≠
Ô£∏
Œ≥
..
Zall Xall Zall Zall
Zall
Œ≥ÃÇ
œÑk Xk (‚àÜŒ≤k )
Ô£´
Ô£∂
1
 T
‚àí1  T 
T
Ô£∑
Xall Xall Xall
Zall
Xall Ô£¨
œÑ
2
Ô£¨ 2 Ô£∑
T
T
T
Ô£≠
.. Ô£∏
Zall Xall Zall Zall
Zall
œÑk k

(22)

(23)

When can Multi-Site Datasets be Pooled for Regression?

Using sub-matrix inverse property, we obtain
 T
‚àí1  T  

T
T
T
Xall Xall Xall
Zall
Xall
(XÃÉall
XÃÉall )‚àí1 XÃÉall
=
T
T
T
T
T
Zall
Xall Zall
Zall
Zall
(ZÃÉall
ZÃÉall )‚àí1 ZÃÉall

(24)

We then have
T
T
ZÃÉall = (I ‚àí Xall (Xall
Xall )‚àí1 Xall
)Zall

(25)
Z1 (Z1T Z1 )‚àí1 Z1T )X1
Z2 (Z2T Z2 )‚àí1 Z2T )X2

Ô£´

T
T
XÃÉall = (I ‚àí Zall (Zall
Zall )‚àí1 Zall
)Xall

Ô£∂

(I ‚àí
Ô£¨ (I ‚àí
Ô£∑
Ô£∑
=Ô£¨
Ô£≠
Ô£∏
..
(I ‚àí Zk (ZkT Zk )‚àí1 ZkT )Xk

(26)

Define
HZi = (I ‚àí Zi (ZiT Zi )‚àí1 ZiT )

(27)

Ô£´
Ô£∂
Ô£∂
1
0
Ô£¨
Ô£∑
Ô£∑
Ô£¨
T
‚àí1 T Ô£¨ œÑ2 2 Ô£∑
T
T Ô£¨ œÑ2 X2 (‚àÜŒ≤2 ) Ô£∑
Œ≤ÃÇ ‚àí Œ≤ ‚àó = (XÃÉall
XÃÉall )‚àí1 XÃÉall
Ô£∏ + (XÃÉall XÃÉall ) XÃÉall Ô£≠ .. Ô£∏
Ô£≠
..
œÑk k
œÑk Xk (‚àÜŒ≤k )
Ô£´
Ô£∂
1
k
X
Ô£¨
œÑ
2 Ô£∑
2
T
T
T Ô£¨
Ô£∑
= (XÃÉall
XÃÉall )‚àí1
œÑi XÃÉiT Xi (‚àÜŒ≤i ) + (XÃÉall
XÃÉall )‚àí1 XÃÉall
Ô£≠ .. Ô£∏
i=2
œÑk k

(28)

Hence, we have
Ô£´

We also observe that
XÃÉiT Xi = XiT HZi Xi = XiT HZ2 i Xi = XÃÉiT XÃÉi

(29)

Therefore, we can apply our previous results to a subset of parameters if we replace Xi by XÃÉi . Since our results only
depend on Œ£ÃÇi , we only need to replace it by
1
1 T
XÃÉi XÃÉi = XiT HZi Xi = Œ£ÃÇxxi ‚àí Œ£ÃÇxzi (Œ£ÃÇzzi )‚àí1 Œ£ÃÇzxi
ni
ni

(30)

This proves the theorem.

2. Proof of Section 3
Definition 3.1. The m-sparse minimal and maximal eigenvalues of C, denoted by œÜmin (m) and œÜmax (m), are
ŒΩ T CŒΩ
ŒΩ:kŒΩk0 ‚â§dme ŒΩ T ŒΩ
min

and

ŒΩ T CŒΩ
ŒΩ:kŒΩk0 ‚â§dme ŒΩ T ŒΩ
max

(31)

We first list down the two key theorem statements that we prove in this section.
Theorem 3.2. Let 0 ‚â§ Œ± ‚â§ 0.4. Assume there exist constants 0 ‚â§ œÅmin ‚â§ œÅmax ‚â§ ‚àû such that

2 !
2Œ±
lim inf œÜmin sp 1 +
‚â• œÅmin , and
n‚Üí‚àû
1 ‚àí 2Œ±
k
X
lim sup œÜmax (sp + min{
ni , kp}) ‚â§ œÅmax .
n‚Üí‚àû

(32)

i=1

p
Then, for Œª ‚àù œÉ nÃÑ log(kp), there exists a constant œâ > 0 such that, with probability converging to 1 for n ‚Üí ‚àû,
1 Œª
sÃÑ log(kp)
kBÃÇ ‚àí B ‚àó k2F ‚â§ œâœÉ 2
,
k
nÃÑ
p
‚àö
where sÃÑ = {(1 ‚àí Œ±) sp + Œ± sh /k}2 , œÉ is the noise level.

(33)

When can Multi-Site Datasets be Pooled for Regression?

Theorem 3.3. Let 0.4 ‚â§ Œ±ÃÉ ‚â§ 1. Assume there exist constants 0 ‚â§ œÅmin ‚â§ œÅmax ‚â§ ‚àû such that
2 !

(1 ‚àí Œ±ÃÉ)
lim inf œÜmin sh 1 +
‚â• kmin , and
n‚Üí‚àû
Œ±ÃÉ
k
X
lim sup œÜmax (sh + min{
ni , kp}) ‚â§ kmax .
n‚Üí‚àû

(34)

i=1

p
Then, for ŒªÃÉ ‚àù œÉ nÃÑ log(kp), there exists a œâ > 0 such that, with probability converging to 1 for n ‚Üí ‚àû, we have
sÃÉ log(kp)
1 Œª
kBÃÇ ‚àí B ‚àó k2F ‚â§ œâœÉ 2
,
k
nÃÑ

(35)

p
p
with sÃÉ = {(1 ‚àí Œ±ÃÉ) sp /k + Œ±ÃÉ sh /k}2 instead of sÃÑ.
‚àö
Comment about Theorem 3.3: We do not penalize by k when the sparsity patterns across sites share few of the features.
To‚àö
see this, first observe that when sparsity patterns are similar, most‚àöofp
the groups we have are non-sparse, and the effects
of kkŒ≤ j k2 and kŒ≤ j k1 have the same scale. This is simply because, k a21 + ... + a2k is close to |a1 |+...+|ak | whenever
|a1 |, ..., |ak | are close. However when sparsity patterns across sites share few
p features only, most of the groups are going
to be sparse. For these groups, we should use kŒ≤ j k2 , because in this setting a21 + 0 + ... + 0 is close to |a1 | + 0 + ... + 0.
3.1. Proof of Theorem 3.2:
We follow the proof procedure from Lasso (Meinshausen & Yu, 2009) and group Lasso (Liu & Zhang, 2009) results. Let
B Œª be the estimator under the absence of noise, i.e., B Œª = BÃÇ Œª,0 , where BÃÇ Œª,Œæ is defined as in (37). The `2 -distance can
then be bounded by kBÃÇ Œª ‚àí B ‚àó k2F ‚â§ 2kBÃÇ Œª ‚àí B Œª k2F + 2kB Œª ‚àí B ‚àó k2F . The first term on the right-hand side represents the
variance of the estimation, while the second term represents the bias. The bias contribution follows directly from Lemma
3.4 below, and the variance bound term follows from Lemma 3.9.
De-noised response. For 0 < Œæ < 1, we define a de-noised version of the response variable as follows,
Yi (Œæ) = Xi Œ≤i + Œæi

(36)

We can regulate the amount of noise with the parameter Œæ.
For Œæ = 0, only the signal is retained. The original observations with the full amount of noise are recovered for Œæ = 1.
Now consider for 0 ‚â§ Œæ ‚â§ 1 the estimator BÃÇ Œª,Œæ ,
BÃÇ Œª,Œæ = arg min
B

k
X

kYi (Œæ) ‚àí Xi Œ≤i k22 + ŒªŒõ(B)

i=1

p
p
X
‚àö X
Œõ(B) = (1 ‚àí Œ±) k
kŒ≤ j k2 + Œ±
kŒ≤ j k1
j=1

(37)

j=1

The ordinary sparse multi-site Lasso estimate is recovered under the full amount of noise so that BÃÇ Œª,1 = BÃÇ Œª . Using
the notation from the previous results, we have BÃÇ Œª,0 = B Œª , for the estimate in the absence of noise. The definition of
the de-noised version of the sparse multi-site Lasso estimator will be helpful for the proof as it allows to characterize the
variance of the estimator.
3.1.1. PART I OF PROOF ‚Äì D EALING WITH BIAS
Let P‚àó be the set of nonzero groups of B ‚àó , i.e., P‚àó = {j : Œ≤ j 6= 0}. The cardinality of P‚àó is denoted by sp . For each j
in P‚àó , let Hj be the set of nonzero elements of Œ≤j , i.e., Hj = {i : Œ≤ij 6= 0}. The number of all nonzero elements of B is
denoted by sh . For the following, let B Œª be the estimator BÃÇ Œª,0 with no noise (as defined in (37)). For each Œª, the solution
B Œª can be written as B Œª = B ‚àó + ŒìŒª . We define Œ≥ j and Œ≥i to be j-th column and i-th row of Œì. Œ≥ is the transpose of the
Œ±
. Then
unfolded vector of Œì by row. Denote Œª2 = Œª(1 ‚àí Œ±) and Œ∑ = 1‚àíŒ±
ŒìŒª = arg min f (Œì)
Œì

(38)

When can Multi-Site Datasets be Pooled for Regression?

The function f (Œì) is given by
Ô£±
Ô£º
Ô£≤X ‚àö
Ô£Ω
X
‚àö
f (Œì) = nÃÑŒ≥ T CŒ≥ + Œª2
( KkŒ≥ j k2 + Œ∑kŒ≥ j k1 ) +
K(kŒ≤ j + Œ≥ j k2 ‚àí kŒ≤ j k2 ) +
Ô£≥
Ô£æ
j‚ààP‚àó
j‚ààP‚àóC
)
(
X
X
j
j
C
j
j
Œ∑kŒ≥ Hj k1
Œª2
Œ∑(kŒ≤Hj + Œ≥Hj k1 ‚àí kŒ≤ Hj )k +
j‚ààP‚àó

(39)

j‚ààP‚àó

The matrix ŒìŒª is the bias of the sparse multi-site Lasso estimator. We derive first a bound on the Frobenius norm of ŒìŒª .
Lemma 3.4. Assume conditions in Theorem3.2. The Frobenius norm of ŒìŒª is then bounded for sufficiently large values of
nÃÑ, given a constant œâ1 > 0, by
ksÃÑ log(kp)
kŒìŒª k2F ‚â§ œâ1 œÉ 2
(40)
nÃÑ
Proof. f (Œì) = 0 whenever Œì = 0 following the definition from (39). For the true solution ŒìŒª , it follows hence that
f (ŒìŒª ) ‚â§ 0. For notational simplicity, we drop the super-script Œª from here on. Using Œ≥ T CŒ≥ ‚â• 0, we have
Ô£º (
Ô£±
)
Ô£Ω
Ô£≤X ‚àö
X
X
X‚àö
X
j
j
j
j
j
Œ∑kŒ≥H C k1 ‚â§
Œ∑kŒ≥Hj k1
(Œ∑kŒ≥ k1 ) +
kkŒ≥ k2 +
( kkŒ≥ k2 ) +
Ô£æ
Ô£≥
j
C
C
j‚ààP‚àó

Since |P‚àó | = sp ,
using (41),

P

j‚ààP‚àó

j‚ààP‚àó

j‚ààP‚àó

j‚ààP‚àó

(41)

j‚ààP‚àó

P
‚àö
j
sp kŒ≥k2 , j‚ààP‚àó kŒ≥H
k ‚â§ sh kŒ≥k2 , and hence,
j 1
‚àö
p
‚àö
Œõ(Œì) ‚â§ 2{(1 ‚àí Œ±) ksp + Œ± sh }kŒ≥k2 = 2 ksÃÑkŒ≥k2
(42)

|Hj | = sh . It follows that

P

j‚ààP‚àó kŒ≥

j

k2 ‚â§

‚àö

Using f (Œì) ‚â§ 0 again and (42), it follows that
‚àö
nÃÑŒ≥ T CŒ≥ ‚â§ 2Œª ksÃÑkŒ≥k2

(43)

Now consider Œ≥ T CŒ≥. Bounding this term from below and plugging the result into (42) will yield the desired upper bound
on the Frobenius norm of Œì. Let kŒ≥ (1) k ‚â• kŒ≥ (2) k ‚â• ... ‚â• kŒ≥ (p) k be the ordered columns of Œì. Let un for n ‚àà N be a
sequence of positive integers, to be chosen later, and define U = {j : kŒ≥ j k2 ‚â• kŒ≥ (un ) k2 }. Define Œ≥(U ) and Œ≥(U C ) by
setting Œ≥ j (U ) = Œ≥ j 1{i ‚àà
/ U } and Œ≥ j (U C ) = Œ≥ j 1{i ‚àà U }, followed by unfolding Œì. Then quantity Œ≥ T CŒ≥ can be written
T
2
as Œ≥ CŒ≥ = ka + bk2 , where a := nÃÑ‚àí1/2 XŒ≥(U ), b := nÃÑ‚àí1/2 XŒ≥(U C ), X = DIAG(X1 , ..., Xk ). Then
Œ≥ T CŒ≥ = ka + bk22 ‚â• (kak2 ‚àí kbk2 )2

(44)

Before proceeding, we need to bound the norm kŒ≥(U C )k2 as a function of un . Assume l =
every j = 1, ..., p that kŒ≥ (j) k2 ‚â§ l/j. Hence,
kŒ≥(U C )k22 ‚â§ (

p
X

kŒ≥ j k2 )2

j=1

Therefore, we have
C

kŒ≥(U )k2 ‚â§

p
X
j=1

j

kŒ≥ k2

p
X
j=un

r

1
j2
+1

1
‚â§ kŒ≥k1
un

r

Pp

j=1

kŒ≥ j k2 . It holds for

(45)

1
un

‚àö Pp
Based on (42), Œõ(Œì) = (1 ‚àí Œ±) k j=1 kŒ≥ j k2 + Œ±kŒ≥k1 , and (46), it follows that
Ô£±
!2 Ô£º
‚àö
Ô£≤ 1
Ô£Ω
ksÃÑ
‚àö
kŒ≥(U C )k22 ‚â§ 4kŒ≥k22
Ô£≥ un (1 ‚àí Œ±) k + Œ± Ô£æ

(46)

(47)

By definition, since Œ≥(U ) has only un nonzero groups,
kak22 = kŒ≥(U )T CŒ≥(U )k22 ‚â• œÜmin (un )kŒ≥(U )k22 ‚â•
(
2 )!

1
ksÃÑ
2
‚àö
œÜmin (un )kŒ≥k2 1 ‚àí 4
un (1 ‚àí Œ±) k + Œ±

(48)

When can Multi-Site Datasets be Pooled for Regression?

Here we explain why we obtain œÜmin (un ) instead of œÜmin (kun ). We denote œÜimin (m) to be m-sparse of nÃÑ‚àí1 XiT Xi . Then
œÜmin (m) = minki=1 œÜimin (m) because of block structure. Since we have un nonzero groups, instead of arbitrary kun
nonzero elements, we obtain a higher value œÜmin (un ) = minki=1 œÜimin (un ) instead of œÜmin (kun ). This is the one place
where we consider the block structure of multi-site design.
Pk
As Œ≥(U C ) has at most min{ i=1 ni , kp} nonzero groups, using again (47), (42) and the block structure of multi-site
design,
kbk22 ‚â§ 4œÜmax (min{

k
X

(
ni , kp})kŒ≥k22

i=1

1
un

!2 )
‚àö
ksÃÑ
‚àö
(1 ‚àí Œ±) k + Œ±

(49)

Pk
Using (49), (48) and (44), along with œÜmax (min{ i=1 ni , kp}) ‚â• œÜmin (un ),
v
)Ô£∂
(
u
‚àö
P
u œÜmax (min{ k ni , kp})
1
ksÃÑ
T
2
i=1
t
2
‚àö
Œ≥ CŒ≥ ‚â• œÜmin (un )kŒ≥k2 √ó Ô£≠1 ‚àí 4
) Ô£∏
(
œÜmin (un )
un (1 ‚àí Œ±) k + Œ±
Ô£´

Using conditions in Theorem 3.2 and setting un =



‚àö

ksÃÑ
‚àö
(1‚àíŒ±) k+Œ±

2

(50)

, it follows that



r
œÅmax
Œ≥ T CŒ≥ ‚â• œÅmin 1 ‚àí 4
kŒ≥k22
œÅmin

(51)

‚àö
Using this result together with (43), which says that Œ≥ T CŒ≥ ‚â§ 2nÃÑ‚àí1 Œª ksÃÑkŒ≥k2 , we have the following for large nÃÑ,
kŒìk2F = kŒ≥k22 ‚â§

Œª2 ksÃÑ
1
‚àö
2
(œÅmin ‚àí 4 œÅmin œÅmax ) nÃÑ2

(52)

The proof of Lemma 3.4 is completed by noticing Œª in Theorem 3.2.
3.1.2. PART II OF PROOF ‚Äì D EALING WITH VARIANCE
The proof for the variance part is two-fold. We first derive a bound on the variance, which is a function of the number of
nonzero groups. We then bound the number of nonzero groups, taking into account the bound on the bias derived above.
Variance of restricted OLS: Before considering the sparse multi-site Lasso estimator, a trivial bound is shown for the
variance of a restricted OLS estimation. For every subset œà ‚äÇ {1, , p}, we use it to select a subset of columns from design
matrix Xi for task i. These columns form a matrix Xiœà . Define Xœà = DIAG(X1œà , X2œà , ..., Xkœà ), and the restricted
OLS-estimator with the noise vector T = (1 , ..., k )T is
Œ∏ÃÇœà = (XœàT Xœà )‚àí1 XœàT 

(53)

The `2 -norm of this estimator can be bounded.
Lemma 3.5. Let mp be a sequence with mp = o(nÃÑ) and mp ‚Üí ‚àû for nÃÑ ‚Üí ‚àû. It holds with probability converging to 1
for n ‚Üí ‚àû
2 log kp kmp
max kŒ∏ÃÇœà k22 ‚â§
œÉ2
(54)
nÃÑ
œÜ2min (mp )
œà:|œà|‚â§mp
Proof. We refer the readers to Lemma 3 in (Meinshausen & Yu, 2009) and Lemma 3 in (Liu & Zhang, 2009) for the
proof. Here, we again use block design structure of multi-site problem, the same as in (48), to obtain œÜmin (mp ) instead of
œÜmin (kmp ).
The variance of the sparse multi-site Lasso estimator can be bounded by the variance of restricted OLS estimators, using
bounds on the number of active groups.

When can Multi-Site Datasets be Pooled for Regression?

Lemma 3.6. If, for a fixed value of Œª, the number of nonzero groups of de-noised estimators BÃÇ Œª,Œæ is for every 0 ‚â§ Œæ ‚â§ 1
bounded by m, then
sup kBÃÇ Œª,0 ‚àí BÃÇ Œª,Œæ k2F ‚â§ C max kŒ∏ÃÇœà k22
(55)
œà:|œà|‚â§m

0‚â§Œæ‚â§1

with C as a generic constant.
Proof. We refer the readers to Lemma 4 and Lemma 5 in (Liu & Zhang, 2009) for the proof.
Œª,Œæ
Let AP
. Define mp to be the largest number of
Œª,Œæ be the set of variables in nonzero groups of the de-noised estimator BÃÇ
P
nonzero groups over all values of 0 ‚â§ Œæ ‚â§ 1. Then we have kmp = sup0‚â§Œæ‚â§1 |AŒª,Œæ |.

Lemma 3.7. Given 0 ‚â§ Œ± ‚â§ 0.5, we have
2
2
T
Œª,Œæ 2
|AP
)k2
Œª,Œæ |Œª (1 ‚àí 2Œ±) ‚â§ k2XAp (Y ‚àí X Œ≤ÃÇ

(56)

Œª,Œæ

where we defined before that X = DIAG(X1 , ..., Xk ), Y T = (Y1T , ..., YkT ). Œ≤ÃÇ Œª,Œæ is the transpose of unfolded vector of
BÃÇ Œª,Œæ by rows. XApŒª,Œæ is Xœà when œà = ApŒª,Œæ
Proof. The conditions for the solution of sparse multi-site Lasso are presented in (Simon et al., 2013). We use Œ≤ÃÇ rather
than Œ≤ÃÇ Œª,Œæ for notational simplicity in this proof. We continue to use our notation Œ≤ÃÇ j to refer the j-th column (here it is a
group) of BÃÇ, and Œ≤ÃÇij to refer the i-th element (task) in Œ≤ÃÇ j . We define X j = DIAG(X1j , ..., Xkj ) and Xij to be the j-th
column of Xi for task i. In other words, we allow for (k ‚àí 1)p number of 0 in Xij .
)
(
Œ≤ÃÇij
Œ≤ÃÇij
jT
‚àö
= 0, when Œ≤ÃÇij 6= 0, Œ≤ÃÇ j 6= 0,
‚àí 2Xi (Y ‚àí X Œ≤ÃÇ) + Œª Œ± j + (1 ‚àí Œ±)
j
kŒ≤ÃÇi k2
kŒ≤ÃÇ k2 / k
T
Œ≤ÃÇij
‚àö = ŒªŒ±vij , with kvij k2 ‚â§ 1, when Œ≤ÃÇij = 0, Œ≤ÃÇ j 6= 0,
‚àí 2Xij (Y ‚àí X Œ≤ÃÇ) + Œª(1 ‚àí Œ±)
kŒ≤ÃÇ j k2 / k


‚àö
T


‚àí2X j (Y ‚àí X Œ≤ÃÇ) ‚â§ Œª k, when Œ≤ÃÇ j = 0.

(57)

2

P
P
Let DŒª,Œæ
= {j ‚àà 1, 2, ..., p|group j is active for BÃÇ Œª,Œæ }. For each j in DŒª,Œæ
, we define Œ≤ÃÇ‚àój to be the vector of all Œ≤ÃÇij 6= 0.
P
Their corresponding columns Xij s from X j , would form a matrix X‚àój . For each j in DŒª,Œæ
, we define Œ≤ÃÇ‚àójC to be the vector
j
j
j
of all Œ≤ÃÇi = 0. Their corresponding columns Xi s from X j , would form a matrix X‚àóC . Then, from (57),
P
DŒª,Œæ

X

P
DŒª,Œæ

T
k2X‚àój (Y

‚àí X Œ≤ÃÇ)k22 ‚â• Œª2 (1 ‚àí Œ±)2 k

j=1

X kŒ≤ÃÇ‚àój k2
2
j=1

(58)

kŒ≤ÃÇ j k22

Based on the fact that ka + bk22 ‚â• (kak2 ‚àí kbk2 )2
P
DŒª,Œæ

X

T
k2X‚àójC (Y

P
DŒª,Œæ

‚àí

X Œ≤ÃÇ)k22

‚â•

j=1

‚àö kŒ≤ÃÇ jC k2
Œª(1 ‚àí Œ±) k ‚àó
‚àí ŒªŒ±kv‚àój C k2
kŒ≤ÃÇ j k2

X
j=1

P
DŒª,Œæ

)
‚àö kŒ≤ÃÇ jC k2 j
‚àó
=
Œª (1 ‚àí Œ±) k
+Œª Œ±
‚àí 2Œª Œ±(1 ‚àí Œ±) k
kv C k2
kŒ≤ÃÇ j k22
kŒ≤ÃÇ j k2 ‚àó
j=1
P (
"
#)
DŒª,Œæ
j
2
X
kŒ≤ÃÇ‚àójC k22
j
j
2
2 kŒ≤ÃÇ‚àóC k2
2 2
2
2
2
‚â•
Œª (1 ‚àí Œ±) k
+ Œª Œ± kv‚àóC k2 ‚àí Œª Œ±(1 ‚àí Œ±) k
+ kv‚àóC k2
kŒ≤ÃÇ j k22
kŒ≤ÃÇ j k22
j=1
X

(

!2

2

2

kŒ≤ÃÇ‚àójC k22

2 2

kv‚àój C k22

2

P
DŒª,Œæ

2

= Œª (1 ‚àí Œ±)(1 ‚àí 2Œ±)k

X kŒ≤ÃÇ jC k22
‚àó
j=1

kŒ≤ÃÇ j k22

P
DŒª,Œæ

2

‚àí Œª Œ±(1 ‚àí 2Œ±)

X
j=1

kv‚àój C k22

(59)

When can Multi-Site Datasets be Pooled for Regression?

Based on (58) and (59), we have
P
DŒª,Œæ

T
p (Y
k2XA
Œª,Œæ

‚àí

X Œ≤ÃÇ)k22

X

=

P
DŒª,Œæ

k2X

jT

(Y ‚àí

X Œ≤ÃÇ)k22

=

j=1

2

‚â• Œª (1 ‚àí Œ±) k

X kŒ≤ÃÇ‚àój k2
2
j 2
j=1 kŒ≤ÃÇ k2

P
DŒª,Œæ

2

+ Œª (1 ‚àí Œ±)(1 ‚àí 2Œ±)k

X kŒ≤ÃÇ jC k22
‚àó
j=1

X kŒ≤ÃÇ‚àój k22 + kŒ≤ÃÇ jC k22
‚àó

‚â• Œª (1 ‚àí Œ±)(1 ‚àí 2Œ±)k

+

kŒ≤ÃÇ j k22

j=1

X

T

k2X‚àójC (Y ‚àí X Œ≤ÃÇ)k22

(60)

kv‚àój C k22

(61)

j=1

kŒ≤ÃÇ j k22

P
DŒª,Œæ

2

‚àí

X Œ≤ÃÇ)k22

j=1

P
DŒª,Œæ

2

P
DŒª,Œæ

T
k2X‚àój (Y

X

P
DŒª,Œæ

2

‚àí Œª Œ±(1 ‚àí 2Œ±)

X
j=1

P
DŒª,Œæ

2

‚àí Œª Œ±(1 ‚àí 2Œ±)

X

kv‚àój C k22

(62)

j=1

P
P
‚â• Œª2 (1 ‚àí Œ±)(1 ‚àí 2Œ±)k|DŒª,Œæ
| ‚àí Œª2 Œ±(1 ‚àí 2Œ±)k|DŒª,Œæ
|
2

2

= Œª (1 ‚àí 2Œ±)

P
k|DŒª,Œæ
|

2

= Œª (1 ‚àí 2Œ±)

2

(63)

|AP
Œª,Œæ |

(64)

The next lemma provides an asymptotic upper bound on the number of selected variables, the proof of which is similar to
Lemma 5 in (Meinshausen & Yu, 2009).
Lemma 3.8. Assume conditions in Theorem 3.2, with probability converging to 1 for n ‚Üí ‚àû,


2
p
Œ± ‚àö
Œ±
ks
+
s
sup |AP
|
‚â§
1
+
p
h
Œª,Œæ
1 ‚àí 2Œ±
1 ‚àí 2Œ±
0‚â§Œæ‚â§1

(65)

Proof. Based on Lemma 3.7,
(1 ‚àí 2Œ±)2 kmp = (1 ‚àí 2Œ±)2 sup |AP
Œª,Œæ | ‚â§
0‚â§Œæ‚â§1

1
Œª,Œæ 2
T
p (Y ‚àí X Œ≤ÃÇ
)k2
sup k2XA
Œª,Œæ
Œª2 0‚â§Œæ‚â§1

(66)

We decompose the right side into two parts and then have
(1 ‚àí 2Œ±)2 kmp ‚â§

1
1
T
T
‚àó
Œª,Œæ
p X(Œ≤ ‚àí Œ≤ÃÇ
p k2
sup k2XA
sup k2XA
)k2 +
Œª,Œæ
Œª,Œæ
Œª 0‚â§Œæ‚â§1
Œª 0‚â§Œæ‚â§1

!2
(67)

Similarly, we know from proof in Lemma 3.5 that
T
2
2
p k ‚â§ 2kmp log(kp)œÉ nÃÑ
sup k2XA
2

0‚â§Œæ‚â§1

(68)

Œª,Œæ

Based on the definition of Œª, there exists a constant $1 > 0, such that
T
2
sup0‚â§Œæ‚â§1 k2XA
p k2
Œª,Œæ

Œª2

‚â§ $12 kmp

(69)

Therefore, we have
p
1
T
‚àó
Œª,Œæ
p X(Œ≤ ‚àí Œ≤ÃÇ
sup k2XA
)k2 + $1 kmp
Œª,Œæ
Œª 0‚â§Œæ‚â§1

(1 ‚àí 2Œ±)2 kmp ‚â§

!2
(70)

P
Define FŒª,Œæ
= {i : Œ≤i‚àó 6= 0} ‚à™ AP
Œª,Œæ . Based on the block trick we used in proof of Lemma 3.4,

T
‚àó
Œª,Œæ 2
‚àó
Œª,Œæ 2
p (Œ≤ ‚àí Œ≤ÃÇ
p X(Œ≤ ‚àí Œ≤ÃÇ
)k2 ‚â§ nÃÑ2 œÜ2max (sp + min{
kXA
)k2 ‚â§ kXFT p XFŒª,Œæ
Œª,Œæ

k
X

Œª,Œæ

ni , kp})kŒ≤ ‚àó ‚àí Œ≤ÃÇ Œª,Œæ k22

(71)

i=1

From the assumption on œÜmax (sp + min{

Pk

i=1

ni , kp}), we know

T
‚àó
Œª,Œæ 2
p X(Œ≤ ‚àí Œ≤ÃÇ
kXA
)k2 ‚â§ nÃÑ2 œÅ2max kŒ≤ ‚àó ‚àí Œ≤ÃÇ Œª,Œæ k22
Œª,Œæ

(72)

When can Multi-Site Datasets be Pooled for Regression?

Therefore, we have
2

(1 ‚àí 2Œ±) kmp ‚â§

‚â§

p
2
nÃÑœÅmax sup kŒ≤ ‚àó ‚àí Œ≤ÃÇ Œª,Œæ k2 + $1 kmp
Œª
0‚â§Œæ‚â§1

!2
(73)

p
2
2
nÃÑœÅmax kŒ≤ ‚àó ‚àí Œ≤ÃÇ Œª,0 k2 + nÃÑœÅmax sup kŒ≤ÃÇ Œª,0 ‚àí Œ≤ÃÇ Œª,Œæ k2 + $1 kmp
Œª
Œª
0‚â§Œæ‚â§1

!2
(74)

Because Œ≤ is the unfolded vector of B, actually sup0‚â§Œæ‚â§1 kŒ≤ÃÇ Œª,0 ‚àí Œ≤ÃÇ Œª,Œæ k2 = sup0‚â§Œæ‚â§1 kBÃÇ Œª,0 ‚àí BÃÇ Œª,Œæ kF . From Lemmas
3.5 and 3.6, definition of Œª and the assumption on œÜmin , we obtain the bound
4nÃÑ2 œÅ2max 2 log(kp) kmp
4nÃÑ2 œÅ2max
sup kŒ≤ÃÇ Œª,0 ‚àí Œ≤ÃÇ Œª,Œæ k22 ‚â§ C
œÉ 2 ‚â§ $22 kmp
2
Œª
Œª2
nÃÑ
œÜ2min (mp )
0‚â§Œæ‚â§1

(75)

Here, $2 > 0 is a constant. We define $ = $1 + $2 . Now, we obtain


2

(1 ‚àí 2Œ±) kmp ‚â§

p
2
nÃÑœÅmax kŒ≤ ‚àó ‚àí Œ≤ÃÇ Œª,0 k2 + $ kmp
Œª

2
(76)

By setting the constant term in Œª large enough, we can have $/(1 ‚àí 2Œ±) ‚â§ 5$ ‚â§ 0.026, and hence
kmp ‚â§ (18/17.5)2 (2œÅmax )2

nÃÑ2 kŒ≤ ‚àó ‚àí Œ≤ÃÇ Œª,0 k22
‚â§
(1 ‚àí 2Œ±)2 Œª2


1+

Œ±
1 ‚àí 2Œ±



p

ksp +

Œ± ‚àö
sh
1 ‚àí 2Œ±

2
(77)

The last inequality is obtained by plugging in Lemma 3.4. The constant can be 1 by setting the constant term in Œª large
enough.
Follow from Lemmas 3.5,3.6, and 3.8, the next lemma bounds the variance part of the sparse multi-sites Lasso estimator:
Lemma 3.9. Assume conditions in Theorem3.2, there exists a constant œâ2 > 0, with probability converging to 1 for
n ‚Üí ‚àû,
ksÃÑ log(kp)
(78)
kB Œª ‚àí BÃÇ Œª,1 k2F = kBÃÇ Œª,0 ‚àí BÃÇ Œª,1 k2F ‚â§ œâ2 œÉ 2
nÃÑ
Proof. We have defined B Œª as the estimator BÃÇ Œª,0 with no noise before Lemma 3.4.
Based on Lemmas 3.5 and 3.6
2 log kp kmp
kBÃÇ Œª,0 ‚àí BÃÇ Œª,1 k2F ‚â§
œÉ2
nÃÑ
œÜ2min (mp )

(79)

Based on Lemma 3.8, assumption on œÜmin and 0 ‚â§ Œ± ‚â§ 0.4,
kBÃÇ Œª,0 ‚àí BÃÇ Œª,1 k2F ‚â§

2 log kp kmp
ksÃÑ log(kp)
œÉ 2 ‚â§ œâ2 œÉ 2
2
nÃÑ
œÜmin (mp )
nÃÑ

(80)

The lemma 3.4 and 3.9 together complete the proof of Theorem 3.2
3.2. Proof of Theorem 3.3:
The proof is similar to that of Theorem 3.2. Recall that in this case, however, we do not penalize
Hence, we have the following result about bias contribution of Theorem 3.3.

‚àö

k on group penalty.

Lemma 3.10. Assume conditions in Theorem 3.3. The Frobenius norm of ŒìŒª is then bounded for sufficiently large values
of nÃÑ, given a constant œâ1 > 0, by
ksÃÉ log(kp)
kŒìŒª k2F ‚â§ œâ1 œÉ 2
(81)
nÃÑ

When can Multi-Site Datasets be Pooled for Regression?

Proof. The proof procedure is same as Lemma 3.4. But instead of (42), we now have
‚àö
‚àö
‚àö
Œõ(ŒìŒª ) ‚â§ 2{(1 ‚àí Œ±ÃÉ) sp + Œ±ÃÉ sh }kŒ≥ Œª k2 = 2 ksÃÉkŒ≥ Œª k2

(82)

‚àö
p
because we do not have k penalization on group penalty. Hence, in Lemma 3.10, we have sÃÉ = {(1 ‚àí Œ±ÃÉ) sp /k +
p
p
‚àö
Œ±ÃÉ sh /k}2 , instead of sÃÑ = {(1 ‚àí Œ±ÃÉ) sp + Œ±ÃÉ sh /k}2 .
Pk
For restricted OLS estimation, we redefine few things here. For every subset œà ‚äÇ {1, ..., kp} with |œà| ‚â§ i=1 ni , we
define Xœà to be the combination of columns from design matrix X, where X = DIAG(X1 , X2 , ..., Xk ). The restricted
OLS-estimator of the noise vector T = (1 , ..., k )T is then given by,
Œ∏ÃÇœà = (XœàT Xœà )‚àí1 XœàT 

(83)

For the variance contribution, the proof is similar to that of Theorem 3.2. We present the required Lemmas for Theorem
3.3 here.
Lemma 3.11. Let mn be a sequence with mn = o(knÃÑ) and mn ‚Üí ‚àû for nÃÑ ‚Üí ‚àû. It holds with probability converging to
1 for n ‚Üí ‚àû
mn
2 log kp
œÉ2
(84)
max kŒ∏ÃÇœà k22 ‚â§
nÃÑ
œÜ2min (mn )
œà:|œà|‚â§mn
Lemma 3.12. If, for a fixed value of Œª, the number of active variables of de-noised estimators BÃÇ Œª,Œæ is for every 0 ‚â§ Œæ ‚â§ 1
bounded by m, then
sup kBÃÇ Œª,0 ‚àí BÃÇ Œª,Œæ k2F ‚â§ C max kŒ∏ÃÇœà k22
(85)
œà:|œà|‚â§m

0‚â§Œæ‚â§1

with C as a generic constant.
Let A1Œª,Œæ be the set of active variables of the de-noised estimator BÃÇ Œª,Œæ . Let mn to be the largest number of active variables
over all values of 0 ‚â§ Œæ ‚â§ 1. Then we have mn = sup0‚â§Œæ‚â§1 |A1Œª,Œæ |.
Lemma 3.13. For any 0 ‚â§ Œ±ÃÉ ‚â§ 1, we have
T
|A1Œª,Œæ |Œª2 Œ±ÃÉ2 ‚â§ k2XA
(Y ‚àí X Œ≤ÃÇ Œª,Œæ )k22
1
Œª,Œæ

(86)

where we defined before that X = DIAG(X1 , ..., Xk ), Y T = (Y1T , ..., YkT ). Œ≤ÃÇ Œª,Œæ is the transpose of unfolded vector of
BÃÇ Œª,Œæ by rows. XA1Œª,Œæ is Xœà when œà = A1Œª,Œæ
Lemma 3.14. Assume conditions in Theorem 3.3, with probability converging to 1 for n ‚Üí ‚àû,
sup |A1Œª,Œæ | ‚â§



‚àö

sh +

0‚â§Œæ‚â§1

1 ‚àí Œ±ÃÉ ‚àö
sp
Œ±ÃÉ

2
(87)

Lemma 3.15. Assume conditions in Theorem3.3, there exists a constant œâ2 > 0, with probability converging to 1 for
n ‚Üí ‚àû,
ksÃÉ log(kp)
kB Œª ‚àí BÃÇ Œª,1 k2F = kBÃÇ Œª,0 ‚àí BÃÇ Œª,1 k2F ‚â§ œâ2 œÉ 2
(88)
nÃÑ
Lemma 3.10 and Lemma 3.15 complete the proof of Theorem 3.3

When can Multi-Site Datasets be Pooled for Regression?

3. Extra set of simulations (corresponding to Section 4.1 in the main paper)
3.1. Hypothesis Test Simulation when p = 6

Square Root of MSE

MSE
Single site
Two sites

5.0

MSE
Œ≤ single site
Œ≤ two sites
Œ≥1 single site
Œ≥1 two sites

75

Square Root of MSE

7.5

50

25

2.5

4

5

6

7

8

9

10

11

12

4

5

6

Sample Size (log2 scale)

7

8

9

10

11

12

Sample Size (log2 scale)

(a)

(b)
1.00

0.75

Sufficient Condition

Same MSE

0.50

Acceptance Rate

Acceptance Rate

0.75

Sufficient Condition

Same MSE

0.50

0.25

0.25

0.00

0.00
4

5

6

7

8

9

10

11

12

4

5

Sample Size (log2 scale)

6

7

8

9

10

11

12

Sample Size (log2 scale)

(c)

(d)

Figure 1. The figure is similar to the simulations done in Figure 3 (which is also the one Figure 3 in the main paper). However, here the
dimension p of Œ≤ is 6 instead of 3. (a,c) are MSE of Œ≤ÃÇ and the corresponding acceptance rate of our hypothesis test (from Section2.1).
(b,d) are MSE of Œ≤ÃÇ and Œ≥ÃÇ1 and the corresponding acceptance rate (from Section2.2). These are based on 100 bootstrap repetitions. The
solid line in (c,d) represents the point where the condition from Theorem 2.3 is equal to 1. The dotted line is when MSE of Œ≤ÃÇ is the same
for single-site and multi-site models.

3.2. Sparse Multi-Sites Lasso Simulation
Table 1. Add multi-sites Lasso on Lasso.
Œ±
CDR
CDV
CDG

0

0.05

0.95

0.97 ( OUR )

1

0.1423
78
5

0.1463
78
5

0.2747
75
3

0.2863
75
3

0.2955
73
1

We report correctly discovered number of active variables (CDV), ratio of CDV and total number of discovered variables
(CDR), and correctly discovered number of always-active features (CDG).
From Table1 and Table 2 we see that our chosen Œ± helps sparse multi-sites Lasso to discover more or preserve always-active
features. The number and rate of correctly discovered number of active variables given by our chosen Œ± are also among
the best.

When can Multi-Site Datasets be Pooled for Regression?
Table 2. Add Lasso on multi-sites Lasso.
Œ±
CDR
CDV
CDG

0

0.05

0.25 ( OUR )

0.95

1

0.2292
80
16

0.2381
80
16

0.2453
79
15

0.2841
75
11

0.2885
73
11

number of always‚àíactive features

number of always‚àíactive features

3.3. Figure Examples for Choosing Œ±

16

6

14

4

12

Type
Correct discovered(truth)
Correct discovered(estimate)

2

10
0.00

0.25

0.50

Œ±

(a)

0.75

1.00

Type
Correct discovered(truth)
Correct discovered(estimate)
0.00

0.25

0.50

Œ±

0.75

1.00

(b)

Figure 2. These plots show that site-active set from simultaneous inference provides information of always-active features (which is then
used to choose the hyper-parameters Œ± an Œª). In (a), we add Lasso on multi-sties Lasso, and Œ± = 0.25 is chosen. Similarly, in Figure(b),
we add multi-sites Lasso on Lasso, and Œ± = 0.97 is chosen.

We here point out a caveat about our choice of Œ± when sparsity patterns share few features and always-active features exist.
In this setting, we do want to discover more always-active features. Hence, we decrease Œ± from 1 and stop at the point
where we just select one more always-active feature. In other words, we choose the Œ± left to the one described in main
body.

When can Multi-Site Datasets be Pooled for Regression?

4. Longer version of Section 4 from the main paper

MSE
Single site
Two sites

1.5
1
0.5

0.25
0.125

Square Root of MSE

Square Root of MSE

MSE
Œ≤ single site
Œ≤ two sites
Œ≥1 single site
Œ≥1 two sites

16

2

11

6

1

0.5
0.25
0.125

0

0

4

5

6
7
8
9 10 11
Sample Size (log2 scale)

12

4

5

6

7

8

9

10

11

12

Sample Size (log2 scale)

(a)

(b)

1.00

Acceptance Rate

0.75

Acceptance Rate

1.00

0.75

Sufficient Condition

Same MSE

0.50

Sufficient Condition

Same MSE

0.50

0.25

0.25

0.00

0.00
4

5

6

7

8

9

10

Sample Size (log2 scale)

(c)

11

12

4

5

6

7

8

9

10

11

12

Sample Size (log2 scale)

(d)

Figure 3. (a,c) are MSE of Œ≤ÃÇ and the corresponding acceptance rate of our hypothesis test (from Section2.1). (b,d) are MSE of Œ≤ÃÇ and Œ≥ÃÇ1
and the corresponding acceptance rate from Section 2.2). These are based on 100 bootstrap repetitions. The solid line in (c,d) represents
the point where the condition from Theorem 2.3 is equal to 1. The dotted line is when MSE of Œ≤ÃÇ is the same for single-site and multi-site
models.

We now provide few more details about the different curves observed in Figure 3, beyond what is reported in the main paper
due to space constraint. First, we check whether the gap between the sufficient condition (from Theorem 2.3) and the point
where single-site and multi-site models have same MSE is small. The solid lines in Figure 3(c,d) correspond to the point
where the condition value defined in Theorem 2.3 is equal to 1. The dotted lines (where condition value is approximately
3.3) are the points where the MSE of multi-site model starts to increase above the MSE of single-site one. In other words,
to the left of the dotted lines that MSE of Œ≤ÃÇ from multi-sites model is smaller than single-site model. To the right of these
lines it is larger. We see that the gap is reasonably small. We then check the type I error of our hypothesis test. On the
left side of solid lines, the sufficient condition holds and our hypothesis test accepts the combination with high rate around
95%, i.e., the type I error is well-controlled. Further, the power of our hypothesis test is evident when MSE of Œ≤ÃÇ from
multi-sites model is worse than single-site model. Though our sufficient condition is conservative for the combination, by
noticing that œá2 test is progressive, our test has a high power on the right side of dotted line. In the regime between the two
lines, the multi-sites model has slightly better MSE of Œ≤ÃÇ compared to single-site model, and our hypothesis test accepts the
combination with high rate.

When can Multi-Site Datasets be Pooled for Regression?

2.5

1.7

Œ± = 0 (group lasso)
Œ± = 0.05 (fixed choice)
Œ± = 0.95 (fixed choice)
Œ± = 0.97 (our method)
Œ± = 1 (lasso)

Square Root of Cross‚àíValidation Error

Square Root of Cross‚àíValidation Error

3.0

2.0

1.5
0.01

0.02

Œª

(a)

0.03

0.04

0.05

1.6

Œ± = 0 (group lasso)
Œ± = 0.05 (fixed choice)
Œ± = 0.25 (our method)
Œ± = 0.95 (fixed choice)
Œ± = 1 (lasso)

1.5

1.4

1.3
0.01

0.02

Œª

0.03

0.04

0.05

(b)

Figure 4. (a) shows the solution path of Œª when sparsity patterns share few features across sites, and group Lasso penalty is added to
balance Lasso penalty. (b) shows the alternate regime where sparsity patters are similar. The `2 loss is plotted, based on 10-fold cross
validation.

4.1. AD dataset details
The two datasets we use are ‚Äì an open-source Alzheimer‚Äôs Disease Neuroimage Initiative (ADNI) dataset, and a local
dataset (ADlocal). ADNI is an open consortium with the goal of understanding AD related cognitive decline, and in the
process, develop clinical interventions aimed at delaying the disease onset. ADlocal corresponds to a recent (smaller)
initiative local study for the AD related decline. We used 318 samples from ADNI and 156 samples from ADlocal. The
input variables are 8 Cerebrospinal fluid (CSF) protein levels, and the response is hippocampus volume. The CSF proteins
are ‚Äú1-38-Tr‚Äù, ‚Äú1-40-Tr‚Äù, ‚Äú1-42-Tr‚Äù, ‚ÄúNFL‚Äù, ‚ÄùAB42‚Äù, ‚Äúhtau‚Äù, ‚Äúptau181 ‚Äù, and ‚ÄúNeurogranin‚Äù. The two datasets have
different age and diagnosis distributions, and hence, we subsample 81 samples from either of sites to control age and
diagnosis variation. Using these 81 samples from each dataset, we perform domain adaptation (using a maximum mean
discrepancy objective as a measure of distance between the two marginals) and transform CSF proteins from ADlocal to
match ADNI. The transformed data are then used to evaluate our proposed framework. The results in Figure 5 are already
explained in the main body.

When can Multi-Site Datasets be Pooled for Regression?

Same sample sizes

1.2

Method
local (before) + ADNI
local (after) + ADNI
ADNI

1.15
1.10
1.05

Method
local (before) + ADNI
local (after) + ADNI
ADNI

1.15
Square Root of MPE

Square root of MPE

1.25

1.10

1.05

32
64
95
127 159 191 223 254 286
(10%) (20%) (30%) (40%) (50%) (60%) (70%) (80%) (90%)
Sample size of labeled ADNI

51
(16%)

1.00

1.00

0.75

0.75

Power increases

0.50

Method
local (before) + ADNI
local (after) + ADNI

0.25

127
(40%)

(b)

Acceptance Rate

Acceptance Rate

(a)

76
102
(24%)
(32%)
Sample size

Method
local (before) + ADNI
local (after) + ADNI

0.50
Power increases
0.25

0.00
32
64
95
127
159
191
223
254
286
(10%) (20%) (30%) (40%) (50%) (60%) (70%) (80%) (90%)
Sample size of labeled ADNI

(c)

51
(16%)

76
102
(24%)
(32%)
Sample size

127
(40%)

(d)

Figure 5. Evaluating combined models. (a,c) In this first setting x-axis represents number/fraction of ADNI labeled samples used in
training along with ADlocal labeled data. The dotted line in (a) is where the sample sizes of ADNI and ADlocal in training datasets
match. y-axis shows square root of mean prediction error (computed on the remaining unused ADNI data) scaled by estimated noise
level in ADNI responses. Error bars give 95% confidence interval. (c) shows the acceptance rate of our hypothesis test. (b,d) show the
same evaluations for the alternate setting where equal number of ADNI and ADlocal samples are used for training.

References
Liu, Han and Zhang, Jian. Estimation consistency of the group lasso and its applications. In AISTATS, pp. 376‚Äì383, 2009.
Meinshausen, Nicolai and Yu, Bin. Lasso-type recovery of sparse representations for high-dimensional data. The Annals
of Statistics, pp. 246‚Äì270, 2009.
Simon, Noah, Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. A sparse-group lasso. Journal of Computational
and Graphical Statistics, 22(2):231‚Äì245, 2013.

