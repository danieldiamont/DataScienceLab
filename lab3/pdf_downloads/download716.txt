When can Multi-Site Datasets be Pooled for Regression?
Hypothesis Tests, `2 -consistency and Neuroscience Applications
(Supplementary Material)

Hao Henry Zhou 1 Yilin Zhang 1 Vamsi K. Ithapu 1 Sterling C. Johnson 1 2 Grace Wahba 1 Vikas Singh 1
In this supplementary document we first present the proofs of all the technical results in Section 2 and 3 of the main paper.
We then expand upon the Section 4 and present extra experiments to strengthen the evaluations from the main paper.
Remarks on transformations in pre-processing step: For all i ∈ {1, ..., k}, after applying the transformation (shift
correction), we pool (Xi , yi ) together to estimate β ∗ . Note that in general the transformation (shift correction) should not
depend on the responses yi , otherwise we get a dependence on the noise. To see this, notice that yi = Xi βi + i where
Xi is the transformed set of features. But when the transformation depends on yi , then Xi will also depend on i , which
causes a poor estimation of β ∗ (and βi ). In situations where the transformations must involve yi , a sensible strategy is to
separate each site’s dataset into two parts, where one part from each site is used to learn the transformation, and the other
part (after applying the learned transformation) is used for pooling towards β ∗ estimation and conducting our hypothesis
test.

1. Proof of Section 2
We now provide the proofs of the results presented in the main paper.
Theorem 2.1. τi =

σ1
σi

achieve the smallest variance in β̂.

Proof. The choice of τi leads to weighted least squares, which is known to be the best linear unbiased estimator (BLUE)
under uncorrelated heteroscedastic errors. The variance of β̂ is equivalent to the case when ∆βi = 0. In the latter case,
BLUE condition holds and setting τi to the above value achieves lowest variance. The equivalence between variances
under two cases completes the proof.

Lemma 2.2. For multi-site model, we have
kBiasβ k22
≤ k(Σ̂k1 )−2 (Σ̂k2 (n1 Σ̂1 )−1 Σ̂k2 + Σ̂k2 )k∗ ,
kG−1/2 ∆βk22




V arβ = σ12 (n1 Σ̂1 )−1 − (n1 Σ̂1 + Σ̂k2 )−1  .
∗

(1)
(2)

Proof. The estimation from single site model is unbiased, and it has the following variance.
V ar1 = tr((X1T X1 )−1 )σ12 = tr((n1 Σ̂1 )−1 )σ12

(3)

1

University of Wisconsin-Madison 2 William S. Middleton Memorial Veteran’s Affairs Hospital. Correspondence to: Hao Zhou
<hzhou@stat.wisc.edu>, Vikas Singh <vsingh@biostat.wisc.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

When can Multi-Site Datasets be Pooled for Regression?

The estimation error from multi-sites model has the following closed form expression


X1
 τ2 X2
∗


β̂ − β = 
..
τk Xk
T 

X1
 τ2 X2  

 
+
 ..  
τk Xk

−1

X1
τ2 X2
  τ2 X2 
 
  ..
  .. 
τk Xk
τk Xk
T 
−1 
X1
X1
 
 
τ2 X2 
  τ2 X2  



..  
..
τk Xk
τk Xk
T 

T 


τ2 X2 (∆β2 )
 

..
τk Xk (∆βk )
(4)

1
τ2 2 

.. 
τk k

First term in the summation from (4) is bias, while second term is variance. We can see that our choice of τi =
heteroscedastic errors issue among sites. We further simplify bias and variance terms, and obtain
V ar2 = tr((n1 Σ̂1 +

k
X

σ1
σi

resolves

ni τi2 Σ̂i )−1 )σ12

(5)

i=2

The reduced variance statement is proved. For the bias term, it is equivalent as shown below.
T 
X1
X1
 τ2 X2   τ2 X2

 
 ..   ..
τk Xk
τk Xk


−1




T 
τ2 X2
τ2 X2
 ..   0
0
τk Xk


0
τ3 X3
0

...
...
...





∆β2 
0

0  G1/2 G−1/2  .. 


τk Xk
∆βk

(6)

A one step Cauchy Schwartz inequality is then applied. Then our final proof is to show k..k2F on
T 
X1
X1
 τ2 X2   τ2 X2

 
 ..   ..
τk Xk
τk Xk


−1




T 
τ2 X2
τ2 X2
 ..   0
τk Xk
0


0
τ3 X3
0

...
...
...


0
0  G1/2
τk Xk

(7)

is equal to right side of the bias relaxation in (1).
It is easy to see that kAk2F = kAT Ak∗ . Based on this, we can see the first term of matrix inverse contributes the (Σ̂k1 )−2
in (1). Let the other part in (7) be L. We have
T  2 T
 
T 

τ22 X2T X2
τ2 X2 X2
n2 τ22 Σ̂2
n2 τ22 Σ̂2
 G
=
 G

..
..
..
..
LLT = 
τk2 XkT Xk
τk2 XkT Xk
nk τk2 Σ̂k
nk τk2 Σ̂k


(8)

After some manipulations, this becomes (Σ̂k2 (n1 Σ̂1 )−1 Σ̂k2 + Σ̂k2 ). The bias part is proved.
Theorem 2.3. a): The multi-sites model has smaller MSE of β̂ than single-site model whenever

2


H0 : G−1/2 ∆β  ≤ σ12 .

(9)

2

b): Further, we have the following test statistic,


 G−1/2 ∆β̂ 2



 ∼ χ2(k−1)∗p


σ1
2

where kG−1/2 ∆β/σ1 k2 is called a “condition value”.

 −1/2
2 !
G

∆β

 ,


σ1
2

(10)

When can Multi-Site Datasets be Pooled for Regression?

Proof. (a): Based on Lemma 2.2, the theorem is proved when right side in (9) is replaced by




σ12 (n1 Σ̂1 )−1 − (n1 Σ̂1 + Σ̂k2 )−1 
∗

k(Σ̂k1 )−2 (Σ̂k2 (n1 Σ̂1 )−1 Σ̂k2 + Σ̂k2 )k∗
We first calculate the numerator

h


i




σ12 (n1 Σ̂1 )−1 − (n1 Σ̂1 + Σ̂k2 )−1  = σ12  (n1 Σ̂1 )−1 (n1 Σ̂1 + Σ̂k2 ) − I (n1 Σ̂1 + Σ̂k2 )−1 
∗
∗


2
−1 k
k −1 
= σ1 (n1 Σ̂1 ) Σ̂2 (n1 Σ̂1 + Σ̂2 ) 

(11)

(12)

∗

The denominator is then given by
k(Σ̂k1 )−2 (Σ̂k2 (n1 Σ̂1 )−1 Σ̂k2 + Σ̂k2 )k∗ = k(Σ̂k1 )−2 ((Σ̂k2 + n1 Σ̂1 )(n1 Σ̂1 )−1 Σ̂k2 )k∗
Remember
=

k((Σ̂k2

Σ̂k1

+

= Σ̂k2 + n1 Σ̂1 , , we continue
n1 Σ̂1 )−1 (n1 Σ̂1 )−1 Σ̂k2 )k∗ = k((n1 Σ̂1 )−1 Σ̂k2 (n1 Σ̂1

(13)
(14)

+

Σ̂k2 )−1 )k∗

(15)

The last step uses the property of k..k∗ norm. The proof is completed by noticing the simplified form of numerator and
denominator. It is clear now that the right side in (9) is exactly σ12 .
(b): First, we show σ12 G is the covariance matrix of ∆β̂. We have
cov(∆β̂i , ∆β̂j ) = cov(β̂i , β̂j ) − cov(β̂i , β̂1 ) − cov(β̂1 , β̂j ) + cov(β̂1 , β̂1 )

(16)

Since each site is independent from other site, we have

(17)

cov(∆β̂i , ∆β̂j ) = cov(β̂1 , β̂1 ) =

σ12 (n1 Σ̂1 )−1 for

i 6= j

(18)

cov(∆β̂i , ∆β̂i ) = cov(β̂i , β̂i ) + cov(β̂1 , β̂1 ) = σ12 ((n1 Σ̂1 )−1 + (ni (σ12 /σi2 )Σ̂i )−1 ) = σ12 ((n1 Σ̂1 )−1 + (ni τi2 Σ̂i )−1 )
(19)
∆β̂ follows Gaussian distribution since it is a linear transformation of Gaussian distribution. It’s expectation is ∆β since
each β̂i is an unbiased estimator. Hence, we have
∆β̂ ∼ N (∆β, σ12 G)

(20)

This distribution result, and noticing the connection between Gaussian and non-central χ2 distributions completes the
proof.
Corollary 2.4. For the case where we have two participating sites, the condition (9) from Theorem 2.3 reduces to
H0 : ∆β T ((n1 Σ̂1 )−1 + (n2 τ22 Σ̂2 )−1 )−1 ∆β ≤ σ12 .

(21)

Proof. The proof is follows by noticing the form of G when k = 2.
Theorem 2.5. Analysis in Section 2.1 holds for β in model with Z confounding features, when we replace Σ̂i with
Σ̃i = Σ̂xxi − Σ̂xzi (Σ̂zzi )−1 Σ̂zxi .

T
Proof. Define γ T = (γ1T , ..., γkT ), Xall
= X1T , τ2 X2T , ..., τk XkT , Zall = Diag (Z1 , τ2 Z2 , ..., τk Zk ). We have


0

  ∗   T



−1
T
T
 τ2 X2 (∆β2 ) 
Xall Xall Xall
β
Zall
Xall
β̂

+
=
−
∗
T
T
T


γ
..
Zall Xall Zall Zall
Zall
γ̂
τk Xk (∆βk )


1
 T
−1  T 
T

Xall Xall Xall
Zall
Xall 
τ
2
 2 
T
T
T

.. 
Zall Xall Zall Zall
Zall
τk k

(22)

(23)

When can Multi-Site Datasets be Pooled for Regression?

Using sub-matrix inverse property, we obtain
 T
−1  T  

T
T
T
Xall Xall Xall
Zall
Xall
(X̃all
X̃all )−1 X̃all
=
T
T
T
T
T
Zall
Xall Zall
Zall
Zall
(Z̃all
Z̃all )−1 Z̃all

(24)

We then have
T
T
Z̃all = (I − Xall (Xall
Xall )−1 Xall
)Zall

(25)
Z1 (Z1T Z1 )−1 Z1T )X1
Z2 (Z2T Z2 )−1 Z2T )X2



T
T
X̃all = (I − Zall (Zall
Zall )−1 Zall
)Xall



(I −
 (I −


=


..
(I − Zk (ZkT Zk )−1 ZkT )Xk

(26)

Define
HZi = (I − Zi (ZiT Zi )−1 ZiT )

(27)




1
0




T
−1 T  τ2 2 
T
T  τ2 X2 (∆β2 ) 
β̂ − β ∗ = (X̃all
X̃all )−1 X̃all
 + (X̃all X̃all ) X̃all  .. 

..
τk k
τk Xk (∆βk )


1
k
X

τ
2 
2
T
T
T 

= (X̃all
X̃all )−1
τi X̃iT Xi (∆βi ) + (X̃all
X̃all )−1 X̃all
 .. 
i=2
τk k

(28)

Hence, we have


We also observe that
X̃iT Xi = XiT HZi Xi = XiT HZ2 i Xi = X̃iT X̃i

(29)

Therefore, we can apply our previous results to a subset of parameters if we replace Xi by X̃i . Since our results only
depend on Σ̂i , we only need to replace it by
1
1 T
X̃i X̃i = XiT HZi Xi = Σ̂xxi − Σ̂xzi (Σ̂zzi )−1 Σ̂zxi
ni
ni

(30)

This proves the theorem.

2. Proof of Section 3
Definition 3.1. The m-sparse minimal and maximal eigenvalues of C, denoted by φmin (m) and φmax (m), are
ν T Cν
ν:kνk0 ≤dme ν T ν
min

and

ν T Cν
ν:kνk0 ≤dme ν T ν
max

(31)

We first list down the two key theorem statements that we prove in this section.
Theorem 3.2. Let 0 ≤ α ≤ 0.4. Assume there exist constants 0 ≤ ρmin ≤ ρmax ≤ ∞ such that

2 !
2α
lim inf φmin sp 1 +
≥ ρmin , and
n→∞
1 − 2α
k
X
lim sup φmax (sp + min{
ni , kp}) ≤ ρmax .
n→∞

(32)

i=1

p
Then, for λ ∝ σ n̄ log(kp), there exists a constant ω > 0 such that, with probability converging to 1 for n → ∞,
1 λ
s̄ log(kp)
kB̂ − B ∗ k2F ≤ ωσ 2
,
k
n̄
p
√
where s̄ = {(1 − α) sp + α sh /k}2 , σ is the noise level.

(33)

When can Multi-Site Datasets be Pooled for Regression?

Theorem 3.3. Let 0.4 ≤ α̃ ≤ 1. Assume there exist constants 0 ≤ ρmin ≤ ρmax ≤ ∞ such that
2 !

(1 − α̃)
lim inf φmin sh 1 +
≥ kmin , and
n→∞
α̃
k
X
lim sup φmax (sh + min{
ni , kp}) ≤ kmax .
n→∞

(34)

i=1

p
Then, for λ̃ ∝ σ n̄ log(kp), there exists a ω > 0 such that, with probability converging to 1 for n → ∞, we have
s̃ log(kp)
1 λ
kB̂ − B ∗ k2F ≤ ωσ 2
,
k
n̄

(35)

p
p
with s̃ = {(1 − α̃) sp /k + α̃ sh /k}2 instead of s̄.
√
Comment about Theorem 3.3: We do not penalize by k when the sparsity patterns across sites share few of the features.
To√
see this, first observe that when sparsity patterns are similar, most√ofp
the groups we have are non-sparse, and the effects
of kkβ j k2 and kβ j k1 have the same scale. This is simply because, k a21 + ... + a2k is close to |a1 |+...+|ak | whenever
|a1 |, ..., |ak | are close. However when sparsity patterns across sites share few
p features only, most of the groups are going
to be sparse. For these groups, we should use kβ j k2 , because in this setting a21 + 0 + ... + 0 is close to |a1 | + 0 + ... + 0.
3.1. Proof of Theorem 3.2:
We follow the proof procedure from Lasso (Meinshausen & Yu, 2009) and group Lasso (Liu & Zhang, 2009) results. Let
B λ be the estimator under the absence of noise, i.e., B λ = B̂ λ,0 , where B̂ λ,ξ is defined as in (37). The `2 -distance can
then be bounded by kB̂ λ − B ∗ k2F ≤ 2kB̂ λ − B λ k2F + 2kB λ − B ∗ k2F . The first term on the right-hand side represents the
variance of the estimation, while the second term represents the bias. The bias contribution follows directly from Lemma
3.4 below, and the variance bound term follows from Lemma 3.9.
De-noised response. For 0 < ξ < 1, we define a de-noised version of the response variable as follows,
Yi (ξ) = Xi βi + ξi

(36)

We can regulate the amount of noise with the parameter ξ.
For ξ = 0, only the signal is retained. The original observations with the full amount of noise are recovered for ξ = 1.
Now consider for 0 ≤ ξ ≤ 1 the estimator B̂ λ,ξ ,
B̂ λ,ξ = arg min
B

k
X

kYi (ξ) − Xi βi k22 + λΛ(B)

i=1

p
p
X
√ X
Λ(B) = (1 − α) k
kβ j k2 + α
kβ j k1
j=1

(37)

j=1

The ordinary sparse multi-site Lasso estimate is recovered under the full amount of noise so that B̂ λ,1 = B̂ λ . Using
the notation from the previous results, we have B̂ λ,0 = B λ , for the estimate in the absence of noise. The definition of
the de-noised version of the sparse multi-site Lasso estimator will be helpful for the proof as it allows to characterize the
variance of the estimator.
3.1.1. PART I OF PROOF – D EALING WITH BIAS
Let P∗ be the set of nonzero groups of B ∗ , i.e., P∗ = {j : β j 6= 0}. The cardinality of P∗ is denoted by sp . For each j
in P∗ , let Hj be the set of nonzero elements of βj , i.e., Hj = {i : βij 6= 0}. The number of all nonzero elements of B is
denoted by sh . For the following, let B λ be the estimator B̂ λ,0 with no noise (as defined in (37)). For each λ, the solution
B λ can be written as B λ = B ∗ + Γλ . We define γ j and γi to be j-th column and i-th row of Γ. γ is the transpose of the
α
. Then
unfolded vector of Γ by row. Denote λ2 = λ(1 − α) and η = 1−α
Γλ = arg min f (Γ)
Γ

(38)

When can Multi-Site Datasets be Pooled for Regression?

The function f (Γ) is given by


X √

X
√
f (Γ) = n̄γ T Cγ + λ2
( Kkγ j k2 + ηkγ j k1 ) +
K(kβ j + γ j k2 − kβ j k2 ) +


j∈P∗
j∈P∗C
)
(
X
X
j
j
C
j
j
ηkγ Hj k1
λ2
η(kβHj + γHj k1 − kβ Hj )k +
j∈P∗

(39)

j∈P∗

The matrix Γλ is the bias of the sparse multi-site Lasso estimator. We derive first a bound on the Frobenius norm of Γλ .
Lemma 3.4. Assume conditions in Theorem3.2. The Frobenius norm of Γλ is then bounded for sufficiently large values of
n̄, given a constant ω1 > 0, by
ks̄ log(kp)
kΓλ k2F ≤ ω1 σ 2
(40)
n̄
Proof. f (Γ) = 0 whenever Γ = 0 following the definition from (39). For the true solution Γλ , it follows hence that
f (Γλ ) ≤ 0. For notational simplicity, we drop the super-script λ from here on. Using γ T Cγ ≥ 0, we have
 (

)

X √
X
X
X√
X
j
j
j
j
j
ηkγH C k1 ≤
ηkγHj k1
(ηkγ k1 ) +
kkγ k2 +
( kkγ k2 ) +


j
C
C
j∈P∗

Since |P∗ | = sp ,
using (41),

P

j∈P∗

j∈P∗

j∈P∗

j∈P∗

(41)

j∈P∗

P
√
j
sp kγk2 , j∈P∗ kγH
k ≤ sh kγk2 , and hence,
j 1
√
p
√
Λ(Γ) ≤ 2{(1 − α) ksp + α sh }kγk2 = 2 ks̄kγk2
(42)

|Hj | = sh . It follows that

P

j∈P∗ kγ

j

k2 ≤

√

Using f (Γ) ≤ 0 again and (42), it follows that
√
n̄γ T Cγ ≤ 2λ ks̄kγk2

(43)

Now consider γ T Cγ. Bounding this term from below and plugging the result into (42) will yield the desired upper bound
on the Frobenius norm of Γ. Let kγ (1) k ≥ kγ (2) k ≥ ... ≥ kγ (p) k be the ordered columns of Γ. Let un for n ∈ N be a
sequence of positive integers, to be chosen later, and define U = {j : kγ j k2 ≥ kγ (un ) k2 }. Define γ(U ) and γ(U C ) by
setting γ j (U ) = γ j 1{i ∈
/ U } and γ j (U C ) = γ j 1{i ∈ U }, followed by unfolding Γ. Then quantity γ T Cγ can be written
T
2
as γ Cγ = ka + bk2 , where a := n̄−1/2 Xγ(U ), b := n̄−1/2 Xγ(U C ), X = DIAG(X1 , ..., Xk ). Then
γ T Cγ = ka + bk22 ≥ (kak2 − kbk2 )2

(44)

Before proceeding, we need to bound the norm kγ(U C )k2 as a function of un . Assume l =
every j = 1, ..., p that kγ (j) k2 ≤ l/j. Hence,
kγ(U C )k22 ≤ (

p
X

kγ j k2 )2

j=1

Therefore, we have
C

kγ(U )k2 ≤

p
X
j=1

j

kγ k2

p
X
j=un

r

1
j2
+1

1
≤ kγk1
un

r

Pp

j=1

kγ j k2 . It holds for

(45)

1
un

√ Pp
Based on (42), Λ(Γ) = (1 − α) k j=1 kγ j k2 + αkγk1 , and (46), it follows that

!2 
√
 1

ks̄
√
kγ(U C )k22 ≤ 4kγk22
 un (1 − α) k + α 

(46)

(47)

By definition, since γ(U ) has only un nonzero groups,
kak22 = kγ(U )T Cγ(U )k22 ≥ φmin (un )kγ(U )k22 ≥
(
2 )!

1
ks̄
2
√
φmin (un )kγk2 1 − 4
un (1 − α) k + α

(48)

When can Multi-Site Datasets be Pooled for Regression?

Here we explain why we obtain φmin (un ) instead of φmin (kun ). We denote φimin (m) to be m-sparse of n̄−1 XiT Xi . Then
φmin (m) = minki=1 φimin (m) because of block structure. Since we have un nonzero groups, instead of arbitrary kun
nonzero elements, we obtain a higher value φmin (un ) = minki=1 φimin (un ) instead of φmin (kun ). This is the one place
where we consider the block structure of multi-site design.
Pk
As γ(U C ) has at most min{ i=1 ni , kp} nonzero groups, using again (47), (42) and the block structure of multi-site
design,
kbk22 ≤ 4φmax (min{

k
X

(
ni , kp})kγk22

i=1

1
un

!2 )
√
ks̄
√
(1 − α) k + α

(49)

Pk
Using (49), (48) and (44), along with φmax (min{ i=1 ni , kp}) ≥ φmin (un ),
v
)
(
u
√
P
u φmax (min{ k ni , kp})
1
ks̄
T
2
i=1
t
2
√
γ Cγ ≥ φmin (un )kγk2 × 1 − 4
) 
(
φmin (un )
un (1 − α) k + α


Using conditions in Theorem 3.2 and setting un =



√

ks̄
√
(1−α) k+α

2

(50)

, it follows that



r
ρmax
γ T Cγ ≥ ρmin 1 − 4
kγk22
ρmin

(51)

√
Using this result together with (43), which says that γ T Cγ ≤ 2n̄−1 λ ks̄kγk2 , we have the following for large n̄,
kΓk2F = kγk22 ≤

λ2 ks̄
1
√
2
(ρmin − 4 ρmin ρmax ) n̄2

(52)

The proof of Lemma 3.4 is completed by noticing λ in Theorem 3.2.
3.1.2. PART II OF PROOF – D EALING WITH VARIANCE
The proof for the variance part is two-fold. We first derive a bound on the variance, which is a function of the number of
nonzero groups. We then bound the number of nonzero groups, taking into account the bound on the bias derived above.
Variance of restricted OLS: Before considering the sparse multi-site Lasso estimator, a trivial bound is shown for the
variance of a restricted OLS estimation. For every subset ψ ⊂ {1, , p}, we use it to select a subset of columns from design
matrix Xi for task i. These columns form a matrix Xiψ . Define Xψ = DIAG(X1ψ , X2ψ , ..., Xkψ ), and the restricted
OLS-estimator with the noise vector T = (1 , ..., k )T is
θ̂ψ = (XψT Xψ )−1 XψT 

(53)

The `2 -norm of this estimator can be bounded.
Lemma 3.5. Let mp be a sequence with mp = o(n̄) and mp → ∞ for n̄ → ∞. It holds with probability converging to 1
for n → ∞
2 log kp kmp
max kθ̂ψ k22 ≤
σ2
(54)
n̄
φ2min (mp )
ψ:|ψ|≤mp
Proof. We refer the readers to Lemma 3 in (Meinshausen & Yu, 2009) and Lemma 3 in (Liu & Zhang, 2009) for the
proof. Here, we again use block design structure of multi-site problem, the same as in (48), to obtain φmin (mp ) instead of
φmin (kmp ).
The variance of the sparse multi-site Lasso estimator can be bounded by the variance of restricted OLS estimators, using
bounds on the number of active groups.

When can Multi-Site Datasets be Pooled for Regression?

Lemma 3.6. If, for a fixed value of λ, the number of nonzero groups of de-noised estimators B̂ λ,ξ is for every 0 ≤ ξ ≤ 1
bounded by m, then
sup kB̂ λ,0 − B̂ λ,ξ k2F ≤ C max kθ̂ψ k22
(55)
ψ:|ψ|≤m

0≤ξ≤1

with C as a generic constant.
Proof. We refer the readers to Lemma 4 and Lemma 5 in (Liu & Zhang, 2009) for the proof.
λ,ξ
Let AP
. Define mp to be the largest number of
λ,ξ be the set of variables in nonzero groups of the de-noised estimator B̂
P
nonzero groups over all values of 0 ≤ ξ ≤ 1. Then we have kmp = sup0≤ξ≤1 |Aλ,ξ |.

Lemma 3.7. Given 0 ≤ α ≤ 0.5, we have
2
2
T
λ,ξ 2
|AP
)k2
λ,ξ |λ (1 − 2α) ≤ k2XAp (Y − X β̂

(56)

λ,ξ

where we defined before that X = DIAG(X1 , ..., Xk ), Y T = (Y1T , ..., YkT ). β̂ λ,ξ is the transpose of unfolded vector of
B̂ λ,ξ by rows. XApλ,ξ is Xψ when ψ = Apλ,ξ
Proof. The conditions for the solution of sparse multi-site Lasso are presented in (Simon et al., 2013). We use β̂ rather
than β̂ λ,ξ for notational simplicity in this proof. We continue to use our notation β̂ j to refer the j-th column (here it is a
group) of B̂, and β̂ij to refer the i-th element (task) in β̂ j . We define X j = DIAG(X1j , ..., Xkj ) and Xij to be the j-th
column of Xi for task i. In other words, we allow for (k − 1)p number of 0 in Xij .
)
(
β̂ij
β̂ij
jT
√
= 0, when β̂ij 6= 0, β̂ j 6= 0,
− 2Xi (Y − X β̂) + λ α j + (1 − α)
j
kβ̂i k2
kβ̂ k2 / k
T
β̂ij
√ = λαvij , with kvij k2 ≤ 1, when β̂ij = 0, β̂ j 6= 0,
− 2Xij (Y − X β̂) + λ(1 − α)
kβ̂ j k2 / k


√
T


−2X j (Y − X β̂) ≤ λ k, when β̂ j = 0.

(57)

2

P
P
Let Dλ,ξ
= {j ∈ 1, 2, ..., p|group j is active for B̂ λ,ξ }. For each j in Dλ,ξ
, we define β̂∗j to be the vector of all β̂ij 6= 0.
P
Their corresponding columns Xij s from X j , would form a matrix X∗j . For each j in Dλ,ξ
, we define β̂∗jC to be the vector
j
j
j
of all β̂i = 0. Their corresponding columns Xi s from X j , would form a matrix X∗C . Then, from (57),
P
Dλ,ξ

X

P
Dλ,ξ

T
k2X∗j (Y

− X β̂)k22 ≥ λ2 (1 − α)2 k

j=1

X kβ̂∗j k2
2
j=1

(58)

kβ̂ j k22

Based on the fact that ka + bk22 ≥ (kak2 − kbk2 )2
P
Dλ,ξ

X

T
k2X∗jC (Y

P
Dλ,ξ

−

X β̂)k22

≥

j=1

√ kβ̂ jC k2
λ(1 − α) k ∗
− λαkv∗j C k2
kβ̂ j k2

X
j=1

P
Dλ,ξ

)
√ kβ̂ jC k2 j
∗
=
λ (1 − α) k
+λ α
− 2λ α(1 − α) k
kv C k2
kβ̂ j k22
kβ̂ j k2 ∗
j=1
P (
"
#)
Dλ,ξ
j
2
X
kβ̂∗jC k22
j
j
2
2 kβ̂∗C k2
2 2
2
2
2
≥
λ (1 − α) k
+ λ α kv∗C k2 − λ α(1 − α) k
+ kv∗C k2
kβ̂ j k22
kβ̂ j k22
j=1
X

(

!2

2

2

kβ̂∗jC k22

2 2

kv∗j C k22

2

P
Dλ,ξ

2

= λ (1 − α)(1 − 2α)k

X kβ̂ jC k22
∗
j=1

kβ̂ j k22

P
Dλ,ξ

2

− λ α(1 − 2α)

X
j=1

kv∗j C k22

(59)

When can Multi-Site Datasets be Pooled for Regression?

Based on (58) and (59), we have
P
Dλ,ξ

T
p (Y
k2XA
λ,ξ

−

X β̂)k22

X

=

P
Dλ,ξ

k2X

jT

(Y −

X β̂)k22

=

j=1

2

≥ λ (1 − α) k

X kβ̂∗j k2
2
j 2
j=1 kβ̂ k2

P
Dλ,ξ

2

+ λ (1 − α)(1 − 2α)k

X kβ̂ jC k22
∗
j=1

X kβ̂∗j k22 + kβ̂ jC k22
∗

≥ λ (1 − α)(1 − 2α)k

+

kβ̂ j k22

j=1

X

T

k2X∗jC (Y − X β̂)k22

(60)

kv∗j C k22

(61)

j=1

kβ̂ j k22

P
Dλ,ξ

2

−

X β̂)k22

j=1

P
Dλ,ξ

2

P
Dλ,ξ

T
k2X∗j (Y

X

P
Dλ,ξ

2

− λ α(1 − 2α)

X
j=1

P
Dλ,ξ

2

− λ α(1 − 2α)

X

kv∗j C k22

(62)

j=1

P
P
≥ λ2 (1 − α)(1 − 2α)k|Dλ,ξ
| − λ2 α(1 − 2α)k|Dλ,ξ
|
2

2

= λ (1 − 2α)

P
k|Dλ,ξ
|

2

= λ (1 − 2α)

2

(63)

|AP
λ,ξ |

(64)

The next lemma provides an asymptotic upper bound on the number of selected variables, the proof of which is similar to
Lemma 5 in (Meinshausen & Yu, 2009).
Lemma 3.8. Assume conditions in Theorem 3.2, with probability converging to 1 for n → ∞,


2
p
α √
α
ks
+
s
sup |AP
|
≤
1
+
p
h
λ,ξ
1 − 2α
1 − 2α
0≤ξ≤1

(65)

Proof. Based on Lemma 3.7,
(1 − 2α)2 kmp = (1 − 2α)2 sup |AP
λ,ξ | ≤
0≤ξ≤1

1
λ,ξ 2
T
p (Y − X β̂
)k2
sup k2XA
λ,ξ
λ2 0≤ξ≤1

(66)

We decompose the right side into two parts and then have
(1 − 2α)2 kmp ≤

1
1
T
T
∗
λ,ξ
p X(β − β̂
p k2
sup k2XA
sup k2XA
)k2 +
λ,ξ
λ,ξ
λ 0≤ξ≤1
λ 0≤ξ≤1

!2
(67)

Similarly, we know from proof in Lemma 3.5 that
T
2
2
p k ≤ 2kmp log(kp)σ n̄
sup k2XA
2

0≤ξ≤1

(68)

λ,ξ

Based on the definition of λ, there exists a constant $1 > 0, such that
T
2
sup0≤ξ≤1 k2XA
p k2
λ,ξ

λ2

≤ $12 kmp

(69)

Therefore, we have
p
1
T
∗
λ,ξ
p X(β − β̂
sup k2XA
)k2 + $1 kmp
λ,ξ
λ 0≤ξ≤1

(1 − 2α)2 kmp ≤

!2
(70)

P
Define Fλ,ξ
= {i : βi∗ 6= 0} ∪ AP
λ,ξ . Based on the block trick we used in proof of Lemma 3.4,

T
∗
λ,ξ 2
∗
λ,ξ 2
p (β − β̂
p X(β − β̂
)k2 ≤ n̄2 φ2max (sp + min{
kXA
)k2 ≤ kXFT p XFλ,ξ
λ,ξ

k
X

λ,ξ

ni , kp})kβ ∗ − β̂ λ,ξ k22

(71)

i=1

From the assumption on φmax (sp + min{

Pk

i=1

ni , kp}), we know

T
∗
λ,ξ 2
p X(β − β̂
kXA
)k2 ≤ n̄2 ρ2max kβ ∗ − β̂ λ,ξ k22
λ,ξ

(72)

When can Multi-Site Datasets be Pooled for Regression?

Therefore, we have
2

(1 − 2α) kmp ≤

≤

p
2
n̄ρmax sup kβ ∗ − β̂ λ,ξ k2 + $1 kmp
λ
0≤ξ≤1

!2
(73)

p
2
2
n̄ρmax kβ ∗ − β̂ λ,0 k2 + n̄ρmax sup kβ̂ λ,0 − β̂ λ,ξ k2 + $1 kmp
λ
λ
0≤ξ≤1

!2
(74)

Because β is the unfolded vector of B, actually sup0≤ξ≤1 kβ̂ λ,0 − β̂ λ,ξ k2 = sup0≤ξ≤1 kB̂ λ,0 − B̂ λ,ξ kF . From Lemmas
3.5 and 3.6, definition of λ and the assumption on φmin , we obtain the bound
4n̄2 ρ2max 2 log(kp) kmp
4n̄2 ρ2max
sup kβ̂ λ,0 − β̂ λ,ξ k22 ≤ C
σ 2 ≤ $22 kmp
2
λ
λ2
n̄
φ2min (mp )
0≤ξ≤1

(75)

Here, $2 > 0 is a constant. We define $ = $1 + $2 . Now, we obtain


2

(1 − 2α) kmp ≤

p
2
n̄ρmax kβ ∗ − β̂ λ,0 k2 + $ kmp
λ

2
(76)

By setting the constant term in λ large enough, we can have $/(1 − 2α) ≤ 5$ ≤ 0.026, and hence
kmp ≤ (18/17.5)2 (2ρmax )2

n̄2 kβ ∗ − β̂ λ,0 k22
≤
(1 − 2α)2 λ2


1+

α
1 − 2α



p

ksp +

α √
sh
1 − 2α

2
(77)

The last inequality is obtained by plugging in Lemma 3.4. The constant can be 1 by setting the constant term in λ large
enough.
Follow from Lemmas 3.5,3.6, and 3.8, the next lemma bounds the variance part of the sparse multi-sites Lasso estimator:
Lemma 3.9. Assume conditions in Theorem3.2, there exists a constant ω2 > 0, with probability converging to 1 for
n → ∞,
ks̄ log(kp)
(78)
kB λ − B̂ λ,1 k2F = kB̂ λ,0 − B̂ λ,1 k2F ≤ ω2 σ 2
n̄
Proof. We have defined B λ as the estimator B̂ λ,0 with no noise before Lemma 3.4.
Based on Lemmas 3.5 and 3.6
2 log kp kmp
kB̂ λ,0 − B̂ λ,1 k2F ≤
σ2
n̄
φ2min (mp )

(79)

Based on Lemma 3.8, assumption on φmin and 0 ≤ α ≤ 0.4,
kB̂ λ,0 − B̂ λ,1 k2F ≤

2 log kp kmp
ks̄ log(kp)
σ 2 ≤ ω2 σ 2
2
n̄
φmin (mp )
n̄

(80)

The lemma 3.4 and 3.9 together complete the proof of Theorem 3.2
3.2. Proof of Theorem 3.3:
The proof is similar to that of Theorem 3.2. Recall that in this case, however, we do not penalize
Hence, we have the following result about bias contribution of Theorem 3.3.

√

k on group penalty.

Lemma 3.10. Assume conditions in Theorem 3.3. The Frobenius norm of Γλ is then bounded for sufficiently large values
of n̄, given a constant ω1 > 0, by
ks̃ log(kp)
kΓλ k2F ≤ ω1 σ 2
(81)
n̄

When can Multi-Site Datasets be Pooled for Regression?

Proof. The proof procedure is same as Lemma 3.4. But instead of (42), we now have
√
√
√
Λ(Γλ ) ≤ 2{(1 − α̃) sp + α̃ sh }kγ λ k2 = 2 ks̃kγ λ k2

(82)

√
p
because we do not have k penalization on group penalty. Hence, in Lemma 3.10, we have s̃ = {(1 − α̃) sp /k +
p
p
√
α̃ sh /k}2 , instead of s̄ = {(1 − α̃) sp + α̃ sh /k}2 .
Pk
For restricted OLS estimation, we redefine few things here. For every subset ψ ⊂ {1, ..., kp} with |ψ| ≤ i=1 ni , we
define Xψ to be the combination of columns from design matrix X, where X = DIAG(X1 , X2 , ..., Xk ). The restricted
OLS-estimator of the noise vector T = (1 , ..., k )T is then given by,
θ̂ψ = (XψT Xψ )−1 XψT 

(83)

For the variance contribution, the proof is similar to that of Theorem 3.2. We present the required Lemmas for Theorem
3.3 here.
Lemma 3.11. Let mn be a sequence with mn = o(kn̄) and mn → ∞ for n̄ → ∞. It holds with probability converging to
1 for n → ∞
mn
2 log kp
σ2
(84)
max kθ̂ψ k22 ≤
n̄
φ2min (mn )
ψ:|ψ|≤mn
Lemma 3.12. If, for a fixed value of λ, the number of active variables of de-noised estimators B̂ λ,ξ is for every 0 ≤ ξ ≤ 1
bounded by m, then
sup kB̂ λ,0 − B̂ λ,ξ k2F ≤ C max kθ̂ψ k22
(85)
ψ:|ψ|≤m

0≤ξ≤1

with C as a generic constant.
Let A1λ,ξ be the set of active variables of the de-noised estimator B̂ λ,ξ . Let mn to be the largest number of active variables
over all values of 0 ≤ ξ ≤ 1. Then we have mn = sup0≤ξ≤1 |A1λ,ξ |.
Lemma 3.13. For any 0 ≤ α̃ ≤ 1, we have
T
|A1λ,ξ |λ2 α̃2 ≤ k2XA
(Y − X β̂ λ,ξ )k22
1
λ,ξ

(86)

where we defined before that X = DIAG(X1 , ..., Xk ), Y T = (Y1T , ..., YkT ). β̂ λ,ξ is the transpose of unfolded vector of
B̂ λ,ξ by rows. XA1λ,ξ is Xψ when ψ = A1λ,ξ
Lemma 3.14. Assume conditions in Theorem 3.3, with probability converging to 1 for n → ∞,
sup |A1λ,ξ | ≤



√

sh +

0≤ξ≤1

1 − α̃ √
sp
α̃

2
(87)

Lemma 3.15. Assume conditions in Theorem3.3, there exists a constant ω2 > 0, with probability converging to 1 for
n → ∞,
ks̃ log(kp)
kB λ − B̂ λ,1 k2F = kB̂ λ,0 − B̂ λ,1 k2F ≤ ω2 σ 2
(88)
n̄
Lemma 3.10 and Lemma 3.15 complete the proof of Theorem 3.3

When can Multi-Site Datasets be Pooled for Regression?

3. Extra set of simulations (corresponding to Section 4.1 in the main paper)
3.1. Hypothesis Test Simulation when p = 6

Square Root of MSE

MSE
Single site
Two sites

5.0

MSE
β single site
β two sites
γ1 single site
γ1 two sites

75

Square Root of MSE

7.5

50

25

2.5

4

5

6

7

8

9

10

11

12

4

5

6

Sample Size (log2 scale)

7

8

9

10

11

12

Sample Size (log2 scale)

(a)

(b)
1.00

0.75

Sufficient Condition

Same MSE

0.50

Acceptance Rate

Acceptance Rate

0.75

Sufficient Condition

Same MSE

0.50

0.25

0.25

0.00

0.00
4

5

6

7

8

9

10

11

12

4

5

Sample Size (log2 scale)

6

7

8

9

10

11

12

Sample Size (log2 scale)

(c)

(d)

Figure 1. The figure is similar to the simulations done in Figure 3 (which is also the one Figure 3 in the main paper). However, here the
dimension p of β is 6 instead of 3. (a,c) are MSE of β̂ and the corresponding acceptance rate of our hypothesis test (from Section2.1).
(b,d) are MSE of β̂ and γ̂1 and the corresponding acceptance rate (from Section2.2). These are based on 100 bootstrap repetitions. The
solid line in (c,d) represents the point where the condition from Theorem 2.3 is equal to 1. The dotted line is when MSE of β̂ is the same
for single-site and multi-site models.

3.2. Sparse Multi-Sites Lasso Simulation
Table 1. Add multi-sites Lasso on Lasso.
α
CDR
CDV
CDG

0

0.05

0.95

0.97 ( OUR )

1

0.1423
78
5

0.1463
78
5

0.2747
75
3

0.2863
75
3

0.2955
73
1

We report correctly discovered number of active variables (CDV), ratio of CDV and total number of discovered variables
(CDR), and correctly discovered number of always-active features (CDG).
From Table1 and Table 2 we see that our chosen α helps sparse multi-sites Lasso to discover more or preserve always-active
features. The number and rate of correctly discovered number of active variables given by our chosen α are also among
the best.

When can Multi-Site Datasets be Pooled for Regression?
Table 2. Add Lasso on multi-sites Lasso.
α
CDR
CDV
CDG

0

0.05

0.25 ( OUR )

0.95

1

0.2292
80
16

0.2381
80
16

0.2453
79
15

0.2841
75
11

0.2885
73
11

number of always−active features

number of always−active features

3.3. Figure Examples for Choosing α

16

6

14

4

12

Type
Correct discovered(truth)
Correct discovered(estimate)

2

10
0.00

0.25

0.50

α

(a)

0.75

1.00

Type
Correct discovered(truth)
Correct discovered(estimate)
0.00

0.25

0.50

α

0.75

1.00

(b)

Figure 2. These plots show that site-active set from simultaneous inference provides information of always-active features (which is then
used to choose the hyper-parameters α an λ). In (a), we add Lasso on multi-sties Lasso, and α = 0.25 is chosen. Similarly, in Figure(b),
we add multi-sites Lasso on Lasso, and α = 0.97 is chosen.

We here point out a caveat about our choice of α when sparsity patterns share few features and always-active features exist.
In this setting, we do want to discover more always-active features. Hence, we decrease α from 1 and stop at the point
where we just select one more always-active feature. In other words, we choose the α left to the one described in main
body.

When can Multi-Site Datasets be Pooled for Regression?

4. Longer version of Section 4 from the main paper

MSE
Single site
Two sites

1.5
1
0.5

0.25
0.125

Square Root of MSE

Square Root of MSE

MSE
β single site
β two sites
γ1 single site
γ1 two sites

16

2

11

6

1

0.5
0.25
0.125

0

0

4

5

6
7
8
9 10 11
Sample Size (log2 scale)

12

4

5

6

7

8

9

10

11

12

Sample Size (log2 scale)

(a)

(b)

1.00

Acceptance Rate

0.75

Acceptance Rate

1.00

0.75

Sufficient Condition

Same MSE

0.50

Sufficient Condition

Same MSE

0.50

0.25

0.25

0.00

0.00
4

5

6

7

8

9

10

Sample Size (log2 scale)

(c)

11

12

4

5

6

7

8

9

10

11

12

Sample Size (log2 scale)

(d)

Figure 3. (a,c) are MSE of β̂ and the corresponding acceptance rate of our hypothesis test (from Section2.1). (b,d) are MSE of β̂ and γ̂1
and the corresponding acceptance rate from Section 2.2). These are based on 100 bootstrap repetitions. The solid line in (c,d) represents
the point where the condition from Theorem 2.3 is equal to 1. The dotted line is when MSE of β̂ is the same for single-site and multi-site
models.

We now provide few more details about the different curves observed in Figure 3, beyond what is reported in the main paper
due to space constraint. First, we check whether the gap between the sufficient condition (from Theorem 2.3) and the point
where single-site and multi-site models have same MSE is small. The solid lines in Figure 3(c,d) correspond to the point
where the condition value defined in Theorem 2.3 is equal to 1. The dotted lines (where condition value is approximately
3.3) are the points where the MSE of multi-site model starts to increase above the MSE of single-site one. In other words,
to the left of the dotted lines that MSE of β̂ from multi-sites model is smaller than single-site model. To the right of these
lines it is larger. We see that the gap is reasonably small. We then check the type I error of our hypothesis test. On the
left side of solid lines, the sufficient condition holds and our hypothesis test accepts the combination with high rate around
95%, i.e., the type I error is well-controlled. Further, the power of our hypothesis test is evident when MSE of β̂ from
multi-sites model is worse than single-site model. Though our sufficient condition is conservative for the combination, by
noticing that χ2 test is progressive, our test has a high power on the right side of dotted line. In the regime between the two
lines, the multi-sites model has slightly better MSE of β̂ compared to single-site model, and our hypothesis test accepts the
combination with high rate.

When can Multi-Site Datasets be Pooled for Regression?

2.5

1.7

α = 0 (group lasso)
α = 0.05 (fixed choice)
α = 0.95 (fixed choice)
α = 0.97 (our method)
α = 1 (lasso)

Square Root of Cross−Validation Error

Square Root of Cross−Validation Error

3.0

2.0

1.5
0.01

0.02

λ

(a)

0.03

0.04

0.05

1.6

α = 0 (group lasso)
α = 0.05 (fixed choice)
α = 0.25 (our method)
α = 0.95 (fixed choice)
α = 1 (lasso)

1.5

1.4

1.3
0.01

0.02

λ

0.03

0.04

0.05

(b)

Figure 4. (a) shows the solution path of λ when sparsity patterns share few features across sites, and group Lasso penalty is added to
balance Lasso penalty. (b) shows the alternate regime where sparsity patters are similar. The `2 loss is plotted, based on 10-fold cross
validation.

4.1. AD dataset details
The two datasets we use are – an open-source Alzheimer’s Disease Neuroimage Initiative (ADNI) dataset, and a local
dataset (ADlocal). ADNI is an open consortium with the goal of understanding AD related cognitive decline, and in the
process, develop clinical interventions aimed at delaying the disease onset. ADlocal corresponds to a recent (smaller)
initiative local study for the AD related decline. We used 318 samples from ADNI and 156 samples from ADlocal. The
input variables are 8 Cerebrospinal fluid (CSF) protein levels, and the response is hippocampus volume. The CSF proteins
are “1-38-Tr”, “1-40-Tr”, “1-42-Tr”, “NFL”, ”AB42”, “htau”, “ptau181 ”, and “Neurogranin”. The two datasets have
different age and diagnosis distributions, and hence, we subsample 81 samples from either of sites to control age and
diagnosis variation. Using these 81 samples from each dataset, we perform domain adaptation (using a maximum mean
discrepancy objective as a measure of distance between the two marginals) and transform CSF proteins from ADlocal to
match ADNI. The transformed data are then used to evaluate our proposed framework. The results in Figure 5 are already
explained in the main body.

When can Multi-Site Datasets be Pooled for Regression?

Same sample sizes

1.2

Method
local (before) + ADNI
local (after) + ADNI
ADNI

1.15
1.10
1.05

Method
local (before) + ADNI
local (after) + ADNI
ADNI

1.15
Square Root of MPE

Square root of MPE

1.25

1.10

1.05

32
64
95
127 159 191 223 254 286
(10%) (20%) (30%) (40%) (50%) (60%) (70%) (80%) (90%)
Sample size of labeled ADNI

51
(16%)

1.00

1.00

0.75

0.75

Power increases

0.50

Method
local (before) + ADNI
local (after) + ADNI

0.25

127
(40%)

(b)

Acceptance Rate

Acceptance Rate

(a)

76
102
(24%)
(32%)
Sample size

Method
local (before) + ADNI
local (after) + ADNI

0.50
Power increases
0.25

0.00
32
64
95
127
159
191
223
254
286
(10%) (20%) (30%) (40%) (50%) (60%) (70%) (80%) (90%)
Sample size of labeled ADNI

(c)

51
(16%)

76
102
(24%)
(32%)
Sample size

127
(40%)

(d)

Figure 5. Evaluating combined models. (a,c) In this first setting x-axis represents number/fraction of ADNI labeled samples used in
training along with ADlocal labeled data. The dotted line in (a) is where the sample sizes of ADNI and ADlocal in training datasets
match. y-axis shows square root of mean prediction error (computed on the remaining unused ADNI data) scaled by estimated noise
level in ADNI responses. Error bars give 95% confidence interval. (c) shows the acceptance rate of our hypothesis test. (b,d) show the
same evaluations for the alternate setting where equal number of ADNI and ADlocal samples are used for training.

References
Liu, Han and Zhang, Jian. Estimation consistency of the group lasso and its applications. In AISTATS, pp. 376–383, 2009.
Meinshausen, Nicolai and Yu, Bin. Lasso-type recovery of sparse representations for high-dimensional data. The Annals
of Statistics, pp. 246–270, 2009.
Simon, Noah, Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. A sparse-group lasso. Journal of Computational
and Graphical Statistics, 22(2):231–245, 2013.

