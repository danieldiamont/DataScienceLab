Optimal Algorithms for Smooth and Strongly Convex
Distributed Optimization in Networks
SUPPLEMENTARY MATERIAL
Kevin Scaman 1 Francis Bach 2 Sébastien Bubeck 3 Yin Tat Lee 3 Laurent Massoulié 1

Abstract
This supplementary document contains complete
proofs of the theorems presented in the article,
as well as an extension of our algorithm to composite problems particularly relevent for machine
learning applications.

1. Optimal Convergence Rates
1.1. Centralized Algorithms
Proof of Theorem 1. This proof relies on splitting the function used by Nesterov to prove oracle complexities for
strongly convex and smooth optimization (Nesterov, 2004;
Bubeck, 2015). Let β ≥ α > 0, G = (V, E) a graph and
A ⊂ V a set of nodes of G. For all d > 0, we denote as
Acd = {v ∈ V : d(A, v) ≥ d} the set of nodes at distance
at least d from A, and let, for all i ∈ V, fiA : `2 → R be
the functions defined as:

β−α >
α
2

 2n kθk2 + 8|A| (θ M1 θ − 2θ1 ) if i ∈ A
A
β−α >
α
fi (θ) = 2n
kθk22 + 8|A
if i ∈ Acd (1)
c | θ M2 θ
d

α
2
otherwise
2n kθk2
where M2 : `2 → `2 is the infinite block diagonal
 1 −1 
matrix with
on the diagonal, and M1 =
−1 1
 1 0 
. First, note that, since 0  M1 + M2  4I,
0 M2
P
n
f¯A = n1 i=1 fiA is α-strongly convex and β-smooth.
Then, Theorem 1 is a direct consequence of the following
lemma:
Lemma 1. If Acd 6= ∅, then for any t ≥ 0 and any black1

MSR-INRIA Joint Center, Palaiseau, France 2 INRIA, Ecole
Normale Supérieure, Paris, France 3 Theory group, Microsoft Research, Redmond, United States. Correspondence to: Kevin Scaman <kevin.scaman@gmail.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

box procedure one has, for all i ∈ {1, ..., n},
α
f¯A (θi,t )−f¯A (θ∗ ) ≥
2

√

κg − 1
√
κg + 1

t
2(1+ 1+dτ
)

kθi,0 −θ∗ k2 ,
(2)

where κg = β/α.
Proof. This lemma relies on the fact that most of the coordinates of the vectors in the memory of any node will
remain equal to 0. More precisely, let ki,t = max{k ∈
N : ∃θ ∈ Mi,t s.t. θk 6= 0} be the last non-zero coordinate of a vector in the memory of node i at time t. Then, under any black-box procedure, we have, for any local computation step,

 ki,t + 1{ki,t ≡ 0 mod 2} if i ∈ A
ki,t + 1{ki,t ≡ 1 mod 2} if i ∈ Acd . (3)
ki,t+1 ≤

ki,t
otherwise
Indeed, local gradients can only increase even dimensions
for nodes in A and odd dimensions for nodes in Acd . The
same holds for gradients of the dual functions, since these
have the same block structure as their convex conjugates.
Thus, in order to reach the third coordinate, algorithms
must first perform one local computation in A, then d communication steps in order for a node in Acd to have a nonzero second coordinate, and finally, one local computation
in Acd . Accordingly, one must perform at least k local computation steps and (k −1)d communication steps to achieve
ki,t ≥ k for at least one node i ∈ V, and thus, for any
k ∈ N∗ ,
∀t < 1 + (k − 1)(1 + dτ ), ki,t ≤ k − 1.
This implies in particular:


t−1
t
∀i ∈ V, ki,t ≤
+1≤
+ 1.
1 + dτ
1 + dτ

(4)

(5)

Furthermore, by definition of ki,t , one has θi,k = 0 for all
k > ki,t , and thus
kθi,t − θ∗ k22 ≥

+∞
X
k=ki,t +1

θk∗ 2 .

(6)

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

and, since f¯A is α-strongly convex,
α
f¯A (θi,t ) − f¯A (θ∗ ) ≥ kθi,t − θ∗ k22 .
2

(7)

Finally, the solution of the global problem minθ f¯A (θ) is
 √ √ k
α
√
θk∗ = √β−
. Combining this result with Eqs. (5), (6)
β+ α
and (7) leads to the desired inequality.
Using the previous lemma with d = ∆ the diameter of G
and A = {v} one of the pair of nodes at distance ∆ returns
the desired result.

1−cos( π )

Proof of Theorem 2. Let γn = 1+cos( πn ) be a decreasing
n
sequence of positive numbers. Since γ2 = 1 and limn γn =
0, there exists n ≥ 2 such that γn ≥ γ > γn+1 . The cases
n = 2 and n ≥ 3 are treated separately. If n ≥ 3, let G
be the linear graph of size n ordered from node v1 to vn ,
and weighted with wi,i+1 = 1 − a1{i = 1}. Then, if
A = {v1 , ..., vdn/32e } and d = (1 − 1/16)n − 1, we have
|Acd | ≥ |A| and Lemma 1 implies:
√
√

(9)

where ⊗ is the Kronecker product and Id is the identity matrix of size d. Also, note that, in Alg.(2),
√ the current values
xt and yt are always in the image of W ⊗Id (i.e. the set of
>
matrices x such
√ that x 1 = 0). The condition number κ(in
the image of W ⊗ Id ) can thus be upper bounded by γl ,
q
and Nesterov’s acceleration requires κγl steps to achieve
any given precision (Bubeck, 2015).
2.2. Multi-Step Dual Accelerated Method

1.2. Decentralized Algorithms

nα
f¯A (θi,t )−f¯A (θ∗ ) ≥
2

√
function F ∗ (λ W ) is
√
√
√
( W ⊗ Id )∇2 F ∗ (λ W )( W ⊗ Id ),

κg − 1
κg + 1

t
2(1+ 1+dτ
)

Proof of Theorem 4. First, since PK (W ) is a gossip matrix, Theorem 3 implies the convergence of Alg.(3). In or2
,
der to simplify the analysis, we multiply W by (1+γ)λ
1 (W )
so that the resulting gossip matrix has a spectrum in [1 −
−1
c−1
2 , 1 + c2 ]. Applying Theorem 6.2 in (Auzinger, 2011)
−1
with α = 1 − c−1
2 , β = 1 + c2 and γ = 0 implies that the
minimum
min

max

−1
p∈PK ,p(0)=0 x∈[1−c−1
2 ,1+c2 ]

|p(t) − 1|

(10)

(1−x))
kθi,0 −θ∗ k2 . is attained by PK (x) = 1− TKT(c2 (c
. Finally, Corollary
2)
K
6.3 of (Auzinger, 2011) leads to
(8)

n
A simple calculation gives κl = 1 + (κg − 1) 2|A|
, and thus
κg ≥ κl /16. Finally, if we take Wa as the Laplacian of the
weighted graph G, a simple calculation gives that, if a = 0,
γ(Wa ) = γn and, if a = 1, the network is disconnected
and γ(Wa ) = 0. Thus, by continuity of the eigenvalues of a
matrix, there exists a value a ∈ [0, 1] such that γ(Wa ) = γ.
2
Finally, by definition of n, one has γ > γn+1 ≥ (n+1)
2,
q
15
and d ≥ 16
( γ2 − 1) − 1 ≥ 5√1 γ when γ ≤ γ3 = 13 .

For the case n = 2, we consider the totally connected
network of 3 nodes, reweight only the edge (v1 , v3 ) by
a ∈ [0, 1], and let Wa be its Laplacian matrix. If a = 1,
then the network is totally connected and γ(Wa ) = 1. If,
on the contrary, a = 0, then the network is a linear graph
and γ(Wa ) = γ3 . Thus, there exists a value a ∈ [0, 1] such
that γ(Wa ) = γ, and applying Lemma 1 with A = {v1 }
and d = 1 returns the desired result, since then κg ≥ 2κl /3
and d = 1 ≥ √13γ .

2. Optimal Decentralized Algorithms
2.1. Single-Step Dual Accelerated Method
Proof of Theorem 3. Each step of the algorithm can be decomposed in first computing gradients, and then communicating these gradients across all neighborhoods. Thus, one
step takes a time 1 + τ . Moreover, the Hessian of the dual

cK

1 − 2 1+c12K

γ(PK (W )) ≥

1+2
where c1 =

√
1− γ
√
1+ γ ,

1
cK
1
1+c2K
1

=

1 − cK
1
1 + cK
1

2
,

(11)

and taking K = b √1γ c implies

1
p



γ(PK (W ))

≤

√1
γ

+1

√1
γ

+1

1 + c1
1 − c1

≤ 2.

(12)

The time required to reach an ε > 0 precisionusq
l
ing Alg.(3) is thus O (1 + Kτ ) γ(PKκ(W
=
)) ln(1/ε)
√

O
κl (1 + √1γ τ ) ln(1/ε) .

3. Composite Problems for Machine Learning
When the local functions are of the form
fi (θ) = gi (Bi θ) + ckθk2 ,

(13)

where Bi ∈ Rmi ×d and gi is smooth and has proximal
operator which is easy to compute (and hence also gi∗ ),
an additional Lagrange multiplier ν can be used to make
the Fenchel conjugate of gi appear in the dual optimization problem. More specifically, from the primal problem

Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks

of Eq. (12), one has, with ρ > 0 an arbitrary parameter:
F (Θ)
√inf
Θ W =0
=

inf
√
Θ W =0, ∀i,xi =Bi θi

The bound on the conditional number may be shown
through the two inequalities:
n

n
1X
gi (xi ) + ckθi k22
n i=1

n
o
1 Xn >
= inf sup
νi Bi θi − gi∗ (νi ) + ckθi k22
Θ λ,ν n
i=1
√
ρ
>
+ tr(λ Θ W )
n
n
1X ∗
=
sup
−
g (νi )
Q
mi , λ∈Rd×n
n i=1 i
ν∈ n
i=1 R

Q(ν, λ) 6

n

+

To maximize the dual problem, we can use (accelerated)
proximal gradient, with the updates:
νi,t+1

=

inf

ν∈Rmi

gi∗ (ν)

√

1
ν − νi,t + η Bi (Bi> νi,t + ρλt W i )2
2
2η
2c
n
√
√ >
ρ X >
(Bi νi,t + ρλt W i ) W i .
= λt − η
2cn i=1
+

λt+1

√
We can rewrite all updates in terms of zt = λt W ∈
Rd×n , as
νi,t+1

=

inf gi∗ (ν)

ν∈Rmi


1
ν − νi,t + η Bi (Bi> νi,t + ρzi,t )2
2
2η
2c
n
ρ X >
= zt − η
(B νi,t + ρzi )Wi> .
2cn i=1 i
+

zt+1

In order to compute the convergence rate of such an algorithm, if we assume that:
• each gi is µ-smooth,
• the largest singular value of each Bi is less than M ,
then we simply need to compute the condition number of
the quadratic function
n

Q(ν, λ) =

n

√
1 X
1 X >
kνi k22 +
kBi νi + ρλ W i k22 .
2µ i=1
4c i=1

With the choice ρ2 = λmax1(W ) µc + M 2 ), it is lower
2
bounded by 1 + µ Mc γ4 , which is a natural upper bound
on κl /γ. Thus this essentially leads to the same convergence rate than the non-compositepcase with the Nesterov
and Chebyshev accelerations, i.e. κl /γ.

1 X > 2
kB νi k2 ,
2c i=1 i
n

Q(ν, λ) >

n

√
1 1 X
1 X
kνi k2 +
kρλ W i k22
2µ i=1
1 + η 4c i=1
n

−

n

√
1 X >
−
kBi νi + ρλ W i k22 .
4cn i=1

n

√
1 X
1 X
kνi k2 +
kρλ W i k22
2µ i=1
2c i=1

1 1 X > 2
kB νi k2 ,
η 4c i=1 i

with η = M 2 µ/c.

References
Auzinger, W. Iterative Solution of Large Linear Systems.
Lecture notes, TU Wien, 2011.
Bubeck, Sébastien. Convex optimization: Algorithms and
complexity. Foundations and Trends in Machine Learning, 8(3-4):231–357, 2015.
Nesterov, Yurii. Introductory lectures on convex optimization : a basic course. Kluwer Academic Publishers,
2004.

