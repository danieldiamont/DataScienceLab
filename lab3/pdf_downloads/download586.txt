Gradient Projection Iterative Sketch for large-scale constrained Least-squares
(Supplementary materials)

Junqi Tang 1 Mohammad Golbabaee 1 Mike E. Davies 1
LS solution xt? , we have:
 T
	
t
kri+1
k2 = sup
v (xi − xt? − η∇f (xi ))

1. Supplementary materials
1.1. the proof for Theorem 1

v∈Ct ∩Bd

Proof. At first we denote the underlying cost function of
GPIS as ft (x):

≤



v T (xi − xt? − η∇f (xi )) + ηv T ∇f (xt? )



	
v T (xi − xt? ) − ηv T (∇f (xi ) − ∇f (xt? ))



v T (I − ηAT S T SA)rit

v∈Ct ∩Bd

for t = 0, we have the cost function of the classical sketch
(CS):

=

sup
v∈Ct ∩Bd

=
1
ft (x) := kSy − SAxk22 ,
2

sup

sup
v∈Ct ∩Bd

(1)

≤

sup
u,v∈Ct ∩Bd

≤ sup
for t = 1, 2, ..., N we have the the cost function of Iterative
Hessian Sketch (IHS):

u,v∈Bd

	

 T
	
v (I − ηAT S T SA)u krit k2

 T
	
v (I − ηAT S T SA)u krit k2 ,
(5)

We denote:
αt = sup v T (I − ηAT S T SA)u,

1
ft (x) = kS t+1 A(x − xt )k22 − mxT AT (y − Axt ), (2)
2

(6)

u,v∈Bd

then by recursive subsitution we have:
and then we denote the optimal solution of ft constrained
t
k2 = kxti+1 − xt? k2 have:
to set K as xt? and kri+1
t
kri+1
k2 = kxti+1 − xt? k2 = kPK (xti − η∇f (xi )) − xt? k2
(3)
then we denote cone Ct to be the smallest close cone at xt?
containing the set K − xt? , again because of the distance
preservation of translation by Lemma 6.3 of (Oymak et al.,
2015), we have:

t
kri+1
k2

=
=

kPK−xt? (xti −
 T
sup

v∈Ct

∩Bd

η∇f (xi ) −

xt? )k2

	
v (xi − xt? − µ∇f (xi )) ,

(4)

t
kri+1
k2 ≤ αti kr0t k2 ,

(7)

and suppose we run GPIHS inner loop kt time, we have:
k

krkt t +1 k2 ≤ {αt } t kr0t k2 ,
and we transfer it in terms of A-norm:
s
L t
k
krkt t +1 kA ≤ {αt } t
kr kA .
µ 0

(8)

(9)

From the main theorems of the Classical sketch (Pilanci &
Wainwright, 2015) and Iterative Hessian Sketch (Pilanci &
Wainwright, 2016) we have following relationships:
kx0? − x? kA ≤ 2ρ0 kAx? − yk2 = 2ρ0 kek2 ,

(10)

and,
kxt? − x? kA ≤ ρt kxt0 − x? kA .

then because of the optimality condition on the constrained
1

Institute of Digital Communications, the University of
Edinbuurgh, EH9 3JL, UK. Correspondence to: Junqi Tang
<J.Tang@ed.ac.uk>.

Then by triangle inequality we have:

Supplementary materials. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017.
JMLR: W&CP. Copyright 2017 by the author(s).

and,

kx10 − x? kA ≤ kx10 − x0? kA + 2ρ0 kek2 ,

(11)

(12)

kxt+1
− x? kA ≤ kxt+1
− xt? kA + ρt kxt0 − x? kA . (13)
0
0

	

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

Then for t = 0 we can have:
kx10 − x? kA ≤ kx10 − x0? kA + 2ρ0 kek2
s
L 0
kt
≤ {αt }
kx − x0? kA + 2ρ0 kek2 ,
µ 0
(14)
for t = 1, 2, ..., N we have:

then chain it. For all the sketched objective function ft (x) ,
t = 0, 1, ..., N , and any pair of vectors x, x0 ∈ K we have:
ft (x) − ft (x0 )− < Oft (x0 ), x − x0 >= kS t A(x − x0 )k22
(23)
If we set x0 = xt? , by first order optimality condition we
immediately have:
ft (x) − ft (xt? ) ≥ kS t A(x − xt? )k22

kxt0 − x? kA
t−1
≤ kxt0 − xt−1
− x? kA
? kA + ρt kx0
s
L t−1
k
kx
− xt−1
≤ {αt } t
? kA
µ 0

kS t

=

≥

A(x − xt? )
kA(x − xt? )k2 k22
kA(x − xt? )k2

t
2
inf n−1 kS vk2 kx − xt? k2A ,

v∈range(A)∩S

+ ρt kxt−1
− x? kA
0
(

s !
)
L
+ ρt kxt−1
− x? kA ,
≤ {αt }
(1 + ρt )
0
µ
(15)
The last inequality holds because:

so we have:

kt

kxt−1
− x?fN −1 kA ≤ kxt−1
− x? kA + kxt−1
− x? k A
?
0
0
≤ {1 + ρt } kxt−1
− x? kA ,
0
(16)
Then we denote:
kt

ρ?t = {αt }

s !
L
+ ρt
(1 + ρt )
µ

and do recursive substitution we can have:
(N
)
Y
t
?
?
kx0 − x kA ≤
ρt kx10 − x? kA .

p

kx −

xt? kA

≤

ft (x) − ft (xt? )
,
inf v∈range(A)∩Sn−1 kS t vk2

From the convergence theory in (Beck & Teboulle, 2009)
which the authors in their Remark 2.1 have stated to hold
for convex constrained sets, for GPIS inner iterates we
have:
βLR supv∈range(A)∩Sn−1 kS t vk22
,
2k
(26)
and for Acc-GPIS inner loop we have:

(17)

ft (xk ) − ft (xt? ) ≤
(18)

2βLR supv∈range(A)∩Sn−1 kS t vk22
,
(k + 1)2
(27)

hence for GPIS:

hence we finish the proof of Theorem 1.

r
kxt+1
0

−

xt? kA

≤

1.2. The proofs for Theorem 2 and 3
Proof. From the theory of the Classical sketch and Iterative
Hessian Sketch we have following relationships:
(19)

and,
kxt? − x? kA ≤ ρt kxt0 − x? kA .

(25)

ft (xk ) − ft (xt? ) ≤

t=1

kx0? − x? kA ≤ 2ρ0 kAx? − yk2 = 2ρ0 kek2 ,

(24)

βLσt R
,
2k

(28)

2βLσt R
,
(k + 1)2

(29)

for Acc-GPIS,
s
kxt+1
− xt? kA ≤
0

(20)

Then by simply towering the inequalities we shall obtain
the desired results in Theorem 2 and 3.

(21)

1.3. The proofs for quantitative bounds of αt , ρt and σt
for Gaussian sketches

Then by triangle inequality we have:
kx10 − x? kA ≤ kx10 − x0? kA + 2ρ0 kek2 ,
and,
kxt+1
− x? kA ≤ kxt+1
− xt? kA + ρt kxt0 − x? kA . (22)
0
0
The remaining task of this proof is just bound the term
kxt+1
− xt? kA for both GPIS and Acc-GPIS algorithm and
0

To prove the results in Proposition 1, 2 and 3 we need the
following concentration lemmas as pillars:
Lemma 1. For any g ∈ Rd , we have:

T
sup v g = max 0, sup
v∈C∩Bd

u∈C∩Sd−1

T

u g


(30)

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

Proof. By the definition of cone projection operator we
have:
sup v T g = kPC (g)k2 ≥ 0
(31)
v∈C∩Bd

Proof. This Lemma follows the result of the simplified
form of the Gordon’s Lemma [Lemma 6.7](Oymak et al.,
2015):
kSAvk2

T

if supv∈C∩Bd v g > 0:

≥

sup v T g = sup kvk2
v∈C∩Bd

≥

v∈C∩Bd

vT g
≤ sup uT g, (32)
kvk2
u∈C∩Sd−1

kSAvk2

(bm − W(AC ∩ Sn−1 ) − θ)kAvk2
√
µ(bm − W(AC ∩ Sn−1 ) − θ)kvk2

≤ (bm + W(AC ∩ Sn−1 ) + θ)kAvk2
√
L(bm + W(AC ∩ Sn−1 ) + θ)kvk2
≤

and meanwhile since C ∩ Sd−1 ∈ C ∩ B d we have:
sup v T g ≥

uT g,

sup

(33)

1.3.1. THE PROOF FOR P ROPOSITION 1

u∈C∩Sd−1

v∈C∩Bd

Proof. Let’s mark out the feasible region of the step-size η:
hence we have:

α(η, S t A)
sup v T g =

v∈C∩Bd

uT g,

sup

=

(34)

sup v T (I − ηAT S T SA)u
u,v∈Bd

u∈C∩Sd−1

≥

sup v T (I − ηAT S T SA)v
v∈Bd

=

Lemma 2. If supu,v∈C∩Bd v T M u > 0, we have:
sup

vT M u =

sup

vT M u

≥

(35)

sup ((1 − ηL(bm +
v∈Bd

u,v∈C∩Sd−1

u,v∈C∩Bd

sup (kvk22 − ηkSAvk22 )

v∈Bd

so if we choose a step size η ≤
Proof. Since u, v ∈ C ∩ B d , kuk2 and kvk2 are both less
than or equal to 1, we can have the following upper bound:
vT M u

sup

=

(

sup
u,v∈C∩Bd

u,v∈C∩Bd

≤

=

sup v T (I − ηAT S T SA)u
u,v∈Bd

v T M u,

=

sup

sup

=

v T M u,

(36)

sup

vT M u =

u,v∈C∩Bd

sup

vT M u

(37)

kSAvk2 ≥

−ku − vk22 + ηkSA(u − v)k22 ]
√
1
≤
sup
[(1 − ηµ(bm − d − θ)2 )ku + vk22
u,v∈Sd−1 4
√
+(ηL(bm + d + θ)2 − 1)ku − vk22 ]

u,v∈C∩Sd−1

Lemma 3. If the entries of the sketching matrix S is i.i.d
drawn from Normal distribution and v ∈ C, we have:
√

µ(bm − W − θ)kvk2 ,

(38)

1
[(u + v)T (I − ηAT S T SA)(u + v)
4

−(u − v)T (I − ηAT S T SA)(u − v)]
1
=
sup
[ku + vk22 − ηkSA(u + v)k22
4
d−1
u,v∈S

u,v∈C∩Sd−1

hence we have:
sup

v T (I − ηAT S T SA)u

u,v∈Sd−1

u,v∈Sd−1

vT M u ≥

we can en-

α(η, S t A)

and meanwhile since C ∩ Sd−1 ∈ C ∩ B d we have:
sup

1
√
L(bm + d+θ)2

( > 0) we have
sure that with probability 1 − e
α(η, S t A) > 0 and the Lemma 2 become applicable:

u,v∈C∩Sd−1

u,v∈C∩Bd

d + θ − )2 )kvk22 ),

(θ−)2
− 2

vT M u
)kvk2 kuk2
kvk2 kuk2

sup

√

The last line of inquality holds with probability at least 1 −
θ2
2e− 2 according to Lemma 3. Then since we have set η ≤
√1
, and meanwhile notice the fact that ku +
L(b + d+θ+)2
m

vk22 ≤ 4 we have:
kSAvk2 ≤

√

L(bm + W + θ)kvk2 ,
(39)
m+1
2
√
θ
Γ(
)
with probability at least 1 − e− 2 . (bm = 2 Γ( m2 ) ≈
2
√
m, W := W(AC ∩ Sn−1 ))

α(η, S t A)
√
1
≤
sup
(1 − ηµ(bm − d − θ)2 ku + vk22
u,v∈Sd−1 4
√
≤ (1 − ηµ(bm − d − θ)2 )

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

If we chose η =

1
√
L(bm + d+θ)2

we have:

!
√
µ (bm − d − θ)2
√
1−
,
L (bm + d + θ)2

t

α(η, S A) ≤

(40)

Then let  → 0, we shall get the result shown in Proposition
1.

1.3.2. T HE PROOF FOR P ROPOSITION 2
Proof. Recall that ρt is defined as:
T

1 t
supv∈AC∩Sn−1 v T ( m
S S t − I)z
,
ρ(S , A) =
1
inf v∈AC∩Sn−1 m kS t vk22
t

(41)

we start by lower-bounding the denominator, by simplified
Gordon’s lemma [Lemma 6.7](Oymak et al., 2015) we directly have:

inf n−1

v∈AC∩S

1
(bm − W − θ)2
kSvk22 ≥
,
m
m

with probability at least (1 − e−
upper bound for the numerator:
T

v

T

St St
−I
m

θ2
2

hence we have the following by [Lemma 6.8](Oymak et al.,
2015):
!
T
St St
T
v
−I z
m


1 1
2
2
≤
(bm kv + zk2 + W + θ) − kv + zk2
4 m


1 1
2
2
(bm kv − zk2 + W + θ) − kv − zk2
+
4 m
 2

1
bm
2bm (W + θ)
2
=
(
− 1)kv + zk2 +
kv + zk2
4
m
m


1
b2m
2bm (W + θ)
2
+
(1 −
)kv − zk2 +
kv − zk2 ,
4
m
m
(45)
2
− θ8
). Note that kv + zk2 +
with probability
√ at least (1 − 8e
kv − zk2 ≤ 2 2 and kv + zk22 + kv − zk22 ≤ 4, we have:
!
T
St St
T
v
−I z
m
b2
2bm (W + θ) kv + zk2 + kv − zk2
(46)
+ | m − 1|
m
4
m
√
2bm (W + θ)
b2
≤
+ | m − 1|
m
m
≤

thus finishes the proof.
(42)
1.3.3. T HE PROOF FOR P ROPOSITION 3
Proof. Recall that σt is defined as:

).Then we move to the
σ(S t , A) =

!

supv∈range(A)∩Sn−1 kS t vk22
,
inf v∈range(A)∩Sn−1 kS t vk22

(47)

by simply apply again the Gordon’s lemma
√ [Lemma
6.7](Oymak et al., 2015), with W(ASd−1 ) ≤ d, we with
obtain the upper bound on the numerator:
√
(48)
sup
kS t vk22 ≤ (bm + d + θ)2 ,

z
T

1
St St
{(v + z)T (
− I)(v + z)
4
m
T
St St
− (v − z)T (
− I)(v − z)}
m
1 1
= { kS t (v + z)k2 − kv + zk2
4 m
1
+ kv − zk2 − kS t (v − z)k2 },
m
=

v∈range(A)∩Sn−1

(43)
and the lower bound:
inf

kS t vk22 ≥ (bm −
n−1

√

v∈range(A)∩S

both with probability at least 1 − e−

θ2
2

d − θ)2 ,

(49)

.

1.4. Details of the implementation of algorithms and
numerical experiments

and,
W(AC ∩ Sn−1 − z) = Eg (

sup

v∈AC∩Sn−1
T

= Eg (g z +

g T (v − z))
sup

v∈AC∩Sn−1
n−1

= W(AC ∩ S

)

v T g) (44)

For our GPIS and Acc-GPIS algorithms, we have several
key points of implemenations:
• Count sketch
As described in the main text.

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

• Line search

• Gradient restart for Acc-GPIS

1.4.1. P ROCEDURE TO GENERATE SYNTHETIC DATA
SETS

The procedure we used to generate a constrained leastsquare problem sized n by 100 with approximately s-sparse
solution and a condition number κ strictly follows:
1) Generate a random matrix A sized n by 100 with i.i.d
entries drawn from N (0, 1).
2) Calculate A’s SVD: A = U ΣV T and replace the singular values diag(Σ)i by a sequence:
diag(Σ)i−1
1

κd

m = 200
m = 400
m = 800

0.05

0.04

0.03

0.02

0.01

0
0

2

4

(50)

3) Generate the ”ground truth” vector xgt sized 100 by
1 randomly with only s non-zero entries in a orthongonal transformed domain Φ, and calculate the l1 norm of it

6

8

10

12

14

16

18

20

sparsity of the solution

Figure 1. Experimental results on the average choices of GPIS’s
step sizes given by line-search scheme (Nesterov, 2013)

Table 1. Synthetic data set for step size experiment
DATA SET

(O’Donoghue & Candes, 2015) has proposed two
heuristic adaptive restart schemes - gradient restart
and function restart for the accelerated gradient methods and have shown significant improvements without
the need of the knowledge of the functional parameters µ and L. Such restart methods are directly applicable for the Acc-GPIS by nature due to its sketched
deterministic iterations. Here we choose the gradient
restart since it achieves comparable performance in
practice as function restart but cost only O(d) operations.

diag(Σ)i =

average step size given by line-search for GPIS

0.06

We implement the line-search scheme given by (Nesterov, 2007) and is described by Algorithm 3 for GPIS
and Acc-GPIS in our experiments with parameters
γu = 2, and γd = 2. Such choice of line-search parameters simply means: when even we find the condition ft (PK (xi − ηOft (xi ))) ≤ mL does not hold,
we shrink the step size by a factor of 2; and then at the
beginning of each iteration, we increase the step size
chosen at previous iteration by a factor of 2, then do
backtracking again. Hence our methods are able to ensure we use an aggressive step size safely in each iteration. This is an important advantage of the sketched
gradient method since we observe that for stochastic gradient such as SAGA a heuristic backtracking
method similar to Algorithm 3 may work but it will
demand a very small γd (tends to 1) otherwise SAGA
may go unstable, and an aggressive choice like our
γd = 2 is unacceptable for SAGA. (Hence we suspect
that SAGA is unlikely to be able to benefit computational gains from line-search as our method does.)

S YN 4

S IZE

S

Φ

(20000, 100)

-

I

(r = kΦxgt k1 ). Hence the constrained set can be described
as K = {x : kΦxk1 ≤ r}.
4) Generate a random error vector w with i.i.d entries such
kAxgt k2
= 10.
that kwk
2
5) Set y = Axgt + w
1.4.2. E XTRA EXPERIMENT FOR STEP SIZE CHOICE
We explore the step size choices the GPIS algorithm produce through using the line-search scheme with respect to
different sparsity level of the solution. The result we shown
is the average of 50 random trials.
The result of the step-size simulation demonstrates that the
step sizes chosen on average by the line-search scheme for
the GPIS algorithm is actually related with the sparsity of
the ground truth xgt : at a regime when the xgt is sparse
enough, the step size one can achieve goes up rapidly w.r.t
the sparsity. While in our Proposition 2 we revealed that
the outerloop of GPIS/Acc-GPIS can benefit from the constrained set, and here surprisingly we also find out numerically that the inner loop’s can also benefit from the constrained set by aggressively choosing the large step sizes.
Such a result echos the analysis of the PGD algorithm on
constrained Least-squares with a Gaussian map A (Oymak
et al., 2015). Further experiments and theoretical analysis
of such greedy step sizes for sketched gradients and full

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

gradients on general maps is of great interest and will go
beyond the state of the art analysis for convex optimization.

References
Beck, Amir and Teboulle, Marc. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
doi: 10.1137/080716542. URL http://dx.doi.
org/10.1137/080716542.
Nesterov, Yurii. Gradient methods for minimizing composite objective function. Technical report, UCL, 2007.
Nesterov, Yurii. Gradient methods for minimizing composite functions. Mathematical Programming, 140(1):
125–161, 2013.
O’Donoghue, Brendan and Candes, Emmanuel. Adaptive
restart for accelerated gradient schemes. Foundations of
computational mathematics, 15(3):715–732, 2015.
Oymak, Samet, Recht, Benjamin, and Soltanolkotabi,
Mahdi. Sharp time–data tradeoffs for linear inverse problems. arXiv preprint arXiv:1507.04793, 2015.
Pilanci, Mert and Wainwright, Martin J. Randomized
sketches of convex programs with sharp guarantees. Information Theory, IEEE Transactions on, 61(9):5096–
5115, 2015.
Pilanci, Mert and Wainwright, Martin J. Iterative hessian
sketch: Fast and accurate solution approximation for
constrained least-squares. Journal of Machine Learning
Research, 17(53):1–38, 2016.

