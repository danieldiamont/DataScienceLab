Gradient Projection Iterative Sketch for large-scale constrained Least-squares
(Supplementary materials)

Junqi Tang 1 Mohammad Golbabaee 1 Mike E. Davies 1
LS solution xt? , we have:
 T
	
t
kri+1
k2 = sup
v (xi âˆ’ xt? âˆ’ Î·âˆ‡f (xi ))

1. Supplementary materials
1.1. the proof for Theorem 1

vâˆˆCt âˆ©Bd

Proof. At first we denote the underlying cost function of
GPIS as ft (x):

â‰¤



v T (xi âˆ’ xt? âˆ’ Î·âˆ‡f (xi )) + Î·v T âˆ‡f (xt? )



	
v T (xi âˆ’ xt? ) âˆ’ Î·v T (âˆ‡f (xi ) âˆ’ âˆ‡f (xt? ))



v T (I âˆ’ Î·AT S T SA)rit

vâˆˆCt âˆ©Bd

for t = 0, we have the cost function of the classical sketch
(CS):

=

sup
vâˆˆCt âˆ©Bd

=
1
ft (x) := kSy âˆ’ SAxk22 ,
2

sup

sup
vâˆˆCt âˆ©Bd

(1)

â‰¤

sup
u,vâˆˆCt âˆ©Bd

â‰¤ sup
for t = 1, 2, ..., N we have the the cost function of Iterative
Hessian Sketch (IHS):

u,vâˆˆBd

	

 T
	
v (I âˆ’ Î·AT S T SA)u krit k2

 T
	
v (I âˆ’ Î·AT S T SA)u krit k2 ,
(5)

We denote:
Î±t = sup v T (I âˆ’ Î·AT S T SA)u,

1
ft (x) = kS t+1 A(x âˆ’ xt )k22 âˆ’ mxT AT (y âˆ’ Axt ), (2)
2

(6)

u,vâˆˆBd

then by recursive subsitution we have:
and then we denote the optimal solution of ft constrained
t
k2 = kxti+1 âˆ’ xt? k2 have:
to set K as xt? and kri+1
t
kri+1
k2 = kxti+1 âˆ’ xt? k2 = kPK (xti âˆ’ Î·âˆ‡f (xi )) âˆ’ xt? k2
(3)
then we denote cone Ct to be the smallest close cone at xt?
containing the set K âˆ’ xt? , again because of the distance
preservation of translation by Lemma 6.3 of (Oymak et al.,
2015), we have:

t
kri+1
k2

=
=

kPKâˆ’xt? (xti âˆ’
 T
sup

vâˆˆCt

âˆ©Bd

Î·âˆ‡f (xi ) âˆ’

xt? )k2

	
v (xi âˆ’ xt? âˆ’ Âµâˆ‡f (xi )) ,

(4)

t
kri+1
k2 â‰¤ Î±ti kr0t k2 ,

(7)

and suppose we run GPIHS inner loop kt time, we have:
k

krkt t +1 k2 â‰¤ {Î±t } t kr0t k2 ,
and we transfer it in terms of A-norm:
s
L t
k
krkt t +1 kA â‰¤ {Î±t } t
kr kA .
Âµ 0

(8)

(9)

From the main theorems of the Classical sketch (Pilanci &
Wainwright, 2015) and Iterative Hessian Sketch (Pilanci &
Wainwright, 2016) we have following relationships:
kx0? âˆ’ x? kA â‰¤ 2Ï0 kAx? âˆ’ yk2 = 2Ï0 kek2 ,

(10)

and,
kxt? âˆ’ x? kA â‰¤ Ït kxt0 âˆ’ x? kA .

then because of the optimality condition on the constrained
1

Institute of Digital Communications, the University of
Edinbuurgh, EH9 3JL, UK. Correspondence to: Junqi Tang
<J.Tang@ed.ac.uk>.

Then by triangle inequality we have:

Supplementary materials. Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017.
JMLR: W&CP. Copyright 2017 by the author(s).

and,

kx10 âˆ’ x? kA â‰¤ kx10 âˆ’ x0? kA + 2Ï0 kek2 ,

(11)

(12)

kxt+1
âˆ’ x? kA â‰¤ kxt+1
âˆ’ xt? kA + Ït kxt0 âˆ’ x? kA . (13)
0
0

	

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

Then for t = 0 we can have:
kx10 âˆ’ x? kA â‰¤ kx10 âˆ’ x0? kA + 2Ï0 kek2
s
L 0
kt
â‰¤ {Î±t }
kx âˆ’ x0? kA + 2Ï0 kek2 ,
Âµ 0
(14)
for t = 1, 2, ..., N we have:

then chain it. For all the sketched objective function ft (x) ,
t = 0, 1, ..., N , and any pair of vectors x, x0 âˆˆ K we have:
ft (x) âˆ’ ft (x0 )âˆ’ < Oft (x0 ), x âˆ’ x0 >= kS t A(x âˆ’ x0 )k22
(23)
If we set x0 = xt? , by first order optimality condition we
immediately have:
ft (x) âˆ’ ft (xt? ) â‰¥ kS t A(x âˆ’ xt? )k22

kxt0 âˆ’ x? kA
tâˆ’1
â‰¤ kxt0 âˆ’ xtâˆ’1
âˆ’ x? kA
? kA + Ït kx0
s
L tâˆ’1
k
kx
âˆ’ xtâˆ’1
â‰¤ {Î±t } t
? kA
Âµ 0

kS t

=

â‰¥

A(x âˆ’ xt? )
kA(x âˆ’ xt? )k2 k22
kA(x âˆ’ xt? )k2

t
2
inf nâˆ’1 kS vk2 kx âˆ’ xt? k2A ,

vâˆˆrange(A)âˆ©S

+ Ït kxtâˆ’1
âˆ’ x? kA
0
(

s !
)
L
+ Ït kxtâˆ’1
âˆ’ x? kA ,
â‰¤ {Î±t }
(1 + Ït )
0
Âµ
(15)
The last inequality holds because:

so we have:

kt

kxtâˆ’1
âˆ’ x?fN âˆ’1 kA â‰¤ kxtâˆ’1
âˆ’ x? kA + kxtâˆ’1
âˆ’ x? k A
?
0
0
â‰¤ {1 + Ït } kxtâˆ’1
âˆ’ x? kA ,
0
(16)
Then we denote:
kt

Ï?t = {Î±t }

s !
L
+ Ït
(1 + Ït )
Âµ

and do recursive substitution we can have:
(N
)
Y
t
?
?
kx0 âˆ’ x kA â‰¤
Ït kx10 âˆ’ x? kA .

p

kx âˆ’

xt? kA

â‰¤

ft (x) âˆ’ ft (xt? )
,
inf vâˆˆrange(A)âˆ©Snâˆ’1 kS t vk2

From the convergence theory in (Beck & Teboulle, 2009)
which the authors in their Remark 2.1 have stated to hold
for convex constrained sets, for GPIS inner iterates we
have:
Î²LR supvâˆˆrange(A)âˆ©Snâˆ’1 kS t vk22
,
2k
(26)
and for Acc-GPIS inner loop we have:

(17)

ft (xk ) âˆ’ ft (xt? ) â‰¤
(18)

2Î²LR supvâˆˆrange(A)âˆ©Snâˆ’1 kS t vk22
,
(k + 1)2
(27)

hence for GPIS:

hence we finish the proof of Theorem 1.

r
kxt+1
0

âˆ’

xt? kA

â‰¤

1.2. The proofs for Theorem 2 and 3
Proof. From the theory of the Classical sketch and Iterative
Hessian Sketch we have following relationships:
(19)

and,
kxt? âˆ’ x? kA â‰¤ Ït kxt0 âˆ’ x? kA .

(25)

ft (xk ) âˆ’ ft (xt? ) â‰¤

t=1

kx0? âˆ’ x? kA â‰¤ 2Ï0 kAx? âˆ’ yk2 = 2Ï0 kek2 ,

(24)

Î²LÏƒt R
,
2k

(28)

2Î²LÏƒt R
,
(k + 1)2

(29)

for Acc-GPIS,
s
kxt+1
âˆ’ xt? kA â‰¤
0

(20)

Then by simply towering the inequalities we shall obtain
the desired results in Theorem 2 and 3.

(21)

1.3. The proofs for quantitative bounds of Î±t , Ït and Ïƒt
for Gaussian sketches

Then by triangle inequality we have:
kx10 âˆ’ x? kA â‰¤ kx10 âˆ’ x0? kA + 2Ï0 kek2 ,
and,
kxt+1
âˆ’ x? kA â‰¤ kxt+1
âˆ’ xt? kA + Ït kxt0 âˆ’ x? kA . (22)
0
0
The remaining task of this proof is just bound the term
kxt+1
âˆ’ xt? kA for both GPIS and Acc-GPIS algorithm and
0

To prove the results in Proposition 1, 2 and 3 we need the
following concentration lemmas as pillars:
Lemma 1. For any g âˆˆ Rd , we have:

T
sup v g = max 0, sup
vâˆˆCâˆ©Bd

uâˆˆCâˆ©Sdâˆ’1

T

u g


(30)

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

Proof. By the definition of cone projection operator we
have:
sup v T g = kPC (g)k2 â‰¥ 0
(31)
vâˆˆCâˆ©Bd

Proof. This Lemma follows the result of the simplified
form of the Gordonâ€™s Lemma [Lemma 6.7](Oymak et al.,
2015):
kSAvk2

T

if supvâˆˆCâˆ©Bd v g > 0:

â‰¥

sup v T g = sup kvk2
vâˆˆCâˆ©Bd

â‰¥

vâˆˆCâˆ©Bd

vT g
â‰¤ sup uT g, (32)
kvk2
uâˆˆCâˆ©Sdâˆ’1

kSAvk2

(bm âˆ’ W(AC âˆ© Snâˆ’1 ) âˆ’ Î¸)kAvk2
âˆš
Âµ(bm âˆ’ W(AC âˆ© Snâˆ’1 ) âˆ’ Î¸)kvk2

â‰¤ (bm + W(AC âˆ© Snâˆ’1 ) + Î¸)kAvk2
âˆš
L(bm + W(AC âˆ© Snâˆ’1 ) + Î¸)kvk2
â‰¤

and meanwhile since C âˆ© Sdâˆ’1 âˆˆ C âˆ© B d we have:
sup v T g â‰¥

uT g,

sup

(33)

1.3.1. THE PROOF FOR P ROPOSITION 1

uâˆˆCâˆ©Sdâˆ’1

vâˆˆCâˆ©Bd

Proof. Letâ€™s mark out the feasible region of the step-size Î·:
hence we have:

Î±(Î·, S t A)
sup v T g =

vâˆˆCâˆ©Bd

uT g,

sup

=

(34)

sup v T (I âˆ’ Î·AT S T SA)u
u,vâˆˆBd

uâˆˆCâˆ©Sdâˆ’1

â‰¥

sup v T (I âˆ’ Î·AT S T SA)v
vâˆˆBd

=

Lemma 2. If supu,vâˆˆCâˆ©Bd v T M u > 0, we have:
sup

vT M u =

sup

vT M u

â‰¥

(35)

sup ((1 âˆ’ Î·L(bm +
vâˆˆBd

u,vâˆˆCâˆ©Sdâˆ’1

u,vâˆˆCâˆ©Bd

sup (kvk22 âˆ’ Î·kSAvk22 )

vâˆˆBd

so if we choose a step size Î· â‰¤
Proof. Since u, v âˆˆ C âˆ© B d , kuk2 and kvk2 are both less
than or equal to 1, we can have the following upper bound:
vT M u

sup

=

(

sup
u,vâˆˆCâˆ©Bd

u,vâˆˆCâˆ©Bd

â‰¤

=

sup v T (I âˆ’ Î·AT S T SA)u
u,vâˆˆBd

v T M u,

=

sup

sup

=

v T M u,

(36)

sup

vT M u =

u,vâˆˆCâˆ©Bd

sup

vT M u

(37)

kSAvk2 â‰¥

âˆ’ku âˆ’ vk22 + Î·kSA(u âˆ’ v)k22 ]
âˆš
1
â‰¤
sup
[(1 âˆ’ Î·Âµ(bm âˆ’ d âˆ’ Î¸)2 )ku + vk22
u,vâˆˆSdâˆ’1 4
âˆš
+(Î·L(bm + d + Î¸)2 âˆ’ 1)ku âˆ’ vk22 ]

u,vâˆˆCâˆ©Sdâˆ’1

Lemma 3. If the entries of the sketching matrix S is i.i.d
drawn from Normal distribution and v âˆˆ C, we have:
âˆš

Âµ(bm âˆ’ W âˆ’ Î¸)kvk2 ,

(38)

1
[(u + v)T (I âˆ’ Î·AT S T SA)(u + v)
4

âˆ’(u âˆ’ v)T (I âˆ’ Î·AT S T SA)(u âˆ’ v)]
1
=
sup
[ku + vk22 âˆ’ Î·kSA(u + v)k22
4
dâˆ’1
u,vâˆˆS

u,vâˆˆCâˆ©Sdâˆ’1

hence we have:
sup

v T (I âˆ’ Î·AT S T SA)u

u,vâˆˆSdâˆ’1

u,vâˆˆSdâˆ’1

vT M u â‰¥

we can en-

Î±(Î·, S t A)

and meanwhile since C âˆ© Sdâˆ’1 âˆˆ C âˆ© B d we have:
sup

1
âˆš
L(bm + d+Î¸)2

( > 0) we have
sure that with probability 1 âˆ’ e
Î±(Î·, S t A) > 0 and the Lemma 2 become applicable:

u,vâˆˆCâˆ©Sdâˆ’1

u,vâˆˆCâˆ©Bd

d + Î¸ âˆ’ )2 )kvk22 ),

(Î¸âˆ’)2
âˆ’ 2

vT M u
)kvk2 kuk2
kvk2 kuk2

sup

âˆš

The last line of inquality holds with probability at least 1 âˆ’
Î¸2
2eâˆ’ 2 according to Lemma 3. Then since we have set Î· â‰¤
âˆš1
, and meanwhile notice the fact that ku +
L(b + d+Î¸+)2
m

vk22 â‰¤ 4 we have:
kSAvk2 â‰¤

âˆš

L(bm + W + Î¸)kvk2 ,
(39)
m+1
2
âˆš
Î¸
Î“(
)
with probability at least 1 âˆ’ eâˆ’ 2 . (bm = 2 Î“( m2 ) â‰ˆ
2
âˆš
m, W := W(AC âˆ© Snâˆ’1 ))

Î±(Î·, S t A)
âˆš
1
â‰¤
sup
(1 âˆ’ Î·Âµ(bm âˆ’ d âˆ’ Î¸)2 ku + vk22
u,vâˆˆSdâˆ’1 4
âˆš
â‰¤ (1 âˆ’ Î·Âµ(bm âˆ’ d âˆ’ Î¸)2 )

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

If we chose Î· =

1
âˆš
L(bm + d+Î¸)2

we have:

!
âˆš
Âµ (bm âˆ’ d âˆ’ Î¸)2
âˆš
1âˆ’
,
L (bm + d + Î¸)2

t

Î±(Î·, S A) â‰¤

(40)

Then let  â†’ 0, we shall get the result shown in Proposition
1.

1.3.2. T HE PROOF FOR P ROPOSITION 2
Proof. Recall that Ït is defined as:
T

1 t
supvâˆˆACâˆ©Snâˆ’1 v T ( m
S S t âˆ’ I)z
,
Ï(S , A) =
1
inf vâˆˆACâˆ©Snâˆ’1 m kS t vk22
t

(41)

we start by lower-bounding the denominator, by simplified
Gordonâ€™s lemma [Lemma 6.7](Oymak et al., 2015) we directly have:

inf nâˆ’1

vâˆˆACâˆ©S

1
(bm âˆ’ W âˆ’ Î¸)2
kSvk22 â‰¥
,
m
m

with probability at least (1 âˆ’ eâˆ’
upper bound for the numerator:
T

v

T

St St
âˆ’I
m

Î¸2
2

hence we have the following by [Lemma 6.8](Oymak et al.,
2015):
!
T
St St
T
v
âˆ’I z
m


1 1
2
2
â‰¤
(bm kv + zk2 + W + Î¸) âˆ’ kv + zk2
4 m


1 1
2
2
(bm kv âˆ’ zk2 + W + Î¸) âˆ’ kv âˆ’ zk2
+
4 m
 2

1
bm
2bm (W + Î¸)
2
=
(
âˆ’ 1)kv + zk2 +
kv + zk2
4
m
m


1
b2m
2bm (W + Î¸)
2
+
(1 âˆ’
)kv âˆ’ zk2 +
kv âˆ’ zk2 ,
4
m
m
(45)
2
âˆ’ Î¸8
). Note that kv + zk2 +
with probability
âˆš at least (1 âˆ’ 8e
kv âˆ’ zk2 â‰¤ 2 2 and kv + zk22 + kv âˆ’ zk22 â‰¤ 4, we have:
!
T
St St
T
v
âˆ’I z
m
b2
2bm (W + Î¸) kv + zk2 + kv âˆ’ zk2
(46)
+ | m âˆ’ 1|
m
4
m
âˆš
2bm (W + Î¸)
b2
â‰¤
+ | m âˆ’ 1|
m
m
â‰¤

thus finishes the proof.
(42)
1.3.3. T HE PROOF FOR P ROPOSITION 3
Proof. Recall that Ïƒt is defined as:

).Then we move to the
Ïƒ(S t , A) =

!

supvâˆˆrange(A)âˆ©Snâˆ’1 kS t vk22
,
inf vâˆˆrange(A)âˆ©Snâˆ’1 kS t vk22

(47)

by simply apply again the Gordonâ€™s lemma
âˆš [Lemma
6.7](Oymak et al., 2015), with W(ASdâˆ’1 ) â‰¤ d, we with
obtain the upper bound on the numerator:
âˆš
(48)
sup
kS t vk22 â‰¤ (bm + d + Î¸)2 ,

z
T

1
St St
{(v + z)T (
âˆ’ I)(v + z)
4
m
T
St St
âˆ’ (v âˆ’ z)T (
âˆ’ I)(v âˆ’ z)}
m
1 1
= { kS t (v + z)k2 âˆ’ kv + zk2
4 m
1
+ kv âˆ’ zk2 âˆ’ kS t (v âˆ’ z)k2 },
m
=

vâˆˆrange(A)âˆ©Snâˆ’1

(43)
and the lower bound:
inf

kS t vk22 â‰¥ (bm âˆ’
nâˆ’1

âˆš

vâˆˆrange(A)âˆ©S

both with probability at least 1 âˆ’ eâˆ’

Î¸2
2

d âˆ’ Î¸)2 ,

(49)

.

1.4. Details of the implementation of algorithms and
numerical experiments

and,
W(AC âˆ© Snâˆ’1 âˆ’ z) = Eg (

sup

vâˆˆACâˆ©Snâˆ’1
T

= Eg (g z +

g T (v âˆ’ z))
sup

vâˆˆACâˆ©Snâˆ’1
nâˆ’1

= W(AC âˆ© S

)

v T g) (44)

For our GPIS and Acc-GPIS algorithms, we have several
key points of implemenations:
â€¢ Count sketch
As described in the main text.

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

â€¢ Line search

â€¢ Gradient restart for Acc-GPIS

1.4.1. P ROCEDURE TO GENERATE SYNTHETIC DATA
SETS

The procedure we used to generate a constrained leastsquare problem sized n by 100 with approximately s-sparse
solution and a condition number Îº strictly follows:
1) Generate a random matrix A sized n by 100 with i.i.d
entries drawn from N (0, 1).
2) Calculate Aâ€™s SVD: A = U Î£V T and replace the singular values diag(Î£)i by a sequence:
diag(Î£)iâˆ’1
1

Îºd

m = 200
m = 400
m = 800

0.05

0.04

0.03

0.02

0.01

0
0

2

4

(50)

3) Generate the â€ground truthâ€ vector xgt sized 100 by
1 randomly with only s non-zero entries in a orthongonal transformed domain Î¦, and calculate the l1 norm of it

6

8

10

12

14

16

18

20

sparsity of the solution

Figure 1. Experimental results on the average choices of GPISâ€™s
step sizes given by line-search scheme (Nesterov, 2013)

Table 1. Synthetic data set for step size experiment
DATA SET

(Oâ€™Donoghue & Candes, 2015) has proposed two
heuristic adaptive restart schemes - gradient restart
and function restart for the accelerated gradient methods and have shown significant improvements without
the need of the knowledge of the functional parameters Âµ and L. Such restart methods are directly applicable for the Acc-GPIS by nature due to its sketched
deterministic iterations. Here we choose the gradient
restart since it achieves comparable performance in
practice as function restart but cost only O(d) operations.

diag(Î£)i =

average step size given by line-search for GPIS

0.06

We implement the line-search scheme given by (Nesterov, 2007) and is described by Algorithm 3 for GPIS
and Acc-GPIS in our experiments with parameters
Î³u = 2, and Î³d = 2. Such choice of line-search parameters simply means: when even we find the condition ft (PK (xi âˆ’ Î·Oft (xi ))) â‰¤ mL does not hold,
we shrink the step size by a factor of 2; and then at the
beginning of each iteration, we increase the step size
chosen at previous iteration by a factor of 2, then do
backtracking again. Hence our methods are able to ensure we use an aggressive step size safely in each iteration. This is an important advantage of the sketched
gradient method since we observe that for stochastic gradient such as SAGA a heuristic backtracking
method similar to Algorithm 3 may work but it will
demand a very small Î³d (tends to 1) otherwise SAGA
may go unstable, and an aggressive choice like our
Î³d = 2 is unacceptable for SAGA. (Hence we suspect
that SAGA is unlikely to be able to benefit computational gains from line-search as our method does.)

S YN 4

S IZE

S

Î¦

(20000, 100)

-

I

(r = kÎ¦xgt k1 ). Hence the constrained set can be described
as K = {x : kÎ¦xk1 â‰¤ r}.
4) Generate a random error vector w with i.i.d entries such
kAxgt k2
= 10.
that kwk
2
5) Set y = Axgt + w
1.4.2. E XTRA EXPERIMENT FOR STEP SIZE CHOICE
We explore the step size choices the GPIS algorithm produce through using the line-search scheme with respect to
different sparsity level of the solution. The result we shown
is the average of 50 random trials.
The result of the step-size simulation demonstrates that the
step sizes chosen on average by the line-search scheme for
the GPIS algorithm is actually related with the sparsity of
the ground truth xgt : at a regime when the xgt is sparse
enough, the step size one can achieve goes up rapidly w.r.t
the sparsity. While in our Proposition 2 we revealed that
the outerloop of GPIS/Acc-GPIS can benefit from the constrained set, and here surprisingly we also find out numerically that the inner loopâ€™s can also benefit from the constrained set by aggressively choosing the large step sizes.
Such a result echos the analysis of the PGD algorithm on
constrained Least-squares with a Gaussian map A (Oymak
et al., 2015). Further experiments and theoretical analysis
of such greedy step sizes for sketched gradients and full

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

gradients on general maps is of great interest and will go
beyond the state of the art analysis for convex optimization.

References
Beck, Amir and Teboulle, Marc. A fast iterative shrinkagethresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183â€“202, 2009.
doi: 10.1137/080716542. URL http://dx.doi.
org/10.1137/080716542.
Nesterov, Yurii. Gradient methods for minimizing composite objective function. Technical report, UCL, 2007.
Nesterov, Yurii. Gradient methods for minimizing composite functions. Mathematical Programming, 140(1):
125â€“161, 2013.
Oâ€™Donoghue, Brendan and Candes, Emmanuel. Adaptive
restart for accelerated gradient schemes. Foundations of
computational mathematics, 15(3):715â€“732, 2015.
Oymak, Samet, Recht, Benjamin, and Soltanolkotabi,
Mahdi. Sharp timeâ€“data tradeoffs for linear inverse problems. arXiv preprint arXiv:1507.04793, 2015.
Pilanci, Mert and Wainwright, Martin J. Randomized
sketches of convex programs with sharp guarantees. Information Theory, IEEE Transactions on, 61(9):5096â€“
5115, 2015.
Pilanci, Mert and Wainwright, Martin J. Iterative hessian
sketch: Fast and accurate solution approximation for
constrained least-squares. Journal of Machine Learning
Research, 17(53):1â€“38, 2016.

