Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

A. Additional Proofs
A.1. Proof of Thm. 2
This proof bears resemblance to the proof provided in Eldan & Shamir (2016)[Lemma 10], albeit once approximating
2
kxk2 , the following construction takes a slightly different route. For completeness, we also state assumptions 1 and 2 from
Eldan & Shamir (2016):
Assumption 1. Given the activation function Ïƒ, there is a constant cÏƒ â‰¥ 1 (depending only on Ïƒ) such that the following
holds: For any L-Lipschitz function f : R â†’ R which is constant outside a bounded interval [âˆ’R, R], and for any Î´, there
RL
exist scalars a, {Î±i , Î²i , Î³i }w
i=1 , where w â‰¤ cÏƒ Î´ , such that the function
h(x) = a +

w
X

Î±i Â· Ïƒ(Î²i x âˆ’ Î³i )

i=1

satisfies
sup |f (x) âˆ’ h(x)| â‰¤ Î´.
xâˆˆR

As discussed in Eldan & Shamir (2016), this assumption is satisfied by ReLU, sigmoid, threshold, and more generally all
standard activation functions we are familiar with.
Assumption 2. The activation function Ïƒ is (Lebesgue) measurable and satisfies
Î±

|Ïƒ (x)| â‰¤ C (1 + |x| )
for all x âˆˆ R and for some constants C, Î± > 0.
Proof. Consider the 4-Lipschitz function

	
l (x) = min x2 , 4 ,
which is constant outside [âˆ’2, 2], as well as the function
` (x) =

d
X

l(xi ) =

i=1

d
X


	
min x2i , 4

i=1

Pw
on Rd . Applying assumption 1, we obtain a function Ëœl(x) having the form a + i=1 Î±i Ïƒ (Î²i x âˆ’ Î³i ) so that

 Î´


sup Ëœl (x) âˆ’ l (x) â‰¤ ,
d
xâˆˆR
and where the width parameter w is at most

8cÏƒ d
Î´ .

Consequently, the function
`Ëœ(x) =

d
X

Ëœl (xi )

i=1

can be expressed in the form a +

Pw

i=1

2

Î±i Ïƒ (Î²i x âˆ’ Î³i ) where w â‰¤ 8cÏƒÎ´ d , yielding an approximation satisfying




sup `Ëœ(x) âˆ’ ` (x) â‰¤ Î´.
xâˆˆRd

We now invoke assumption 1 again to approximate the 1-Lipschitz function
ï£±
ï£´
x < âˆ’0.5
ï£²0
f (x) = x + 0.5 x âˆˆ [âˆ’0.5, 0.5]
ï£´
ï£³
1
x > 0.5

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks



PwÌƒ
and obtain an approximation fËœ (x) = aÌƒ + i=1 Î±Ìƒi Ïƒ Î²Ìƒi x âˆ’ Î³Ìƒi satisfying

 rÎ´
Ëœ

(6)
sup f (x) âˆ’ f (x) â‰¤
2
xâˆˆR
p
where wÌƒ â‰¤ cÏƒ 1/2Î´.


Now consider the composition fËœ â—¦ cÂµ Â· `Ëœ âˆ’ cÂµ , where cÂµ > 0 is to be determined later. This composition has the form
ï£«
ï£¶
w
w
X
X
a+
ui Ïƒ ï£­
vi,j Ïƒ (hwi,j , xi + bi,j ) + ci ï£¸
i=1

j=1

o
n
p
for appropriate scalars a, ui , ci , vi,j , bi,j and vectors wi,j , and where w is at most max 8cÏƒ d2 /Î´, cÏƒ 1/2Î´ . It is now


left to bound the approximation error obtained by fËœ â—¦ cÂµ Â· `Ëœ âˆ’ cÂµ . Define for any  > 0,
n
o
2
R = x âˆˆ Rd : 1 âˆ’  â‰¤ kxk2 â‰¤ 1 +  .
Since Âµ is continuous, there exists  > 0 such that
Z
Âµ (x) dx â‰¤
R
d

Now, for any x âˆˆ R such that 1 +  â‰¤

2
kxk2

Î´
.
4

we have

n
o
2
`Ëœ(x) â‰¥ ` (x) âˆ’ Î´ = min kxk2 , 4 âˆ’ Î´ â‰¥ 1 +  âˆ’ Î´.
Assuming Î´ < /2, we have the above is at least
1 + /2.
Taking cÂµ = 1/, we get
and thus


 c 
Âµ
= 0.5,
cÂµ Â· `Ëœ(x) âˆ’ cÂµ = cÂµ Â· `Ëœ(x) âˆ’ 1 â‰¥
2
r !
r


Î´
Î´
Ëœ
Ëœ
f cÂµ Â· ` (x) âˆ’ cÂµ âˆˆ 1 âˆ’
,1 +
,
2
2
2

(7)
2

For any x âˆˆ Rd satisfying 1 +  â‰¤ kxk2 . A similar argument shows that for any x âˆˆ R satisfying kxk2 â‰¤ 1 âˆ’  we have
r r !


Î´
Î´
fËœ cÂµ Â· `Ëœ(x) âˆ’ cÂµ âˆˆ âˆ’
,
.
(8)
2
2
Combining both Eq. (7) and Eq. (8) we obtain
Z  

2
fËœ cÂµ Â· `Ëœ(x) âˆ’ cÂµ âˆ’ 1 (kxk2 â‰¤ 1) Âµ (x) dx
d
ZR  

2
=
fËœ cÂµ Â· `Ëœ(x) âˆ’ cÂµ âˆ’ 1 (kxk2 â‰¤ 1) Âµ (x) dx
R
Z
 

2
+
fËœ cÂµ Â· `Ëœ(x) âˆ’ cÂµ âˆ’ 1 (kxk2 â‰¤ 1) Âµ (x) dx
Rd \R
Z
Z
Î´
â‰¤
2Âµ (x) dx +
Âµ (x) dx
R
Rd \R 4
Î´
Î´
â‰¤ + = Î´,
2 2
Where
summand in the penultimate inequality is justified due to fËœ being bounded in the interval
h p the first
p i
p
âˆš
âˆ’ Î´/2, 1 + Î´/2 by Eq. (6), and assuming 1 + Î´/2 â‰¤ 2, and the second summand justified due to Equations
(7) and (8), concluding the proof of the lemma.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

A.2. Proof of Thm. 3
Consider an input distribution of the form
x = srv,
where v is drawn from a certain distribution on the unit L1 sphere {x : kxk1 = 1} to be specified later, and s is uniformly
distributed on [1, 1 + ].
Let
F (x) =

N
X

aj [hwj , xi + bj ]+

j=1

be a 2-layer ReLU network of width N , such that with respect to the distribution above,
 


 
Î´
.
Ex (f (kxk1 ) âˆ’ F (x))2 = Ev Es (f (sr) âˆ’ F (srv))2 v
â‰¤
2
By Markovâ€™s inequality, this implies
 


1
Prv Es f (sr) âˆ’ F (srv)2 v â‰¤ Î´ â‰¥
.
2
 

By the assumption on f , and the fact that s is uniform on [1, 1 + ], we have that Es f (sr) âˆ’ F (srv)2 v â‰¤ Î´ only if fËœN
is not a linear function on the line between rv and (1 + )rv. In other words, this line must be crossed by the hyperplane
{x : hwj , xi + bj = 0} for some neuron j. Thus, we must have
Prv (âˆƒj âˆˆ {1, . . . , N }, s âˆˆ [1, 1 + ] s.t. hwj , srvi + bj = 0) â‰¥

1
.
2

(9)

The left hand side equals
Prv (âˆƒj âˆˆ {1 . . . N }, s âˆˆ [1, 1 + ] s.t. hwj , vi = âˆ’bj /sr)


bj
bj
and âˆ’
= Prv âˆƒj âˆˆ {1 . . . N } s.t. hwj , vi between âˆ’
r
(1 + )r


bj
bj
â‰¤ Prv âˆƒj âˆˆ {1 . . . N } s.t. hwj , vi betwen âˆ’
and âˆ’ (1 âˆ’ )
r
r


N
X
bj
bj
â‰¤
Prv hwj , vi between âˆ’
and âˆ’ (1 âˆ’ )
r
r
j=1


b
b
â‰¤ N Â· sup Prv hw, vi between âˆ’ and âˆ’ (1 âˆ’ )
r
r
wâˆˆRd ,bâˆˆR



âˆ’rw
= N Â· sup Prv
, v âˆˆ [1 âˆ’ , 1] = N Â· sup Prv (hw, vi âˆˆ [1 âˆ’ , 1]) ,
b
d
wâˆˆR ,bâˆˆR
wâˆˆRd
1
where in the first inequality we used the fact that 1+
â‰¥ 1 âˆ’  for all  âˆˆ (0, 1), and in the second inequality we used a
union bound. Combining these inequalities with Eq. (9), we get that

N â‰¥

1
.
supw Prv (hw, vi âˆˆ [1 âˆ’ , 1])

As a result, to prove the theorem, it is enough to construct a distribution for v on the on the unit L1 ball, such that for any
w,
Prv (hw, vi âˆˆ [1 âˆ’ , 1]) â‰¤ OÌƒ( + exp(âˆ’â„¦(d)))
(10)
By the inequality above, we would then get that N = â„¦Ìƒ(min{1/, exp(â„¦(d))}).
Specifically, consider a distribution over v defined as follows: First, we sample Ïƒ âˆˆ {âˆ’1, +1}d uniformly at random, and
n âˆˆ Rd from a standard Gaussian distribution, and define


 
1
1
vÌ‚ =
Ïƒ + cd I âˆ’ ÏƒÏƒ > n ,
d
d

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

where cd > 0 is a parameter dependent on d to be determined later. It is easily verified that hÏƒ/d, vÌ‚i = hÏƒ/d, Ïƒ/di
independent of n, hence vÌ‚ lies on the hyperplane containing the facet of the L1 ball on which Ïƒ/d resides. Calling this
facet FÏƒ , we define v to have the same distribution as vÌ‚, conditioned on vÌ‚ âˆˆ FÏƒ .
We begin by arguing that
Prv (hw, vi âˆˆ [1 âˆ’ , 1]) â‰¤ 2 Â· PrvÌ‚ (hw, vÌ‚i âˆˆ [1 âˆ’ , 1]) .

(11)

To see this, let A = {x : hw, xi âˆˆ [1 âˆ’ , 1]}, and note that the left hand side equals
Pr(v âˆˆ A) = EÏƒ [Pr(v âˆˆ A|Ïƒ)] = EÏƒ [Pr (vÌ‚ âˆˆ A|Ïƒ, vÌ‚ âˆˆ FÏƒ )]


Pr(vÌ‚ âˆˆ A âˆ© FÏƒ |Ïƒ)
1
= EÏƒ
â‰¤
EÏƒ [Pr(vÌ‚ âˆˆ A|Ïƒ)]
Pr(vÌ‚ âˆˆ FÏƒ |Ïƒ)
minÏƒ Pr(vÌ‚ âˆˆ FÏƒ |Ïƒ)
Pr(vÌ‚ âˆˆ A)
.
=
minÏƒ Pr(vÌ‚ âˆˆ FÏƒ |Ïƒ)
Therefore, to prove Eq. (11), it is enough to prove that Pr(vÌ‚ âˆˆ FÏƒ |Ïƒ) â‰¥ 1/2 for any Ïƒ. As shown earlier, vÌ‚ lies on the
hyperplane containing FÏƒ , the facet of the L1 ball in which Ïƒ/d resides. Thus, vÌ‚ can be outside FÏƒ , only if
 at least one
of its coordinates has a different sign than Ïƒ. By definition of vÌ‚, this can only happen if cd (I âˆ’ ÏƒÏƒ > /d)nâˆž â‰¥ 1. The
probability of this event (over the random draw of n) equals


!
 



d


X


1
1
1


Pr
max cd nj âˆ’ hÏƒ, ni Ïƒj  â‰¥ 1 = Pr
max nj âˆ’ Ïƒj Â·
Ïƒi ni  â‰¥
.
 cd
d
d i=1
jâˆˆ{1...d} 
jâˆˆ{1...d} 
Since Ïƒi âˆˆ {âˆ’1, 1} for all i, the event on the right hand side can only occur if |nj | â‰¥ 1/2cd for some j. Recalling that
each nj has a standard Gaussian distribution, this probability can be upper bounded by








1
1
1
1
Pr
max |nj | â‰¥
â‰¤ d Â· Pr |n1 | â‰¥
= 2d Â· Pr n1 â‰¥
â‰¤ 2d Â· exp âˆ’ 2 ,
2cd
2cd
2cd
4cd
jâˆˆ{1...d}
where we used a union bound and a standard Gaussian tail bound. Thus, by picking
s
1
cd =
,
4 log(4d)
we can ensure that the probability is at most 1/2, hence proving that Pr(vÌ‚ âˆˆ FÏƒ |Ïƒ) â‰¥ 1/2 and validating Eq. (11).
With Eq. (11) in hand, we now turn to upper bound
 
 


1
1
>
Pr (hw, vÌ‚i âˆˆ [1 âˆ’ , 1]) = Pr
w, Ïƒ + cd I âˆ’ ÏƒÏƒ
n âˆˆ [1 âˆ’ , 1] .
d
d
By the equation above, we have that conditioned on Ïƒ, the distribution of hw, vÌ‚i is Gaussian with mean hÏƒ, wi /d and
variance
!

2

2

2 !
2
c2d
1
c2d
hw, Ïƒi
cd kwk
1
w
2
>
>
Â·w
I âˆ’ ÏƒÏƒ
w = 2 Â· kwk âˆ’
=
Â· 1âˆ’
,Ïƒ
.
d2
d
d
d
d
d kwk
By Hoeffdingâ€™s inequality, we have that for any t > 0,



 hÏƒ, wi 
 > t Â· kwk
PrÏƒ 
â‰¤ 2 exp(âˆ’2t2 )
d 
d
and
PrÏƒ


 r !
 w

d


â‰¤ 2 exp(âˆ’d).
 kwk , Ïƒ  > 2

This means that with probability at least 1 âˆ’ 2 exp(âˆ’d) âˆ’ 2 exp(âˆ’2t2 ) over the choice of Ïƒ, the distribution of hw, vÌ‚i

2
(conditioned on Ïƒ) is Gaussian with mean bounded in absolute value by t kwk /d, and variance of at least cd kwk
Â·
d

2

1 âˆ’ d1 Â· d2 = 12 cd kwk
. To continue, we utilize the following lemma:
d

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Lemma 1. Let n be a Gaussian random variable on R with mean Âµ and variance v 2 for some v > 0. Then for any
 âˆˆ (0, 1),
r


2
|Âµ|

Pr (n âˆˆ [1 âˆ’ , 1]) â‰¤
Â· max 1,
Â·
.
Ï€
v
1âˆ’
Proof. Since the probability can only increase if we replace the mean Âµ by |Âµ|, we will assume without loss of generality
that Âµ â‰¥ 0.
By definition of a Gaussian distribution, and using the easily-verified fact that exp(âˆ’z 2 ) â‰¤ min{1, 1/z} for all z â‰¥ 0, the
probability equals
âˆš

2Ï€v 2

1





(x âˆ’ Âµ)2

(x âˆ’ Âµ)2
exp âˆ’
dx â‰¤ âˆš
Â· max exp âˆ’
v2
v2
2Ï€v 2 xâˆˆ[1âˆ’,1]
1âˆ’





v

1
1
â‰¤ âˆš
Â· max min 1,
= âˆš Â· max min
,
|x âˆ’ Âµ|
v |x âˆ’ Âµ|
2Ï€ xâˆˆ[1âˆ’,1]
2Ï€v 2 xâˆˆ[1âˆ’,1]
max{1, Âµv }

1

= âˆš Â· max
= âˆš Â· max
Âµ
2Ï€ xâˆˆ[1âˆ’,1] max{v, |x âˆ’ Âµ|}
2Ï€ xâˆˆ[1âˆ’,1] max{1, v } Â· max{v, |x âˆ’ Âµ|}
Âµ
max{1, Âµv }
max{1, v }


â‰¤ âˆš Â· max
= âˆš Â·
2Ï€ xâˆˆ[1âˆ’,1] max{Âµ, |x âˆ’ Âµ|}
2Ï€ max{Âµ, minxâˆˆ[1âˆ’,1] |x âˆ’ Âµ|}

Z

1

A simple case analysis reveals that the denominator is at least2

1âˆ’
2 ,

from which the result follows.

Using this lemma and the previous observations, we get that with probability at least 1 âˆ’ 2 exp(âˆ’d) âˆ’ 2 exp(âˆ’2t2 ) over
the choice of Ïƒ,
r


t kwk /d

2
âˆš
Â· max 1,
Pr(hw, vÌ‚i âˆˆ [1 âˆ’ , 1]|Ïƒ) â‰¤
Â·
Ï€
1âˆ’
cd kwk / 2d
r


2
t

=
Â· max 1, âˆš
Â·
.
Ï€
1
âˆ’

cd 2
Letting E be the event that Ïƒ is such that this inequality is satisfied (and noting that its probability of non-occurence is at
most 2 exp(âˆ’d) + 2 exp(âˆ’2t2 )), we get overall that
Pr(hw, vÌ‚i âˆˆ [1 âˆ’ , 1]) = Pr(E) Â· Pr(hw, vÌ‚i âˆˆ [1 âˆ’ , 1]|E) + Pr(Â¬E) Â· Pr(hw, vÌ‚i âˆˆ [1 âˆ’ , 1]|Â¬E)
â‰¤ 1 Â· Pr(hw, vÌ‚i âˆˆ [1 âˆ’ , 1]|E) + Pr(Â¬E) Â· 1
r


2
t

â‰¤
Â· max 1, âˆš
Â·
+ 2 exp(âˆ’d) + 2 exp(âˆ’2t2 ).
Ï€
1âˆ’
cd 2
Recalling Eq. (11) and the definition of cd , we get that
r
n
o
p
8

Pr(hw, vi âˆˆ [1 âˆ’ , 1]) â‰¤
Â· max 1, t Â· 2 log(4d) Â·
+ 2 exp(âˆ’d) + 2 exp(âˆ’2t2 ).
Ï€
1âˆ’
q

Picking t = 12 log 1âˆ’
, we get the bound

r

( s 
)
!

8
1âˆ’

Â· max 1, log
log(4d) + 2 Â·
+ 2 exp(âˆ’d) = OÌƒ ( + exp(âˆ’d)) .
Ï€

1âˆ’

This justifies Eq. (10), from which the result follows.
2
If Âµ âˆˆ [1 âˆ’ , 1], then we get max{Âµ, 0} = Âµ â‰¥ 1 âˆ’ . If Âµ > 1, we get max{Âµ, Âµ âˆ’ 1} > 1 â‰¥ 1 âˆ’ . If Âµ < 1 âˆ’ , we get
max{Âµ, 1 âˆ’  âˆ’ Âµ} â‰¥ (1 âˆ’ )/2.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

A.3. Proof of Thm. 4
The proof rests largely on the following key result:
Theorem 7. Let Gn be the family of piece-wise linear functions on the domain [0, 1] comprised of at most n linear segments.
d
Let Gnd be the family of piece-wise linear functions on the domain [0, 1] , with the property that for any g âˆˆ Gnd and any
d
affine transformation h : R â†’ Rd , g â—¦ h âˆˆ Gn . Suppose f : [0, 1] â†’ R is C 2 . Then for all Î» > 0
Z
c Â· Î»2 Â· ÏƒÎ» (f )5
,
inf
(f âˆ’ g)2 dÂµd â‰¥
d
n4
gâˆˆGn
[0,1]d
where c =

5
4096 .

Thm. 7 establishes that the error of a piece-wise linear approximation of a C 2 function cannot decay faster than quartically
in the number of linear segments of any one-dimensional projection of the approximating function. Note that this result is
stronger than a bound in terms of the total number of linear regions in Rd , since that number can be exponentially higher
(in the dimension) than n as defined in the theorem.
Before proving Thm. 7, let us explain how we can use it to prove Thm. 4. To that end, we use the result in Telgarsky (2016,
Lemma 3.2), of which the following is an immediate corollary:
d
Corollary 2. Let Nm,l
denote the family of ReLU neural networks receiving input of dimension d and having depth l and
maximal width m. Then
d
d
Nm,l
âŠ† G(2m)
l.

Combining this corollary with Thm. 7, the result follows. The remainder of this subsection will be devoted to proving
Thm. 7.
A.3.1. S OME T ECHNICAL T OOLS
Definition 2. Let Pi denote the ith Legendre Polynomial given by Rodriguesâ€™ formula:
i i
1 di h 2
x
âˆ’
1
.
Pi (x) = i
2 i! dxi
These polynomials are useful for the following analysis since they obey the orthogonality relationship
Z 1
2
Î´ij .
Pi (x) Pj (x) dx =
2i
+1
âˆ’1
Since we are interested in approximations on small intervals where
n the
o approximating function is linear, we use the change
of variables x =

2
`t

âˆ’ 2` a âˆ’ 1 to obtain an orthogonal family PÌƒi

âˆž

i=1

of shifted Legendre polynomials on the interval

[a, a + `] with respect to the L2 norm. The first few polynomials of this family are given by
PÌƒ0 (x) = 1


2
2
a+1
PÌƒ1 (x) = x âˆ’
`
`


 2

6
12a 6
6a
6a
x
+
+
1
.
PÌƒ2 (x) = 2 x2 âˆ’
+
+
`
`2
`
`2
`
The shifted Legendre polynomials obey the orthogonality relationship
Z a+`
`
PÌƒi (x) PÌƒj (x) dx =
Î´ij .
2i + 1
a
Definition 3. We define the Fourier-Legendre series of a function f : [a, a + `] â†’ R to be
f (x) =

âˆž
X
i=0

aÌƒi PÌƒi (x) ,

(12)

(13)

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

where the Fourier-Legendre Coefficients aÌƒi are given by
aÌƒi =

2i + 1
`

Z

a+`

PÌƒi (x) f (x) dx.
a

Theorem 8. A generalization of Parsevalâ€™s identity yields
2

kf kL2 = `

âˆž
X
i=0

aÌƒ2i
.
2i + 1

(14)

Definition 4. A function f is Î»-strongly convex if for all w, u and Î± âˆˆ (0, 1),
f (Î±w + (1 âˆ’ Î±)u) â‰¤ Î±f (w) + (1 âˆ’ Î±)f (u) âˆ’

Î»
2
Î±(1 âˆ’ Î±) kw âˆ’ uk2 .
2

A function is Î»-strongly concave, if âˆ’f is Î»-strongly convex.
A.3.2. O NE - DIMENSIONAL L OWER B OUNDS
We begin by proving two useful lemmas; the first will allow us to compute the error of a linear approximation of onedimensional functions on arbitrary intervals, and the second will allow us to infer bounds on the entire domain of approximation, from the lower bounds we have on small intervals where the approximating function is linear.
Lemma 2. Let f âˆˆ C 2 . Then the error of the optimal linear approximation of f denoted P f on the interval [a, a + `]
satisfies
âˆž
X
aÌƒ2i
2
.
(15)
kf âˆ’ P f kL2 = `
2i + 1
i=2
Proof. A standard result on Legendre polynomials is that given any function f on the interval [a, a + `], the best linear
approximation (w.r.t. the L2 norm) is given by
P f = aÌƒ0 PÌƒ0 (x) + aÌƒ1 PÌƒ1 (x) ,
where PÌƒ0 , PÌƒ1 are the shifted Legendre polynomials of degree 0 and 1 respectively, and aÌƒ0 , aÌƒ1 are the first two FourierLegendre coefficients of f as defined in Eq. (3). The square of the error obtained by this approximation is therefore
2

2

2

kf âˆ’ P f k = kf k âˆ’ 2 hf, P f i + kP f k
!


âˆž
2
2
X
aÌƒ
aÌƒ2i
aÌƒ
1
1
âˆ’ 2 aÌƒ20 +
=`
+ aÌƒ20 +
2i + 1
3
3
i=0
=`

âˆž
X
i=2

aÌƒ2i
.
2i + 1

Where in the second equality we used the orthogonality relationship from Eq. (13), and the generalized Parsevalâ€™s identity
from Eq. (14).
2

Lemma 3. Suppose f : [0, 1] â†’ R satisfies kf âˆ’ P f kL2 â‰¥ c`5 for some constant c > 0, and on any interval [a, a + `] âŠ†
[0, 1]. Then
Z 1
c
inf
(f âˆ’ g)2 dÂµ â‰¥ 4 .
gâˆˆGn 0
n
Proof. Let g âˆˆ Gn be some function, let a0 = 0, a1 , . . . , anâˆ’1 , an = 1 denote its partition into segments of length
`j = aj âˆ’ ajâˆ’1 , where g is linear when restricted to any interval [ajâˆ’1 , aj ], and let gj , j = 1, . . . , n denote the linear

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

restriction of g to the interval [ajâˆ’1 , aj ]. Then
1

Z

2

(f âˆ’ g) dÂµ =
0

â‰¥

n Z
X
j=1
n
X

aj

2

(f âˆ’ gj ) dÂµ

ajâˆ’1

c`5j

j=1
n
X

=c

`5j .

(16)

j=1

Now, recall HoÌˆlderâ€™s sum inequality which states that for any p, q satisfying
n
X

ï£«
|xj yj | â‰¤ ï£­

j=1

n
X

1
p

+

1
q

= 1 we have

ï£¶1/q
ï£¶1/p ï£«
n
X
q
p
|yj | ï£¸ .
|xj | ï£¸ ï£­
j=1

j=1

Plugging in xj = `j , yj = 1 âˆ€j âˆˆ {1, . . . , n} we have
n
X

ï£¶1/p
ï£«
n
X
p
|`j | â‰¤ ï£­
|`j | ï£¸ n1/q ,

j=1

and using the equalities

Pn

j=1

|`j | = 1 and

p
q

j=1

= p âˆ’ 1 we get that
1
npâˆ’1

â‰¤

n
X

p

|`j | .

(17)

j=1

Plugging the inequality from Eq. (17) with p = 5 in Eq. (16) yields
1

Z

2

(p âˆ’ g) dÂµ â‰¥
0

c
,
n4

concluding the proof of the lemma.
Our first lower bound for approximation using piece-wise linear functions is for non-linear target functions of the simplest
kind. Namely, we obtain lower bounds on quadratic functions.
Theorem 9. If Gn is the family of piece-wise linear functions with at most n linear segments in the interval [0, 1], then for
any quadratic function p(x) = p2 x2 + p1 x + p0 , we have
Z
inf

gâˆˆGn

1

(p âˆ’ g)2 dÂµ â‰¥

0

p22
.
180n4

(18)

Proof. Observe that since p is a degree 2 polynomial, we have that its coefficients satisfy aÌƒi = 0 âˆ€i â‰¥ 3, so from Lemma 2
aÌƒ2 `
its optimal approximation error equals 52 . Computing aÌƒ2 can be done directly from the equation
p (x) =

2
X

aÌƒi PÌƒi (x) ,

i=0

Which gives aÌƒ2 =

p2 `2
6

due to Eq. (12). This implies that
2

kp âˆ’ P pk =

p22 `5
.
180

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Note that for quadratic functions, the optimal error is dependent solely on the length of the interval. Using Lemma 3 with
p22
we get
c = 180
1

Z

2

(p âˆ’ g) dÂµ â‰¥
0

p22
,
180n4

concluding the proof of the theorem.

Computing a lower bound for quadratic functions is made easy since the bound on any interval [a, a + `] depends on ` but
not on a. This is not the case in general, as can be seen by observing monomials of high degree k. As k grows, xk on


the interval [0, 0.5] converges rapidly to 0, whereas on 1 âˆ’ k1 , 1 its second derivative is lower bounded by k(kâˆ’1)
, which
4
indicates that indeed a lower bound for xk will depend on a.
For non-quadratic functions, however, we now show that a lower bound can be derived under the assumption of strong
convexity (or strong concavity) in [0, 1].
Theorem 10. Suppose f : [0, 1] â†’ R is C 2 and either Î»-strongly convex or Î»-strongly concave. Then
Z
inf

gâˆˆGn

1

(f âˆ’ g)2 dÂµ â‰¥ cÎ»2 nâˆ’4 ,

(19)

0

where c > 0 is a universal constant.

Proof. We first stress that an analogous assumption to Î»-strong convexity would be that f is Î»-strongly concave, since the
same bound can be derived under concavity by simply applying the theorem to the additive inverse of f , and observing
that the additive inverse of any piece-wise linear approximation of f is in itself, of course, a piece-wise linear function. For
this reason from now on we shall use the convexity assumption, but will also refer without loss of generality to concave
functions.
As in the previous proof, we first prove a bound on intervals of length ` and then generalize for the unit interval. From
Lemma 2, it suffices that we lower bound aÌƒ2 (although this might not give the tightest lower bound in terms of constants,
it is possible to show that it does give a tight bound over all C 2 functions). We compute
5
aÌƒ2 =
`

Z

5
=
`

Z

a+`

PÌƒ2 (x) f (x) dx
a
a+`


P2

a


2
2
x âˆ’ a âˆ’ 1 f (x) dx,
`
`

using the change of variables t = 2` x âˆ’ 2` a âˆ’ 1, dt = 2` dx, we get the above equals
5
2

Z

5
=
4

Z

1




`
`
t + + a dt
2
2



`
`
2
3t âˆ’ 1 f
t + + a dt.
2
2

P2 (t) f
âˆ’1
1
âˆ’1

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

We now integrate by parts twice, taking the anti-derivative of the polynomial to obtain


Z

5 1
`
`
3t2 âˆ’ 1 f
t + + a dt
4 âˆ’1
2
2



1

Z
 0 `

5
`
`
`
5` 1 3
3
t âˆ’t f
=
t âˆ’t f
t+ +a
t + + a dt
âˆ’
4
2
2
8 âˆ’1
2
2
âˆ’1


Z 1

`
5`
`
t âˆ’ t3 f 0
=
t + + a dt
8 âˆ’1
2
2
 2
 
1
t
t4
`
`
5`
0
âˆ’
f
t+ +a
=
8
2
4
2
2
âˆ’1
 2
 

2 Z 1
4
5`
t
t
`
`
âˆ’
âˆ’
f 00
t + + a dt
16 âˆ’1 2
4
2
2
 2
 

2 Z 1
t
5` 0
5`
t4
`
`
0
00
= (f (a + `) âˆ’ f (a)) âˆ’
âˆ’
f
t + + a dt.
32
16 âˆ’1 2
4
2
2


2
4
But since t2 âˆ’ t4 âˆˆ 0, 14 âˆ€t âˆˆ [âˆ’1, 1] and since f 00 > 0 due to strong convexity, we have that
 



Z 1 2
Z
t
t4
`
`
1 1 00 `
`
âˆ’
f 00
t + + a dt â‰¤
t + + a dt.
f
2
4
2
2
4 âˆ’1
2
2
âˆ’1

(20)

Plugging this inequality in Eq. (20) yields
5`2
5` 0
(f (a + `) âˆ’ f 0 (a)) âˆ’
aÌƒ2 â‰¥
32
64

Z

1

f

00

âˆ’1




`
`
t + + a dt
2
2

5`2 0
5` 0
(f (a + `) âˆ’ f 0 (a)) âˆ’
(f (a + `) âˆ’ f 0 (a))
32
64


` 5` 0
= 1âˆ’
(f (a + `) âˆ’ f 0 (a)) ,
2 32
=

but ` â‰¤ 1, so the above is at least
5` 0
(f (a + `) âˆ’ f 0 (a)) .
(21)
64
By Lagrangeâ€˜s intermediate value theorem, there exists some Î¾ âˆˆ [a, a + `] such that f 0 (a + `) âˆ’ f 0 (a) = `f 00 (Î¾), so
Eq. (21) is at least
5`2 00
f (Î¾) ,
64
and by using the strong convexity of f again, we get that
aÌƒ2 â‰¥

5Î»`2
.
64

Lemma 2 now gives
2

kf âˆ’ P f k = `

âˆž
X
i=2

aÌƒ2i
2i + 1

aÌƒ2
â‰¥` 2
5
5Î»2 `5
â‰¥
.
4096
Finally, by using Lemma 3 we conclude
Z
inf

gâˆˆGn

0

1

(f âˆ’ g)2 dÂµ â‰¥

5Î»2
.
4096n4

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

We now derive a general lower bound for functions f : [0, 1] â†’ R.
Theorem 11. Suppose f : [0, 1] â†’ R is C 2 . Then for any Î» > 0
1

Z

(f âˆ’ g)2 dÂµ â‰¥

inf

gâˆˆGn

0

c Â· Î»2 Â· ÏƒÎ» (f )5
.
n4

2

Proof. First, observe that if f is Î»-strongly convex on [a, b], then f ((b âˆ’ a) x + a) is Î» (b âˆ’ a) -strongly convex on [0, 1]
since âˆ€x âˆˆ [0, 1],
âˆ‚
2
2
f ((b âˆ’ a) x + a) = (b âˆ’ a) f 00 ((b âˆ’ a) x + a) â‰¥ Î» (b âˆ’ a) .
âˆ‚x2
Now, we use the change of variables x = (b âˆ’ a) t + a, dx = (b âˆ’ a) dt
Z
inf

gâˆˆGn

b

(f (x) âˆ’ g(x))2 dx

a

Z

1

= inf (b âˆ’ a)
gâˆˆGn

0

Z

1

= inf (b âˆ’ a)
gâˆˆGn

â‰¥

2

(f ((b âˆ’ a) t + a) âˆ’ g ((b âˆ’ a) t + a)) dt
2

(f ((b âˆ’ a) t + a) âˆ’ g (t)) dt
0
5

c Â· Î»2 Â· (b âˆ’ a)
,
n4

(22)

where the inequality follows from an application of Thm. 10. Back to the theorem statement, if ÏƒÎ» = 0 then the bound
trivially holds, therefore assume Î» > 0 such that ÏƒÎ» > 0. Since f is strongly convex on a set of measure ÏƒÎ» > 0, the
theorem follows by applying the inequality from Eq. (22).

A.3.3. M ULTI - DIMENSIONAL L OWER B OUNDS
We now move to generalize the bounds in the previous subsection to general dimension d. Namely, we can now turn to
proving Thm. 7.

Proof of Thm. 7. Analogously to the proof of Thm. 11, we identify a neighborhood of f in which the restriction of f to a
line in a certain direction is non-linear. We then integrate along all lines in that direction and use the result of Thm. 11 to
establish the lower bound.
Before we can prove the theorem, we need to assert that indeed there exists a set having a strictly positive measure where
d
f has strong curvature along a certain direction. Assuming f is not piece-wise linear; namely, we have some x0 âˆˆ [0, 1]
>
such that H (f ) (x0 ) 6= 0. Since H (f ) is continuous, we have that the function hv (x) = v H (f ) (x) v is continuous
and there exists a direction v âˆˆ Sdâˆ’1 where without loss of generality hv (x0 ) > 0. Thus, we have an open neighborhood
containing x0 where restricting f to the direction v forms a strongly convex function, which implies that indeed ÏƒÎ» > 0
for small enough Î» > 0.
We now integrate the approximation error on f in the neighborhood U along the direction v. Compute

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Z
inf

d
gâˆˆGn

[0,1]d

Z

(f âˆ’ g)2 dÂµd
Z

2

(f âˆ’ g) dÂµ1 dÂµdâˆ’1

= inf

d
gâˆˆGn

u:hu,vi=0

Z

Î²:(u+Î²v)âˆˆ[0,1]

Z

2

â‰¥ inf

d
gâˆˆGn

d

(f âˆ’ g) dÂµ1 dÂµdâˆ’1
u:hu,vi=0

Î²:(u+Î²v)âˆˆU

Z
â‰¥

(Âµ1 ({Î² : (u + Î²v) âˆˆ U }))
u:hu,vi=0
Z
2

=

5Î»
4096n4

5Î»2
â‰¥
4096n4
=

5

5Î»2
dÂµdâˆ’1
4096n4
5

|Âµ1 ({Î² : (u + Î²v) âˆˆ U })| dÂµdâˆ’1
u:hu,vi=0

!5

Z
Âµ1 ({Î² : (u + Î²v) âˆˆ U }) dÂµdâˆ’1
u:hu,vi=0

5Î»2 ÏƒÎ»5
,
4096n4

where in the second inequality we used Thm. 11 and in the third inequality we used Jensenâ€˜s inequality with respect to the
5
convex function x 7â†’ |x| .
A.4. Proof of Thm. 6
We begin by monitoring the rate of growth of the error when performing either an addition or a multiplication. Suppose
that the given input aÌƒ, bÌƒ is of distance at most Î´ > 0 from the desired target values a, b, i.e., |a âˆ’ aÌƒ| â‰¤ Î´, |b âˆ’ bÌƒ| â‰¤ Î´. Then
for addition we have









(a + b) âˆ’ aÌƒ + bÌƒ  â‰¤ |a âˆ’ aÌƒ| + b âˆ’ bÌƒ â‰¤ 2Î´,
and for multiplication we compute the product error estimation




aÌƒ Â· bÌƒ âˆ’ a Â· b â‰¤ |(a + Î´) Â· (b + Î´) âˆ’ a Â· b|


= Î´ (a + b) + Î´ 2  .

(23)

Now, we have bounded the error of approximating the product of two numbers which we only have approximations of, but
since the computation of the product itself cannot be done with perfect accuracy using ReLU networks, we need to suffer
the error of approximating a product, as shown in Thm. 5. We add the error of approximating the product of aÌƒ Â· bÌƒ, which
we may assume is at most Î´ (assuming Î˜ (log2 (M/Î´)) bits are used for the product, since each intermediate computation
is bounded in the interval [âˆ’M, M ]). Overall, we get an error bound of


Î´ (a + b) + Î´ 2 + Î´  â‰¤ 3M Î´.
From this, we see that at each stage the error grows by at most a multiplicative factor of 3M . After t operations, and with
tâˆ’1
1âˆ’t
an initial estimation error of Î´, we have that the error is bounded by (3M )
Î´. Choosing Î´ â‰¤ (3M )
 to guarantee
approximation , we have from Thm. 5 that each operation will require at most
&
!'
  

tâˆ’1
M (3M )
1
4 log
+ 13 â‰¤ c log
+ t log (M )


width and

&
2 log

tâˆ’1

M (3M )


!'

  

1
+ 9 â‰¤ c log
+ t log (M )


depth for some universal c > 0. Composing the networks performing each operation, we arrive at a total network width
and depth of at most


 
1
c t log
+ t2 log (M ) .


Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

Now, our target function is approximated to accuracy  by a function which our network approximates to the same accuracy
, for a total approximation error of the target function by our network of 2.

B. L1 Ball Indicator Experiment
In this section, we run a similar experiment to the one presented in 3.1, this time with respect to indicators of L1 balls.
For this experiment, we sampled 5 Â· 105 data instances uniformly at random from the 14-dimensional L1 unit sphere
(i.e. each instance is of dimension 15). We then scaled the norm of each instance independently by a scaler chosen
uniformly from the interval [0, 2]. To each instance, we associated a target value computed according to the target function
f (x) = 1 (kxk1 â‰¤ 1). Another 5 Â· 104 examples were generated in a similar manner and used as a validation set.
We trained 5 ReLU networks on this dataset:
â€¢ One 3-layer network, with a first hidden layer of size 100, a second hidden layer of size 20, and a linear output neuron.
â€¢ Four 2-layer networks, with hidden layer of sizes 100, 200, 400 and 800, and a linear output neuron.
Training was performed with backpropagation, using the TensorFlow library. We used the squared loss `(y, y 0 ) = (y âˆ’y 0 )2
and batches of size 100. For all networks, we chose a momentum parameter of 0.95, and a learning rate starting at 0.1,
decaying by a multiplicative factor of 0.95 every 1000 batches, and stopping at 10âˆ’4 .
The results are presented in Fig. 3. Like the L2 ball experiment, we see that adding depth when learning such functions is
much more helpful than increasing width. In fact, here the improvement by increased width is hardly noticeable, and the
width 400 network actually obtained a slightly better error than the width 800 network. In contrast, the 3-layer network
converged to a significantly better solution.

Depth-Width Tradeoffs in Approximating Natural Functions with Neural Networks

0.07

3-layer, width 100
2-layer, width 100
2-layer, width 200
2-layer, width 400
2-layer, width 800

0.065

RMSE (training set)

0.06

0.055

0.05

0.045

0.04

0.035

0.03

0.025

0.02
0

20

40

60

80

100

120

140

160

180

200

Batch number (x1000)
0.07

3-layer, width 100
2-layer, width 100
2-layer, width 200
2-layer, width 400
2-layer, width 800

RMSE (validation set)

0.065

0.06

0.055

0.05

0.045

0.04

0.035

0.03

0.025

0.02
0

20

40

60

80

100

120

140

160

180

200

Batch number (x1000)
Figure 3. The L1 experiment results, depicting the networkâ€™s root mean square error over the training set (top) and validation set (bottom),
as a function of the number of batches processed. Best viewed in color.

