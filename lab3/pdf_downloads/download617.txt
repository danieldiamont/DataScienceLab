Fast Bayesian Permanental Processes

A. Supplementary Material
Accompanying the submission Fast Bayesian Intensity Estimation for the Permanental Process.
A.1. Exact Expected Log Loss
We evaluate our estimated λ̂ using the expectation under the true PP(λ) of the log likelihood under PP(λ̂), where PP is
the Poisson process. Adams et al. (2009) approximate this quantity using Monte Carlo, employing numerical integration
for (1). It turns out that for the computational cost of one such numerical integration, we may compute the expected loss
using standard results for Lévy processes (Cont & Tankov, 2004). An elementary self contained argument runs as follows:
h
i
h
h
ii
EX∼PP(λ) log pX∼PP(λ̂) (X) = Ecard(X) EX∼PP(λ)| card(X) log pX∼PP(λ̂) (X)
h


i
= Ecard(X) card(X) log Λ̂(Ω) + H(λ, λ̂) − Λ̂(Ω)


= Γ(Ω) log Λ̂(Ω) + H(λ, λ̂) − Λ̂(Ω)
Z


=
λ(x) log λ̂(x) − λ̂(x) dx,
x∈Ω

where Ω is the sampling domain, H(λ, λ̂) :=

R

λ(x)
x∈Ω Λ(Ω)

log
R

λ̂(x)
Λ̂(Ω)

dx is the cross-entropy between the probability density

functions proportional to λ and λ̂ and we recall Λ(S) := x∈S λ(x) dx. The first line is the tower law of expectation. To
see the second line, note that we may sample X ∼ PP(λ) by first sampling card(X) ∼ Poisson(Λ(Ω)), and then drawing
each element of X according to the probability density proportional to λ. The third line uses the Poisson expectation
Ecard(X) [card(X)] = Γ(Ω) and the fourth some simple algebra.
As an aside, we may therefore write the Kullback-Leibler divergence in a form resembling that for probability distributions:
h
i


DKL PP(f ) PP(g) = EX∼PP(λ) log pX∼PP(λ) (X) − log pX∼PP(λ̂) (X)


Z
f (x)
=
f (x) log
+ g(x) − f (x) dx.
g(x)
x∈Ω
A.2. Bayesian Decision Theory for the Expected Log Loss
To determine the intensity function which maximises the expected log likelihood we define the loss
`(λ, λ0 ) := Eni ∼N (Bi ),i=1,2,...,m|λ log p (N (Bi ) = ni , i = 1, 2, . . . , m|λ0 )
where N (Bi ) is the random
variable representing the number of points in the set Bi ⊆ Ω, Ω is the domain of the process
R
and we recall Λ(S) := x∈S λ(x) dx. It is well known that (Baddeley, 2007)
p(N (Bi ) = ni , i = 1, 2, . . . , m|λ) =

Y Λ(Bi )ni
i

ni !

exp(−Λ(Ω)).

Bayesian decision theory considers the expected loss
L(λ0 ) := Eλ|D [`(λ, λ0 )] ,
where the expectation is with respect to the posterior
S predictive distribution given the data D. Combining these expressions
and assuming without loss of generality that Ω = i Bi yields
"
#
hX
i
0
0
0
L(λ ) = Eλ|D Eni ∼N (Bi ),i=1,2,...,m|λ
(ni log Λ (Bi ) − log(ni !) − Λ (Bi )) .
i
∗

0

The optimal choice is Λ := argmaxλ0 L(λ ), so by stationarity


λ∗ (Bi ) = Eλ|D Eni ∼N (Bi )|λ [ni ]
= Eλ|D [Λ(Bi )] ,
∗

and so λ = Eλ|D [λ], the expectation of the posterior predictive distribution.

Poisson intensity λ

Fast Bayesian Permanental Processes
data locations xi
true intensity λ(x)
predictive median
[0.1,0.9] pred. interval
predictive samples

40

20

0
0.0

0.5

1.0

1.5
2.0
input domain Ω = [0, π]

2.5

3.0

(a) λ0

50

0
0.0

0.5

1.0

1.5

2.0

2.5

3.0

2.0

2.5

3.0

2.0

2.5

3.0

2.0

2.5

3.0

(b) λ1

50
25
0
0.0

0.5

1.0

1.5

(c) λ2
40

20

0
0.0

0.5

1.0

1.5

(d) λ3

50

0
0.0

0.5

1.0

1.5

(e) λ4
Figure 6. Predictive distributions for the test problems of subsection 6.2.

Fast Bayesian Permanental Processes

A.3. Standard Laplace Approximations for the GP
Following e.g. (Rasmussen & Williams, 2006), assume that we are given an independent and identically distributed sample
{(xi , yi )}1≤i≤m , and the goal is to estimate p(y|x). Let the true joint in f = (f (xi ))i , y = (y(xi ))i be
log p(y, f |X, k) = log p(y|f ) + log p(f |X, k)
1
1
m
= log p(y|f ) − f > K −1 f − log |K| −
log 2π,
2
2
2
where K = (k(xi ), xj )ij and X = (x1 , x2 , . . . , xm ). The Laplace approximation fits a normal to the posterior,
log p(f |y, X) ≈ log N (f |fˆ, Q)
1
1
m
= − (f − fˆ)> Q−1 (f − fˆ) − log |Q| −
log 2π
2
2
2
:= log q(f |y, X).
fˆ and Q come from a second order approximation of the log posterior at its mode, i.e.
fˆ = argmax p(y|f , X)
f

= argmax p(y, f |X)
f

Q−1 = −



∂2

log
p(y,
f
|X)
 ˆ
>
∂f ∂f
f =f

= K −1 + W


∂2
Wii = −
log p(yi |fi )
2
∂fi
fi =fˆi
Taylor expanding log p(y, f |X) at f = fˆ,
1
log p(y, f |X) ≈ log p(y, fˆ|X) − (f − fˆ)> Q−1 (f − fˆ)
2
1
m
1
1
= log p(y|f = fˆ) − fˆ> K −1 fˆ − log |K| −
log 2π − (f − fˆ)> Q−1 (f − fˆ)
2
2
2
2
:= log q(y, f |X)

(17)

Now
Z
log

1
m
1
exp(− x> H −1 x)dx =
log 2π + log |H|
2
2
2

So we get the approximate marginal likelihood
log Z := log p(y|X)
Z
≈ log q(y, f |X)df
1
= log p(y|f = fˆ) − fˆ> K −1 fˆ −
2
1
= log p(y|f = fˆ) − fˆ> K −1 fˆ −
2

1
1
log |K| − log |K −1 + W |
2
2
1
log |I + KW |
2

(18)

This is a standard textbook approach (Rasmussen & Williams, 2006), but we can get the same approximation via
log p(y|X) ≈ log q(y, fˆ|X) − log q(fˆ|y, X),

(19)

since the right hand side is true for all f , not just fˆ. Hence we need only subtract the approximate log likelihoods as above.
By evaluating at fˆ, the second r.h.s. term in (17), vanishes immediately.

