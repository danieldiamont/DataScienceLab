A Distributional Perspective on Reinforcement Learning

A. Related Work
To the best of our knowledge, the work closest to ours are
two papers (Morimura et al., 2010b;a) studying the distributional Bellman equation from the perspective of its cumulative distribution functions. The authors propose both
parametric and nonparametric solutions to learn distributions for risk-sensitive reinforcement learning. They also
provide some theoretical analysis for the policy evaluation
setting, including a consistency result in the nonparametric case. By contrast, we also analyze the control setting,
and emphasize the use of the distributional equations to improve approximate reinforcement learning.
The variance of the return has been extensively studied in the risk-sensitive setting. Of note, Tamar et al.
(2016) analyze the use of linear function approximation
to learn this variance for policy evaluation, and Prashanth
& Ghavamzadeh (2013) estimate the return variance in the
design of a risk-sensitive actor-critic algorithm. Mannor
& Tsitsiklis (2011) provides negative results regarding the
computation of a variance-constrained solution to the optimal control problem.
The distributional formulation also arises when modelling
uncertainty. Dearden et al. (1998) considered a Gaussian
approximation to the value distribution, and modelled the
uncertainty over the parameters of this approximation using a Normal-Gamma prior. Engel et al. (2005) leveraged
the distributional Bellman equation to define a Gaussian
process over the unknown value function. More recently,
Geist & Pietquin (2010) proposed an alternative solution to
the same problem based on unscented Kalman filters. We
believe much of the analysis we provide here, which deals
with the intrinsic randomness of the environment, can also
be applied to modelling uncertainty.
Our work here is based on a number of foundational results, in particular concerning alternative optimality criteria. Early on, Jaquette (1973) showed that a moment optimality criterion, which imposes a total ordering on distributions, is achievable and defines a stationary optimal policy, echoing the second part of Theorem 1. Sobel (1982)
is usually cited as the first reference to Bellman equations
for the higher moments (but not the distribution) of the return. Chung & Sobel (1987) provides results concerning
the convergence of the distributional Bellman operator in
total variation distance. White (1988) studies â€œnonstandard
MDP criteriaâ€ from the perspective of optimizing the stateaction pair occupancy.
A number of probabilistic frameworks for reinforcement
learning have been proposed in recent years. The planning as inference approach (Toussaint & Storkey, 2006;
Hoffman et al., 2009) embeds the return into a graphical
model, and applies probabilistic inference to determine the

sequence of actions leading to maximal expected reward.
Wang et al. (2008) considered the dual formulation of reinforcement learning, where one optimizes the stationary
distribution subject to constraints given by the transition
function (Puterman, 1994), in particular its relationship to
linear approximation. Related to this dual is the Compress
and Control algorithm Veness et al. (2015), which describes
a value function by learning a return distribution using density models. One of the aims of this work was to address
the question left open by their work of whether one could
be design a practical distributional algorithm based on the
Bellman equation, rather than Monte Carlo estimation.

B. Proofs
Lemma 1 (Partition lemma). Let A1 , A2 , . . . be a set of
random variables describing a partition of â„¦, i.e. Ai (Ï‰) âˆˆ
{0, 1} and for any Ï‰ there is exactly one Ai with Ai (Ï‰) =
1. Let U, V be two random variables. Then
 X
dp U, V â‰¤
dp (Ai U, Ai V ).
i

Proof. We will give the proof for p < âˆ, noting that the
D

D

same applies to p = âˆ. Let Yi := Ai U and Zi := Ai V ,
respectively. First note that


dpp (Ai U, Ai V ) = inf E |Yi âˆ’ Zi |p
Yi ,Zi
h 
i
= inf E E |Yi âˆ’ Zi |p | Ai .
Yi ,Zi

Now, |Ai U âˆ’ Ai V | = 0 whenever Ai = 0. It follows that
we can choose Yi , Zi so that also |Yi âˆ’ Zi |p = 0 whenever
Ai = 0, without increasing the expected norm. Hence
p

dpp (Ai U, Ai V ) =



inf Pr{Ai = 1} E |Yi âˆ’ Zi |p | Ai = 1 .

Yi ,Zi

(8)

Next, we claim that
h
i
X
p
(9)
inf
Pr{Ai = 1} E Ai U âˆ’ Ai V  | Ai = 1
i
U,V
h
i
X
p
â‰¤ inf
Pr{Ai = 1} E |Yi âˆ’ Zi  | Ai = 1 .
i

Y1 ,Y2 ,...
Z1 ,Z2 ,...

Specifically, the left-hand side of the equation is an infimum over all r.v.â€™s whose cumulative distributions are FU
and FV , respectively, while the right-hand side is an infimum over sequences of r.vâ€™s Y1 , Y2 , . . . and Z1 , Z2 , . . .
whose cumulative distributions are FAi U , FAi V , respectively. To prove this upper bound, consider the c.d.f. of
U:
FU (y) = Pr{U â‰¤ y}
X
=
Pr{Ai = 1} Pr{U â‰¤ y | Ai = 1}
Xi
=
Pr{Ai = 1} Pr{Ai U â‰¤ y | Ai = 1}.
i

A Distributional Perspective on Reinforcement Learning

Hence the distribution FU is equivalent, in an almost sure
sense, to one that first picks an element Ai of the partition,
then picks a value for U conditional on the choice Ai . On
D
the other hand, the c.d.f. of Yi = Ai U is
FAi U (y) = Pr{Ai = 1} Pr{Ai U â‰¤ y | Ai = 1}

Lemma 3. T Ï€ : Z â†’ Z is a Î³-contraction in dÂ¯p .
Proof. Consider Z1 , Z2 âˆˆ Z. By definition,
dÂ¯p (T Ï€ Z1 , T Ï€ Z2 ) = sup dp (T Ï€ Z1 (x, a), T Ï€ Z2 (x, a)).
x,a

By the properties of dp , we have

+ Pr{Ai = 0}I [y â‰¥ 0] .

dp (T Ï€ Z1 (x, a), T Ï€ Z2 (x, a))
= dp (R(x, a) + Î³P Ï€ Z1 (x, a), R(x, a) + Î³P Ï€ Z2 (x, a))
â‰¤ Î³dp (P Ï€ Z1 (x, a), P Ï€ Z2 (x, a))

= Pr{Ai = 1} Pr{Ai U â‰¤ y | Ai = 1}

Thus the right-hand side infimum in (9) has the additional
constraint that it must preserve the conditional c.d.fs, in
particular when y â‰¥ 0. Put another way, instead of having the freedom to completely reorder the mapping U :
â„¦ â†’ R, we can only reorder it within each element of the
partition. We now write
dpp (U, V

â‰¤ Î³ sup dp (Z1 (x0 , a0 ), Z2 (x0 , a0 )),
x0 ,a0

where the last line follows from the definition of P Ï€ (see
(4)). Combining with (10) we obtain
dÂ¯p (T Ï€ Z1 , T Ï€ Z2 ) = sup dp (T Ï€ Z1 (x, a), T Ï€ Z2 (x, a))
x,a

) = inf kU âˆ’ V kp
U,V


= inf E |U âˆ’ V |p

â‰¤ Î³ sup dp (Z1 (x0 , a0 ), Z2 (x0 , a0 ))
x0 ,a0

U,V

X

(a)

= inf

U,V

= inf

U,V

X

i

i



Pr{Ai = 1} E |U âˆ’ V |p | Ai = 1



Pr{Ai = 1} E |Ai U âˆ’ Ai V |p | Ai = 1 ,

where (a) follows because A1 , A2 , . . . is a partition. Using
(9), this implies
dpp (U, V

)
X

i
h
p
Pr{Ai = 1} E Ai U âˆ’ Ai V  | Ai = 1
i
U,V
h
i
X
p
Pr{Ai = 1} E Yi âˆ’ Zi  | Ai = 1
â‰¤ inf
= inf

i

Y1 ,Y2 ,...
Z1 ,Z2 ,...

(b)

=

(c)

=

X

h
i
p
inf Pr{Ai = 1} E Yi âˆ’ Zi  | Ai = 1

i Yi ,Zi

X

i

(10)

+ Pr{Ai = 0} Pr{Ai U â‰¤ y | Ai = 0}

= Î³ dÂ¯p (Z1 , Z2 ).
Proposition 1 (Sobel, 1982). Consider two value distributions Z1 , Z2 âˆˆ Z, and write V(Zi ) to be the vector of
variances of Zi . Then
kE T Ï€ Z1 âˆ’ E T Ï€ Z2 kâˆ â‰¤ Î³ kE Z1 âˆ’ E Z2 kâˆ , and

kV(T Ï€ Z1 ) âˆ’ V(T Ï€ Z2 )kâˆ â‰¤ Î³ 2 kVZ1 âˆ’ VZ2 kâˆ .

Proof. The first statement is standard, and its proof follows
from E T Ï€ Z = T Ï€ E Z, where the second T Ï€ denotes the
usual operator over value functions. Now, by independence
of R and P Ï€ Zi :


V(T Ï€ Zi (x, a)) = V R(x, a) + Î³P Ï€ Zi (x, a)
= V(R(x, a)) + Î³ 2 V(P Ï€ Zi (x, a)).

dp (Ai U, Ai V ),

because in (b) the individual components of the sum are
independently minimized; and (c) from (8).
Lemma 2. dÂ¯p is a metric over value distributions.
Proof. The only nontrivial property is the triangle inequality. For any value distribution Y âˆˆ Z, write
dÂ¯p (Z1 , Z2 ) = sup dp (Z1 (x, a), Z2 (x, a))
x,a

And now
kV(T Ï€ Z1 ) âˆ’ V(T Ï€ Z2 )kâˆ


= sup V(T Ï€ Z1 (x, a)) âˆ’ V(T Ï€ Z2 (x, a))
x,a



= sup Î³ 2  [V(P Ï€ Z1 (x, a)) âˆ’ V(P Ï€ Z2 (x, a))] 
x,a


= sup Î³ 2  E [V(Z1 (X 0 , A0 )) âˆ’ V(Z2 (X 0 , A0 ))] 
x,a


â‰¤ sup Î³ 2 V(Z1 (x0 , a0 )) âˆ’ V(Z2 (x0 , a0 ))
x0 ,a0

(a)

â‰¤ sup [dp (Z1 (x, a), Y (x, a)) + dp (Y (x, a), Z2 (x, a))]
x,a

â‰¤ sup dp (Z1 (x, a), Y (x, a)) + sup dp (Y (x, a), Z2 (x, a))
x,a

x,a

= dÂ¯p (Z1 , Y ) + dÂ¯p (Y, Z2 ),
where in (a) we used the triangle inequality for dp .

â‰¤ Î³ 2 kVZ1 âˆ’ VZ2 kâˆ .

Lemma 4. Let Z1 , Z2 âˆˆ Z. Then
kE T Z1 âˆ’ E T Z2 kâˆ â‰¤ Î³ kE Z1 âˆ’ E Z2 kâˆ ,
and in particular E Zk â†’ Qâˆ— exponentially quickly.

A Distributional Perspective on Reinforcement Learning

Proof. The proof follows by linearity of expectation. Write
TD for the distributional operator and TE for the usual operator. Then
kE TD Z1 âˆ’ E TD Z2 kâˆ = kTE E Z1 âˆ’ TE E Z2 kâˆ
â‰¤ Î³ kZ1 âˆ’ Z2 kâˆ .

Theorem 1 (Convergence in the control setting). Let
Zk := T Zkâˆ’1 with Z0 âˆˆ Z. Let X be measurable and
suppose that A is finite. Then
lim

inf

kâ†’âˆ Z âˆ—âˆ— âˆˆZ âˆ—âˆ—

dp (Zk (x, a), Z âˆ—âˆ— (x, a)) = 0

âˆ€x, a.

If X is finite, then Zk converges to Z âˆ—âˆ— uniformly. Furthermore, if there is a total ordering â‰º on Î âˆ— , such that for any
Z âˆ— âˆˆ Z âˆ—,
T Z âˆ— = T Ï€ Z âˆ— with Ï€ âˆˆ GZ âˆ— , Ï€ â‰º Ï€ 0 âˆ€Ï€ 0 âˆˆ GZ âˆ— \ {Ï€},
then T has a unique fixed point Z âˆ— âˆˆ Z âˆ— .
The gist of the proof of Theorem 1 consists in showing that
for every state x, there is a time k after which the greedy
policy w.r.t. Qk is mostly optimal. To clearly expose the
steps involved, we will first assume a unique (and therefore deterministic) optimal policy Ï€ âˆ— , and later return to
the general case; we will denote the optimal action at x by
Ï€ âˆ— (x). For notational convenience, we will write Qk :=
E Zk and Gk := GZk . Let B := 2 supZâˆˆZ kZkâˆ < âˆ
and let k := Î³ k B. We first define the set of states Xk âŠ† X
whose values must be sufficiently close to Qâˆ— at time k:
n
o
âˆ—
Xk := x : Qâˆ— (x, Ï€ âˆ— (x)) âˆ’ max
Q
(x,
a)
>
2
.
k
âˆ—
a6=Ï€ (x)

Indeed, by Lemma 4, we know that after k iterations

(11)

|Qk (x, a) âˆ’ Qâˆ— (x, a)| â‰¤ Î³ k |Q0 (x, a) âˆ’ Qâˆ— (x, a)| â‰¤ k .
For x âˆˆ X , write aâˆ— := Ï€ âˆ— (x). For any a âˆˆ A, we deduce
that
Qk (x, aâˆ— ) âˆ’ Qk (x, a) â‰¥ Qâˆ— (x, aâˆ— ) âˆ’ Qâˆ— (x, a) âˆ’ 2k .
It follows that if x âˆˆ Xk , then also Qk (x, aâˆ— ) > Qk (x, a0 )
for all a0 6= Ï€ âˆ— (x): for these states, the greedy policy
Ï€k (x) := arg maxa Qk (x, a) corresponds to the optimal
policy Ï€ âˆ— .
Lemma 5. For each x âˆˆ X there exists a k such
that, for all k 0 â‰¥ k, x âˆˆ Xk0 , and in particular
arg maxa Qk (x, a) = Ï€ âˆ— (x).
Proof. Because A is finite, the gap
âˆ†(x) := Qâˆ— (x, Ï€ âˆ— (x)) âˆ’ max
Qâˆ— (x, a)
âˆ—
a6=Ï€ (x)

is attained for some strictly positive âˆ†(x) > 0. By definition, there exists a k such that
k = Î³ k B <

âˆ†(x)
,
2

and hence every x âˆˆ X must eventually be in Xk .
This lemma allows us to guarantee the existence of an
iteration k after which sufficiently many states are wellbehaved, in the sense that the greedy policy at those states
chooses the optimal action. We will call these states
â€œsolvedâ€. We in fact require not only these states to be
solved, but also most of their successors, and most of the
successors of those, and so on. We formalize this notion as
follows: fix some Î´ > 0, let Xk,0 := Xk , and define for
i > 0 the set

	
Xk,i := x : x âˆˆ Xk , P (Xkâˆ’1,iâˆ’1 | x, Ï€ âˆ— (x)) â‰¥ 1 âˆ’ Î´ ,

As the following lemma shows, any x is eventually contained in the recursively-defined sets Xk,i , for any i.

Lemma 6. For any i âˆˆ N and any x âˆˆ X , there exists a k
such that for all k 0 â‰¥ k, x âˆˆ Xk0 ,i .

Proof. Fix i and let us suppose that Xk,i â†‘ X . By Lemma
5, this is true for i = 0. We infer that for any probability
measure P on X , P (Xk,i ) â†’ P (X ) = 1. In particular, for
a given x âˆˆ Xk , this implies that
P (Xk,i | x, Ï€ âˆ— (x)) â†’ P (X | x, Ï€ âˆ— (x)) = 1.
Therefore, for any x, there exists a time after which it is
and remains a member of Xk,i+1 , the set of states for which
P (Xkâˆ’1,i | x, Ï€ âˆ— (x)) â‰¥ 1 âˆ’ Î´. We conclude that Xk,i+1 â†‘
X also. The statement follows by induction.
Proof of Theorem 1. The proof is similar to policy
iteration-type results, but requires more care in dealing
with the metric and the possibly infinite state space.
We will write Wk (x) := Zk (x, Ï€k (x)), define W âˆ—
similarly and with some overload of notation write
T Wk (x) := Wk+1 (x) = T Zk (x, Ï€k+1 (x)). Finally, let
Sik (x) := I [x âˆˆ Xk,i ] and SÌ„ik (x) = 1 âˆ’ Sik (x).
Fix i > 0 and x âˆˆ Xk+1,i+1 âŠ† Xk . We begin by using
Lemma 1 to separate the transition from x into a solved
term and an unsolved term:
P Ï€k Wk (x) = Sik Wk (X 0 ) + SÌ„ik Wk (X 0 ),
where X 0 is the random successor from taking action
Ï€k (x) := Ï€ âˆ— (x), and we write Sik = Sik (X 0 ), SÌ„ik =
SÌ„ik (X 0 ) to ease the notation. Similarly,
P Ï€k W âˆ— (x) = Sik W âˆ— (X 0 ) + SÌ„ik W âˆ— (X 0 ).

A Distributional Perspective on Reinforcement Learning

Now

such that
Zk+1 = T Ï€Ì„k Zkâˆ’i+1 .

dp (Wk+1 (x), W âˆ— (x)) = dp (T Wk (x), T W âˆ— (x))
(a)

Now denote by Z âˆ—âˆ— the set of nonstationary optimal policies. If we take any Z âˆ— âˆˆ Z âˆ— , we deduce that

âˆ—

â‰¤ Î³dp (P Ï€k Wk (x), P Ï€ W âˆ— (x))

(b)

â‰¤ Î³dp (Sik Wk (X 0 ), Sik W âˆ— (X 0 ))
+

Î³dp (SÌ„ik Wk (X 0 ), SÌ„ik W âˆ— (X 0 )),

inf

Z âˆ—âˆ— âˆˆZ âˆ—âˆ—

(12)

where in (a) we used Properties P1 and P2 of the Wasserstein metric, and in (b) we separate states for which Ï€k =
Ï€ âˆ— from the rest using Lemma 1 ({Sik , SÌ„ik } form a partition of â„¦). Let Î´i := Pr{X 0 âˆˆ
/ Xk,i } = E{SÌ„ik (X 0 )} =
0
k
kSÌ„i (X )kp . From property P3 of the Wasserstein metric,
we have

dp (T Ï€Ì„k Z âˆ— (x, a), Z âˆ—âˆ— (x, a)) â‰¤

Î´B
,
1âˆ’Î³

since Z âˆ— corresponds to some optimal policy Ï€ âˆ— and Ï€Ì„k is
optimal along most of the trajectories from (x, a). In effect,
T Ï€Ì„k Z âˆ— is close to the value distribution of the nonstationary optimal policy Ï€Ì„k Ï€ âˆ— . Now for this Z âˆ— ,
inf dp (Zk (x, a), Z âˆ—âˆ— (x, a))

Z âˆ—âˆ—

â‰¤ dp (Zk (x, a), T Ï€Ì„k Z âˆ— (x, a))

dp (SÌ„ik Wk (X 0 ), SÌ„ik W âˆ— (X 0 ))
â‰¤ sup dp (SÌ„ik (X 0 )Wk (x0 ), SÌ„ik (X 0 )W âˆ— (x0 ))
x0

+ inf
dp (T Ï€Ì„k Z âˆ— (x, a), Z âˆ—âˆ— (x, a))
âˆ—âˆ—
Z

â‰¤ kSÌ„ik (X 0 )kp sup dp (Wk (x0 ), W âˆ— (x0 ))

â‰¤ dp (T Ï€Ì„k Zkâˆ’i+1 (x, a), T Ï€Ì„k Z âˆ— (x, a)) +

â‰¤ Î´i sup dp (Wk (x0 ), W âˆ— (x0 ))

â‰¤ Î³iB +

x0

x0

â‰¤ Î´i B.
Recall that B < âˆ is the largest attainable kZkâˆ . Since
also Î´i < Î´ by our choice of x âˆˆ Xk+1,i+1 , we can upper
bound the second term in (12) by Î³Î´B. This yields
dp (Wk+1 (x), W âˆ— (x)) â‰¤

Î³dp (Sik Wk (X 0 ), Sik W âˆ— (X 0 )) + Î³Î´B.

By induction on i > 0, we conclude that for x âˆˆ Xk+i,i
and some random state X 00 i steps forward,
dp (Wk+i (x), W âˆ— (x)) â‰¤
Î³ i dp (S0k Wk (X 00 ), S0k W âˆ— (X 00 )) +
â‰¤ Î³iB +

Î´B
.
1âˆ’Î³

Î´B
1âˆ’Î³

Hence for any x âˆˆ X ,  > 0, we can take Î´, i, and finally k
large enough to make dp (Wk (x), W âˆ— (x)) < . The proof
then extends to Zk (x, a) by considering one additional application of T .

We now consider the more general case where there are
multiple optimal policies. We expand the definition of Xk,i
as follows:

	
Xk,i := x âˆˆ Xk : âˆ€Ï€ âˆ— âˆˆ Î âˆ—âˆ—, Eâˆ—P (Xkâˆ’1,iâˆ’1 | x, aâˆ— ) â‰¥ 1âˆ’Î´ ,
a âˆ¼Ï€ (x)

Because there are finitely many actions, Lemma 6 also
holds for this new definition. As before, take x âˆˆ Xk,i , but
now consider the sequence of greedy policies Ï€k , Ï€kâˆ’1 , . . .
selected by successive applications of T , and write
T Ï€Ì„k := T Ï€k T Ï€kâˆ’1 Â· Â· Â· T Ï€kâˆ’i+1 ,

2Î´B
,
1âˆ’Î³

Î´B
1âˆ’Î³

using the same argument as before with the newly-defined
Xk,i . It follows that
inf

Z âˆ—âˆ— âˆˆZ âˆ—âˆ—

dp (Zk (x, a), Z âˆ—âˆ— (x, a)) â†’ 0.

When X is finite, there exists a fixed k after which Xk =
X . The uniform convergence result then follows.

To prove the uniqueness of the fixed point Z âˆ— when T selects its actions according to the ordering â‰º, we note that
for any optimal value distribution Z âˆ— , its set of greedy policies is Î âˆ— . Denote by Ï€ âˆ— the policy coming first in the orâˆ—
dering over Î âˆ— . Then T = T Ï€ , which has a unique fixed
point (Section 3.3).
Proposition 4. That T has a fixed point Z âˆ— = T Z âˆ— is
insufficient to guarantee the convergence of {Zk } to Z âˆ— .
We provide here a sketch of the result. Consider a single
state x1 with two actions, a1 and a2 (Figure 8). The first
action yields a reward of 1/2, while the other either yields
0 or 1 with equal probability, and both actions are optimal.
Now take Î³ = 1/2 and write R0 , R1 , . . . for the received
rewards. Consider a stochastic policy that takes action a2
with probability p. For p = 0, the return is
Zp=0 =

1 1
= 1.
1âˆ’Î³ 2

For p = 1, on the other hand, the return is random and is
given by the following fractional number (in binary):
Zp=1 = R0 .R1 R2 R3 Â· Â· Â· .

A Distributional Perspective on Reinforcement Learning
R = 1/2

R = 0 or 1

a2

a1
x1

Figure 8. A simple example illustrating the effect of a nonstationary policy on the value distribution.

As a result, Zp=1 is uniformly distributed between 0 and 2!
In fact, note that

consider the d1 metric between this distribution P and another distribution Q. The first distribution is

0 w.p. 1/2
P =
1 w.p. 1/2.
In this example, i âˆˆ {1, 2}, P1 = 0, and P2 = 1. Now
consider the distribution with the same support but that puts
probability p on 0:

0 w.p. p
Q=
1 w.p. 1 âˆ’ p.
The distance between P and Q is

Zp=0 = 0.11111 Â· Â· Â· = 1.
For some intermediary value of p, we obtain a different
probability of the different digits, but always putting some
probability mass on all returns in [0, 2].
Now suppose we follow the nonstationary policy that takes
a1 on the first step, then a2 from there on. By inspection, the return will be uniformly distributed on the interval
[1/2, 3/2], which does not correspond to the return under
any value of p. But now we may imagine an operator T
which alternates between a1 and a2 depending on the exact value distribution it is applied to, which would in turn
converge to a nonstationary optimal value distribution.
Lemma 7 (Sample Wasserstein distance). Let {Pi } be a
collection of random variables, I âˆˆ N a random index
independent from {Pi }, and consider the mixture random
variable P = PI . For any random variable Q independent
of I,
dp (P, Q) â‰¤ E dp (Pi , Q),

d1 (P, Q) = |p âˆ’ 21 |.
This is d1 (P, Q) = 12 for p âˆˆ {0, 1}, and strictly less than
1
2 for any other values of p. On the other hand, the corresponding expected distance (after sampling an outcome x1
or x2 with equal probability) is
EI d1 (Pi , Q) = 12 p + 12 (1 âˆ’ p) = 21 .
Hence d1 (P, Q) < EI d1 (Pi , Q) for p âˆˆ (0, 1). This shows
that the bound is in general strict. By inspection, it is clear
that the two gradients are different.
R=0

R=1

x1

x2
Â½

Â½

x

iâˆ¼I

and in general the inequality is strict and
âˆ‡Q dp (PI , Q) 6= E âˆ‡Q dp (Pi , Q).

Figure 9. Example MDP in which the expected sample Wasserstein distance is greater than the Wasserstein distance.

iâˆ¼I

Proof. We prove this using Lemma 1. Let Ai := I [I = i].
We write
dp (P, Q) = dp (PI , Q)
X

X
= dp
A i Pi ,
Ai Q
i
i
X
â‰¤
dp (Ai Pi , Ai Q)
Xi
â‰¤
Pr{I = i}dp (Pi , Q)
i

= EI dP (Pi , Q).

where in the penultimate line we used the independence of
I from Pi and Q to appeal to property P3 of the Wasserstein
metric.
To show that the bound is in general strict, consider the
mixture distribution depicted in Figure 9. We will simply

Proposition 5. Fix some next-state distribution Z and policy Ï€. Consider a parametric value distribution ZÎ¸ , and
and define the Wasserstein loss
LW (Î¸) := dp (ZÎ¸ (x, a), R(x, a) + Î³Z(X 0 , Ï€(X 0 ))).
Let r âˆ¼ R(x, a) and x0 âˆ¼ P (Â· | x, a) and consider the
sample loss
LW (Î¸, r, x0 ) := dp (ZÎ¸ (x, a), r + Î³Z(x0 , Ï€(x0 )).
Its expectation is an upper bound on the loss LW :
LW (Î¸) â‰¤ E LW (Î¸, r, x0 ),
R,P

in general with strict inequality.
The result follows directly from the previous lemma.

A Distributional Perspective on Reinforcement Learning

(a)

(b)

Stochastic Bellman Target
Wasserstein

Monte-Carlo Target
Wasserstein

FZ

d1 (Z â‡¡ , Zâœ“ )

Categorical

Categorical

# Atoms

Return

Figure 10. (a) Wasserstein distance between ground truth distribution Z Ï€ and approximating distributions ZÎ¸ . Varying number of atoms
in approximation, training target, and loss function. (b) Approximate cumulative distributions for five representative states in CliffWalk.

C. Algorithmic Details
While our training regime closely follows that of DQN
(Mnih et al., 2015), we use Adam (Kingma & Ba, 2015)
instead of RMSProp (Tieleman & Hinton, 2012) for gradient rescaling. We also performed some hyperparameter tuning for our final results. Specifically, we evaluated two hyperparameters over our five training games
and choose the values that performed best. The hyperparameter values we considered were VMAX âˆˆ {3, 10, 100}
and adam âˆˆ {1/L, 0.1/L, 0.01/L, 0.001/L, 0.0001/L},
where L = 32 is the minibatch size. We found VMAX = 10
and adam = 0.01/L performed best. We used the same
step-size value as DQN (Î± = 0.00025).
Pseudo-code for the categorical algorithm is given in Algorithm 1. We apply the Bellman update to each atom separately, and then project it into the two nearest atoms in the
original support. Transitions to a terminal state are handled
with Î³t = 0.

D. Comparison of Sampled Wasserstein Loss
and Categorical Projection
Lemma 3 proves that for a fixed policy Ï€ the distributional
Bellman operator is a Î³-contraction in dÂ¯p , and therefore
that T Ï€ will converge in distribution to the true distribution
of returns Z Ï€ . In this section, we empirically validate these
results on the CliffWalk domain shown in Figure 11. The
dynamics of the problem match those given by Sutton &
Barto (1998). We also study the convergence of the distributional Bellman operator under the sampled Wasserstein
loss and the categorical projection (Equation 7) while fol-

r = -1

safe path
optimal path

S

The Cliï¬€

G

r = -100

Figure 11. CliffWalk Environment (Sutton & Barto, 1998).

lowing a policy that tries to take the safe path but has a 10%
chance of taking another action uniformly at random.
We compute a ground-truth distribution of returns Z Ï€ using
10000 Monte-Carlo (MC) rollouts from each state. We then
perform two experiments, approximating the value distribution at each state with our discrete distributions.
In the first experiment, we perform supervised learning using either the Wasserstein loss or categorical projection
(Equation 7) with cross-entropy loss. We use Z Ï€ as the
supervised target and perform 5000 sweeps over all states
to ensure both approaches have converged. In the second
experiment, we use the same loss functions, but the training
target comes from the one-step distributional Bellman operator with sampled transitions. We use VMIN = âˆ’100 and
VMAX = âˆ’1.4 For the sample updates we perform 10 times
as many sweeps over the state space. Fundamentally, these
experiments investigate how well the two training regimes
4

Because there is a small probability of larger negative returns,
some approximation error is unavoidable. However, this effect is
relatively negligible in our experiments.

A Distributional Perspective on Reinforcement Learning

(minimizing the Wasserstein or categorical loss) minimize
the Wasserstein metric under both ideal (supervised target)
and practical (sampled one-step Bellman target) conditions.
In Figure 10a we show the final Wasserstein distance
d1 (Z Ï€ , ZÎ¸ ) between the learned distributions and the
ground-truth distribution as we vary the number of atoms.
The graph shows that the categorical algorithm does indeed
minimize the Wasserstein metric in both the supervised and
sample Bellman setting. It also highlights that minimizing
the Wasserstein loss with stochastic gradient descent is in
general flawed, confirming the intuition given by Proposition 5. In repeat experiments the process converged to
different values of d1 (Z Ï€ , ZÎ¸ ), suggesting the presence of
local minima (more prevalent with fewer atoms).
Figure 10 provides additional insight into why the sampled
Wasserstein distance may perform poorly. Here, we see the
cumulative densities for the approximations learned under
these two losses for five different states along the safe path
in CliffWalk. The Wasserstein has converged to a fixedpoint distribution, but not one that captures the true (Monte
Carlo) distribution very well. By comparison, the categorical algorithm captures the variance of the true distribution
much more accurately.

E. Supplemental Videos and Results

# Games Superior

In Figure 13 we provide links to supplemental videos showing the C51 agent during training on various Atari 2600
games. Figure 12 shows the relative performance of C51
over the course of training. Figure 14 provides a table
of evaluation results, comparing C51 to other state-of-theart agents. Figures 15â€“18 depict particularly interesting
frames.

C51 vs. DQN

C51 vs. HUMAN

DQN vs. HUMAN

Training Frames (millions)

Figure 12. Number of Atari games where an agentâ€™s training performance is greater than a baseline (fully trained DQN & human).
Error bands give standard deviations, and averages are over number of games.

GAMES

Freeway
Pong
Q*Bert
Seaquest
Space Invaders

VIDEO URL

http://youtu.be/97578n9kFIk
http://youtu.be/vIz5P6s80qA
http://youtu.be/v-RbNX4uETw
http://youtu.be/d1yz4PNFUjI
http://youtu.be/yFBwyPuO2Vg

Figure 13. Supplemental videos of C51 during training.

A Distributional Perspective on Reinforcement Learning

GAMES

Alien
Amidar
Assault
Asterix
Asteroids
Atlantis
Bank Heist
Battle Zone
Beam Rider
Berzerk
Bowling
Boxing
Breakout
Centipede
Chopper Command
Crazy Climber
Defender
Demon Attack
Double Dunk
Enduro
Fishing Derby
Freeway
Frostbite
Gopher
Gravitar
H.E.R.O.
Ice Hockey
James Bond
Kangaroo
Krull
Kung-Fu Master
Montezumaâ€™s Revenge
Ms. Pac-Man
Name This Game
Phoenix
Pitfall!
Pong
Private Eye
Q*Bert
River Raid
Road Runner
Robotank
Seaquest
Skiing
Solaris
Space Invaders
Star Gunner
Surround
Tennis
Time Pilot
Tutankham
Up and Down
Venture
Video Pinball
Wizard Of Wor
Yarsâ€™ Revenge
Zaxxon

RANDOM

227.8
5.8
222.4
210.0
719.1
12,850.0
14.2
2,360.0
363.9
123.7
23.1
0.1
1.7
2,090.9
811.0
10,780.5
2,874.5
152.1
-18.6
0.0
-91.7
0.0
65.2
257.6
173.0
1,027.0
-11.2
29.0
52.0
1,598.0
258.5
0.0
307.3
2,292.3
761.4
-229.4
-20.7
24.9
163.9
1,338.5
11.5
2.2
68.4
-17,098.1
1,236.3
148.0
664.0
-10.0
-23.8
3,568.0
11.4
533.4
0.0
16,256.9
563.5
3,092.9
32.5

HUMAN

7,127.7
1,719.5
742.0
8,503.3
47,388.7
29,028.1
753.1
37,187.5
16,926.5
2,630.4
160.7
12.1
30.5
12,017.0
7,387.8
35,829.4
18,688.9
1,971.0
-16.4
860.5
-38.7
29.6
4,334.7
2,412.5
3,351.4
30,826.4
0.9
302.8
3,035.0
2,665.5
22,736.3
4,753.3
6,951.6
8,049.0
7,242.6
6,463.7
14.6
69,571.3
13,455.0
17,118.0
7,845.0
11.9
42,054.7
-4,336.9
12,326.7
1,668.7
10,250.0
6.5
-8.3
5,229.2
167.6
11,693.2
1,187.5
17,667.9
4,756.5
54,576.9
9,173.3

DQN

1,620.0
978.0
4,280.4
4,359.0
1,364.5
279,987.0
455.0
29,900.0
8,627.5
585.6
50.4
88.0
385.5
4,657.7
6,126.0
110,763.0
23,633.0
12,149.4
-6.6
729.0
-4.9
30.8
797.4
8,777.4
473.0
20,437.8
-1.9
768.5
7,259.0
8,422.3
26,059.0
0.0
3,085.6
8,207.8
8,485.2
-286.1
19.5
146.7
13,117.3
7,377.6
39,544.0
63.9
5,860.6
-13,062.3
3,482.8
1,692.3
54,282.0
-5.6
12.2
4,870.0
68.1
9,989.9
163.0
196,760.4
2,704.0
18,098.9
5,363.0

DDQN

3,747.7
1,793.3
5,393.2
17,356.5
734.7
106,056.0
1,030.6
31,700.0
13,772.8
1,225.4
68.1
91.6
418.5
5,409.4
5,809.0
117,282.0
35,338.5
58,044.2
-5.5
1,211.8
15.5
33.3
1,683.3
14,840.8
412.0
20,130.2
-2.7
1,358.0
12,992.0
7,920.5
29,710.0
0.0
2,711.4
10,616.0
12,252.5
-29.9
20.9
129.7
15,088.5
14,884.5
44,127.0
65.1
16,452.7
-9,021.8
3,067.8
2,525.5
60,142.0
-2.9
-22.8
8,339.0
218.4
22,972.2
98.0
309,941.9
7,492.0
11,712.6
10,163.0

DUEL

4,461.4
2,354.5
4,621.0
28,188.0
2,837.7
382,572.0
1,611.9
37,150.0
12,164.0
1,472.6
65.5
99.4
345.3
7,561.4
11,215.0
143,570.0
42,214.0
60,813.3
0.1
2,258.2
46.4
0.0
4,672.8
15,718.4
588.0
20,818.2
0.5
1,312.5
14,854.0
11,451.9
34,294.0
0.0
6,283.5
11,971.1
23,092.2
0.0
21.0
103.0
19,220.3
21,162.6
69,524.0
65.3
50,254.2
-8,857.4
2,250.8
6,427.3
89,238.0
4.4
5.1
11,666.0
211.4
44,939.6
497.0
98,209.5
7,855.0
49,622.1
12,944.0

PRIOR . DUEL .

3,941.0
2,296.8
11,477.0
375,080.0
1,192.7
395,762.0
1,503.1
35,520.0
30,276.5
3,409.0
46.7
98.9
366.0
7,687.5
13,185.0
162,224.0
41,324.5
72,878.6
-12.5
2,306.4
41.3
33.0
7,413.0
104,368.2
238.0
21,036.5
-0.4
812.0
1,792.0
10,374.4
48,375.0
0.0
3,327.3
15,572.5
70,324.3
0.0
20.9
206.0
18,760.3
20,607.6
62,151.0
27.5
931.6
-19,949.9
133.4
15,311.5
125,117.0
1.2
0.0
7,553.0
245.9
33,879.1
48.0
479,197.0
12,352.0
69,618.1
13,886.0

C 51
3,166
1,735
7,203
406,211
1,516
3,692,500
976
28,742
14,074
1,645
81.8
97.8
748
9,646
15,600
179,877
47,092
130,955
2.5
3,454
8.9
33.9
3,965
33,641
440
38,874
-3.5
1,909
12,853
9,735
48,192
0.0
3,415
12,542
17,490
0.0
20.9
15,095
23,784
17,322
55,839
52.3
266,434
-13,901
8,342
5,747
49,095
6.8
23.1
8,329
280
15,612
1,520
949,604
9,300
35,050
10,513

Figure 14. Raw scores across all games, starting with 30 no-op actions. Reference values from Wang et al. (2016).

A Distributional Perspective on Reinforcement Learning

Figure 15. F REEWAY: Agent differentiates action-value distributions under pressure.

Figure 16. Q*B ERT: Top, left and right: Predicting which actions are unrecoverably fatal. Bottom-Left: Value distribution shows steep
consequences for wrong actions. Bottom-Right: The agent has made a huge mistake.

Figure 17. S EAQUEST: Left: Bimodal distribution. Middle: Might hit the fish. Right: Definitely going to hit the fish.

Figure 18. S PACE I NVADERS: Top-Left: Multi-modal distribution with high uncertainty. Top-Right: Subsequent frame, a more certain
demise. Bottom-Left: Clear difference between actions. Bottom-Middle: Uncertain survival. Bottom-Right: Certain success.

