Bidirectional Learning for Time-series Models with Hidden Units

A. Supplementary Materials for Bidirectional Learning for Time-series Models with Hidden
Units
Here, we derive specific learning rules suggested by (27)-(28) as well as those with approximation with (29). These learning
rule can be derived in a way similar to the learning rules (18)-(22) are derived from (17). We also provide some of the
details, which are omitted in the derivation of (18)-(22).
The learning rules for U and Z are derived from (27)-(28) as follows:
U[d] ← U[d] + η log pθ (x[t] |x[<t] , h[<t] )

t−1
X

α[s−1] (h[s] − hH[s] iφ )>

(32)

β [s−1] (h[s] − hH[s] iφ )>

(33)

x[s−δ] (h[s] − hH[s] iφ )>

(34)

h[s−δ] (h[s] − hH[s] iφ )>

(35)

s=`
t−1
X

Z[d] ← Z[d] + η log pθ (x[t] |x[<t] , h[<t] )

s=`

U[δ] ← U[δ] + η log pθ (x[t] |x[<t] , h[<t] )

t−1
X
s=`

Z[δ] ← Z[δ] + η log pθ (x[t] |x[<t] , h[<t] )

t−1
X
s=`

for 1 ≤ δ < d, where hH[s] iφ denotes the expected values of h[s] with respect to the conditional distribution given by the
following pφ :
pφ (h[s] |x[<s] , h[<s] ) =

1
exp(−Eφ (h[s] |x[<s] , h[<s] ))
Z0

(36)

for any binary vectors h[s] , where Z 0 is a normalization factor for the probabilities to sum up to one, and
[s]

[<s]

Eφ (h |x

[<s]

,h

d−1
d−1
X
X
[s−δ] > [δ] [s]
)=−
(x
) U h −
(h[s−δ] )> Z[δ] h[s] − (α[s−1] )> U[d] h[s] − (β [s−1] )> Z[d] h[s] .
δ=1

δ=1

(37)
The energy in (37) can be decomposed into the energy associated with each hidden unit j as follows:
X
[s]
Eφ (h[s] |x[<s] , h[<s] ) =
Eφ,j (hj |x[<s] , h[<s] )

(38)

j∈H

where H denotes the set of hidden units, and
[s]

Eφ,j (hj |x[<s] , h[<s] ) = −

d−1
X

[δ]

[s]

(x[s−δ] )> U:,j hj −

δ=1

d−1
X

[δ]

[s]

[d]

[s]

[d]

[s]

(h[s−δ] )> Z:,j hj − (α[s−1] )> U:,j hj − (β [s−1] )> Z:,j hj ,

δ=1

(39)
where U:,j denotes a column vector corresponding to the j-th column of U, and Z:,j is defined analogously.
Then (36) can be expressed as
pφ (h[s] |x[<s] , h[<s] ) =

Y

[s]

pφ,j (hj |x[<s] , h[<s] ),

(40)

j∈H

where
[s]

exp(−Eφ,j (hj |x[<s] , h[<s] ))

[s]

pφ,j (hj |x[<s] , h[<s] ) =

[s]

[s]

exp(−Eφ,j (hj = 0|x[<s] , h[<s] )) + exp(−Eφ,j (hj = 1|x[<s] , h[<s] ))

(41)

[s]

=

exp(−Eφ,j (hj |x[<s] , h[<s] ))
[s]

1 + exp(−Eφ,j (hj = 1|x[<s] , h[<s] ))

.

(42)

Bidirectional Learning for Time-series Models with Hidden Units

The j-th element of hH[s] iφ is then given by
[s]

[s]

hHj iφ = pφ,j (hj = 1|x[<s] , h[<s] )

(43)

In (32)-(35), the value of hH[s] iφ is computed with the latest values of φ. Let φ[t−1] be the value of φ immediately before
step t. With the recursive computation of (29), the learning rules of (32)-(35) are approximated with the following learning
rules:
U

[d]

←U

[d]

[t]

+ η (1 − γ) log pθ (x |x

[<t]

[<t]

,h

)

t−1
X

γ t−1−s α[s−1] (h[s] − hH[s] iφ[s−1] )>

(44)

γ t−1−s β [s−1] (h[s] − hH[s] iφ[s−1] )>

(45)

γ t−1−s x[s−δ] (h[s] − hH[s] iφ[s−1] )>

(46)

γ t−1−s h[s−δ] (h[s] − hH[s] iφ[s−1] )>

(47)

s=`
t−1
X

Z[d] ← Z[d] + η (1 − γ) log pθ (x[t] |x[<t] , h[<t] )

s=`

U[δ] ← U[δ] + η (1 − γ) log pθ (x[t] |x[<t] , h[<t] )

t−1
X
s=`

Z[δ] ← Z[δ] + η (1 − γ) log pθ (x[t] |x[<t] , h[<t] )

t−1
X
s=`

for 1 ≤ δ < d, where the quantity such as
G0t−1 ≡

t−1
X

γ t−1−s α[s−1] (h[s] − hH[s] iφ[s−1] )>

(48)

s=`

can be computed recursively as
G0t ← γ G0t−1 + (1 − γ) α[t−1] (h[t] − hH[t] iφ[t−1] )> .

(49)
[t]

One may consider real-valued units as well (Dasgupta & Osogami, 2017; Osogami, 2016). For example, each of xi and
[t]
hj may have a Gaussian distribution with the following density:
2 
[t]
[t]
xi − Eθ,i (xi = 1|x[<t] , h[<t] )
=p
exp −
2 σi2
2 π σi2
2 

[t]
[t]
hj − Eφ,j (hj = 1|x[<t] , h[<t] )
1
[t] [<t]
[<t]
exp −
pφ,i (hj |x , h ) = q
,
2 σj2
2 π σj2
[t]
pθ,i (xi |x[<t] , h[<t] )



1

(50)
(51)

[t]

where σi2 and σj2 are variance parameters, Eφ,j is given by (39), and Eθ,i (xi = 1|x[<t] , h[<t] ) is given by
[s]

Eθ,i (xi = 1|x[<s] , h[<s] ) = −bi −

d−1
X
δ=1

[δ]

(x[s−δ] )> W:,i −

d−1
X
δ=1

[δ]

[d]

[d]

(h[s−δ] )> V:,i − (α[s−1] )> W:,i − (β [s−1] )> V:,i . (52)

