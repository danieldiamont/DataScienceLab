Supplementary Material for Stochastic Adaptive Quasi-Newton
Methods for Minimizing Expected Values

Wenbo Gao 1 Donald Goldfarb 1 Chaoxu Zhou 1

]

A. The Empirical Process Framework
We use the framework developed by Goldfarb, Iyengar, and Zhou (2017) for proving convergence of stochastic
algorithms. These results originate in empirical process theory (W. van der Vaart & Wellner, 1996). The problem
to be minimized has the form
min F (x) = Eξ f (x, ξ)
x

We require the following assumptions on F, f for our analysis:
Assumptions:
1. There exist constants L ≥ ` > 0 such that for every x ∈ Rn and every realization of ξ, the Hessian of f with
respect to x satisfies
`I  ∇2x f (x, ξ)  LI
That is, f (x, ξ) is strongly convex for all ξ, with the eigenvalues of ∇2x f (x, ξ) bounded below and above by
` and L, respectively.
2. Fk (x) is standard self-concordant for every possible sampling ξ1 , . . . , ξmk .
3. There exist compact sets D0 and D with x∗ ∈ D and D0 ⊆ D, such that if x0 is chosen in D0 , then for all
possible realizations of the samples ξ1 , . . . , ξmk for every k, the sequence of iterates {xk }∞
k=0 produced by
the algorithm is contained within D. We write D = sup{kx − yk : x, y ∈ D} for the diameter of D.
Furthermore, we assume that the objective values and gradients are bounded:
u = sup sup f (x, ξ) < ∞
ξ

x∈D

l = inf inf f (x, ξ) > −∞
ξ x∈D

γ = sup sup k∇f (x, ξ)k < ∞
ξ

x∈D

The key theorem of this framework is a concentration bound which limits the divergence of Fk (x) from F (x).
δ
Theorem A.1 (Corollary 1, (Goldfarb et al., 2017)). For any δ > 0 and 0 <  < min{D, 2L
}, we have


Dn
mk (δ − 2L)2
P(sup |Fk (x) − F (x)| ≥ δ) ≤ 2nn/2 n exp −

2(u − l)2
x∈D
1

Department of Industrial Engineering and Operations Research, Columbia University. Correspondence to: Chaoxu
Zhou <cz2364@columbia.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright
2017 by the author(s).

Stochastic Adaptive Quasi-Newton Methods

Consequently, if mk ≥ 3, then we have
r
E sup |Fk (x) − F (x)| ≤ C
x∈D

and
E|Fk (x∗k )

∗

− F (x )| ≤ C

r

log mk
mk

log mk
mk

where C is the constant
C = 4(|u| + |l|)n

n/2




√
u−l
D exp −n log √
+ (u − l) n + 1
2 2L
n

We will also use Theorem A.1 to bound gk and Gk . Assumption 1 implies that the partial derivatives
and

∂2F
∂xi xj (x)

∂F
∂xi (x)

are also uniformly bounded for x ∈ D. Hence, we can apply Theorem A.1 to each of the n entries
2

F
of the gradient, and each of the n2 entries ∂x∂i ∂x
of the Hessian. Taking a union bound over the resulting
j
n + n inequalities, we obtain the following concentration inequality for the sampled gradients and Hessians:
∂F
∂xi
2

δ
Corollary A.2. For any δ > 0 and 0 <  < min{D, 2L
},



P(sup kGk (x) − G(x)k > δ or sup kgk (x) − g(x)k > δ) ≤ C1 −n exp −C2 mk (δ − C3 )2
x∈D

x∈D

where C1 , C2 , C3 are constants depending only on F .
Recall the definitions of δk , ρk , αk , and ηk above. In our analysis of stochastic methods, the
q gradients and
Hessians are those of the empirical objective function. That is to say, ρk = gkT Hk gk and δk = dTk Gk dk , where
gk and Gk are the gradient and Hessian of Fk .
We say that a constant c is global if it depends only on the properties of the function F , and is completely
independent of the realization of the samples ξ1 , . . . , ξmk .
For convenience, we state again the main result for adaptive step sizes:
Theorem A.3 (Lemma 4.1, (Gao & Goldfarb, 2016)). Let ρk = ∇f (xk )T Hk ∇f (xk ). If αk is chosen to be
αk = ρδ2k , then
k
f (xk + tk dk ) ≤ f (xk ) − ω(ηk )
where ηk =

ρk
δk

and ω is the function ω(z) = z − log(1 + z).

B. Convergence of SA-GD
The SA-GD method corresponds to Hk = I. More generally, we may assume that the sequence of matrices Hk
has bounded eigenvalues.
λI  Hk  ΛI
for all Hk
(1)
Since this condition is satisfied for L-BFGS, the following results also apply to L-BFGS (with slightly different
constants).
Theorem B.1. Let  > 0 be fixed. At each iteration, we draw m i.i.d samples ξ1 , . . . , ξm , where the size of m
satisfies

2
1−r
log m
≤
2
m
4C
and C is the constant in Theorem A.1 and r = 1 −

2 3/2
√λ `
.
( `+γ)Λ2 L

Then we have

EF (xk+1 ) − F (x∗ ) ≤ 
when k = log(−1 2(u − l))/ log r.

Stochastic Adaptive Quasi-Newton Methods

For matrices Hk with bounded eigenvalues, ηk can readily be bounded in terms of the empirical gradients, and
the sequence {ηk }∞
k=0 is bounded.
Theorem B.2. There exists a global constant Γ =
for all k.

γ
√
`

such that ηk ≤ Γ for all k. Furthermore, ηk ≥

λ
√
kgk k
Λ L

Proof. By Assumption 1 (strong convexity), Gk satisfies `I  Gk  LI. Thus, from the definition of ηk , we have
g T Hk gk
kgk kkHk gk k
1
ηk = q k
≤ √
= √ kgk k
`kHk gk k
`
gkT Hk Gk Hk gk
By Assumption 3, we find that kgk k = kgk (xk )k ≤ γ. Hence, we may take Γ =
ηk = q

gkT Hk gk

≥

gkT Hk Gk Hk gk

γ
√
.
`

We also find that

λ
√ kgk k
Λ L

Lemma B.3. The empirical objective function Fk (x) satisfies
Fk (xk+1 ) − Fk (x∗k ) ≤ r(Fk (xk ) − Fk (x∗k ))
for the global constant r = 1 −

`
(1+Γ)L

< 1.

Proof. Observe that the function ω(z) satisfies ω(z) ≥ 21 (1 + Γ)−1 z 2 for all z ∈ [0, Γ]. Also, recall that the
strongly convex function Fk satisfies kgk (x)k2 ≥ 2`(Fk (x) − Fk (x∗k )). By Theorem A.3 and Theorem B.2, we find
that
1
Fk (xk+1 ) − Fk (x∗k ) ≤ Fk (xk ) − Fk (x∗k ) − ω(ηk ) ≤ Fk (xk ) − Fk (x∗k ) − (1 + Γ)−1 ηk2
2
1
λ2
∗
≤ Fk (xk ) − Fk (xk ) − (1 + Γ)−1 2 kgk k2
Λ L

2
λ2 `
≤ 1−
(Fk (xk ) − Fk (x∗k ))
(1 + Γ)Λ2 L
Thus, we may take r = 1 −

λ2 `
(1+Γ)Λ2 L .

For SA-GD in particular, λ = Λ = 1, so r = 1 −

`
(1+Γ)L .

We are now ready to prove Theorem B.1.
Proof. By Lemma B.3, we calculate that
Fk (xk+1 ) − Fk (x∗k ) ≤ r(Fk (xk ) − Fk (x∗k ))
= r(Fk−1 (xk ) − Fk−1 (x∗k−1 ))
+ r(Fk (xk ) − F (xk ) − Fk−1 (xk ) + F (xk ))
+ r(Fk−1 (x∗k−1 ) − F (x∗ ) − Fk (x∗k ) + F (x∗ ))
≤ r(Fk−1 (xk ) − Fk−1 (x∗k−1 ))
+ r(sup |Fk (x) − F (x)| + sup |Fk−1 (x) − F (x)|)
x∈D

+

r(|Fk (x∗k )

x∈D
∗

− F (x )| + |Fk−1 (x∗k−1 ) − F (x∗ )|)

Stochastic Adaptive Quasi-Newton Methods

By iterating this expansion, we find that
Fk (xk+1 ) − Fk (x∗k ) ≤ rk (F0 (x1 ) − F0 (x∗0 ))
+

k
X
j=1

+

k
X

rj (sup |Fk+1−j (x) − F (x)| + sup |Fk−j (x) − F (x)|)
x∈D

x∈D

rj (|Fk+1−j (x∗k+1−j ) − F (x∗ )| + |Fk−j (x∗k−j ) − F (x∗ )|)

j=1

Decompose Fk (xk+1 ) − Fk (x∗k ) as
Fk (xk+1 ) − Fk (x∗k ) = F (xk+1 ) − F (x∗ ) + [Fk (xk+1 ) − F (xk+1 )] + [F (x∗ ) − Fk (x∗k )]
We can move the terms in square brackets to the right hand side, and upper bound them, to obtain
F (xk+1 ) − F (x∗ ) ≤ rk (F0 (x1 ) − F0 (x∗0 ))
+ sup |Fk (x) − F (x)|
x∈D

+

k
X
j=1

rj (sup |Fk+1−j (x) − F (x)| + sup |Fk−j (x) − F (x)|)
x∈D

(2)

x∈D

+ |Fk (x∗k ) − F (x∗ )|
+

k
X

rj (|Fk+1−j (x∗k+1−j ) − F (x∗ )| + |Fk−j (x∗k−j ) − F (x∗ )|)

j=1

Suppose that we draw a constant number of samples mk = m at each iteration. Taking expectations on both
sides of equation (2) and applying the concentration bound of Theorem A.1, we obtain
r

k

log m X j
r
EF (xk+1 ) − F (x ) ≤ r (u − l) + 2C
m j=0
r
2C
log m
k
≤ r (u − l) +
1−r
m
∗

k

In order to obtain an -optimal solution, we may use sufficiently large samples, and take sufficiently many
iterations, so that

rk (u − l) ≤
2
r
2C
log m

≤
1−r
m
2
This yields the given bounds on m and k in Theorem B.1.
In particular, it suffices to take m = O(−2 log −1 ) and k = O(log −1 ).

C. Convergence of SA-BFGS
Our goal in this section is to prove that SA-BFGS converges superlinearly with probability 1.
Theorem C.1. Suppose that we draw mk samples on the k-th step, where m−1
k converges R-superlinearly to 0.
Then SA-BFGS converges to the optimal solution x∗ almost surely.
Our arguments closely follow the proofs given in (Powell, 1976) and (Griewank & Toint, 1982) for the deterministic
BFGS method.

Stochastic Adaptive Quasi-Newton Methods

Along the way, we will also consider the behavior of SA-BFGS when -optimality suffices, and mk is held constant.
Note that the results preceding Lemma C.10 do not depend on any particular choice of sample sizes mk .
We introduce the following assumption in this section:
4. The Hessian G(x) is Lipschitz continuous with constant LH .
The adaptive step size is known to satisfy the Armijo-Wolfe conditions in the deterministic setting. A similar
property holds for the empirical objective functions.
Theorem C.2 (Theorem 6.2, (Gao & Goldfarb, 2016)). The adaptive step size tk satisfies the Armijo condition
for α = 12 , for the empirical objective function Fk (x).
Recall that the SA-BFGS algorithm performs a BFGS update at step k only if tk satisfies the Wolfe condition.
If tk does not satisfy the Wolfe condition, then we take a SA-GD step instead. In this case, the direction is −gk
and the step size is the adaptive step size for SA-GD.
We use q(j) to denote the the index of the j-th BFGS step, or equivalently, the index at which the j-th BFGS
update is performed. The steps {q(j)}∞
j=1 where we perform BFGS updates will be referred to as update times.
Later on, we will see that if mk grows at a sufficient rate, then all q(j) exist with probability 1.
The following technical lemma is used in the analysis of BFGS; it can also be found in (Byrd et al., 1987) and
(Powell, 1976).
R1
Lemma C.3. Let k = q(j) be an update time. Let Gk = 0 Gk (xk + τ sk )dτ , and let θk denote the angle between
the vectors −gk and sk . Then
1. yk = Gk sk , and sTk yk ≤ Lksk k2 .
2. ksk k ≤ 1` kgk k cos θk
3. If the Wolfe condition is satisfied on step k, then hyk , sk i ≥ (1 − β)h−gk , sk i and ksk k ≥

(1−β)
L kgk k cos θk .

Proof. The first statement follows from the definition yk = gk (xk+1 ) − gk (xk ). Since Gk (x)  LI for all x, we
also have Gk  LI, and hence sTk yk = sTk Gk sk ≤ Lksk k2 .
The second statement follows from the Armijo condition (Theorem C.2) and Taylor’s theorem. Let x be a point
on the line [xk , xk+1 ] with Fk (xk+1 ) = Fk (xk ) + hgk , sk i + 12 sTk Gk (x)sk . Since Fk (xk+1 ) − Fk (xk ) ≤ 21 hgk , sk i, we
have 12 h−gk , sk i ≥ 12 sTk Gk (x)sk ≥ 12 mksk k2 as desired.
The Wolfe condition implies that hyk , sk i = hgk (xk+1 ) − gk (xk ), sk i ≥ (1 − β)h−gk , sk i. Writing h−gk , sk i =
kgk kksk k cos θk , we have Lksk k2 ≥ (1 − β)kgk kksk k cos θk , which gives the last statement.
The next result is the key technical lemma in proving that SA-BFGS converges R-linearly. Its proof is identical
to the deterministic case (Powell, 1976).
Lemma C.4. There exists a global constant c such that
k
Y

kgq(j) k2
≤ ck
h−g
,
s
i
q(j)
q(j)
j=1
Proof. By considering the BFGS update formula, we have
Tr(Bj+1 ) = Tr(Bj ) −

sTj Bj2 sj
yjT yj
+
sTj Bj sj
sTj yj
1/2

Recall from Lemma C.3 that yj = Gj sj . Therefore, writing zj = Gj sj , we have
yjT yj
zjT Gj zj
=
≤L
sTj yj
zjT zj

Stochastic Adaptive Quasi-Newton Methods

where the last inequality follows from Assumption 1. Let c1 = Tr(B0 ) + kL. The BFGS formula implies that
Tr(Bq(k+1) ) ≤ Tr(B0 ) + kL ≤ c1 k, and since Bq(k+1) is positive definite, we also have
k
2
X
sTq(j) Bq(j)
sq(j)
j=1

sTq(j) Bq(j) sq(j)

≤ Tr(B0 ) + kL ≤ c1 k

Observe that sTj Bj2 sj = t2j kgj k2 and that sTj Bj sj = tj h−gj , sj i. By the arithmetic mean-geometric mean (AMGM) inequality,
k
Y
tq(j) kgq(j) k2
≤ ck1
(3)
h−g
,
s
i
q(j)
q(j)
j=1
Next, we use the recursive formula for the determinant:
det(Bj+1 ) =

yjT sj
det(Bj )
sTj Bj sj

Since the Wolfe condition is satisfied, we have
yjT sj = (gj (xj+1 ) − gj (xj ))T sj ≥ (1 − β)h−gj , sj i
Therefore,
det(Bq(k+1) ) ≥ det(B0 )

k
Y
1−β
t
j=1 q(j)

By the AM-GM inequality applied to the eigenvalues of Bq(k+1) , we find that det(Bq(k+1) ) ≤ (c1 k/n)n ≤ ck2 for a
Qk
c1
,
≤ ck2 . Multiplying this together with inequality (3), and taking c = (1−β)c
global constant c2 . Hence, j=1 t1−β
2
q(j)
we find that
k
Y
kgq(j) k2
≤ ck
h−g
,
s
i
q(j)
q(j)
j=1
as desired.
Lemma C.5. At least
Lemma C.4.

1
2k

of the angles θq(1) , . . . , θq(k) satisfy cos2 θq(j) > (`/c)2 , where c is the constant of

Proof. By Lemma C.3, ksj k ≤ 1` kgj k cos θj . Substituting this in Lemma C.4 yields
ck ≥

Hence,

Qk

j=1

k
k
Y
Y
kgq(j) k2
`
1
k+1
≥
=
`
2
2
h−gq(j) , sq(j) i j=1 cos θq(j)
cos θq(j)
j=1
j=1
k
Y

cos2 θq(j) ≥ (`/c)k . It follows that at least 12 k of the angles must satisfy cos2 θq(j) ≥ (`/c)2 .

We can proceed to show that stochastic adaptive BFGS converges R-linearly. The argument proceeds by showing
that if k is not an update time, then SA-BFGS inherits the Q-linear convergence rate of SA-GD, and if k = q(j),
then we can measure the decrement with Lemma C.4.
Lemma C.6. If k is not an update time, then
Fk (xk+1 ) − Fk (x∗k ) ≤ r(Fk (xk ) − Fk (x∗k ))
where r = 1 −

3/2
√`
.
( `+γ)L

Proof. This follows from Lemma B.3 for SA-GD.

Stochastic Adaptive Quasi-Newton Methods

Lemma C.7. Let k = q(j). Then

Fk (xk+1 ) − Fk (x∗k ) ≤ 1 − (1 − β)`L−1 cos2 θk (Fk (xk ) − Fk (x∗k ))
Proof. Since the adaptive step size tk satisfies the Armijo condition for α = 12 , we have
Fk (xk+1 ) − Fk (xk ) ≤

1
1
hgk , sk i = − kgk kksk k cos θk
2
2

Using Lemma C.3, we rewrite ksk k in terms of kgk k, cos θk to obtain
1
Fk (xk+1 ) − Fk (xk ) ≤ − (1 − β)L−1 kgk k2 cos2 θk
2
Since kgk k2 ≥ 2`(Fk (xk ) − Fk (x∗k )), we rearrange to obtain
Fk (xk+1 ) − Fk (x∗k ) ≤ (1 − (1 − β)`L−1 cos2 θk )(Fk (xk ) − Fk (x∗k ))

Theorem C.8. Suppose that we draw samples of size mk at step k, where m−1
converges superlinearly to 0.
k
With probability 1, SA-BFGS converges R-linearly.
Proof. Let ν = max{1 − (1 − β)`L−1 (`/c)2 , r} < 1. Let I1 (k) be the 0-1 indicator variable for the event that
k is a BFGS update time, and let I2 (k) be the indicator for the event that k is a BFGS update time and
cos2 θk ≥ (`/c)2 . Combining Lemma C.6 and Lemma C.7 by using these indicator variables, we have
Fk (xk+1 ) − Fk (x∗k ) ≤ (1 − (1 − β)`L−1 cos2 θk )I1 (k) r1−I1 (k) (Fk (xk ) − Fk (x∗k ))
≤ (1 − (1 − β)`L−1 (`/c)2 )I2 (k) r1−I1 (k) (Fk (xk ) − Fk (x∗k ))
≤ ν I2 (k)+1−I1 (k) (Fk (xk ) − Fk (x∗k ))
For any t ≤ k, let b(t) =
Therefore

Pt

j=0 I1 (j).

Rewritten with indicators, Lemma C.5 states that

Pt

j=0 I2 (j)

≥

k
X

1
(I2 (j) + 1 − I1 (j)) ≥ k − b
2
j=0

Define I3 (k) = I2 (k) + 1 − I1 (k). Iterating the above expansion, we have
Fk (xk+1 ) − Fk (x∗k ) ≤ ν I3 (k) (Fk (xk ) − Fk (x∗k ))
≤ ν I3 (k) (Fk−1 (xk ) − Fk−1 (x∗k−1 ) + (Fk (xk ) − Fk−1 (xk )) + (Fk−1 (x∗k−1 ) − Fk (x∗k ))
≤ν

Pk

i=0 I3 (i)

+

k
X

ν

(F0 (x0 ) − F0 (x∗0 ))

Pk

i=j I3 (i)

+

k
X

[sup |Fj (x) − F (x)| + sup |Fj−1 (x) − F (x)|]
x∈D

j=1

ν

Pk

i=j I3 (i)

x∈D

[|Fj (x∗j ) − F (x∗ )| + |Fj−1 (x∗j−1 ) − F (x∗ )|]

j=1

(F0 (x0 ) − F0 (x∗0 ))
X
+2
ν k−b/2−j (sup |Fj (x) − F (x)| + |Fj (x∗j ) − F (x∗ )|)

≤ν

k−b/2

0≤j≤k−b/2

+2

k
X
j>k−b/2

x∈D

(sup |Fj (x) − F (x)| + |Fj (x∗j ) − F (x∗ )|)
x∈D

1
2 b(t).

Stochastic Adaptive Quasi-Newton Methods

In the last inequality, we have simply split the sums into two sums, one running over the indices 0 ≤ j ≤ k − b/2
and the other over k − b/2 < j ≤ k. Writing the left side as
Fk (xk+1 ) − Fk (x∗k ) = F (xk+1 ) − F (x∗ ) + (Fk (xk+1 ) − F (xk+1 )) + (F (x∗ ) − Fk (x∗k ))
we can move terms to the right to obtain
F (xk+1 ) − F (x∗ ) ≤ ν k−b/2 (F0 (x0 ) − F0 (x∗0 ))
+ sup |Fk (x) − F (x)| + |Fk (x∗k ) − F (x∗ )|
x∈D
X
+2
ν k−b/2−j (sup |Fj (x) − F (x)| + |Fj (x∗j ) − F (x∗ )|)
x∈D

0≤j≤k−b/2

+2

k
X

(sup |Fj (x) − F (x)| + |Fj (x∗j ) − F (x∗ )|)

j>k−b/2

x∈D

Taking expectations, and applying Theorem A.1 on the right, we have
s
X
log mj
∗
k−b/2
k−b/2−j
+ 4C
EF (xk+1 ) − F (x ) ≤ ν
(u − l) + 4C
ν
mj
0≤j≤k−b/2

Our choice of mj satisfies mj = Ω(ν −2j ), so

q

log mj
mj

k
X
j>k−b/2

s

log mj
mj

(4)

√
= O(ν j j). Hence, by bounding each term with a multiple

of ν k−b/2 , we may find a global constant φ, with 1 > φ > ν, and a global constant c3 , such that
EF (xk+1 ) − F (x∗ ) ≤ c3 φk−b/2
Clearly b ≤ k, and thus we find that
EF (xk+1 ) − F (x∗ ) ≤ c3 φk/2
Now, fix any constant ϕ with φ < ϕ < 1. By Markov’s inequality,
P(F (xk ) − F (x∗ ) ≥ ϕk/2 ) ≤
Since

P∞  φ k/2
k=0

ϕ

E(F (xk ) − F (x∗ ))
≤ c3
ϕk/2

 k/2
φ
ϕ

< ∞, the Borel-Cantelli Lemma implies that the sequence of events Ak with
Ak = {F (xk ) − F (x∗ ) > ϕk/2 }

occurs finitely often with probability 1. Therefore, with probability 1, SA-BFGS converges R-linearly.
Before proceeding further, let us digress briefly to consider the behavior of SA-BFGS when we are satisfied with
an -optimal solution, and wish to hold the number of samples constant.
Lemma C.9. Let  > 0. Suppose we draw m i.i.d samples at each step, where m = O(2 (log −1 )3 ). Then
SA-BFGS converges in expectation to an -optimal solution after k steps, where k = O(−1 ).
Proof. Note that equation (4) in the proof of Theorem C.8 holds in the absence of any assumptions on the sample
sizes mk . Suppose that we take mk = m. Then we have
s
s
k
X
X
log mj
log mj
∗
k−b/2
k−b/2−j
EF (xk+1 ) − F (x ) ≤ ν
(u − l) + 4C
+ 4C
ν
mj
mj
0≤j≤k−b/2
j>k−b/2
r


log m
1
≤ ν k/2 (u − l) + 4C
+ k/2
m
1−ν

Stochastic Adaptive Quasi-Newton Methods

Therefore, in order to obtain an -optimal solution from SA-BFGS, we may take

ν k/2 (u − l) ≤
2
r


4C
log m
1

+ k/2 ≤
1−r
m
1−ν
2
Thus, it suffices to take k = log(−1 2(u − l))/ log ν. Substituting this value of k into the second inequality, we
see that it suffices to take m = O(2 (log −1 )3 ).
We now concern ourselves with R-superlinear convergence to the true optimal solution. Henceforth, we assume
that the sample sizes grow so that m−1
k converges R-superlinearly to 0.
P∞
Lemma C.10. We have k=0 ω(ηk ) < ∞ with probability 1. In particular, ηk → 0 almost surely.
Proof. By Theorem A.3, we find that
Fk (xk+1 ) ≤ Fk (xk ) − ω(ηk )
= Fk−1 (xk ) + (Fk (xk ) − Fk−1 (xk )) − ω(ηk )
≤ F0 (x0 ) +

k
X

(Fj (xj ) − Fj−1 (xj )) −

j=1

≤ F0 (x0 ) +

k
X

k
X

sup |Fj (x) − Fj−1 (x)| −

j=1 x∈D

≤ F0 (x0 ) + 2

≤ F0 (x0 ) + 2

k
X

sup |Fj (x) − F (x)| −

P∞

j=1

k
X

k
X
j=0

∞
X

k
X

sup |Fj (x) − F (x)| −

ω(ηj )

j=0

j=1 x∈D

j=1 x∈D

Let Y =

ω(ηj )

j=0

ω(ηj )

ω(ηj )

j=0

supx∈D |Fj (x) − F (x)|. By the monotone convergence theorem and Theorem A.1, we have
s
∞
∞
X
X
log mj
EY =
E sup |Fj (x) − F (x)| ≤ C
mj
x∈D
j=1
j=1

By our choice of mj , the latter sum is finite.PThis implies that P(Y < ∞) = 1. Since Fk (x) is bounded below
∞
on D by Assumption 3, we necessarily have k=0 ω(ηk ) < ∞ whenever Y < ∞. Thus ηk → 0 with probability
1.
Theorem C.11. Fix any β < 1. With probability 1, there exists a finite index k0 such that the Wolfe condition
is satisfied for all k ≥ k0 .
Proof. This follows from Theorem 6.3 in (Gao & Goldfarb, 2016), for any realization of the empirical objective
functions F0 , F1 , . . . such that ηk → 0. By Lemma C.10, the event ηk → 0 occurs with probability 1.
In particular, this implies that with probability 1, there exists a finite time k0 after which every step is a BFGS
step, and BFGS updates are always performed.
P∞
Corollary C.12. With probability 1, we have k=0 kxk − x∗ k < ∞.
∗
k/2
Proof. This follows from Theorem C.8. Let {xk }∞
k=0 be any instance of the algorithm where F (xk ) ≤ F (x )+ϕ
for all k ≥ k0 , for some index k0 . Since F (x) is strongly convex,

kxk − x∗ k ≤
for all k ≥ k0 . Hence

P∞

k=0

2
2
(F (xk ) − F (x∗ )) ≤ ϕk/2
`
`

kxk − x∗ k < ∞. By Theorem C.8, this occurs with probability 1.

Stochastic Adaptive Quasi-Newton Methods

Let us define ek = max{kxk − x∗ k, kxk+1 − x∗ k}. Corollary C.12 implies that

P∞

k=0 ek

< ∞.

Next, we perform a detailed analysis of the evolution of Hk+1 . By applying Corollary A.2, we can use a modified
form of the classical argument ((Griewank & Toint, 1982)) on a path-by-path basis.
−2/5

Corollary C.13. Let σk = mk . By taking δ = σk in Corollary A.2, we can find global constants c4 and
ω < 1 such that
P(sup kGk (x) − G(x)k > σk or sup kgk (x) − g(x)k > σk ) ≤ c4 ω k
x∈D

x∈D

Hence, with probability 1, there exists an index k0 such that for all k ≥ k0 , we have both sup kGk (x)−G(x)k < σk
x∈D

and sup kgk (x) − g(x)k < σk .
x∈D

By construction, {σk } converges to 0 at a R-superlinear rate.
Proof. The first part follows by Corollary A.2. Taking  =

δ
2L+1 ,

our probability bound is

2
C3 2 1/5
P(sup kGk (x) − G(x)k > σk or sup kgk (x) − g(x)k > σk ) ≤ C1 exp( n log mk − C2 (1 −
) mk )
5
2L + 1
x∈D
x∈D
m

1/5

Since logkmk → 0 and mk = Ω(k 5 ) by construction, we can find the desired ω < 1. The second statement then
follows immediately from the Borel-Cantelli Lemma.
P∞
Let Ω denote the space of paths where
k=0 ek < ∞ and for some k0 , supx∈D kGk (x) − G(x)k ≤ σk and
supx∈D kgk (x) − g(x)k ≤ σk for all k ≥ k0 . By Corollary C.12 and Corollary C.13, P(Ω) = 1. Henceforth, we
restrict our analysis to the paths belonging to Ω.
The BFGS algorithm is invariant under a linear change of variables, so without loss of generality, we may assume
that G(x∗ ) = I. This corresponds to the change of variables Fe(y) = F (G(x∗ )−1/2 y), y = G(x∗ )1/2 x. Define two
‘hypothetical’ updates:
∗
T
∗
T
bk+1 = Bk − Bk sk sk Bk + Gk (x )sk sk Gk (x )
B
sTk Bk sk
sTk Gk (x∗ )sk
∗
T
∗
T
ek+1 = Bk − Bk sk sk Bk + G(x )sk sk G(x )
B
sTk Bk sk
sTk G(x∗ )sk

Lemma C.14. We have
ek+1 − Ik2 ≤ kBk − Ik2
kB
F
F
and
e k+1 − Ik2F ≤ kHk − Ik2F
kH
Proof. For brevity, we write s = sk , B = Bk , H = Hk . By a routine calculation (see §4 of (Griewank & Toint,
1982)), we have
"
2
 T 2 2 !#
T 3
T 2
s
B
s
s B s
s
B
s
ek+1 − Ik2F − kBk+1 − Ik2F = − 1 −
+2
−
kB
sT Bs
sT Bs
sT Bs
and
e k+1 − Ik2F − kHk+1 − Ik2F = −
kH

"

sT Hs
1− T
s s

2
+2

sT H 2 s
−
sT s



sT Hs
sT s

2 !#

The Cauchy-Schwarz inequality implies that the latter terms in the brackets are non-positive, which gives the
desired result.

Stochastic Adaptive Quasi-Newton Methods

Lemma C.15. Every path in Ω satisfies
ek+1 k ≤ O(ek + σk )
kBk+1 − B
and
e k+1 k ≤ (kHk − Ik + 1)O(ek + σk )
kHk+1 − H
Proof. We again write s = sk , y = yk , B = Bk , H = Hk for brevity.
bk+1 k, as both updates are performed with sampled gradients, and then
We can bound the difference kBk+1 − B
bk+1 − B
ek+1 }.
use Corollary C.13 to bound kB
Take ∆ = Gk (x∗ )s − y. By Lemma C.3, we can write y = Gk (b
x)s for some x
b on the line segment [xk , xk+1 ], and
we deduce that:
1. `ksk2 ≤ y T s ≤ Lksk2
2. k∆k ≤ LH ek ksk.
3.

yT ∆
sT y

≤ LLH ek

Hence, writing

1
sT y+∆T s

=

1
sT y

−

yT ∆
,
sT y+y T ∆

we have
 T

 yy
(y + ∆)(y + ∆)T 
b


kBk+1 − Bk+1 k =  T −

s y
(y + ∆)T s


T
T
T
 y∆ + ∆y + ∆∆
y T ∆(yy T + y∆T + ∆y T + ∆∆T ) 


= −
+

sT y
sT y + y T ∆
≤ O(ek )
∗

Next, write yb = Gk (x )s and ye = G(x∗ )s. Since our path lies in Ω, we know that kGk (x∗ ) − G(x∗ )k ≤ σk . Let
∆ = yb − ye, so k∆k ≤ σk ksk, and perform the same calculation as above to obtain


 ye∆T + ∆e
y T + ∆∆T
yeT ∆(e
y yeT + ye∆T + ∆e
y T + ∆∆T ) 
b
e


+
kBk+1 − Bk+1 k = −

sT ye
sT ye + yeT ∆
≤ O(σk )
ek+1 k ≤ O(ek + σk ).
Hence, kBk+1 − B
A similar calculation holds for H.
ssT
ssT
−
(y + ∆)T s sT y




sy T
(y + ∆)sT
ysT
s(y + ∆)T
−
H
+
H
−
+
(y + ∆)T s sT y
(y + ∆)T s sT y

b k+1 k = k
kHk+1 − H

+

s(y + ∆)T H(y + ∆)sT
sy T HysT
−
k
T
2
((y + ∆) s)
(sT y)2
ssT
(y+∆)T s

T

− sssT y ≤ O(ek ) and that the other terms are bounded by
b k+1 − H
e k+1 k ≤ O(σk +kHkσk ). Thus, we have kHk+1 − H
e k+1 k ≤
O(kHkek ). The same calculation shows that kH
(kHk − Ik + 1)O(ek + σk ).

It is elementary, though tedious, to verify that

Corollary C.16. By Lemma C.15, Lemma C.14, and the triangle inequality,
ek+1 k + kB
ek+1 − Ik ≤ kBk − Ik + O(ek + σk )
kBk+1 − Ik ≤ kBk+1 − B
and
e k+1 k + kH
e k+1 − Ik ≤ (kHk − Ik + 1)O(ek + σk )
kHk+1 − Ik ≤ kHk+1 − H

Stochastic Adaptive Quasi-Newton Methods

A lemma of Griewank and Toint shows that this forces the convergence of {kBk − Ik} and {kHk − Ik}.
Lemma C.17 (Lemma 3.3 of (Griewank & Toint,
P∞ 1982)). Let {φk } and {δk } be sequences of non-negative
numbers such that φk+1 ≤ (1 + δk )φk + δk and k=1 δk < ∞. Then {φk } converges.
P∞
In our case, we take δk = ek + σk , as k=0 (ek + σk ) < ∞ by Corollary C.12 and Corollary C.13.
Following §4 of (Griewank & Toint, 1982), our previous results yield the Dennis-Moré ((Dennis Jr. & Moré,
1974)) condition:
k(Bk − I)sk k
=0
lim
k→∞
ksk k
It only remains to show that this implies R-superlinear convergence in the stochastic setting. Since I = G(x∗ ),
we have
kBk sk − G(x∗ )sk k = k − gk − G(x∗ )sk + gk (xk+1 ) − gk (xk+1 )k
= kgk (xk+1 ) − gk − G(x∗ )sk − gk (xk+1 )k
Z 1
=k
(Gk (xk + τ sk ) − G(x∗ ))sk dτ − gk (xk+1 )k
0

Z
=k

1
∗

Z

1

(G(xk + τ sk ) − G(x ))sk dτ +
0

(Gk (x + τ sk ) − G(xk + τ sk ))sk dτ − gk (xk+1 )k
0

≥ kgk (xk+1 )k − (LH ek + σk )ksk k
and therefore
therefore

kgk (xk+1 )k
ksk k

→ 0. By Assumption 1, the empirical objective function Fk (x) is strongly convex, and
|kgk (xk+1 ) − gk (x∗ )k − kgk (x∗ ) − g(x∗ )k|
kgk (xk+1 )k
≥
ksk k
kxk+1 − x∗ k + kxk − x∗ k

(5)

k
To complete the analysis, let ak = kgksk+1
, bk = kgk (x∗ ) − g(x∗ )k, and zk = kxk − x∗ k. Our above results show
kk
that ak → 0, and bk ≤ σk tends to 0 R-superlinearly. For convenience, we assume without loss of generality that
{bk } converges Q-superlinearly, by replacing {bk } by the Q-superlinear sequence bounding σk if necessary.

Rearrange inequality (5) to obtain
`zk+1 = `kxk+1 − x∗ k ≤ kgk (xk+1 ) − gk (x∗ )k ≤ ak (zk+1 + zk ) + bk
Eventually, ak < 21 `, as ak → 0. Beyond that point, we find that
zk+1 ≤

2
ak
zk + bk ≤ ak zk + bk
` − ak
`

(6)

Let ck = max{ak zk , bk }. Clearly zk+1 ≤ (2 + 2` )ck , so it suffices to prove that {ck } converges superlinearly. There
are two cases to consider. If ck+1 = ak+1 zk+1 , then
(2 + 2` )ck
ak+1 zk+1
ck+1
=
≤ ak+1
=
ck
ck
ck


2+

2
`


ak+1

and ak → 0. Otherwise, if ck+1 = bk+1 , then
ck+1
bk+1
bk+1
=
≤
ck
ck
bk
and by construction, {bk } converges to 0 superlinearly, so

bk+1
bk

→ 0.

This proves that zk converges R-superlinearly, and completes the proof of Theorem C.1.

Stochastic Adaptive Quasi-Newton Methods

D. Additional Experiments
To complement the numerical experiments for general stochastic optimization problems, we provide additional
results for ERM (empirical risk minimization) problems. We compare all the algorithms in section 8 on ridge
regression problems, that is,
n

minp

w∈R

1X
(yi − Xi β)2 + λkwk22 ,
n i=1

where we set n = 106 , Xi ∼ N (0, Σ(ρ)), Σ(ρ) = (1 − ρ2 )Ip + ρ2 J (here J is the all-ones matrix), β is a fixed p
dimensional vector and λ = 1. We test problems of size p = 100, 500 and ρ = 0, 0.5, 0.9. From the figures, we
ρ = 0, p = 500

ρ = 0, p = 100

8

6

7
5
6

5

log(f(x k ) - f(x * ))

log(f(x k ) - f(x * ))

4

3

2

4

3

2

1

1

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0

-1
0

0.2

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0

-1

-2
0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

0

2

10

20

30

40

50

60

40

50

60

40

50

60

CPU time(s)

CPU time(s)

ρ = 0.5, p = 500
ρ = 0.5, p = 100

8

6
7
5
6

log(f(x k ) - f(x * ))

log(f(x k ) - f(x * ))

4

3

2

1

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0

-1

-2
0

0.2

5

4

3

2

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

1

0

-1
0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

0

2

10

20

30

CPU time(s)

CPU time(s)

ρ = 0.9, p = 500

ρ = 0.9, p = 100

8

5

7

6

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

3

2

5

log(f(x k ) - f(x * ))

log(f(x k ) - f(x * ))

4

1

4

3

2

1

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0
0
-1

-2

-1
0

0.2

0.4

0.6

0.8

1

CPU time(s)

1.2

1.4

1.6

1.8

2

0

10

20

30

CPU time(s)

may draw similar conclusions as to those in section 6 for the methods that use an adaptive step length. One

Stochastic Adaptive Quasi-Newton Methods

interesting finding in this set of experiments is that the robust SGD methods do not work well especially for
p = 500.

References
Byrd, Richard H., Nocedal, Jorge, and Yuan, Ya-Xiang. Global convergence of a class of quasi-Newton methods
on convex problems. Siam. J. Numer. Anal., (5):1171–1190, 1987.
Dennis Jr., John E. and Moré, Jorge J. Characterization of superlinear convergence and its application to
quasi-Newton methods. Math. Comp., 28(106):549–560, 1974.
Gao, Wenbo and Goldfarb, Donald. Quasi-Newton methods: Superlinear convergence without line search for
self-concordant functions. in review. arXiv:1612.06965, 2016.
Goldfarb, Donald, Iyengar, Garud, and Zhou, Chaoxu. Linear convergence of stochastic Frank-Wolfe variants.
In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 1066–1074,
2017.
Griewank, Andreas and Toint, Philippe L. Local convergence analysis for partitioned quasi-Newton updates.
Numer. Math., 39:429–448, 1982.
Powell, Michael J. D. Some global convergence properties of a variable metric algorithm for minimization
without exact line searches. In Cottle, Richard and Lemke, C.E. (eds.), Nonlinear Programming, volume IX.
SIAM-AMS Proceedings, 1976.
W. van der Vaart, Aad. and Wellner, Jon A. Weak Convergence and Empirical Processes. Springer New York,
1996.

