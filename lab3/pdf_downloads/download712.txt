Supplementary Material for Stochastic Adaptive Quasi-Newton
Methods for Minimizing Expected Values

Wenbo Gao 1 Donald Goldfarb 1 Chaoxu Zhou 1

]

A. The Empirical Process Framework
We use the framework developed by Goldfarb, Iyengar, and Zhou (2017) for proving convergence of stochastic
algorithms. These results originate in empirical process theory (W. van der Vaart & Wellner, 1996). The problem
to be minimized has the form
min F (x) = EÎ¾ f (x, Î¾)
x

We require the following assumptions on F, f for our analysis:
Assumptions:
1. There exist constants L â‰¥ ` > 0 such that for every x âˆˆ Rn and every realization of Î¾, the Hessian of f with
respect to x satisfies
`I  âˆ‡2x f (x, Î¾)  LI
That is, f (x, Î¾) is strongly convex for all Î¾, with the eigenvalues of âˆ‡2x f (x, Î¾) bounded below and above by
` and L, respectively.
2. Fk (x) is standard self-concordant for every possible sampling Î¾1 , . . . , Î¾mk .
3. There exist compact sets D0 and D with xâˆ— âˆˆ D and D0 âŠ† D, such that if x0 is chosen in D0 , then for all
possible realizations of the samples Î¾1 , . . . , Î¾mk for every k, the sequence of iterates {xk }âˆž
k=0 produced by
the algorithm is contained within D. We write D = sup{kx âˆ’ yk : x, y âˆˆ D} for the diameter of D.
Furthermore, we assume that the objective values and gradients are bounded:
u = sup sup f (x, Î¾) < âˆž
Î¾

xâˆˆD

l = inf inf f (x, Î¾) > âˆ’âˆž
Î¾ xâˆˆD

Î³ = sup sup kâˆ‡f (x, Î¾)k < âˆž
Î¾

xâˆˆD

The key theorem of this framework is a concentration bound which limits the divergence of Fk (x) from F (x).
Î´
Theorem A.1 (Corollary 1, (Goldfarb et al., 2017)). For any Î´ > 0 and 0 <  < min{D, 2L
}, we have


Dn
mk (Î´ âˆ’ 2L)2
P(sup |Fk (x) âˆ’ F (x)| â‰¥ Î´) â‰¤ 2nn/2 n exp âˆ’

2(u âˆ’ l)2
xâˆˆD
1

Department of Industrial Engineering and Operations Research, Columbia University. Correspondence to: Chaoxu
Zhou <cz2364@columbia.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright
2017 by the author(s).

Stochastic Adaptive Quasi-Newton Methods

Consequently, if mk â‰¥ 3, then we have
r
E sup |Fk (x) âˆ’ F (x)| â‰¤ C
xâˆˆD

and
E|Fk (xâˆ—k )

âˆ—

âˆ’ F (x )| â‰¤ C

r

log mk
mk

log mk
mk

where C is the constant
C = 4(|u| + |l|)n

n/2




âˆš
uâˆ’l
D exp âˆ’n log âˆš
+ (u âˆ’ l) n + 1
2 2L
n

We will also use Theorem A.1 to bound gk and Gk . Assumption 1 implies that the partial derivatives
and

âˆ‚2F
âˆ‚xi xj (x)

âˆ‚F
âˆ‚xi (x)

are also uniformly bounded for x âˆˆ D. Hence, we can apply Theorem A.1 to each of the n entries
2

F
of the gradient, and each of the n2 entries âˆ‚xâˆ‚i âˆ‚x
of the Hessian. Taking a union bound over the resulting
j
n + n inequalities, we obtain the following concentration inequality for the sampled gradients and Hessians:
âˆ‚F
âˆ‚xi
2

Î´
Corollary A.2. For any Î´ > 0 and 0 <  < min{D, 2L
},



P(sup kGk (x) âˆ’ G(x)k > Î´ or sup kgk (x) âˆ’ g(x)k > Î´) â‰¤ C1 âˆ’n exp âˆ’C2 mk (Î´ âˆ’ C3 )2
xâˆˆD

xâˆˆD

where C1 , C2 , C3 are constants depending only on F .
Recall the definitions of Î´k , Ïk , Î±k , and Î·k above. In our analysis of stochastic methods, the
q gradients and
Hessians are those of the empirical objective function. That is to say, Ïk = gkT Hk gk and Î´k = dTk Gk dk , where
gk and Gk are the gradient and Hessian of Fk .
We say that a constant c is global if it depends only on the properties of the function F , and is completely
independent of the realization of the samples Î¾1 , . . . , Î¾mk .
For convenience, we state again the main result for adaptive step sizes:
Theorem A.3 (Lemma 4.1, (Gao & Goldfarb, 2016)). Let Ïk = âˆ‡f (xk )T Hk âˆ‡f (xk ). If Î±k is chosen to be
Î±k = ÏÎ´2k , then
k
f (xk + tk dk ) â‰¤ f (xk ) âˆ’ Ï‰(Î·k )
where Î·k =

Ïk
Î´k

and Ï‰ is the function Ï‰(z) = z âˆ’ log(1 + z).

B. Convergence of SA-GD
The SA-GD method corresponds to Hk = I. More generally, we may assume that the sequence of matrices Hk
has bounded eigenvalues.
Î»I  Hk  Î›I
for all Hk
(1)
Since this condition is satisfied for L-BFGS, the following results also apply to L-BFGS (with slightly different
constants).
Theorem B.1. Let  > 0 be fixed. At each iteration, we draw m i.i.d samples Î¾1 , . . . , Î¾m , where the size of m
satisfies

2
1âˆ’r
log m
â‰¤
2
m
4C
and C is the constant in Theorem A.1 and r = 1 âˆ’

2 3/2
âˆšÎ» `
.
( `+Î³)Î›2 L

Then we have

EF (xk+1 ) âˆ’ F (xâˆ— ) â‰¤ 
when k = log(âˆ’1 2(u âˆ’ l))/ log r.

Stochastic Adaptive Quasi-Newton Methods

For matrices Hk with bounded eigenvalues, Î·k can readily be bounded in terms of the empirical gradients, and
the sequence {Î·k }âˆž
k=0 is bounded.
Theorem B.2. There exists a global constant Î“ =
for all k.

Î³
âˆš
`

such that Î·k â‰¤ Î“ for all k. Furthermore, Î·k â‰¥

Î»
âˆš
kgk k
Î› L

Proof. By Assumption 1 (strong convexity), Gk satisfies `I  Gk  LI. Thus, from the definition of Î·k , we have
g T Hk gk
kgk kkHk gk k
1
Î·k = q k
â‰¤ âˆš
= âˆš kgk k
`kHk gk k
`
gkT Hk Gk Hk gk
By Assumption 3, we find that kgk k = kgk (xk )k â‰¤ Î³. Hence, we may take Î“ =
Î·k = q

gkT Hk gk

â‰¥

gkT Hk Gk Hk gk

Î³
âˆš
.
`

We also find that

Î»
âˆš kgk k
Î› L

Lemma B.3. The empirical objective function Fk (x) satisfies
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ r(Fk (xk ) âˆ’ Fk (xâˆ—k ))
for the global constant r = 1 âˆ’

`
(1+Î“)L

< 1.

Proof. Observe that the function Ï‰(z) satisfies Ï‰(z) â‰¥ 21 (1 + Î“)âˆ’1 z 2 for all z âˆˆ [0, Î“]. Also, recall that the
strongly convex function Fk satisfies kgk (x)k2 â‰¥ 2`(Fk (x) âˆ’ Fk (xâˆ—k )). By Theorem A.3 and Theorem B.2, we find
that
1
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ Fk (xk ) âˆ’ Fk (xâˆ—k ) âˆ’ Ï‰(Î·k ) â‰¤ Fk (xk ) âˆ’ Fk (xâˆ—k ) âˆ’ (1 + Î“)âˆ’1 Î·k2
2
1
Î»2
âˆ—
â‰¤ Fk (xk ) âˆ’ Fk (xk ) âˆ’ (1 + Î“)âˆ’1 2 kgk k2
Î› L

2
Î»2 `
â‰¤ 1âˆ’
(Fk (xk ) âˆ’ Fk (xâˆ—k ))
(1 + Î“)Î›2 L
Thus, we may take r = 1 âˆ’

Î»2 `
(1+Î“)Î›2 L .

For SA-GD in particular, Î» = Î› = 1, so r = 1 âˆ’

`
(1+Î“)L .

We are now ready to prove Theorem B.1.
Proof. By Lemma B.3, we calculate that
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ r(Fk (xk ) âˆ’ Fk (xâˆ—k ))
= r(Fkâˆ’1 (xk ) âˆ’ Fkâˆ’1 (xâˆ—kâˆ’1 ))
+ r(Fk (xk ) âˆ’ F (xk ) âˆ’ Fkâˆ’1 (xk ) + F (xk ))
+ r(Fkâˆ’1 (xâˆ—kâˆ’1 ) âˆ’ F (xâˆ— ) âˆ’ Fk (xâˆ—k ) + F (xâˆ— ))
â‰¤ r(Fkâˆ’1 (xk ) âˆ’ Fkâˆ’1 (xâˆ—kâˆ’1 ))
+ r(sup |Fk (x) âˆ’ F (x)| + sup |Fkâˆ’1 (x) âˆ’ F (x)|)
xâˆˆD

+

r(|Fk (xâˆ—k )

xâˆˆD
âˆ—

âˆ’ F (x )| + |Fkâˆ’1 (xâˆ—kâˆ’1 ) âˆ’ F (xâˆ— )|)

Stochastic Adaptive Quasi-Newton Methods

By iterating this expansion, we find that
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ rk (F0 (x1 ) âˆ’ F0 (xâˆ—0 ))
+

k
X
j=1

+

k
X

rj (sup |Fk+1âˆ’j (x) âˆ’ F (x)| + sup |Fkâˆ’j (x) âˆ’ F (x)|)
xâˆˆD

xâˆˆD

rj (|Fk+1âˆ’j (xâˆ—k+1âˆ’j ) âˆ’ F (xâˆ— )| + |Fkâˆ’j (xâˆ—kâˆ’j ) âˆ’ F (xâˆ— )|)

j=1

Decompose Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) as
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) = F (xk+1 ) âˆ’ F (xâˆ— ) + [Fk (xk+1 ) âˆ’ F (xk+1 )] + [F (xâˆ— ) âˆ’ Fk (xâˆ—k )]
We can move the terms in square brackets to the right hand side, and upper bound them, to obtain
F (xk+1 ) âˆ’ F (xâˆ— ) â‰¤ rk (F0 (x1 ) âˆ’ F0 (xâˆ—0 ))
+ sup |Fk (x) âˆ’ F (x)|
xâˆˆD

+

k
X
j=1

rj (sup |Fk+1âˆ’j (x) âˆ’ F (x)| + sup |Fkâˆ’j (x) âˆ’ F (x)|)
xâˆˆD

(2)

xâˆˆD

+ |Fk (xâˆ—k ) âˆ’ F (xâˆ— )|
+

k
X

rj (|Fk+1âˆ’j (xâˆ—k+1âˆ’j ) âˆ’ F (xâˆ— )| + |Fkâˆ’j (xâˆ—kâˆ’j ) âˆ’ F (xâˆ— )|)

j=1

Suppose that we draw a constant number of samples mk = m at each iteration. Taking expectations on both
sides of equation (2) and applying the concentration bound of Theorem A.1, we obtain
r

k

log m X j
r
EF (xk+1 ) âˆ’ F (x ) â‰¤ r (u âˆ’ l) + 2C
m j=0
r
2C
log m
k
â‰¤ r (u âˆ’ l) +
1âˆ’r
m
âˆ—

k

In order to obtain an -optimal solution, we may use sufficiently large samples, and take sufficiently many
iterations, so that

rk (u âˆ’ l) â‰¤
2
r
2C
log m

â‰¤
1âˆ’r
m
2
This yields the given bounds on m and k in Theorem B.1.
In particular, it suffices to take m = O(âˆ’2 log âˆ’1 ) and k = O(log âˆ’1 ).

C. Convergence of SA-BFGS
Our goal in this section is to prove that SA-BFGS converges superlinearly with probability 1.
Theorem C.1. Suppose that we draw mk samples on the k-th step, where mâˆ’1
k converges R-superlinearly to 0.
Then SA-BFGS converges to the optimal solution xâˆ— almost surely.
Our arguments closely follow the proofs given in (Powell, 1976) and (Griewank & Toint, 1982) for the deterministic
BFGS method.

Stochastic Adaptive Quasi-Newton Methods

Along the way, we will also consider the behavior of SA-BFGS when -optimality suffices, and mk is held constant.
Note that the results preceding Lemma C.10 do not depend on any particular choice of sample sizes mk .
We introduce the following assumption in this section:
4. The Hessian G(x) is Lipschitz continuous with constant LH .
The adaptive step size is known to satisfy the Armijo-Wolfe conditions in the deterministic setting. A similar
property holds for the empirical objective functions.
Theorem C.2 (Theorem 6.2, (Gao & Goldfarb, 2016)). The adaptive step size tk satisfies the Armijo condition
for Î± = 12 , for the empirical objective function Fk (x).
Recall that the SA-BFGS algorithm performs a BFGS update at step k only if tk satisfies the Wolfe condition.
If tk does not satisfy the Wolfe condition, then we take a SA-GD step instead. In this case, the direction is âˆ’gk
and the step size is the adaptive step size for SA-GD.
We use q(j) to denote the the index of the j-th BFGS step, or equivalently, the index at which the j-th BFGS
update is performed. The steps {q(j)}âˆž
j=1 where we perform BFGS updates will be referred to as update times.
Later on, we will see that if mk grows at a sufficient rate, then all q(j) exist with probability 1.
The following technical lemma is used in the analysis of BFGS; it can also be found in (Byrd et al., 1987) and
(Powell, 1976).
R1
Lemma C.3. Let k = q(j) be an update time. Let Gk = 0 Gk (xk + Ï„ sk )dÏ„ , and let Î¸k denote the angle between
the vectors âˆ’gk and sk . Then
1. yk = Gk sk , and sTk yk â‰¤ Lksk k2 .
2. ksk k â‰¤ 1` kgk k cos Î¸k
3. If the Wolfe condition is satisfied on step k, then hyk , sk i â‰¥ (1 âˆ’ Î²)hâˆ’gk , sk i and ksk k â‰¥

(1âˆ’Î²)
L kgk k cos Î¸k .

Proof. The first statement follows from the definition yk = gk (xk+1 ) âˆ’ gk (xk ). Since Gk (x)  LI for all x, we
also have Gk  LI, and hence sTk yk = sTk Gk sk â‰¤ Lksk k2 .
The second statement follows from the Armijo condition (Theorem C.2) and Taylorâ€™s theorem. Let x be a point
on the line [xk , xk+1 ] with Fk (xk+1 ) = Fk (xk ) + hgk , sk i + 12 sTk Gk (x)sk . Since Fk (xk+1 ) âˆ’ Fk (xk ) â‰¤ 21 hgk , sk i, we
have 12 hâˆ’gk , sk i â‰¥ 12 sTk Gk (x)sk â‰¥ 12 mksk k2 as desired.
The Wolfe condition implies that hyk , sk i = hgk (xk+1 ) âˆ’ gk (xk ), sk i â‰¥ (1 âˆ’ Î²)hâˆ’gk , sk i. Writing hâˆ’gk , sk i =
kgk kksk k cos Î¸k , we have Lksk k2 â‰¥ (1 âˆ’ Î²)kgk kksk k cos Î¸k , which gives the last statement.
The next result is the key technical lemma in proving that SA-BFGS converges R-linearly. Its proof is identical
to the deterministic case (Powell, 1976).
Lemma C.4. There exists a global constant c such that
k
Y

kgq(j) k2
â‰¤ ck
hâˆ’g
,
s
i
q(j)
q(j)
j=1
Proof. By considering the BFGS update formula, we have
Tr(Bj+1 ) = Tr(Bj ) âˆ’

sTj Bj2 sj
yjT yj
+
sTj Bj sj
sTj yj
1/2

Recall from Lemma C.3 that yj = Gj sj . Therefore, writing zj = Gj sj , we have
yjT yj
zjT Gj zj
=
â‰¤L
sTj yj
zjT zj

Stochastic Adaptive Quasi-Newton Methods

where the last inequality follows from Assumption 1. Let c1 = Tr(B0 ) + kL. The BFGS formula implies that
Tr(Bq(k+1) ) â‰¤ Tr(B0 ) + kL â‰¤ c1 k, and since Bq(k+1) is positive definite, we also have
k
2
X
sTq(j) Bq(j)
sq(j)
j=1

sTq(j) Bq(j) sq(j)

â‰¤ Tr(B0 ) + kL â‰¤ c1 k

Observe that sTj Bj2 sj = t2j kgj k2 and that sTj Bj sj = tj hâˆ’gj , sj i. By the arithmetic mean-geometric mean (AMGM) inequality,
k
Y
tq(j) kgq(j) k2
â‰¤ ck1
(3)
hâˆ’g
,
s
i
q(j)
q(j)
j=1
Next, we use the recursive formula for the determinant:
det(Bj+1 ) =

yjT sj
det(Bj )
sTj Bj sj

Since the Wolfe condition is satisfied, we have
yjT sj = (gj (xj+1 ) âˆ’ gj (xj ))T sj â‰¥ (1 âˆ’ Î²)hâˆ’gj , sj i
Therefore,
det(Bq(k+1) ) â‰¥ det(B0 )

k
Y
1âˆ’Î²
t
j=1 q(j)

By the AM-GM inequality applied to the eigenvalues of Bq(k+1) , we find that det(Bq(k+1) ) â‰¤ (c1 k/n)n â‰¤ ck2 for a
Qk
c1
,
â‰¤ ck2 . Multiplying this together with inequality (3), and taking c = (1âˆ’Î²)c
global constant c2 . Hence, j=1 t1âˆ’Î²
2
q(j)
we find that
k
Y
kgq(j) k2
â‰¤ ck
hâˆ’g
,
s
i
q(j)
q(j)
j=1
as desired.
Lemma C.5. At least
Lemma C.4.

1
2k

of the angles Î¸q(1) , . . . , Î¸q(k) satisfy cos2 Î¸q(j) > (`/c)2 , where c is the constant of

Proof. By Lemma C.3, ksj k â‰¤ 1` kgj k cos Î¸j . Substituting this in Lemma C.4 yields
ck â‰¥

Hence,

Qk

j=1

k
k
Y
Y
kgq(j) k2
`
1
k+1
â‰¥
=
`
2
2
hâˆ’gq(j) , sq(j) i j=1 cos Î¸q(j)
cos Î¸q(j)
j=1
j=1
k
Y

cos2 Î¸q(j) â‰¥ (`/c)k . It follows that at least 12 k of the angles must satisfy cos2 Î¸q(j) â‰¥ (`/c)2 .

We can proceed to show that stochastic adaptive BFGS converges R-linearly. The argument proceeds by showing
that if k is not an update time, then SA-BFGS inherits the Q-linear convergence rate of SA-GD, and if k = q(j),
then we can measure the decrement with Lemma C.4.
Lemma C.6. If k is not an update time, then
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ r(Fk (xk ) âˆ’ Fk (xâˆ—k ))
where r = 1 âˆ’

3/2
âˆš`
.
( `+Î³)L

Proof. This follows from Lemma B.3 for SA-GD.

Stochastic Adaptive Quasi-Newton Methods

Lemma C.7. Let k = q(j). Then

Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ 1 âˆ’ (1 âˆ’ Î²)`Lâˆ’1 cos2 Î¸k (Fk (xk ) âˆ’ Fk (xâˆ—k ))
Proof. Since the adaptive step size tk satisfies the Armijo condition for Î± = 12 , we have
Fk (xk+1 ) âˆ’ Fk (xk ) â‰¤

1
1
hgk , sk i = âˆ’ kgk kksk k cos Î¸k
2
2

Using Lemma C.3, we rewrite ksk k in terms of kgk k, cos Î¸k to obtain
1
Fk (xk+1 ) âˆ’ Fk (xk ) â‰¤ âˆ’ (1 âˆ’ Î²)Lâˆ’1 kgk k2 cos2 Î¸k
2
Since kgk k2 â‰¥ 2`(Fk (xk ) âˆ’ Fk (xâˆ—k )), we rearrange to obtain
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ (1 âˆ’ (1 âˆ’ Î²)`Lâˆ’1 cos2 Î¸k )(Fk (xk ) âˆ’ Fk (xâˆ—k ))

Theorem C.8. Suppose that we draw samples of size mk at step k, where mâˆ’1
converges superlinearly to 0.
k
With probability 1, SA-BFGS converges R-linearly.
Proof. Let Î½ = max{1 âˆ’ (1 âˆ’ Î²)`Lâˆ’1 (`/c)2 , r} < 1. Let I1 (k) be the 0-1 indicator variable for the event that
k is a BFGS update time, and let I2 (k) be the indicator for the event that k is a BFGS update time and
cos2 Î¸k â‰¥ (`/c)2 . Combining Lemma C.6 and Lemma C.7 by using these indicator variables, we have
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ (1 âˆ’ (1 âˆ’ Î²)`Lâˆ’1 cos2 Î¸k )I1 (k) r1âˆ’I1 (k) (Fk (xk ) âˆ’ Fk (xâˆ—k ))
â‰¤ (1 âˆ’ (1 âˆ’ Î²)`Lâˆ’1 (`/c)2 )I2 (k) r1âˆ’I1 (k) (Fk (xk ) âˆ’ Fk (xâˆ—k ))
â‰¤ Î½ I2 (k)+1âˆ’I1 (k) (Fk (xk ) âˆ’ Fk (xâˆ—k ))
For any t â‰¤ k, let b(t) =
Therefore

Pt

j=0 I1 (j).

Rewritten with indicators, Lemma C.5 states that

Pt

j=0 I2 (j)

â‰¥

k
X

1
(I2 (j) + 1 âˆ’ I1 (j)) â‰¥ k âˆ’ b
2
j=0

Define I3 (k) = I2 (k) + 1 âˆ’ I1 (k). Iterating the above expansion, we have
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) â‰¤ Î½ I3 (k) (Fk (xk ) âˆ’ Fk (xâˆ—k ))
â‰¤ Î½ I3 (k) (Fkâˆ’1 (xk ) âˆ’ Fkâˆ’1 (xâˆ—kâˆ’1 ) + (Fk (xk ) âˆ’ Fkâˆ’1 (xk )) + (Fkâˆ’1 (xâˆ—kâˆ’1 ) âˆ’ Fk (xâˆ—k ))
â‰¤Î½

Pk

i=0 I3 (i)

+

k
X

Î½

(F0 (x0 ) âˆ’ F0 (xâˆ—0 ))

Pk

i=j I3 (i)

+

k
X

[sup |Fj (x) âˆ’ F (x)| + sup |Fjâˆ’1 (x) âˆ’ F (x)|]
xâˆˆD

j=1

Î½

Pk

i=j I3 (i)

xâˆˆD

[|Fj (xâˆ—j ) âˆ’ F (xâˆ— )| + |Fjâˆ’1 (xâˆ—jâˆ’1 ) âˆ’ F (xâˆ— )|]

j=1

(F0 (x0 ) âˆ’ F0 (xâˆ—0 ))
X
+2
Î½ kâˆ’b/2âˆ’j (sup |Fj (x) âˆ’ F (x)| + |Fj (xâˆ—j ) âˆ’ F (xâˆ— )|)

â‰¤Î½

kâˆ’b/2

0â‰¤jâ‰¤kâˆ’b/2

+2

k
X
j>kâˆ’b/2

xâˆˆD

(sup |Fj (x) âˆ’ F (x)| + |Fj (xâˆ—j ) âˆ’ F (xâˆ— )|)
xâˆˆD

1
2 b(t).

Stochastic Adaptive Quasi-Newton Methods

In the last inequality, we have simply split the sums into two sums, one running over the indices 0 â‰¤ j â‰¤ k âˆ’ b/2
and the other over k âˆ’ b/2 < j â‰¤ k. Writing the left side as
Fk (xk+1 ) âˆ’ Fk (xâˆ—k ) = F (xk+1 ) âˆ’ F (xâˆ— ) + (Fk (xk+1 ) âˆ’ F (xk+1 )) + (F (xâˆ— ) âˆ’ Fk (xâˆ—k ))
we can move terms to the right to obtain
F (xk+1 ) âˆ’ F (xâˆ— ) â‰¤ Î½ kâˆ’b/2 (F0 (x0 ) âˆ’ F0 (xâˆ—0 ))
+ sup |Fk (x) âˆ’ F (x)| + |Fk (xâˆ—k ) âˆ’ F (xâˆ— )|
xâˆˆD
X
+2
Î½ kâˆ’b/2âˆ’j (sup |Fj (x) âˆ’ F (x)| + |Fj (xâˆ—j ) âˆ’ F (xâˆ— )|)
xâˆˆD

0â‰¤jâ‰¤kâˆ’b/2

+2

k
X

(sup |Fj (x) âˆ’ F (x)| + |Fj (xâˆ—j ) âˆ’ F (xâˆ— )|)

j>kâˆ’b/2

xâˆˆD

Taking expectations, and applying Theorem A.1 on the right, we have
s
X
log mj
âˆ—
kâˆ’b/2
kâˆ’b/2âˆ’j
+ 4C
EF (xk+1 ) âˆ’ F (x ) â‰¤ Î½
(u âˆ’ l) + 4C
Î½
mj
0â‰¤jâ‰¤kâˆ’b/2

Our choice of mj satisfies mj = â„¦(Î½ âˆ’2j ), so

q

log mj
mj

k
X
j>kâˆ’b/2

s

log mj
mj

(4)

âˆš
= O(Î½ j j). Hence, by bounding each term with a multiple

of Î½ kâˆ’b/2 , we may find a global constant Ï†, with 1 > Ï† > Î½, and a global constant c3 , such that
EF (xk+1 ) âˆ’ F (xâˆ— ) â‰¤ c3 Ï†kâˆ’b/2
Clearly b â‰¤ k, and thus we find that
EF (xk+1 ) âˆ’ F (xâˆ— ) â‰¤ c3 Ï†k/2
Now, fix any constant Ï• with Ï† < Ï• < 1. By Markovâ€™s inequality,
P(F (xk ) âˆ’ F (xâˆ— ) â‰¥ Ï•k/2 ) â‰¤
Since

Pâˆž  Ï† k/2
k=0

Ï•

E(F (xk ) âˆ’ F (xâˆ— ))
â‰¤ c3
Ï•k/2

 k/2
Ï†
Ï•

< âˆž, the Borel-Cantelli Lemma implies that the sequence of events Ak with
Ak = {F (xk ) âˆ’ F (xâˆ— ) > Ï•k/2 }

occurs finitely often with probability 1. Therefore, with probability 1, SA-BFGS converges R-linearly.
Before proceeding further, let us digress briefly to consider the behavior of SA-BFGS when we are satisfied with
an -optimal solution, and wish to hold the number of samples constant.
Lemma C.9. Let  > 0. Suppose we draw m i.i.d samples at each step, where m = O(2 (log âˆ’1 )3 ). Then
SA-BFGS converges in expectation to an -optimal solution after k steps, where k = O(âˆ’1 ).
Proof. Note that equation (4) in the proof of Theorem C.8 holds in the absence of any assumptions on the sample
sizes mk . Suppose that we take mk = m. Then we have
s
s
k
X
X
log mj
log mj
âˆ—
kâˆ’b/2
kâˆ’b/2âˆ’j
EF (xk+1 ) âˆ’ F (x ) â‰¤ Î½
(u âˆ’ l) + 4C
+ 4C
Î½
mj
mj
0â‰¤jâ‰¤kâˆ’b/2
j>kâˆ’b/2
r


log m
1
â‰¤ Î½ k/2 (u âˆ’ l) + 4C
+ k/2
m
1âˆ’Î½

Stochastic Adaptive Quasi-Newton Methods

Therefore, in order to obtain an -optimal solution from SA-BFGS, we may take

Î½ k/2 (u âˆ’ l) â‰¤
2
r


4C
log m
1

+ k/2 â‰¤
1âˆ’r
m
1âˆ’Î½
2
Thus, it suffices to take k = log(âˆ’1 2(u âˆ’ l))/ log Î½. Substituting this value of k into the second inequality, we
see that it suffices to take m = O(2 (log âˆ’1 )3 ).
We now concern ourselves with R-superlinear convergence to the true optimal solution. Henceforth, we assume
that the sample sizes grow so that mâˆ’1
k converges R-superlinearly to 0.
Pâˆž
Lemma C.10. We have k=0 Ï‰(Î·k ) < âˆž with probability 1. In particular, Î·k â†’ 0 almost surely.
Proof. By Theorem A.3, we find that
Fk (xk+1 ) â‰¤ Fk (xk ) âˆ’ Ï‰(Î·k )
= Fkâˆ’1 (xk ) + (Fk (xk ) âˆ’ Fkâˆ’1 (xk )) âˆ’ Ï‰(Î·k )
â‰¤ F0 (x0 ) +

k
X

(Fj (xj ) âˆ’ Fjâˆ’1 (xj )) âˆ’

j=1

â‰¤ F0 (x0 ) +

k
X

k
X

sup |Fj (x) âˆ’ Fjâˆ’1 (x)| âˆ’

j=1 xâˆˆD

â‰¤ F0 (x0 ) + 2

â‰¤ F0 (x0 ) + 2

k
X

sup |Fj (x) âˆ’ F (x)| âˆ’

Pâˆž

j=1

k
X

k
X
j=0

âˆž
X

k
X

sup |Fj (x) âˆ’ F (x)| âˆ’

Ï‰(Î·j )

j=0

j=1 xâˆˆD

j=1 xâˆˆD

Let Y =

Ï‰(Î·j )

j=0

Ï‰(Î·j )

Ï‰(Î·j )

j=0

supxâˆˆD |Fj (x) âˆ’ F (x)|. By the monotone convergence theorem and Theorem A.1, we have
s
âˆž
âˆž
X
X
log mj
EY =
E sup |Fj (x) âˆ’ F (x)| â‰¤ C
mj
xâˆˆD
j=1
j=1

By our choice of mj , the latter sum is finite.PThis implies that P(Y < âˆž) = 1. Since Fk (x) is bounded below
âˆž
on D by Assumption 3, we necessarily have k=0 Ï‰(Î·k ) < âˆž whenever Y < âˆž. Thus Î·k â†’ 0 with probability
1.
Theorem C.11. Fix any Î² < 1. With probability 1, there exists a finite index k0 such that the Wolfe condition
is satisfied for all k â‰¥ k0 .
Proof. This follows from Theorem 6.3 in (Gao & Goldfarb, 2016), for any realization of the empirical objective
functions F0 , F1 , . . . such that Î·k â†’ 0. By Lemma C.10, the event Î·k â†’ 0 occurs with probability 1.
In particular, this implies that with probability 1, there exists a finite time k0 after which every step is a BFGS
step, and BFGS updates are always performed.
Pâˆž
Corollary C.12. With probability 1, we have k=0 kxk âˆ’ xâˆ— k < âˆž.
âˆ—
k/2
Proof. This follows from Theorem C.8. Let {xk }âˆž
k=0 be any instance of the algorithm where F (xk ) â‰¤ F (x )+Ï•
for all k â‰¥ k0 , for some index k0 . Since F (x) is strongly convex,

kxk âˆ’ xâˆ— k â‰¤
for all k â‰¥ k0 . Hence

Pâˆž

k=0

2
2
(F (xk ) âˆ’ F (xâˆ— )) â‰¤ Ï•k/2
`
`

kxk âˆ’ xâˆ— k < âˆž. By Theorem C.8, this occurs with probability 1.

Stochastic Adaptive Quasi-Newton Methods

Let us define ek = max{kxk âˆ’ xâˆ— k, kxk+1 âˆ’ xâˆ— k}. Corollary C.12 implies that

Pâˆž

k=0 ek

< âˆž.

Next, we perform a detailed analysis of the evolution of Hk+1 . By applying Corollary A.2, we can use a modified
form of the classical argument ((Griewank & Toint, 1982)) on a path-by-path basis.
âˆ’2/5

Corollary C.13. Let Ïƒk = mk . By taking Î´ = Ïƒk in Corollary A.2, we can find global constants c4 and
Ï‰ < 1 such that
P(sup kGk (x) âˆ’ G(x)k > Ïƒk or sup kgk (x) âˆ’ g(x)k > Ïƒk ) â‰¤ c4 Ï‰ k
xâˆˆD

xâˆˆD

Hence, with probability 1, there exists an index k0 such that for all k â‰¥ k0 , we have both sup kGk (x)âˆ’G(x)k < Ïƒk
xâˆˆD

and sup kgk (x) âˆ’ g(x)k < Ïƒk .
xâˆˆD

By construction, {Ïƒk } converges to 0 at a R-superlinear rate.
Proof. The first part follows by Corollary A.2. Taking  =

Î´
2L+1 ,

our probability bound is

2
C3 2 1/5
P(sup kGk (x) âˆ’ G(x)k > Ïƒk or sup kgk (x) âˆ’ g(x)k > Ïƒk ) â‰¤ C1 exp( n log mk âˆ’ C2 (1 âˆ’
) mk )
5
2L + 1
xâˆˆD
xâˆˆD
m

1/5

Since logkmk â†’ 0 and mk = â„¦(k 5 ) by construction, we can find the desired Ï‰ < 1. The second statement then
follows immediately from the Borel-Cantelli Lemma.
Pâˆž
Let â„¦ denote the space of paths where
k=0 ek < âˆž and for some k0 , supxâˆˆD kGk (x) âˆ’ G(x)k â‰¤ Ïƒk and
supxâˆˆD kgk (x) âˆ’ g(x)k â‰¤ Ïƒk for all k â‰¥ k0 . By Corollary C.12 and Corollary C.13, P(â„¦) = 1. Henceforth, we
restrict our analysis to the paths belonging to â„¦.
The BFGS algorithm is invariant under a linear change of variables, so without loss of generality, we may assume
that G(xâˆ— ) = I. This corresponds to the change of variables Fe(y) = F (G(xâˆ— )âˆ’1/2 y), y = G(xâˆ— )1/2 x. Define two
â€˜hypotheticalâ€™ updates:
âˆ—
T
âˆ—
T
bk+1 = Bk âˆ’ Bk sk sk Bk + Gk (x )sk sk Gk (x )
B
sTk Bk sk
sTk Gk (xâˆ— )sk
âˆ—
T
âˆ—
T
ek+1 = Bk âˆ’ Bk sk sk Bk + G(x )sk sk G(x )
B
sTk Bk sk
sTk G(xâˆ— )sk

Lemma C.14. We have
ek+1 âˆ’ Ik2 â‰¤ kBk âˆ’ Ik2
kB
F
F
and
e k+1 âˆ’ Ik2F â‰¤ kHk âˆ’ Ik2F
kH
Proof. For brevity, we write s = sk , B = Bk , H = Hk . By a routine calculation (see Â§4 of (Griewank & Toint,
1982)), we have
"
2
 T 2 2 !#
T 3
T 2
s
B
s
s B s
s
B
s
ek+1 âˆ’ Ik2F âˆ’ kBk+1 âˆ’ Ik2F = âˆ’ 1 âˆ’
+2
âˆ’
kB
sT Bs
sT Bs
sT Bs
and
e k+1 âˆ’ Ik2F âˆ’ kHk+1 âˆ’ Ik2F = âˆ’
kH

"

sT Hs
1âˆ’ T
s s

2
+2

sT H 2 s
âˆ’
sT s



sT Hs
sT s

2 !#

The Cauchy-Schwarz inequality implies that the latter terms in the brackets are non-positive, which gives the
desired result.

Stochastic Adaptive Quasi-Newton Methods

Lemma C.15. Every path in â„¦ satisfies
ek+1 k â‰¤ O(ek + Ïƒk )
kBk+1 âˆ’ B
and
e k+1 k â‰¤ (kHk âˆ’ Ik + 1)O(ek + Ïƒk )
kHk+1 âˆ’ H
Proof. We again write s = sk , y = yk , B = Bk , H = Hk for brevity.
bk+1 k, as both updates are performed with sampled gradients, and then
We can bound the difference kBk+1 âˆ’ B
bk+1 âˆ’ B
ek+1 }.
use Corollary C.13 to bound kB
Take âˆ† = Gk (xâˆ— )s âˆ’ y. By Lemma C.3, we can write y = Gk (b
x)s for some x
b on the line segment [xk , xk+1 ], and
we deduce that:
1. `ksk2 â‰¤ y T s â‰¤ Lksk2
2. kâˆ†k â‰¤ LH ek ksk.
3.

yT âˆ†
sT y

â‰¤ LLH ek

Hence, writing

1
sT y+âˆ†T s

=

1
sT y

âˆ’

yT âˆ†
,
sT y+y T âˆ†

we have
 T

 yy
(y + âˆ†)(y + âˆ†)T 
b


kBk+1 âˆ’ Bk+1 k =  T âˆ’

s y
(y + âˆ†)T s


T
T
T
 yâˆ† + âˆ†y + âˆ†âˆ†
y T âˆ†(yy T + yâˆ†T + âˆ†y T + âˆ†âˆ†T ) 


= âˆ’
+

sT y
sT y + y T âˆ†
â‰¤ O(ek )
âˆ—

Next, write yb = Gk (x )s and ye = G(xâˆ— )s. Since our path lies in â„¦, we know that kGk (xâˆ— ) âˆ’ G(xâˆ— )k â‰¤ Ïƒk . Let
âˆ† = yb âˆ’ ye, so kâˆ†k â‰¤ Ïƒk ksk, and perform the same calculation as above to obtain


 yeâˆ†T + âˆ†e
y T + âˆ†âˆ†T
yeT âˆ†(e
y yeT + yeâˆ†T + âˆ†e
y T + âˆ†âˆ†T ) 
b
e


+
kBk+1 âˆ’ Bk+1 k = âˆ’

sT ye
sT ye + yeT âˆ†
â‰¤ O(Ïƒk )
ek+1 k â‰¤ O(ek + Ïƒk ).
Hence, kBk+1 âˆ’ B
A similar calculation holds for H.
ssT
ssT
âˆ’
(y + âˆ†)T s sT y




sy T
(y + âˆ†)sT
ysT
s(y + âˆ†)T
âˆ’
H
+
H
âˆ’
+
(y + âˆ†)T s sT y
(y + âˆ†)T s sT y

b k+1 k = k
kHk+1 âˆ’ H

+

s(y + âˆ†)T H(y + âˆ†)sT
sy T HysT
âˆ’
k
T
2
((y + âˆ†) s)
(sT y)2
ssT
(y+âˆ†)T s

T

âˆ’ sssT y â‰¤ O(ek ) and that the other terms are bounded by
b k+1 âˆ’ H
e k+1 k â‰¤ O(Ïƒk +kHkÏƒk ). Thus, we have kHk+1 âˆ’ H
e k+1 k â‰¤
O(kHkek ). The same calculation shows that kH
(kHk âˆ’ Ik + 1)O(ek + Ïƒk ).

It is elementary, though tedious, to verify that

Corollary C.16. By Lemma C.15, Lemma C.14, and the triangle inequality,
ek+1 k + kB
ek+1 âˆ’ Ik â‰¤ kBk âˆ’ Ik + O(ek + Ïƒk )
kBk+1 âˆ’ Ik â‰¤ kBk+1 âˆ’ B
and
e k+1 k + kH
e k+1 âˆ’ Ik â‰¤ (kHk âˆ’ Ik + 1)O(ek + Ïƒk )
kHk+1 âˆ’ Ik â‰¤ kHk+1 âˆ’ H

Stochastic Adaptive Quasi-Newton Methods

A lemma of Griewank and Toint shows that this forces the convergence of {kBk âˆ’ Ik} and {kHk âˆ’ Ik}.
Lemma C.17 (Lemma 3.3 of (Griewank & Toint,
Pâˆž 1982)). Let {Ï†k } and {Î´k } be sequences of non-negative
numbers such that Ï†k+1 â‰¤ (1 + Î´k )Ï†k + Î´k and k=1 Î´k < âˆž. Then {Ï†k } converges.
Pâˆž
In our case, we take Î´k = ek + Ïƒk , as k=0 (ek + Ïƒk ) < âˆž by Corollary C.12 and Corollary C.13.
Following Â§4 of (Griewank & Toint, 1982), our previous results yield the Dennis-MoreÌ ((Dennis Jr. & MoreÌ,
1974)) condition:
k(Bk âˆ’ I)sk k
=0
lim
kâ†’âˆž
ksk k
It only remains to show that this implies R-superlinear convergence in the stochastic setting. Since I = G(xâˆ— ),
we have
kBk sk âˆ’ G(xâˆ— )sk k = k âˆ’ gk âˆ’ G(xâˆ— )sk + gk (xk+1 ) âˆ’ gk (xk+1 )k
= kgk (xk+1 ) âˆ’ gk âˆ’ G(xâˆ— )sk âˆ’ gk (xk+1 )k
Z 1
=k
(Gk (xk + Ï„ sk ) âˆ’ G(xâˆ— ))sk dÏ„ âˆ’ gk (xk+1 )k
0

Z
=k

1
âˆ—

Z

1

(G(xk + Ï„ sk ) âˆ’ G(x ))sk dÏ„ +
0

(Gk (x + Ï„ sk ) âˆ’ G(xk + Ï„ sk ))sk dÏ„ âˆ’ gk (xk+1 )k
0

â‰¥ kgk (xk+1 )k âˆ’ (LH ek + Ïƒk )ksk k
and therefore
therefore

kgk (xk+1 )k
ksk k

â†’ 0. By Assumption 1, the empirical objective function Fk (x) is strongly convex, and
|kgk (xk+1 ) âˆ’ gk (xâˆ— )k âˆ’ kgk (xâˆ— ) âˆ’ g(xâˆ— )k|
kgk (xk+1 )k
â‰¥
ksk k
kxk+1 âˆ’ xâˆ— k + kxk âˆ’ xâˆ— k

(5)

k
To complete the analysis, let ak = kgksk+1
, bk = kgk (xâˆ— ) âˆ’ g(xâˆ— )k, and zk = kxk âˆ’ xâˆ— k. Our above results show
kk
that ak â†’ 0, and bk â‰¤ Ïƒk tends to 0 R-superlinearly. For convenience, we assume without loss of generality that
{bk } converges Q-superlinearly, by replacing {bk } by the Q-superlinear sequence bounding Ïƒk if necessary.

Rearrange inequality (5) to obtain
`zk+1 = `kxk+1 âˆ’ xâˆ— k â‰¤ kgk (xk+1 ) âˆ’ gk (xâˆ— )k â‰¤ ak (zk+1 + zk ) + bk
Eventually, ak < 21 `, as ak â†’ 0. Beyond that point, we find that
zk+1 â‰¤

2
ak
zk + bk â‰¤ ak zk + bk
` âˆ’ ak
`

(6)

Let ck = max{ak zk , bk }. Clearly zk+1 â‰¤ (2 + 2` )ck , so it suffices to prove that {ck } converges superlinearly. There
are two cases to consider. If ck+1 = ak+1 zk+1 , then
(2 + 2` )ck
ak+1 zk+1
ck+1
=
â‰¤ ak+1
=
ck
ck
ck


2+

2
`


ak+1

and ak â†’ 0. Otherwise, if ck+1 = bk+1 , then
ck+1
bk+1
bk+1
=
â‰¤
ck
ck
bk
and by construction, {bk } converges to 0 superlinearly, so

bk+1
bk

â†’ 0.

This proves that zk converges R-superlinearly, and completes the proof of Theorem C.1.

Stochastic Adaptive Quasi-Newton Methods

D. Additional Experiments
To complement the numerical experiments for general stochastic optimization problems, we provide additional
results for ERM (empirical risk minimization) problems. We compare all the algorithms in section 8 on ridge
regression problems, that is,
n

minp

wâˆˆR

1X
(yi âˆ’ Xi Î²)2 + Î»kwk22 ,
n i=1

where we set n = 106 , Xi âˆ¼ N (0, Î£(Ï)), Î£(Ï) = (1 âˆ’ Ï2 )Ip + Ï2 J (here J is the all-ones matrix), Î² is a fixed p
dimensional vector and Î» = 1. We test problems of size p = 100, 500 and Ï = 0, 0.5, 0.9. From the figures, we
Ï = 0, p = 500

Ï = 0, p = 100

8

6

7
5
6

5

log(f(x k ) - f(x * ))

log(f(x k ) - f(x * ))

4

3

2

4

3

2

1

1

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0

-1
0

0.2

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0

-1

-2
0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

0

2

10

20

30

40

50

60

40

50

60

40

50

60

CPU time(s)

CPU time(s)

Ï = 0.5, p = 500
Ï = 0.5, p = 100

8

6
7
5
6

log(f(x k ) - f(x * ))

log(f(x k ) - f(x * ))

4

3

2

1

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0

-1

-2
0

0.2

5

4

3

2

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

1

0

-1
0.4

0.6

0.8

1

1.2

1.4

1.6

1.8

0

2

10

20

30

CPU time(s)

CPU time(s)

Ï = 0.9, p = 500

Ï = 0.9, p = 100

8

5

7

6

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

3

2

5

log(f(x k ) - f(x * ))

log(f(x k ) - f(x * ))

4

1

4

3

2

1

S-GD
SA-GD
SA-GD-I
SA-BFGS
SA-BFGS-I
SA-BFGS-GD
R-S-GD-C
R-S-GD-V

0
0
-1

-2

-1
0

0.2

0.4

0.6

0.8

1

CPU time(s)

1.2

1.4

1.6

1.8

2

0

10

20

30

CPU time(s)

may draw similar conclusions as to those in section 6 for the methods that use an adaptive step length. One

Stochastic Adaptive Quasi-Newton Methods

interesting finding in this set of experiments is that the robust SGD methods do not work well especially for
p = 500.

References
Byrd, Richard H., Nocedal, Jorge, and Yuan, Ya-Xiang. Global convergence of a class of quasi-Newton methods
on convex problems. Siam. J. Numer. Anal., (5):1171â€“1190, 1987.
Dennis Jr., John E. and MoreÌ, Jorge J. Characterization of superlinear convergence and its application to
quasi-Newton methods. Math. Comp., 28(106):549â€“560, 1974.
Gao, Wenbo and Goldfarb, Donald. Quasi-Newton methods: Superlinear convergence without line search for
self-concordant functions. in review. arXiv:1612.06965, 2016.
Goldfarb, Donald, Iyengar, Garud, and Zhou, Chaoxu. Linear convergence of stochastic Frank-Wolfe variants.
In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, pp. 1066â€“1074,
2017.
Griewank, Andreas and Toint, Philippe L. Local convergence analysis for partitioned quasi-Newton updates.
Numer. Math., 39:429â€“448, 1982.
Powell, Michael J. D. Some global convergence properties of a variable metric algorithm for minimization
without exact line searches. In Cottle, Richard and Lemke, C.E. (eds.), Nonlinear Programming, volume IX.
SIAM-AMS Proceedings, 1976.
W. van der Vaart, Aad. and Wellner, Jon A. Weak Convergence and Empirical Processes. Springer New York,
1996.

