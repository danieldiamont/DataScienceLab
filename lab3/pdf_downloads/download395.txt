Self-Paced Co-training

Fan Ma 1 Deyu Meng * 1 Qi Xie 1 Zina Li 1 Xuanyi Dong 2

In this supplementary material, we present the condition of
-expanding with respect to the proposed serial co-training
process, and give the proof that SPaCo is an efficient PAC
learning algorithm if such condition is satisfied.
Notation and Definition: We assume that examples are
drawn from some distributions D over an instance space
X = X1 Ã— X2 , where X1 and X2 correspond to two
different â€œviewsâ€ of examples. Let c denote the target
function, and let X + and X âˆ’ (for simplicity we assume
we are doing binary classification) denote the positive and
negative regions of X, respectively . For i âˆˆ 1, 2, let
Xi+ = {xj âˆˆ Xi : ci (xj ) = 1}, so we can think of X +
as X1+ Ã— X2+ , and let Xiâˆ’ = Xi âˆ’ Xi+ . Let D+ and Dâˆ’
denote the marginal distribution of D over X + and X âˆ’ ,
respectively.
For S1 âŠ† X1 and S2 âŠ† X2 , let boldface Si denote the
event that an example hx1 , x2 i has xi âˆˆ Si . The P (Sni )
denotes the possibility mass on example for which we are
confident under ith view in the nth training round. Below we give the definition of -expanding affixing marks of
training round.
Definition 1 (Balcan et al., 2004) Let X + denote the positive region and D+ denote the distribution over X + , and
Xi (i = 1, 2) is the training data set in the ith view. For
S1 âŠ† X1 and S2 âŠ† X2 , the D+ is -expanding if the following inequality holds:
P (S1 âŠ• S2 ) â‰¥  min(P (S1 âˆ§ S2 ), P (SÂ¯1 âˆ§ SÂ¯2 )),

(1)

where P (S1 âˆ§ S2 ) denotes the probability of examples for
being confident in both views, and P (S1 âŠ• S2 ) denotes
the probability of examples for being confident in only one
view.
To present training order of classifier under each view, we
add superscript for distinguishing the order of iteration.
The reivsed definition is:
1

Xiâ€™an Jiaotong University, Xiâ€™an, China 2 University of Technology Sydney, Sydney, Australia. Correspondence to: Deyu
Meng <dymeng@xjtu.edu.cn>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Definition 2 D+ is -expanding in the serial training process if
nâˆ’1
nâˆ’1
n
n
P (Sni âŠ• Snâˆ’1
3âˆ’i ) â‰¥  min(P (S3âˆ’i âˆ§ Si ), P (S3âˆ’i âˆ§ Si ))
(2)

This -expanding definition is the same as that defined in
(Balcan et al., 2004) except for the round mark in each
view. When D+ satisfies -expanding in every training
round and there are sufficient unlabeled instances, classifiers under each view can acquire arbitrary accuracy with
probability 1 âˆ’ Î´ after enough training rounds as described
in Theorem 1.
Theorem 1 Let f in and Î´f in be the desired accuracy and
confidence parameters. Suppose that serial -expanding
condition is satisfied in each training round, then we can
achieve error rate f in with probability 1âˆ’Î´f in by running
1
the SPaCo for N = O( 1 log f1in + 1 Â· pinit
) rounds, each time
running algorithm A1 and algorithm A2 with accuracy and
Â·
Î´f in
confidence parameters set to 8f in and 2N
respectively.
Similar to proof in (Balcan et al., 2004), we begin by stating
two lemmas that will be useful for the analysis. For both
lemmas, let Sin âŠ† Xi+ , and all probabilities are with the
respect to D+ .
Lemma1 Suppose P (Sn3âˆ’i âˆ§ Snâˆ’1
) â‰¤ P (Sn3âˆ’i âˆ§ Sinâˆ’1 ),
i
nâˆ’1

n
n
P (S3âˆ’i |S3âˆ’i âˆ¨ Si ) â‰¥ 1 âˆ’ 8 and P (Sn+1
|Sn3âˆ’i âˆ¨
i
nâˆ’1
n+1


n
Si ) â‰¥ 1 âˆ’ 8 , then P (Si âˆ§ S3âˆ’i ) â‰¥ (1 + 2 )P (Sn3âˆ’i âˆ§
Snâˆ’1
)
i
Proof
P (Sn+1
âˆ§ Sn3âˆ’i )
i
â‰¥ P (Sn+1
, Sn3âˆ’i âˆ¨ Snâˆ’1
) + P (Sn3âˆ’i , Sn3âˆ’i âˆ¨ Sinâˆ’1 )
i
i
âˆ’ P (Sn3âˆ’i âˆ¨ Snâˆ’1
)
i

â‰¥ (1 âˆ’ )(1 + )P (Sn3âˆ’i âˆ§ Snâˆ’1
)
i
4

â‰¥ (1 + )P (Sn3âˆ’i âˆ§ Snâˆ’1
)
i
2

(3)

n
Lemma2 Suppose P (Sn3âˆ’i âˆ§Snâˆ’1
) > P (S3âˆ’i âˆ§Snâˆ’1
) and
i
i
nâˆ’1
n+1 n
nâˆ’1
n
let Î³ = 1 âˆ’ P (S3âˆ’i âˆ§ Si ), if P (Si |S3âˆ’i âˆ¨ Si ) >
nâˆ’1
n+1
n
n
1âˆ’ Î³
) > 1âˆ’ Î³
âˆ§
8 and P (S3âˆ’i |S3âˆ’i âˆ¨Si
8 , then P (Si
nâˆ’1

n
n
S3âˆ’i ) â‰¥ (1 + 2 )P (S3âˆ’i âˆ§ Si )

Self-Paced Co-training

Proof
Î³ = P (Sn3âˆ’i âŠ• Snâˆ’1
) + P (Sn3âˆ’i âˆ§ Snâˆ’1
)
i
i
â‰¥ (1 + )P (Sn3âˆ’i âˆ§ Snâˆ’1
)
i
â‰¥ (1 + )(1 âˆ’

P (Sn3âˆ’i

âˆ¨

(4)

Snâˆ’1
))
i

From inequality 4 we can get P (Sn3âˆ’i âˆ¨ Snâˆ’1
) â‰¥ 1âˆ’
i
Thus
Î³
Î³
)(1 âˆ’
)
4
1+
Î³
â‰¥ (1 âˆ’ Î³)(1 + )
8
Î³
â‰¥ (1 + )P (Sn3âˆ’i âˆ§ Snâˆ’1
)
i
8

Î³
1+ .

P (Sn+1
âˆ§ Sn3âˆ’i ) â‰¥ (1 âˆ’
i

(5)

From Lemma 1 and Lemma 2, we present that with fine
tuned confidence condition, classifiers trained in a serial
way possess same character compared with classifiers built
paralleled after each iteration. Therefore, we conclude
that with the modified -expanding condition fulfilled, after
same number of iterations, classifiers trained serially can
achieve same error rate with same confidence as shown in
the original -expanding theorem (Balcan et al., 2004).

References
Balcan, Maria-Florina, Blum, Avrim, and Yang, Ke. Cotraining and expansion: Towards bridging theory and
practice. In Advances in neural information processing
systems, pp. 89â€“96, 2004.

