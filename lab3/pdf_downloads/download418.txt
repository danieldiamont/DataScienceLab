Supplement for Variational Boosting: Iteratively Refining Posterior
Approximations

A. Initializing Components
Introducing a new component requires initialization of component parameters. When our component distributions are
mixtures of Gaussians, we found that the optimization procedure is sensitive to initialization. This section describes
an importance-weighting scheme for initialization that produces (empirically) good initial values of component and
mixing parameters.
Conceptually, a good initial component is located in a region
of the target π(x) that is underrepresented by the existing
approximation q (C) . A good initial weight is close to the
proportion of mass in the unexplained region. Following
this principle, we construct this component by first drawing
importance-weighted samples from our existing approximation
x(`) ∼ q (C) ,

w(`) =

π(x

(`)

)

q (C) (x(`) )

for ` = 1, . . . , L.
(9)

The samples with the largest weights w(`) tell us where
regions of the target are poorly represented by our approximation. In fact, as L grows, and if q (C) is “close” enough
to π, we can interpret {x(`) , w(`) } as a weighted sample
from π. Based on this interpretation, we can fit a mixture
distribution (or some components of a mixture distribution)
to this weighted sample using maximum likelihood, and
recover a type of target approximation. For mixture distributions, an efficient inference procedure is ExpectationMaximization (EM) (Dempster et al., 1977).
This approach, however, presents a few complications. First,
we must adapt EM to fit a weighted sample. Second, importance weights can suffer from extremely high variance
— one or two w(`) values may be extremely large compared
to all other weights. This destabilizes our new component
parameters and mixing weight, particularly the variance of
the component. Intuitively, if a single weight w(`) is extremely large, this would correspond to many samples being
located in a single location, and maximum likelihood with
EM would want to shrink the variance of the new component to zero right on that location. To combat this behavior,
we use a simple method to break up the big weights using a
resampling and re-weighting step before applying weighted

EM. Empirically, this improves our new component initializations and subsequent ELBO convergence.
Weighted EM Expectation-maximization is typically
used to perform maximum likelihood in latent variable models. Mixture distributions are easily represented with latent
variables — a sample’s latent variable corresponds to the
mixture component that produced it. EM starts with some
initialization of model parameters (e.g.,component means,
variances and mixing weights). The algorithm then iterates between two steps: 1) the E-step, which computes the
distribution over the latent variables given the current setting of parameters, and 2) the M-step, which maximizes the
expected complete data log-likelihood with respect to the
distributions computed in the E-step.
We suppress details of the general treatment of EM, and
focus on EM for mixture models as presented in (Bishop,
2006). For mixture distributions, the E-step computes “responsibilities”, or the probability that a datapoint came
from one of the components. The M-step then computes
a weighted maximum likelihood, where the log-likelihood
of a datapoint for a particular component is weighted by
the associated “responsibility”. This weighted maximum
likelihood is an easy entry-point for an additional set of
weights — weights associated with each datapoint from the
importance-weighting.
More concretely, for a sample of data, x(`) , C mixture
components, and current mixture component parameters
and weights λ = {ρc , λc }C
c=1 , the E-step computes the
following quantities
γc(`) = p(z (`) = c|x(`) , λ)
∝ p(x(`) |z (`),λc = c)p(z (`) = c)
(`)

where γc is the “responsibility” of cluster c for datapoint
`. The M-step then computes component parameters by a
weighted maximum likelihood
λ∗c = arg max
λ

L
X

γc(`) · ln p(x(`) |z (`) = c, λc ) .

`=1

To incorporate importance weights w(`) , we only need to

Variational Boosting: Iteratively Refining Posterior Approximations

slightly change the M-step:
λ∗c = arg max
λ

L
X

w(`) · γc(`) · ln p(x(`) |z (`) = c, λc ) .

`=1

Because we are adding a new component, we would like our
weighted EM routine to leave the remaining components
unchanged. For instance, we want λ1 , . . . , λC−1 to be fixed,
while λC is free to explain the weighted sample. This can
be accomplished in a straightforward manner by simply
clamping the first C − 1 parameters during the M-step.
Resampling importance weights If our current approximation q (C) is sufficiently different in certain regions of
the posterior, then some weights w(`) will end up being
large compared to other weights. For instance, the objective KL(q||p) tends to under-cover regions of the posterior,
allowing π(x) to be much larger than q (c) (x), meaning the
weight associated with x will be large. This will create
instability in the weighted EM approximation — likelihood
maximization will want to put a zero-variance component
on the single highest-weighted sample, which does not accurately reflect the local curvature of π(x). To combat this, we
construct a slightly more complicated proposal distribution.
Conceptually, we first create this naïve importance-weighted
sample, and then find samples with outlier weights, and
break those samples up. We do this by constructing a new
proposal distribution that mixes the existing proposal, q (C) ,
and component means located at the outlier samples. We
define this proposal to be
X
q (IW ) (x) ∝ p0 q (C) (x) +
w(`) N (x|x(`) , Σ(`) ) (10)
`∈O

where ` ∈ O denote the set ofPoutlier samples from our
original sample, and p0 = 1 − `∈O w(`) is the mass not
placed on outlier samples. The variance of each outlier component, Σ(`) is set to some heuristic value — we typically
use the diagonal of the covariance of q (C) as a good-enough
guess.
We then create a new importance-weighted sample, using
q (IW ) and π(x) just as we did before. By placing new
components (with some non-zero variance) on the outlier
samples, which are known to be in a region of high target
probability and low approximate probability, we assume that
there is more local probability around that region that needs
to be explored. This allows us to inflate the local variance of
the samples in this region — the region that weighted EM
will place a component. Algorithm 1 unites the components
from above sections into our final initialization procedure.

decide on the rank of the variational approximation, some
more appropriate for certain settings given dimensionality
and computation constraints. For instance, we can greedily
incorporate new rank components. Alternatively, we can fit
a sequence of components r = 1, 2, . . . , rmax , and choose
the best result (in terms of KL). In the Bayesian neural
network model, we report results for a fixed schedule of
ranks. In the hierarchical Poisson model, we monitor the
change in marginal variances to decide the appropriate rank.
In both cases, we require a stopping criterion. For a single Gaussian, one such criterion is the average change in
marginal variances — if the marginal variation along each
dimension remains the same from rank r to r + 1, then the
new covariance component is not incorporating explanatory
power, particularly if marginal variances are of interest. As
the KL(q||π) objective tends to underestimate variances
when restricted to a particular model class, we observe that
the marginal variances grow as new covariance rank components are added. When fitting rank r + 1, we can monitor
the average absolute change in marginal variance (or standard deviation) as more covariance structure is incorporated.
Figure 5 in this supplement depicts this measurement for
the D = 37-dimensional ‘frisk‘ posterior.
To justify sequentially adding ranks to mixture components we consider the KL-divergence between a rank-r
Gaussian approximation to a full covariance
Pr Gaussian,
KL(qr ||p), where qr (θ) = N (0, I(v) + l=1 fk fk| ) and
p(θ) = N (0, Σ). For simplicity, we assume both distributions have zero mean. If the true posterior is non-Gaussian
we will attempt to approximate the best full-rank Gaussian
with a low-rank Gaussian thus suffering an unrepresentable
KL-divergence between the family of Gaussians and the
true posterior. We also assume that the diagonal component,
I(v), and the first r − 1 columns of F = [f1 , . . . , fr ] are
held fixed. Then we have

KL(qr ||p)
1
tr Σ−1
=
2

I(v) +

r
X

!!
fl fl|

l=1

− k + log det Σ
− log det I(v) +

r
X

!
fl fl|



l=1

B. Fitting the Rank
To specify the ELBO objective, we need to choose a rank
r for the component covariance. There are a many ways to

which we differentiate with respect to vr , remove terms that

Variational Boosting: Iteratively Refining Posterior Approximations

Algorithm 1 Importance-weighted initialization of new components. This algorithm takes in the target distribution, π(x),
the current approximate distribution q (C) (x), and a number of samples L. This returns an initial value of new component
parameters, λC+1 and a new mixing weight ρC+1 .
1: procedure I NIT C OMP(π, q (C) , L)
2:
x(`) ∼ q (C) for ` = 1, . . . , L

. sample from existing approx
. set importance weights

π(x(`) )
q (C) (x(`) )

3:

w(`) ←

4:
5:
6:

O ← outlier-weights({w(`) })
q (IW ) ← make-mixture(O, {w(`) , x(`) }, q (C) )
(`)
xr ∼ q (IW ) for ` = 1, . . . , L

7:

wr ←

(`)

. break up big weights
. sample from new mixture

π(x(`)
r )
q (IW ) (x(`) )

. re-sampled importance weights

(`)
(`)
weighted-em({xr , wr })

8:
λC+1 , ρC+1 ←
9:
return λC+1 , ρC+1
10: end procedure

. fit new component

C. Experiment Figures

do not depend on vr , and set to zero, yielding

C.1. Frisk Model
∂
KL(qr ||p)
∂vr

1
= Σ−1 vr −
2

I(v) +

r
X

→Σ

vr  = 0

vr

C

=

C −1 −

C −1 fr fr| C −1
1 + fr| C −1 fr


fr .

We can thus determine the optimal fr from the following
equation

Σ

−1

−C

−1



fr =

C.2. Bayes Neural Network Results
Table 3 depict out of sample log probability results for the
Bayesian neural network as ranks vary.

−1



r−1
X


|
|

fl fl +fr fr 
vr = I(v) +


l=1
|
{z
}


Figures 6 and 7 depict VBoost approximations of the ‘frisk‘
model.

l=1


−1

!−1
fl fl|



−

C −1 fr fr| C −1

!

2

1 + ||fr ||C
2

fr

(11)

where we have defined fr| C −1 fr = ||fr ||C . Eq. (11) is
reminiscent of an eigenvalue problem indicating that the
optimal solution for fr should maximally explain Σ−1 −
C −1 , i.e. the P
parameter space not already explained by
r−1
C = I(v) + l=1 fl fl| . This provides justification for
the previously proposed stopping criterion that monitors
the increase in marginal variances since incorporating a
new vector into the low-rank approximation should grow
the marginal variances if extra correlations are captured.
This is due to minimizing KL(qr ||p) which underestimates
the variances when dependencies between parameters are
broken.

Variational Boosting: Iteratively Refining Posterior Approximations

1.6
1.4

ave % diff in marginal sds

1.2
1.0
0.8
0.6
0.4
0.2
0.0

1

2

3

4

5

6

7

8

9

rank

Figure 5. Mean percent change in marginal variances for the Poisson GLM. After rank 5, the average percent change is less than 5% —
this estimate is slightly noisy due to the stochastic optimization procedure.

alpha_0

alpha_1

rank 0
mcmc

beta_1

beta_0

rank 0
mcmc

lnsigma_a_0

rank 0
mcmc

alpha_0

rank 0
mcmc

lnsigma_b_0

rank 0
mcmc

alpha_1

rank 1
mcmc

beta_1

rank 0
mcmc

lnsigma_a_0

rank 1
mcmc

(a) Rank 0 (MFVI)
alpha_0

alpha_1

rank 2
mcmc

beta_1

lnsigma_a_0

(c) Rank 2

alpha_0

rank 2
mcmc

lnsigma_b_0

rank 2
mcmc

rank 1
mcmc

lnsigma_b_0

rank 1
mcmc

rank 1
mcmc

(b) Rank 1
beta_0

rank 2
mcmc

rank 2
mcmc

beta_0

rank 1
mcmc

alpha_1

rank 3
mcmc

beta_1

rank 2
mcmc

beta_0

rank 3
mcmc

lnsigma_a_0

rank 3
mcmc

rank 3
mcmc

lnsigma_b_0

rank 3
mcmc

rank 3
mcmc

(d) Rank 3

Figure 6. Comparison of single Gaussian component marginals by rank for a 37-dimensional Poisson GLM posterior. The top left plot is a
diagonal Gaussian approximation. The next plots show the how the marginal variances inflate as the covariance is allotted more capacity.

Variational Boosting: Iteratively Refining Posterior Approximations

1-component

2-component

0.6

0.5

0.4

0.4

0.2

0.3
0.2

0.1

0.1

0.0

0.0
0.0

0.1

0.2

0.3

0.4

0.5

VI Std Devs

0.5

0.4
0.3

0.6

0.2

0.3

0.4

0.5

0.6

0.0

0.5

0.4

0.4

0.3
0.2

0.1

0.1

0.0

0.0
0.1

0.2

0.3

0.4

0.5

0.6

VI Std Devs

0.5

0.4
0.2

0.2

0.3

0.4

0.5

0.6

0.0

0.005

VI Covs

0.005

VI Covs

0.005

0.000
0.005

0.005

0.010

0.005

0.000

0.005

0.010

0.010

4-component

8-component
0.005

0.005

VI Covs

0.005

VI Covs

0.010

0.000
0.005

0.000

0.005

MCMC Covs (~20k samples)

0.010

0.000

0.005

0.010

0.000
0.005

0.010
0.005

0.005

12-component

0.010

0.010

0.6

MCMC Covs (~20k samples)

0.010

0.010

0.5

0.000

MCMC Covs (~20k samples)

0.005

0.4

0.010
0.010

MCMC Covs (~20k samples)

0.000

0.3

0.005

0.010
0.000

0.2

3-component
0.010

0.005

0.1

MCMC Std Devs (~20k samples)

0.010

0.010

0.6

0.0
0.1

0.010

0.010

0.5

0.2

(a) Marginal standard deviations
2-component

0.005

0.4

0.3

MCMC Std Devs (~20k samples)

0.000

0.3

0.1
0.0

1-component

0.2

12-component

0.6

0.5
0.3

0.1

MCMC Std Devs (~20k samples)

8-component

0.6

VI Std Devs

VI Std Devs

0.1

MCMC Std Devs (~20k samples)

MCMC Std Devs (~20k samples)

VI Covs

0.2
0.0

0.0

4-component

0.0

VI Covs

0.3
0.1

MCMC Std Devs (~20k samples)
0.6

3-component

0.6

0.5

VI Std Devs

VI Std Devs

0.6

0.010
0.010

0.005

0.000

0.005

MCMC Covs (~20k samples)

0.010

0.010

0.005

0.000

0.005

0.010

MCMC Covs (~20k samples)

(b) Pairwise covariances
Figure 7. A comparison of standard deviations and covariances for the frisk model. The MCMC-inferred values are along the horizontal
axis, with the variational boosting values along the vertical axis. While the rank 3 plus diagonal covariance structure is able to account for
most of the marginal variances, the largest one is still underestimated. Incorporating more rank 3 components allows the approximation to
account for this variance. Similarly, the non-zero covariance measurements improve as more components are added.

Variational Boosting: Iteratively Refining Posterior Approximations

wine
boston
concrete
power-plant
yacht
energy-efficiency

pbp

mfvi

rank 5

rank 10

rank 15

-0.990 (± 0.08)
-2.902 (± 0.64)
-3.162 (± 0.15)
-2.798 (± 0.04)
-0.990 (± 0.08)
-1.971 (± 0.11)

-0.973 (± 0.05)
-2.658 (± 0.18)
-3.248 (± 0.07)
-2.812 (± 0.03)
-0.973 (± 0.05)
-2.451 (± 0.12)

-0.972 (± 0.05)
-2.670 (± 0.16)
-3.247 (± 0.06)
-2.814 (± 0.03)
-0.972 (± 0.05)
-2.452 (± 0.12)

-0.972 (± 0.05)
-2.696 (± 0.14)
-3.261 (± 0.06)
-2.838 (± 0.03)
-0.972 (± 0.05)
-2.469 (± 0.11)

-0.973 (± 0.05)
-2.743 (± 0.12)
-3.286 (± 0.05)
-2.867 (± 0.02)
-0.973 (± 0.05)
-2.502 (± 0.09)

Table 3. Comparison of test log probability for PBP (Hernández-Lobato & Adams, 2015) to Variational Inference with various ranks.
Each entry shows the average predictive performance of the model on a specific dataset and the standard deviation across the 20 trials —
bold indicates the best average (though not necessarily statistical significance).

