Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to
Non-smooth Concave Maximization
Supplementary File

A. Technical Proofs
A.1. Proof of Theorem 1
Proof. “⇐”: If the pair (w̄, ᾱ) is a sparse saddle point for L, then from the definition of conjugate convexity and inequality (3) we have
P (w̄) = max L(w̄, α) ≤ L(w̄, ᾱ) ≤ min L(w, ᾱ).
α∈F N

kwk0 ≤k

On the other hand, we know that for any kwk0 ≤ k and α ∈ F

N

L(w, α) ≤ max L(w, α0 ) = P (w).
α0 ∈F N

By combining the preceding two inequalities we obtain
P (w̄) ≤ min L(w, ᾱ) ≤ min P (w) ≤ P (w̄).
kwk0 ≤k

kwk0 ≤k

Therefore P (w̄) = minkwk0 ≤k P (w), i.e., w̄ solves the problem in (1), which proves the necessary condition (a). Moreover,
the above arguments lead to
P (w̄) = max L(w̄, α) = L(w̄, ᾱ).
α∈F N

Then from the maximizing argument property of convex conjugate we know that ᾱi ∈ ∂li (w̄> xi ). Thus the necessary
condition (b) holds. Note that

2
N
N

1 X
1 X ∗
λ


ᾱi xi  −
li (ᾱi ) + C,
(11)
L(w, ᾱ) = w +

2
Nλ
N
i=1

where C is a quantity not dependent on w.
minkwk0 ≤k L(w, ᾱ), it must hold that

Let F̄ = supp(w̄).
N

w̄ = HF̄

i=1

1 X
−
ᾱi xi
N λ i=1

!

Since the above analysis implies L(w̄, ᾱ) =
N

= Hk

1 X
−
ᾱi xi
N λ i=1

!
.

This validates the necessary condition (c).
“⇒”: Conversely, let us assume that w̄ is a k-sparse solution to the problem (1) (i.e., conditio(a)) and let ᾱi ∈ ∂li (w̄> xi )
(i.e., condition (b)). Again from the maximizing argument property of convex conjugate we know that li (w̄> xi ) =
ᾱi w̄> xi − li∗ (ᾱi ). This leads to
L(w̄, α) ≤ P (w̄) = max L(w̄, α) = L(w̄, ᾱ).

(12)

α∈F N

The sufficient condition (c) guarantees that F̄ contains the top k (in absolute value) entries of − N1λ
on the expression in (11) we can see that the following holds for any k-sparse vector w
L(w̄, ᾱ) ≤ L(w, ᾱ).
By combining the inequalities (12) and (13) we get that for any kwk0 ≤ k and α ∈ F N ,
L(w̄, α) ≤ L(w̄, ᾱ) ≤ L(w, ᾱ).
This shows that (w̄, ᾱ) is a sparse saddle point of the Lagrangian L.

PN

i=1

ᾱi xi . Then based
(13)

Dual Iterative Hard Thresholding

A.2. Proof of Theorem 2
Proof. “⇒”: Let (w̄, ᾱ) be a saddle point for L. On one hand, note that the following holds for any k-sparse w0 and
α0 ∈ F N
min L(w, α0 ) ≤ L(w0 , α0 ) ≤ max L(w0 , α),
α∈F N

kwk0 ≤k

which implies
min L(w, α) ≤ min

max

α∈F N kwk0 ≤k

max L(w, α).

kwk0 ≤k α∈F N

(14)

On the other hand, since (w̄, ᾱ) is a saddle point for L, the following is true:
max L(w, α) ≤ max L(w̄, α)

min

kwk0 ≤k α∈F N

α∈F N

≤ L(w̄, ᾱ) ≤ min L(w, ᾱ)
kwk0 ≤k

≤ max

(15)

min L(w, α).

α∈F N kwk0 ≤k

By combining (14) and (15) we prove the equality in (4).
“⇐”: Assume that the equality in (4) holds. Let us define w̄ and ᾱ such that
max L(w̄, α) = min

max L(w, α)

min L(w, ᾱ) = max

min L(w, α)

α∈F N

kwk0 ≤k

kwk0 ≤k α∈F N

.

α∈F N kwk0 ≤k

Then we can see that for any α ∈ F N ,
L(w̄, ᾱ) ≥ min L(w, ᾱ) = max L(w̄, α0 ) ≥ L(w̄, α),
kwk0 ≤k

α0 ∈F N

where the “=” is due to (4). In the meantime, for any kwk0 ≤ k,
L(w̄, ᾱ) ≤ max L(w̄, α) =
α∈F N

min L(w0 , ᾱ) ≤ L(w, ᾱ).

kw0 k0 ≤k

This shows that (w̄, ᾱ) is a sparse saddle point for L.
A.3. Proof of Lemma 1
Proof. For any fixed α ∈ F N , then it is easy to verify that the k-sparse minimum of L(w, α) with respect to w is attained
at the following point:
!
N
1 X
αi xi .
w(α) = arg min L(w, α) = Hk −
N λ i=1
kwk0 ≤k
Thus we have
D(α) = min L(w, α) = L(w(α), α)
kwk0 ≤k

=
ζ1

=

N
 λ
1 X
αi w(α)> xi − li∗ (αi ) + kw(α)k2
N i=1
2
N
1 X ∗
λ
−li (αi ) − kw(α)k2 ,
N i=1
2

where “ζ1 ” follows from the above definition of w(α).
Now let us consider two arbitrary dual variables α0 , α00 ∈ F N and any g(α00 ) ∈ N1 [w(α00 )> x1 −∂l1∗ (α100 ), ..., w(α00 )> xN −
∗
00
∂lN
(αN
)]. From the definition of D(α) and the fact that L(w, α) is concave with respect to α at any fixed w we can derive
that
D(α0 ) = L(w(α0 ), α0 )
≤ L(w(α00 ), α0 )
≤ L(w(α00 ), α00 ) + hg(α00 ), α0 − α00 i .

Dual Iterative Hard Thresholding

This shows that D(α) is a concave function and its super-differential is as given in the theorem.
If we further assume that w(α) is unique and {li∗ }i=1,...,N are differentiable at any α, then ∂D(α) = N1 [w(α)> x1 −
∗
∂l1∗ (α1 ), ..., w(α)> xN − ∂lN
(αN )] becomes unique, which implies that ∂D(α) is the unique super-gradient of D(α).
A.4. Proof of Theorem 3
Proof. “⇒”: Given the conditions in the theorem, it can be known from Theorem 1 that the pair (w̄, ᾱ) forms a sparse
saddle point of L. Thus based on the definitions of sparse saddle point and dual function D(α) we can show that
D(ᾱ) = min L(w, ᾱ) ≥ L(w̄, ᾱ) ≥ L(w̄, α) ≥ D(α).
kwk0 ≤k

This implies that ᾱ solves the dual problem in (5). Furthermore, Theorem 2 guarantees the following
D(ᾱ) = max

min L(w, α) = min

α∈F N kwk0 ≤k

max L(w, α) = P (w̄).

kwk0 ≤k α∈F N

This indicates that the primal and dual optimal values are equal to each other.
“⇐”: Assume that ᾱ solves the dual problem in (5) and D(ᾱ) = P (w̄). Since D(ᾱ) ≤ P (w) holds for any kwk0 ≤ k, w̄
must be the sparse minimizer of P (w). It follows that
max

min L(w, α) = D(ᾱ) = P (w̄) = min

α∈F N kwk0 ≤k

max L(w, α).

kwk0 ≤k α∈F N

From the “⇐” argument in the proof of Theorem 2 and Corollary 1 we get that the conditions (a)∼(c) in Theorem 1 should
be satisfied for (w̄, ᾱ).
A.5. Proof of Theorem 4
We need a series of technical lemmas to prove this theorem. The following lemmas shows that under proper conditions,
w(α) is locally smooth around w̄ = w(ᾱ).
Lemma 2. Let X = [x1 , ..., xN ] ∈ Rd×N be the data matrix. Assume that {li }i=1,...,N are differentiable and
¯ := w̄min −
If kα − ᾱk ≤

λN ¯
2σmax (X) ,

1 0
kP (w̄)k∞ > 0.
λ

then supp(w(α)) = supp(w̄) and
kw(α) − w̄k ≤

σmax (X, k)
kα − ᾱk.
Nλ

Proof. For any α ∈ F N , let us define
N

w̃(α) = −

1 X
αi xi .
N λ i=1
0

Consider F̄ = supp(w̄). Given ¯ > 0, it is known from Theorem 3 that w̄ = HF̄ (w̃(ᾱ)) and P λ(w̄) = HF̄ c (−w̃(ᾱ)). Then
λN ¯
¯ > 0 implies F̄ is unique, i.e., the top k entries of w̃(ᾱ) is unique. Given that kα − ᾱk ≤ 2σmax
(X) , it can be shown that
1
σmax (X)
¯
kX(α − ᾱ)k ≤
kα − ᾱk ≤ .
Nλ
Nλ
2
This indicates that F̄ still contains the (unique) top k entries of w̃(α). Therefore,
kw̃(α) − w̃(ᾱ)k =

supp(w(α)) = F̄ = supp(w̄).
Then it must hold that

This proves the desired bound.

kw(α) − w(ᾱ)k = kHF̄ (w̃(α)) − HF̄ (w̃(ᾱ)) k
1
=
kX (α − ᾱ)k
N λ F̄
σmax (X, k)
≤
kα − ᾱk.
Nλ

Dual Iterative Hard Thresholding

p
The following lemma bounds the estimation error kα − ᾱk = O( hD0 (α), ᾱ − αi) when the primal loss {li }N
i=1 are
smooth.
Lemma 3. Assume that the primal loss functions {li (·)}N
i=1 are 1/µ-smooth. Then the following inequality holds for any
α, α00 ∈ F and g(α00 ) ∈ ∂D(α00 ):
D(α0 ) ≤ D(α00 ) + hg(α00 ), α0 − α00 i −

2
λN µ + σmin
(X, k) 0
kα − α00 k2 .
2
2λN

Moreover, ∀α ∈ F and g(α) ∈ ∂D(α),
s
kα − ᾱk ≤

2λN 2 hg(α), ᾱ − αi
2 (X, k) .
λN µ + σmin

Proof. Recall that
D(α) =

N
λ
1 X ∗
−l (αi ) − kw(α)k2 ,
N i=1 i
2

Now let us consider two arbitrary dual variables α0 , α00 ∈ F. The assumption of li being 1/µ-smooth implies that its
convex conjugate function li∗ is µ-strongly-convex. Let F 00 = supp(w(α00 )). Then
N
λ
1 X ∗ 0
−li (αi ) − kw(α0 )k2
N i=1
2

!2
N
N

λ
1 X ∗ 0
1 X 0


−li (αi ) − Hk −
αi xi 
=

N i=1
2
N λ i=1

D(α0 ) =


N
 λ
0
1 X  ∗ 00
µ

≤
−li (αi ) − li∗ (αi00 )(αi0 − αi00 ) − (αi0 − αi00 )2 − HF 00
N i=1
2
2
≤

N

1 X 0
−
α xi
N λ i=1 i

!2





N
N
 λ
0
1 X  ∗ 00
µ
1 X >
−li (αi ) − li∗ (αi00 )(αi0 − αi00 ) − (αi0 − αi00 )2 − kw(α00 )k2 +
x w(α00 )(αi0 − αi00 )
N i=1
2
2
N i=1 i

1
(α0 − α00 )> XF>00 XF 00 (α0 − α00 )
2λN 2
2
λN µ + σmin
(X, k) 0
≤D(α00 ) + hg(α00 ), α0 − α00 i −
kα − α00 k2 .
2λN 2
−

This proves the first desirable inequality in the lemma. By invoking the above inequality and using the fact D(α) ≤ D(ᾱ)
we get that
2
λN µ + σmin
(X, k)
D(ᾱ) ≤D(α) + hg(α), ᾱ − αi −
kα − ᾱk2
2λN 2
2
λN µ + σmin
(X, k)
≤D(ᾱ) + hg(α), ᾱ − αi −
kα − ᾱk2 ,
2λN 2
which leads to the second desired bound.
The following lemma gives a simple expression of the gap for properly related primal-dual pairs.
Lemma 4. Given a dual variable α ∈ F N and the related primal variable
!
N
1 X
w = Hk −
αi xi .
N λ i=1
The primal-dual gap P D (w, α) can be expressed as:
P D (w, α) =

N

1 X
li (w> xi ) + li∗ (αi ) − αi w> xi .
N i=1

Dual Iterative Hard Thresholding

Proof. It is directly to know from the definitions of P (w) and D(α) that
P (w) − D(α)
N
1 X
λ
=
li (w> xi ) + kwk2 −
N i=1
2

=

N
 λ
1 X
αi w> xi − li∗ (αi ) + kwk2
N i=1
2

!

N

1 X
li (w> xi ) + li∗ (αi ) − αi w> xi .
N i=1

This shows the desired expression.
Based on Lemma 4, we can derive the following lemma which establishes a bound on the primal-dual gap.
Lemma 5. Consider a primal-dual pair (w, α) satisfying
N

1 X
−
αi xi
N λ i=1

w = Hk

!
.

Then the following inequality holds for any g(α) ∈ ∂D(α) and β ∈ [∂l1 (w> x1 ), ..., ∂lN (w> xN )]:
P (w) − D(α) ≤ hg(α), β − αi.
Proof. For any i ∈ [1, ..., N ], from the maximizing argument property of convex conjugate we have
li (w> xi ) = w> xi li0 (w> xi ) − li∗ (li0 (w> xi )),
and

0

0

li∗ (αi ) = αi li∗ (αi ) − li (li∗ (αi )).
By summing both sides of above two equalities we get
li (w> xi ) + li∗ (αi )
0

0

=w> xi li0 (w> xi ) + αi li∗ (αi ) − (li (li∗ (αi )) + li∗ (li0 (w> xi )))
ζ1

0

0

≤w> xi li0 (w> xi ) + αi li∗ (αi ) − li∗ (αi )li0 (w> xi ),
where “ζ1 ” follows from Fenchel-Young inequality. Therefore
hg(α), β − αi
N
0
1 X >
=
(w xi − li∗ (αi ))(li0 (w> xi ) − αi )
N i=1

=
ζ2

≥

N

0
0
1 X > 0 >
w xi li (w xi ) − li∗ (αi )li0 (w> xi ) − αi w> xi + αi li∗ (αi )
N i=1
N
1 X
(li (w> xi ) + αi li∗ (αi ) − w> xi )
N i=1

ζ3

=P (w) − D(α),
where “ζ2 ” follows from (16) and “ζ3 ” follows from Lemma 4. This proves the desired bound.
The following simple result is also needed in our iteration complexity analysis.
Lemma 6. For any  > 0,
1 ln t
+
≤
t
t
holds when t ≥ max

3


	
ln 3 , 1 .

(16)

Dual Iterative Hard Thresholding

Proof. Obviously, the inequality 1t +
t implies that 1t ≤ 3 . Also, we have

ln t
t

≤  holds for  ≥ 1. When  < 1, it holds that ln( 3 ) ≥ 1. Then the condition on
ln( 3 ln 3 )
ln( 3 )2
ln t
2
≤ 3 3 ≤ 3  3 = ,
t
3
 ln 
 ln 

where the first “≤” follows the fact that ln t/t is decreasing when t ≥ 1 while the second “≤” follows ln x < x for all
x > 0. Therefore we have 1t + lnt t ≤ .
We are now in the position to prove the main theorem.
(t)

of Theorem 4. Part(a): Let us consider g (t) ∈ ∂D(α(t) ) with gi
we can verify that kw(t) k ≤ r/λ. Therefore we have
kg (t) k ≤ c0 =

=

1
> (t)
N (xi w

0

(t)

− li∗ (αi )). From the expression of w(t)

r + λρ
√ .
λ N

Let h(t)p= kα(t) − ᾱk and v (t) = hg (t) , ᾱ − α(t) i. The concavity of D implies v (t) ≥ 0. From Lemma 3 we know that
h(t) ≤ 2λN 2 v (t) /(λN µ + σmin (X, k)). Then


(h(t) )2 =kPF N α(t−1) + η (t−1) g (t−1) − ᾱk2
≤kα(t−1) + η (t−1) g (t−1) − ᾱk2
=(h(t−1) )2 − 2η (t−1) v (t−1) + (η (t−1) )2 kg (t−1) k2
≤(h(t−1) )2 −
Let η (t) =

λN 2
(λN µ+σmin (X,k))(t+1) .

η (t−1) (λN µ + σmin (X, k)) (t−1) 2
(h
) + (η (t−1) )2 c20 .
λN 2

Then we obtain


(t) 2

(h ) ≤

1
1−
t



(h(t−1) )2 +

λ2 N 4 c20
.
(λN µ + σmin (X, k))2 t2

By recursively applying the above inequality we get
(h(t) )2 ≤

λ2 N 4 c20
(λN µ + σmin (X, k))2



1 ln t
+
t
t




= c1

1 ln t
+
t
t


.

This proves the desired bound in part(a).
Part(b): Let us consider  =

λN ¯
2σmax (X) .

From part(a) and Lemma 6 we obtain
kα(t) − ᾱk ≤ 

after t ≥ t0 =

3c1
2

(t)
1
ln 3c
2 . It follows from Lemma 2 that supp(w ) = supp(w̄).

0
Let β (t) := [l10 ((w(t) )> x1 ), ..., lN
((w(t) )> xN )]. According to Lemma 5 we have
(t)

P D = P (w(t) ) − D(α(t) )
≤ hg (t) , β (t) − α(t) i
≤ kg (t) k(kβ (t) − ᾱk + kᾱ − α(t) k).
0
Since ¯ = w̄min − λ1 kP 0 (w̄)k∞ > 0, it follows from Theorem 2 that ᾱ = [l10 (w̄> x1 ), ..., lN
(w̄> xN )]. Given that t ≥ t0 ,
from the smoothness of li and Lemma 2 we get

kβ (t) − ᾱk ≤

1 (t)
σmax (X, k) (t)
kw − w̄k ≤
kα − ᾱk, .
µ
µλN

Dual Iterative Hard Thresholding

where in the first “≤” we have used kxi k ≤ 1. Therefore, the following is valid when t ≥ t0 :
(t)

P D ≤ kg (t) k(kβ (t) − ᾱk + kᾱ − α(t) k)


σmax (X, k)
kα(t) − ᾱk.
≤ c0 1 +
µλN


Since t ≥ t1 , from part(a) and Lemma 6 we get kα(t) − ᾱk ≤
implies

(t)
P D

c0 (1+

σmax (X,k)
µλN

)

, which according to the above inequality

≤ . This proves the desired bound.

A.6. Proof of Theorem 5
0

(t)

(t)

(t)
− lj∗ (αi )). Let h(t) = kα(t) − ᾱk and v (t) = hg (t) , ᾱ − α(t) i.
Proof. Part(a): Let us consider g (t) with gj = N1 (x>
j w
p
(t)
The concavity of D implies v ≥ 0. From Lemma 3 we know that h(t) ≤ 2λN 2 v (t) /(λN µ + σmin (X, k)). Let
(t)
(t)
(t)
gBi := HB (t) (g (t) ) and vBi := hgBi , ᾱ − α(t) i Then
i



(t−1)
(h(t) )2 =kPF N α(t−1) + η (t−1) gBi
− ᾱk2
(t−1)

≤kα(t−1) + η (t−1) gBi

− ᾱk2

(t−1)

=(h(t−1) )2 − 2η (t−1) vBi

(t−1) 2

+ (η (t−1) )2 kgBi

k .

By taking conditional expectation (with respect to uniform random block selection, conditioned on α(t−1) ) on both sides
of the above inequality we get
E[(h(t) )2 | α(t−1) ]
m
m
1 X (t−1) 2 (t−1) 2
1 X (t−1) (t−1)
2η
vBi +
(η
) kgBi k
≤(h(t−1) )2 −
m i=1
m i=1
2η (t−1) (t−1) (η (t−1) )2 (t−1) 2
v
+
kg
k
m
m
η (t−1) (λN µ + σmin (X, k)) (t−1) 2 (η (t−1) )2 2
≤(h(t−1) )2 −
(h
) +
c0 ..
λmN 2
m
=(h(t−1) )2 −

Let η (t) =

λmN 2
(λN µ+σmin (X,k))(t+1) .

Then we obtain

E[(h(t) )2 | α(t−1) ] ≤


1−

1
t



(h(t−1) )2 +

λ2 mN 4 c20
.
(λN µ + σmin (X, k))2 t2

By taking expectation on both sides of the above over α(t−1) , we further get


1
λ2 mN 4 c20
E[(h(t) )2 ] ≤ 1 −
E[(h(t−1) )2 ] +
.
t
(λN µ + σmin (X, k))2 t2
This recursive inequality leads to
E[(h(t) )2 ] ≤

λ2 mN 4 c20
(λN µ + σmin (X, k))2



1 ln t
+
t
t




= c2

1 ln t
+
t
t


.

This proves the desired bound in part(a).
Part(b): Let us consider  =

λN ¯
2σmax (X) .

From part(a) and Lemma 6 we obtain
E[kα(t) − ᾱk] ≤ δ

3c2
(t)
2
after t ≥ t2 = δ3c
− ᾱk ≤ E[kα(t) − ᾱk]/δ ≤  holds with
2 2 ln δ 2 2 . Then from Markov inequality we know that kα
(t)
(t)
probability at least 1 − δ. Lemma 2 shows that kα − ᾱk ≤  implies supp(w ) = supp(w̄). Therefore when t ≥ t2 , the
event supp(w(t) ) = supp(w̄) occurs with probability at least 1 − δ.

Dual Iterative Hard Thresholding

Similar to the proof arguments of Theorem 4(b) we can further show that when t ≥ 4t2 , with probability at least 1 − δ/2
kα(t) − ᾱk ≤
which then leads to

λN ¯
,
2σmax (X)



σmax (X, k)
(t)
P D ≤ c0 1 +
kα(t) − ᾱk.
µλN

Since t ≥ t3 , from the arguments in part(a) and Lemma 6 we get that kα(t) − ᾱk ≤


c0 (1+

σmax (X,k)
µλN

at least 1 − δ/2. Let us consider the following events:
(t)

• A: the event of P D ≤ ;
• B: the event of kα(t) − ᾱk ≤
• C: the event of kα(t) − ᾱk ≤

λN ¯
2σmax (X) ;

c0 (1+

σmax (X,k)
µλN

)

.

When t ≥ max{4t2 , t3 }, we have the following holds:
P(A) ≥ P(A | B)P(B) ≥ P(C | B)P(B) ≥ (1 − δ/2)2 ≥ 1 − δ.
This proves the desired bound.

)

holds with probability

