Supplement to “An Infinite Hidden Markov Model with
Similarity-Biased Transitions”, ICML 2017
Colin Dawson, Chaofan Huang, Clayton Morrison
June 13, 2017
This supplement contains additional derivations to accompany the model definition and Gibbs updates for the
paper “An Infinite Hidden Markov Model with Similariy-Biased Transitions”, published in ICML 2017. Section
1 concerns the derivation of the augmented data representation referred to as the “Markov Jump Process with
Failed Transitions” (MJP-FT). Section 2 fills in details for the Gibbs sampling steps to sample the rescaled HDP
used by the HDP-HMM-LT. Section 3 gives a derivation for the updates to the binary state vectors, θ, in the
version of the HDP-HMM-LT used in the cocktail party experiment. Finally, section 4 gives the details for the
Hamiltonian Monte Carlo update for ` in the version of the model used in the Bach chorale experiment.

1. Details of the Markov Jump Process with Failed Transitions Representation
We can gain stronger intuition, as well as simplify posterior inference, by re-casting the HDP-HMM-LT as a
continuous time Markov Jump Process where some of the attempts to jump from one state to another fail, and
where the failure probability increases as a function of the “distance” between the states.
Let φ be defined as in the last section, and let β, θ and π be defined as in the Normalized Gamma Process
representation of the ordinary HDP-HMM. That is,
β ∼ GEM(γ)

(1)

i.i.d

θj ∼ H
π

jj 0

(2)

| β, θ ∼ Gamma(αβ , 1)
j0

(3)

0

Now suppose that when the process is in state j, jumps to state j are made at rate πjj 0 . This defines a continuoustime Markov Process where the off-diagonal elements of the transition rate matrix are the off diagonal elements
of π. In addition, self-jumps are allowed, and occur with rate πjj . If we only observe the jumps and not the
durations between jumps, this is an ordinary Markov chain, whose transition matrix is obtained by appropriately
normalizing π. If we do not observe the jumps themselves, but instead an observation is generated once per
jump from a distribution that depends on the state being jumped to, then we have an ordinary HMM.
We modify this process as follows. Suppose that each jump attempt from state j to state j 0 has a chance of failing,
which is an increasing function of the “distance” between the states. In particular, let the success probability be
φjj 0 (recall that we assumed above that 0 ≤ φjj 0 ≤ 1 for all j, j 0 ). Then, the rate of successful jumps from j to
j 0 is πjj 0 φjj 0 , and the corresponding rate of unsuccessful jump attempts is πjj 0 (1 − φjj 0 ). To see this, denote by
Njj 0 the total number of jump attempts to j 0 in a unit interval of time spent in state j. Since we are assuming
the process is Markovian, the total number of attempts is Poisson(πjj 0 ) distributed. Conditioned on Njj 0 , njj 0
will be successful, where
njj 0 | Njj 0 ∼ Binom(Njj 0 , φjj 0 )
(4)
It is easy to show (and well known) that the marginal distribution of njj 0 is Poisson(πjj 0 φjj 0 ), and the marginal
distributionPof q̃jj 0 := Njj 0 − njj 0 is Poisson(πjj 0 (1 − φjj 0 )). The rate of successful jumps from state j overall is
then Tj := j 0 πjj 0 φjj 0 .
Let t index jumps, so that zt indicates the tth state visited by the process (couting self-jumps as a new time
step). Given that the process is in state j at discretized time t − 1 (that is, zt−1 = j), it is a standard property of
1

Markov Processes that the probability that the first successful jump is to state j 0 (that is, zt = j 0 ) is proportional
to the rate of successful attempts to j 0 , which is πjj 0 φjj 0 .
Let ũt indicate the time elapsed between the tth and and t − 1th successful jump (where we assume that the
first observation occurs when the first successful jump from a distinguished initial state is made). We have
ũt | zt−1 ∼ Exp(Tzt−1 )

(5)

where ũt is independent of zt .
During this period, there will be q̃j 0 t unsuccessful attempts to jump to state j 0 , where
q̃j 0 t | zt−1 ∼ Poisson(ũt πzt−1 j 0 (1 − φzt−1 j 0 ))

(6)

Define the following additional variables
Tj = {t | zt−1 = j}
X
qjj 0 =
q̃j 0 t

(7)
(8)

t∈Tj

uj =

X

ũt

(9)

t∈Tj

and let Q = (qjj 0 )j,j 0 ≥1 be the matrix of unsuccessful jump attempt counts, and u = (uj )j≥1 be the vector of
the total times spent in each state.
Since each of the ũt with t ∈ Tj are i.i.d. Exp(Tj ), we get the marginal distribution
ind

uj | z, π, φ ∼ Gamma(nj· , Tj )

(10)

by the standard property that sums of i.i.d. Exponential distributions has a Gamma distribution with shape
equal to the number of variates in the sum, and rate equal to the rate of the individual exponentials. Moreover,
since the q̃j 0 t with t ∈ Tj are Poisson distributed, the total number of failed attempts in the total duration uj is
ind

qjj 0 ∼ Poisson(uj πjj 0 (1 − φjj 0 )).

(11)

Thus if we marginalize out the individual ũt and q̃j 0 t , we have a joint distribution over z, u, and Q, conditioned
on the transition rate matrix π and the success probability matrix φ, which is
!
T
Y
Y
Y
p(qjj 0 | uj πjj 0 , φjj 0 )
(12)
p(z, u, Q | π, φ) =
p(zt | zt−1 )
p(uj | z, π, φ)

=

t=1

j

Y πzt−1 zt φzt−1 zt

!

t

j0

Y

n
Tj j·

n −1

uj j· e−Tj uj
Tzt−1
Γ(n
)
j·
j
Y
q
q 0
0
×
e−uj πjj0 (1−φjj0 ) uj jj πjjjj0 (1 − φjj 0 )qjj0 (qjj 0 !)−1

(13)
(14)

j0

=

Y

n +qj· −1

Γ(nj· )−1 uj j·

(15)

j

×

Y

n

πjjjj0

0 +qjj 0

n

0

φjjjj0 (1 − φjj 0 )qjj0 e−πjj0 φjj0 uj e−πjj0 (1−φjj0 )uj (qjj 0 !)−1

(16)

j0

=

Y
j

n +qj· −1

Γ(nj· )−1 uj j·

Y

n

πjjjj0

0 +qjj 0

n

0

φjjjj0 (1 − φjj 0 )qjj0 e−πjj0 uj (qjj 0 !)−1

(17)

j0

Setting aside terms that do not depend on π, we get the conditional likelihood function used in sampling π:
Y Y n 0 +q 0
jj
p(z, u, Q | π, φ) ∝
πjjjj0
e−πjj0 uj
(18)
j

j0

which, combined with the independent Gamma priors on π yields conditionally independent Gamma posteriors:
ind.

πjj 0 | z, u, Q, β, α ∼ Gamma(αβj 0 + njj 0 + qjj 0 , 1 + uj )

(19)

2. Inference details for hyperparameters of the rescaled HDP
2.1. Sampling π, β, α and γ
The joint conditional over γ, α, β and π given the augmented data D = (z, u, Q, M, r, w) factors as
p(γ, α, β, π | D) = p(γ | D)p(α | D)p(β | γ, D)p(π | α, β, D)

(20)

We will derive these four factors in reverse order.
Sampling π

The entries in π are conditionally independent given α and β, so we have the prior
YY
αβ 0 −1
p(π | β, α) =
Γ(αβj 0 )−1 πjj 0 j
exp(−πjj 0 ),

(21)

j0

j

and the likelihood given {z, u, Q} given by (17). Combining these, we have
Y n +q −1 Y
αβ 0 +n 0 +q 0 −1
Γ(αβj 0 )−1 πjj 0 j jj jj
uj j· j·
p(π, z, u, Q | β, α, φ) =

(22)

j0

j

n

0

× e−(1+uj )πjj0 φjjjj0 (1 − φjj 0 )qjj0 (qjj 0 !)−1

(23)

Conditioning on everything except π, we get
p(π | Q, u, z, β, α) ∝

YY

αβ 0 +njj 0 +qjj 0 −1

πjj 0 j

exp(−(1 + uj )πjj 0 )

(24)

j0

j

and thus we see that the πjj 0 are conditionally independent given u, z and Q, and distributed according to
ind

πjj 0 | njj 0 , qjj 0 , βj 0 , α ∼ Gamma(αβj 0 + njj 0 + qjj 0 , 1 + uj )
Sampling β

(25)

Consider the conditional distribution of β having integrated out π. The prior density of β is
p(β | γ) =

Γ(γ) Y Jγ −1
β
Γ( Jγ )J j j

(26)

After integrating out π in (22), we have
p(z, u, Q | β, α, γ, φ) =

J
Y
j=1

u−1
j

J
Y

unjj0 +qjj0 −1 (1 + uj )−(αβj0 +njj0 +qjj0 )

(27)

j 0 =1

Γ(αβj 0 + njj 0 + qjj 0 ) njj0
φjj 0 (1 − φjj 0 )qjj0 (qjj 0 !)−1
Γ(αβj 0 )

nj· +qj·
J
Y
uj
−1 −1
−α
=
Γ(nj· ) uj (1 + uj )
1 + uj
j=1
×

×

J
Y
Γ(αβj 0 + njj 0 + qjj 0 ) njj0
φjj 0 (1 − φjj 0 )qjj0 (qjj 0 !)−1
Γ(αβj 0 )
0

(28)
(29)

(30)

j =1

where we have used the fact that the βj sum to 1. Therefore
p(β | z, u, Q, α, γ) ∝

J
Y
j=1

γ

βjJ

−1

J
Y
Γ(αβj 0 + njj 0 + qjj 0 )
.
Γ(αβj 0 )
0

j =1

(31)

Following (Teh et al., 2006), we can write the ratios of Gamma functions as polynomials in βj , as
p(β | z, u, Q, α, γ) ∝

J
Y

γ
J

βj

−1

njj 0
J
Y
X
j 0 =1

j=1

s(njj 0 + qjj 0 , mjj 0 )(αβj 0 )mjj0

(32)

mjj 0 =1

where s(m, n) is an unsigned Stirling number of the first kind, which is used to represent the number of permutations of n elements such that there are m distinct cycles.
This admits an augmented data representation, where we introduce a random matrix M = (mjj 0 )1≤j,j 0 ≤J , whose
entries are conditionally independent given β, Q and z, with
s(njj 0 + qjj 0 , m)αm βjm0
p(mjj 0 = m | βj 0 , α, njj 0 , qjj 0 ) = Pn 0 +q 0
0
jj
jj
s(njj 0 + qjj 0 , m0 )αm0 βjm0
m0 =0

(33)

for integer m ranging between 0 and njj 0 + qjj 0 . Note that s(n, 0) = 0 if n > 0, s(0, 0) = 1, s(0, m) = 0 if m > 0,
and we have the recurrence relation s(n + 1, m) = ns(n, m) + s(n, m − 1), and so we could compute each of
these coefficients explicitly; however, it is typically simpler and more computationally efficient to sample from
this distribution by simulating the number of occupied tables in a Chinese Restaurant Process with n customers,
than it is to enumerate its probabilities.
For each mjj 0 we simply draw njj 0 assignments of customers to tables according to the Chinese Restaurant
Process and set mjj 0 to be the number of distinct tables realized; that is, assign the first customer to a table,
setting mjj 0 to 1, and then, after n customers are assigned, assign the n + 1th customer to a new table with
probability αβj 0 /(n+αβj 0 ), in which case we increment mjj 0 , and to an existing table with probability n/(n+α),
in which case we do not increment mjj 0 .
Then, we have joint distribution
p(β, M | z, u, Q, α, γ) ∝

J
Y

γ

βjJ

−1

J
Y

m

s(njj 0 + qjj 0 , mjj 0 )αmjj0 βj 0 jj

0

(34)

j 0 =1

j=1

which yields (32) when marginalized over M. Again discarding constants in β and regrouping yields
p(β | M, z, u, θ, α, γ) ∝

J
Y

γ

βjJ0

+m·j 0 −1

(35)

j 0 =1

which is Dirichlet:
β | M, γ ∼ Dirichlet(
Sampling α and γ

γ
γ
+ m·1 , . . . , + m·J )
J
J

(36)

Assume that α and γ have Gamma priors, parameterized by shape, a and rate, b:
baαα aα −1
α
exp(−bα α)
Γ(aα )
a
bγγ aγ−1
p(γ) =
γ
exp(−bγ γ)
Γ(aγ )

(37)

p(α) =

(38)

Having integrated out π, we have

nj· +qj·
J
Γ(γ) m·· Y Jγ +m·j −1
uj
−1 −1
−α
p(β, z, u, Q, M | α, γ, φ) =
α
βj
Γ(nj· ) uj (1 + uj )
Γ( Jγ )J
1 + uj
j=1
×

J
Y
j 0 =1

n

0

s(njj 0 + qjj 0 , mjj 0 )φjjjj0 (1 − φjj 0 )qjj0 (qjj 0 !)−1

(39)

(40)

We can also integrate out β, to yield
Γ(γ)
Γ(γ + m·· )

nj· +qj·
Y Γ( γ + m·j )
uj
−1
J
u
×
Γ( Jγ )Γ(nj· ) j
1 + uj
j

p(z, u, Q, M | α, γ, φ) = αm·· e−

P

j 00

×

log(1+uj 00 )α

J
Y

n

(41)
(42)

0

s(njj 0 + qjj 0 , mjj 0 )φjjjj0 (1 − φjj 0 )qjj0 (qjj 0 !)−1

(43)

j 0 =1

demonstrating that α and γ are independent given φ and the augmented data, with
X
p(α | z, u, Q, M) ∝ αaα +m·· exp(−(bα +
log(1 + uj ))α)

(44)

j

and
p(γ | z, u, Q, M) ∝ γ

aγ−1

exp(−bγ γ)

QJ

γ
j=1 Γ( J
Γ( Jγ )J Γ(γ +

Γ(γ)

+ m·j )

(45)

m·· )

So we see that
α | z, u, Q, M ∼ Gamma(aα + m·· , bα +

X

log(1 + uj ))

(46)

j

To sample γ, we introduce a new set of auxiliary variables, r = (r1 , . . . , rJ ) and w with the following distributions:
 γ r
Γ( Jγ )
0
s(m
,
r)
·j
Γ( Jγ + m·j 0 )
J
Γ(γ + m·· ) γ−1
p(w | m·· γ) =
w
(1 − w)m·· −1
Γ(γ)Γ(m·· )

p(rj 0 = r | m·j 0 , γ) =

r = 1, . . . , m·j
w ∈ (0, 1)

(47)
(48)

so that
p(γ, r, w | M) ∝ γ aγ−1 exp(−bγ γ)wγ−1 (1 − w)m·· −1

J
Y

s(m·j 0 , rj 0 )

j 0 =1

 γ  rj 0
J

(49)

and
p(γ | r, w) ∝ γ aγ +r· −1 exp(−(bγ − log(w))γ),

(50)

γ | r, w, z, u, Q, M ∼ Gamma(aγ + r· , bγ − log(w))

(51)

which is to say

3. Derivation of η update in the Cocktail Party and Synthetic Data Experiments
In principle, η can have any distribution over binary vectors, but we will suppose for simplicity that it can be
factored into D independent coordinate-wise Bernoulli variates. Let µd be the Bernoulli parameter for the dth
coordinate.
The similarity function φjj 0 is the Laplacian kernel:
φjj 0 = Φ(ηj , ηj 0 ) = exp(−λdjj 0 )

(52)

PD
where djj 0 d = |ηjd − ηj 0 d | is Hamming distance in the dth coordinate, djj 0 := d=1 djj 0 is the total Hamming
distance between ηj and ηj 0 , and λ ≥ 0 (if λ = 0, the φjj 0 are identically 1, and so do not have any influence,
reducing the model to an ordinary HDP-HMM).

Let
φjj 0 −d = exp(−λ(djj 0 − djj 0 d ))

(53)

so that φjj 0 = φjj 0 −d e−λdjj0 d .
Since the matrix φ is assumed to be symmetric, we have
Y e−λ(njj0 +nj0 j )|1−θj0 d | (1 − φjj 0 −d e−λ|1−θj0 d | )qjj0 +qj0 j
p(z, Q | ηjd = 1, η \ ηjd )
∝
p(z, Q | ηjd = 0, η \ ηjd )
e−λ(njj0 +nj0 j )|θj0 d | (1 − φjj 0 −d e−λ|θj0 d | )qjj0 +qj0 j
j 0 6=j
θ 0
j d
Y  1 − φjj 0 −d e−λ (−1) (qjj0 +qj0 j )
−λ(cjd0 −cjd1 )
=e
1 − φjj 0 −d
0

(54)

(55)

j 6=j

where cjd0 and cjd1 are the number of successful jumps to or from state j, to or from states with a 0 or 1,
respectively, in position d. That is,
X

cjd0 =
{j 0

njj 0 + nj 0 j

X

cjd1 =
{j 0

| θj 0 d =0}

njj 0 + nj 0 j

(56)

| θj 0 d =1}

Therefore, we can Gibbs sample ηjd from its conditional posterior Bernoulli distribution given the rest
where we compute the Bernoulli parameter via the log-odds




p(ηjd = 1)p(z, Q | ηjd = 1, η \ ηjd )p(Y | z, ηjd = 1, η \ ηjd )
p(ηjd = 1 | Y, z, Q, η \ ηjd )
= log
log
p(ηjd = 0 | Y, z, Q, η \ ηjd )
p(ηjd = 0)p(z, Q | ηjd = 0, η \ ηjd )p(Y | z, ηjd = 0, η \ ηjd )
!


(−d)
X
1 − φjj 0 e−λ
µd
θj 0 d
= log
+ (cjd1 − cjd0 )λ +
(−1) (qjj 0 + qj 0 j ) log
(−d)
1 − µd
1 − φjj 0
j 0 6=j


X
f (yt ; ηjd = 1, ηj \ ηjd )
+
log
f (yt ; ηjd = 0, ηj \ ηjd )

of η,

(57)
(58)
(59)

{t | zt =j}

Suppose also that the observed data Y consists of a T × K matrix, where the tth row yt = (yt1 , . . . , ytK )T is a
K-dimensional feature vector associated with time t, and let W be a D × K weight matrix with kth column wk ,
such that
f (yt ; ηj ) = g(yt ; WT ηj )
(60)
for a suitable parametric function g. We assume for simplicity that g factors as
T

g(yt ; W ηj ) =

K
Y

gk (ytk ; wk · ηj )

(61)

k=1
(−d)

Define xtk = wk · θzt , and xtk
coordinate removed. Then

log

= wk−d · θz−d
, where θj−d and wk−d are θj and wk , respectively, with the dth
t

f (yt ; ηjd = 1, ηj \ ηjd )
f (yt ; ηjd = 0, ηj \ ηjd )


=

K
X

(−d)

log

gk (ytk ; xtk

k=1

+ wdk )

(−d)

!
.

(62)

gk (ytk ; xtk )

If gk (y; x) is a Normal density with mean x and unit variance, then
(−d)

log

gk (ytk ; xtk

+ wdk )

(−d)
gk (ytk ; xtk )

!
(−d)

= −wdk (ytk − xtk

1
+ wdk )
2

(63)

4. Derivation of HMC update for ` in the Bach Chorale Experiment
We have a set of states with parameters `j , j = 1, . . . , J. In the previous version of the model, `j was a binary
state vector on which both the similarities φjj 0 and the emission distribution Fj depended. Here, we define the
latent locations `j = (`j1 , `jD ) to be locations in RD , independent of the emission distributions, so that during
inference they are informed solely by the transitions.
We set


λ
φjj 0 (`j , `j 0 ) = exp − d2jj 0
2



where djj 0 is the Euclidean distance between `j and `j 0 ; that is,
d2jj 0 =

X
(`jd − `j 0 d )2
d

Since now `j are continuous locations, we use Hamlitonian Monte Carlo (Duane et al., 1987; Neal et al., 2011) to
sample them jointly. HMC is a variation on Metropolis-Hastings algorithm which is designed to more efficiently
explore a high-dimensional continuous distribution by adopting a proposal distribution which incorporates an
auxiliary “momentum” variable to make it more likely that proposals will go in useful directions and improve
mixing compared to naive movement.
To do Hamiltonian Monte Carlo to sample from the conditional posterior of ` given z and Q, we need to compute
the gradient of the log posterior, which is just the sum of the gradient of the log prior and the gradient of the
log likelihood.
Assume independent and isotropic Gaussian priors on each `j , so we have
h` X 2
`jd
p(`j ) ∝ exp −
2

!
,

d

where h` is the prior precision which does not depend on d.
Then the log prior density, up to an additive constant c, is
h` X 2
`jd
2

log p(`j ) = c −

d

The relevant log likelihood is the log of the probability of the z and Q variables given the φjj 0 . In particular, we
have
YY n 0
L := p(z, Q | φ) ∝
φjjjj0 (1 − φjj 0 )qjj0
j

j0

so that
log L =

XX
j

(njj 0 log(φjj 0 ) + qjj 0 log(1 − φjj 0 ))

j0

The j, d coordinate of the gradient of the log prior is simply −h` `jd .
To get the j, d coordinate of the gradient of the log likelihood, we can apply the chain rule to terms as is
convenient. In particular,
XX
∂L
∂ log(φjj 0 ) ∂d2jj 0 X X
∂ log(1 − φjj 0 ) ∂(1 − φjj 0 ) ∂d2jj 0
0
=
njj 0
+
q
jj
∂`jd
∂d2jj 0
∂`jd
∂(1 − φjj 0 )
∂d2jj 0
∂`jd
0
0
j
j
j

j

We have the following components:
λ
∂ log(φjj 0 )
=−
∂d2jj 0
2
∂d2jj 0
= 2djj 0 d I(j 6= j 0 )
∂`jd
∂ log(1 − φjj 0 )
1
=
∂(1 − φjj 0 )
1 − φjj 0
∂(1 − φjj 0 )
λ
= φjj 0
∂d2jj 0
2
which yields
XX
XX
∂L
φjj 0
njj 0 djj 0 d I(j 6= j 0 ) + λ
qjj 0 djj 0 d
= −λ
I(j 6= j)
∂`jd
1 − φjj 0
j
j
j0
j0


X
φjj 0
= −λ
djj 0 d njj 0 − qjj 0
1 − φjj 0
0
0
(j,j ):j6=j

References
Duane, Simon, Kennedy, Anthony D, Pendleton, Brian J, and Roweth, Duncan. Hybrid monte carlo. Physics
letters B, 195(2):216–222, 1987.
Neal, Radford M et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 2:
113–162, 2011.
Teh, Yee Whye, Jordan, Michael I, Beal, Matthew J, and Blei, David M. Hierarchical Dirichlet processes. Journal
of the American Statistical Association, 101(476), 2006.

