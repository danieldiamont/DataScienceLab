Analytical Guarantees on Numerical Precision of Deep Neural Networks

Supplementary Material
The main purpose of this supplementary section is to provide proofs for Theorems 1 and 2.

Preliminaries

where both inequalities are due to the union bound.
The next result is also straightforward, but quite useful in
obtaining upper bounds that are fully determined by averages.

Here we shall give a proof of (1) as well as preliminary
results that will be needed to complete the proofs of Theorems 1 and 2.

Proposition 3. Given a random variable X and an event
E, we have:

Proposition 1. The fixed point error probability pe,f x is
upper bounded as shown in (1).

E [X · 1E ] = E [X|E] Pr(E)

(16)

where 1E denotes the indicator function of the event E.

Proof. From the definitions of pe,f x , pe,f l , and pm ,
pe,f x

Proof. By the law of total expectation,

= Pr{Ŷf x 6= Y }
= Pr{Ŷf x 6= Y, Ŷf x = Ŷf l } + Pr{Ŷf x 6= Y, Ŷf x 6= Ŷf l }
= Pr{Ŷf l 6= Y, Ŷf x = Ŷf l } + Pr{Ŷf x 6= Y, Ŷf x 6= Ŷf l }
≤ pe,f l + pm .

Next is a simple result that allows us to replace the problem of upper bounding pm by several smaller and easier
problems by virtue of the union bound.
Proposition 2. In a M -class classification problem, the total mismatch probability can be upper bounded as follows:
pm ≤

E [X · 1E ]
= E [X · 1E | E] Pr(E) + E [X · 1E | E c ] Pr(E c )
= E [X · 1 | E] Pr(E) + E [X · 0 | E c ] Pr(E c )
= E [X|E] Pr(E).

Proof of Theorem 1
Let us define pm,j→i for i 6= j as follows.
pm,j→i = Pr{Ŷf x = i | Ŷf l = j}

M
M
X
X
j=1 i=1,i6=j

We first prove the following Lemma.
(15)

Proof.

pm = Pr(Ŷf x 6= Ŷf l ) = Pr 

M
[



Lemma 1. Given BX and BF , if the output of the floatingpoint network is Ŷf l = j, then that of the fixed-point network would be Ŷf x = i with a probability upper bounded
as follows:

(Ŷf x 6= j, Ŷf l = j)



j=1

≤

M
X

pm,j→i ≤
Pr(Ŷf x 6= j, Ŷf l = j)

∆2A
24

j=1

=

M
X

Pr(Ŷf x 6= j|Ŷf l = j) Pr(Ŷf l = j)

=

j=1

≤


Pr 

M
[

i=1,i6=j

M
M
X
X
j=1 i=1,i6=j

Ŷf x




= iŶf l = j  Pr(Ŷf l = j)

Pr(Ŷf x = i|Ŷf l = j) Pr(Ŷf l = j)

+

∆2W
24




 ∂(Zi −Zj ) 2 



h∈A
∂Ah
Ŷf l = j 


2
|Zi − Zj |

P

E



j=1
M
X

(17)

Pr(Ŷf x = i|Ŷf l = j) Pr(Ŷf l = j)

E





 ∂(Zi −Zj ) 2 



h∈W
∂wh
Ŷf l = j 
 . (18)

|Zi − Zj |2

P

Proof. We can claim that, if i 6= j:
pm,j→i ≤ Pr{Zi + qZi > Zj + qZj | Ŷf l = j}
where the equality holds for M = 2.

(19)

Analytical Guarantees on Numerical Precision of Deep Neural Networks

Lemma 2. Given BA and BW , pm,j→i is upper bounded
as follows:

From the law of total probability,
pm,j→i
Z


≤ fX (x) Pr zi + qzi > zj + qzj | Ŷf l = j, x dx,

"

Y sinh (T · DA,h )
·
T · DA,h
h∈A
#
Y sinh (T · DW,h ) 
Ŷf l = j

T · DW,h

pm,j→i ≤ E e−T ·V

(20)
where x denotes the input of the network, or equivalently
an element from the dataset and fX () is the distribution of
the input data. But for one specific x given Ŷf l = j, we
have:
Pr zi + qzi > zj + qzj






1
= Pr qzi − qzj  > |zj − zi |
2

where the 12 term is due to the symmetry of the distribution
of the quantization noise around zero per output. By (7),
we can claim that
qzi − qzj =

X

qah

h∈A

X
∂(zi − zj )
∂(zi − zj )
+
qwh
.
∂ah
∂wh
h∈W
(21)

Note that qzi − qzj is a zero mean random variable with the
following variance

where T =
DA,h =

∆A
2

3VP
, V = Zj −
∆2A,h + h∈W ∆2W,h
∂(Zi −Zj )
∂(Zi −Zj )
, and DW,h = ∆2W · ∂W
.
∂Ah
h
P

·

h∈A

h∈A

h∈W

h
i

Pr qzi − qzj > v ≤ e−tv E et(qzi −qzj )
for any t > 0. Because quantizations noise terms are independent, by (21),
h
i
Y
E et(qzi −qzj ) =

0

h

E etqah dah

i Y

∂(z −z )

0

h

=

where dah


+

∆2W
24

E




 ∂(Zi −Zj ) 2

h∈W 
∂wh

P

|Zi − Zj |2



1Ŷf l =j 


(23)

1
∆A

Z

∆A
2

sinh (tdwh )
tdwh

=

−

∆A
2

∂(zi −zj )
∂wh .



td0ah ∆A
2



sinh (tdah )
tdah

d0a ∆A
h
.
2

where dwh =

Similarly,
d0w ∆W
h
2

0

h

E etqwh dwh

We start with the following lemma.

i

=

.

Hence,

Pr qzi − qzj > v
Y sinh (tda,h ) Y sinh (tdw,h )
≤ e−tv
.
tda,h
tdw,h
h∈A

h∈W

−tv +

X

ln sinh (tda,h ) − ln (tda,h )



h∈A

Proof of Theorem 2

Also,

0

By taking logarithms, the right-hand-side is given by
which can be simplified into (8) in Theorem 1.

i

etqah dah dqah

2
= 0
sinh
tdah ∆A

From (20) and (22), we can derive (18).

i=1,i6=j

i

E etqah dah =

0

h

E etqwh dwh

h∈W

i
j
and d0wh =
where d0ah =
∂ah
h
i
0
E etqah dah is given by

By Chebyshev’s inequality, we obtain

Pr zi + qzi > zj + qzj




P
P
 ∂(zi −zj ) 2
 ∂(zi −zj ) 2
2
+
∆
∆2A h∈A  ∂a


W
h∈W
∂wh 
h
.
≤
2
24 |zi − zj |
(22)

Plugging (18) of Lemma 1 into (15) and using (16),





P
 ∂(Zi −Zj ) 2
M
M
2


X
X
∂Ah
 ∆A  h∈A
pm ≤
E
1Ŷf l =j 


2
24
|Z
−
Z
|
i
j
j=1

Zi ,

Proof. The setup is similar to that of Lemma 1. Denote
v = zj − zi . By the Chernoff bound,

h∈A





X  ∂(zi − zj ) 2 ∆2 X  ∂(zi − zj ) 2

 + W


 ∂ah 
 ∂wh  .
12
12

∆2A

(24)

h∈W

+

X
h∈W


ln sinh (tdw,h ) − ln (tdw,h ) .

(25)

Analytical Guarantees on Numerical Precision of Deep Neural Networks

This term corresponds to a linear function of t added to a
sum of log-moment generating functions. It is hence convex in t. By taking derivative with respective to t and setting to zero,
v+

X
X
|A| + |W|
da,h
dw,h
=
+
.
t
tanh(tda,h )
tanh(tdw,h )
h∈A

h∈W

But tanh(x) = x − 31 x3 + o x , so dropping fifth order
terms yields:

5

v+

|A| + |W|
=
t
X
1
h∈A

t(1 −

(tda,h
3

)2

+
)

X

1

h∈W

(tdw,h )2
)
3

t(1 −

.

Note, for the terms inside the summations, we divided numerator and denominator by da,h and dw,h , respectively,
then factored the denominator by t. Now, me multiply both
sides by t to get:
tv+ |A| + |W| =
X
1
1−

h∈A

(tda,h )2
3

+

1

X
h∈W

1−

(tdw,h )2
3

.

1
2
Also 1−x
+ o(x4 ), so we drop fourth order
2 = 1 + x
terms:

tv + |A| + |W|
 X

X
(tda,h )2
(tdw,h )2
=
1+
+
1+
3
3
h∈A

h∈W

which yields:
3v
P
2+
2
(d
)
a,h
h∈A
h∈W (dw,h )

t= P

(26)

By plugging (25) into (26) and using the similar method of
Lemma 1, we can derive (24) of Lemma 2.
Theorem 2 is obtained by plugging (24) of Lemma 2 into
(i,j)
(15) and using (16). Of course, DAh is the random variable of da,h when ŷf x = i and ŷf l = j, and the same ap(i,j)
plies to Dwh and dw,h . We dropped the superscript (i, j)
in the Lemma as it was not needed for the consistency of
the definitions.

