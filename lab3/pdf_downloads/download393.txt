Supplemental Materials for: Stochastic
Gradient MCMC Methods for Hidden Markov
Models
Yi-An Ma, Nicholas J. Foti, Emily B. Fox

1

Gradient of the Posterior

For the hidden Markov model (HMM), the posterior distribution of all hyperparameters θ can be calculated by the Bayes rule, where
p(θ|y) ∝ p(y|θ)p(θ).
Since
p(y, x|θ) = π0 (x0 )

T
Y
t=1

Axt ,xt−1 ·

T
Y

p(yt |xt ),

t=1

where y = (y1 , · · · yT ) denotes the data as real valued vector, and x = (x1 , · · · xT )
as discrete valued vector with xt ∈ {1, · · · K}, ∀t. We can directly marginalize
out the hidden variables, x, with matrix multiplication as
p(y|θ) = 1T
T P (yT )A · · · P (y1 )A π 0 ,
where P (yT ) is a diagonal matrix and Pi,j (yt ) = p(yt |xt = i)δi,j ; 1T
T = (1, · · · , 1)
T
is a row vector of k ones ( denotes transpose); (π 0 )i = π0 (x0 = i). Hence the
same as Eq. (2), (3) and (8) of the main paper, the posterior distribution is:
p(θ|y) = 1T
T P (yT )A · · · P (y1 )A π 0 · p(θ).
When we divide the whole sequence into subsequences of
yτ,L = (yτ −L , . . . , yτ , . . . , yτ +L ),
1

the posterior can be rewritten as:
Y

p(θ|y) ∝ 1T

P (yτ,L )π 0 · p(θ),

(1)

yτ,L ∈S

where S is the minimum set of yτ,L covering y.
We can then use gradient information of the posterior distribution to construct
MCMC algorithms. The gradient of the log-posterior distribution is:
∂ ln p(θ|y)
=
∂θi

∂ (P (yτ,L )A)
· · · P (y1,L )Aπ 0
∂ ln p(θ)
∂θi
+
.
T
1 P (y|S|,L )A · · · P (yτ,L )A · · · P (y1,L )Aπ 0
∂θi

|S| 1T P (y|S|,L )A · · ·
X
τ =1

T
Denote qT
τ +L+1 = 1T P (yT )A · · · P (yt+1 )A and π τ −L−1 = P (yt−1 )A · · · P (y1 )Aπ 0 .
Then

∂U (θ)
∂ ln p(y|θ) ∂p(θ)
=−
−
∂θi
∂θi
∂θi
∂P (yτ )
T
X qτ +L+1 ∂θ π τ −L−1 ∂ ln p(θ)
i
−
,
=−
∂θi
P
(y
qT
τ )π τ −L−1
τ +L+1

(2)

yτ ∈Se

as shown in Eq. (11) of the main paper.

2

Lyapunov Exponent

The question of buffer length is equivalent to: for two random vectors π and π ∗ ,
what’s the expected length of LB such that after the application of P (yLB ), π and
π ∗ will synchronize? This is a question of random dynamical systems and can be
answered through defining the Lyapunov exponent.
We first transform π through stereographic projection into K − 1 dimensions
and denote as: r. Then operator P (yt )A[ · ] is projected to new space and the
equivalent dynamics over r becomes: Fyt . We define the Lyapunov exponent L
through the projected random dynamics Fyt as
Z
L=
ln ||∇r Fy (r)||dµy dµr ,
(3)
Ω×RK−1

2

where y ∈ Ω. Measure µy corresponds to the distribution of the data yt , and µr is
the invariant measure of r under the dynamics of P (yt )A, which will be estimated
through sampling.
Once the Lyapunov exponent L is calculated, we can set the buffer length:
 
δ
1
,
(4)
B = ln
L
δ0
where δ = 10−3 is the error tolerance and δ0 = 2 is the maximum initial error for
probability vectors.

3

Subsequence Sampling Procedure

We use the following sampling procedure to obtain the subsequences used to
compute stochastic gradient estimates. In order to enforce the non-overlapping
mixing-time constraint between adjacent subsequences, we sample them sequene
tially. This
results in the following form for the probability of the minibatch S:
Q
R−1
e =
p(S)
n=0 L/|Sn |, where |S0 | = T , |Sn | = |Sn−1 | − (ν + 2B + 2L) − Loverlap .
The quantity Loverlap is calculated as follows:
L0overlap = |τn |,
LToverlap = |T − τn |,
LLoverlap =
LR
overlap =

n−1

min

n0 =1,τn0 <τn
n−1

min

n0 =1,τn0 >τn

{|τn − τn0 |} − L − B,
{|τn − τn0 |} − L − B.

If min{L0overlap , LToverlap , LLoverlap , LR
overlap } ≥ 2ν + 3L + 3B, the minimum number
of observations required to fit an entire subsequence while respecting minimum
gap ν, Loverlap = 0. Otherwise, Loverlap equals to the sum of all the above terms
that are less than 2ν + 3L + 3B.
e provides the correct probability of the miniSince T  L, B, ν, then p(S)
e
batch S.

3

|| A − Atrue ||F

Diagonally Dominant

Reversed Cycles

3

3

2

2

1

1

0
0

200
runtime (sec)

0
0

400

100
200
300
runtime (sec)

Figure 1: Synthetic experiments with hard-to-capture dynamics. Diagonally dominant
(DD) (left) and reversed cycles (RC) (right) experiments. First Row: The emission distributions corresponding to 8 different states. Arrows in the RC case indicate the Markov
transition structure with transition between bridge states as dashed arrows. Second Row:
Decrease of error in transition matrix estimation versus runtime. Comparisons are made
for SG-RLD algorithms with estimated buffer, without buffer, and treating data as i.i.d.
All of the experiments use a constant computation budget by varying the number of subchains, |S̃|, with the length of the subchains, L.

4
4.1

Detailed Descriptions of Experiments
Evaluating Buffer Effectiveness

The first data set, diagonally dominant (DD) consists of a Markov chain that
heavily self-transitions. Most subchains in a minibatch thus contain redundant information with observations generated from the same latent state. Although
transitions are rarely observed, the emission means are set to be distinct so that
4

this example is likelihood-dominated and highly identifiable. See Fig. 1 (top left).
e = 10 subsequences in order to incorpoFor this data we choose L = 2 and |S|
rate observations from distant parts of the observation sequence. This corresponds
to an extreme setting where each gradient is based only on 5 observations. The
transition matrix and emission parameters used for this experiment were:


ADD






=






.999 .001 0
0
0
0
0
0
0 .999 .001 0
0
0
0
0
0
0 .999 .001 0
0
0
0
0
0
0 .999 .001 0
0
0
0
0
0
0 .999 .001 0
0
0
0
0
0
0 .999 .001 0
0
0
0
0
0
0 .999 .001
.001 0
0
0
0
0
0 .999







.






µDD = {(0, 20); (20, 0); (−30, −30); (30, −30); (−20, 0); (0, −20); (30, 30); (−30, 30); }
and ΣDD = I for all states.
The second dataset we consider contains two reversed cycles (RC): the Markov
chain strongly transitions from states 1 → 2 → 3 → 1 and 5 → 7 → 6 → 5 with
a small probability of transiting between cycles via bridge states 4 and 8. See
Fig. 1 (top right). The emission means for the two cycles are very similar but
occur in reverse order with respect to the transitions. The emission variance is
larger, making states 1 and 5, 2 and 6, 3 and 7 indiscernible by themselves. Transition information in observing long enough dynamics is thus crucial to identify
e = 4. Note that
between states 1, 2, 3 and 5, 6, 7. Therefore, we set L = 5 and |S|
same amount of data are used in the calculation of the gradient. The transition
matrix and emission parameters were:


.01 0 .85 0 0
0
0 1
 .99 .01 0 0 0
0
0 0 


 0 .99 0 0 0

0
0
0


 0

0
.15
0
0
0
0
0
.
ARC = 
 0
0
0 1 .01 0 .85 0 


 0

0
0
0
.99
.01
0
0


 0
0
0 0 0 .99 0 0 
0
0
0 0 0
0 .15 0
5

µ = {(−50, 0); (30, −30); (30, 30); (−100, −10); (40, −40); (−65, 0); (40, 40); (100, 10)} ,
and ΣRC = 20 ∗ I for all states.
We use a non-conjugate flat prior to demonstrate the flexibility of our algorithm. We initialize with a short run of k-means clustering to ensure that different
states have different emission parameters.

4.2

Non-conjugate Emission Distribution

For the non-conjugate experiment, we used the following transition matrix:


.1 .9
.
.9 .1
ln(y − µk )2
2σk2
For emission probability, we use a log-normal distribution: pk (y) ∝ e
with parameters: µ1 = 0, µ2 = 4; σ1 = σ2 = 2.
In the non-conjugate model, we use the following priors on the emission parameters: µ1 , µ2 , σ1 , σ2 ∼ N (0, 1).
−

6

