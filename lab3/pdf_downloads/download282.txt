Variational Inference for Sparse and Undirected Models: Appendix

John Ingraham 1 Debora Marks 1

1. Appendix I: PVI algorithm
See Algorithm 1.

2. Appendix II: Experiments
2.1. Spin Models
We generated two synthetic systems. The first system was
ferromagnetic (all J ‚â• 0) with 64 spins, where neighboring spins xi , xj have a nonzero interaction of Jij = 0.2
if adjacent on a 4 √ó 4 √ó 4 periodic lattice. This coupling
strength equates to being slightly above the critical temperature, meaning the system will be highly correlated despite
the underlying interactions being only nearest-neighbor.
The second system was a diluted Sherrington-Kirkpatrick
spin glass (Sherrington & Kirkpatrick, 1975; Aurell & Ekeberg, 2012) with 100 spins. The couplings in this model
were defined by ErdoÃãs-Renyi random graphs (ErdoÃãs &
ReÃÅnyi, 1960)
 non-zero edge weights distributed as
 with
Jij ‚àº N 0, N1p where N p is the average degree. We
generated 5 random systems where the average degree was
N p = 100(0.02) = 2. Across all of the systems, we used
Swendsen-Wang sampling (Swendsen & Wang, 1987) to
sample synthetic data and checked that the sampling was
sufficient to eliminate autocorrelation in the data.
For inference, we tested both L1 -regularized deterministic
approaches as well as a variational approach based on Persistent VI. The L1 regularized approaches included Pseudolikelihood, (PL) (Aurell & Ekeberg, 2012), Minimum
Probability Flow (MPF) (Sohl-Dickstein et al., 2011), and
Persistent Contrastive Divergence (PCD) (Tieleman, 2008).
Additionally, we tested the proposed alternative regularization method of Pseudolikelihood Decimation (Decelle &
Ricci-Tersenghi, 2014).
For L1 regularized Pseudolikelihood and Minimum Probability Flow, we selected the hyperparameter Œª using 10fold cross-validation over 10 logarithmically spaced values
on the interval [0.01, 10]. We performed L1 regulariza1
Harvard Medical School, Boston, Massachusetts. Correspondence to: John Ingraham <ingraham@fas.harvard.edu>, Debora
Marks <debbie@hms.harvard.edu>.

tion of the deterministic objectives using optimizers from
(Schmidt, 2010), and chose the corresponding L1 hyperparameter for PCD + L1 based on the optimal cross-validated
value of Œª that was selected for L1 -regularized Pseudolikelihood.
For the hierarchical model inferred with Persisent VI, we
placed a separate noncentered Horseshoe prior over the
fields and couplings, in accodance with the (centered) hierarchy
sh ‚àº C+ (0, 1),

sJ ‚àº C+ (0, 1),

œÉi ‚àº C+ (0, sh ),

œÉij ‚àº C+ (0, sJ ),

hi ‚àº N (0, œÉi2 ),

2
Jij ‚àº N (0, œÉij
).

where C+ (0, 1) is the standard Half-Cauchy distribution.
We then used PVI-3 with 100 persistent Markov chains
and performed stochastic gradient descent using Adam
(Kingma & Ba, 2014) with default momentum and a learning rate that linearly decayed from 0.01 to 0 over 5 √ó 104
iterations.
2.2. Synthetic Protein Data
We constructed a synthetic Potts spin glass with sparse interactions chosen to reflect an underlying 3D structure. After forming a contact topology from a random packed polymer, we generated synthetic group-Student-t distributed
sitewise bias vectors hi (each 20 √ó 1) and Gaussian distributed coupling matrices Jij (each 20 √ó 20) to mirror
the strong site-bias and weak-coupling regime of proteins.
Since this system is highly frustrated, we thinned 2 √ó 106
sweeps of Gibbs sampling to 2000 sequences that exhibited
no autocorrelation.
Given 400 of the 2000 synthetic sequences1 , we inferred
L2 and group L1 -regularized MAP estimates under a pseudolikelihood approximation with 5-fold cross validation
to choose hyperparameters from 6 values in the range
{0.3, 1.0, 3.0, 10.0, 30.0, 100.0}. We also ran PVI-10 with
40 persistent Markov chains and 5000 iterations of stochastic gradient descent with Adam2 (Kingma & Ba, 2014). We
note that the current standards of the field are based on L2
1

We find this effective sample size to mirror natural protein
families (unpublished)
2
Œ± = 0.01, Œ≤1 = 0.9, Œ≤2 = 0.999, no decay

Variational Inference for Sparse and Undirected Models: Appendix

Algorithm 1 Persistent Variational Inference (PVI-n) with Gaussian q(Œ∏|œÜ)
Require: Model. Undirected p(x|Œ∏) defined by k features {fi (x)}ki=1 on x ‚àà {1, . . . , q}D
Require: Data. Expectations of the features {ED [fi (x)]}ki=1 and sample size N
Require: Prior. Prior gradient ‚àá log P (Œ∏)
Require: Number of Gibbs sweeps n, Markov Chains M , variational samples Q
Require: Initial variational parameters ¬µ0 , log œÉ 0 (e.g. {0, ‚àí3})
// Initialize parameters and Markov chains xÃÉ
¬µ ‚Üê ¬µ0 , log œÉ ‚Üê log œÉ 0
xÃÉ(1:M ) ‚Üê RandInt(1, q)
t‚Üê0
while not converged do
// Estimate ‚àáELBO with Q samples from the variational distribution
‚àá¬µ L ‚Üê 0, ‚àálog œÉ L ‚Üê 0
for s = 1 . . . Q do
 ‚àº N (0, I|¬µ| )
Œ∏ ‚Üê¬µ+œÉ
// Estimate model-dependent expectations E, where Ei = Ep(x|Œ∏) [fi (x)]
E‚Üê0
for m = 1 . . . M do
for j = 1 . . . n do
xÃÉ(m) ‚Üê GibbsSweep(p(x|Œ∏), xÃÉ(m) )
E ‚Üê E + M1n {fi (xÃÉ(m) )}ki=1
end for
end for
// Compute stochastic gradient
G ‚Üê N (ED [fi (x)] ‚àí E) + ‚àá log P (Œ∏)
1
‚àá¬µ L ‚Üê ‚àá¬µ L + Q
G
1
(G  (Œ∏ ‚àí ¬µ) + 1)
‚àálog œÉ L ‚Üê ‚àálog œÉ L + Q
end for
// Update parameters with Robbins-Monro sequence {œÅt }
¬µ ‚Üê ¬µ + œÅt ‚àá ¬µ L
log œÉ ‚Üê log œÉ + œÅt ‚àálog œÉ
t‚Üêt+1
end while

Variational Inference for Sparse and Undirected Models: Appendix

and Group L1 regularized Pseudolikelihood (Balakrishnan
et al., 2011; Ekeberg et al., 2013).

categorical counts that were drawn from a DirichletMultinomial hierarchy with a log-uniform hyperprior over
the (symmetric) concentration parameter Œ±.

2.3. Real Protein Data

Given observed frequencies fi and fj for letters xi and xj
together with a candidate sample size N , we (i) use Bayes‚Äô
theorem to sample underlying distributions pi , pj that produced the observed frequencies, (ii) generate N samples
from the null joint distribution pi pTj , and (iii) compute the
sample Mutual Information of this synthetic count data (Algorithm 2).

2.3.1. S AMPLE REWEIGHTING
Natural protein sequences share a common evolutionary
history that introduces significant redundancy and correlation between related sequences. Treating them as independent data is biased by both the overrepresentation of certain
sequences due to the evolutionary process (phylogeny) or
the human sampling process (biased sequencing of particular species). Thus, we follow a standard practice of correcting the overrepresentation of sequences by a samplereweighting approach (Ekeberg et al., 2013).
Sequence reweighting. If we were to treat all data as independent, then every sample would receive unit weight
in the log likelihood. To correct for the over and underrepresentation of certain sequences, we estimate relative sequence weights using a common inverse neighborhood density based approach from the field (Ekeberg et al.,
2013). We set the relative weight of each sequence proportional to the inverse number of neighboring sequences that
differ by a normalized Hamming distance of less than Œ∏.
We use the established value of Œ∏ = 0.2.
Effective sample size estimation. We propose a new
definition for an effective sample size Nef f of correlated
discrete data and derive an algorithm for estimating it from
count data. The estimator is based on the assumption that
in limited data regimes for sparsely coupled systems, the
sample Mutual Information between random variables is
dominated by random, coincidental correlations rather than
actual correlations due to underlying interactions. This is
consistent with classic results on the bias of information
quantities in limited data regimes known as ‚ÄúMiller Maddow bias‚Äù (Miller, 1955; Paninski, 2003). If we can define
a null model for how such coincidental correlations would
arise for a given random sample of size N , then we define
Nef f as the sample size that matches the expected null MI
to the observed MI.
Ei,j [MInull |Nef f ] = Ei,j [MIdata ]

(1)

The expectation on the right is given by the average sample Mutual Information in the data, while the expectation
on the left will be specific to a null model for Mutual Information Ei,j [M Inull |N ]. Given a noisy estimator for
Ei,j [M Inull |Nef f ], we solve for Nef f by matching the expectations with Robbins-Monro stochastic optimization.
To define the null model of mutual information
Ei,j [M Inull |N ] we treat every variable as independent

We also experimented with using both MAP and posterior
mean estimators as plugin approximations pÃÇi , pÃÇj for the
latent distributions, but found that each of these were biased estimators of the true sample size in simulation. Posterior mode estimates generally underestimated the null entropy (pÃÇi too rough) while the posterior mean overestimated the entropy (pÃÇi too smooth). It seems reasonable
that this would be the behavior of point estimates that do
not account for the uncertainty in the null distributions that
is signaled by the roughness of the frequency data.
We note that this estimator will become invalid as the data
become strong, since the assumption that Mutual Information is dominated by sampling noise will break down.
However, for the real protein data that we examined, we
found that this approach for effective sample size correction was critical for Bayesian methods such as Peristent VI
to be able to set the top level hyperparameters (the sparsity
levels) from the data.
Algorithm 2 Sample the null mutual information as a function of sample size Ei,j [M Inull |N ]
Require: Sample size N
Require: Observed frequencies fi , fj
Sample positions i ‚àà [L], j ‚àà [L] \ i
Set count data Ci ‚Üê N fi , Cj ‚Üê N fj
Sample concentration parameter Œ±i |Ci , Œ±j |Cj with numerical CDF
Sample null distributions pi |Ci , Œ±i , pj |Cj , Œ±j from
Dirichlet
Sample joint count data M(xi , xj ) from categorical joint
distribution pi pTj
1
Compute
sample frequencies
P
P f = N M(xi , xj ), fi =
1
1
xj M(xi , xj ), fj = N
xi M(xi , xj )
N
Compute sample Mutual Information M I
=
P
f (xi ,xj )
f
(x
,
x
)
log
i
j
xi ,xj
fi (xi )fj (xj )

2.3.2. I NFERENCE AND RESULTS
Alignment Our sequence alignment was based on the
Pfam 27.0 family PF00018, which we subsequently processed to remove all sequences with more than 25% gaps.

Variational Inference for Sparse and Undirected Models: Appendix

Indels Natural sequences contain insertions and deletions that are coded by ‚Äògaps‚Äô in alignments. We treated
these as a 21st character (in addition to amino acids) and fit
a q = 21 state Potts model. We acknowledge that, while
this may be standard practice in the field, it is a strong independence approximation because all of the gaps in deletions are perfectly correlated.
Inference We used 10,000 iterations of PVI-10 with 10
variational samples per iteration and 40 persistent Gibbs
chains.
Comparison to 3D structure We collected about 260 3D
structures of SH3 domains referenced on PF00018 (Pfam
27.0) and computed minimum atom distances between all
positions in the Pfam alignment. For each pair i, j, we used
the median of distances across all structures to summarize
the ‚Äútypical‚Äù minimum atom distance between i and j.

References
Aurell, Erik and Ekeberg, Magnus. Inverse ising inference using
all the data. Physical review letters, 108(9):090201, 2012.
Balakrishnan, Sivaraman, Kamisetty, Hetunandan, Carbonell,
Jaime G, Lee, Su-In, and Langmead, Christopher James.
Learning generative models for protein fold families. Proteins: Structure, Function, and Bioinformatics, 79(4):1061‚Äì
1078, 2011.
Decelle, AureÃÅlien and Ricci-Tersenghi, Federico. Pseudolikelihood decimation algorithm improving the inference of the interaction network in a general class of ising models. Physical
review letters, 112(7):070603, 2014.
Ekeberg, Magnus, LoÃàvkvist, Cecilia, Lan, Yueheng, Weigt, Martin, and Aurell, Erik. Improved contact prediction in proteins:
using pseudolikelihoods to infer potts models. Physical Review
E, 87(1):012707, 2013.
ErdoÃãs, Paul and ReÃÅnyi, A. On the evolution of random graphs.
Publ. Math. Inst. Hungar. Acad. Sci, 5:17‚Äì61, 1960.
Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
Miller, George A. Note on the bias of information estimates. Information theory in psychology: Problems and methods, 2(95):
100, 1955.
Paninski, Liam. Estimation of entropy and mutual information.
Neural computation, 15(6):1191‚Äì1253, 2003.
Schmidt, Mark. Graphical model structure learning with l1regularization. PhD thesis, UNIVERSITY OF BRITISH
COLUMBIA (Vancouver, 2010.
Sherrington, David and Kirkpatrick, Scott. Solvable model of a
spin-glass. Physical review letters, 35(26):1792, 1975.
Sohl-Dickstein, Jascha, Battaglino, Peter B, and DeWeese,
Michael R. New method for parameter estimation in probabilistic models: minimum probability flow. Physical review
letters, 107(22):220601, 2011.
Swendsen, Robert H and Wang, Jian-Sheng. Nonuniversal critical
dynamics in monte carlo simulations. Physical review letters,
58(2):86, 1987.
Tieleman, Tijmen. Training restricted boltzmann machines using approximations to the likelihood gradient. In Proceedings
of the 25th international conference on Machine learning, pp.
1064‚Äì1071. ACM, 2008.

