Adaptive Sampling Probabilities for Non-Smooth Optimization

Hongseok Namkoong 1 Aman Sinha 2 Steve Yadlowsky 2 John C. Duchi 2 3

Abstract
Standard forms of coordinate and stochastic gradient methods do not adapt to structure in data;
their good behavior under random sampling is
predicated on uniformity in data. When gradients in certain blocks of features (for coordinate
descent) or examples (for SGD) are larger than
others, there is a natural structure that can be exploited for quicker convergence. Yet adaptive
variants often suffer nontrivial computational
overhead. We present a framework that discovers and leverages such structural properties at a
low computational cost. We employ a bandit optimization procedure that “learns” probabilities
for sampling coordinates or examples in (nonsmooth) optimization problems, allowing us to
guarantee performance close to that of the optimal stationary sampling distribution. When such
structures exist, our algorithms achieve tighter
convergence guarantees than their non-adaptive
counterparts, and we complement our analysis
with experiments on several datasets.

1. Introduction
Identifying and adapting to structural aspects of problem
data can often improve performance of optimization algorithms. In this paper, we study two forms of such structure:
variance in the relative importance of different features and
observations (as well as blocks thereof). As a motivating
concrete example, consider the `p regression problem
(
minimize
x

f (x) := kAx − bkpp =

n
X

)
|aTi x − bi |p

, (1)

i=1

where ai denote the rows of A ∈ Rn×d . When the columns
(features) of A have highly varying norms—say because
1

Management Science & Engineering, Stanford University, USA 2 Electrical Engineering, Stanford University, USA
3
Statistics, Stanford University, USA. Correspondence to:
Hongseok Namkoong <hnamk@stanford.edu>, Aman Sinha
<amans@stanford.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

certain features are infrequent—we wish to leverage this
during optimization. Likewise, when rows ai have disparate norms, “heavy” rows of A influence the objective
more than others. We develop optimization algorithms that
automatically adapt to such irregularities for general nonsmooth convex optimization problems.
Standard (stochastic) subgradient methods (Nemirovski
et al., 2009), as well as more recent accelerated variants for
smooth, strongly convex incremental optimization problems (e.g. Johnson and Zhang, 2013; Defazio et al., 2014),
follow deterministic or random procedures that choose data
to use to compute updates in ways that are oblivious to conditioning and structure. As our experiments demonstrate,
choosing blocks of features or observations—for instance,
all examples belonging to a particular class in classification problems—can be advantageous. Adapting to such
structure can lead to substantial gains, and we propose
a method that adaptively updates the sampling probabilities from which it draws blocks of features/observations
(columns/rows in problem (1)) as it performs subgradient
updates. Our method applies to both coordinate descent
(feature/column sampling) and mirror descent (observation/row sampling). Heuristically, our algorithm learns to
sample informative features/observations using their gradient values and requires overhead only logarithmic in the
number of blocks over which it samples. We show that
our method optimizes a particular bound on convergence,
roughly sampling from the optimal stationary probability
distribution in hindsight, and leading to substantial improvements when the data has pronounced irregularity.
When the objective f (·) is smooth and the desired solution accuracy is reasonably low, (block) coordinate descent
methods are attractive because of their tractability (Nesterov, 2012; Necoara et al., 2011; Beck and Tetruashvili,
2013; Lee and Sidford, 2013; Richtárik and Takáč, 2014;
Lu and Xiao, 2015). In this paper, we consider potentially
non-smooth functions and present an adaptive block coordinate descent method, which iterates over b blocks of
coordinates, reminiscent of AdaGrad (Duchi et al., 2011).
Choosing a good sampling distribution for coordinates
in coordinate descent procedures is nontrivial (Lee and
Sidford, 2013; Necoara et al., 2011; Shalev-Shwartz and
Zhang, 2012; Richtárik and Takáč, 2015; Csiba et al., 2015;
Allen-Zhu and Yuan, 2015). Most work focuses on choos-

Adaptive Sampling Probabilities for Non-Smooth Optimization

ing a good stationary distribution using problem-specific
knowledge, which may not be feasible; this motivates automatically adapting to individual problem instances. For example, Csiba et al. (2015) provide an updating scheme for
the probabilities in stochastic dual ascent. However, the update requires O(b) time per iteration, making it impractical
for large-scale problems. Similarly, Nutini et al. (2015) observe that the Gauss-Southwell rule (choosing the coordinate with maximum gradient value) achieves better performance, but this also requires O(b) time per iteration. Our
method roughly emulates this behavior via careful adaptive
sampling and bandit optimization, and we are able to provide a number of a posteriori optimality guarantees.
In addition to coordinate descent methods, we also consider
the finite-sum minimization problem
n

minimize
x∈X

1X
fi (x),
n i=1

We develop these ideas in the coming sections, focusing
first in Section 2 on adaptive procedures for (non-smooth)
coordinate descent methods and developing the necessary
bandit optimization and adaptivity machinery. In Section 3,
we translate our development into convergence results for
finite-sum convex optimization problems. Complementing
our theoretical results, we provide a number of experiments
in Section 4 that show the importance of our algorithmic
development and the advantages of exploiting block structures in problem data.

2. Adaptive sampling for coordinate descent
We begin with the convex optimization problem
x∈X

Notation for coordinate descent Without loss of genrality we assume that the first d1 coordinates of x ∈ Rd
correspond to X1 , the second d2 to X2 , and so on. We let
Uj ∈ {0, 1}d×dj be the matrix identifying the jth block, so
that Id = [U1 · · · Ud ]. We define the projected subgradient
vectors for each block j by
Gj (x) = Uj Uj> f 0 (x) ∈ Rd ,

where the fi are convex and may be non-smooth. Variancereduction techniques for finite-sum problems often yield
substantial gains (Johnson and Zhang, 2013; Defazio et al.,
2014), but they generally require smoothness. More
broadly, importance sampling estimates (Strohmer and Vershynin, 2009; Needell et al., 2014; Zhao and Zhang, 2014;
2015; Csiba and Richtárik, 2016) can yield improved convergence, but the only work that allows online, problemspecific adaptation of sampling probabilities of which we
are aware is Gopal (2016). However, these updates require
O(b) computation and do not have optimality guarantees.

minimize f (x)

end, we develop an adaptive procedure that exploits variability in block “importance” online. In the coming sections, we show that we obtain certain near-optimal guarantees, and that the computational overhead over a simple
random choice of block j ∈ [b] is at most O(log b). In
addition, under some natural structural assumptions on the
blocks and problem data, we show how our adaptive sampling scheme provides convergence guarantees polynomially better in the dimension than those of naive uniform
sampling or gradient descent.

(2)

where X = X1 × · · · × Xb ⊂ Rd is a Cartesian
product
P
of closed convex sets Xj ⊂ Rdj with j dj = d, and
f is convex and Lipschitz. When there is a natural block
structure in the problem, some blocks have larger gradient norms than others, and we wish to sample these blocks
more often in the coordinate descent algorithm. To that

where f 0 (x) ∈ ∂f (x) is a fixed element of the subdifferential ∂f (x). Define x[j] := Uj> x ∈ Rdj and G[j] (x) =
Uj> Gj (x) = Uj> f 0 (x) ∈ Rdj . Let ψj denote a differentiable 1-strongly convex function on Xj with respect to the
norm k·kj , meaning for all ∆ ∈ Rdj we have


1
2
ψj x[j] + ∆ ≥ ψj x[j] + ∇ψj (x[j] )> ∆ + k∆kj ,
2
and let k·kj,∗ be the dual norm of k·kj . Let Bj (u, v) =
ψj (u) − ψj (v) − ∇ψj (v)> (u − v) be the Bregman divergence associated with ψj , and define the tensorized diverPb
gence B(x, y) := j=1 Bj (x[j] , y[j] ). Throughout the paper, we assume the following.
Assumption 1. For all x, y ∈ X , we have B(x, y) ≤ R2
2

and G[j] (x)j,∗ ≤ L2 /b for j = 1, . . . , b.
2.1. Coordinate descent for non-smooth functions
The starting point of our analysis is the simple observation
that if a coordinate J ∈ [b] is chosen according to a probability vector p > 0, then the importance sampling estimator
GJ (x)/pJ satisfies Ep [GJ (x)/pJ ] = f 0 (x) ∈ ∂f (x).
Thus the randomized coordinate subgradient method
of Algorithm 1 is essentially a stochastic mirror descent method (Nemirovski and Yudin, 1983; Beck and
Teboulle, 2003; Nemirovski et al., 2009), and as long
−1
2
as sup
√x∈X E[kpJ GJ (x)k∗ ] < ∞ it converges at rate
O(1/ T ). With this insight, a variant of standard stochastic mirror descent analysis yields the following convergence guarantee for Algorithm 1 with non-stationary probabilities (cf. Dang and Lan (2015), who do not quite as
carefully track dependence on the sampling distribution

Adaptive Sampling Probabilities for Non-Smooth Optimization

Algorithm 1 Non-smooth Coordinate Descent
Input: Stepsize αx > 0, Probabilities p1 , . . . , pT .
Initialize: x1 = x
for t ← 1, . . . , T
Sample Jt ∼ pt
Update x:
+
(*
xt+1
[Jt ]

G[J ] (xt )
t
,x
pt
Jt

← argminx∈X

return x̄T ←

Jt

1
T

PT

t=1

x

+

1
αx

BJt

Algorithm 2 Stepsize Doubling Coordinate Descent
Initialize: x1 = x, p1 = p, k = 1
while t ≤PT do

2
t
while l=1 (plJl )−2 G[Jl ] (xl )J ,∗ ≤ 4k , t ≤ T do
l
Run inner loop of Algorithm 1 with

)


x, xt[Jt ]

t

p). Throughout, we define the expected sub-optimality gap
of an algorithm outputing an estimate x
b by S(f, x
b) :=
E[f (b
x)] − inf x∗ ∈X f (x∗ ). See Section A.1 for the proof.
Proposition 1. Under Assumption 1, Algorithm 1 achieves


 
T
b G (xt )2
2
X
X
[j]
R
α
x
j,∗ 
S(f, xT ) ≤
+
. (3)
E
t
αx T 2T t=1
p
j
j=1
where S(f, x̄T ) = E[f (x̄T )] − inf x∈X f (x).
t
Asqan immediate consequence, if pq
≥ pmin > 0 and αx =
2pmin
2
R
L
T , then S(f, x̄T ) ≤ RL
T pmin . To make this
more concrete, we consider sampling from the uniform distribution pt ≡ 1b 1 so that pmin = 1/b, and assume homogeneous block sizes dj = d/b for simplicity. Algorithm 1
solves problem (2) to -accuracy within O(bR2 L2 /2 ) iterations, where each iteration approximately costs O(d/b)
plus the cost of projecting into Xj . In contrast, mirror descent with the same constraints and divergence B achieves
the same accuracy within O(R2 L2 /2 ) iterations, taking
O(d) time plus the cost of projecting to X per iteration. As
the projection costs are linear in the number b of blocks, the
two algorithms are comparable.

In practice, coordinate descent procedures can significantly
outperform full gradient updates through efficient memory
usage. For huge problems, coordinate descent methods can
leverage data locality by choosing appropriate block sizes
so that each gradient block fits in local memory.
2.2. Optimal stepsizes by doubling
In the the upper bound (3), we wish to choose the optimal
stepsize αx that minimizes this bound. However, the term
 Pb kG[j] (xt )k2j,∗ 
PT
is unknown a priori. We cirt=1 E
j=1
ptj
cumvent this issue by using the doubling trick (e.g. ShalevShwartz, 2012, Section 2.3.1) to achieve the best possible
rate in hindsight. To simplify our analysis, we assume that
there is some pmin > 0 such that

	
pt ∈ ∆b := p ∈ Rb+ : p> 1 = 1, p ≥ pmin .

2
Pt
Maintaining the running sum
p−2 G[J ] (xl )
l=1

l,Jl

l

Jl ,∗

αx,k =

√


2R 4k +

L2
bp2
min

− 1
2

t←t+1
k ←k+1 P
T
return x̄T ← T1 t=1 xt

requires incremental time O(dJt ) at each iteration t, choosing the stepsizes adaptively via Algorithm 2 only requires
a constant factor of extra computation over using a fixed
step size. The below result shows that the doubling trick in
Algorithm 2 acheives (up to log factors) the performance
of the optimal stepsize that minimizes the regret bound (3).
Proposition 2. Under Assumption 1, Algorithm 2 achieves


  12

T
b G (xt )2
X
X
[j]
R
j,∗ 
E
S(f, x̄T ) ≤ 6 
t
T t=1
p
j
j=1
r


RL
4bT L2
2
log
+
b pmin T log 4
pmin
where S(f, x̄T ) = E[f (x̄T )] − inf x∈X f (x).
2.3. Adaptive probabilities
We now present an adaptive updating scheme for pt , the
sampling probabilities. From Proposition 2, the stationary
distribution achieving the smallest regret upper bound minimizes the criterion



 


b G (xt )2
T
T
G[J ] (xt )2
X
X
X
[j]
t
j,∗ 
Jt ,∗ 
E
E
,
=
2
p
p
j
Jt
t=1
t=1
j=1
where the equality follows from the tower property. Since
xt depends on the pt , we view this as an online convex
optimization problem and choose p1 , . . . , pT to minimize
the regret

!
T
b
X
X


1
1 
G[j] (xt )2
.
(4)
max
E
t − p
j,∗
p∈∆b
p
j
j
t=1
j=1
Note that due to the block coordinate nature of Algorithm 1,

2
we only compute G[j] (xt )j,∗ for the sampled j = Jt at
each iteration. Hence, we treat this as a multi-armed bandit
problem where the arms are the blocks j = 1, . . . , b and

2
we only observe the loss G[j] (xt )j,∗ /(ptJt )2 associated
with the arm Jt pulled at time t.

Adaptive Sampling Probabilities for Non-Smooth Optimization

Algorithm 3 Coordinate Descent with Adaptive Sampling
Input: Stepsize αp > 0, Threshold pmin > 0 with
b
P = {p ∈ R+
: p> 1 = 1, p ≥ pmin }
1
1
Initialize: x = x, p = p
for t ← 1, . . . , T
Sample Jt ∼ pt
Choose αx,k according to Algorithm 2
Update x:
+
)
(*
G[J ] (xt )
t
,x
pt
Jt

xt+1
← argminx∈X
[J ]

Jt

t

+

1
αx,k



B x, xt[Jt ]

Update p: for `bt,j (x) defined in (5),
wt+1 ← pt exp(−(αp `bt,Jt (xt )/ptJt )eJt ),

pt+1 ← argminq∈P Dkl q||wt+1
PT
return x̄T ← T1 t=1 xt

scaling (5) ensures that we penalize blocks with low signal (as opposed to rewarding those with high signal) which
enforces diversity in the sampled coordinates as well. In
Section A.3, we will see how this scaling plays a key role
in proving optimality of Algorithm 3. Here, the signal is
measured by the relative size of the gradient in the block
against the probability of sampling the block. This means
that blocks with large “surprises”—those with higher gradient norms relative to their sampling probability—will get
sampled more frequently in the subsequent iterations. Algorithm 3 guarantees low regret for the online convex optimization problem (4) which in turn yields the following
guarantee for Algorithm 3.
Theorem 3. Under Assumption
q 1, the adaptive updates in
Algorithm 3 with αp =

By using a bandit algorithm—another coordinate descent
method— to update p, we show that our updates achieve
performance comparable to the best stationary probability
in ∆b in hindsight. To this end, we first bound the regret (4)
by the regret of a linear bandit problem. By convexity of
d −1
x 7→ 1/x and dx
x = −x−2 , we have
T
X


b
X


G[j] (xt )2
E
j,∗

t=1

1
1
−
ptj
pj

j=1

!

(∗)

Now, let us view the vector (∗) as the loss vector for a constrained linear bandit problem with feasibility region ∆b .
We wish to apply EXP3 (due to Auer et al. (2002)) or equivalently, a 1-sparse mirror descent to p with ψP (p) = p log p
(see, for example, Section 5.3 of Bubeck and Cesa-Bianchi
(2012) for the connections). However, EXP3 requires the
loss values be positive in order to be in the region where
ψP is strongly convex, so we scale our problem using the
fact that p and pt ’s are probability vectors. Namely,

achieve

v


u
T
b
u
X
X
kG[j] (xt )k2j,∗
6R u
 (6)
t min
E
S(f, x̄T ) ≤
p∈∆b
T
pj
t=1
j=1
{z
}
|
(a):best in hindsight



8LR T log b
+
T pmin
b
|
{z

 14
+√

2RL
log
bT pmin



4bT L2
pmin


.

}

where S(f, x̄T ) = E[f (x̄T )] − inf x∈X f (x).
See Section A.3 for the proof. Note that there is a trade-off
in the regret bound (6) in terms of pmin : for small pmin ,
the first term is small, as the the set ∆b is large, but second (regret) term is large, and vice versa. To interpret the
bound (6), take pmin = δ/b for some δ ∈ (0, 1). The first
term dominates the remainder as long as T = Ω(b log b);
we require T  (bR2 L2 /2 ) to guarantee convergence of
coordinate descent in Proposition 1, so that we roughly expect the first term in the bound (6) to dominate. Thus, Algorithm 3 attains the best convergence guarantee for the
optimal stationary sampling distribution in hindsight.
2.4. Efficient updates for p

 n
ob

2
E − G[j] (xt )j,∗ /(ptj )2

T
X

t



,p − p

The updates for p in Algorithm 3 can be done in O(log b)
time by using a balanced binary tree. Let Dkl (p||q) :=
Pd
pi
i=1 pi log qi denote the Kullback-Leibler divergence between p and q. Ignoring the subscript on t so that w =
wt+1 , p = pt and J = Jt , the new probability vector q is
given by the minimizer of

j=1

t=1

=

2b log b
T

(b):regret for bandit problem





*
+
T


ob
n
X
2


≤
E  − G[j] (xt )j,∗ /(ptj )2
, pt − p  .


j=1
t=1
|
{z
}

T
X

p2min
L2

hD
E

`bt (xt ), pt − p

Ei

,

t=1

where
`bt,j (x) := −



G[j] (x)2

j,∗

(ptj )2

+

L2
.
bp2min

(5)

Using scaled loss values, we perform EXP3 (Algorithm
3). Intuitively, we penalize the probability of the sampled block by the strength of the signal on the block. The

Dkl (q||w) s.t. q > 1 = 1, q ≥ pmin ,

(7)

where w is the previous probability vector p modified only
at the index J. We store w in a binary tree, keeping values up to their normalization factor. At each node, we
also store the sum of elements in the left/right subtree for

Adaptive Sampling Probabilities for Non-Smooth Optimization

Algorithm 4 KL Projection
P
1: Input: J, pJ , wJ , mass = i wi
2: wcand ← pJ · mass.
3: if wcand /(mass −wJ + wcand ) ≤ pmin then
pmin
4:
wcand ← 1−p
(mass −wJ )
min
5: Update(wcand , J)

efficient sampling (for completeness, the pseudo-code for
sampling from the binary tree in O(log b) time is given in
Section B.3). The total mass of the tree can be accessed by
inspecting the root of the tree alone.
The following proposition shows that it suffices to touch at
most one element in the tree to do the update. See Section B
for the proof.
Proposition 4. The solution to (7) is given by
(
(1−pJ )
1
wj if wJ ≥ pmin
1−pmin
J +wJ
,
qj6=J = 1−p
1−pmin
otherwise
1−pJ wj
(
(1−pJ )
1
w if wJ ≥ pmin
1−pmin
qJ = 1−pJ +wJ
pmin
otherwise.
As seen in Algorithm 4, we need to modify at most one
element in the binary tree. Here, the update function modifies the value at index J and propagates the value up the
tree so that the sum of left/right subtrees are appropriately
updated. We provide the full pseudocode in Section B.2.
2.5. Example
The optimality guarantee given in Theorem 3 is not directly
interpretable since the term (a) in the upper bound (6)
is only optimal given the iterates x1 , . . . , xT despite the
fact that xt ’s themselves depend on the sampling probabilities. Hence, we now study a setting where we can further
bound (6) to get a explicit regret bound for Algorithm 3 that
is provably better than non-adaptive counterparts. Indeed,
under certain structural assumptions on the problem similar
to those of McMahan and Streeter (2010) and Duchi et al.
(2011), our adaptive sampling algorithm provably achieves
regret polynomially better in the dimension than either using a uniform sampling distribution or gradient descent.
Consider the SVM objective
n


1X
f (x) =
1 − yi zi> x +
n i=1
where
is small and d 	is large. Here, ∂f (x) =
Pn n 
1
1
1 − yi zi> x ≥ 0 zi . Assume that for some
i=1
n
fixed
Lj := βj −α , we have |∂j f (x)|2 ≤
Pnα ∈ (1,2∞) and
1
2
i=1 |zi,j | ≤ Lj . In particular, this is the case if we
n
have sparse features zU ∈ {−1, 0, +1}d with power law

Algorithm
ACD
UCD
GD

α ∈ [2, ∞)
α ∈ (1, 2)


2
2
R
R 2 2−α
log d
d


4

4
5
5
R 3
R 3
3
+ 
d log d + 
d log 3 d

R 2
d log d


R 2
d log d


Table 1. Runtime comparison (computations needed to guarantee -optimality gap) under heavy-tailed block structures.
ACD=adaptive coordinate descent, UCD=uniform coordinate descent, GD=gradient descent

tails P (|zU,j | = 1) = βj −α where U is a uniform random
variable over {1, . . . , n}.
Take Cj = {j} for j = 1, . . . , d (and b = d). First, we
show that although for the uniform distribution p = 1/d
2
d
X
E[kGj (xt )k ]
∗

j=1

1/d

≤d

d
X

L2j = O(d log d),

j=1

the term (a) in (6) can be orders of magnitude smaller.
Proposition 5. Let b = d, pmin = δ/d for some δ ∈ (0, 1),
2
and Cj = {j}. If kGj (x)k∗ ≤ L2j := βj −α for some
α ∈ (1, ∞), then
(

2
d
X
E[Gj (xt )∗ ]
O(log d), if α ∈ [2, ∞)
min
=
2−α
p∈∆b ,p≥pmin
p
O(d
), if α ∈ (1, 2).
j
j=1

We defer the proof of the proposition to Section A.5. Using
this bound, we can show explicit regret bounds for Algorithm 3. From Theorem 3 and Proposition 5, we have that
Algorithm 3 attains
( R log d
O( √T ),
if α ∈ [2, ∞)
S(f, x̄T ) ≤
α
R
1−
√ O(d
2 ),
if α ∈ (1, 2)
T


+ O Rd3/4 T −3/4 log5/4 d .
Setting above to be less than  and inverting with respect to
T , we obtain the iteration complexity in Table 1.
To see the runtime bounds for uniformly sampled coordinate descent and gradient descent, recall the regret
bound (3) given in Proposition 1. Plugging ptj = 1/d in
the bound, we obtain
p
√
S(f, x̄T ) ≤ O(R log d 2dT ).
p
Pd
for αx = 2R2 /(L2 T d) where L2 := j=1 L2j . Simip
larly, gradient descent with αx = 2R2 /(L2 T ) attains
p
√
S(f, x̄T ) ≤ O(R log d 2T ).
Since each gradient descent update takes O(d), we obtain
the same runtime bound.

Adaptive Sampling Probabilities for Non-Smooth Optimization

While non-adaptive algorithms such as uniformly-sampled
coordinate descent or gradient descent have the same runtime for all α, our adaptive sampling method automatically
tunes to the value of α. Note that for α ∈ (1, ∞), the first
term in the runtime bound for our adaptive method given in
Table 1 is strictly better than that of uniform coordinate descent or gradient descent. In particular, for α ∈ [2, ∞) the
best stationary sampling distribution in hindsight yields an
improvement that is at most O(d) better in the dimension.
However, due to the remainder terms for the bandit problem, this improvement only matters (i.e.first term is larger
than second) when
 

√
O Rd− 32 log d
if α ∈ [2, ∞)

 3
=
O Rd 2 (1−α) log−5/2 d
if α ∈ (1, 2).
In Section 4, we show that these remainder terms can be
made smaller than what their upper bounds indicate. Empirically, our adaptive method outperforms the uniformlysampled counterpart for larger values of  than above.

3. Adaptive probabilities for stochastic
gradient descent
Consider the empirical risk minimization problem
)
( n
1X
fi (x) =: f (x)
minimize
x∈X
n i=1
where X ∈ Rd is a closed convex set and fi (·) are convex functions. Let C1 , . . . , Cb be a partition of the n samples so that each example belongs to some Cj , a set of size
nj := |Cj | (note that the index j now refers to blocks of examples instead of coordinates). These block structures naturally arise, for example, when Cj ’s are the examples with
the same label in a multi-class classification problem. In
this stochastic optimization setting, we now sample a block
Jt ∼ pt at each iteration t, and perform gradient updates
using a gradient estimate on the block CJt . We show how
a similar adaptive updating scheme for pt ’s again achieves
the optimality guarantees given in Section 2.
3.1. Mirror descent with non-stationary probabilities
Following the approach of (Nemirovski et al., 2009), we
run mirror descent for the updates on x. At iteration
t, a block Jt is drawn from a b-dimensional probability vector pt . We assume that we have access to unbiased stochastic gradients
Gj (x) for each block. That is,
P
E[Gj (x)] = n1j i∈Cj ∂fi (x). In particular, the estimate
GJt (xt ) := ∂fIt (x) where It is drawn uniformly in CJt
gives the usual unbiased stochastic gradient of minibatch
size 1. The other extreme is obtained
Pby using a minibatch
size of nj where GJt (xt ) := n1J
i∈CJ ∂fi (x). Then,
t

t

the importance sampling estimator

nJt
nptJ

GJt (xt ) is an un-

t

biased estimate for the subgradient of the objective.
Let ψ be a differentiable 1-strongly convex function on
X with respect to the norm k·k as before and denote by
k·k∗ the dual norm of k·k. Let B(x, y) = ψ(x) − ψ(y) −
∇ψ(y)> (x−y) be the Bregman divergence associated with
ψ. In this section, we assume the below (standard) bound.
Assumption 2. For all x, y ∈ X , we have B(x, y) ≤ R2
2
and kGj (x)k∗ ≤ L for j = 1, . . . , b.
We use these stochastic gradients to perform mirror updates, replacing the update in Algorithm 1 with the update
xt+1 ← argmin



x∈X



nJt 

1
t
t
B(x,
x
)
. (8)
G
(x
),
x
+
Jt
nptJt
αx

From a standard argument (e.g., (Nemirovski et al., 2009)),
we obtain the following convergence guarantee. The proof
follows an argument similar to that of Proposition 1.
Proposition 6. Under Assumption 2, the updates (8) attain


T
b
2
t 2
2
X
X
n
kG
(x
)k
R
αx
j
j
∗
S(f, x̄T ) ≤
+
E
. (9)
2 pt
αx T 2T t=1
n
j
j=1
where S(f, x̄T ) = E[f (x̄T )] − inf x∈X f (x).
Again, we wish to choose the optimal step size αx that
minimizes the regret bound (9). To this end, modify
the doubling trick given in Algorithm 2 as follows: use


Pt
n2J
l
GJ (xl )2 for the second while condition,
2 2
l=1 n pl,J

l

l

∗


− 12
L2 maxj n2
and stepsizes αx,k = 2R 4k + n2 p2 j
. Then,
min
similar to Proposition 2, we have


 21
T
b
2 
X
X

nj
R
Gj (xt )2 
E
S(f, x̄T ) ≤ 6 
2 pt
∗
T t=1
n
j
j=1


√
b
2 X
n2j
2RL maxj nj
4T
L
.
+
log 
pmin T log 4
n
pmin j=1 n2
√

3.2. Adaptive probabilities
Now, we consider an adaptive updating scheme for pt ’s
similar to Section 2.3. Using the scaled gradient estimate

`bt,j (x) := −

nj
kGj (x)k∗
nptj

2
+

L2 maxj n2j
n2 p2min

(10)

to run EXP3, we obtain Algorithm 5. Again, the additive
scaling L2 (maxj nj /npmin )2 is to ensure that `b ≥ 0. As in
Section 2.4, the updates for p in Algorithm 5 can be done in
O(log b) time. We can also show similar optimality guarantees for Algorithm 5 as before. The proof is essentially
the same to that given in Section A.3.

Adaptive Sampling Probabilities for Non-Smooth Optimization

Algorithm 5 Mirror Descent with Adaptive Sampling
Input: Stepsize αp > 0
Initialize: x1 = x, p1 = p
for t ← 1, . . . , T
Sample Jt ∼ pt
Choose αx,k according to (modified) Algorithm 2.
Update x:
)
(
← argminx∈X
xt+1
J

1
pt
Jt

t




GJt (xt ), x +

1
αx,k



B x, xtJt

Update p:
wt+1 ← pt exp(−(αp `bt,Jt (xt )/ptJt )eJt )

pt+1 ← argminq∈P Dkl q||wt+1
PT
return x̄T ← T1 t=1 xt

Theorem 7. Let W :=
Algorithm 5 with αp =

L maxj nj
pmin n .
q

1
W2

2 log b
bT

Under Assumption 2,
achieves



T
b
X
X

n2j 
6R
2
GJt (xt ) 
E
min
S(f, x̄T ) ≤
2p
∗
T p∈∆b t=1
n
j
j=1
!
√
b
1
2RW
4T L2 X n2j
+ W (2T b log b) 4 +
log
T log 4
pmin j=1 n2
where S(f, x̄T ) = E[f (x̄T )] − inf x∈X f (x).
With equal block sizes nj = n/b and pmin = δ/b for
some δ ∈ (0, 1), the first term in the boudn of Theorem 7 is O(T L2 ) which dominates the second term if
T = Ω(b log b). Since we usually have T = Θ(n) for
SGD, as long as n = Ω(b log b) we have
v


u

 
T
b G (xt )2
u
X
X
[j]
Ru
j,∗ 
E
S(f, x̄T ) ≤ O  t min
.
T p∈∆b t=1
p
j
j=1


That is, Algorithm 5 attains the best regret bound achieved
by the optimal stationary distribution in hindsight had the
xt ’s had remained the same. Further, under similar struc2
tural assumptions kGj (x)k∗ ∝ j −α as in Section 2.5, we
can prove that the regret bound for our algorithm is better
than that of the uniform distribution.

4. Experiments
We compare performance of our adaptive approach with
stationary sampling distributions on real and synthetic
datasets. To minimize parameter tuning, we fix αp at the
value suggested by theory in Theorems 3 and 7. However,
we make a heuristic modification to our adaptive algorithm
since rescaling the bandit gradient (5) and (10) dwarfs the
signals in gradient values if L is too large. We present
performance of our algorithm with respect to multiple estimates of the Lipschitz constant L̂ = L/c for c > 1, where

L is the actual upper bound.1 We√tune the stepsize αx for
both methods, using the form β/ t and tuning β.
For all our experiments, we compare our method against
the uniform distribution and blockwise Lipschitz sampling
distribution pj ∝ Lj where Lj is the Lipschitz constant
of the j-th block (Zhao and Zhang, 2015). We observe
that the latter method often performs very well with respect to iteration count. However, since computing the
blockwise Lipschitz sampling distribution takes O(nd), the
method is not competitive in large-scale settings. Our algorithm, on the other hand, adaptively learns the latent structure and often outperforms stationary counterparts with respect to runtime. While all of our plots are for a single
run with a random seed, we can reject the null hypothesis
f (xTuniform ) < f (xTadaptive ) at 99% confidence for all instances where our theory guarantees it. We take k·k = k·k2
throughout this section.
4.1. Adaptive sampling for coordinate descent
Synthetic Data We begin with coordinate descent, first
verifying the intuition of Section 2.5 on a synthetic dataset.
We consider the problem minimizekxk∞ ≤1 n1 kAx − bk1 ,
and we endow A ∈ Rn×d with the following block structure: the columns are drawn as aj ∼ j −α/2 N (0, I). Thus,
the gradients of the columns decay in a heavy-tailed manner as in Section 2.5 so that L2j = j −α . We set n = d =
b = 256; the effects of changing ratios n/d and b/d manifest themselves via relative norms of the gradients in the
columns, which we control via α instead. We run all experiments with pmin = 0.1/b and multiple values of c.
Results are shown in Figure 1, where we show the optimality gap vs. runtime in (a) and the learned sampling
distribution in (b). Increasing α (stronger block structure)
improves our relative performance with respect to uniform
sampling and our ability to accurately learn the underlying
block structure. Experiments over more α and c in Section
C further elucidate the phase transition from uniform-like
behavior to regimes learning/exploiting structure.
We also compare our method with (non-preconditioned)
SGD using leverage scores pj ∝ kaj k1 given by (Yang
et al., 2016). The leverage scores (i.e., sampling distribution proportional to blockwise Lipschitz constants) roughly
correpond to using pj ∝ j −α/2 , which is the stationary
distribution that minimizes the bound (3); in this synthetic
setting, this sampling probability coincides with the actual
block structure. Although this is expensive to compute, taking O(nd) time, it exploits the latent block structure very
well as expected. Our method quickly learns the structure
and performs comparably with this “optimal” distribution.
1

We guarantee a positive loss by taking max(`bt,j (x), 0).

Adaptive Sampling Probabilities for Non-Smooth Optimization
100

100

100

10-2

10-1

10-1

-1

10

10-3

-2

10
0

0.5

1

1.5

2

2.5

3

3.5

4

0

0.5

1

1.5

2

0

0.5

1

1.5

(a) Optimality gap

2

2.5

0

1

1.5

2

2.5

100

150

200

250

(a) Optimality gap

1

1

1

1

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0

0.5

0
0

50

100

150

200

250

0

50

100

150

200

250

0

0
0

50

100

(b) Learned sampling distribution

150

200

250

0

50

(b) Learned sampling distribution

Figure 1. Adaptive coordinate descent (left to right: α = 0.4, 2.2)

Figure 3. Adaptive SGD (left to right: α = 0.4, 6)

-4

0

10

10

101

0

10

7
6

10

-1

10

-2

10

-3

5
100

4
10-1

3
2
10

1
0

50

100

150

200

250

300

0
350 100

101

102

-1

10-4

103
0

(a) Optimality gap

(b) Learned distribution

Figure 2. Model selection for nucleotide sequences

Model selection Our algorithm’s ability to learn underlying block structure can be useful in its own right as an online feature selection mechanism. We present one example
of this task, studying an aptamer selection problem (Cho
et al., 2013), which consists of n = 2900 nucleotide sequences (aptamers) that are one-hot-encoded with all kgrams of the sequence, where 1 ≤ k ≤ 5 so that d =
105, 476. We train an l1 -regularized SVM on the binary
labels, which denote (thresholded) binding affinity of the
aptamer. We set the blocksize as 50 features (b = 2110)
and pmin = 0.01/b. Results are shown in Figure 2, where
we see that adaptive feature selection certainly improves
training time in (a). The learned sampling distribution depicted in (b) for the best case (c = 107 ) places larger weight
on features known as G-complexes; these features are wellknown to affect binding affinities (Cho et al., 2013).
4.2. Adaptive sampling for SGD
Synthetic data We use the same setup as in Section 4.1
but now endow block structure on the rows of A rather than
the columns. In Figure 3, we see that when there is little
block structure (α = 0.4) all sampling schemes perform
similarly. When the block structure is apparent (α = 6),
our adaptive method again learns the underlying structure

50

100

150

(a) CUB-200

200

250

0

2

4

6

8

10

(b) ALOI

Figure 4. Optimality gap for CUB-200-2011 and ALOI

and outperforms uniform sampling. We provide more experiments in Section C to illustrate behaviors over more
c and α. We note that our method is able to handle online data streams unlike stationary methods such as leverage scores.
CUB-200-2011/ALOI We apply our method to two
multi-class object detection datasets: Caltech-UCSD
Birds-200-2011 (Wah et al., 2011) and ALOI (Geusebroek
et al., 2005). Labels are used to form blocks so that b = 200
for CUB and b = 1000 for ALOI. We use softmax loss for
CUB-200-2011 and a binary SVM loss for ALOI, where
in the latter we do binary classification between shells and
non-shell objects. We set pmin = 0.5/b to enforce enough
exploration. For the features, outputs of the last fullyconnected layer of ResNet-50 (He et al., 2016) are used
for CUB so that we have 2049-dimensional features. Since
our classifier x is (b · d)-dimensional, this is a fairly large
scale problem. For ALOI, we use default histogram features (d = 128). In each case, we have n = 5994 and n =
108, 000 respectively. We use X := {x ∈ Rm : kxk2 ≤ r}
where r = 100 for CUB and r = 10 for ALOI. We observe
in Figure 4 that our adaptive sampling method outperforms
stationary counterparts.

Adaptive Sampling Probabilities for Non-Smooth Optimization

Acknowledgements
HN was supported by the Samsung Scholarship. AS and
SY were supported by Stanford Graduate Fellowships and
AS was also supported by a Fannie & John Hertz Foundation Fellowship. JCD was supported by NSF-CAREER1553086.

References
Z. Allen-Zhu and Y. Yuan. Even faster accelerated coordinate descent using non-uniform sampling. arXiv preprint
arXiv:1512.09103, 2015.

J. C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient
methods for online learning and stochastic optimization.
Journal of Machine Learning Research, 12:2121–2159,
2011.
J.-M. Geusebroek, G. J. Burghouts, and A. W. Smeulders.
The amsterdam library of object images. International
Journal of Computer Vision, 61(1):103–112, 2005.
S. Gopal. Adaptive sampling for sgd by exploiting side
information. In Proceedings of The 33rd International
Conference on Machine Learning, pages 364–372, 2016.

P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002.

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.

A. Beck and M. Teboulle. Mirror descent and nonlinear
projected subgradient methods for convex optimization.
Operations Research Letters, 31:167–175, 2003.

R. Johnson and T. Zhang. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in Neural Information Processing Systems 26, 2013.

A. Beck and L. Tetruashvili. On the convergence of block
coordinate descent type methods. SIAM Journal on Optimization, 23(4):2037–2060, 2013.

Y. T. Lee and A. Sidford. Efficient accelerated coordinate
descent methods and faster algorithms for solving linear
systems. In 54th Annual Symposium on Foundations of
Computer Science, pages 147–156. IEEE, 2013.

S. Bubeck and N. Cesa-Bianchi. Regret analysis of
stochastic and nonstochastic multi-armed bandit problems. Foundations and Trends in Machine Learning, 5
(1):1–122, 2012.

Z. Lu and L. Xiao. On the complexity analysis of randomized block-coordinate descent methods. Mathematical
Programming, 152(1-2):615–642, 2015.

N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and
games. Cambridge University Press, 2006.
M. Cho, S. S. Oh, J. Nie, R. Stewart, M. Eisenstein,
J. Chambers, J. D. Marth, F. Walker, J. A. Thomson,
and H. T. Soh. Quantitative selection and parallel characterization of aptamers. Proceedings of the National
Academy of Sciences, 110(46), 2013.
D. Csiba and P. Richtárik. Importance sampling for minibatches. arXiv preprint arXiv:1602.02283, 2016.
D. Csiba, Z. Qu, and P. Richtárik. Stochastic dual coordinate ascent with adaptive probabilities. arXiv preprint
arXiv:1502.08053, 2015.
C. D. Dang and G. Lan. Stochastic block mirror descent methods for nonsmooth and stochastic optimization. SIAM Journal on Optimization, 25(2):856–881,
2015.
A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A
fast incremental gradient method with support for nonstrongly convex composite objectives. In Advances in
Neural Information Processing Systems 27, 2014.

B. McMahan and M. Streeter. Adaptive bound optimization for online convex optimization. In Proceedings of
the Twenty Third Annual Conference on Computational
Learning Theory, 2010.
I. Necoara, Y. Nesterov, and F. Glineur. A random coordinate descent method on large optimization problems with linear constraints. University Politehnica
Bucharest, Tech. Rep, 2011.
D. Needell, R. Ward, and N. Srebro. Stochastic gradient
descent, weighted sampling, and the randomized Kaczmarz algorithm. In Advances in Neural Information Processing Systems 27, pages 1017–1025, 2014.
A. Nemirovski and D. Yudin. Problem Complexity and
Method Efficiency in Optimization. Wiley, 1983.
A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic
programming. SIAM Journal on Optimization, 19(4):
1574–1609, 2009.
Y. Nesterov. Efficiency of coordinate descent methods on
huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012.

Adaptive Sampling Probabilities for Non-Smooth Optimization

J. Nutini, M. Schmidt, I. H. Laradji, M. Friedlander, and
H. Koepke. Coordinate descent converges faster with
the gauss-southwell rule than random selection. arXiv
preprint arXiv:1506.00552, 2015.
P. Richtárik and M. Takáč. Iteration complexity of randomized block-coordinate descent methods for minimizing a
composite function. Mathematical Programming, 144
(1-2):1–38, 2014.
P. Richtárik and M. Takáč.
Parallel coordinate descent methods for big data optimization.
Mathematical Programming, page Online first, 2015.
URL http://link.springer.com/article/
10.1007/s10107-015-0901-6.
S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends in Machine Learning,
4(2):107–194, 2012.
S. Shalev-Shwartz and T. Zhang. Proximal stochastic
dual coordinate ascent. arXiv preprint arXiv:1211.2717,
2012.
T. Strohmer and R. Vershynin. A randomized Kaczmarz algorithm with exponential convergence. Journal
of Fourier Analysis and Applications, 15(2):262–278,
2009.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
J. Yang, Y.-L. Chow, C. Ré, and M. W. Mahoney. Weighted
sgd for p regression with randomized preconditioning. In Proceedings of the Twenty-Seventh Annual ACMSIAM Symposium on Discrete Algorithms, pages 558–
569. Society for Industrial and Applied Mathematics,
2016.
P. Zhao and T. Zhang.
Accelerating minibatch
stochastic gradient descent using stratified sampling.
arXiv:1405.3080 [stat.ML], 2014.
P. Zhao and T. Zhang. Stochastic optimization with importance sampling. In Proceedings of the 32nd International
Conference on Machine Learning, 2015.

Adaptive Sampling Probabilities for Non-Smooth Optimization

A. Proofs
A.1. Proof of Proposition 1
Let σt := σ(x1 , . . . , xt , J1 , . . . , Jt−1 ) and Et [·] = E[·|σt ].
By convexity
of f and
i unbiasedness of our gradient estimah
1
t
tor Et pt GJt (x ) = g(xt ) ∈ ∂f (xt ), we have
Jt

f (xt ) − f (x) ≤ Et





1 

t
t
G
(x
),
x
−
x
.
Jt
ptJt

We use the following lemma.
Lemma 1. For any t = 1, . . . , T , we have


1 

1
GJt (xt ), xt − x ≤
B(x, xt ) − B(x, xt+1 )
t
pJt
αx

αx 
G[J ] (xt )2 .
+
t
Jt ,∗
2(ptJt )2



t
and strong convexity of ψJt given by BJt xt+1
,
x
[Jt ] ≥
[Jt ]
2

1  t+1
t 
2 x[Jt ] − x[Jt ]  . For step (c), we used Fenchel-Young’s
Jt

inequality:
E
1 D
t+1
t
t
−
x
G
(x
),
x
[J
]
[J
]
t
[Jt ]
t
ptJt
2


αx 
t+1
t 
G[J ] (xt )2 + 1 
≤
−
x
x
[Jt ]  .
t
Jt ,∗
2(ptJt )2
2αx [Jt ]
Jt
Furthermore, due to the fact that xt and xt+1 only differ in
the Jt -th block, we have




BJt x[Jt ] , xt[Jt ] − BJt x[Jt ] , xt+1
[Jt ]
= B(x, xt ) − B(x, xt+1 )
from which (d) follows.

Proof of Lemma
Using convexity and Lemma 1 to bound f (xt ) − f (x) and

1 

summing each side over t = 1, . . . , T , we conclude
GJt (xt ), xt − x
ptJt
T E[f (x̄T ) − f (x)]


1 

1 

" T
#
t
t+1
t
t
t+1
= t GJt (x ), x
− x + t GJt (x ), x − x
X
pJt
pJt
≤E
(f (xt ) − f (x))
E
1 D
t=1
= t G[Jt ] (xt ), xt+1
[Jt ] − x[Jt ]


T
pJt
X

1 

t
t
D
E
G
(x
),
x
−
x
≤
E
Jt
1
ptJt
+ t G[Jt ] (xt ), xt[Jt ] − xt+1
t=1
[J
]
t
pJt
T
2
B(x, x1 ) αx X
1 




E
(a) 1 D
≤
E t 2 G[Jt ] (xt )J ,∗
+
t+1
t+1
t
t
≤
∇ψJt x[Jt ] − ∇ψJt x[Jt ] , x[Jt ] − x[Jt ]
αx
2 t=1 (pJt )
αx




E
1 D
T
b G (xt )2
X
X
(∗) R2
[j]
α
+ t G[Jt ] (xt ), xt[Jt ] − xt+1
x
j,∗ 
[Jt ]
≤
E
+
pJt
t
αx
2 t=1
p
j
j=1




(b) 1 
≤
BJt x[Jt ] , xt[Jt ] − BJt x[Jt ] , xt+1
[Jt ]
where in step (∗) we used the tower law E[·] = E[Et [·]].
αx


2
The second result follows from the bound ptj ≥ pmin .
1  t+1

−
x[Jt ] − xt[Jt ] 
2αx
Jt
E
A.2. Proof of Proposition 2
1 D
+ t G[Jt ] (xt ), xt[Jt ] − xt+1
[J
]
t
pJt
Denote by Ek the indices in epoch k. Let K be the total




(c) 1 
number of epochs used in Algorithm 2. Applying Lemma 1
≤
BJt x[Jt ] , xt[Jt ] − BJt x[Jt ] , xt+1
[Jt ]
to each of the epochs, we obtain
αx
2
αx 
T
t
X

+
1 

2 G[Jt ] (x )Jt ,∗
t
GJt (xt ), xt − x
2 pJt
t
p
t=1 Jt


αx 
(d) 1
t
t+1
t 2

K X
=
G[Jt ] (x ) J ,∗ .
B(x, x ) − B x, x
+
X

1 

t
αx
2(ptJt )2
GJt (xt ), xt − x
=
t
p
k=1 t∈Ek Jt
where in step (a) we used the optimality conditions for the
(
)
K
X αx,k 
X
mirror update in Algorithm 1. Step (b) follows from the

R2
t 2

≤
+
 G[Jt ] (x ) Jt ,∗
algebraic relation
t 2
αx,k
t∈Ek 2 pJt
k=1
D
E
t+1
t


K 
∇ψJt (xt+1
(a) X
[Jt ] ) − ∇ψJt (x[Jt ] ), x[Jt ] − x[Jt ]
R2
αx,k
L2






≤
+
4k + 2
t+1
t
αx,k
2
bpmin
= BJt x[Jt ] , xt[Jt ] − BJt x[Jt ] , xt+1
k=1
[Jt ] − BJt x[Jt ] , x[Jt ] ,

Adaptive Sampling Probabilities for Non-Smooth Optimization


K 
L2 (b) √ X k
L
√
≤ 2R
+ 2 ≤ 2R
2 +
bpmin
pmin b
k=1
k=1


√
L
K+1
√
= 2R 2
−2+K
pmin b

 21


t 2
T G
X
(c) √
[Jt ] (x ) Jt ,∗

≤ 2R 4 
2
ptJt
t=1

!


t 2
T G
X
(x
)
[J
]
L
t
Jt ,∗ 
√
+
log 4
2
t
pmin b log 4
pJ
t=1
√

K
X

s

4k

t

where
(a)
follows
from
noting
that



P
G[J ] (xt )2 / pt 2 ≤ 4k + L22 , (b)
J
t
t∈Ek
bpmin
J ,∗
√t
√
√t
from a + b ≤
a + b. In step (c), we used the
stopping condition of the K − 1th epoch


t 2
T G
X
[Jt ] (x ) Jt ,∗
≥ 4K−1 .

t 2
p
t=1
Jt

A.3. Proof of Theorem 3
From Proposition 2, it suffices to show that Algorithm 3
with αp attains the regret bound


 


T
b G (xt )2
G[j] (xt )2
X
X
[j]
j,∗ 
j,∗
−
sup
E
t
p
p
j
p∈P t=1
j
j=1
r
2T log b
L2
.
(11)
≤ 2
pmin
b
Note that the bandit updates in Algorithm 3 correspond to
Pb
mirror descent updates with ψP (p) = j=1 pj log pj and
P
b
ψP? (u) = j=1 exp(uj − 1) (Beck and Teboulle, 2003).
We wish show that the bandit mirror√descent updates in
Algorithm 3 achieves regret scaling as T . To this end, we
first state a standard result for mirror descent algorithms.
See for example, Cesa-Bianchi and Lugosi (2006, Ch.11)
or Bubeck and Cesa-Bianchi (2012, Thm 5.3). We outline
the proof for completeness in Appendix A.4.
Lemma 2 (Bubeck and Cesa-Bianchi (2012), Thm 5.3).
The following bound holds for Algorithm 3 for any p ∈ P.
T D
E
X
`bt (xt ), pt − p

Taking expectations on both sides, we have

t=1

T (E[f (x̄T )] − f (x))

≤



T
T
X
X

1 

(E[f (xt )] − f (x)) ≤
E t GJt (xt ), xt − x
pJt
t=1
t=1
2  12
"  T 
X G[Jt ] (xt )J ,∗
√
t

≤ 2RE 4 
2
ptJt
t=1

#


t 2
T G
X
[Jt ] (x ) Jt ,∗
L

√
log 4
+
2
pmin b log 4
ptJ
t=1

T


1 X
BψP (p, p1 )
BψP? ∇ψP (pt ) − αp `bt (xt ), ∇ψP (pt )
+
αp
αp t=1

(12)

≤

A straightforward calculation gives that


BψP? ∇ψP (pt ) − αp `bt (xt ), ∇ψP (pt )
=

t




  12
T
b G (xt )2
X
X
√
[j]
(b)
j,∗ 
E
= 4 2R 
t
p
j
t=1
j=1



 
√
T
b G (xt )2
X
X
[j]
2RL
j,∗ 
√
+
log 4
E
t
p
pmin b log 4
j
t=1
j=1
where (a) follows from Jensen’s inequality and (b) from
the tower law.



ptj exp(−αp `bt,j (xt )) + αp `bt,j (xt ) − 1

j=1

t

 
 12


t 2
T G
X
(a) √
[Jt ] (x ) Jt ,∗

≤ 4 2R E 
2
ptJt
t=1
 



√
t 2
T G
X
(x
)
[J
]
2RL
t
Jt ,∗ 
√
+
log 4E 
2
t
pmin b log 4
pJ
t=1

b
X

≤

L4 αp2
αp2 t
pJt `bt,Jt (xt )2 ≤ 4 2 t
2
2pmin b pJt

(13)

since ez − z − 1 ≤ z 2 for z ≤ 0 where we used the fact
that `b ≥ 0. From convexity, we have for p ∈ P




 
T
b G (xt )2
G[j] (xt )2
X
X
[j]
j,∗
j,∗ 
E
−
t
p
p
j
j
t=1
j=1




 
T
b G (xt )2
G[j] (xt )2
X
[j]
(a) X
j,∗
j,∗ 
=
E
−
t
p
p
j
j
t=1
j=1


T
X
L2
E pt − p, 2 1
bpmin
t=1


 


T
b
G[j] (xt )2
2
X
(b) X
L
j,∗
−
+ 2  (ptj − pj )
≤
E
t )2
(p
bp
j
min
t=1
j=1
+

Adaptive Sampling Probabilities for Non-Smooth Optimization
(c)

=

T
hD
Ei
X
E `bt (xt ), pt − p

Now, noting that ∇ψP (wt+1 ) = ∇ψP (pt ) + αp `bt (xt ), we
obtain the result.

t=1



T
1
log b αp X L4
E t
≤
+
αp
2 t=1 p4min b2
p Jt

(d)

A.5. Proof of Proposition 5
Let us first solve for the KKT conditions of the following
minimization problem

log b αp L4
≤
+
T
αp
2 p4min b

(e)

minimize
n

t

where in (a) we used the fact that p, p are probabilities and in (b) we used convexity of g(p) =

 2
Pb
1
G[j] (xt )
+ p> 1. To obtain (d), we used
j=1 pj

j,∗

Dkl (p||p1 ) ≤ log b, Lemma 2 and (13). Finally,
q step (e)
p2min
2b log b
follows from tower law. Setting αp = L2
, we
T
obtain


2
2
T
b
X
X
kGj (xt )k∗
kGj (xt )k∗

E
−
max
t
p∈P
p
p
j
j
t=1
j=1
r
2
L
2T log b
.
(14)
≤ 2
pmin
b
Using this in the bound of Proposition 2, we obtain the desired result.

p∈R

pj

Taking the first order conditions for the Lagrangian
L(p, η, θ) =

b
X
L2j
j=1

pj

− η(p> − 1) − θ> (p − pmin 1),

we have
Lj
=
pj = p
|η + θj |

√Lj

if Lj ≥

pmin

otherwise

(

|η|

p

|η|pmin

where the last equality
follows from complementary
slacko
n
p
ness. Let I := j : Lj ≥ |η|pmin . Plugging pj ’s into
the equality constraint, we get
b
X

1 X
pj = p
Lj + (b − |I|)pmin = 1
|η| j∈I
j=1

From Algorithm 3, we have
so that

αp `bt (xt )> (pt − p)

p

>
= ∇ψP (wt+1 ) − ∇ψP (pt ) (pt − p)

|η| =

X
1
Lj .
1 − (b − |I|)pmin

(16)

j∈I

= BψP (p, pt ) + BψP (pt , wt+1 ) − BψP (p, wt+1 ). (15)
For any p ∈ P, we have for all p ∈ P,
BψP (p, wt+1 ) ≥ BψP (p, pt+1 ) + BψP (pt+1 , wt+1 )
>
≡ ∇ψP (p) − ∇ψP (wt+1 ) (p − pt+1 ) ≥ 0.

Then, by plugging in pj into the objective and using the
above identity for |η| yields
b
X
L2j
j=1

pj

=

p

|η|

X

Lj +

j∈I

1
pmin

X

Now, let L2j = cj −α so that I =
`bt (xt )> (pt − p)

L2j

j∈I c

= ((1 − (b − |I|)pmin ) |η| +

The latter inequality is just the optimality condition for
pt+1 = argminp∈P BψP (p, wt+1 ). Applying the first
equality in (15) and summing for t = 1, . . . , T , we obtain
αp

j=1

subject to p> 1 = 1, p ≥ pmin .

A.4. Proof of Lemma 2

T
X

b
X
L2j

1
pmin


j:j≤

X

L2j . (17)

j∈I c



c
|η|p2min

 α1 

.

When α ∈ [2, ∞), we have

t=1
|I|
X

≤ BψP (p, p1 ) − BψP (p, pT +1 )
+

T
X

BψP (pt , wt+1 ) − BψP (pt+1 , wt+1 )



t=1

≤ BψP (p, p1 ) +

T
X

BψP (pt , wt+1 )

= BψP (p, p1 ) +

t=1

j=1

|I|
X

  
b
j −α/2 = O(log |I|) = O log
.
|η|
j=1

2
From
 (16), 1we
 have |η| = O(log b) and |I| =
α
b2
O
= o(b). Using these in the bound (17),
log2 b

we obtain

t=1
T
X

Lj = c

BψP? (∇ψP (wt+1 ), ∇ψP (pt )).

b
X
L2j
j=1

b
≤ O(log2 b) + c(b − |I|)1−α = O(log2 b)
pj
δ

Adaptive Sampling Probabilities for Non-Smooth Optimization

and J ∗ := max {1 ≤ j ≤ b : f (j) ≥ 0}. We first show
that the optimal dual variable η ∗ is given by

which was the result for α ∈ [2, ∞).
When α ∈ (1, 2), we have
|I|
X

Lj = c

j=1

|I|
X



j −α/2 = Θ |I|1−α/2

eη



j=1


=Θ

b2
|η|

j=1

pj

= Θ(b2−α )

B.1. Proof of Proposition 4
We are interested in finding the solution to the projection
problem

	
minimize Dkl (q||w) : q > 1 = 1, q ≥ pmin
q

where w ∈ Rb+ is a probability vector with its value at J-th
element shrunken down. Let the Lagrangian be

j=1

w(J(η∗ )+1) exp(η − 1) < pmin .
Now, note that eη
∗

∗

−1

−1

(19)

satisfies

=

1 − (b − J(η ∗ ))pmin
PJ(η∗ )
i=1 w(j)

Next, we show that J ∗ = b − 1 or b are the only possibilities.
1. Case wJ < pmin : Here, wJ = w(b) since p ≥ pmin .
Noting that wj = pj for j 6= J, if
w(b) Pb

1

j=1

qj log

and

from p> 1 = 1. Plugging this back into (19), we have that
f (J(η ∗ )) ≥ 0 and f (J(η ∗ )) < 0. Since f (j) is nonincreasing in j, it follows that J(η ∗ ) = J ∗ which show the
claim.

B. Procedures for Efficient Updates

b
X

1 − (b − J ∗ )pmin
.
PJ ∗
i=1 w(j)

∗

eη

which gives the second result.

L(q, η, θ) =

=

w(J(η∗ )) exp(η ∗ − 1) ≥ pmin

so that from (16), |η| = Θ(b
) and |I| = Θ(b). Using
these in the bound (17) for the objective, we obtain
L2j

−1

To this end, let J(η) := |I(η)|. For the optimal dual variable η ∗ , we have that J(η ∗ ) satisfies

 α1 − 12 !

2−α

b
X

∗

qj
− η(q > 1 − 1) − θ> (q − pmin 1)
wj

= wJ

w(j)

then J ∗ = b and eη

∗

−1

1
1 − pJ + wJ

=

Pb 1

j=1

wj

≥ pmin ,

. Otherwise, we

surely have that

where θ ∈ Rb+ . Writing down the first order conditions for
q, we have
q
log + 1 − η1 − θ = 0
w
>

where η ∈ R is the dual variable for q 1 = 1 − bpmin
and θ ∈ Rb+ is the dual variable for q ≥ 0. From strict
complementarity, we have that

1
w(j)

we have J ∗ = b and eη

∗

where I(η) := {1 ≤ j ≤ b : wj exp(η − 1) ≥ pmin }.
Now, assume that w is sorted and stored in a binary tree up
to a constant s. At each node, we also store the sum of the
values in the right/left subtree. Denote by w(1) ≥ . . . ≥
w(b) the sorted values of w. Let

=

Pb 1

j=1

wj

.

Combining the two cases and noting that


∗
q = weη −1 − pmin
+ pmin ,
+

we have

1

1−pJ +wJ w

q =  1−pmin
 1−pJ w − pmin
+ pmin
+

w(j)

−1

≥ w(b) ≥ pmin ,

(18)

j∈I(η)

i=1

2. Case wJ ≥ pmin : since

j=1

Then, it suffices to solve for η ∗ such that
X
(wj exp(η − 1) − pmin ) = 1 − bpmin

f (j) := w(j) (1 − (b − j)pmin ) − pmin

since pj = wj for j 6= J and pJ ≥ pmin . It follows
∗
min
that J ∗ = b − 1 and eη −1 = 1−p
1−pJ .

w(b) Pb

qj = (wj exp(η − 1) − pmin )+ + pmin

j
X

1 − pmin
1 − pmin
w(b−1) Pb−1
= w(b−1)
≥ w(b−1) ≥ pmin
1 − pJ
j=1 w(j)

if wJ ≥

pmin (1−pJ )
1−pmin

otherwise

Since wj ≥ pmin for j 6= J, result follows.

Adaptive Sampling Probabilities for Non-Smooth Optimization

B.2. Update
We recapitulate that a key characteristic of our tree implementation is that at each node, in addition to the value v, we
store suml , sumr , the sum of elements in the left/right subtree. Below, we give an algorithm that modifies an element
of the tree and updates suml , sumr of its parents accordingly. All node comparisons are based on their corresponding index values.
Algorithm 6 Update
1: wnew , J
2: Set value at index J to wnew
3: Initialize node with that of index J
4: while node.parent ! = NULL do
5:
if node is a left child then
6:
node.parent.suml ← node.parent.sumr
7:
+wnew − wold
8:
else
9:
node.parent.sumr ← node.parent.sumr
10:
+wnew − wold
11:
12: node ← node.parent

B.3. Sampling
For completeness, we give the pseudo-code for sampling
from the BST in O(log b).
Algorithm 7 Sample Tree
1: coin ← Uniform(0,1), node ← root
2: coin ← scale · coin
3: while node is not a leaf do
4:
if coin < node.v then
5:
Return node
6:
elseif coin < node.v + node.suml then
7:
coin ← coin − node.v
8:
node ← node.left
9:
else
10:
coin ← coin − node.v − node.suml
11:
node ← node.right
12: return node

C. Detailed synthetic experiments
Here we present further experiments for Sections 4.1 and
4.2. Namely, Figures 5 and 6 shows experiments over more
α and more values of c for each α. This more detailed
setup illustrates the phase change in behavior: from emulating uniform sampling at small α to learning and taking advantage of structure at large α. Interestingly, we see
that even though we are able to learn structure in sampling
over examples (Figure 6), the magnitude of improvement

over uniform sampling is smaller than in the coordinate
sampling case (Figure 5). We postulate that this is due to
the following effect: for an -optimality gap, SGD requires
O(R2 L2 /2 ) iterations, whereas O(R2 L2 b/2 ) iterations.
Since our bandit algorithm requires roughly O(b log b) iterations to learn structure, it has more time to take advantage
of this structure in coordinate sampling before reaching a
given optimality gap. For SGD, our adaptive algorithm
does better than leverage scores when α = 2.2 which is a
result of learning a more aggressive sampling distribution.
Figure 7 analyzes the effects of stepsize on the performance
of our algorithm. Specifically, we consider the same synthetic dataset as in Section 4.1, and we fix the number of
iterations to 105 . Varying the stepsize parameter β, we
track the optimality gap at the end of the procedure for our
method as well as uniform sampling. Although the sensitivity to stepsize (the width of the bowl) appears the same in
both approaches, our method is able to handle larger stepsizes (our “bowl” is shifted to the right) and learns more
efficiently for a given stepsize (our “bowl” is deeper) at
larger α. Importantly, we achieve these advantages solely
by tilting the sampling distribution to match the underlying
data’s structure.
Finally, we consider using the Gauss-Southwell rule for the
experiments in Section 4.1; similar to using blockwise Lipschitz constants, the Gauss-Southwell rule descends on the
coordinate with the largest gradient magnitude at every iteration. This method is inefficient except for specialized
loss functions, as seen in Figure 8.

Adaptive Sampling Probabilities for Non-Smooth Optimization
100

100

100

100

10-1
10-1

10-1
10-1
10-2
0

1

2

3

4

5

6

0

1

2

3

4

5

0

6

2

4

6

0

8

1

2

3

4

5

6

(a) Optimality gap vs. runtime
1

1

1

1

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0

0
0

50

100

150

200

250

0
0

50

100

150

200

0

250

0

(b) Learned probability distribution compared to j

−α

50

100

150

200

250

0

50

100

150

200

250

. We use c that resulted in the best performance.

Figure 5. Adaptive coordinate descent (left to right: α = 0.2, 0.4, 1.0, 2.2)
100

100

100

-1

10-1

100

10-1

10

10-2

10-1

0

0.5

1

1.5

2

2.5

3

3.5

4

10-3

10-2

10-2
0

0.5

1

1.5

2

2.5

3

3.5

0

4

0.5

1

1.5

2

2.5

3

3.5

4

0

1

2

3

4

(a) Optimality gap vs. runtime
1

1

1

1

0.8

0.8

0.8

0.8

0.6

0.6

0.6

0.6

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0

0
0

50

100

150

200

250

0
0

50

100

150

200

0

250

0

(b) Learned probability distribution compared to j

−α

50

100

150

200

250

Figure 6. Adaptive SGD (left to right: α = 0.4, 1.8, 2.2, 6.0)

10-1
-2

10

10-3

10-2

10-1

100

(a) α = 0.4

101

102

10-3

10-2

10-1

100

101

102

(b) α = 2.2

Figure 7. Optimality gap vs. stepsize after 105 iterations

0

50

100

. We use c that resulted in the best performance.

150

200

250

Adaptive Sampling Probabilities for Non-Smooth Optimization

100

100

100

100

10-1
10-1
10-1
10-1
10-2
0

1

2

3

4

5

0

1

2

3

4

5

0

1

2

3

4

5

6

7

8

0

1

2

3

4

5

Figure 8. Optimality gap vs. runtime for the same experiments as in Figure 5 with the Gauss-Southwell rule (left to right: α =
0.2, 0.4, 1.0, 2.2)

