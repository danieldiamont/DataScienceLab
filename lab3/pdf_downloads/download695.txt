Supplementary Materials

A

Inverse polynomial kernel and Gaussian kernel

In this appendix, we describe the properties of the two types of kernels — the inverse polynomial
kernel (14) and the Gaussian RBF kernel (15). We prove that the associated reproducing kernel
Hilbert Spaces (RKHS) of these kernels contain filters taking the form h : z 7→ σ(hw, zi) for
particular activation functions σ.

A.1

Inverse polynomial kernel

We first verify that the function (14) is a kernel function. This holds since that we can find a
mapping ϕ : Rd1 → `2 (N) such that K(z, z 0 ) = hϕ(z), ϕ(z 0 )i. We use zi to represent the i-th
coordinate of an infinite-dimensional vector z. The (k1 , . . . , kj )-th coordinate of ϕ(z), where j ∈ N
j+1
and k1 , . . . , kj ∈ [d1 ], is defined as 2− 2 xk1 . . . xkj . By this definition, we have
hϕ(x), ϕ(y)i =

∞
X

X

2−(j+1)

j=0

zk1 . . . zkj zk0 1 . . . zk0 j .

(k1 ,...,kj )∈[d1

(21)

]j

Since kzk2 ≤ 1 and kz 0 k2 ≤ 1, the series on the right-hand side is absolutely convergent. The inner
term on the right-hand side of equation (21) can be simplified to
X
zk1 . . . zkj zk0 1 . . . zk0 j = (hz, z 0 i)j .
(22)
(k1 ,...,kj )∈[d1 ]j

Combining equations (21) and (22) and using the fact that |hz, z 0 i| ≤ 1, we have
hϕ(z), ϕ(z 0 )i =

∞
X

(i)

2−(j+1) (hz, z 0 i)j =

j=0

1
= K(z, z 0 ),
2 − hz, z 0 i

which verifies that K is a kernel function and ϕ is the associated feature map. Next, we prove that
the associated RKHS contains the class of nonlinear filters. The lemma was proved by Zhang et al.
[2]. We include the proof to make the paper self-contained.
P∞
j
Lemma 1. Assume that the function σ(x) has a polynomial expansion σ(t) =
j=0 aj t . Let
qP
∞
j+1 a2 λ2j . If C (kwk ) < ∞, then the RKHS induced by the inverse polynomial
Cσ (λ) :=
σ
2
j=0 2
j
kernel contains function h : z 7→ σ(hw, zi) with Hilbert norm khkH = Cσ (kwk2 ).

1

Proof. Let ϕ be the feature map that we have defined for the polynomial inverse kernel. We define
vector w ∈ `2 (N) as follow: the (k1 , . . . , kj )-th coordinate of w, where j ∈ N and k1 , . . . , kj ∈ [d1 ],
j+1
is equal to 2 2 aj wk1 . . . wkj . By this definition, we have
σ(hw, zi) =

∞
X

aj (hw, zi)j =

t=0

∞
X

aj

j=0

X

wk1 . . . wkj zk1 . . . zkj = hw, ϕ(z)i,

(k1 ,...,kj )∈[d1

(23)

]j

P
j
where the first equation holds since σ(x) has a polynomial expansion σ(x) = ∞
j=0 aj x , the second
by expanding the inner product, and the third by definition of w and ϕ(z). The `2 -norm of w is
equal to:
kwk22 =

∞
X

X

2j+1 a2j

j=0

w2k1 w2k2 · · · w2kj =

(k1 ,...,kj )∈[d1

∞
X

2
2j+1 a2j kwk2j
2 = Cσ (kwk2 ) < ∞.

(24)

j=0

]j

By the basic property of the RKHS, the Hilbert norm of h is equal to the `2 -norm of w. Combining
equations (23) and (24), we conclude that h ∈ H and khkH = kwk2 = Cσ (kwk2 ).
According to Lemma 1, it suffices to upper bound Cσ (λ) for a particular activation function
σ. To make Cσ (λ) < ∞, the coefficients {aj }∞
j=0 must quickly converge to zero, meaning that the
activation function must be sufficiently smooth. For polynomial functions of degree `, the definition
of Cσ implies that Cσ (λ) = O(λ` ). For the sinusoid activation σ(t) := sin(t), we have
v
uX
u∞
22j+2
2
Cσ (λ) = t
· (λ2 )2j+1 ≤ 2eλ .
((2j + 1)!)2
j=0

For the erf function and the smoothed hinge loss function defined in Section 3.4, Zhang et al. [2,
2
Proposition 1] proved that Cσ (λ) = O(ecλ ) for universal numerical constant c > 0.

A.2

Gaussian kernel

The Gaussian kernel also induces an RKHS that contains a particular class of nonlinear filters. The
proof is similar to that of Lemma 1.
P∞
j
Lemma 2. Assume that the function σ(x) has a polynomial expansion σ(t) =
j=0 aj t . Let
qP
∞ j!e2γ 2 2j
Cσ (λ) :=
j=0 (2γ)j aj λ . If Cσ (kwk2 ) < ∞, then the RKHS induced by the Gaussian kernel
contains the function h : z 7→ σ(hw, zi) with Hilbert norm khkH = Cσ (kwk2 ).
Proof. When kzk2 = kz 0 k2 = 1, It is well-known [see, e.g. 1] the following mapping ϕ : Rd1 → `2 (N)
is a feature map for the Gaussian RBF kernel: the (k1 , . . . , kj )-th coordinate of ϕ(z), where j ∈ N
and k1 , . . . , kj ∈ [d1 ], is defined as e−γ ((2γ)j /j!)1/2 xk1 . . . xkj . Similar to equation (23), we define a
vector w ∈ `2 (N) as follow: the (k1 , . . . , kj )-th coordinate of w, where j ∈ N and k1 , . . . , kj ∈ [d1 ],
is equal to eγ ((2γ)j /j!)−1/2 aj wk1 . . . wkj . By this definition, we have
σ(hw, zi) =

∞
X
t=0

j

aj (hw, zi) =

∞
X
j=0

aj

X
(k1 ,...,kj )∈[d1 ]j

2

wk1 . . . wkj zk1 . . . zkj = hw, ϕ(z)i.

(25)

The `2 -norm of w is equal to:
kwk22 =

∞
X
j!e2γ 2
a
(2γ)j j
j=0

X

w2k1 w2k2 · · · w2kj =

(k1 ,...,kj )∈[d1

∞
X
j!e2γ 2
2
a kwk2j
2 = Cσ (kwk2 ) < ∞.
(2γ)j j

(26)

j=0

]j

Combining equations (23) and (24), we conclude that h ∈ H and khkH = kwk2 = Cσ (kwk2 ).
Comparing Lemma 1 and Lemma 2, we find that the Gaussian kernel imposes a stronger condition on the smoothness of the activation function. For polynomial functions of degree `, we still
have Cσ (λ) = O(λ` ). For the sinusoid activation σ(t) := sin(t), it can be verified that
v
u
∞
 λ2 2j+1
X
u
1
2
Cσ (λ) = te2γ
≤ eλ /(4γ)+γ .
·
(2j + 1)!
2γ
j=0

However, the value of Cσ (λ) is infinite when σ is the erf function or the smoothed hinge loss,
meaning that the Gaussian kernel’s RKHS doesn’t contain filters activated by these two functions.

B

Convex relaxation for nonlinear activation

In this appendix, we provide a detailed derivation of the relaxation for nonlinear activation functions
that we previously sketched in Section 3.2. Recall that the filter output is σ(hwj , zi). Appendix A
shows that given a sufficiently smooth activation function σ, we can find some kernel function
K : Rd1 × Rd1 → R and a feature map ϕ : Rd1 → `2 (N) satisfying K(z, z 0 ) ≡ hϕ(z), ϕ(z 0 )i, such that
σ(hwj , zi) ≡ hwj , ϕ(z)i.

(27)

Here wj ∈ `2 (N) is a countable-dimensional vector and ϕ := (ϕ1 , ϕ2 , . . . ) is a countable sequence
of functions. Moreover, the `2 -norm of wj is bounded as kwj k2 ≤ Cσ (kwj k2 ) for a monotonically
increasing function Cσ that depends on the kernel (see Lemma 1 and Lemma 2). As a consequence,
we may use ϕ(z) as the vectorized representation of the patch z, and use wj as the linear transformation weights, then the problem is reduced to training a CNN with the identity activation
function.
The filter is parametrized by an infinite-dimensional vector wj . Our next step is to reduce the
original ERM problem to a finite-dimensional one. In order to minimize the empirical risk, one
only needs to concern the output on the training data, that is, the output of hwj , ϕ(zp (xi ))i for
all (i, p) ∈ [n] × [P ]. Let T be the orthogonal projector onto the linear subspace spanned by the
vectors {ϕ(zp (xi )) : (i, p) ∈ [n] × [P ]}. Then we have
∀ (i, p) ∈ [n] × [P ] :

hwj , ϕ(zp (xi ))i = hwj , T ϕ(zp (xi ))i = hT wj , ϕ(zp (xi ))i.

The last equation follows since the orthogonal projector T is self-adjoint. Thus, for empirical risk
minimization, we can without loss of generality assume that wj belongs to the linear subspace
spanned by {ϕ(zp (xi )) : (i, p) ∈ [n] × [P ]} and reparametrize it by:
X
wj =
βj,(i,p) ϕ(zp (xi )).
(28)
(i,p)∈[n]×[P ]

3

Let βj ∈ RnP be a vector whose whose (i, p)-th coordinate is βj,(i,p) . In order to estimate wj , it
suffices to estimate the vector βj . By definition, the vector satisfies the relation βj> Kβj = kwj k22 ,
where K is the nP × nP kernel matrix defined in Section 3.2. As a consequence, if we can find a
matrix Q such that QQ> = K, then we have the norm constraint
q
kQ> βj k2 = βj> Kβj = kwj k2 ≤ Cσ (kwj k2 ) ≤ Cσ (B).
(29)
Let v(z) ∈ RnP be a vector whose (i, p)-th coordinate is equal to K(z, zp (xi )). Then by equations (27) and (28), the filter output can be written as

σ hwj , zi ≡ hwj , ϕ(z)i ≡ hβj , v(z)i.
(30)
For any patch zp (xi ) in the training data, the vector v(zp (xi )) belongs to the column space of the
kernel matrix K. Therefore, letting Q† represent the pseudo-inverse of matrix Q, we have
∀ (i, p) ∈ [n] × [P ] :

hβj , v(zp (xi ))i = βj> QQ† v(zp (xi )) = h(Q> )† Q> βj , v(zp (xi ))i.

It means that if we replace the vector βj on the right-hand side of equation (30) by the vector
(Q> )† Q> βj , then it won’t change the empirical risk. Thus, for ERM we can parametrize the filters
by
hj (z) := h(Q> )† Q> βj , v(z)i = hQ† v(z), Q> βj i.

(31)

Let Z(x) be an P × nP matrix whose p-th row is equal to Q† v(zp (x)). Similar to the steps in
equation (6), we have
fk (x) =

r
X

r

X

>
>
Z(x)Q> βj = tr Z(x)
= tr(Z(x)Ak ),
αk,j
Q> βj αk,j

j=1

j=1

Pr

>
>
where Ak :=
j=1 Q βj αk,j . If we let A := (A1 , . . . , Ad2 ) denote the concatenation of these
matrices, then this larger matrix satisfies the constraints:

Constraint (C1): max kQ> βj k2 ≤ Cσ (B1 ) and
j∈[r]

max
(k,j)∈[d2 ]×[r]

kαk,j k2 ≤ B2 .

Constraint (C2): The matrix A has rank at most r.
We relax these two constraints to the nuclear norm constraint:
p
kAk∗ ≤ Cσ (B1 )B2 r d2 .

(32)

By comparing constraints (8) and (32), we see that the only difference is that the term B1 in the
norm bound has been replaced by Cσ (B1 ). This change is needed because we have used the kernel
trick to handle nonlinear activation functions.

References
[1] I. Steinwart and A. Christmann. Support vector machines. Springer, New York, 2008.
[2] Y. Zhang, J. D. Lee, and M. I. Jordan. `1 -regularized neural networks are improperly learnable
in polynomial time. In Proceedings on the 33rd International Conference on Machine Learning,
2016.
4

