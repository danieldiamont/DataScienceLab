Supplementary Material of
Differentially Private Clustering in High-Dimensional Euclidean Spaces

Maria-Florina Balcan 1 Travis Dick 1 Yingyu Liang 2 Wenlong Mou 3 Hongyang Zhang 1

1. Additional Related Work
Non-Private Clustering: There is a wide range of prior work on the problem of center-based clustering in the absence of
privacy requirement. It is known that exact optimization of objective function in Rd is not computationally possible (Dasgupta, 2008) even for the problem of 2-means clustering. To avoid the computational obstacle, several approximation
algorithms have been developed, e.g., by the local swap (Kanungo et al., 2002; Arya et al., 2004), careful seeding (Arthur
& Vassilvitskii, 2007), or enumeration via sample-based loss estimator (Kumar et al., 2010). Another line of research
focuses on the recovery of optimal data partition under stability or separation assumption (Balcan et al., 2009; Awasthi &
Balcan, 2014).
It is worth noting that most of existing work for clustering in Rd with reasonable approximation guarantee relies on the
construction of a candidate set of centers. In particular, MatousÌŒek (2000) constructed a (1 + )-approximate candidate set
via griding argument, which was widely applied in the later work (Kanungo et al., 2002; Makarychev et al., 2015). Kumar
et al. (2010) took the average of a randomly sampled subset of data points as the set of candidate centers. However, none
of these approaches can be easily adapted to the differentially private settings.

2. Why Direct Extension Failed
It would be natural to ask, if one has read (MatousÌŒek, 2000), why their discretization methods cannot directly extend to
private setting, with randomized decision in the sub-division procedure. In the following example, we will see that, the
privatized vanilla recursive partition, i.e., direct use of discretization routine in Algorithm 1, will lead to arbitrarily bad
performance in our settings.

Data Points
Candidate Centers
(Center of each cube)

Figure 1. Illustration for the Worst Case for Partition Procedure

Consider a set of n points S in Rp , with p â‰¥ 1+log n. The initial cube we are working on is [âˆ’1, 1]p , which contains set S.
1

Carnegie Mellon University, Pittsburgh, PA, USA 2 Princeton University, Princeton, NJ, USA 3 Peking University, Beijing, China.
Correspondence to: Wenlong Mou <mouwenlong@pku.edu.cn>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Let S = {xi }ni=1 and xi = [1/2, 0, 0, Â· Â· Â· , 0]T + Î·Ïƒi , where Î· = p21n2 and Ïƒi âˆˆ [0, Â±1, Â±1, Â· Â· Â· , Â±1]T , Ïƒi 6= Ïƒj , âˆ€i 6= j.
Apparently S is a small cluster with clustering loss less than 1. Using the hierarchical partition procedure, we will get
2p cubes corresponding to 2p hyperoctants. According to their definition, each points in S is divided into a separate cube.
Due to calculation similar to Lemma 1, none of these cubes will be further divided, with high probability, and the partition
procedure ends up returning a set of candidate centers with distance at least 21 âˆ’ Î· to each of xi , and the resulted clustering
loss becomes at least â„¦(n). In Figure 1, we illustrate the bad case for discretization procedure.
Whatâ€™s wrong with this method? In (MatousÌŒek, 2000), the discretization procedure can carry on until meeting a threshold
that guarantees candidate cost at the same scale of optimal clustering loss. However, for privacy reasons, the partition
procedure has to stop, in order to hide the accurate location of a single data point. Thus we do not want a cluster to be
divided into too many parts during the procedure, as in the example above.
Fortunately, this worst case is extremely atypical in high dimensions, and can be avoided via repeated random shift. Using
probabilistic arguments in high dimensions, we can make sure that each optimal cluster is fully contained in the cube with
a proper scale.

3. Omitted Proofs in Section 4
Theorem 1 (Theorem 1 in the Main Body). The set C generated by Algorithm 1 satisfies |C| â‰¤ n log n, with probability
1 âˆ’ Î´.
Proof. First of all, it is easy to verify that the function f (Â·) defined in Algorithm 1 satisfies
Î´
.
n10
Consider a (non-private) tree T partitioning generated as follows: the root node corresponds to the initial cube Q. For
each round, we subdivide the cube in each dimensions evenly, resulting in 2p cubes. A smaller cube will be active when
it contains at least one point, until the depth of this node in the tree grows to be log n. The tree has at most log n levels,
while each level has at most n nodes. So the size of this tree is upper bounded with n log n. Let TÌƒ denote the partitioning
tree generated by Algorithm 1, we have




Pr TÌƒ * T â‰¤ Pr âˆƒn âˆˆ leaf(T ), n is partitioned in TÌƒ


X
â‰¤
Pr n is partitioned in TÌƒ
Pr (Qi partitioned|Qi empty) = f (0) â‰¤

nâˆˆleaf(T )

â‰¤|T |f (0) â‰¤ Î´.
With probability 1 âˆ’ Î´, the number of cells generated by Algorithm 1 is no more than |T |, and the size of C is thus
bounded.
Theorem 2 (Theorem 2 in the Main Body). Algorithm 1 preserves -differential privacy.
Proof. Consider a layers of discretization with a â‰¤ log n. By adaptive composition lemma, it suffices to show that
given the active cubes and candidate centers constructed in upper layers, the points added to C in this layer preserves

log n -differential privacy.
Given current C and A fixed, modification on a single data point will influence the point counts of at most two active cubes.
By definition we only need to show

 




(l) d
(l) d
Pr0 {Qi }2l=1 âŠ† A .
âˆ€i, Pr {Qi }2l=1 âŠ† A â‰¤ exp
S
2 log n S
It is easy to verify our construction of f (Â·) satisfies this bound.
Theorem 3 (Theorem 3 in the Main Body). The following event holds with probability at least 1 âˆ’ Î´: In Algorithm 1, when

a cube Qi is removed from A and its subdivided cubes are not inserted to A, then we have either |Qi âˆ© X| â‰¤ O Î³ log nÎ´ ,
or the edge length of Qi is at most Î›
n.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Proof. It is easy to verify that f (Â·) satisfies



n
n
Î´

Pr Qi partitioned |Qi âˆ© X| â‰¥ 10Î³ log
â‰¥ f 10Î³ log
â‰¥ 1 âˆ’ 10 .
Î´
Î´
n
Consider a (non-private) partition tree T 0 . The construction of this tree is based on bisection for each dimension, similar to
the proof of Theorem 1. However, this time a cell will stop being partitioned when the number of data points inside is less
than 10Î³ log nÎ´ . Apparently we have |T 0 | â‰¤ |T | â‰¤ 2p n log n, where T is the tree constructed in the proof of Theorem 1.
Since leaf nodes of T 0 satisfy the desired properties of this theorem, it suffices to show that T 0 âŠ† TÌƒ with probability 1 âˆ’ Î´.
Actually, we have




Pr T 0 * TÌƒ â‰¤ Pr âˆƒn âˆˆ internal(T 0 ), n is not partitioned in TÌƒ


X
â‰¤
Pr n is not partitioned in TÌƒ
nâˆˆinternal(T 0 )



n 
â‰¤ Î´.
â‰¤|T 0 | 1 âˆ’ f 10Î³ log
Î´
Theorem 4 (Theorem 4 in the Main Body). Algorithm 2 preserves -differential privacy.
Proof. The proof of privacy is simply done by T -fold composition theorem over the T independent trials of the private
partition procedure, each of which preserves T -differential privacy.

Theorem 5 (Theorem 5 in the Main Body). Algorithm 2 outputs an O(log3 n), O(kÎ³( T ) log nÎ´ ) candidate set with
n
n
probability at least 1 âˆ’ Î´, where Î³(c) = 40
c log Î´ log n, and T = k log Î´ .
Proof. Suppose the optimal set of centers are uâˆ—1 , uâˆ—2 , Â· Â· Â· , uâˆ—k , fixed but unknown. Let Sjâˆ— be the cluster induced by uâˆ—j , i.e.
q
P
Sj = {i : j = argminl kxi âˆ’ ul k}. Let rjâˆ— = |S1âˆ— | iâˆˆS âˆ— kxi âˆ’ ul k2 .
j

j


For âˆ€j = 1, 2, Â· Â· Â· , k, we say uâˆ—j is captured by C with factor L iff B uâˆ—j , Lrjâˆ— + O( n1 ) âˆ© C 6= âˆ…. If each uâˆ—j with at least

Î³( T ) log nÎ´ is captured by C with factor L, let uÌƒj âˆˆ B uâˆ—j , Lrjâˆ— + O( n1 ) âˆ© C, we have
ï£«
ï£¶
n
k X
k
X
X
X
X
ï£­
min kxi âˆ’ uÌƒj k2 â‰¤
kxj âˆ’ uÌƒj k2 â‰¤
kxj âˆ’ uâˆ—j k2 + |Sj | Â· kuâˆ—j âˆ’ uÌƒj k2 ï£¸
i=1

j

j=1 iâˆˆSjâˆ—

j=1

X

=OPT +

uâˆ—
j âˆˆcaptured

iâˆˆSjâˆ—

|Sj | Â· kuâˆ—j âˆ’ uÌƒj k2 +

X

|Sj | Â· kuâˆ—j âˆ’ uÌƒj k2

uâˆ—
/
j âˆˆcaptured



n 
â‰¤OPT + L2 Â· OPT + O(Î›2 ) + O kÎ³( ) log Î›2 .
T
Î´
Thus, to prove the quality of our candidate set, it suffices to show that each uâˆ—j with |Sjâˆ— | â‰¥ kÎ³( T ) log
3
2

n
Î´

is captured by C

with factor O(log n), with high probability.
P
For âˆ€j âˆˆ {1, 2, Â· Â· Â· , k} fixed, since |S1âˆ— | iâˆˆS âˆ— kxi âˆ’ ul k2 = rjâˆ— 2 , using Chebyshev Inequality, we have
j

j



 
B(uâˆ—j , 2rjâˆ— ) âˆ© Sjâˆ—  â‰¥ 1 Sjâˆ—  .
2
Given the randomized shift vector v in Algorithm 2 fixed, consider an (infinite) rectangular grid Gv , constructed by recursive
bisection on each dimension of Qv . There must exist a cube QÌ„(j) âˆˆ Gv , such that uâˆ—j âˆˆ QÌ„(j) and the edge length of QÌ„(j)
is between 2prjâˆ— and 4prjâˆ— . If any point in QÌ„(j) is added during the execution of Algorithm 1, its distance to uâˆ—j is at most
3
âˆš
4rjâˆ— p p = rjâˆ— Â· O(log 2 n). Therefore, if QÌ„j is ever active during the discretization, this cluster center must have been
captured by C.
According to Theorem 3, conditioned on the event that no failure occurs (which has negligible probability), there can only
be two cases for which QÌ„(j) does not appear in C:

Differentially Private Clustering in High-Dimensional Euclidean Spaces

â€¢ The edge length of QÌ„(j) is less than n1 ;



â€¢ QÌ„(j) âˆ© Sjâˆ—  â‰¤ O Î³( T ) log nÎ´ .
If it is the first case, we can turn to a cube QÌ„0 (j) âˆˆ Gv that contains QÌ„(j) with edge length exactly Î˜( n1 ). Since our
definition of capturing allows O( n1 ) additive error on the radius, uâˆ—j will also be captured if QÌ„0 (j) becomes active. Since
QÌ„0 (j) has larger size and potentially more points in Sjâˆ— , we only need to show that a cube containing uâˆ—j with edge length
at least 2prjâˆ— is likely to a large number of points in Sjâˆ— and thus becomes active in Algorithm 1.
Letâ€™s now turn to the randomness of v, in terms of captures for uâˆ—j , shifting Qv and Gv uniformly at random is equivalent
to shifting QÌ„(j) uniformly at random, given the fact that it contains uâˆ—j .
(The location of the cell of this scale that contains uâˆ—j may vary in the grid. However, uâˆ—j is indifferent with â€whichâ€ cell
contains it, its capture only depends on its relative location within this axis-aligned cell.)
Therefore, if |Sj | â‰¥ â„¦(Î³( T ) log nÎ´ ), we will have




n 
Pr uâˆ—j âˆˆ captured(v) â‰¥ Pr Sjâˆ— âˆ© QÌ„(j) â‰¥ â„¦(Î³( ) log )
T
Î´

 âˆ—
 1 âˆ—
â‰¥ Pr Sj âˆ© QÌ„(j) â‰¥ |Sj |)
2

âˆ—
âˆ—
â‰¥ Pr B(uj , 2rj ) âŠ† QÌ„j

p
2
1
= 1âˆ’
â‰¥
.
p
27
For each j âˆˆ {1, 2, Â· Â· Â· , k}, we assign 27 log nÎ´ independent trials to it, and it is easy to see that it is captured by C with
probability at least 1 âˆ’ nÎ´ . By aggregating the 27k log nÎ´ trails together and applying union bound, we conclude that with
probability at least 1 âˆ’ Î´, we have
 
n
âˆ€j âˆˆ {1, 2, Â· Â· Â· , k}, |Sjâˆ— | â‰¥ â„¦ Î³( ) log
â‡’ âˆƒuÌƒj âˆˆ C captures uâˆ—j .
T
Î´
So the proof is completed.

4. Omitted Proofs in Section 5
Theorem 6 (Theorem 6 in the Main Body). Algorithm 3 preserves -differential privacy.
Proof. The privacy guarantee is straightforward using composition theorem over T rounds of the algorithm, and an
additional exponential mechanism that selects the best one. It is easy to verify the sensitivity of loss increments
L(Z âˆ’ {x} + {y}) âˆ’ L(Z) is 8Î›2 , the privacy guarantee of exponential mechanism in each round follows.
Theorem 7 (Theorem 7 in the Main Body). With probability 1 âˆ’ Î´, the output of Algorithm 3 satisfies
 2 2

k Î›
n|C|
L(Z) â‰¤ 30OPT + O
log2
.

Î´
Proof. The proof inspires from (Gupta et al., 2010). Basically, we use the following fact, which is derived from the
construction of swap pairs and Lemma 2.2 in (Kanungo et al., 2002):
For clustering centers Z, there exists a set of k swaps {(xi , yi )}ki=1 such that
k
X
i=1

L(Z) âˆ’ L(Z âˆ’ {xi } + {yi }) â‰¥ 3L(Z) âˆ’ OPT âˆ’ 2

n
X

kxi âˆ’ z(oi )k2 ,

i=1

where oi is the optimal cluster to which xi is assigned, and z(o) is point oâ€™s nearest center in Z.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Following the proof for local swap heuristics, we can bound the extra term.
R=

n
n X
X

kxi âˆ’ z(oi )k2 â‰¤ 2OPT + L(Z) + 2

i=1 i=1

n
X

kxi âˆ’ oi k Â· kxi âˆ’ zi k

i=1

â‰¤2OPT + L(Z) + 2

p
OP T Â· L(Z).

Putting them together, we have that âˆƒx âˆˆ Z, y âˆˆ C such that
1
L(Z) âˆ’ L(Z âˆ’ {x} + {y}) â‰¥ (3L(Z) âˆ’ OPT âˆ’ 2R)
k

p
1
â‰¥
L(Z) âˆ’ 5OPT âˆ’ 4 OPT Â· L(Z)
k
1
â‰¥ (L(Z) âˆ’ 30OPT) .
2k
The rest of this proof
 proceedsjust as in (Gupta et al., 2010): exponential mechanism guarantees the bound 4 holds with
with failure probability nÎ´ . T rounds of iteration guarantees the multiplicative term being
an additive term O 1 log n|C|
Î´
2

reduced to constant order, except for the case of OPT = O( k log2 n|C|
Î´ ) where all the excess loss goes to the additive
term. Combining these facts together using union bound over failure probability we conclude the result.
Theorem 8 (Theorem 8 in the Main Body). Under the following assumptions:
â€¢ Algorithm candidate ({xi }ni=1 , , Î´) preserves -differential privacy for {xi }ni=1 .
â€¢ Given any C, Algorithm localswap ({xi }ni=1 , C, , Î´) preserves -differential privacy for {xi }ni=1 .
Algorithm 4 preserves -differential privacy.

Proof. In each of T -rounds, the two sub-routines each preserves 6T
-DP. Given the centers u1 , u2 , Â· Â· Â· , uk in projected
space fixed, changing the position for one of data points (and also resulting the change in projected space) will affect at

-DP, and the recovery procedure also
most two clustering centers. The noised version of cluster count sj preserves 12T

preserves 12T -DP given the cluster counts known and fixed. At last, the exponential mechanism preserves 6 -DP, putting
them together using composition theorem, we have the privacy guarantee.

Theorem 9 (Theorem 9 in the Main Body). Instantiated by algorithms that guarantee:
â€¢ With probability 32 , algorithm candidate ({xi }ni=1 , , Î´) can output an (Î±, Ïƒ1 ())-approximate candidate set.
â€¢ With probability 32 , algorithm localswap ({xi }ni=1 , C, , Î´) achieves clustering loss with multiplicative approximation factor c and additive term Ïƒ2 (), compared with optimal clustering centers in the discrete space.
Algorithm 4 achieves the following bound with probability 1 âˆ’ Î´:
L
where Ïƒi0 = Ïƒi




2 log 1/Î´





k
{zj }j=1



â‰¤ 3cÎ±OPT +

3CÏƒ10

+

3Ïƒ20

+O

dÎ›2 log3
2

1
Î´

!
,

for i = 1, 2

Proof. With probability 13 , the centers u1 , u2 , Â· Â· Â· , uk in projected space satisfies
n
X
i=1

min kyi âˆ’ uj k2 â‰¤ cOPTdiscrete + Ïƒ2 â‰¤ cÎ±(OPTprojected + Ïƒ1 ) + Ïƒ2 .
j

Differentially Private Clustering in High-Dimensional Euclidean Spaces

The JL transform guarantees that with probability 12 , the pairwise distances between points uniformly satisfies
âˆ€i, j âˆˆ {1, 2, Â· Â· Â· n},

kyi âˆ’ yj k2 â‰¤



1
1Â±
kxi âˆ’ xj k2 .
2

This event is also independent with randomized algorithms in projected spaces.
Since the optimal clustering loss for k-means depends only on the pairwise distances between data points, we have the
following fact under the event that JL transform successfully preserves pairwise distances: (Cj denotes optimal clustering
assignment)
OPTprojected â‰¤

X

kui âˆ’ Î½jâˆ— k2 =

k
X

iâˆˆCjâˆ—

â‰¤

j=1

1 X X
kui âˆ’ ul k2
2|Cjâˆ— |
âˆ—
âˆ—
iâˆˆCj lâˆˆCj

k
3X 1 X X
3
kxi âˆ’ xl k2 = OPT.
2 j=1 2|Cjâˆ— |
2
âˆ—
âˆ—
iâˆˆCj lâˆˆCj

On the other hand, we have
X

kxi âˆ’ Âµj k2 =

j=1

iâˆˆSj

â‰¤2

k
X
j=1

k
X

1 XX
kxi âˆ’ xl k2
2|Sj |
iâˆˆSj lâˆˆSj

X
1 XX
kui âˆ’ ul k2 = 2
kui âˆ’ Î½j k2 ,
2|Sj |
iâˆˆSj lâˆˆSj

iâˆˆSj

where Âµj (Âµâˆ—j , resp.) are clustering center for Sj (Cjâˆ— , resp.) in Rd , and Î½j (Î½jâˆ— , resp.) are clustering center for Sj (Cjâˆ— ,
resp.) in Rp .
The noise added to cluster sizes, as well as the Laplacian mechanism, leads to the additional term. Thus we have the
desired bound to hold with constant probability. By repeating it with T independent trials and selecting the best, the failure
probability is reduced to Î´. The additive loss induced by exponential mechanism in the last step is dominated by previous
terms.

5. Omitted Proofs in Section 6
Theorem 10 (Theorem 10 in the Main Body). Algorithm 5 preserves -differential privacy.
Proof. The privacy proof is straightforward: for 2s
Î· rounds of the algorithm, each round is exponential mechanism ac
-DP, the privacy proof then follows via
companied by Laplacian mechanism. Using the fact that both parts preserves 2T
composition.
Theorem 11 (Theorem 11 in the Main Body). The output of Algorithm 5 satisfies the following with probability at least
1 âˆ’ Î´:
!
ds
n
X
Î›2 s2 ln Î·Î´
1
2
kxi âˆ’ vk â‰¤
OPT + O
.
1âˆ’Î·
Î·2 
i=1
Proof. Let Ï€(j) denote the index of entries in Âµ with the j-th largest absolute value. Note that removing j entries from
{1, 2, Â· Â· Â· d} makes the largest absolute value at least Âµj+1 , exponential mechanism guarantees that, for the index rj sampled
in j-th round, we have the following with probability at least Î·Î´
2s :
|Âµrj | â‰¥ |ÂµÏ€(j) | âˆ’ O

ds
Î›s ln Î·Î´

Î·n

!
,

s
j = 1, 2, Â· Â· Â· , .
Î·

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Let ÂµÌƒ =

Ps/Î·

j=1

ÂµÏ€(j) eÏ€(j) , For âˆ€j âˆˆ {1, 2, Â· Â· Â· , d}, let Sj = {i âˆˆ [n] : xij 6= 0}, cj = |Sj | and vj =
d
X

nkÂµÌƒ âˆ’ Âµk2 = n

d
X

Âµ2Ï€(j) =

j=s/Î·+1

2
c2Ï€(j) vÏ€(j)

n

j=s/Î·+1

1
cj

P

iâˆˆSj

xij .

.

On the other hand, we have
ï£¶
d
X
X
cj (n âˆ’ cj ) 2
c
(n
âˆ’
c
)
j
ï£­ j
|xij âˆ’ cj |2 ï£¸ â‰¥
vj2 +
vj .
kxi âˆ’ Âµk2 =
n
n
j=1
i=1
j=1

n
X

d
X

ï£«

iâˆˆSj

Sort the d entries again according to the value of cj and let Ï„ (j) denote the index of entries in Âµ with the j-th largest cj , we
have
d
d
2
X
X
c2Ï€(j) vÏ€(j)
c2Ï„ (j) vÏ„2(j)
nkÂµÌƒ âˆ’ Âµk2 =
â‰¤
.
n
n
j=s/Î·+1

Since

Pd

j=1 cj

j=s/Î·+1

â‰¤ ns, Markov inequality implies |{j âˆˆ [d] : cj â‰¥ Î·n}| â‰¤ Î·s . Thus we have
d
X
j=s/Î·+1

c2Ï„ (j) vÏ„2(j)
n

â‰¤

Î·
1âˆ’Î·

d
X

cÏ„ (j) (n âˆ’ cÏ„ (j) )vÏ„2(j)
n

j=s/Î·+1

d
2
Î· X cÏ„ (j) (n âˆ’ cÏ„ (j) )vÏ„ (j)
Î·
â‰¤
â‰¤
OPT.
1 âˆ’ Î· j=1
n
1âˆ’Î·

Putting them together, we have
n
X

2

kxi âˆ’ vk =

i=1

n
X

kxi âˆ’ Âµk2 + nkÂµ âˆ’ vk2

i=1
2

â‰¤OPT + nkÂµ âˆ’ ÂµÌƒk + O
â‰¤

1
OPT + O
1âˆ’Î·

ds
Î›2 s2 ln Î·Î´

!

Î·2 
!
ds

Î›2 s2 ln Î·Î´
Î·2 

.

Theorem 12 (Theorem 12 in the Main Body). For k-median clustering problem, there is an -differential private algorithms that runs in poly(k, d, n) time, which releases a set of centers zÌƒ1 , zÌƒ2 , Â· Â· Â· , zÌƒk , that satisfies the following with
probability 1 âˆ’ Î´:
 2

n
(k + d)Î›
log3
.
L(zÌƒ1 , zÌƒ2 , Â· Â· Â· , zÌƒk ) â‰¤ O(log3/2 n) Â· OPT + O

Î´
Proof. By plugging in the error bounds and simple calculation, the centers u1 , u2 , Â· Â· Â· , uk in projected space satisfy the
following with probability 1 âˆ’ Î´:
n
X
i=1

min kyi âˆ’ uj k2 â‰¤ O(log3/2 n) Â· OPTprojected + O
j



(k 2 + d)Î›
n
log3

Î´


.

The JL transform preserves distances with up to 1 Â± 21 multiplicative error with probability 32 . To make use of this fact, we
replace sum-of-square decomposition with triangle inequality, using data points as intermediate step.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

OPTprojected â‰¤

X

kui âˆ’ Î½jâˆ— k â‰¤

iâˆˆCjâˆ—

â‰¤

k
X

min

j=1

l

X

kui âˆ’ ul k

iâˆˆCj

k
X
3X
3
min
kxi âˆ’ xl k = OPT.
2 j=1 l
2
iâˆˆCj

On the other hand, we have
X

kxi âˆ’ Âµj k â‰¤

j=1

iâˆˆSj

â‰¤2

k
X
j=1

k
X

min
l

X
lâˆˆSj

min
l

X

kxi âˆ’ xl k

lâˆˆSj

kui âˆ’ ul k â‰¤ 4

X

kui âˆ’ Î½j k,

iâˆˆSj

where Âµj (Âµâˆ—j , resp.) are clustering center for Sj (Cjâˆ— , resp.) in Rd , and Î½j (Î½jâˆ— , resp.) are clustering center for Sj (Cjâˆ— ,
resp.) in Rp .
The excess losses incurred by log-concave sampling and discrete exponential mechanisms in the algorithm are dominated
by the previous term. Putting them altogether, we have the bound.

6. Additional Experimental Details
In this section we provide additional details about experiments in our paper. We will first give a detailed description on
the real-world and synthetic datasets we are using, and then provide additional details on the comparison to existing works
including (Nock et al., 2016) and (Su et al., 2016). We will also present results about effect of number of clusters k and
dimension d on the clustering loss.
6.1. Description of Datasets
We compare our algorithm against the non-private k-means++ algorithm, SuLQ, k-variates++ (Nock et al., 2016), lowdimensional algorithm (Su et al., 2016) and Sample and Aggregate on the following datasets.
MNIST: We used the raw pixels of the MNIST (LeCun et al., 1998) dataset. It has 70000 examples and 784 features.
CIFAR-10: We used the CIFAR10 dataset (Krizhevsky, 2009). Rather than the raw pixels, we obtained our feature representations from layer in3c (160 features) of a Google Inception (Szegedy et al., 2015) network trained for the classification
task. We also created a second version of this dataset using the feature representations from layer in4d (144 features).
Our dataset contains 100000 randomly sampled examples from this dataset.
Synthetic: For experiments in the main body, we used a synthetic datasets of 100000 samples drawn from an equal-weight
mixture of 64 Gaussians in R100 . Each Gaussianâ€™s covariance matrix is the identity and the mean is randomly sampled from
[0, 100]d . For further experiments in Appendix, we change the value of k and d to illustrate the effect of these parameters.
In these experiments we always set k equal to the number of Gaussians in the mixture, with k âˆˆ {8, 16, 32, 64}, and we
also select d âˆˆ {5, 50, 500, 5000} to illustrate effect of dimension.
6.2. Additional Comparisons to Existing Works
We can also compare our approach with recent existing works such as (Nock et al., 2016) and (Nock et al., 2016) through
experiments. Based on experimental comparisons, we find that our algorithm is the only one that works reasonably well
simultaneously for large d and large k, while keeping good performance with small k and d.
It has been noticed that, the griding algorithm in (Su et al., 2016) only works for spaces with constant dimensions (see
e.g. (Park et al., 2016)). So we are unable to evaluate Su et al.â€™s griding algorithm on the above datasets. Actually their
algorithm has time and space complexity that is exponential in the data dimension (because it constructs a regular grid in
d dimensions). In experiments, their grids caused memory allocation failure for d > 6, with 16GB RAM memory.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

On the other hand, k-variates++ algorithm (Nock et al., 2016) is not designed for clustering problem with k more than
constant. In their experimental part for privacy, (Nock et al., 2016) only did experiments with k â‰¤ 5. Actually, its noise
scale Ëœ relies upon empirical estimates of data-dependent parameter Î´w and Î´s . We estimated these parameters on MNIST
dataset with varying k and plug them into the formula. As k becomes larger than 5, the formula for setting parameter Ëœ in
Theorem 12 of (Nock et al., 2016) becomes negative, which makes the algorithm invalid.
Therefore, we carried on three sets of experiments to compare our methods with recent baselines.
â€¢ To compare with (Nock et al., 2016) in high dimensions, we did experiments on MNIST dataset with k âˆˆ {2, 3, 4, 5}.
â€¢ To compare with (Su et al., 2016) with many clusters, we did experiments on Gaussian mixture dataset with k = 32
and d = 5, which will be discussed in the results about varying d.
â€¢ To compare all approaches together, we also carried out a small scale experiment on a mixture of Gaussian dataset
with 100000 samples in d = 3 dimensions with k = 4.
In Figure 2, we do the comparison in the first setting, where we set the privacy parameter  = 0.5. The plot is in
logarithmic scale, and we can easily seen that 2 ends up adding too much noise for slightly larger k, while our algorithm
performs reasonably well.

k-means objective

1018
k-variates++
ours

1016

10

14

1012
1010

2

3

4

5

k
Figure 2. Comparison to (Nock et al., 2016) on MNIST dataset with k = 2, 3, 4, 5

We show the results of the third setting in Table 1, and compare all existing approaches together. Among all private
methods, the griding algorithm in (Su et al., 2016) achieves the lowest objective value in this setting, while our algorithm
and SuLQ both achieved reasonably good performance. As our focus is on modern big data setting with high-dimensional
datasets, in this cases the griding algorithm cannot be applied.
Table 1. Objective values for all baseline algorithms on small-scale synthetic dataset.

Algorithm
k-means++
(Su et al., 2016)
SuLQ k-means
Ours
(Nock et al., 2016)
S&A

Objective Value
2.636e5
2.638e5
2.927e5
2.985e5
1.390e6
2.831e6

It is also worth noticing that running (Su et al., 2016) after randomized dimensionality reduction is not a polynomial-time
algorithm. Actually the regular grids need nâ„¦(d) cells to preserve good performance, and for the case of d âˆ¼ log n, the
running time and storage is still prohibitive. Even for constant-dimensional case like d = 5, the griding algorithm (Su
et al., 2016) runs more than 5 hours while our algorithm runs within 15 minutes on the same machine. As the time and
space costs for (Su et al., 2016) grows rapidly with d âˆ¼ log n > 5, the costs will become prohibitive.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

6.3. Additional CIFAR-10 Dataset
In this section we present the empirical comparison of algorithms on our second CIFAR-10 dataset, which is identical to
the first except features are instead taken from a later layer of the inception network (layer in4d). Figure 3 shows the
k-means objective of each algorithm when run for values of k from 2 to 64. Results are averaged over 5 runs.

k-means objective

8 Ã—10

4

7
6
5
non-private
ours
SuLQ

4
3

21

22

23

24

25

26

k
Figure 3. Objective values for various algorithms on the CIFAR-10 dataset with features extracted from layer in4d of a Google inception
network.

6.4. Effect of dimension
In this section we directly evaluate the effect of the dimension on the objective value of the various algorithms. For
this evaluation, we generate samples of size n = 100000 sampled from a mixture of 32 Gaussians with dimensions
d âˆˆ {5, 50, 500, 5000}. All algorithms are run with the privacy parameter  = 0.5 and k = 32. Figure 4 shows the results
of this experiment. This provides further justification for our claim that our algorithm scales to larger dimensions better
than the existing algorithms. Again, the sample and aggregate algorithm is omitted from the plot because its objective
value is several orders of magnitude worse and makes the plot difficult to read, even in log scale.

k-means objective

1014
non-private
ours
SuLQ

1012
1010

10

8

106

5

50

500

5000

dimension
Figure 4. Objective values for various algorithms on synthetic datasets of increasing dimension.

To make a comprehensive comparison, we also run the griding algorithm in (Su et al., 2016) for the case of d = 5. (As we
have discussed before, their algorithm is prohibitive for larger d). Their algorithm got clustering loss 1.84 Ã— 107 , which
is much smaller compared to our clustering loss 6.43 Ã— 107 . However, the running time of their algorithm is much longer
than us in this setting. And thereâ€™s no obvious approach for their algorithm to scale up with k, as we have discussed.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

6.5. Effect of number of intrinsic clusters
Finally, we directly evaluate the effect of the number of intrinsic clusters in the dataset. To do this, we generate datasets
of n = 100000 sampled from mixtures of G âˆˆ {8, 16, 32, 64} Gaussians in 100 dimensional space. All algorithms are run
with k set to the true number of intrinsic clusters for each dataset. Figure 5 shows the results of this comparison.

k-means objective

9
12 Ã—10

non-private
ours
SuLQ

10
8
6
4
2
0

10

20

30

40

50

60

Number of Clusters
Figure 5. Objective values for various algorithms on synthetic datasets of growing intrinsic numbers of clusters.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

References
Arthur, David and Vassilvitskii, Sergei. k-means++: The advantages of careful seeding. In ACM-SIAM Symposium on
Discrete Algorithms, pp. 1027â€“1035, 2007.
Arya, Vijay, Garg, Naveen, Khandekar, Rohit, Meyerson, Adam, Munagala, Kamesh, and Pandit, Vinayaka. Local search
heuristics for k-median and facility location problems. SIAM Journal on computing, 33(3):544â€“562, 2004.
Awasthi, Pranjal and Balcan, Maria-Florina. Center based clustering: A foundational perspective. 2014.
Balcan, Maria-Florina, Blum, Avrim, and Gupta, Anupam. Approximate clustering without the approximation. In ACMSIAM Symposium on Discrete Algorithms, pp. 1068â€“1077, 2009.
Dasgupta, Sanjoy. The hardness of k-means clustering. Department of Computer Science and Engineering, University of
California, San Diego, 2008.
Gupta, Anupam, Ligett, Katrina, McSherry, Frank, Roth, Aaron, and Talwar, Kunal. Differentially private combinatorial
optimization. In ACM-SIAM symposium on Discrete Algorithms, pp. 1106â€“1125, 2010.
Kanungo, Tapas, Mount, David M, Netanyahu, Nathan S, Piatko, Christine D, Silverman, Ruth, and Wu, Angela Y. A local
search approximation algorithm for k-means clustering. In Annual Symposium on Computational Geometry, pp. 10â€“18,
2002.
Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
Kumar, Amit, Sabharwal, Yogish, and Sen, Sandeep. Linear-time approximation schemes for clustering problems in any
dimensions. Journal of the ACM, 57(2):5, 2010.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. In Proceedings
of the IEEE, 1998.
Makarychev, Konstantin, Makarychev, Yury, Sviridenko, Maxim, and Ward, Justin. A bi-criteria approximation algorithm
for k means. arXiv preprint arXiv:1507.04227, 2015.
MatousÌŒek, JirÄ±. On approximate geometric k-clustering. Discrete & Computational Geometry, 24(1):61â€“84, 2000.
Nock, Richard, Canyasse, RaphaeÌˆl, Boreli, Roksana, and Nielsen, Frank. k-variates++: more pluses in the k-means++.
arXiv preprint arXiv:1602.01198, 2016.
Park, Mijung, Foulds, Jimmy, Chaudhuri, Kamalika, and Welling, Max. Practical privacy for expectation maximization.
arXiv preprint arXiv:1605.06995, 2016.
Su, Dong, Cao, Jianneng, Li, Ninghui, Bertino, Elisa, and Jin, Hongxia. Differentially private k-means clustering. In
Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy, pp. 26â€“37. ACM, 2016.
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. IEEE Conference on Computer Vision and
Pattern Recognition, 2015.

