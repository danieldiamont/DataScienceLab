Supplementary Material of
Differentially Private Clustering in High-Dimensional Euclidean Spaces

Maria-Florina Balcan 1 Travis Dick 1 Yingyu Liang 2 Wenlong Mou 3 Hongyang Zhang 1

1. Additional Related Work
Non-Private Clustering: There is a wide range of prior work on the problem of center-based clustering in the absence of
privacy requirement. It is known that exact optimization of objective function in Rd is not computationally possible (Dasgupta, 2008) even for the problem of 2-means clustering. To avoid the computational obstacle, several approximation
algorithms have been developed, e.g., by the local swap (Kanungo et al., 2002; Arya et al., 2004), careful seeding (Arthur
& Vassilvitskii, 2007), or enumeration via sample-based loss estimator (Kumar et al., 2010). Another line of research
focuses on the recovery of optimal data partition under stability or separation assumption (Balcan et al., 2009; Awasthi &
Balcan, 2014).
It is worth noting that most of existing work for clustering in Rd with reasonable approximation guarantee relies on the
construction of a candidate set of centers. In particular, Matoušek (2000) constructed a (1 + )-approximate candidate set
via griding argument, which was widely applied in the later work (Kanungo et al., 2002; Makarychev et al., 2015). Kumar
et al. (2010) took the average of a randomly sampled subset of data points as the set of candidate centers. However, none
of these approaches can be easily adapted to the differentially private settings.

2. Why Direct Extension Failed
It would be natural to ask, if one has read (Matoušek, 2000), why their discretization methods cannot directly extend to
private setting, with randomized decision in the sub-division procedure. In the following example, we will see that, the
privatized vanilla recursive partition, i.e., direct use of discretization routine in Algorithm 1, will lead to arbitrarily bad
performance in our settings.

Data Points
Candidate Centers
(Center of each cube)

Figure 1. Illustration for the Worst Case for Partition Procedure

Consider a set of n points S in Rp , with p ≥ 1+log n. The initial cube we are working on is [−1, 1]p , which contains set S.
1

Carnegie Mellon University, Pittsburgh, PA, USA 2 Princeton University, Princeton, NJ, USA 3 Peking University, Beijing, China.
Correspondence to: Wenlong Mou <mouwenlong@pku.edu.cn>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Let S = {xi }ni=1 and xi = [1/2, 0, 0, · · · , 0]T + ησi , where η = p21n2 and σi ∈ [0, ±1, ±1, · · · , ±1]T , σi 6= σj , ∀i 6= j.
Apparently S is a small cluster with clustering loss less than 1. Using the hierarchical partition procedure, we will get
2p cubes corresponding to 2p hyperoctants. According to their definition, each points in S is divided into a separate cube.
Due to calculation similar to Lemma 1, none of these cubes will be further divided, with high probability, and the partition
procedure ends up returning a set of candidate centers with distance at least 21 − η to each of xi , and the resulted clustering
loss becomes at least Ω(n). In Figure 1, we illustrate the bad case for discretization procedure.
What’s wrong with this method? In (Matoušek, 2000), the discretization procedure can carry on until meeting a threshold
that guarantees candidate cost at the same scale of optimal clustering loss. However, for privacy reasons, the partition
procedure has to stop, in order to hide the accurate location of a single data point. Thus we do not want a cluster to be
divided into too many parts during the procedure, as in the example above.
Fortunately, this worst case is extremely atypical in high dimensions, and can be avoided via repeated random shift. Using
probabilistic arguments in high dimensions, we can make sure that each optimal cluster is fully contained in the cube with
a proper scale.

3. Omitted Proofs in Section 4
Theorem 1 (Theorem 1 in the Main Body). The set C generated by Algorithm 1 satisfies |C| ≤ n log n, with probability
1 − δ.
Proof. First of all, it is easy to verify that the function f (·) defined in Algorithm 1 satisfies
δ
.
n10
Consider a (non-private) tree T partitioning generated as follows: the root node corresponds to the initial cube Q. For
each round, we subdivide the cube in each dimensions evenly, resulting in 2p cubes. A smaller cube will be active when
it contains at least one point, until the depth of this node in the tree grows to be log n. The tree has at most log n levels,
while each level has at most n nodes. So the size of this tree is upper bounded with n log n. Let T̃ denote the partitioning
tree generated by Algorithm 1, we have




Pr T̃ * T ≤ Pr ∃n ∈ leaf(T ), n is partitioned in T̃


X
≤
Pr n is partitioned in T̃
Pr (Qi partitioned|Qi empty) = f (0) ≤

n∈leaf(T )

≤|T |f (0) ≤ δ.
With probability 1 − δ, the number of cells generated by Algorithm 1 is no more than |T |, and the size of C is thus
bounded.
Theorem 2 (Theorem 2 in the Main Body). Algorithm 1 preserves -differential privacy.
Proof. Consider a layers of discretization with a ≤ log n. By adaptive composition lemma, it suffices to show that
given the active cubes and candidate centers constructed in upper layers, the points added to C in this layer preserves

log n -differential privacy.
Given current C and A fixed, modification on a single data point will influence the point counts of at most two active cubes.
By definition we only need to show

 




(l) d
(l) d
Pr0 {Qi }2l=1 ⊆ A .
∀i, Pr {Qi }2l=1 ⊆ A ≤ exp
S
2 log n S
It is easy to verify our construction of f (·) satisfies this bound.
Theorem 3 (Theorem 3 in the Main Body). The following event holds with probability at least 1 − δ: In Algorithm 1, when

a cube Qi is removed from A and its subdivided cubes are not inserted to A, then we have either |Qi ∩ X| ≤ O γ log nδ ,
or the edge length of Qi is at most Λ
n.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Proof. It is easy to verify that f (·) satisfies



n
n
δ

Pr Qi partitioned |Qi ∩ X| ≥ 10γ log
≥ f 10γ log
≥ 1 − 10 .
δ
δ
n
Consider a (non-private) partition tree T 0 . The construction of this tree is based on bisection for each dimension, similar to
the proof of Theorem 1. However, this time a cell will stop being partitioned when the number of data points inside is less
than 10γ log nδ . Apparently we have |T 0 | ≤ |T | ≤ 2p n log n, where T is the tree constructed in the proof of Theorem 1.
Since leaf nodes of T 0 satisfy the desired properties of this theorem, it suffices to show that T 0 ⊆ T̃ with probability 1 − δ.
Actually, we have




Pr T 0 * T̃ ≤ Pr ∃n ∈ internal(T 0 ), n is not partitioned in T̃


X
≤
Pr n is not partitioned in T̃
n∈internal(T 0 )



n 
≤ δ.
≤|T 0 | 1 − f 10γ log
δ
Theorem 4 (Theorem 4 in the Main Body). Algorithm 2 preserves -differential privacy.
Proof. The proof of privacy is simply done by T -fold composition theorem over the T independent trials of the private
partition procedure, each of which preserves T -differential privacy.

Theorem 5 (Theorem 5 in the Main Body). Algorithm 2 outputs an O(log3 n), O(kγ( T ) log nδ ) candidate set with
n
n
probability at least 1 − δ, where γ(c) = 40
c log δ log n, and T = k log δ .
Proof. Suppose the optimal set of centers are u∗1 , u∗2 , · · · , u∗k , fixed but unknown. Let Sj∗ be the cluster induced by u∗j , i.e.
q
P
Sj = {i : j = argminl kxi − ul k}. Let rj∗ = |S1∗ | i∈S ∗ kxi − ul k2 .
j

j


For ∀j = 1, 2, · · · , k, we say u∗j is captured by C with factor L iff B u∗j , Lrj∗ + O( n1 ) ∩ C 6= ∅. If each u∗j with at least

γ( T ) log nδ is captured by C with factor L, let ũj ∈ B u∗j , Lrj∗ + O( n1 ) ∩ C, we have


n
k X
k
X
X
X
X

min kxi − ũj k2 ≤
kxj − ũj k2 ≤
kxj − u∗j k2 + |Sj | · ku∗j − ũj k2 
i=1

j

j=1 i∈Sj∗

j=1

X

=OPT +

u∗
j ∈captured

i∈Sj∗

|Sj | · ku∗j − ũj k2 +

X

|Sj | · ku∗j − ũj k2

u∗
/
j ∈captured



n 
≤OPT + L2 · OPT + O(Λ2 ) + O kγ( ) log Λ2 .
T
δ
Thus, to prove the quality of our candidate set, it suffices to show that each u∗j with |Sj∗ | ≥ kγ( T ) log
3
2

n
δ

is captured by C

with factor O(log n), with high probability.
P
For ∀j ∈ {1, 2, · · · , k} fixed, since |S1∗ | i∈S ∗ kxi − ul k2 = rj∗ 2 , using Chebyshev Inequality, we have
j

j



 
B(u∗j , 2rj∗ ) ∩ Sj∗  ≥ 1 Sj∗  .
2
Given the randomized shift vector v in Algorithm 2 fixed, consider an (infinite) rectangular grid Gv , constructed by recursive
bisection on each dimension of Qv . There must exist a cube Q̄(j) ∈ Gv , such that u∗j ∈ Q̄(j) and the edge length of Q̄(j)
is between 2prj∗ and 4prj∗ . If any point in Q̄(j) is added during the execution of Algorithm 1, its distance to u∗j is at most
3
√
4rj∗ p p = rj∗ · O(log 2 n). Therefore, if Q̄j is ever active during the discretization, this cluster center must have been
captured by C.
According to Theorem 3, conditioned on the event that no failure occurs (which has negligible probability), there can only
be two cases for which Q̄(j) does not appear in C:

Differentially Private Clustering in High-Dimensional Euclidean Spaces

• The edge length of Q̄(j) is less than n1 ;



• Q̄(j) ∩ Sj∗  ≤ O γ( T ) log nδ .
If it is the first case, we can turn to a cube Q̄0 (j) ∈ Gv that contains Q̄(j) with edge length exactly Θ( n1 ). Since our
definition of capturing allows O( n1 ) additive error on the radius, u∗j will also be captured if Q̄0 (j) becomes active. Since
Q̄0 (j) has larger size and potentially more points in Sj∗ , we only need to show that a cube containing u∗j with edge length
at least 2prj∗ is likely to a large number of points in Sj∗ and thus becomes active in Algorithm 1.
Let’s now turn to the randomness of v, in terms of captures for u∗j , shifting Qv and Gv uniformly at random is equivalent
to shifting Q̄(j) uniformly at random, given the fact that it contains u∗j .
(The location of the cell of this scale that contains u∗j may vary in the grid. However, u∗j is indifferent with ”which” cell
contains it, its capture only depends on its relative location within this axis-aligned cell.)
Therefore, if |Sj | ≥ Ω(γ( T ) log nδ ), we will have




n 
Pr u∗j ∈ captured(v) ≥ Pr Sj∗ ∩ Q̄(j) ≥ Ω(γ( ) log )
T
δ

 ∗
 1 ∗
≥ Pr Sj ∩ Q̄(j) ≥ |Sj |)
2

∗
∗
≥ Pr B(uj , 2rj ) ⊆ Q̄j

p
2
1
= 1−
≥
.
p
27
For each j ∈ {1, 2, · · · , k}, we assign 27 log nδ independent trials to it, and it is easy to see that it is captured by C with
probability at least 1 − nδ . By aggregating the 27k log nδ trails together and applying union bound, we conclude that with
probability at least 1 − δ, we have
 
n
∀j ∈ {1, 2, · · · , k}, |Sj∗ | ≥ Ω γ( ) log
⇒ ∃ũj ∈ C captures u∗j .
T
δ
So the proof is completed.

4. Omitted Proofs in Section 5
Theorem 6 (Theorem 6 in the Main Body). Algorithm 3 preserves -differential privacy.
Proof. The privacy guarantee is straightforward using composition theorem over T rounds of the algorithm, and an
additional exponential mechanism that selects the best one. It is easy to verify the sensitivity of loss increments
L(Z − {x} + {y}) − L(Z) is 8Λ2 , the privacy guarantee of exponential mechanism in each round follows.
Theorem 7 (Theorem 7 in the Main Body). With probability 1 − δ, the output of Algorithm 3 satisfies
 2 2

k Λ
n|C|
L(Z) ≤ 30OPT + O
log2
.

δ
Proof. The proof inspires from (Gupta et al., 2010). Basically, we use the following fact, which is derived from the
construction of swap pairs and Lemma 2.2 in (Kanungo et al., 2002):
For clustering centers Z, there exists a set of k swaps {(xi , yi )}ki=1 such that
k
X
i=1

L(Z) − L(Z − {xi } + {yi }) ≥ 3L(Z) − OPT − 2

n
X

kxi − z(oi )k2 ,

i=1

where oi is the optimal cluster to which xi is assigned, and z(o) is point o’s nearest center in Z.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Following the proof for local swap heuristics, we can bound the extra term.
R=

n
n X
X

kxi − z(oi )k2 ≤ 2OPT + L(Z) + 2

i=1 i=1

n
X

kxi − oi k · kxi − zi k

i=1

≤2OPT + L(Z) + 2

p
OP T · L(Z).

Putting them together, we have that ∃x ∈ Z, y ∈ C such that
1
L(Z) − L(Z − {x} + {y}) ≥ (3L(Z) − OPT − 2R)
k

p
1
≥
L(Z) − 5OPT − 4 OPT · L(Z)
k
1
≥ (L(Z) − 30OPT) .
2k
The rest of this proof
 proceedsjust as in (Gupta et al., 2010): exponential mechanism guarantees the bound 4 holds with
with failure probability nδ . T rounds of iteration guarantees the multiplicative term being
an additive term O 1 log n|C|
δ
2

reduced to constant order, except for the case of OPT = O( k log2 n|C|
δ ) where all the excess loss goes to the additive
term. Combining these facts together using union bound over failure probability we conclude the result.
Theorem 8 (Theorem 8 in the Main Body). Under the following assumptions:
• Algorithm candidate ({xi }ni=1 , , δ) preserves -differential privacy for {xi }ni=1 .
• Given any C, Algorithm localswap ({xi }ni=1 , C, , δ) preserves -differential privacy for {xi }ni=1 .
Algorithm 4 preserves -differential privacy.

Proof. In each of T -rounds, the two sub-routines each preserves 6T
-DP. Given the centers u1 , u2 , · · · , uk in projected
space fixed, changing the position for one of data points (and also resulting the change in projected space) will affect at

-DP, and the recovery procedure also
most two clustering centers. The noised version of cluster count sj preserves 12T

preserves 12T -DP given the cluster counts known and fixed. At last, the exponential mechanism preserves 6 -DP, putting
them together using composition theorem, we have the privacy guarantee.

Theorem 9 (Theorem 9 in the Main Body). Instantiated by algorithms that guarantee:
• With probability 32 , algorithm candidate ({xi }ni=1 , , δ) can output an (α, σ1 ())-approximate candidate set.
• With probability 32 , algorithm localswap ({xi }ni=1 , C, , δ) achieves clustering loss with multiplicative approximation factor c and additive term σ2 (), compared with optimal clustering centers in the discrete space.
Algorithm 4 achieves the following bound with probability 1 − δ:
L
where σi0 = σi




2 log 1/δ





k
{zj }j=1



≤ 3cαOPT +

3Cσ10

+

3σ20

+O

dΛ2 log3
2

1
δ

!
,

for i = 1, 2

Proof. With probability 13 , the centers u1 , u2 , · · · , uk in projected space satisfies
n
X
i=1

min kyi − uj k2 ≤ cOPTdiscrete + σ2 ≤ cα(OPTprojected + σ1 ) + σ2 .
j

Differentially Private Clustering in High-Dimensional Euclidean Spaces

The JL transform guarantees that with probability 12 , the pairwise distances between points uniformly satisfies
∀i, j ∈ {1, 2, · · · n},

kyi − yj k2 ≤



1
1±
kxi − xj k2 .
2

This event is also independent with randomized algorithms in projected spaces.
Since the optimal clustering loss for k-means depends only on the pairwise distances between data points, we have the
following fact under the event that JL transform successfully preserves pairwise distances: (Cj denotes optimal clustering
assignment)
OPTprojected ≤

X

kui − νj∗ k2 =

k
X

i∈Cj∗

≤

j=1

1 X X
kui − ul k2
2|Cj∗ |
∗
∗
i∈Cj l∈Cj

k
3X 1 X X
3
kxi − xl k2 = OPT.
2 j=1 2|Cj∗ |
2
∗
∗
i∈Cj l∈Cj

On the other hand, we have
X

kxi − µj k2 =

j=1

i∈Sj

≤2

k
X
j=1

k
X

1 XX
kxi − xl k2
2|Sj |
i∈Sj l∈Sj

X
1 XX
kui − ul k2 = 2
kui − νj k2 ,
2|Sj |
i∈Sj l∈Sj

i∈Sj

where µj (µ∗j , resp.) are clustering center for Sj (Cj∗ , resp.) in Rd , and νj (νj∗ , resp.) are clustering center for Sj (Cj∗ ,
resp.) in Rp .
The noise added to cluster sizes, as well as the Laplacian mechanism, leads to the additional term. Thus we have the
desired bound to hold with constant probability. By repeating it with T independent trials and selecting the best, the failure
probability is reduced to δ. The additive loss induced by exponential mechanism in the last step is dominated by previous
terms.

5. Omitted Proofs in Section 6
Theorem 10 (Theorem 10 in the Main Body). Algorithm 5 preserves -differential privacy.
Proof. The privacy proof is straightforward: for 2s
η rounds of the algorithm, each round is exponential mechanism ac
-DP, the privacy proof then follows via
companied by Laplacian mechanism. Using the fact that both parts preserves 2T
composition.
Theorem 11 (Theorem 11 in the Main Body). The output of Algorithm 5 satisfies the following with probability at least
1 − δ:
!
ds
n
X
Λ2 s2 ln ηδ
1
2
kxi − vk ≤
OPT + O
.
1−η
η2 
i=1
Proof. Let π(j) denote the index of entries in µ with the j-th largest absolute value. Note that removing j entries from
{1, 2, · · · d} makes the largest absolute value at least µj+1 , exponential mechanism guarantees that, for the index rj sampled
in j-th round, we have the following with probability at least ηδ
2s :
|µrj | ≥ |µπ(j) | − O

ds
Λs ln ηδ

ηn

!
,

s
j = 1, 2, · · · , .
η

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Let µ̃ =

Ps/η

j=1

µπ(j) eπ(j) , For ∀j ∈ {1, 2, · · · , d}, let Sj = {i ∈ [n] : xij 6= 0}, cj = |Sj | and vj =
d
X

nkµ̃ − µk2 = n

d
X

µ2π(j) =

j=s/η+1

2
c2π(j) vπ(j)

n

j=s/η+1

1
cj

P

i∈Sj

xij .

.

On the other hand, we have

d
X
X
cj (n − cj ) 2
c
(n
−
c
)
j
 j
|xij − cj |2  ≥
vj2 +
vj .
kxi − µk2 =
n
n
j=1
i=1
j=1

n
X

d
X



i∈Sj

Sort the d entries again according to the value of cj and let τ (j) denote the index of entries in µ with the j-th largest cj , we
have
d
d
2
X
X
c2π(j) vπ(j)
c2τ (j) vτ2(j)
nkµ̃ − µk2 =
≤
.
n
n
j=s/η+1

Since

Pd

j=1 cj

j=s/η+1

≤ ns, Markov inequality implies |{j ∈ [d] : cj ≥ ηn}| ≤ ηs . Thus we have
d
X
j=s/η+1

c2τ (j) vτ2(j)
n

≤

η
1−η

d
X

cτ (j) (n − cτ (j) )vτ2(j)
n

j=s/η+1

d
2
η X cτ (j) (n − cτ (j) )vτ (j)
η
≤
≤
OPT.
1 − η j=1
n
1−η

Putting them together, we have
n
X

2

kxi − vk =

i=1

n
X

kxi − µk2 + nkµ − vk2

i=1
2

≤OPT + nkµ − µ̃k + O
≤

1
OPT + O
1−η

ds
Λ2 s2 ln ηδ

!

η2 
!
ds

Λ2 s2 ln ηδ
η2 

.

Theorem 12 (Theorem 12 in the Main Body). For k-median clustering problem, there is an -differential private algorithms that runs in poly(k, d, n) time, which releases a set of centers z̃1 , z̃2 , · · · , z̃k , that satisfies the following with
probability 1 − δ:
 2

n
(k + d)Λ
log3
.
L(z̃1 , z̃2 , · · · , z̃k ) ≤ O(log3/2 n) · OPT + O

δ
Proof. By plugging in the error bounds and simple calculation, the centers u1 , u2 , · · · , uk in projected space satisfy the
following with probability 1 − δ:
n
X
i=1

min kyi − uj k2 ≤ O(log3/2 n) · OPTprojected + O
j



(k 2 + d)Λ
n
log3

δ


.

The JL transform preserves distances with up to 1 ± 21 multiplicative error with probability 32 . To make use of this fact, we
replace sum-of-square decomposition with triangle inequality, using data points as intermediate step.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

OPTprojected ≤

X

kui − νj∗ k ≤

i∈Cj∗

≤

k
X

min

j=1

l

X

kui − ul k

i∈Cj

k
X
3X
3
min
kxi − xl k = OPT.
2 j=1 l
2
i∈Cj

On the other hand, we have
X

kxi − µj k ≤

j=1

i∈Sj

≤2

k
X
j=1

k
X

min
l

X
l∈Sj

min
l

X

kxi − xl k

l∈Sj

kui − ul k ≤ 4

X

kui − νj k,

i∈Sj

where µj (µ∗j , resp.) are clustering center for Sj (Cj∗ , resp.) in Rd , and νj (νj∗ , resp.) are clustering center for Sj (Cj∗ ,
resp.) in Rp .
The excess losses incurred by log-concave sampling and discrete exponential mechanisms in the algorithm are dominated
by the previous term. Putting them altogether, we have the bound.

6. Additional Experimental Details
In this section we provide additional details about experiments in our paper. We will first give a detailed description on
the real-world and synthetic datasets we are using, and then provide additional details on the comparison to existing works
including (Nock et al., 2016) and (Su et al., 2016). We will also present results about effect of number of clusters k and
dimension d on the clustering loss.
6.1. Description of Datasets
We compare our algorithm against the non-private k-means++ algorithm, SuLQ, k-variates++ (Nock et al., 2016), lowdimensional algorithm (Su et al., 2016) and Sample and Aggregate on the following datasets.
MNIST: We used the raw pixels of the MNIST (LeCun et al., 1998) dataset. It has 70000 examples and 784 features.
CIFAR-10: We used the CIFAR10 dataset (Krizhevsky, 2009). Rather than the raw pixels, we obtained our feature representations from layer in3c (160 features) of a Google Inception (Szegedy et al., 2015) network trained for the classification
task. We also created a second version of this dataset using the feature representations from layer in4d (144 features).
Our dataset contains 100000 randomly sampled examples from this dataset.
Synthetic: For experiments in the main body, we used a synthetic datasets of 100000 samples drawn from an equal-weight
mixture of 64 Gaussians in R100 . Each Gaussian’s covariance matrix is the identity and the mean is randomly sampled from
[0, 100]d . For further experiments in Appendix, we change the value of k and d to illustrate the effect of these parameters.
In these experiments we always set k equal to the number of Gaussians in the mixture, with k ∈ {8, 16, 32, 64}, and we
also select d ∈ {5, 50, 500, 5000} to illustrate effect of dimension.
6.2. Additional Comparisons to Existing Works
We can also compare our approach with recent existing works such as (Nock et al., 2016) and (Nock et al., 2016) through
experiments. Based on experimental comparisons, we find that our algorithm is the only one that works reasonably well
simultaneously for large d and large k, while keeping good performance with small k and d.
It has been noticed that, the griding algorithm in (Su et al., 2016) only works for spaces with constant dimensions (see
e.g. (Park et al., 2016)). So we are unable to evaluate Su et al.’s griding algorithm on the above datasets. Actually their
algorithm has time and space complexity that is exponential in the data dimension (because it constructs a regular grid in
d dimensions). In experiments, their grids caused memory allocation failure for d > 6, with 16GB RAM memory.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

On the other hand, k-variates++ algorithm (Nock et al., 2016) is not designed for clustering problem with k more than
constant. In their experimental part for privacy, (Nock et al., 2016) only did experiments with k ≤ 5. Actually, its noise
scale ˜ relies upon empirical estimates of data-dependent parameter δw and δs . We estimated these parameters on MNIST
dataset with varying k and plug them into the formula. As k becomes larger than 5, the formula for setting parameter ˜ in
Theorem 12 of (Nock et al., 2016) becomes negative, which makes the algorithm invalid.
Therefore, we carried on three sets of experiments to compare our methods with recent baselines.
• To compare with (Nock et al., 2016) in high dimensions, we did experiments on MNIST dataset with k ∈ {2, 3, 4, 5}.
• To compare with (Su et al., 2016) with many clusters, we did experiments on Gaussian mixture dataset with k = 32
and d = 5, which will be discussed in the results about varying d.
• To compare all approaches together, we also carried out a small scale experiment on a mixture of Gaussian dataset
with 100000 samples in d = 3 dimensions with k = 4.
In Figure 2, we do the comparison in the first setting, where we set the privacy parameter  = 0.5. The plot is in
logarithmic scale, and we can easily seen that 2 ends up adding too much noise for slightly larger k, while our algorithm
performs reasonably well.

k-means objective

1018
k-variates++
ours

1016

10

14

1012
1010

2

3

4

5

k
Figure 2. Comparison to (Nock et al., 2016) on MNIST dataset with k = 2, 3, 4, 5

We show the results of the third setting in Table 1, and compare all existing approaches together. Among all private
methods, the griding algorithm in (Su et al., 2016) achieves the lowest objective value in this setting, while our algorithm
and SuLQ both achieved reasonably good performance. As our focus is on modern big data setting with high-dimensional
datasets, in this cases the griding algorithm cannot be applied.
Table 1. Objective values for all baseline algorithms on small-scale synthetic dataset.

Algorithm
k-means++
(Su et al., 2016)
SuLQ k-means
Ours
(Nock et al., 2016)
S&A

Objective Value
2.636e5
2.638e5
2.927e5
2.985e5
1.390e6
2.831e6

It is also worth noticing that running (Su et al., 2016) after randomized dimensionality reduction is not a polynomial-time
algorithm. Actually the regular grids need nΩ(d) cells to preserve good performance, and for the case of d ∼ log n, the
running time and storage is still prohibitive. Even for constant-dimensional case like d = 5, the griding algorithm (Su
et al., 2016) runs more than 5 hours while our algorithm runs within 15 minutes on the same machine. As the time and
space costs for (Su et al., 2016) grows rapidly with d ∼ log n > 5, the costs will become prohibitive.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

6.3. Additional CIFAR-10 Dataset
In this section we present the empirical comparison of algorithms on our second CIFAR-10 dataset, which is identical to
the first except features are instead taken from a later layer of the inception network (layer in4d). Figure 3 shows the
k-means objective of each algorithm when run for values of k from 2 to 64. Results are averaged over 5 runs.

k-means objective

8 ×10

4

7
6
5
non-private
ours
SuLQ

4
3

21

22

23

24

25

26

k
Figure 3. Objective values for various algorithms on the CIFAR-10 dataset with features extracted from layer in4d of a Google inception
network.

6.4. Effect of dimension
In this section we directly evaluate the effect of the dimension on the objective value of the various algorithms. For
this evaluation, we generate samples of size n = 100000 sampled from a mixture of 32 Gaussians with dimensions
d ∈ {5, 50, 500, 5000}. All algorithms are run with the privacy parameter  = 0.5 and k = 32. Figure 4 shows the results
of this experiment. This provides further justification for our claim that our algorithm scales to larger dimensions better
than the existing algorithms. Again, the sample and aggregate algorithm is omitted from the plot because its objective
value is several orders of magnitude worse and makes the plot difficult to read, even in log scale.

k-means objective

1014
non-private
ours
SuLQ

1012
1010

10

8

106

5

50

500

5000

dimension
Figure 4. Objective values for various algorithms on synthetic datasets of increasing dimension.

To make a comprehensive comparison, we also run the griding algorithm in (Su et al., 2016) for the case of d = 5. (As we
have discussed before, their algorithm is prohibitive for larger d). Their algorithm got clustering loss 1.84 × 107 , which
is much smaller compared to our clustering loss 6.43 × 107 . However, the running time of their algorithm is much longer
than us in this setting. And there’s no obvious approach for their algorithm to scale up with k, as we have discussed.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

6.5. Effect of number of intrinsic clusters
Finally, we directly evaluate the effect of the number of intrinsic clusters in the dataset. To do this, we generate datasets
of n = 100000 sampled from mixtures of G ∈ {8, 16, 32, 64} Gaussians in 100 dimensional space. All algorithms are run
with k set to the true number of intrinsic clusters for each dataset. Figure 5 shows the results of this comparison.

k-means objective

9
12 ×10

non-private
ours
SuLQ

10
8
6
4
2
0

10

20

30

40

50

60

Number of Clusters
Figure 5. Objective values for various algorithms on synthetic datasets of growing intrinsic numbers of clusters.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

References
Arthur, David and Vassilvitskii, Sergei. k-means++: The advantages of careful seeding. In ACM-SIAM Symposium on
Discrete Algorithms, pp. 1027–1035, 2007.
Arya, Vijay, Garg, Naveen, Khandekar, Rohit, Meyerson, Adam, Munagala, Kamesh, and Pandit, Vinayaka. Local search
heuristics for k-median and facility location problems. SIAM Journal on computing, 33(3):544–562, 2004.
Awasthi, Pranjal and Balcan, Maria-Florina. Center based clustering: A foundational perspective. 2014.
Balcan, Maria-Florina, Blum, Avrim, and Gupta, Anupam. Approximate clustering without the approximation. In ACMSIAM Symposium on Discrete Algorithms, pp. 1068–1077, 2009.
Dasgupta, Sanjoy. The hardness of k-means clustering. Department of Computer Science and Engineering, University of
California, San Diego, 2008.
Gupta, Anupam, Ligett, Katrina, McSherry, Frank, Roth, Aaron, and Talwar, Kunal. Differentially private combinatorial
optimization. In ACM-SIAM symposium on Discrete Algorithms, pp. 1106–1125, 2010.
Kanungo, Tapas, Mount, David M, Netanyahu, Nathan S, Piatko, Christine D, Silverman, Ruth, and Wu, Angela Y. A local
search approximation algorithm for k-means clustering. In Annual Symposium on Computational Geometry, pp. 10–18,
2002.
Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
Kumar, Amit, Sabharwal, Yogish, and Sen, Sandeep. Linear-time approximation schemes for clustering problems in any
dimensions. Journal of the ACM, 57(2):5, 2010.
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. In Proceedings
of the IEEE, 1998.
Makarychev, Konstantin, Makarychev, Yury, Sviridenko, Maxim, and Ward, Justin. A bi-criteria approximation algorithm
for k means. arXiv preprint arXiv:1507.04227, 2015.
Matoušek, Jirı. On approximate geometric k-clustering. Discrete & Computational Geometry, 24(1):61–84, 2000.
Nock, Richard, Canyasse, Raphaël, Boreli, Roksana, and Nielsen, Frank. k-variates++: more pluses in the k-means++.
arXiv preprint arXiv:1602.01198, 2016.
Park, Mijung, Foulds, Jimmy, Chaudhuri, Kamalika, and Welling, Max. Practical privacy for expectation maximization.
arXiv preprint arXiv:1605.06995, 2016.
Su, Dong, Cao, Jianneng, Li, Ninghui, Bertino, Elisa, and Jin, Hongxia. Differentially private k-means clustering. In
Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy, pp. 26–37. ACM, 2016.
Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. IEEE Conference on Computer Vision and
Pattern Recognition, 2015.

