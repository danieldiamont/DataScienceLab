Nearly Optimal Robust Matrix Completion

5. Appendix
We divide this section into five parts. In the first part we prove some common lemmas. In the second part we give the
convergence guarantee for PG-RMC . In the third part we give another algorithm which has a sample complexity of
¬µ2 rœÉ ‚àó
O(¬µ4 r3 n log2 n log  1 ) and prove its convergence guarantees. In the fourth part we prove a generalized form of lemma
1. In the fifth part we present some additional experiments.
For the sake of convenience in the following proofs, we will define some notations here.
We define p =

|‚Ñ¶k,t |
mn

and we consider the following equivalent update step for L(t+1) in the analysis:
L(t+1) := Pk (M (t) )
H := E (t) + Œ≤G
Se(t) := HT Œ∂ M ‚àí L(t)



D := L(t) ‚àí L‚àó + Se(t) ‚àí Se‚àó

M (t) := L‚àó + H
:= Se‚àó ‚àí Se(t)
E (t) 
P‚Ñ¶
G := 1 I ‚àí pq,t D
Œ≤
‚àö
2 n‚àökDk‚àû
Œ≤ :=
p

The singular values of L‚àó are denoted by œÉ1‚àó , . . . , œÉr‚àó where |œÉ1‚àó | ‚â• . . . ‚â• |œÉr‚àó | and we will let Œª1 , . . . , Œªn denote the
singular values of M (t) where |Œª1 | ‚â• . . . ‚â• |Œªn |.
5.1. Common Lemmas
We will begin by restating some lemmas from previous work that we will use in our proofs.
First, we restate Weyl‚Äôs perturbation lemma from (Bhatia, 1997), a key tool in our analysis:
Lemma 2. Suppose B = A + E ‚àà Rm√ón matrix. Let Œª1 , ¬∑ ¬∑ ¬∑ , Œªk and œÉ1 , ¬∑ ¬∑ ¬∑ , œÉk be the singular values of B and A
respectively such that Œª1 ‚â• ¬∑ ¬∑ ¬∑ ‚â• Œªk and œÉ1 ‚â• ¬∑ ¬∑ ¬∑ ‚â• œÉk . Then:
|Œªi ‚àí œÉi | ‚â§ kEk2 ‚àÄ i ‚àà [k].
This lemma establishes a bound on the spectral norm of a sparse matrix.
Lemma 3. Let S ‚àà Rm√ón be a sparse matrix with row and column sparsity œÅ. Then,
kSk2 ‚â§ œÅ max{m, n} kSk‚àû
Proof. For any pair of unit vectors u and v, we have:
>

v Su =

X

vi uj Sij ‚â§

1‚â§i‚â§m,1‚â§j‚â§n

X

|Sij |

1‚â§i‚â§m,1‚â§j‚â§n

vi2 + u2j
2
Ô£∂

!

Ô£´
X
X
1Ô£≠ X 2 X
‚â§
vi
|Sij | +
u2j
|Sij |Ô£∏ ‚â§ œÅ max{m, n} kSk‚àû
2
1‚â§i‚â§m

1‚â§j‚â§n

1‚â§j‚â§n

1‚â§i‚â§m

Lemma now follows by using kSk2 = maxu,v,kuk2 =1,kvk2 =1 uT Sv.
Now, we define a 0-mean random matrix with small higher moments values.
Definition 1 (Definition 7, (Jain & Netrapalli, 2015)). H is a random matrix of size m √ó n with each of its entries drawn
independently satisfying the following moment conditions:
E[hij ] = 0,

|hij | < 1,

k

E[|hij | ] ‚â§

1
max{m,n} ,

for i, j ‚àà [n] and 2 ‚â§ k ‚â§ 2 log n.
We now restate two useful lemmas from (Jain & Netrapalli, 2015):
Lemma 4 (Lemma 8 and 10 of (Jain & Netrapalli, 2015)). We have the following two claims:
‚àö
‚Ä¢ Suppose H satisfies Definition 1. Then, w.p. ‚â• 1 ‚àí 1/n10+log Œ± , we have: kHk2 ‚â§ 3 Œ±.

Nearly Optimal Robust Matrix Completion

‚Ä¢ Let A be a m √ó n matrix with n ‚â• m. Suppose ‚Ñ¶ ‚äÜ [m] √ó [n] is obtained by sampling each element with probability
1
p ‚â• 4n
. Then, the following matrix H satisfies Defintion 1:


‚àö
p
1
H := ‚àö
A ‚àí P‚Ñ¶ (A) .
p
2 n kAk‚àû
Lemma 5 (Lemma 13, (Jain & Netrapalli, 2015)). Let A ‚àà Rn√ón be a symmetric matrix with eigenvalues œÉ1 , ¬∑ ¬∑ ¬∑ , œÉn
where |œÉ1 | ‚â• ¬∑ ¬∑ ¬∑ ‚â• |œÉn |. Let B = A + C be a perturbation of A satisfying kCk2 ‚â§ œÉ2k and let Pk (B) = U ŒõU > be the
rank-k projection of B. Then, Œõ‚àí1 exists and we have:


1. A ‚àí AU Œõ‚àí1 U > A2 ‚â§ |œÉk+1 | + 5 kCk2 ,

‚àía+2


2. AU Œõ‚àía U > A2 ‚â§ 4 |œÉ2k |
‚àÄa ‚â• 2.
We now provide a lemma that bounds k ¬∑ k‚àû norm of an incoherent matrix with its operator norm.
Lemma 6. Let A ‚àà Rm√ón be a rank r, ¬µ-incoherent matrix. Then for any C ‚àà Rn√óm , we have:
¬µ2 r
kACAk‚àû ‚â§ ‚àö
kACAk2
mn
Proof. Let A = U Œ£V > . Then, ACA = U U > ACAV V > . The lemma now follows by using definition of incoherence
with the fact that kU > ACAV k2 ‚â§ kACAk2 .
We now present a lemma that shows improvement in the error kL ‚àí L‚àó k‚àû by using gradient descent on L(t) .
Lemma 7. Let L‚àó , ‚Ñ¶, Se‚àó satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of
any stage q:


z 
2
‚àó
1. L‚àó ‚àí L(t) ‚àû ‚â§ 2¬µm r œÉk+1
+ 12 œÉk‚àó


z 
2


‚àó
+ 21 œÉk‚àó
2. Se‚àó ‚àí Se(t)  ‚â§ 8¬µm r œÉk+1
‚àû

3. Supp(Se(t) ) ‚äÜ Supp(Se‚àó )
‚àó
are the k and (k + 1)th singular values of L‚àó . Also, let E1 = Se(t) ‚àí Se‚àó and E3 =
where z ‚â• ‚àí3 and œÉk‚àó and œÉk+1



P‚Ñ¶q,t
I‚àí p
L(t) ‚àí L‚àó + Se(t) ‚àí Se‚àó be the error terms defined also in (6). Then, the following holds w.p ‚â• 1 ‚àí

n‚àí(10+log Œ±) :
1
kE1 + E3 k2 ‚â§
100


 z 
1
‚àó
œÉk+1 +
œÉk‚àó
2

(8)

Proof. Note from Lemma 4,
1
1
E3 =
Œ≤
Œ≤
satisfies Definition 1 with Œ≤ =

‚àö
2 n
‚àö
p




P‚Ñ¶q,t  (t)
I‚àí
L ‚àí L‚àó + Se(t) ‚àí Se‚àó ,
p

¬∑ kL(t) ‚àí L‚àó + Se(t) ‚àí Se‚àó k‚àû .

We now bound the spectral norm of E1 + E3 as follows:




 1  (Œ∂1 )
‚àö
 e(t) e‚àó 

kE1 + E3 k2 ‚â§ kE1 k2 + Œ≤ ¬∑ 
E
‚â§
œÅn
S
‚àí
S

 + 3Œ≤ Œ±,
 Œ≤ 3
‚àû
2

 z


  1 z  
2 r ‚àö
(Œ∂2 ) 1
1
60¬µ r n
 ‚àó 
 ‚àó 
‚àó
‚àó
‚â§
œÉkq +1 +
œÉkq +
Œ± œÉkq +1  +
œÉkq  ,
200
2
m
p
2




z
(Œ∂3 ) 1
1
‚â§
œÉk‚àóq +1 +
œÉk‚àóq .
100
2


 


where (Œ∂1 ) follows from Lemma 3 and 4, (Œ∂2 ) follows by our assumptions on œÅ, L(t) ‚àí L‚àó ‚àû , Se(t) ‚àí Se‚àó 

‚àû

assumption that n = O (m) and (Œ∂3 ) follows from our assumption on p.

and our

Nearly Optimal Robust Matrix Completion

In the following lemma, we prove that the value of the threshold computed using œÉk (M (t) ) = œÉk (L‚àó + E1 + E3 ), where
E1 , E3 are defined in (6), closely tracks the threshold that we would have gotten had we had access to the true eigenvalues
of L‚àó , œÉk‚àó .
Lemma 8. Let L‚àó , ‚Ñ¶, Se‚àó satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of
any stage q:


z 
2
‚àó
1. L‚àó ‚àí L(t) ‚àû ‚â§ 2¬µm r œÉk+1
+ 12 œÉk‚àó


z 
2


‚àó
2. Se‚àó ‚àí Se(t)  ‚â§ 8¬µm r œÉk+1
+ 21 œÉk‚àó
‚àû

3. Supp(Se(t) ) ‚äÜ Supp(Se‚àó )
‚àó
where z ‚â• ‚àí3 and œÉk‚àó and œÉk+1
are the k and (k + 1)th singular values of L‚àó . Also, let E1 = Se(t) ‚àí Se‚àó and E3 =



P‚Ñ¶q,t
L(t) ‚àí L‚àó + Se(t) ‚àí Se‚àó be the error terms defined also in (6). Then, the following holds ‚àÄz > ‚àí3 w.p
I‚àí p

‚â• 1 ‚àí n‚àí(10+log Œ±) :
7
8

‚àó
œÉk+1

 z+1 !
1
+
œÉk‚àó ‚â§
2

 z+1 !
1
9
Œªk+1 +
Œªk ‚â§
2
8

‚àó
œÉk+1

 z+1 !
1
+
œÉk‚àó ,
2

(9)

where Œªk := œÉk (M (t) ) = œÉk (L‚àó + E1 + E3 ) and E1 , E3 are defined in (6).


‚àó
 ‚â§ kE1 + E3 k2 We
Proof. Using Weyl‚Äôs inequality (Lemma 2), we have: : |Œªk ‚àí œÉk‚àó | ‚â§ kE1 + E3 k2 and Œªk+1 ‚àí œÉk+1
now proceed to prove the lemma as follows:

 z+1
 z+1 
 z+1



1
1
1

‚àó
‚àó
‚àó


Œªk ‚àí œÉk+1 ‚àí
œÉk  ‚â§ Œªk+1 ‚àí œÉk+1 +
|Œªk ‚àí œÉk‚àó | ,
Œªk+1 +


2
2
2
 z+1 ! (Œ∂)

 z 
 z+1 !
1
1
1
1
‚àó
‚àó
‚â§ kE1 + E3 k2 1 +
‚â§
œÉk+1 +
œÉk
,
1+
2
100
2
2
 z+1 !
1
1
‚àó
œÉk+1 +
œÉk‚àó ,
‚â§
8
2
where (Œ∂) follows from Lemma 7 and the last inequality follows from the assumption that z ‚â• ‚àí3.
Next, we show that the projected gradient descent update (6) leads to a better estimate of L‚àó , i.e., we bound kL(t+1) ‚àíL‚àó k‚àû .
Under the assumptions of the below given Lemma, the proof follows arguments similar to (Netrapalli et al., 2014) with
additional challenge that arises due to more involved error terms E1 , E3 .
Our proof proceeds by first symmetrizing our matrices by rectangular dilation. We first begin by noting some properties of
symmetrized matrices used in the proof of the following lemma.
>
Remark 1. Let Abe a m √ó
 n dimensional matrix with singular value decomposition U Œ£V . We denote its symmetrized
>
0 A
. Then:
version by As :=
A 0
1. The eigenvalue decomposition of As is given by As = Us Œ£s Us> where



1 V
V
Œ£
Us := ‚àö
Œ£s :=
U
‚àíU
0
2

0
Pk (A> )
Pk (A)
0
 > j

(A A)
0
3. We have A2j
=
s
0
(AA> )j

0
‚àíŒ£





2. P2k (As ) =

As2j+1 =

0
(A> A)j A>
> j
(AA ) A
0





Nearly Optimal Robust Matrix Completion

4. We have



V Œ£‚àíj V >
0
=
when j is even
0
U Œ£‚àíj U >


0
V Œ£‚àíj U >
>
Us Œ£‚àíj
U
=
when j is odd
s
s
U Œ£‚àíj V >
0

>
Us Œ£‚àíj
s Us

Lemma 9. Let L = Pk (L‚àó + H), where H is any perturbation matrix that satisfies the following:
œÉ‚àó
1. kHk2 ‚â§ 4k
œÉ‚àó
2. ‚àÄi ‚àà [n], a ‚â§ d log2 n e with œÖ ‚â§ 4k
q

 
 >


e H > H a V ‚àó  , e> HH > a U ‚àó  ‚â§ (œÖ)2a ¬µ r
i
i
mq
2
2

 > >
 


e H HH > a U ‚àó  , e> H H > H a V ‚àó  ‚â§ (œÖ)2a+1 ¬µ r
i
i
m
2
2
where œÉk‚àó is the k th singular value of L‚àó . Also, let L‚àó satisfy Assumption 1. Then, the following holds:
kL ‚àí L‚àó k‚àû ‚â§


¬µ2 r ‚àó
œÉk+1 + 20 kHk2 + 8œÖ
m

where ¬µ and r are the rank and incoherence of the matrix L‚àó respectively.
Proof. Let Ls , Hs and L‚àós denote the symmetrized forms of L, H and L‚àó respectively. Now, we have:
Ls = P2k (L‚àós + Hs )
Let l = m + n. Let Œª1 , ¬∑ ¬∑ ¬∑ , Œªl be the eigenvalues of Ms = L‚àós + Hs with |Œª1 | ‚â• |Œª2 | ¬∑ ¬∑ ¬∑ ‚â• |Œªl |. Let u1 , u2 , ¬∑ ¬∑ ¬∑ , ul be the
3œÉ ‚àó
corresponding eigenvectors of Ms . Using Lemma 2 along with the assumption on kHs k2 , we have: |Œª2k | ‚â• 4k .
Let U ŒõV be the eigen vector decomposition of L. Let Us Œõs Us> to be the eigen vector decomposition of Ls . Then, using
Remark 1 we have ‚àÄ i ‚àà [2k]:


Hs
L‚àó ui
‚àó
(Ls + Hs ) ui = Œªi ui , i.e. I ‚àí
ui = s .
Œªi
Œªi
As |Œª2k | ‚â•

‚àó
3œÉk
4

and kHs k2 ‚â§ 41 œÉk‚àó , we can apply the Taylor‚Äôs series expansion to get the following expression for ui :
Ô£´
Ô£∂
j
‚àû 
1 Ô£≠X Hs Ô£∏ L‚àós ui
ui =
.
Œªi j=0 Œªi
Œªi

That is,
Ls =

2k
X

Œªi ui u>
i =

i=1

=

X

2k
X

Œª‚àí1
i

i=1
2k
X

0‚â§s,t<‚àû i=1

‚àí(s+t+1)

Œªi

X
0‚â§s,t<‚àû



Hs
Œªi

s

‚àó
L‚àós ui u>
i Ls

‚àó t
Hss L‚àós ui u>
i Ls Hs =

X



Hs
Œªi

t
,

Hss L‚àós Us Œõs‚àí(s+t+1) Us> L‚àós Hst .

0‚â§s,t<‚àû

Subtracting L‚àós on both sides and taking operator norm, we get:




X




‚àó
>
‚àó
s ‚àó
‚àí(s+t+1) > ‚àó t
‚àó


Hs Ls Us Œõs
Us Ls Hs ‚àí Ls  ,
kLs ‚àí Ls k‚àû = Us Œõs Us ‚àí Ls ‚àû = 
0‚â§s,t<‚àû

‚àû


X
 ‚àó

 s ‚àó
‚àí1 > ‚àó
‚àó
‚àí(s+t+1) > ‚àó t 

= Ls Us Œõs Us Ls ‚àí Ls ‚àû +
Us Ls Hs  .
Hs Ls Us Œõs
1‚â§s+t<‚àû

‚àû

(10)

Nearly Optimal Robust Matrix Completion

We separately bound the first and the second term of RHS. The first term can be bounded as follows:

 


 (Œ∂1 )  ‚àó
 ‚àó
0
V Œõ‚àí1 U > ‚àó
‚àó
> ‚àó
‚àó

Ls Us Œõ‚àí1
Ls ‚àí Ls 
s Us Ls ‚àí Ls ‚àû = Ls
U Œõ‚àí1 V >
0
‚àû

(11)

 (Œ∂2 ) ¬µ2 r  ‚àó

 (Œ∂3 ) 2
 ‚àó 

 + 5 kHk ,
L V Œõ‚àí1 U > L‚àó ‚àí L‚àó  ‚â§ ‚àö¬µ r œÉk+1
‚â§ L‚àó V Œõ‚àí1 U > L‚àó ‚àí L‚àó ‚àû ‚â§ ‚àö
2
2
mn
mn

(12)

where (Œ∂1 ) follows Remark 1, (Œ∂2 ) from Lemma 6 and (Œ∂3 ) follows from Claim 1 of Lemma 5 after symmetrization.
We now bound second term of RHS of (10) which we again split in two parts. We first bound the terms with s + t > log n:


 s ‚àó
> ‚àó t
U
L
H
Hs Ls Us Œõ(s+t+1)
s
s
s s

‚àû





‚â§ Hss L‚àós Us Œõs‚àí(s+t+1) Us> L‚àós Hst 

2

(Œ∂1 )

s+t

‚â§ kHs k2


4

2
œÉk‚àó

‚àí(s+t‚àí1)


(s+t‚àí1) (Œ∂ )
 (s+t‚àí1)
2
2
1
‚â§ 4 kHk2 kHk2 ‚àó
‚â§ 4 kHk2
œÉk
2
 (s+t‚àí1‚àílog n)
 (s+t‚àí1‚àílog n)
2
¬µ r
1
1
4
‚â§4
,
kHk2
‚â§ kHk2
n
2
m
2

(13)

where (Œ∂1 ) follows from the second claim of Lemma 5 and noting that kHs k2 = kHk2 and (Œ∂2 ) follows from assumption
on kHk2 and using the fact that s + t ‚â• log n.
Summing up over all terms with s + t > log n, we get from (13) and (12):
kLs ‚àí L‚àós k‚àû ‚â§


¬µ2 r  ‚àó 
œÉk+1 + 20 kHk2 +
m



 s ‚àó

Hs Ls Us Œõs‚àí(s+t+1) Us> L‚àós Hst 

X

‚àû

0<s+t‚â§log n

(14)

where the first inequality follows because m ‚â§ n.
Now, for terms corresponding to 1 ‚â§ s + t ‚â§ log n, we have:




 s ‚àó

 > s ‚àó

max
Hs Ls Us Œõs‚àí(s+t+1) Us> L‚àós Hst  =
eq1 Hs Ls Us Œõs‚àí(s+t+1) Us> L‚àós Hst eq2 
q1 ‚àà[m+n],q2 ‚àà[m+n]
‚àû



 

 
 > t ‚àó
 ‚àó ‚àó >
s ‚àó
‚àí(s+t+1) > ‚àó ‚àó 


‚â§
max e>
H
U
(U
)
U
Œõ
U
U
Œ£
max
e
H
U
Œ£

s
q1 s s 2
s
s
s
s
s s
q2
s 2 ,
q1 ‚àà[m+n]

We will now bound the terms,

2

max

q1 ‚àà[m+n]

q2 ‚àà[m+n]

 > s ‚àó
eq Hs Us  . Note from Remark 1.1 that Us‚àó =
1
2

‚àö1
2

 ‚àó
V
U‚àó


V‚àó
. Now, we have
‚àíU ‚àó

the following cases for Hss :
"
Hsj

=

H >H
0

 2s

0
s
HH > 2

#

"

when s is even

Hsj

0
=
b s c
H H >H 2

H > HH >
0

b 2s c #
when s is odd

In these two cases, we have:
"
Hss Us‚àó =

‚àö1
2

s
H >H 2 V ‚àó
s
HH > 2 U ‚àó

#
s
H >H 2 V ‚àó
s
‚àí HH > 2 U ‚àó

This leads to the following 4 cases for

for s even
for s odd

max

q1 ‚àà[m+n]

"
Hss Us‚àó =

b s c #
‚àíH > HH > 2 U ‚àó
b s c
H H >H 2 V ‚àó

 > s ‚àó
eq Hs Us  :
1
2


 2s ‚àó 
 >

>
max
H
H
V 
e
0
q
0
q ‚àà[n]
2

 s
 > >
> b 2 c ‚àó
H
HH
U
max
e

0
q
0

q ‚àà[n]

‚àö1
2

b s c
H > HH > 2 U ‚àó
b s c
H H >H 2 V ‚àó

2


 s ‚àó
 >

> 2
max
HH
U 
e
0
q
0
q ‚àà[m]
2

b 2s c ‚àó 
 >

>
max
H
H
H
V
e

0
q
0

q ‚àà[m]

(15)

2

Nearly Optimal Robust Matrix Completion

We can now bound the terms in (15) as follows:


 s ‚àó

Us> L‚àós Hst 
Hs Ls Us Œõ‚àí(s+t+1)
s

‚àû

(Œ∂1 )

‚â§


¬µ2 r s+t 

 ‚àó
œÖ
Ls Us Œõs‚àí(s+t+1) Us> L‚àós 
m
2
 s+t‚àí1
 s+t‚àí1
(Œ∂2 ) 4¬µ2 r
4¬µ2 r
2
1
‚â§
‚â§
œÖ s+t
œÖ
m
œÉk‚àó
m
2

(16)

where (Œ∂1 ) follows from the second assumption of the Lemma and the preceding argument and (Œ∂2 ) follows from Claim 2
of Lemma 5 and the final step follows from our bound on œÖ.


Finally, note from the Remark 1 that kL‚àós ‚àí Ls k‚àû = L‚àó ‚àí L(t+1) ‚àû . Now, summing up (16) over all 1 ‚â§ s + t ‚â§ log n
and combining with (14), the lemma is proved.
In the next lemma, we show that with the threshold chosen in the algorithm, we show an improvement in the estimation of
Se‚àó by Se(t) .
Lemma 10. In the tth iterate of the q th stage, assume the following holds:


z 
2
‚àó
1. L‚àó ‚àí L(t) ‚àû ‚â§ 2¬µm r œÉk+1
+ 12 œÉk‚àó

 z 

 z 
9
1
‚àó
‚àó
‚àó
(t)
‚àó
+ 1
2. 87 Œ∑ œÉk+1
2 œÉk ‚â§ Œ∂ ‚â§ 8 Œ∑ œÉk+1 + 2 œÉk
‚àó
where œÉk‚àó and œÉk+1
are the k and (k + 1)th singular values of L‚àó , Œªk and Œªk+1 are the k and (k + 1)th singular values of
(t)
M and, r and ¬µ are the rank and incoherence of the m √ó n matrix L‚àó respectively. Then we have


 
1. Supp Se(t) ‚äÜ Supp Se‚àó





2. Se(t) ‚àí Se‚àó 

‚àû

‚â§

8¬µ2 r
m

‚àó
œÉk+1
+


1 z
2

œÉk‚àó



Proof. We first prove the first claim of the lemma. Consider an index pair (i, j) ‚àà
/ Supp(Se‚àó ).
 z  (Œ∂ )

 2¬µ2 r 
1
1
16¬µ2 r (t) (Œ∂2 ) (t)

(t) 
‚àó
Œ∂
‚â§ Œ∂
œÉk+1 +
œÉk‚àó ‚â§
Mij ‚àí Lij  ‚â§
m
2
7mŒ∑
where (Œ∂1 ) follows from the second assumption of the lemma and (Œ∂2 ) follows from our setting of Œ∑ =
do not threshold any entry that is not corrupted by Se‚àó .

4¬µ2 r
m .

Hence, we

Now, we prove the second claim of the lemma. Consider an index entry (i, j) ‚àà Supp(Se‚àó ). Here, we consider two cases:
(t)
(t)
‚àó
1. The entry (i, j) ‚àà Supp(Se(t) ): Here the entry (i, j) is thresholded. We know that Lij + Seij = L‚àóij + Seij
from which
we get

 
 

 e(t) e‚àó   ‚àó


(t) 
Sij ‚àí Sij  = Lij ‚àí Lij  ‚â§ L‚àó ‚àí L(t) 
‚àû




(t) 
‚àó
2. The entry (i, j) ‚àà
/ Supp(Se(t) ): Here the entry (i, j) is not thresholded. We know that L‚àóij + Seij
‚àí Lij  ‚â§ Œ∂ (t) from
which we get
 


 e‚àó 

(t) 
Sij  ‚â§ Œ∂ (t) + L‚àóij ‚àí Lij 

 z 

 z 
(Œ∂2 ) 36¬µ2 r
1
2¬µ2 r
1
‚àó
‚àó
œÉk+1
+
œÉk‚àó +
œÉk+1
+
œÉk‚àó
‚â§
8m
2
m
2

 z 
8¬µ2 r
1
‚àó
‚â§
œÉk+1
+
œÉk‚àó
m
2
2

where (Œ∂2 ) follows from the second assumption along with our setting of Œ∑ = 4 ¬µmr .
The above two cases prove the second statement of the lemma.

Nearly Optimal Robust Matrix Completion

We will now prove Lemma 1





P‚Ñ¶
Proof of Lemma 1: Recall the definitions of E1 = Se‚àó ‚àí Se(t) , E2 = L(t) ‚àí L‚àó , E3 = I ‚àí pq,t (E2 ‚àí E1 ) and
q
Œ≤ = 2 np kE2 ‚àí E1 k‚àû . Recall that H := E1 + E3 From Lemma 4, we have that Œ≤1 E3 satisfies Definition 1. This implies
that the matrix

1
Œ≤

(E1 + E3 ) satisfies the conditions of Lemma 14. Now, we have ‚àÄ1 ‚â§ a ‚â§ dlog ne and ‚àÄi ‚àà [n]:




> !a 




1
1
‚àó
ei (HH > )a U ‚àó  = Œ≤ 2a 
e
U
H
H


i
2


Œ≤
Œ≤
2
r
r 


2a
r
2a
(Œ∂)
œÅn
r
r
n
‚â§ Œ≤ 2a
kE1 k‚àû + c log n
¬µ
‚â§¬µ
œÅn kE1 k‚àû + 2c
(kE1 ‚àí E2 k‚àû ) log n
Œ≤
m
m
p
where (Œ∂) follows from the application of Lemma 14 along with the incoherence assumption on U ‚àó . The other statements
of the lemma can be proved in a similar manner by invocations of the different claims of Lemma 14.

5.2. Algorithm PG-RMC
20¬µ2 nrœÉ ‚àó

1
). Consider the stage q reached at the termination of the algorithm.
Proof of Theorem 1: We know that T ‚â• log(

We know from Lemma 11 that:



T ‚àí3 ‚àó  8¬µ2 r ‚àó
2

1. E (T ) ‚àû ‚â§ 8¬µm r œÉk‚àóq +1 + 21
œÉkq ‚â§ m œÉkq +1 + 10n



T ‚àí3  ‚àó  2¬µ2 r ‚àó
2

2. L(T ) ‚àí L‚àó ‚àû ‚â§ 2¬µm r œÉk‚àóq +1 + 12
œÉkq  ‚â§ m œÉkq +1 + 10n

Combining this with Lemmas 2 and 7, we get:




1
m

(T ) 
‚àó
‚àó
œÉkq +1 +
œÉkq +1 (M ) ‚â• œÉkq +1 ‚àí
100
10n¬µ2 r

When the while loop terminates, Œ∑œÉkq +1 M (T ) <


2n ,

which from (17), implies that œÉk‚àóq +1 <





kL ‚àí L‚àó k‚àû = L(T ) ‚àí L‚àó 

‚àû

‚â§

(17)
m
7n¬µ2 r .

So we have:

2¬µ2 r ‚àó


œÉ
+
‚â§
.
m kq +1 10n
2n

We will now bound the number of iterations required for the PG-RMC to converge.
17 ‚àó
œÉkq‚àí1 +1 ‚àÄq ‚â• 1. By recursively applying this inequality, we get
From claim 2 of Lemma 12, we have œÉk‚àóq +1 ‚â§ 32

q ‚àó
q
17
œÉk‚àóq +1 ‚â§ 32 œÉ1‚àó . We know that when the algorithm terminates, œÉk‚àóq +1 < 7¬µ2 r . Since, 17
œÉ1 is an upper bound for
32
 2 ‚àó
7¬µ
rœÉ
1
œÉk‚àóq +1 , an upper bound for the number of iterations is 5 log
. Also, note that an upper bound to this quantity is


used to partition the samples provided to the algorithm. This happens with probability ‚â• 1‚àíT 2 n‚àí(10+log Œ±) ‚â• 1‚àín‚àí log Œ± .
This concludes the proof.

In the following lemma, we show that we make progress simultaneously in the estimation of both Se‚àó and L‚àó by Se(t) and
L(t) . We make use of Lemmas 9 and 10 to show progress in the estimation of one affects the other alternatively. We also
emphasize the roles of the following quantities in enabling us to prove our convergence result:
1. kHk2 - We use Lemma 7 to bound this quantity
2. The analysis of the following 4 quantities is crucial to obtaining error bounds in kk‚àû norm




 >

 >

j
j
e 0 H > H 2 V ‚àó 
e 0 HH > 2 U ‚àó 
for j even
max
max
q
q




0
0
q ‚àà[n]
q ‚àà[m]

2

2
j
j
 > >





e>0 H H > H b 2 c V ‚àó 
e 0 H HH > b 2 c U ‚àó 
for j odd
max
max
q
q




q 0 ‚àà[n]
q 0 ‚àà[m]
2

We use Lemma 1 to bound this quantity.

2

Nearly Optimal Robust Matrix Completion

Lemma 11. Let L‚àó , ‚Ñ¶, Se‚àó and Se(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the q th stage of
Algorithm 1, Se(t) and L(t) satisfy:
 t‚àí3  !


8¬µ2 r  ‚àó 
1
 e(t) e‚àó 
 ‚àó 
S ‚àí S  ‚â§
œÉkq +1  +
œÉkq  ,
m
2
‚àû

 

Supp Se(t) ‚äÜ Supp Se‚àó ,
!
!
  1 t‚àí2  
  1 t‚àí2  


9
7
 ‚àó 
 ‚àó 
 ‚àó 
 ‚àó 
(t+1)
‚â§ Œ∑ œÉkq +1  +
Œ∑ œÉkq +1  +
œÉkq  ‚â§ Œ∂
œÉkq  and
8
2
8
2
 t‚àí3  !


2¬µ2 r  ‚àó 
1
 (t)
 ‚àó 
‚àó
L ‚àí L  ‚â§
œÉkq +1  +
œÉkq  .
m
2
‚àû
with probability ‚â• 1 ‚àí ((q ‚àí 1)T + t ‚àí 1)n‚àí(10+log Œ±) where T is the number of iterations in the inner loop.
Proof. We prove the lemma by induction on both q and t. Recall that E (t) = Se‚àó ‚àí Se(t)
Base Case: q = 1 and t = 0
We begin by first proving an upper bound on kL‚àó k‚àû . We do this as follows:

 r
r
r
 X
2
X


 ‚àó ‚àó 
 ‚àó  X

‚àó 
‚àó
‚àó
‚àó
uik vjk  ‚â§ ‚àö¬µ r œÉ1‚àó
Lij  = 
œÉk‚àó u‚àóik vjk
‚â§ œÉ1‚àó
œÉk uik vjk  ‚â§


mn
k=1

k=1

k=1

where the last inequality follows from Cauchy-Schwartz and the incoherence of U ‚àó . This directly proves the third claim
of the lemma for the base case. Recall, that Œ∂ (0) = Œ∑œÉ1‚àó . We now have from the thresholding step and the incoherence
assumption on L‚àó :
(Œ∂)


2
1. E (0) ‚àû ‚â§ 8¬µm r (œÉ2‚àó + 2œÉ1‚àó ) ‚â§

 

2. Supp Se(t) ‚äÜ Supp Se‚àó .

8¬µ2 r
m


8œÉk‚àó1 , and

where (Œ∂) follows from Lemma 12.
Finally, from Lemma 8, we have:

 9


7
Œ∑ œÉk‚àó1 +1 + 4œÉk‚àó1 ‚â§ Œ∂ (1) = Œ∑ œÉk1 +1 (M (t) ) + 4œÉk1 (M (t) ) ‚â§ Œ∑ œÉk‚àó1 +1 + 4œÉk‚àó1
8
8
So the base case of induction is satisfied.
Induction over t
We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:



t‚àí3 ‚àó 
2
a) E (t) ‚àû ‚â§ 8¬µm r œÉk‚àóq +1 + 12
œÉkq


 
b) Supp Se(t) ‚äÜ Supp Se‚àó .



t‚àí3 ‚àó 
2
c) L‚àó ‚àí L(t) ‚àû ‚â§ 2¬µm r œÉk‚àóq +1 + 21
œÉkq







t‚àí2  ‚àó 
t‚àí2  ‚àó 




d) 78 Œ∑ œÉk‚àóq +1  + 12
œÉkq  ‚â§ Œ∂ (t+1) ‚â§ 89 Œ∑ œÉk‚àóq +1  + 12
œÉkq 
with probability 1 ‚àí ((q ‚àí 1)T + t ‚àí 1)n‚àí(10+log Œ±) . Then by Lemma 9, we have:


 (t+1)

‚àí L‚àó 
L

‚àû

‚â§


¬µ2 r  ‚àó
œÉkq +1 + 20 kHk2 + 8œÖ
m

(18)

Nearly Optimal Robust Matrix Completion

From Lemma 1, we have:

(Œ∂1 )

1
+ 8Œ≤Œ± log n ‚â§
100
‚àû





œÖ ‚â§ œÅn E (t) 

œÉk‚àóq +1

!
 t‚àí3
(Œ∂2 ) 1
1
‚àó
+
œÉkq + 8Œ≤Œ± log n ‚â§
2
50

œÉk‚àóq +1

!
 t‚àí3
1
‚àó
+
œÉkq
(19)
2



where (Œ∂1 ) follows from our assumptions on œÅ and our inductive hypothesis on E (t) ‚àû and (Œ∂2 ) follows from our




assumption on p and by noticing that kDk‚àû ‚â§ E (t) ‚àû + L‚àó ‚àí L(t) ‚àû . Recall that D = L(t) ‚àí L‚àó + Se(t) ‚àí Se‚àó .
From Lemma 7:
1
kHk2 ‚â§
100

œÉk‚àóq +1

!
 t‚àí3
1
‚àó
+
œÉkq
2

(20)

with probability ‚â• 1 ‚àí n‚àí(10+log Œ±) . From Equations (20), (19) and (18), we have:
!
 t‚àí2


2¬µ2 r
1
 ‚àó
(t+1) 
‚àó
‚àó
œÉkq
œÉkq +1 +
 ‚â§
L ‚àí L
m
2
‚àû
which by union bound holds with probability ‚â• 1 ‚àí ((q ‚àí 1)T + t)n‚àí(10+log Œ±) . Hence, using Lemma 10 and our inductive
hypothesis on Œ∂ (t+1) we have:



t‚àí2 ‚àó 
2
œÉkq
1. E (t+1) ‚àû ‚â§ 8¬µm r œÉk‚àóq +1 + 21


 
2. Supp Se(t+1) ‚äÜ Supp Se‚àó .
which also holds with probability ‚â• 1 ‚àí ((q ‚àí 1)T + t)n‚àí(10+log Œ±) . This concludes the proof for induction over t.




Finally, from Lemma 8 and our bounds on E (t+1) ‚àû and L‚àó ‚àí L(t+1) ‚àû , we have:

  1 t‚àí1  
7


 ‚àó 
Œ∑ œÉk‚àóq +1  +
œÉkq 
8
2

!
‚â§ Œ∂

(t+2)


  1 t‚àí1  
9


 ‚àó 
‚â§ Œ∑ œÉk‚àóq +1  +
œÉkq 
8
2

!

Induction Over Stages q
We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:



T ‚àí3 ‚àó  8¬µ2 rœÉk‚àóq +1
2

œÉkq ‚â§
1. E (T ) ‚àû ‚â§ 8¬µm r œÉk‚àóq +1 + 21
+ 10n
, and
m

 

2. Supp Se(T ) ‚äÜ Supp Se‚àó .




T ‚àí2  ‚àó 
T ‚àí2  ‚àó 




3. 87 Œ∑ œÉk‚àóq +1  + 12
œÉkq  ‚â§ Œ∂ (T +1) ‚â§ 98 Œ∑ œÉk‚àóq +1  + 12
œÉkq 
with probability ‚â• 1 ‚àí (qT ‚àí 1)n‚àí(10+log Œ±) . From Lemmas 2 and 7, we get:






m
1


(T )
‚àó
‚àó
œÉkq +1 +
‚àí œÉkq +1  ‚â§ kHk2 ‚â§
œÉkq +1 M
100
10n¬µ2 r

with probability 1 ‚àí n‚àí(10+log Œ±) . We know that Œ∑œÉkq +1 M (t) ‚â•



 (T +1)

‚àí L‚àó 
L

‚àû


2n





which with (21) implies that œÉk‚àóq +1  >

!
 T ‚àí2


1
2¬µ2 r
m
‚àó
‚àó
+
œÉkq ‚â§
œÉkq +1 +
2
m
20n¬µ2 rn


œÉk‚àó +1
2¬µ2 r
2¬µ2 r  ‚àó  (Œ∂4 ) 2¬µ2 r  ‚àó 
‚â§
œÉk‚àóq +1 + q
‚â§
2œÉkq +1 ‚â§
8œÉkq+1
m
2
m
m

2¬µ2 r
‚â§
m

œÉk‚àóq +1

(21)
m
10n¬µ2 r .

Nearly Optimal Robust Matrix Completion

where (Œ∂4 ) follows from Lemma 12. By union bound this holds with probability ‚â• 1 ‚àí qT n‚àí(10+log Œ±) .
Now, from Lemma 10 and the inductive hypothesis on Œ∂ T +1 , we have through a similar series of arguments as above:




2
1. E (0) ‚àû ‚â§ 8¬µm r 8œÉk‚àóq+1


 
2. Supp Se(0) ‚äÜ Supp Se‚àó
which holds with probability ‚â• 1 ‚àí qT n‚àí(10+log Œ±) .




Recall, now that L(0) = L(T +1) . Finally, from Lemma 8 and our bounds on E (0) ‚àû and L(0) ‚àí L‚àó ‚àû , we have:






9 
7  ‚àó






Œ∑ œÉkq+1 +1  + 4 œÉk‚àóq+1  ‚â§ Œ∂ (1) ‚â§ Œ∑ œÉk‚àóq+1 +1  + 4 œÉk‚àóq+1 
8
8

Lemma 12. Suppose at the beginning of the q th stage of algorithm 1:




2¬µ2 r
1. L‚àó ‚àí L(0) ‚àû ‚â§ m 2œÉk‚àóq‚àí1 +1




8¬µ2 r
2. E (0) ‚àû ‚â§ m 2œÉk‚àóq‚àí1 +1
Then, the following hold:
1. œÉk‚àóq ‚â•

15 ‚àó
32 œÉkq‚àí1 +1

2. œÉk‚àóq +1 ‚â§

17 ‚àó
32 œÉkq‚àí1 +1

with probability ‚â• 1 ‚àí n‚àí(10+log Œ±)
Proof. We know that:
Œªkq ‚â§ œÉk‚àóq + kHk2 ,

Œªkq‚àí1 +1 ‚â• œÉk‚àóq‚àí1 +1 ‚àí kHk2 ,

Œªkq ‚â•

Œªkq‚àí1 +1
2

Combining the three inequalities, we get:
œÉk‚àóq ‚â•

œÉk‚àóq‚àí1 +1 ‚àí 3 kHk2
2

Applying Lemma 7, we get the first claim of the lemma.
Similar to the first claim, we have:
Œªkq +1 ‚â• œÉk‚àóq +1 ‚àí kHk2 ,

Œªkq‚àí1 +1 ‚â§ œÉk‚àóq‚àí1 +1 + kHk2 ,

Again, combining the three inequalities, we get:
œÉk‚àóq +1 ‚â§

œÉk‚àóq‚àí1 +1 + 3 kHk2

Another application of Lemma 7 gives the second claim.

2

Œªkq +1 ‚â§

Œªkq‚àí1 +1
2

Nearly Optimal Robust Matrix Completion

b = R-RMC(‚Ñ¶, P‚Ñ¶ (M ), , r, Œ∑, œÉ): Non-convex Robust Matrix Completion
Algorithm 3 L
1: Input: Observed entries ‚Ñ¶, Matrix P‚Ñ¶ (M ) ‚àà Rm√ón , convergence criterion , target rank r, thresholding parameter Œ∑,
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

upper bound on œÉ1‚àó œÉ
2
T ‚Üê 10 log 20¬µ nrœÉ
Partition ‚Ñ¶ into rT + 1 subsets {‚Ñ¶0 } ‚à™ {‚Ñ¶q,t : q ‚àà [r], t ‚àà [T ]} using 2
L(0) = 0, Œ∂ (0) ‚Üê Œ∑œÉ
mn
M (0) ‚Üê |‚Ñ¶
P‚Ñ¶0 (M ‚àí HT Œ∂ (M ))
0|
q ‚Üê 0

do
while œÉq+1 (M (0) ) > 2Œ∑m
q ‚Üê q+1
for Iteration t = 0 to t = T do
S (t) = HŒ∂ (P‚Ñ¶q,t (M ‚àí L(t) ))
P‚Ñ¶q,t (L(t) + S (t) ‚àí M )
M (t) = L(t) ‚àí |‚Ñ¶mn
q,t |

12:

L(t+1) = Pq (M (t) )

13:

(t+1)

14:
15:
16:
17:

/*Number of inner iterations*/

/*Projection onto set of sparse matrices*/
/*Gradient Descent Update*/
/*Projected Gradient Descent step*/




1 t
2

(t)

(t)

Set threshold Œ∂
‚Üê Œ∑ œÉq+1 (M ) +
œÉq (M
end for
S (0) = S (T ) , L(0) = L(T +1) , M (0) = M (T ) , Œ∂ (0) = Œ∂ (T +1)
end while
Return: L(T +1)


)

5.3. Algorithm R-RMC
Proof of Theorem 2: We know that T ‚â• log(

20¬µ2 nrœÉ1‚àó
).


Consider the stage q reached at the termination of the algorithm. We know from Lemma 13 that:



T ‚àí1 ‚àó  8¬µ2 r ‚àó
2

‚àó
1. E (T ) ‚àû ‚â§ 8¬µm r œÉq+1
+ 12
œÉq ‚â§ m œÉq+1 + 10n


2. L(T ) ‚àí L‚àó ‚àû ‚â§

2¬µ2 r
m


‚àó
œÉq+1
+


1 T ‚àí1
2


œÉq‚àó ‚â§

2¬µ2 r ‚àó
m œÉq+1

+


10n

Combining this with Lemmas 2 and 7, we get:

œÉq+1 (M

(T )

)‚â•


When the while loop terminates, Œ∑œÉq+1 M (T ) <

‚àó
œÉq+1


2n ,

1
‚àí
100


‚àó
œÉq+1
+

m
10n¬µ2 r


(22)

‚àó
which from (22), implies that œÉq+1
<





kL ‚àí L‚àó k‚àû = L(T ) ‚àí L‚àó 

‚àû

‚â§

m
7n¬µ2 r .

So we have:



2¬µ2 r ‚àó
|œÉkq +1 | +
‚â§
.
m
10n
2n


As in the case of the proof of Theorem 1, the following lemma shows that we simultaneously make progress in both
the estimation of L‚àó and Se‚àó by L(t) and Se(t) respectively. Similar to Lemma 11, we make use of Lemmas 10 and
9 to show
how improvement
in estimation
of one of the quantities
affects the other



 and the other
 five terms, kHk2 ,

j ‚àó 

j
 >
 >

 > >
 >

>
> j ‚àó
> j ‚àó
max
U  , max
U  and max
eq0 H H V  , max
eq0 HH
eq0 H HH
eq0 H H > H V ‚àó 
0
0
0
0
q ‚àà[n]

2

q ‚àà[m]

2

q ‚àà[n]

2

q ‚àà[m]

2

are analyzed the same way:
Lemma 13. Let L‚àó , ‚Ñ¶, Se‚àó and Se(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the q th stage of

Nearly Optimal Robust Matrix Completion

Algorithm 3, Se(t) and L(t) satisfy:
 t‚àí1 !
1
‚àó
œÉq‚àó ,
œÉq+1
+
2
‚àû


 
Supp Se(t) ‚äÜ Supp Se‚àó ,
!
!
 t
 t

 ‚àó 


 ‚àó

1
1
7
9
‚àó 
œÉq  ‚â§ Œ∂ (t+1) ‚â§ Œ∑ œÉq+1
œÉq‚àó  and
+
Œ∑ œÉq+1  +
8
2
8
2
 t‚àí1 !


2¬µ2 r
1
 (t)
‚àó
‚àó
œÉq+1 +
œÉq‚àó .
L ‚àí L  ‚â§
m
2
‚àû
8¬µ2 r
‚â§
m



 e(t) e‚àó 
S ‚àí S 

with probability ‚â• 1 ‚àí ((q ‚àí 1)T + t ‚àí 1)n‚àí(10+log Œ±) where T is the number of iterations in the inner loop.
Proof. We prove the lemma by induction on both q and t.
Base Case: q = 1 and t = 0
We begin by first proving an upper bound on kL‚àó k‚àû . We do this as follows:


r
r
r
 X
X
 ‚àó ‚àó  ¬µ2 r ‚àó
 ‚àó ‚àó ‚àó 
 ‚àó  X
‚àó ‚àó ‚àó 
uik vjk  ‚â§
œÉk uik vjk  ‚â§ œÉ1‚àó
Lij  = 
œÉk uik vjk  ‚â§
œÉ


m 1
k=1

k=1

k=1

where the last inequality follows from Cauchy-Schwartz and the incoherence of U ‚àó . This directly proves the third claim
of the lemma for the base case. Recall that Œ∂ (0) = Œ∑œÉ1‚àó . We also note that due to the thresholding step and the incoherence
assumption on L‚àó , we have:


2
1. E (0) ‚àû ‚â§ 8¬µm r (œÉ2‚àó + 2œÉ1‚àó )


 
2. Supp Se(t) ‚äÜ Supp Se‚àó .


From Lemma 8 and our bounds on E (0) and L(1) ‚àí L‚àó ‚àû , we have:
9
7
Œ∑ (|œÉ2‚àó | + |œÉ1‚àó |) ‚â§ Œ∂ (1) ‚â§ Œ∑ (|œÉ2‚àó | + |œÉ1‚àó |)
8
8
So the base case of induction is satisfied.
Induction over t
We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:



t‚àí1 ‚àó 
2
‚àó
a) E (t) ‚àû ‚â§ 8¬µm r |œÉq+1
| + 12
|œÉq |


 
b) Supp Se(t) ‚äÜ Supp Se‚àó .


c) L‚àó ‚àí L(t) ‚àû ‚â§
d)

7
8Œ∑



‚àó 
œÉq+1
+

2¬µ2 r
m


1 t
2



‚àó
|+
|œÉq+1


1 t‚àí1
2


|œÉq‚àó |


 ‚àó 

‚àó 
œÉq  ‚â§ Œ∂ (t+1) ‚â§ 9 Œ∑ œÉq+1
+
8


1 t
2

 ‚àó 
œÉq 

with probability 1 ‚àí ((q ‚àí 1)T + t ‚àí 1)n‚àí(10+log Œ±) .
Then by Lemma 9, we have:


 (t+1)

‚àí L‚àó 
L

‚àû

‚â§


¬µ2 r  ‚àó
|œÉkq +1 | + 20 kHk2 + 8œÖ
m

(23)

Nearly Optimal Robust Matrix Completion

From Lemma 1, we have:
(Œ∂1 )

1
+ 8Œ≤Œ± log n ‚â§
100
‚àû





œÖ ‚â§ œÅn E (t) 

‚àó
œÉq+1

 t‚àí1 !
(Œ∂2 ) 1
1
œÉq‚àó + 8Œ≤Œ± log n ‚â§
+
2
50

‚àó
œÉq+1

 t‚àí1 !
1
œÉq‚àó
+
2

(24)



where (Œ∂1 ) follows from our assumptions on œÅ and our inductive hypothesis on E (t) ‚àû and (Œ∂2 ) follows from our




assumption on p and by noticing that kDk‚àû ‚â§ E (t) ‚àû + L‚àó ‚àí L(t) ‚àû . Recall that D = L(t) ‚àí L‚àó + Se(t) ‚àí Se‚àó .
From Lemma 7:
1
kHk2 ‚â§
100

‚àó
œÉq+1

 t‚àí1 !
1
+
œÉq‚àó
2

(25)

with probability ‚â• 1 ‚àí n‚àí(10+log Œ±) . From Equations (25), (24) and (23), we have:
 t !


2¬µ2 r
1
 ‚àó
(t+1) 
‚àó
œÉq+1 +
œÉq‚àó
 ‚â§
L ‚àí L
m
2
‚àû
which by union bound holds with probability ‚â• 1 ‚àí ((q ‚àí 1)T + t)n‚àí(10+log Œ±) . Hence, using Lemma 10 and the inductive
hypothesis on Œ∂ (t+1) we have:



t 
2
‚àó
1. E (t+1) ‚àû ‚â§ 8¬µm r œÉq+1
+ 21 œÉq‚àó


 
2. Supp Se(t+1) ‚äÜ Supp Se‚àó .
which also holds with probability ‚â• 1 ‚àí ((q ‚àí 1)T + t)n‚àí(10+log Œ±) . This concludes the proof for induction over t.




Finally, using Lemma 8 and our bounds on E (t+1) ‚àû and L(t+1) ‚àí L‚àó ‚àû , we have:
 ‚àó 
7
+
Œ∑ œÉq+1
8

!
!
 t+1
 t+1
 ‚àó 
 ‚àó
 ‚àó
1
1
9
(t+2)
œÉq  ‚â§ Œ∂
œÉq 
‚â§ Œ∑ œÉq+1  +
2
8
2

Induction Over Stages q
We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:

‚àó


T ‚àí1 ‚àó  8¬µ2 rœÉq+1
2

‚àó
+ 12
œÉq ‚â§
+ 10n
1. E (T ) ‚àû ‚â§ 8¬µm r œÉq+1
m

 

2. Supp Se(T ) ‚äÜ Supp Se‚àó




T   
T   
‚àó 
‚àó 
3. 87 Œ∑ œÉq+1
+ 12 œÉq‚àó  ‚â§ Œ∂ (T +1) ‚â§ 89 Œ∑ œÉq+1
+ 21 œÉq‚àó 
with probability ‚â• 1 ‚àí (qT ‚àí 1)n‚àí(10+log Œ±) .
From Lemmas 2 and 7 we get:






1
m

‚àó 
‚àó
œÉq+1
+
œÉq+1 M (T ) ‚àí œÉq+1
 ‚â§ kHk2 ‚â§
100
10n¬µ2 r

with probability 1 ‚àí n‚àí(10+log Œ±) . We know that Œ∑œÉq+1 M (t) ‚â•



 (T +1)

‚àí L‚àó 
L

‚àû


2n

‚àó
which with (26) implies that œÉq+1
>

 T !


1
2¬µ2 r
m
‚àó
+
œÉq‚àó ‚â§
œÉq+1
+
2
m
20¬µ2 rn


‚àó

œÉq+1
2¬µ2 r
2¬µ2 r
‚àó
‚àó
‚â§
œÉq+1
+
‚â§
2œÉq+1
m
2
m
2¬µ2 r
‚â§
m

‚àó
œÉq+1

(26)
m
10n¬µ2 r .

Nearly Optimal Robust Matrix Completion

By union bound this holds with probability ‚â• 1 ‚àí qT n‚àí(10+log Œ±) .
Now, from Lemma 10 and our inductive hypothesis on Œ∂ (T +1) , we have through a similar series of arguments as above:



2
‚àó
1. E (0) ‚àû ‚â§ 8¬µm r 2œÉq+1


 
2. Supp Se(0) ‚äÜ Supp Se‚àó
which holds with probability ‚â• 1 ‚àí qT n‚àí(10+log Œ±) .




Recall, now that L(0) = L(T +1) . Finally, from Lemma 8 and our bounds on E (0) ‚àû and L(0) ‚àí L‚àó ‚àû , we have:
9  ‚àó   ‚àó 
7  ‚àó   ‚àó 
+ œÉq+1
Œ∑ œÉq+2 + œÉq+1 ‚â§ Œ∂ (1) ‚â§ Œ∑ œÉq+2
8
8

5.4. Proof of a generalized form of Lemma 1
Lemma 14. Suppose H = H1 + H2 and H ‚àà Rm√ón where H1 satisfies Definition 1 (Definition 7 from (Jain &
Netrapalli, 2015)) and H2 is a matrix with column and row sparsity œÅ. Let U be a matrix with rows denoted as
u1 , . . . , um and let V be a matrix with rows denoted as v1 , . . . , vn . Let eq be the q th vector from standard basis. Let
œÑ = max{max kui k , max kvi k}. Then, for 0 ‚â§ a ‚â§ log n:
i‚àà[m]

i‚àà[n]



a 
 



>
> a 
max e>
V  , max e>
U  ‚â§ (œÅn kH2 k‚àû + c log n)2a œÑ
q H H
q HH
q‚àà[n]
2 q‚àà[m]
2


 
a 
 > >
 >

> a 
>
max eq H HH
U  , max eq H H H V  ‚â§ (œÅn kH2 k‚àû + c log n)2a+1 œÑ

q‚àà[n]

2 q‚àà[m]

2

c

with probability n‚àí2 log 4 +4 .
Proof. Similar to (Jain & Netrapalli, 2015), we will prove the statement for q = 1 and it can be proved for q ‚àà [n] by
taking a union bound over all q. For the sake of brevity, we will prove only the inequality:

a 


>
max e>
H
H
V  ‚â§ (œÅn kH2 k‚àû + c log n)2a œÑ
q
q‚àà[n]

2

The rest of the lemma follows by applying similar arguments to the appropriate quantities.
Let œâ : [2a] ‚Üí {1, 2} be a function used to index a single term in the expansion of (H > H)a . We express the term as
follows:
(H > H)a =

a
XY

>
Hœâ(2i‚àí1)
Hœâ(2i)

œâ i=1

We will now fix one such term œâ and then bound the length of the following random vector:
vœâ = e >
1

a
Y

>
(Hœâ(2i‚àí1)
Hœâ(2i) )V

i=1

Let Œ± be used to denote a tuple (i, j) of integers used to index entries in a matrix. Let T (i) be used to denote the parity
function computed on i, i.e, 0 if i is divisible by 2 and 1 otherwise. This function indicates if the matrix in the expansion
p,q
is transposed or not. We now introduce B(i,j),(k,l)
, p ‚àà {1, 2}, q ‚àà {0, 1} and Ap(i,j) , p ‚àà {1, 2} which are defined as
follows:

Nearly Optimal Robust Matrix Completion

Ap(i,j) := Œ¥i,1 (Œ¥p,1 + Œ¥p,2 1{(i,j)‚ààSupp(H2 )} )
p,q
:= (Œ¥q,1 Œ¥j,l + Œ¥q,0 Œ¥i,k )(Œ¥p,1 + Œ¥p,2 1{(k,l)‚ààSupp(H2 )} )
B(i,j),(k,l)

where Œ¥i,j = 1 if i = j and 0 otherwise. We will subsequently write the random vector vœâ in terms of the individual entries
p,q
and Ap(i,j) is to ensure consistency in the terms used to describe vœâ . We will use
of the matrices. The role of B(i,j),(k,l)
hi,Œ± to refer to (Hi )Œ± .
With this notation in hand, we are ready to describe vœâ .
vœâ =

X

œâ(2),T (2)
(2a)
Aœâ(1)
. . . BŒ±œâ(2a),T
hœâ(1),Œ±1 ¬∑ ¬∑ ¬∑ hœâ(2a),Œ±2a vŒ±2a (2)
Œ±1 BŒ±1 Œ±2
2a‚àí1 Œ±2a

Œ±1 ,...,Œ±2a
Œ±1 (1)=1

We now write the squared length of vœâ as follows:
X
œâ(2),T (2)
(2a)
Xœâ =
Aœâ(1)
. . . BŒ±œâ(2a),T
hœâ(1),Œ±1 ¬∑ ¬∑ ¬∑ hœâ(2a),Œ±2a
Œ±1 BŒ±1 Œ±2
2a‚àí1 Œ±2a
Œ±1 ,...,Œ±2a ,Œ±01 ,...,Œ±02a
Œ±1 (1)=1,Œ±01 (1)=1
œâ(2),T (2)

Aœâ(1)
Œ±1 BŒ±0 Œ±0
1

2

œâ(2a),T (2a)
hœâ(1),Œ±01
0
2a‚àí1 Œ±2a

. . . BŒ±0

¬∑ ¬∑ ¬∑ hœâ(2a),Œ±02a hvŒ±2a (2) , vŒ±02a (2) i

We can see from the above equations that the entries used to represent vœâ are defined with respect to paths in a bipartite
graph. In the following, we introduce notations to represent entire paths rather than just individual edges:
Let Œ± := (Œ±1 , . . . , Œ±2a ) and
œâ(2),T (2)
(2a)
Œ∂Œ± := Aœâ(1)
. . . BŒ±œâ(2a),T
hœâ(1),Œ±1 . . . hœâ(2a),Œ±2a
Œ±1 BŒ±1 Œ±2
2a‚àí1 Œ±2a

Now, we can write:
Xœâ =

X

Œ∂Œ± Œ∂Œ±0 hvŒ±2a (2) , vŒ±02a (2) i

Œ±,Œ±0
Œ±1 (1)=Œ±01 (1)=1

Calculating the k th moment expansion of Xœâ for some number k, we obtain:
E[Xœâk ] =

X

i]
E[Œ∂Œ±1 . . . Œ∂Œ±2k hvŒ±12a (2) , vŒ±22a (2) i . . . hvŒ±2k‚àí1 (2) , vŒ±2k
2a (2)
2a

(27)

Œ±1 ,...,Œ±2k

We now show how to bound the above moment effectively. Notice that the moment is defined with respect to a collection
of 2k paths. We denote this collection by ‚àÜ := (Œ±1 , . . . , Œ±2k ). For each such collection, we define a partition Œì(‚àÜ) of
the index set {(s, l) : s ‚àà [2k], l ‚àà [2a]} where (s, l) and (s0 , l0 ) are in the same equivalence class if œâ(l) = œâ(l0 ) = 1 and
0
Œ±ls = Œ±ls0 . Additionally, each (s, l) such that œâ(l) = 2 is in a separate equivalence class.
We bound the expression in (27) by partitioning all possible collections of 2k paths based on the partitions defined by them
in the above manner. We then proceed to bound the contribution of any one specific path to (27) following a particular
partition Œì, the number of paths satisfying that particular partition and finally, the total number of partitions. Consider a
partition Œì with non-zero contribution to the k th moment. Since, H1 is a matrix with 0 mean, any equivalence class of
Œì containing an index (s, l) such that œâ(l) = 1 contains at least two elements (Otherwise, for any ‚àÜ satisfying Œì has 0
contribution to the k th as the element in the singleton equivalence class has mean 0).
We proceed to bound (27) by taking absolute values:
X
E[Xœâk ] ‚â§
E[|Œ∂Œ±1 | . . . |Œ∂Œ±2k ||hvŒ±12a (2) , vŒ±22a (2) i| . . . |hvŒ±2k‚àí1 (2) , vŒ±2k
i|]
2a (2)
2a

Œ±1 ,...,Œ±2k

(28)

Nearly Optimal Robust Matrix Completion

We now fix one particular partition and bound the contribution to (28) of all collections of paths ‚àÜ that correspond to a
valid partition Œì.
We construct from Œì a directed multigraph G. The equivalence classes of Œì form the vertex set of G, V (G). There are
4 kinds of edges in G where each type is indexed by a tuple (p, q) where p ‚àà {1, 2}, q ‚àà {0, 1}. We denote the edge
sets corresponding to these 4 edge types by E(1,0) , E(1,1) , E(2,0) and E(2,1) respectively. An edge of type (p, q) exists
from equivalence class Œ≥1 to equivalence class Œ≥2 if there exists (s, l) ‚àà Œ≥1 and (s0 , l0 ) ‚àà Œ≥2 such that l0 = l + 1, s = s0 ,
œâ(s0 ) = p and T (l0 ) = q.
The summation in 28 can be written as follows:









i
E[|Œ∂Œ±1 | . . . |Œ∂Œ±2k | hvŒ±12a (2) , vŒ±22a (2) i . . . hvŒ±2k‚àí1 (2) , vŒ±2k
]
(2)
2a
2a
!#
"
!
2k Y
2k
2a 
2a‚àí1

Y
Y
Y œâ(l+1),T (l+1)


œâ(1)
2k
E
‚â§œÑ
AŒ±s1
BŒ±s ,Œ±s
hœâ(l),Œ±sl 
l+1
l
s=1

‚â§ œÑ 2k

s=1 l=1

l=1

2k
Y

(Œ∂1 )

œâ(1)

AŒ±s1

s=1

2a‚àí1
Y

!
œâ(l+1),T (l+1)

BŒ±s ,Œ±s
l

l+1

l=1

w

œÑ 2k kH2 k‚àû2
=
nw1

2k
Y

Y
Œ≥‚ààV1 (G)

œâ(1)
AŒ±s1

s=1

2a‚àí1
Y

1
n

Y

kH2 k‚àû

Œ≥‚ààV2 (G)

!
œâ(l+1),T (l+1)
BŒ±s ,Œ±s
l
l+1

l=1

where (Œ∂1 ) follows from the moment conditions on H1 . V1 (G) and V2 (G) are the vertices in the graph corresponding to
tuples (i, j) such that œâ(j) = 1 and œâ(j) = 2 respectively and w1 = |V1 (G)|, w2 = |V2 (G)|.
We first consider an equivalence class Œ≥1 such that there exists an index (s, l) ‚àà Œ≥1 and l = 1. We form a spanning tree
T1 of all the nodes reachable from Œ≥1 with Œ≥1 as root. We then remove the nodes V (T1 ) from the graph G and repeat
l
S
this procedure until we obtain a set of l trees T1 , . . . , Tl with roots Œ≥1 , . . . , Œ≥l such that
V (Gi ) = V (G). This happens
i=1

because every node is reachable from some equivalence class which contains an index of the form (s, 1). Also, each of
these trees Ti , ‚àÄ i ‚àà [l] is disjoint in their vertex sets. Given this decomposition, we can factorize the above product as
follows:

E[Xœâk |Œì] ‚â§

l
w
œÑ 2k kH2 k‚àû2 Y X
Aœâ(1)
Œ±Œ≥j
w1
n
j=1
Œ±Œ≥ ,Œ≥‚ààTj

Y
{Œ≥,Œ≥ 0 }‚ààE

BŒ±1,0
Œ≥ Œ±Œ≥ 0

(1,0) (Tj )

Y

BŒ±1,1
Œ≥ Œ±Œ≥ 0

{Œ≥,Œ≥ 0 }‚ààE(1,1) (Tj )

Y
{Œ≥,Œ≥ 0 }‚ààE(2,0) (Tj )

BŒ±2,0
Œ≥ Œ±Œ≥ 0

Y

BŒ±2,1
Œ≥ Œ±Œ≥ 0

(29)

{Œ≥,Œ≥ 0 }‚ààE(2,1) (Tj )

where the inner sum is over all possible assignments to the elements in the equivalence classes of tree Tj .
For a single connected component, we can compute the summation bottom up from the leaves. First, notice that as each
BŒ±i,jŒ≥ ,Œ±Œ≥ 0 is bounded by 1:
P
Œ±Œ≥ 0

P
Œ±Œ≥ 0

BŒ±2,1
‚â§ œÅn
Œ≥ Œ±Œ≥ 0
BŒ±1,1
=n
Œ≥ Œ±Œ≥ 0

P
Œ±Œ≥ 0

P
Œ±Œ≥ 0

BŒ±2,0
‚â§ œÅn
Œ≥ Œ±Œ≥ 0
BŒ±1,0
=n
Œ≥ Œ±Œ≥ 0

Where the first two follow from the sparsity of H2 . Every node in the tree Tj with the exception of the root has a single
incoming edge. For the root, Œ≥j , we have:

Nearly Optimal Robust Matrix Completion

P

œâ(1)

AŒ±1

‚â§ œÅn for œâ(1) = 2

P

Œ±1

œâ(1)

AŒ±1

= n for œâ(1) = 1

Œ±1

From the above two observations, we have:

X
Œ±1 ,...,Œ±vj

Aœâ(1)
Œ±1

Y

BŒ±1,0
Œ≥ Œ±Œ≥ 0

{Œ≥,Œ≥ 0 }‚ààE(1,0) (Tj )

Y

Y

BŒ±1,1
Œ≥ Œ±Œ≥ 0

{Œ≥,Œ≥ 0 }‚ààE(1,1) (Tj )

BŒ±2,0
Œ≥ Œ±Œ≥ 0

{Œ≥,Œ≥ 0 }‚ààE(2,0) (Tj )

Y
{Œ≥,Œ≥ 0 }‚ààE

BŒ±2,1
‚â§ (œÅn)w2,j nw1,j
Œ≥ Œ±Œ≥ 0

(2,1) (Tj )

where wk,j represents the number of vertices in the j th component which contain tuples (y, z) such that œâ(z) = k for
k ‚àà {1, 2}.
Plugging the above in (29) gives us
w

E[Xœâk (Œì)] ‚â§

P
P
œÑ 2k kH2 k‚àû2
w
(œÅn) j w2,j n j w1,j = œÑ 2k kH2 k‚àû2 (œÅn)w2
w1
n

Let a1 and a2 be defined as |{i : œâ(i) = 1}| and |{i : œâ(i) = 2}| respectively (Note that w2 = 2a2 k).h Sumi
ming up over all possible partitions (there are at most (2a1 k)2a1 k of them), we get our final bound on E XÃÇœâk as
œÑ 2k (œÅn kH2 k‚àû )2a2 k (2a1 k)2a1 k .
Now, we bound the probability that XÃÇœâ is too large. Choosing k =
inequality, we obtain:

l

   
h 
i
 
 k
2a1 2
2a2
Pr XÃÇœâ  > (c log n) œÑ (œÅn kH2 k‚àû )
‚â§ E XÃÇœâ 

‚â§

2ka1
c log n

log n
a1

m

and applying the k th moment Markov

1
(c log n)2a1 œÑ 2 (œÅn kH2 k‚àû )2a2

k

2ka1

c

‚â§ n‚àí2 log 4
Taking a union bound over all the 22a possible œâ, over values of a from 1 to log n and over the n values of q, and summing
up the high probability bound over all possible values of œâ, we get the required result.
5.5. Additional Experimental Results
We detail some additional experiments performed with Algorithm 1 in this section. The experiments were performed on
synthetic data and real world data sets.
Synthetic data. We generate a random matrix M ‚àà R2000√ó2000 in the same way as described in Section 4. In these
experiments our aim is to analyze the behavior of the algorithm in extremal cases. We consider two of such cases : 1)
sampling probability is very low (Figure 3 (a)), 2) number of corruptions is very large (Figure 3 (b)). In the first case, we
see that the we get a reasonably good probability of recovery (‚àº 0.8) even with very low sampling probability (0.07). In
the second case, we observe that the time taken to recover seems almost independent of the number of corruptions as long
as they are below a certain threshold. In our experiments we saw that on increasing the œÅ to 0.2 the probability of recovery
went to 0. To compute the probability of recovery we ran the experiment 20 times and counted the number of successful
runs.
Foreground-background separation. We present results for one more real world data set in this section. We applied
our PG-RMC method (with varying p) to the Escalator video. Figure 4 (a) shows one frame from the video. Figure 4
(b) shows the extracted background from the video by using our method (PG-RMC , Algorithm 1) with probability of
sampling p = 0.05. Figure 4 (c) compares objective function value for different p values.

Nearly Optimal Robust Matrix Completion

n = 2000, ¬µ = 1, r = 5, p = 0.1
log ||L‚àó ‚àí LÃÇ||F ‚â§ 0.1

1

p = 0.09
p = 0.08
p = 0.07
p = 0.06

0.5

0
0

5

Time(s)

10

Prob of recovery

Prob of recovery

n = 2000, ¬µ = 1, r = 5, œÅ = 0.01
log ||L‚àó ‚àí LÃÇ||F ‚â§ 0.1

1

œÅ = 0.08
œÅ = 0.1
œÅ = 0.18

0.5

0
0

5

10

Time(s)

(a)
(b)
Figure 3: We run the PG-RMC algorithm with extremal values of sampling probability and fraction of corruptions, and
record the probability with which we recover the original matrix, (a) : time vs probability of recovery for very small values
of sampling probability, (b) : time vs probability of recovery for large number of corruptions (œÅn2 )

log ||M ‚àí LÃÇ ‚àí SÃÇ||F

¬µ = 1, r = 5
20
10
0

p = 0.01
p = 0.05
p = 0.1
St-NcRPCA

-10
-20
0

20

40

Time(s)
(a)
(b)
(c)
Figure 4: PG-RMC on Escalator video. (a): a video frame (b): an extracted background frame (c): time vs error for
different sampling probabilities; PG-RMC takes 7.3s while St-NcRPCA takes 52.9s

60

