Nearly Optimal Robust Matrix Completion

5. Appendix
We divide this section into five parts. In the first part we prove some common lemmas. In the second part we give the
convergence guarantee for PG-RMC . In the third part we give another algorithm which has a sample complexity of
Âµ2 rÏƒ âˆ—
O(Âµ4 r3 n log2 n log  1 ) and prove its convergence guarantees. In the fourth part we prove a generalized form of lemma
1. In the fifth part we present some additional experiments.
For the sake of convenience in the following proofs, we will define some notations here.
We define p =

|â„¦k,t |
mn

and we consider the following equivalent update step for L(t+1) in the analysis:
L(t+1) := Pk (M (t) )
H := E (t) + Î²G
Se(t) := HT Î¶ M âˆ’ L(t)



D := L(t) âˆ’ Lâˆ— + Se(t) âˆ’ Seâˆ—

M (t) := Lâˆ— + H
:= Seâˆ— âˆ’ Se(t)
E (t) 
Pâ„¦
G := 1 I âˆ’ pq,t D
Î²
âˆš
2 nâˆškDkâˆ
Î² :=
p

The singular values of Lâˆ— are denoted by Ïƒ1âˆ— , . . . , Ïƒrâˆ— where |Ïƒ1âˆ— | â‰¥ . . . â‰¥ |Ïƒrâˆ— | and we will let Î»1 , . . . , Î»n denote the
singular values of M (t) where |Î»1 | â‰¥ . . . â‰¥ |Î»n |.
5.1. Common Lemmas
We will begin by restating some lemmas from previous work that we will use in our proofs.
First, we restate Weylâ€™s perturbation lemma from (Bhatia, 1997), a key tool in our analysis:
Lemma 2. Suppose B = A + E âˆˆ RmÃ—n matrix. Let Î»1 , Â· Â· Â· , Î»k and Ïƒ1 , Â· Â· Â· , Ïƒk be the singular values of B and A
respectively such that Î»1 â‰¥ Â· Â· Â· â‰¥ Î»k and Ïƒ1 â‰¥ Â· Â· Â· â‰¥ Ïƒk . Then:
|Î»i âˆ’ Ïƒi | â‰¤ kEk2 âˆ€ i âˆˆ [k].
This lemma establishes a bound on the spectral norm of a sparse matrix.
Lemma 3. Let S âˆˆ RmÃ—n be a sparse matrix with row and column sparsity Ï. Then,
kSk2 â‰¤ Ï max{m, n} kSkâˆ
Proof. For any pair of unit vectors u and v, we have:
>

v Su =

X

vi uj Sij â‰¤

1â‰¤iâ‰¤m,1â‰¤jâ‰¤n

X

|Sij |

1â‰¤iâ‰¤m,1â‰¤jâ‰¤n

vi2 + u2j
2
ï£¶

!

ï£«
X
X
1ï£­ X 2 X
â‰¤
vi
|Sij | +
u2j
|Sij |ï£¸ â‰¤ Ï max{m, n} kSkâˆ
2
1â‰¤iâ‰¤m

1â‰¤jâ‰¤n

1â‰¤jâ‰¤n

1â‰¤iâ‰¤m

Lemma now follows by using kSk2 = maxu,v,kuk2 =1,kvk2 =1 uT Sv.
Now, we define a 0-mean random matrix with small higher moments values.
Definition 1 (Definition 7, (Jain & Netrapalli, 2015)). H is a random matrix of size m Ã— n with each of its entries drawn
independently satisfying the following moment conditions:
E[hij ] = 0,

|hij | < 1,

k

E[|hij | ] â‰¤

1
max{m,n} ,

for i, j âˆˆ [n] and 2 â‰¤ k â‰¤ 2 log n.
We now restate two useful lemmas from (Jain & Netrapalli, 2015):
Lemma 4 (Lemma 8 and 10 of (Jain & Netrapalli, 2015)). We have the following two claims:
âˆš
â€¢ Suppose H satisfies Definition 1. Then, w.p. â‰¥ 1 âˆ’ 1/n10+log Î± , we have: kHk2 â‰¤ 3 Î±.

Nearly Optimal Robust Matrix Completion

â€¢ Let A be a m Ã— n matrix with n â‰¥ m. Suppose â„¦ âŠ† [m] Ã— [n] is obtained by sampling each element with probability
1
p â‰¥ 4n
. Then, the following matrix H satisfies Defintion 1:


âˆš
p
1
H := âˆš
A âˆ’ Pâ„¦ (A) .
p
2 n kAkâˆ
Lemma 5 (Lemma 13, (Jain & Netrapalli, 2015)). Let A âˆˆ RnÃ—n be a symmetric matrix with eigenvalues Ïƒ1 , Â· Â· Â· , Ïƒn
where |Ïƒ1 | â‰¥ Â· Â· Â· â‰¥ |Ïƒn |. Let B = A + C be a perturbation of A satisfying kCk2 â‰¤ Ïƒ2k and let Pk (B) = U Î›U > be the
rank-k projection of B. Then, Î›âˆ’1 exists and we have:


1. A âˆ’ AU Î›âˆ’1 U > A2 â‰¤ |Ïƒk+1 | + 5 kCk2 ,

âˆ’a+2


2. AU Î›âˆ’a U > A2 â‰¤ 4 |Ïƒ2k |
âˆ€a â‰¥ 2.
We now provide a lemma that bounds k Â· kâˆ norm of an incoherent matrix with its operator norm.
Lemma 6. Let A âˆˆ RmÃ—n be a rank r, Âµ-incoherent matrix. Then for any C âˆˆ RnÃ—m , we have:
Âµ2 r
kACAkâˆ â‰¤ âˆš
kACAk2
mn
Proof. Let A = U Î£V > . Then, ACA = U U > ACAV V > . The lemma now follows by using definition of incoherence
with the fact that kU > ACAV k2 â‰¤ kACAk2 .
We now present a lemma that shows improvement in the error kL âˆ’ Lâˆ— kâˆ by using gradient descent on L(t) .
Lemma 7. Let Lâˆ— , â„¦, Seâˆ— satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of
any stage q:


z 
2
âˆ—
1. Lâˆ— âˆ’ L(t) âˆ â‰¤ 2Âµm r Ïƒk+1
+ 12 Ïƒkâˆ—


z 
2


âˆ—
+ 21 Ïƒkâˆ—
2. Seâˆ— âˆ’ Se(t)  â‰¤ 8Âµm r Ïƒk+1
âˆ

3. Supp(Se(t) ) âŠ† Supp(Seâˆ— )
âˆ—
are the k and (k + 1)th singular values of Lâˆ— . Also, let E1 = Se(t) âˆ’ Seâˆ— and E3 =
where z â‰¥ âˆ’3 and Ïƒkâˆ— and Ïƒk+1



Pâ„¦q,t
Iâˆ’ p
L(t) âˆ’ Lâˆ— + Se(t) âˆ’ Seâˆ— be the error terms defined also in (6). Then, the following holds w.p â‰¥ 1 âˆ’

nâˆ’(10+log Î±) :
1
kE1 + E3 k2 â‰¤
100


 z 
1
âˆ—
Ïƒk+1 +
Ïƒkâˆ—
2

(8)

Proof. Note from Lemma 4,
1
1
E3 =
Î²
Î²
satisfies Definition 1 with Î² =

âˆš
2 n
âˆš
p




Pâ„¦q,t  (t)
Iâˆ’
L âˆ’ Lâˆ— + Se(t) âˆ’ Seâˆ— ,
p

Â· kL(t) âˆ’ Lâˆ— + Se(t) âˆ’ Seâˆ— kâˆ .

We now bound the spectral norm of E1 + E3 as follows:




 1  (Î¶1 )
âˆš
 e(t) eâˆ— 

kE1 + E3 k2 â‰¤ kE1 k2 + Î² Â· 
E
â‰¤
Ïn
S
âˆ’
S

 + 3Î² Î±,
 Î² 3
âˆ
2

 z


  1 z  
2 r âˆš
(Î¶2 ) 1
1
60Âµ r n
 âˆ— 
 âˆ— 
âˆ—
âˆ—
â‰¤
Ïƒkq +1 +
Ïƒkq +
Î± Ïƒkq +1  +
Ïƒkq  ,
200
2
m
p
2




z
(Î¶3 ) 1
1
â‰¤
Ïƒkâˆ—q +1 +
Ïƒkâˆ—q .
100
2


 


where (Î¶1 ) follows from Lemma 3 and 4, (Î¶2 ) follows by our assumptions on Ï, L(t) âˆ’ Lâˆ— âˆ , Se(t) âˆ’ Seâˆ— 

âˆ

assumption that n = O (m) and (Î¶3 ) follows from our assumption on p.

and our

Nearly Optimal Robust Matrix Completion

In the following lemma, we prove that the value of the threshold computed using Ïƒk (M (t) ) = Ïƒk (Lâˆ— + E1 + E3 ), where
E1 , E3 are defined in (6), closely tracks the threshold that we would have gotten had we had access to the true eigenvalues
of Lâˆ— , Ïƒkâˆ— .
Lemma 8. Let Lâˆ— , â„¦, Seâˆ— satisfy Assumptions 1,2,3 respectively. Also, let the following hold for the t-th inner-iteration of
any stage q:


z 
2
âˆ—
1. Lâˆ— âˆ’ L(t) âˆ â‰¤ 2Âµm r Ïƒk+1
+ 12 Ïƒkâˆ—


z 
2


âˆ—
2. Seâˆ— âˆ’ Se(t)  â‰¤ 8Âµm r Ïƒk+1
+ 21 Ïƒkâˆ—
âˆ

3. Supp(Se(t) ) âŠ† Supp(Seâˆ— )
âˆ—
where z â‰¥ âˆ’3 and Ïƒkâˆ— and Ïƒk+1
are the k and (k + 1)th singular values of Lâˆ— . Also, let E1 = Se(t) âˆ’ Seâˆ— and E3 =



Pâ„¦q,t
L(t) âˆ’ Lâˆ— + Se(t) âˆ’ Seâˆ— be the error terms defined also in (6). Then, the following holds âˆ€z > âˆ’3 w.p
Iâˆ’ p

â‰¥ 1 âˆ’ nâˆ’(10+log Î±) :
7
8

âˆ—
Ïƒk+1

 z+1 !
1
+
Ïƒkâˆ— â‰¤
2

 z+1 !
1
9
Î»k+1 +
Î»k â‰¤
2
8

âˆ—
Ïƒk+1

 z+1 !
1
+
Ïƒkâˆ— ,
2

(9)

where Î»k := Ïƒk (M (t) ) = Ïƒk (Lâˆ— + E1 + E3 ) and E1 , E3 are defined in (6).


âˆ—
 â‰¤ kE1 + E3 k2 We
Proof. Using Weylâ€™s inequality (Lemma 2), we have: : |Î»k âˆ’ Ïƒkâˆ— | â‰¤ kE1 + E3 k2 and Î»k+1 âˆ’ Ïƒk+1
now proceed to prove the lemma as follows:

 z+1
 z+1 
 z+1



1
1
1

âˆ—
âˆ—
âˆ—


Î»k âˆ’ Ïƒk+1 âˆ’
Ïƒk  â‰¤ Î»k+1 âˆ’ Ïƒk+1 +
|Î»k âˆ’ Ïƒkâˆ— | ,
Î»k+1 +


2
2
2
 z+1 ! (Î¶)

 z 
 z+1 !
1
1
1
1
âˆ—
âˆ—
â‰¤ kE1 + E3 k2 1 +
â‰¤
Ïƒk+1 +
Ïƒk
,
1+
2
100
2
2
 z+1 !
1
1
âˆ—
Ïƒk+1 +
Ïƒkâˆ— ,
â‰¤
8
2
where (Î¶) follows from Lemma 7 and the last inequality follows from the assumption that z â‰¥ âˆ’3.
Next, we show that the projected gradient descent update (6) leads to a better estimate of Lâˆ— , i.e., we bound kL(t+1) âˆ’Lâˆ— kâˆ .
Under the assumptions of the below given Lemma, the proof follows arguments similar to (Netrapalli et al., 2014) with
additional challenge that arises due to more involved error terms E1 , E3 .
Our proof proceeds by first symmetrizing our matrices by rectangular dilation. We first begin by noting some properties of
symmetrized matrices used in the proof of the following lemma.
>
Remark 1. Let Abe a m Ã—
 n dimensional matrix with singular value decomposition U Î£V . We denote its symmetrized
>
0 A
. Then:
version by As :=
A 0
1. The eigenvalue decomposition of As is given by As = Us Î£s Us> where



1 V
V
Î£
Us := âˆš
Î£s :=
U
âˆ’U
0
2

0
Pk (A> )
Pk (A)
0
 > j

(A A)
0
3. We have A2j
=
s
0
(AA> )j

0
âˆ’Î£





2. P2k (As ) =

As2j+1 =

0
(A> A)j A>
> j
(AA ) A
0





Nearly Optimal Robust Matrix Completion

4. We have



V Î£âˆ’j V >
0
=
when j is even
0
U Î£âˆ’j U >


0
V Î£âˆ’j U >
>
Us Î£âˆ’j
U
=
when j is odd
s
s
U Î£âˆ’j V >
0

>
Us Î£âˆ’j
s Us

Lemma 9. Let L = Pk (Lâˆ— + H), where H is any perturbation matrix that satisfies the following:
Ïƒâˆ—
1. kHk2 â‰¤ 4k
Ïƒâˆ—
2. âˆ€i âˆˆ [n], a â‰¤ d log2 n e with Ï… â‰¤ 4k
q

 
 >


e H > H a V âˆ—  , e> HH > a U âˆ—  â‰¤ (Ï…)2a Âµ r
i
i
mq
2
2

 > >
 


e H HH > a U âˆ—  , e> H H > H a V âˆ—  â‰¤ (Ï…)2a+1 Âµ r
i
i
m
2
2
where Ïƒkâˆ— is the k th singular value of Lâˆ— . Also, let Lâˆ— satisfy Assumption 1. Then, the following holds:
kL âˆ’ Lâˆ— kâˆ â‰¤


Âµ2 r âˆ—
Ïƒk+1 + 20 kHk2 + 8Ï…
m

where Âµ and r are the rank and incoherence of the matrix Lâˆ— respectively.
Proof. Let Ls , Hs and Lâˆ—s denote the symmetrized forms of L, H and Lâˆ— respectively. Now, we have:
Ls = P2k (Lâˆ—s + Hs )
Let l = m + n. Let Î»1 , Â· Â· Â· , Î»l be the eigenvalues of Ms = Lâˆ—s + Hs with |Î»1 | â‰¥ |Î»2 | Â· Â· Â· â‰¥ |Î»l |. Let u1 , u2 , Â· Â· Â· , ul be the
3Ïƒ âˆ—
corresponding eigenvectors of Ms . Using Lemma 2 along with the assumption on kHs k2 , we have: |Î»2k | â‰¥ 4k .
Let U Î›V be the eigen vector decomposition of L. Let Us Î›s Us> to be the eigen vector decomposition of Ls . Then, using
Remark 1 we have âˆ€ i âˆˆ [2k]:


Hs
Lâˆ— ui
âˆ—
(Ls + Hs ) ui = Î»i ui , i.e. I âˆ’
ui = s .
Î»i
Î»i
As |Î»2k | â‰¥

âˆ—
3Ïƒk
4

and kHs k2 â‰¤ 41 Ïƒkâˆ— , we can apply the Taylorâ€™s series expansion to get the following expression for ui :
ï£«
ï£¶
j
âˆ 
1 ï£­X Hs ï£¸ Lâˆ—s ui
ui =
.
Î»i j=0 Î»i
Î»i

That is,
Ls =

2k
X

Î»i ui u>
i =

i=1

=

X

2k
X

Î»âˆ’1
i

i=1
2k
X

0â‰¤s,t<âˆ i=1

âˆ’(s+t+1)

Î»i

X
0â‰¤s,t<âˆ



Hs
Î»i

s

âˆ—
Lâˆ—s ui u>
i Ls

âˆ— t
Hss Lâˆ—s ui u>
i Ls Hs =

X



Hs
Î»i

t
,

Hss Lâˆ—s Us Î›sâˆ’(s+t+1) Us> Lâˆ—s Hst .

0â‰¤s,t<âˆ

Subtracting Lâˆ—s on both sides and taking operator norm, we get:




X




âˆ—
>
âˆ—
s âˆ—
âˆ’(s+t+1) > âˆ— t
âˆ—


Hs Ls Us Î›s
Us Ls Hs âˆ’ Ls  ,
kLs âˆ’ Ls kâˆ = Us Î›s Us âˆ’ Ls âˆ = 
0â‰¤s,t<âˆ

âˆ


X
 âˆ—

 s âˆ—
âˆ’1 > âˆ—
âˆ—
âˆ’(s+t+1) > âˆ— t 

= Ls Us Î›s Us Ls âˆ’ Ls âˆ +
Us Ls Hs  .
Hs Ls Us Î›s
1â‰¤s+t<âˆ

âˆ

(10)

Nearly Optimal Robust Matrix Completion

We separately bound the first and the second term of RHS. The first term can be bounded as follows:

 


 (Î¶1 )  âˆ—
 âˆ—
0
V Î›âˆ’1 U > âˆ—
âˆ—
> âˆ—
âˆ—

Ls Us Î›âˆ’1
Ls âˆ’ Ls 
s Us Ls âˆ’ Ls âˆ = Ls
U Î›âˆ’1 V >
0
âˆ

(11)

 (Î¶2 ) Âµ2 r  âˆ—

 (Î¶3 ) 2
 âˆ— 

 + 5 kHk ,
L V Î›âˆ’1 U > Lâˆ— âˆ’ Lâˆ—  â‰¤ âˆšÂµ r Ïƒk+1
â‰¤ Lâˆ— V Î›âˆ’1 U > Lâˆ— âˆ’ Lâˆ— âˆ â‰¤ âˆš
2
2
mn
mn

(12)

where (Î¶1 ) follows Remark 1, (Î¶2 ) from Lemma 6 and (Î¶3 ) follows from Claim 1 of Lemma 5 after symmetrization.
We now bound second term of RHS of (10) which we again split in two parts. We first bound the terms with s + t > log n:


 s âˆ—
> âˆ— t
U
L
H
Hs Ls Us Î›(s+t+1)
s
s
s s

âˆ





â‰¤ Hss Lâˆ—s Us Î›sâˆ’(s+t+1) Us> Lâˆ—s Hst 

2

(Î¶1 )

s+t

â‰¤ kHs k2


4

2
Ïƒkâˆ—

âˆ’(s+tâˆ’1)


(s+tâˆ’1) (Î¶ )
 (s+tâˆ’1)
2
2
1
â‰¤ 4 kHk2 kHk2 âˆ—
â‰¤ 4 kHk2
Ïƒk
2
 (s+tâˆ’1âˆ’log n)
 (s+tâˆ’1âˆ’log n)
2
Âµ r
1
1
4
â‰¤4
,
kHk2
â‰¤ kHk2
n
2
m
2

(13)

where (Î¶1 ) follows from the second claim of Lemma 5 and noting that kHs k2 = kHk2 and (Î¶2 ) follows from assumption
on kHk2 and using the fact that s + t â‰¥ log n.
Summing up over all terms with s + t > log n, we get from (13) and (12):
kLs âˆ’ Lâˆ—s kâˆ â‰¤


Âµ2 r  âˆ— 
Ïƒk+1 + 20 kHk2 +
m



 s âˆ—

Hs Ls Us Î›sâˆ’(s+t+1) Us> Lâˆ—s Hst 

X

âˆ

0<s+tâ‰¤log n

(14)

where the first inequality follows because m â‰¤ n.
Now, for terms corresponding to 1 â‰¤ s + t â‰¤ log n, we have:




 s âˆ—

 > s âˆ—

max
Hs Ls Us Î›sâˆ’(s+t+1) Us> Lâˆ—s Hst  =
eq1 Hs Ls Us Î›sâˆ’(s+t+1) Us> Lâˆ—s Hst eq2 
q1 âˆˆ[m+n],q2 âˆˆ[m+n]
âˆ



 

 
 > t âˆ—
 âˆ— âˆ— >
s âˆ—
âˆ’(s+t+1) > âˆ— âˆ— 


â‰¤
max e>
H
U
(U
)
U
Î›
U
U
Î£
max
e
H
U
Î£

s
q1 s s 2
s
s
s
s
s s
q2
s 2 ,
q1 âˆˆ[m+n]

We will now bound the terms,

2

max

q1 âˆˆ[m+n]

q2 âˆˆ[m+n]

 > s âˆ—
eq Hs Us  . Note from Remark 1.1 that Usâˆ— =
1
2

âˆš1
2

 âˆ—
V
Uâˆ—


Vâˆ—
. Now, we have
âˆ’U âˆ—

the following cases for Hss :
"
Hsj

=

H >H
0

 2s

0
s
HH > 2

#

"

when s is even

Hsj

0
=
b s c
H H >H 2

H > HH >
0

b 2s c #
when s is odd

In these two cases, we have:
"
Hss Usâˆ— =

âˆš1
2

s
H >H 2 V âˆ—
s
HH > 2 U âˆ—

#
s
H >H 2 V âˆ—
s
âˆ’ HH > 2 U âˆ—

This leads to the following 4 cases for

for s even
for s odd

max

q1 âˆˆ[m+n]

"
Hss Usâˆ— =

b s c #
âˆ’H > HH > 2 U âˆ—
b s c
H H >H 2 V âˆ—

 > s âˆ—
eq Hs Us  :
1
2


 2s âˆ— 
 >

>
max
H
H
V 
e
0
q
0
q âˆˆ[n]
2

 s
 > >
> b 2 c âˆ—
H
HH
U
max
e

0
q
0

q âˆˆ[n]

âˆš1
2

b s c
H > HH > 2 U âˆ—
b s c
H H >H 2 V âˆ—

2


 s âˆ—
 >

> 2
max
HH
U 
e
0
q
0
q âˆˆ[m]
2

b 2s c âˆ— 
 >

>
max
H
H
H
V
e

0
q
0

q âˆˆ[m]

(15)

2

Nearly Optimal Robust Matrix Completion

We can now bound the terms in (15) as follows:


 s âˆ—

Us> Lâˆ—s Hst 
Hs Ls Us Î›âˆ’(s+t+1)
s

âˆ

(Î¶1 )

â‰¤


Âµ2 r s+t 

 âˆ—
Ï…
Ls Us Î›sâˆ’(s+t+1) Us> Lâˆ—s 
m
2
 s+tâˆ’1
 s+tâˆ’1
(Î¶2 ) 4Âµ2 r
4Âµ2 r
2
1
â‰¤
â‰¤
Ï… s+t
Ï…
m
Ïƒkâˆ—
m
2

(16)

where (Î¶1 ) follows from the second assumption of the Lemma and the preceding argument and (Î¶2 ) follows from Claim 2
of Lemma 5 and the final step follows from our bound on Ï….


Finally, note from the Remark 1 that kLâˆ—s âˆ’ Ls kâˆ = Lâˆ— âˆ’ L(t+1) âˆ . Now, summing up (16) over all 1 â‰¤ s + t â‰¤ log n
and combining with (14), the lemma is proved.
In the next lemma, we show that with the threshold chosen in the algorithm, we show an improvement in the estimation of
Seâˆ— by Se(t) .
Lemma 10. In the tth iterate of the q th stage, assume the following holds:


z 
2
âˆ—
1. Lâˆ— âˆ’ L(t) âˆ â‰¤ 2Âµm r Ïƒk+1
+ 12 Ïƒkâˆ—

 z 

 z 
9
1
âˆ—
âˆ—
âˆ—
(t)
âˆ—
+ 1
2. 87 Î· Ïƒk+1
2 Ïƒk â‰¤ Î¶ â‰¤ 8 Î· Ïƒk+1 + 2 Ïƒk
âˆ—
where Ïƒkâˆ— and Ïƒk+1
are the k and (k + 1)th singular values of Lâˆ— , Î»k and Î»k+1 are the k and (k + 1)th singular values of
(t)
M and, r and Âµ are the rank and incoherence of the m Ã— n matrix Lâˆ— respectively. Then we have


 
1. Supp Se(t) âŠ† Supp Seâˆ—





2. Se(t) âˆ’ Seâˆ— 

âˆ

â‰¤

8Âµ2 r
m

âˆ—
Ïƒk+1
+


1 z
2

Ïƒkâˆ—



Proof. We first prove the first claim of the lemma. Consider an index pair (i, j) âˆˆ
/ Supp(Seâˆ— ).
 z  (Î¶ )

 2Âµ2 r 
1
1
16Âµ2 r (t) (Î¶2 ) (t)

(t) 
âˆ—
Î¶
â‰¤ Î¶
Ïƒk+1 +
Ïƒkâˆ— â‰¤
Mij âˆ’ Lij  â‰¤
m
2
7mÎ·
where (Î¶1 ) follows from the second assumption of the lemma and (Î¶2 ) follows from our setting of Î· =
do not threshold any entry that is not corrupted by Seâˆ— .

4Âµ2 r
m .

Hence, we

Now, we prove the second claim of the lemma. Consider an index entry (i, j) âˆˆ Supp(Seâˆ— ). Here, we consider two cases:
(t)
(t)
âˆ—
1. The entry (i, j) âˆˆ Supp(Se(t) ): Here the entry (i, j) is thresholded. We know that Lij + Seij = Lâˆ—ij + Seij
from which
we get

 
 

 e(t) eâˆ—   âˆ—


(t) 
Sij âˆ’ Sij  = Lij âˆ’ Lij  â‰¤ Lâˆ— âˆ’ L(t) 
âˆ




(t) 
âˆ—
2. The entry (i, j) âˆˆ
/ Supp(Se(t) ): Here the entry (i, j) is not thresholded. We know that Lâˆ—ij + Seij
âˆ’ Lij  â‰¤ Î¶ (t) from
which we get
 


 eâˆ— 

(t) 
Sij  â‰¤ Î¶ (t) + Lâˆ—ij âˆ’ Lij 

 z 

 z 
(Î¶2 ) 36Âµ2 r
1
2Âµ2 r
1
âˆ—
âˆ—
Ïƒk+1
+
Ïƒkâˆ— +
Ïƒk+1
+
Ïƒkâˆ—
â‰¤
8m
2
m
2

 z 
8Âµ2 r
1
âˆ—
â‰¤
Ïƒk+1
+
Ïƒkâˆ—
m
2
2

where (Î¶2 ) follows from the second assumption along with our setting of Î· = 4 Âµmr .
The above two cases prove the second statement of the lemma.

Nearly Optimal Robust Matrix Completion

We will now prove Lemma 1





Pâ„¦
Proof of Lemma 1: Recall the definitions of E1 = Seâˆ— âˆ’ Se(t) , E2 = L(t) âˆ’ Lâˆ— , E3 = I âˆ’ pq,t (E2 âˆ’ E1 ) and
q
Î² = 2 np kE2 âˆ’ E1 kâˆ . Recall that H := E1 + E3 From Lemma 4, we have that Î²1 E3 satisfies Definition 1. This implies
that the matrix

1
Î²

(E1 + E3 ) satisfies the conditions of Lemma 14. Now, we have âˆ€1 â‰¤ a â‰¤ dlog ne and âˆ€i âˆˆ [n]:




> !a 




1
1
âˆ—
ei (HH > )a U âˆ—  = Î² 2a 
e
U
H
H


i
2


Î²
Î²
2
r
r 


2a
r
2a
(Î¶)
Ïn
r
r
n
â‰¤ Î² 2a
kE1 kâˆ + c log n
Âµ
â‰¤Âµ
Ïn kE1 kâˆ + 2c
(kE1 âˆ’ E2 kâˆ ) log n
Î²
m
m
p
where (Î¶) follows from the application of Lemma 14 along with the incoherence assumption on U âˆ— . The other statements
of the lemma can be proved in a similar manner by invocations of the different claims of Lemma 14.

5.2. Algorithm PG-RMC
20Âµ2 nrÏƒ âˆ—

1
). Consider the stage q reached at the termination of the algorithm.
Proof of Theorem 1: We know that T â‰¥ log(

We know from Lemma 11 that:



T âˆ’3 âˆ—  8Âµ2 r âˆ—
2

1. E (T ) âˆ â‰¤ 8Âµm r Ïƒkâˆ—q +1 + 21
Ïƒkq â‰¤ m Ïƒkq +1 + 10n



T âˆ’3  âˆ—  2Âµ2 r âˆ—
2

2. L(T ) âˆ’ Lâˆ— âˆ â‰¤ 2Âµm r Ïƒkâˆ—q +1 + 12
Ïƒkq  â‰¤ m Ïƒkq +1 + 10n

Combining this with Lemmas 2 and 7, we get:




1
m

(T ) 
âˆ—
âˆ—
Ïƒkq +1 +
Ïƒkq +1 (M ) â‰¥ Ïƒkq +1 âˆ’
100
10nÂµ2 r

When the while loop terminates, Î·Ïƒkq +1 M (T ) <


2n ,

which from (17), implies that Ïƒkâˆ—q +1 <





kL âˆ’ Lâˆ— kâˆ = L(T ) âˆ’ Lâˆ— 

âˆ

â‰¤

(17)
m
7nÂµ2 r .

So we have:

2Âµ2 r âˆ—


Ïƒ
+
â‰¤
.
m kq +1 10n
2n

We will now bound the number of iterations required for the PG-RMC to converge.
17 âˆ—
Ïƒkqâˆ’1 +1 âˆ€q â‰¥ 1. By recursively applying this inequality, we get
From claim 2 of Lemma 12, we have Ïƒkâˆ—q +1 â‰¤ 32

q âˆ—
q
17
Ïƒkâˆ—q +1 â‰¤ 32 Ïƒ1âˆ— . We know that when the algorithm terminates, Ïƒkâˆ—q +1 < 7Âµ2 r . Since, 17
Ïƒ1 is an upper bound for
32
 2 âˆ—
7Âµ
rÏƒ
1
Ïƒkâˆ—q +1 , an upper bound for the number of iterations is 5 log
. Also, note that an upper bound to this quantity is


used to partition the samples provided to the algorithm. This happens with probability â‰¥ 1âˆ’T 2 nâˆ’(10+log Î±) â‰¥ 1âˆ’nâˆ’ log Î± .
This concludes the proof.

In the following lemma, we show that we make progress simultaneously in the estimation of both Seâˆ— and Lâˆ— by Se(t) and
L(t) . We make use of Lemmas 9 and 10 to show progress in the estimation of one affects the other alternatively. We also
emphasize the roles of the following quantities in enabling us to prove our convergence result:
1. kHk2 - We use Lemma 7 to bound this quantity
2. The analysis of the following 4 quantities is crucial to obtaining error bounds in kkâˆ norm




 >

 >

j
j
e 0 H > H 2 V âˆ— 
e 0 HH > 2 U âˆ— 
for j even
max
max
q
q




0
0
q âˆˆ[n]
q âˆˆ[m]

2

2
j
j
 > >





e>0 H H > H b 2 c V âˆ— 
e 0 H HH > b 2 c U âˆ— 
for j odd
max
max
q
q




q 0 âˆˆ[n]
q 0 âˆˆ[m]
2

We use Lemma 1 to bound this quantity.

2

Nearly Optimal Robust Matrix Completion

Lemma 11. Let Lâˆ— , â„¦, Seâˆ— and Se(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the q th stage of
Algorithm 1, Se(t) and L(t) satisfy:
 tâˆ’3  !


8Âµ2 r  âˆ— 
1
 e(t) eâˆ— 
 âˆ— 
S âˆ’ S  â‰¤
Ïƒkq +1  +
Ïƒkq  ,
m
2
âˆ

 

Supp Se(t) âŠ† Supp Seâˆ— ,
!
!
  1 tâˆ’2  
  1 tâˆ’2  


9
7
 âˆ— 
 âˆ— 
 âˆ— 
 âˆ— 
(t+1)
â‰¤ Î· Ïƒkq +1  +
Î· Ïƒkq +1  +
Ïƒkq  â‰¤ Î¶
Ïƒkq  and
8
2
8
2
 tâˆ’3  !


2Âµ2 r  âˆ— 
1
 (t)
 âˆ— 
âˆ—
L âˆ’ L  â‰¤
Ïƒkq +1  +
Ïƒkq  .
m
2
âˆ
with probability â‰¥ 1 âˆ’ ((q âˆ’ 1)T + t âˆ’ 1)nâˆ’(10+log Î±) where T is the number of iterations in the inner loop.
Proof. We prove the lemma by induction on both q and t. Recall that E (t) = Seâˆ— âˆ’ Se(t)
Base Case: q = 1 and t = 0
We begin by first proving an upper bound on kLâˆ— kâˆ . We do this as follows:

 r
r
r
 X
2
X


 âˆ— âˆ— 
 âˆ—  X

âˆ— 
âˆ—
âˆ—
âˆ—
uik vjk  â‰¤ âˆšÂµ r Ïƒ1âˆ—
Lij  = 
Ïƒkâˆ— uâˆ—ik vjk
â‰¤ Ïƒ1âˆ—
Ïƒk uik vjk  â‰¤


mn
k=1

k=1

k=1

where the last inequality follows from Cauchy-Schwartz and the incoherence of U âˆ— . This directly proves the third claim
of the lemma for the base case. Recall, that Î¶ (0) = Î·Ïƒ1âˆ— . We now have from the thresholding step and the incoherence
assumption on Lâˆ— :
(Î¶)


2
1. E (0) âˆ â‰¤ 8Âµm r (Ïƒ2âˆ— + 2Ïƒ1âˆ— ) â‰¤

 

2. Supp Se(t) âŠ† Supp Seâˆ— .

8Âµ2 r
m


8Ïƒkâˆ—1 , and

where (Î¶) follows from Lemma 12.
Finally, from Lemma 8, we have:

 9


7
Î· Ïƒkâˆ—1 +1 + 4Ïƒkâˆ—1 â‰¤ Î¶ (1) = Î· Ïƒk1 +1 (M (t) ) + 4Ïƒk1 (M (t) ) â‰¤ Î· Ïƒkâˆ—1 +1 + 4Ïƒkâˆ—1
8
8
So the base case of induction is satisfied.
Induction over t
We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:



tâˆ’3 âˆ— 
2
a) E (t) âˆ â‰¤ 8Âµm r Ïƒkâˆ—q +1 + 12
Ïƒkq


 
b) Supp Se(t) âŠ† Supp Seâˆ— .



tâˆ’3 âˆ— 
2
c) Lâˆ— âˆ’ L(t) âˆ â‰¤ 2Âµm r Ïƒkâˆ—q +1 + 21
Ïƒkq







tâˆ’2  âˆ— 
tâˆ’2  âˆ— 




d) 78 Î· Ïƒkâˆ—q +1  + 12
Ïƒkq  â‰¤ Î¶ (t+1) â‰¤ 89 Î· Ïƒkâˆ—q +1  + 12
Ïƒkq 
with probability 1 âˆ’ ((q âˆ’ 1)T + t âˆ’ 1)nâˆ’(10+log Î±) . Then by Lemma 9, we have:


 (t+1)

âˆ’ Lâˆ— 
L

âˆ

â‰¤


Âµ2 r  âˆ—
Ïƒkq +1 + 20 kHk2 + 8Ï…
m

(18)

Nearly Optimal Robust Matrix Completion

From Lemma 1, we have:

(Î¶1 )

1
+ 8Î²Î± log n â‰¤
100
âˆ





Ï… â‰¤ Ïn E (t) 

Ïƒkâˆ—q +1

!
 tâˆ’3
(Î¶2 ) 1
1
âˆ—
+
Ïƒkq + 8Î²Î± log n â‰¤
2
50

Ïƒkâˆ—q +1

!
 tâˆ’3
1
âˆ—
+
Ïƒkq
(19)
2



where (Î¶1 ) follows from our assumptions on Ï and our inductive hypothesis on E (t) âˆ and (Î¶2 ) follows from our




assumption on p and by noticing that kDkâˆ â‰¤ E (t) âˆ + Lâˆ— âˆ’ L(t) âˆ . Recall that D = L(t) âˆ’ Lâˆ— + Se(t) âˆ’ Seâˆ— .
From Lemma 7:
1
kHk2 â‰¤
100

Ïƒkâˆ—q +1

!
 tâˆ’3
1
âˆ—
+
Ïƒkq
2

(20)

with probability â‰¥ 1 âˆ’ nâˆ’(10+log Î±) . From Equations (20), (19) and (18), we have:
!
 tâˆ’2


2Âµ2 r
1
 âˆ—
(t+1) 
âˆ—
âˆ—
Ïƒkq
Ïƒkq +1 +
 â‰¤
L âˆ’ L
m
2
âˆ
which by union bound holds with probability â‰¥ 1 âˆ’ ((q âˆ’ 1)T + t)nâˆ’(10+log Î±) . Hence, using Lemma 10 and our inductive
hypothesis on Î¶ (t+1) we have:



tâˆ’2 âˆ— 
2
Ïƒkq
1. E (t+1) âˆ â‰¤ 8Âµm r Ïƒkâˆ—q +1 + 21


 
2. Supp Se(t+1) âŠ† Supp Seâˆ— .
which also holds with probability â‰¥ 1 âˆ’ ((q âˆ’ 1)T + t)nâˆ’(10+log Î±) . This concludes the proof for induction over t.




Finally, from Lemma 8 and our bounds on E (t+1) âˆ and Lâˆ— âˆ’ L(t+1) âˆ , we have:

  1 tâˆ’1  
7


 âˆ— 
Î· Ïƒkâˆ—q +1  +
Ïƒkq 
8
2

!
â‰¤ Î¶

(t+2)


  1 tâˆ’1  
9


 âˆ— 
â‰¤ Î· Ïƒkâˆ—q +1  +
Ïƒkq 
8
2

!

Induction Over Stages q
We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:



T âˆ’3 âˆ—  8Âµ2 rÏƒkâˆ—q +1
2

Ïƒkq â‰¤
1. E (T ) âˆ â‰¤ 8Âµm r Ïƒkâˆ—q +1 + 21
+ 10n
, and
m

 

2. Supp Se(T ) âŠ† Supp Seâˆ— .




T âˆ’2  âˆ— 
T âˆ’2  âˆ— 




3. 87 Î· Ïƒkâˆ—q +1  + 12
Ïƒkq  â‰¤ Î¶ (T +1) â‰¤ 98 Î· Ïƒkâˆ—q +1  + 12
Ïƒkq 
with probability â‰¥ 1 âˆ’ (qT âˆ’ 1)nâˆ’(10+log Î±) . From Lemmas 2 and 7, we get:






m
1


(T )
âˆ—
âˆ—
Ïƒkq +1 +
âˆ’ Ïƒkq +1  â‰¤ kHk2 â‰¤
Ïƒkq +1 M
100
10nÂµ2 r

with probability 1 âˆ’ nâˆ’(10+log Î±) . We know that Î·Ïƒkq +1 M (t) â‰¥



 (T +1)

âˆ’ Lâˆ— 
L

âˆ


2n





which with (21) implies that Ïƒkâˆ—q +1  >

!
 T âˆ’2


1
2Âµ2 r
m
âˆ—
âˆ—
+
Ïƒkq â‰¤
Ïƒkq +1 +
2
m
20nÂµ2 rn


Ïƒkâˆ— +1
2Âµ2 r
2Âµ2 r  âˆ—  (Î¶4 ) 2Âµ2 r  âˆ— 
â‰¤
Ïƒkâˆ—q +1 + q
â‰¤
2Ïƒkq +1 â‰¤
8Ïƒkq+1
m
2
m
m

2Âµ2 r
â‰¤
m

Ïƒkâˆ—q +1

(21)
m
10nÂµ2 r .

Nearly Optimal Robust Matrix Completion

where (Î¶4 ) follows from Lemma 12. By union bound this holds with probability â‰¥ 1 âˆ’ qT nâˆ’(10+log Î±) .
Now, from Lemma 10 and the inductive hypothesis on Î¶ T +1 , we have through a similar series of arguments as above:




2
1. E (0) âˆ â‰¤ 8Âµm r 8Ïƒkâˆ—q+1


 
2. Supp Se(0) âŠ† Supp Seâˆ—
which holds with probability â‰¥ 1 âˆ’ qT nâˆ’(10+log Î±) .




Recall, now that L(0) = L(T +1) . Finally, from Lemma 8 and our bounds on E (0) âˆ and L(0) âˆ’ Lâˆ— âˆ , we have:






9 
7  âˆ—






Î· Ïƒkq+1 +1  + 4 Ïƒkâˆ—q+1  â‰¤ Î¶ (1) â‰¤ Î· Ïƒkâˆ—q+1 +1  + 4 Ïƒkâˆ—q+1 
8
8

Lemma 12. Suppose at the beginning of the q th stage of algorithm 1:




2Âµ2 r
1. Lâˆ— âˆ’ L(0) âˆ â‰¤ m 2Ïƒkâˆ—qâˆ’1 +1




8Âµ2 r
2. E (0) âˆ â‰¤ m 2Ïƒkâˆ—qâˆ’1 +1
Then, the following hold:
1. Ïƒkâˆ—q â‰¥

15 âˆ—
32 Ïƒkqâˆ’1 +1

2. Ïƒkâˆ—q +1 â‰¤

17 âˆ—
32 Ïƒkqâˆ’1 +1

with probability â‰¥ 1 âˆ’ nâˆ’(10+log Î±)
Proof. We know that:
Î»kq â‰¤ Ïƒkâˆ—q + kHk2 ,

Î»kqâˆ’1 +1 â‰¥ Ïƒkâˆ—qâˆ’1 +1 âˆ’ kHk2 ,

Î»kq â‰¥

Î»kqâˆ’1 +1
2

Combining the three inequalities, we get:
Ïƒkâˆ—q â‰¥

Ïƒkâˆ—qâˆ’1 +1 âˆ’ 3 kHk2
2

Applying Lemma 7, we get the first claim of the lemma.
Similar to the first claim, we have:
Î»kq +1 â‰¥ Ïƒkâˆ—q +1 âˆ’ kHk2 ,

Î»kqâˆ’1 +1 â‰¤ Ïƒkâˆ—qâˆ’1 +1 + kHk2 ,

Again, combining the three inequalities, we get:
Ïƒkâˆ—q +1 â‰¤

Ïƒkâˆ—qâˆ’1 +1 + 3 kHk2

Another application of Lemma 7 gives the second claim.

2

Î»kq +1 â‰¤

Î»kqâˆ’1 +1
2

Nearly Optimal Robust Matrix Completion

b = R-RMC(â„¦, Pâ„¦ (M ), , r, Î·, Ïƒ): Non-convex Robust Matrix Completion
Algorithm 3 L
1: Input: Observed entries â„¦, Matrix Pâ„¦ (M ) âˆˆ RmÃ—n , convergence criterion , target rank r, thresholding parameter Î·,
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

upper bound on Ïƒ1âˆ— Ïƒ
2
T â† 10 log 20Âµ nrÏƒ
Partition â„¦ into rT + 1 subsets {â„¦0 } âˆª {â„¦q,t : q âˆˆ [r], t âˆˆ [T ]} using 2
L(0) = 0, Î¶ (0) â† Î·Ïƒ
mn
M (0) â† |â„¦
Pâ„¦0 (M âˆ’ HT Î¶ (M ))
0|
q â† 0

do
while Ïƒq+1 (M (0) ) > 2Î·m
q â† q+1
for Iteration t = 0 to t = T do
S (t) = HÎ¶ (Pâ„¦q,t (M âˆ’ L(t) ))
Pâ„¦q,t (L(t) + S (t) âˆ’ M )
M (t) = L(t) âˆ’ |â„¦mn
q,t |

12:

L(t+1) = Pq (M (t) )

13:

(t+1)

14:
15:
16:
17:

/*Number of inner iterations*/

/*Projection onto set of sparse matrices*/
/*Gradient Descent Update*/
/*Projected Gradient Descent step*/




1 t
2

(t)

(t)

Set threshold Î¶
â† Î· Ïƒq+1 (M ) +
Ïƒq (M
end for
S (0) = S (T ) , L(0) = L(T +1) , M (0) = M (T ) , Î¶ (0) = Î¶ (T +1)
end while
Return: L(T +1)


)

5.3. Algorithm R-RMC
Proof of Theorem 2: We know that T â‰¥ log(

20Âµ2 nrÏƒ1âˆ—
).


Consider the stage q reached at the termination of the algorithm. We know from Lemma 13 that:



T âˆ’1 âˆ—  8Âµ2 r âˆ—
2

âˆ—
1. E (T ) âˆ â‰¤ 8Âµm r Ïƒq+1
+ 12
Ïƒq â‰¤ m Ïƒq+1 + 10n


2. L(T ) âˆ’ Lâˆ— âˆ â‰¤

2Âµ2 r
m


âˆ—
Ïƒq+1
+


1 T âˆ’1
2


Ïƒqâˆ— â‰¤

2Âµ2 r âˆ—
m Ïƒq+1

+


10n

Combining this with Lemmas 2 and 7, we get:

Ïƒq+1 (M

(T )

)â‰¥


When the while loop terminates, Î·Ïƒq+1 M (T ) <

âˆ—
Ïƒq+1


2n ,

1
âˆ’
100


âˆ—
Ïƒq+1
+

m
10nÂµ2 r


(22)

âˆ—
which from (22), implies that Ïƒq+1
<





kL âˆ’ Lâˆ— kâˆ = L(T ) âˆ’ Lâˆ— 

âˆ

â‰¤

m
7nÂµ2 r .

So we have:



2Âµ2 r âˆ—
|Ïƒkq +1 | +
â‰¤
.
m
10n
2n


As in the case of the proof of Theorem 1, the following lemma shows that we simultaneously make progress in both
the estimation of Lâˆ— and Seâˆ— by L(t) and Se(t) respectively. Similar to Lemma 11, we make use of Lemmas 10 and
9 to show
how improvement
in estimation
of one of the quantities
affects the other



 and the other
 five terms, kHk2 ,

j âˆ— 

j
 >
 >

 > >
 >

>
> j âˆ—
> j âˆ—
max
U  , max
U  and max
eq0 H H V  , max
eq0 HH
eq0 H HH
eq0 H H > H V âˆ— 
0
0
0
0
q âˆˆ[n]

2

q âˆˆ[m]

2

q âˆˆ[n]

2

q âˆˆ[m]

2

are analyzed the same way:
Lemma 13. Let Lâˆ— , â„¦, Seâˆ— and Se(t) satisfy Assumptions 1,2,3 respectively. Then, in the tth iteration of the q th stage of

Nearly Optimal Robust Matrix Completion

Algorithm 3, Se(t) and L(t) satisfy:
 tâˆ’1 !
1
âˆ—
Ïƒqâˆ— ,
Ïƒq+1
+
2
âˆ


 
Supp Se(t) âŠ† Supp Seâˆ— ,
!
!
 t
 t

 âˆ— 


 âˆ—

1
1
7
9
âˆ— 
Ïƒq  â‰¤ Î¶ (t+1) â‰¤ Î· Ïƒq+1
Ïƒqâˆ—  and
+
Î· Ïƒq+1  +
8
2
8
2
 tâˆ’1 !


2Âµ2 r
1
 (t)
âˆ—
âˆ—
Ïƒq+1 +
Ïƒqâˆ— .
L âˆ’ L  â‰¤
m
2
âˆ
8Âµ2 r
â‰¤
m



 e(t) eâˆ— 
S âˆ’ S 

with probability â‰¥ 1 âˆ’ ((q âˆ’ 1)T + t âˆ’ 1)nâˆ’(10+log Î±) where T is the number of iterations in the inner loop.
Proof. We prove the lemma by induction on both q and t.
Base Case: q = 1 and t = 0
We begin by first proving an upper bound on kLâˆ— kâˆ . We do this as follows:


r
r
r
 X
X
 âˆ— âˆ—  Âµ2 r âˆ—
 âˆ— âˆ— âˆ— 
 âˆ—  X
âˆ— âˆ— âˆ— 
uik vjk  â‰¤
Ïƒk uik vjk  â‰¤ Ïƒ1âˆ—
Lij  = 
Ïƒk uik vjk  â‰¤
Ïƒ


m 1
k=1

k=1

k=1

where the last inequality follows from Cauchy-Schwartz and the incoherence of U âˆ— . This directly proves the third claim
of the lemma for the base case. Recall that Î¶ (0) = Î·Ïƒ1âˆ— . We also note that due to the thresholding step and the incoherence
assumption on Lâˆ— , we have:


2
1. E (0) âˆ â‰¤ 8Âµm r (Ïƒ2âˆ— + 2Ïƒ1âˆ— )


 
2. Supp Se(t) âŠ† Supp Seâˆ— .


From Lemma 8 and our bounds on E (0) and L(1) âˆ’ Lâˆ— âˆ , we have:
9
7
Î· (|Ïƒ2âˆ— | + |Ïƒ1âˆ— |) â‰¤ Î¶ (1) â‰¤ Î· (|Ïƒ2âˆ— | + |Ïƒ1âˆ— |)
8
8
So the base case of induction is satisfied.
Induction over t
We first prove the inductive step over t (for a fixed q). By inductive hypothesis we assume that:



tâˆ’1 âˆ— 
2
âˆ—
a) E (t) âˆ â‰¤ 8Âµm r |Ïƒq+1
| + 12
|Ïƒq |


 
b) Supp Se(t) âŠ† Supp Seâˆ— .


c) Lâˆ— âˆ’ L(t) âˆ â‰¤
d)

7
8Î·



âˆ— 
Ïƒq+1
+

2Âµ2 r
m


1 t
2



âˆ—
|+
|Ïƒq+1


1 tâˆ’1
2


|Ïƒqâˆ— |


 âˆ— 

âˆ— 
Ïƒq  â‰¤ Î¶ (t+1) â‰¤ 9 Î· Ïƒq+1
+
8


1 t
2

 âˆ— 
Ïƒq 

with probability 1 âˆ’ ((q âˆ’ 1)T + t âˆ’ 1)nâˆ’(10+log Î±) .
Then by Lemma 9, we have:


 (t+1)

âˆ’ Lâˆ— 
L

âˆ

â‰¤


Âµ2 r  âˆ—
|Ïƒkq +1 | + 20 kHk2 + 8Ï…
m

(23)

Nearly Optimal Robust Matrix Completion

From Lemma 1, we have:
(Î¶1 )

1
+ 8Î²Î± log n â‰¤
100
âˆ





Ï… â‰¤ Ïn E (t) 

âˆ—
Ïƒq+1

 tâˆ’1 !
(Î¶2 ) 1
1
Ïƒqâˆ— + 8Î²Î± log n â‰¤
+
2
50

âˆ—
Ïƒq+1

 tâˆ’1 !
1
Ïƒqâˆ—
+
2

(24)



where (Î¶1 ) follows from our assumptions on Ï and our inductive hypothesis on E (t) âˆ and (Î¶2 ) follows from our




assumption on p and by noticing that kDkâˆ â‰¤ E (t) âˆ + Lâˆ— âˆ’ L(t) âˆ . Recall that D = L(t) âˆ’ Lâˆ— + Se(t) âˆ’ Seâˆ— .
From Lemma 7:
1
kHk2 â‰¤
100

âˆ—
Ïƒq+1

 tâˆ’1 !
1
+
Ïƒqâˆ—
2

(25)

with probability â‰¥ 1 âˆ’ nâˆ’(10+log Î±) . From Equations (25), (24) and (23), we have:
 t !


2Âµ2 r
1
 âˆ—
(t+1) 
âˆ—
Ïƒq+1 +
Ïƒqâˆ—
 â‰¤
L âˆ’ L
m
2
âˆ
which by union bound holds with probability â‰¥ 1 âˆ’ ((q âˆ’ 1)T + t)nâˆ’(10+log Î±) . Hence, using Lemma 10 and the inductive
hypothesis on Î¶ (t+1) we have:



t 
2
âˆ—
1. E (t+1) âˆ â‰¤ 8Âµm r Ïƒq+1
+ 21 Ïƒqâˆ—


 
2. Supp Se(t+1) âŠ† Supp Seâˆ— .
which also holds with probability â‰¥ 1 âˆ’ ((q âˆ’ 1)T + t)nâˆ’(10+log Î±) . This concludes the proof for induction over t.




Finally, using Lemma 8 and our bounds on E (t+1) âˆ and L(t+1) âˆ’ Lâˆ— âˆ , we have:
 âˆ— 
7
+
Î· Ïƒq+1
8

!
!
 t+1
 t+1
 âˆ— 
 âˆ—
 âˆ—
1
1
9
(t+2)
Ïƒq  â‰¤ Î¶
Ïƒq 
â‰¤ Î· Ïƒq+1  +
2
8
2

Induction Over Stages q
We now prove the induction over q. Suppose the hypothesis holds for stage q. At the end of stage q, we have:

âˆ—


T âˆ’1 âˆ—  8Âµ2 rÏƒq+1
2

âˆ—
+ 12
Ïƒq â‰¤
+ 10n
1. E (T ) âˆ â‰¤ 8Âµm r Ïƒq+1
m

 

2. Supp Se(T ) âŠ† Supp Seâˆ—




T   
T   
âˆ— 
âˆ— 
3. 87 Î· Ïƒq+1
+ 12 Ïƒqâˆ—  â‰¤ Î¶ (T +1) â‰¤ 89 Î· Ïƒq+1
+ 21 Ïƒqâˆ— 
with probability â‰¥ 1 âˆ’ (qT âˆ’ 1)nâˆ’(10+log Î±) .
From Lemmas 2 and 7 we get:






1
m

âˆ— 
âˆ—
Ïƒq+1
+
Ïƒq+1 M (T ) âˆ’ Ïƒq+1
 â‰¤ kHk2 â‰¤
100
10nÂµ2 r

with probability 1 âˆ’ nâˆ’(10+log Î±) . We know that Î·Ïƒq+1 M (t) â‰¥



 (T +1)

âˆ’ Lâˆ— 
L

âˆ


2n

âˆ—
which with (26) implies that Ïƒq+1
>

 T !


1
2Âµ2 r
m
âˆ—
+
Ïƒqâˆ— â‰¤
Ïƒq+1
+
2
m
20Âµ2 rn


âˆ—

Ïƒq+1
2Âµ2 r
2Âµ2 r
âˆ—
âˆ—
â‰¤
Ïƒq+1
+
â‰¤
2Ïƒq+1
m
2
m
2Âµ2 r
â‰¤
m

âˆ—
Ïƒq+1

(26)
m
10nÂµ2 r .

Nearly Optimal Robust Matrix Completion

By union bound this holds with probability â‰¥ 1 âˆ’ qT nâˆ’(10+log Î±) .
Now, from Lemma 10 and our inductive hypothesis on Î¶ (T +1) , we have through a similar series of arguments as above:



2
âˆ—
1. E (0) âˆ â‰¤ 8Âµm r 2Ïƒq+1


 
2. Supp Se(0) âŠ† Supp Seâˆ—
which holds with probability â‰¥ 1 âˆ’ qT nâˆ’(10+log Î±) .




Recall, now that L(0) = L(T +1) . Finally, from Lemma 8 and our bounds on E (0) âˆ and L(0) âˆ’ Lâˆ— âˆ , we have:
9  âˆ—   âˆ— 
7  âˆ—   âˆ— 
+ Ïƒq+1
Î· Ïƒq+2 + Ïƒq+1 â‰¤ Î¶ (1) â‰¤ Î· Ïƒq+2
8
8

5.4. Proof of a generalized form of Lemma 1
Lemma 14. Suppose H = H1 + H2 and H âˆˆ RmÃ—n where H1 satisfies Definition 1 (Definition 7 from (Jain &
Netrapalli, 2015)) and H2 is a matrix with column and row sparsity Ï. Let U be a matrix with rows denoted as
u1 , . . . , um and let V be a matrix with rows denoted as v1 , . . . , vn . Let eq be the q th vector from standard basis. Let
Ï„ = max{max kui k , max kvi k}. Then, for 0 â‰¤ a â‰¤ log n:
iâˆˆ[m]

iâˆˆ[n]



a 
 



>
> a 
max e>
V  , max e>
U  â‰¤ (Ïn kH2 kâˆ + c log n)2a Ï„
q H H
q HH
qâˆˆ[n]
2 qâˆˆ[m]
2


 
a 
 > >
 >

> a 
>
max eq H HH
U  , max eq H H H V  â‰¤ (Ïn kH2 kâˆ + c log n)2a+1 Ï„

qâˆˆ[n]

2 qâˆˆ[m]

2

c

with probability nâˆ’2 log 4 +4 .
Proof. Similar to (Jain & Netrapalli, 2015), we will prove the statement for q = 1 and it can be proved for q âˆˆ [n] by
taking a union bound over all q. For the sake of brevity, we will prove only the inequality:

a 


>
max e>
H
H
V  â‰¤ (Ïn kH2 kâˆ + c log n)2a Ï„
q
qâˆˆ[n]

2

The rest of the lemma follows by applying similar arguments to the appropriate quantities.
Let Ï‰ : [2a] â†’ {1, 2} be a function used to index a single term in the expansion of (H > H)a . We express the term as
follows:
(H > H)a =

a
XY

>
HÏ‰(2iâˆ’1)
HÏ‰(2i)

Ï‰ i=1

We will now fix one such term Ï‰ and then bound the length of the following random vector:
vÏ‰ = e >
1

a
Y

>
(HÏ‰(2iâˆ’1)
HÏ‰(2i) )V

i=1

Let Î± be used to denote a tuple (i, j) of integers used to index entries in a matrix. Let T (i) be used to denote the parity
function computed on i, i.e, 0 if i is divisible by 2 and 1 otherwise. This function indicates if the matrix in the expansion
p,q
is transposed or not. We now introduce B(i,j),(k,l)
, p âˆˆ {1, 2}, q âˆˆ {0, 1} and Ap(i,j) , p âˆˆ {1, 2} which are defined as
follows:

Nearly Optimal Robust Matrix Completion

Ap(i,j) := Î´i,1 (Î´p,1 + Î´p,2 1{(i,j)âˆˆSupp(H2 )} )
p,q
:= (Î´q,1 Î´j,l + Î´q,0 Î´i,k )(Î´p,1 + Î´p,2 1{(k,l)âˆˆSupp(H2 )} )
B(i,j),(k,l)

where Î´i,j = 1 if i = j and 0 otherwise. We will subsequently write the random vector vÏ‰ in terms of the individual entries
p,q
and Ap(i,j) is to ensure consistency in the terms used to describe vÏ‰ . We will use
of the matrices. The role of B(i,j),(k,l)
hi,Î± to refer to (Hi )Î± .
With this notation in hand, we are ready to describe vÏ‰ .
vÏ‰ =

X

Ï‰(2),T (2)
(2a)
AÏ‰(1)
. . . BÎ±Ï‰(2a),T
hÏ‰(1),Î±1 Â· Â· Â· hÏ‰(2a),Î±2a vÎ±2a (2)
Î±1 BÎ±1 Î±2
2aâˆ’1 Î±2a

Î±1 ,...,Î±2a
Î±1 (1)=1

We now write the squared length of vÏ‰ as follows:
X
Ï‰(2),T (2)
(2a)
XÏ‰ =
AÏ‰(1)
. . . BÎ±Ï‰(2a),T
hÏ‰(1),Î±1 Â· Â· Â· hÏ‰(2a),Î±2a
Î±1 BÎ±1 Î±2
2aâˆ’1 Î±2a
Î±1 ,...,Î±2a ,Î±01 ,...,Î±02a
Î±1 (1)=1,Î±01 (1)=1
Ï‰(2),T (2)

AÏ‰(1)
Î±1 BÎ±0 Î±0
1

2

Ï‰(2a),T (2a)
hÏ‰(1),Î±01
0
2aâˆ’1 Î±2a

. . . BÎ±0

Â· Â· Â· hÏ‰(2a),Î±02a hvÎ±2a (2) , vÎ±02a (2) i

We can see from the above equations that the entries used to represent vÏ‰ are defined with respect to paths in a bipartite
graph. In the following, we introduce notations to represent entire paths rather than just individual edges:
Let Î± := (Î±1 , . . . , Î±2a ) and
Ï‰(2),T (2)
(2a)
Î¶Î± := AÏ‰(1)
. . . BÎ±Ï‰(2a),T
hÏ‰(1),Î±1 . . . hÏ‰(2a),Î±2a
Î±1 BÎ±1 Î±2
2aâˆ’1 Î±2a

Now, we can write:
XÏ‰ =

X

Î¶Î± Î¶Î±0 hvÎ±2a (2) , vÎ±02a (2) i

Î±,Î±0
Î±1 (1)=Î±01 (1)=1

Calculating the k th moment expansion of XÏ‰ for some number k, we obtain:
E[XÏ‰k ] =

X

i]
E[Î¶Î±1 . . . Î¶Î±2k hvÎ±12a (2) , vÎ±22a (2) i . . . hvÎ±2kâˆ’1 (2) , vÎ±2k
2a (2)
2a

(27)

Î±1 ,...,Î±2k

We now show how to bound the above moment effectively. Notice that the moment is defined with respect to a collection
of 2k paths. We denote this collection by âˆ† := (Î±1 , . . . , Î±2k ). For each such collection, we define a partition Î“(âˆ†) of
the index set {(s, l) : s âˆˆ [2k], l âˆˆ [2a]} where (s, l) and (s0 , l0 ) are in the same equivalence class if Ï‰(l) = Ï‰(l0 ) = 1 and
0
Î±ls = Î±ls0 . Additionally, each (s, l) such that Ï‰(l) = 2 is in a separate equivalence class.
We bound the expression in (27) by partitioning all possible collections of 2k paths based on the partitions defined by them
in the above manner. We then proceed to bound the contribution of any one specific path to (27) following a particular
partition Î“, the number of paths satisfying that particular partition and finally, the total number of partitions. Consider a
partition Î“ with non-zero contribution to the k th moment. Since, H1 is a matrix with 0 mean, any equivalence class of
Î“ containing an index (s, l) such that Ï‰(l) = 1 contains at least two elements (Otherwise, for any âˆ† satisfying Î“ has 0
contribution to the k th as the element in the singleton equivalence class has mean 0).
We proceed to bound (27) by taking absolute values:
X
E[XÏ‰k ] â‰¤
E[|Î¶Î±1 | . . . |Î¶Î±2k ||hvÎ±12a (2) , vÎ±22a (2) i| . . . |hvÎ±2kâˆ’1 (2) , vÎ±2k
i|]
2a (2)
2a

Î±1 ,...,Î±2k

(28)

Nearly Optimal Robust Matrix Completion

We now fix one particular partition and bound the contribution to (28) of all collections of paths âˆ† that correspond to a
valid partition Î“.
We construct from Î“ a directed multigraph G. The equivalence classes of Î“ form the vertex set of G, V (G). There are
4 kinds of edges in G where each type is indexed by a tuple (p, q) where p âˆˆ {1, 2}, q âˆˆ {0, 1}. We denote the edge
sets corresponding to these 4 edge types by E(1,0) , E(1,1) , E(2,0) and E(2,1) respectively. An edge of type (p, q) exists
from equivalence class Î³1 to equivalence class Î³2 if there exists (s, l) âˆˆ Î³1 and (s0 , l0 ) âˆˆ Î³2 such that l0 = l + 1, s = s0 ,
Ï‰(s0 ) = p and T (l0 ) = q.
The summation in 28 can be written as follows:









i
E[|Î¶Î±1 | . . . |Î¶Î±2k | hvÎ±12a (2) , vÎ±22a (2) i . . . hvÎ±2kâˆ’1 (2) , vÎ±2k
]
(2)
2a
2a
!#
"
!
2k Y
2k
2a 
2aâˆ’1

Y
Y
Y Ï‰(l+1),T (l+1)


Ï‰(1)
2k
E
â‰¤Ï„
AÎ±s1
BÎ±s ,Î±s
hÏ‰(l),Î±sl 
l+1
l
s=1

â‰¤ Ï„ 2k

s=1 l=1

l=1

2k
Y

(Î¶1 )

Ï‰(1)

AÎ±s1

s=1

2aâˆ’1
Y

!
Ï‰(l+1),T (l+1)

BÎ±s ,Î±s
l

l+1

l=1

w

Ï„ 2k kH2 kâˆ2
=
nw1

2k
Y

Y
Î³âˆˆV1 (G)

Ï‰(1)
AÎ±s1

s=1

2aâˆ’1
Y

1
n

Y

kH2 kâˆ

Î³âˆˆV2 (G)

!
Ï‰(l+1),T (l+1)
BÎ±s ,Î±s
l
l+1

l=1

where (Î¶1 ) follows from the moment conditions on H1 . V1 (G) and V2 (G) are the vertices in the graph corresponding to
tuples (i, j) such that Ï‰(j) = 1 and Ï‰(j) = 2 respectively and w1 = |V1 (G)|, w2 = |V2 (G)|.
We first consider an equivalence class Î³1 such that there exists an index (s, l) âˆˆ Î³1 and l = 1. We form a spanning tree
T1 of all the nodes reachable from Î³1 with Î³1 as root. We then remove the nodes V (T1 ) from the graph G and repeat
l
S
this procedure until we obtain a set of l trees T1 , . . . , Tl with roots Î³1 , . . . , Î³l such that
V (Gi ) = V (G). This happens
i=1

because every node is reachable from some equivalence class which contains an index of the form (s, 1). Also, each of
these trees Ti , âˆ€ i âˆˆ [l] is disjoint in their vertex sets. Given this decomposition, we can factorize the above product as
follows:

E[XÏ‰k |Î“] â‰¤

l
w
Ï„ 2k kH2 kâˆ2 Y X
AÏ‰(1)
Î±Î³j
w1
n
j=1
Î±Î³ ,Î³âˆˆTj

Y
{Î³,Î³ 0 }âˆˆE

BÎ±1,0
Î³ Î±Î³ 0

(1,0) (Tj )

Y

BÎ±1,1
Î³ Î±Î³ 0

{Î³,Î³ 0 }âˆˆE(1,1) (Tj )

Y
{Î³,Î³ 0 }âˆˆE(2,0) (Tj )

BÎ±2,0
Î³ Î±Î³ 0

Y

BÎ±2,1
Î³ Î±Î³ 0

(29)

{Î³,Î³ 0 }âˆˆE(2,1) (Tj )

where the inner sum is over all possible assignments to the elements in the equivalence classes of tree Tj .
For a single connected component, we can compute the summation bottom up from the leaves. First, notice that as each
BÎ±i,jÎ³ ,Î±Î³ 0 is bounded by 1:
P
Î±Î³ 0

P
Î±Î³ 0

BÎ±2,1
â‰¤ Ïn
Î³ Î±Î³ 0
BÎ±1,1
=n
Î³ Î±Î³ 0

P
Î±Î³ 0

P
Î±Î³ 0

BÎ±2,0
â‰¤ Ïn
Î³ Î±Î³ 0
BÎ±1,0
=n
Î³ Î±Î³ 0

Where the first two follow from the sparsity of H2 . Every node in the tree Tj with the exception of the root has a single
incoming edge. For the root, Î³j , we have:

Nearly Optimal Robust Matrix Completion

P

Ï‰(1)

AÎ±1

â‰¤ Ïn for Ï‰(1) = 2

P

Î±1

Ï‰(1)

AÎ±1

= n for Ï‰(1) = 1

Î±1

From the above two observations, we have:

X
Î±1 ,...,Î±vj

AÏ‰(1)
Î±1

Y

BÎ±1,0
Î³ Î±Î³ 0

{Î³,Î³ 0 }âˆˆE(1,0) (Tj )

Y

Y

BÎ±1,1
Î³ Î±Î³ 0

{Î³,Î³ 0 }âˆˆE(1,1) (Tj )

BÎ±2,0
Î³ Î±Î³ 0

{Î³,Î³ 0 }âˆˆE(2,0) (Tj )

Y
{Î³,Î³ 0 }âˆˆE

BÎ±2,1
â‰¤ (Ïn)w2,j nw1,j
Î³ Î±Î³ 0

(2,1) (Tj )

where wk,j represents the number of vertices in the j th component which contain tuples (y, z) such that Ï‰(z) = k for
k âˆˆ {1, 2}.
Plugging the above in (29) gives us
w

E[XÏ‰k (Î“)] â‰¤

P
P
Ï„ 2k kH2 kâˆ2
w
(Ïn) j w2,j n j w1,j = Ï„ 2k kH2 kâˆ2 (Ïn)w2
w1
n

Let a1 and a2 be defined as |{i : Ï‰(i) = 1}| and |{i : Ï‰(i) = 2}| respectively (Note that w2 = 2a2 k).h Sumi
ming up over all possible partitions (there are at most (2a1 k)2a1 k of them), we get our final bound on E XÌ‚Ï‰k as
Ï„ 2k (Ïn kH2 kâˆ )2a2 k (2a1 k)2a1 k .
Now, we bound the probability that XÌ‚Ï‰ is too large. Choosing k =
inequality, we obtain:

l

   
h 
i
 
 k
2a1 2
2a2
Pr XÌ‚Ï‰  > (c log n) Ï„ (Ïn kH2 kâˆ )
â‰¤ E XÌ‚Ï‰ 

â‰¤

2ka1
c log n

log n
a1

m

and applying the k th moment Markov

1
(c log n)2a1 Ï„ 2 (Ïn kH2 kâˆ )2a2

k

2ka1

c

â‰¤ nâˆ’2 log 4
Taking a union bound over all the 22a possible Ï‰, over values of a from 1 to log n and over the n values of q, and summing
up the high probability bound over all possible values of Ï‰, we get the required result.
5.5. Additional Experimental Results
We detail some additional experiments performed with Algorithm 1 in this section. The experiments were performed on
synthetic data and real world data sets.
Synthetic data. We generate a random matrix M âˆˆ R2000Ã—2000 in the same way as described in Section 4. In these
experiments our aim is to analyze the behavior of the algorithm in extremal cases. We consider two of such cases : 1)
sampling probability is very low (Figure 3 (a)), 2) number of corruptions is very large (Figure 3 (b)). In the first case, we
see that the we get a reasonably good probability of recovery (âˆ¼ 0.8) even with very low sampling probability (0.07). In
the second case, we observe that the time taken to recover seems almost independent of the number of corruptions as long
as they are below a certain threshold. In our experiments we saw that on increasing the Ï to 0.2 the probability of recovery
went to 0. To compute the probability of recovery we ran the experiment 20 times and counted the number of successful
runs.
Foreground-background separation. We present results for one more real world data set in this section. We applied
our PG-RMC method (with varying p) to the Escalator video. Figure 4 (a) shows one frame from the video. Figure 4
(b) shows the extracted background from the video by using our method (PG-RMC , Algorithm 1) with probability of
sampling p = 0.05. Figure 4 (c) compares objective function value for different p values.

Nearly Optimal Robust Matrix Completion

n = 2000, Âµ = 1, r = 5, p = 0.1
log ||Lâˆ— âˆ’ LÌ‚||F â‰¤ 0.1

1

p = 0.09
p = 0.08
p = 0.07
p = 0.06

0.5

0
0

5

Time(s)

10

Prob of recovery

Prob of recovery

n = 2000, Âµ = 1, r = 5, Ï = 0.01
log ||Lâˆ— âˆ’ LÌ‚||F â‰¤ 0.1

1

Ï = 0.08
Ï = 0.1
Ï = 0.18

0.5

0
0

5

10

Time(s)

(a)
(b)
Figure 3: We run the PG-RMC algorithm with extremal values of sampling probability and fraction of corruptions, and
record the probability with which we recover the original matrix, (a) : time vs probability of recovery for very small values
of sampling probability, (b) : time vs probability of recovery for large number of corruptions (Ïn2 )

log ||M âˆ’ LÌ‚ âˆ’ SÌ‚||F

Âµ = 1, r = 5
20
10
0

p = 0.01
p = 0.05
p = 0.1
St-NcRPCA

-10
-20
0

20

40

Time(s)
(a)
(b)
(c)
Figure 4: PG-RMC on Escalator video. (a): a video frame (b): an extracted background frame (c): time vs error for
different sampling probabilities; PG-RMC takes 7.3s while St-NcRPCA takes 52.9s

60

