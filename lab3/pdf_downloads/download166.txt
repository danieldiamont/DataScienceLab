Logarithmic Time One-Against-Some

A. Proof of theorem 2
Proof. For the fixed tree at timestep t (there have been
t ‚àí 1 previous splits) with a fixed partition function in
the nodes, the weighted entropy of class labels is Wt =
P
{n‚ààLeaves} pn Hn .
When we split the tth node, the weak learning assumption
implies entropy decreases by Œ≥ according to:


pr
pl
Hl +
Hr + Œ≥
Hn ‚â•
pn
pn

Algorithm

Parameter

Default Value

Binary

Learning Rate
Loss

1
logistic

Max Depth
Num Candidates
Depth Penalty (Œª)

log2 (#classes)
4 log2 (#classes)
1

Recall Tree

Table 3. Algorithm hyperparameters for various algorithms. ‚ÄúBinary‚Äù refers to hyperparameters inherited via reduction to binary
classification.

where Œ≥ is the advantage of the weak learner. Hence,
Wt ‚àí Wt+1 = pn Hn ‚àí pl Hl ‚àí pr Hr ‚â• pn Œ≥

ALOI (Geusebroek et al., 2005) is a color image collection
of one-thousand small objects recorded for scientific purposes (Geusebroek et al., 2005). We use the same train-test
split and representation as Choromanska et. al. (Choromanska & Langford, 2015).

We can bound pn according to
max pn ‚â•
n

1
t

which implies

Œ≥
t
This can be solved recursively to get:
Wt ‚àí Wt+1 ‚â•

Wt+1 ‚â§ W1 ‚àí Œ≥

Imagenet consists of features extracted from intermediate
layers of a convolutional neural network trained on the ILVSRC2012 challenge dataset. This dataset was originally
developed to study transfer learning in visual tasks (Oquab
et al., 2014); more details are at http://www.di.ens.
fr/willow/research/cnn/. We utilize a predictor
linear in this representation.

t
X
1
i=1

i

‚â§ W1 ‚àí Œ≥ ln(t + 1)

= H1 ‚àí Œ≥ ln(t + 1)

where the second inequality follows from lower bounding
a harmonic series, and H1 is the marginal Shannon entropy
of the class labels.
To finish the proof, we bound the multiclass loss in terms
of the average entropy. For any leaf node n we can assign
the most likely label, y = arg maxi œÄni so the error rate is
n = 1 ‚àí œÄny .
X

Wt+1 =

pn

X
{n‚ààLeaves}

X

=

pn

X

X

1
œÄni

œÄni ln

1
œÄny

i

pn ln

{n‚ààLeaves}

‚â•

œÄni ln

i

{n‚ààLeaves}

‚â•

X

B. Datasets

1
1 ‚àí n

pn n

{n‚ààLeaves}

=
Putting these inequalities together we have:
 ‚â§ H1 ‚àí Œ≥ ln(t + 1)

LTCB is the Large Text Compression Benchmark, consisting of the first billion bytes of a particular Wikipedia
dump (Mahoney, 2009). Originally developed to study text
compression, it is now commonly used as a language modeling benchmark where the task is to predict the next word
in the sequence. We limit the vocabulary to 80000 words
plus a single out-of-vocabulary indicator; utilize a model
linear in the 6 previous unigrams, the previous bigram, and
the previous trigram; and utilize a 90-10 train-test split on
entire Wikipedia articles.
ODP(Bennett & Nguyen, 2009) is a multiclass dataset derived from the Open Directory Project. We utilize the same
train-test split and labels from (Choromanska & Langford,
2015). Specifically there is a fixed train-test split of 2:1,
the representation of a document is a bag of words, and
the class label is the most specific category associated with
each document.

C. Experimental Methodology
Default Performance Methodology Hyperparameter
selection can be computationally burdensome for large data
sets, which is relevant to any claims of decreased training
times. Therefore we report results using the default values indicated in Table 3. For the larger data sets (Imagenet, ODP), we do a single pass over the training data;

Logarithmic Time One-Against-Some
Table 2. Empirical comparison summary. When OAA training is accelerated using parallelism and gradient subsampling, wall clock
times are parenthesized. Training times are for defaults, i.e., without hyperparameter optimization. Asterisked LOMTree results are
from (Choromanska & Langford, 2015).

Test Error
Default Tuned

Training Time

Inference Time
per example

12.1%
8.6%
16.5%‚àó

571s
1972s
112s

67¬µs
194¬µs
28¬µs

84.7%
91.1%
96.7%

82.2%
88.4%
90.1%‚àó

446d (20.4h)
71.4h
14.0h

118ms
4ms
0.56ms

OAA
Recall Tree
LOMTree

78.7%
78.0%
78.4%

76.8%
77.6%
-

764d (19.1h)
4.8h
4.3h

3600¬µs
76¬µs
51¬µs

OAA
Recall Tree
LOMTree

91.2%
96.2%
95.4%

90.6%
93.8%
93.5%‚àó

133d (1.3h)
1.5h
0.6h

560ms
1.9ms
0.52ms

Dataset

Method

ALOI

OAA
Recall Tree
LOMTree

12.2%
11.4%
21.4%

Imagenet

OAA
Recall Tree
LOMTree

LTCB

ODP

for the smaller data set (ALOI), we do multiple passes over
the training data, monitoring a 10% held-out portion of the
training data to determine when to stop optimizing.
Tuned Performance Methodology For tuned performance, we use random search over hyperparameters, taking the best result over 59 probes. For the smaller data set
(ALOI), we optimize validation error on a 10% held-out
subset of the training data. For the larger data sets (Imagenet, ODP), we optimize progressive validation loss on
the initial 10% of the training data. After determining hyperparameters we retrain with the entire training set and
report the resulting test error.
When available we report published LOMtree results, although they utilized a different method for optimizing hyperparameters.
Timing Measurements All timings are taken from the
same 24 core xeon server machine. Furthermore, all algorithms are implemented in the Vowpal Wabbit toolkit and
therefore share file formats, parser, and binary classification base learner implying differences are attributable to
the different reductions. Our baseline OAA implementation is mature and highly tuned: it always exploits vectorization, and furthermore can optionally utilize multicore
training and negative gradient subsampling to accelerate
training. For the larger datasets these latter features were
necessary to complete the experiments: estimated unaccelerated training times are given, along with wall clock times
in parenthesis.

