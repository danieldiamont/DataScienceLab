A. Proofs
Sketch of the proof for Theorem 1. We need to the show
that for every Q̃ ∈ O(n), there exits a tuple of vectors (u1 , . . . , un ) ∈ R × · · · × Rn such that Q̃ =
M1 (u1 , . . . , un ). Algorithm 1 shows how a QR decomposition can be performed using the matrices {Hk (uk )}nk=1
while ensuring that the upper triangular matrix R has positive diagonal elements. If we apply this algorithm to an
orthogonal matrix Q̃, we get a tuple (u1 , . . . , un ) which
satisfies
QR = Hn (un ) . . . H1 (u1 )R = Q̃.
Note that the matrix R must be orthogonal since R = Q0 Q̃.
Therefore, R = I, since the only upper triangular matrix with positive diagonal elements is the identity matrix.
Hence, we have

where dA, dB, and dC represent infinitesimal perturbations and

0

0
∂L
∂C ∂L
∂C ∂L
C :=
, A :=
, B :=
.
∂C
∂A ∂C
∂B ∂C
Proof of Theorem 2. Let C = h − U T −1 U 0 h where
(U, h) ∈ Rn×m × Rn and T = striu(U 0 U ) + 21 diag(U 0 U ).
Notice that the matrix T can be written using the Hadamard
product as follows
T = B ◦ (U 0 U ),

(1)

where B = striu(Jm ) + 21 Im and Jm is the m × m matrix
of all ones.
Calculating the infinitesimal perturbations of C gives
dC =(I − U T −1 U 0 )dh
− dU T −1 U 0 h − U T −1 dU 0 h

M1 (u1 , . . . , un ) = Hn (un ) . . . H1 (u1 ) = Q̃.

+ U T −1 dT T −1 U 0 h.
Using Equation (1) we can write
dT = B ◦ (dU 0 U + U 0 dU ).

Algorithm 1 QR decomposition using the mappings {Hk }.
For a matrix B ∈ Rn×n , {Bk,k }1≤k≤n denote its diagonal
elements, and Bk..n,k = (Bk,k , . . . , Bn,k )0 ∈ Rn−k+1 .
Require: A ∈ Rn×n is a full-rank matrix.
Ensure: Q and R where Q = Hn (un ) . . . H1 (u1 ) and R
is upper triangular with positive diagonal elements such
that A = QR
R←A
Q ← I {Initialise Q to the identity matrix}
for k = 1 to n − 1 do
if Rk,k == kRk..n,k k then
un−k+1 = (0, . . . , 0, 1)0 ∈ Rn−k+1
else
un−k+1 ← Rk..n,k − kRk..n,k k (1, 0, . . . , 0)0
un−k+1 ← un−k+1 / kun−k+1 k
end if
R ← Hn−k+1 (un−k+1 )R
Q ← QHn−k+1 (un−k+1 )
end for
u1 = sgn(Rn,n ) ∈ R
R ← H1 (u1 )R
Q ← QH1 (u1 )
Lemma 1. (Giles, 2008) Let A, B, and C be real or complex matrices, such that C = f (A, B) where f is some differentiable mapping. Let L be some scalar quantity which
depends on C. Then we have the following identity
0

0

0

Tr(C dC) = Tr(A dA) + Tr(B dB),

By substituting this back into the expression of dC, multi0
plying the left and right-hand sides by C , and applying the
trace we get
0

0

Tr(C dC) = Tr(C (I − U T −1 U 0 )dh)
0

0

− Tr(C dU T −1 U 0 h) − Tr(C U T −1 dU 0 h)
0

+ Tr(C U T −1 (B ◦ (dU 0 U + U 0 dU ))T −1 U 0 h).
Now using the identity Tr(AB) = Tr(BA), where the second dimension of A agrees with the first dimension of B,
0
we can rearrange the expression of Tr(C dC) as follows
0

0

Tr(C dC) = Tr(C (I − U T −1 U 0 )dh)
0

0

− Tr(T −1 U 0 hC dU ) − Tr(hC U T −1 dU 0 )
0

+ Tr(T −1 U 0 hC U T −1 (B ◦ (dU 0 U + U 0 dU ))).
To simplify the expression, we will use the short notations
C̃ = (T 0 )−1 U 0 C,
h̃ = T −1 U 0 h,
0

Tr(C dC) becomes
0

0

Tr(C dC) = Tr((C − C̃ 0 U 0 )dh)
0

− Tr(h̃C dU ) − Tr(hC̃ 0 dU 0 )
+ Tr(h̃C̃ 0 (B ◦ (dU 0 U + U 0 dU ))).

where δi,j is the Kronecker delta and J·K is the Iversion
bracket (i.e. JpK = 1 if p is true and JpK = 0 otherwise).

Now using the two following identities of the trace
Tr(A0 ) = Tr(A),

In order to compute the gradients in Equations (14) and
(15). we first need to compute h̃ = T −1 U 0 h and C̃ =
∂L
. This is equivalent to solving the triangular
(T 0 )−1 U 0 ∂C
∂L
.
systems of equations T h̃ = U 0 h and T 0 C̃ = U 0 ∂C

Tr(A(B ◦ C)) = Tr((A ◦ B 0 )C)),
0

we can rewrite Tr(C dC) as follows
0

0

Tr(C dC) =Tr((C − C̃ 0 U 0 )dh)
0

0

Solving the triangular system T h̃ = U 0 h. For 1 ≤ k ≤
m, we can express the k-th row of this system as

0

− Tr(h̃C dU ) − Tr(hC̃ dU )
+ Tr((h̃C̃ 0 ◦ B 0 )dU 0 U )

tk,k h̃k +

+ Tr((h̃C̃ 0 ◦ B 0 )U 0 dU ).

=

By rearranging and taking the transpose of the third and
fourth term of the right-hand side we obtain
0

0

=

Tr(C dC) =Tr((C − C̃ 0 U 0 )dh)

m
X

tk,j h̃j =

j=k+1
n
X

vr,k hr −

r=k

0

− Tr(h̃C dU ) − Tr(C̃h0 dU )

Factorising by dU inside the Tr we get
0

Tr(C dC) = Tr((C − C̃ 0 U 0 )dh)−
h
i
0
Tr((h̃C + C̃h0 − (C̃ h̃0 ) ◦ B + (h̃C̃ 0 ) ◦ B 0 U 0 )dU ).

tk,k =

0
U∗,k
U∗,k
,
2

iθ

x1 = e |x1 |, we have (Mezzadri, 2006)

U∗,j h̃j ),

(2)
tk,k C̃k +

2
0
U∗,k
H∗,k+1 ,
0 U
U∗,k
∗,k

k−1
X

tj,k C̃j =

j=1
n
X

B. Algorithm Explanation
Let U := (vi,j )1≤j≤m
1≤i≤n . Then the element of the matrix
T := striu(U 0 U ) + 12 diag(U 0 U ) can be expressed as

1 + δi,j

=

n
X

n
X


vj,k

j=k


vj,k

j=1

,

(4)

(5)
(6)

∂L
. Similarly to
Solving the triangular system T 0 C̃ = U 0 ∂C
the previous case, we have for 1 ≤ k ≤ m

=

vk,i vk,j

(3)

Equations (5) and (6) explain the lines
Pm 8 and 9 in Algorithm 1. Note that H∗,1 = h − j=1 U∗,j h̃j = h −
Pm
−1 0
U h]j = h − U T −1 U 0 h = W h. Hence,
j=1 U∗,j [T
when h = h(t−1) , we have H∗,1 = C (t) , which explains
line 16 in Algorithm 1.

Taking this fact into account, a similar argument to that
used in the proof of Theorem 1 can be used here.

k=j

vr,j h̃j ,

j=k+1

H∗,k = H∗,k+1 − h̃k U∗,k .

Sketch of the proof for Corollary 1. For any nonzero complex valued vector x ∈ Cn , if we chose u = x + eiθ kxk e1
uu∗
and H = −e−iθ (I − 2 kuk
2 ), where θ ∈ R is such that

ti,j = Ji ≤ jK

r
X

vr,k

we get

h̃k =

h =C − U C̃.

Pn

vr,k vr,j h̃j ,

j=k+1 r=j
n
X

where the passage from Equation (3) to P
(4) is justified ber
cause vr,j = 0 for j > r. Therefore, j=k+1 vr,j h̃j =
Pm
j=k+1 vr,j h̃j .
Pm
By setting H∗,k+1 := h − j=k+1 U∗,j h̃j , and noting that

Using lemma 1 we conclude that
h
i
U =U (h̃C̃ 0 ) ◦ B 0 + (C̃ h̃0 ) ◦ B − C h̃0 − hC̃ 0 ,

Hx = ||x||e1

n
X

j=k+1

+ Tr(((h̃C̃ 0 ) ◦ B 0 )U 0 dU ).

0

vj,k hj ,

r=k+1
m
X

0
= U∗,k
(h −

+ Tr(((C̃ h̃0 ) ◦ B)U 0 dU )

j=k
m
X

vj,k hj −

j=k
n
X

n
X


vr,k

r=1


0 
= U∗,k

∂L
∂C



∂L
∂C



−
j

∂L
∂C

k−1
n
XX


,
j

vr,j vr,k C̃j ,

(7)

j=1 r=k

−
r
k−1
X

n
X
r=1

vr,k

k−1
X
j=1



∂L
−
U∗,j C̃j  ,
∂C j=1

vr,j C̃j ,

(8)

Operation
FP

0
h̃k ← N2k U∗,k
H∗,k+1
H∗,k ← H∗,k+1 − h̃k U∗,k

Flop count
for iteration k
2(n − k) + 3
2n

BP

0
H∗,k+1
h̃k ← N2k U∗,k
g ← g − C̃k U∗,k
G∗,k ← −h̃k g − C̃k H∗,k+1

2(n − k) + 3
2(n − k + 1)
3n

Total Flop count
for m iteration
(4n − m + 2)m

(7n − 2m + 3)m

Table 1. Time complexities of different operations in algorithm 1. It is assumed that the matrix U ∈ Rn×m is defined as in Equation (9).

where the passage
(7) to (8) is justified by
Pn from Equation P
n
the fact that r=k vr,j vr,k C̃j =
r=1 vr,j vr,k C̃j (since
vr,k = 0 for r < k).
Pk−1
∂L
By setting g := ∂C
(t) −
j=1 U∗,j C̃j , we can write
2
0
C̃k = U 0 U∗,k U∗,k g which explains the lines 12 and
∗,k
13 in Algorithm 1. Note also that after m-iterations in
the backwardPpropagation loop in Algorithm 1, we have
m
∂L
∂L
∂L
g = ∂C
(t) −
j=1 U∗,j C̃j = ∂C (t) − U C̃ = ∂h(t−1) . This
explains line 17 of Algorithm 1.
Finally, note that from Equation (14), we have for 1 ≤ i ≤
n and 1 ≤ k ≤ m




∂L
∂L
=−
h̃k − hi C̃k +
∂U i,k
∂C i
m


X
vi,j ((h̃C̃ 0 ) ◦ B 0 )j,k + ((C̃ h̃0 ) ◦ B)j,k ,
j=1


=−
m
X

∂L
∂C


h̃k − hi C̃k +
i


vi,j

j=1

Jj ≤ kK
Jk ≤ jK
+ C̃j h̃k
h̃j C̃k
1 + δj,k
1 + δj,k


,


∂L
h̃k − hi C̃k +
=−
∂C i
m


X
vi,j h̃j C̃k Jk < jK + C̃j h̃k Jj ≤ kK ,


j=1


=C̃k 

m
X



k
X

Table 1 shows the flop count for different operations in the
local backward and forward propagation steps in Algorithm
1.

D. Matlab implementation of Algorithm 1
% Inputs: U - matrix of reflection vectors
%
h - hidden state at time-step t-1
%
BPg - Grad of loss w.r.t C=Wh
% Outputs: g, G, C=Wh
[n, m] = size(U);
G=zeros(n, m); H = zeros(n, m+1);
N = zeros(m); h_tilde = zeros(m);
% Zero-initialisation not required above!
H(:,m+1)=h; g=BPg;
%%--Forward propagation--%%
for k =0:m-1
N(m-k) = U(:, m-k)' * U(:, m-k);
h_tilde(m-k)=2 / N(m-k) * ...
U(:, m-k)' * H(:,m-k+1);
H(:,m-k)=H(:,m-k+1) - ...
h_tilde(m-k) * U(:,m-k);
end
C = H(:,1)
%%--Backward propagation--%%
for k=1:m
c_tilde_k = 2*U(:,k)' * g / N(k);
g = g - c_tilde_k * U(:,k);
G(:, k)=-h_tilde(k) * g - ...
c_tilde_k*H(:,k+1);
end

vi,j h̃j − hi 

j=k+1



C. Time complexity



∂L
+ h̃k 
vi,j C̃j −
∂C
j=1




.

i

Therefore, when C = C (t) and h = h(t−1) we have


∂L
= −C̃k H∗,k+1 − h̃k g,
∂U (t) ∗,k
Pk−1
∂L
where g = ∂C
(t) −
j=1 C̃j U∗,j . This explains lines 14
and 18 of Algorithm 1.

Figure 1. MATLAB code performing one-step FP and BP required to compute C (t) , ∂h∂L
(t−1) (variable g is the code), and
∂L
(variable G is the code). The required inputs for the FP and
∂U (t)
∂L
BP are, respectively, the tuples (U, h(t−1) ) and (U, C (t) , ∂C
(t) ).
∂L
Note that ∂C (t) is variable BPg in the Matlab code.

References
Giles, Mike B. An extended collection of matrix derivative
results for forward and reverse mode automatic differentiation. 2008.

Mezzadri, Francesco. How to generate random matrices
from the classical compact groups. arXiv preprint mathph/0609050, 2006.

