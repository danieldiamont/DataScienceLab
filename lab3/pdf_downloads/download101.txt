Lazifying Conditional Gradient Algorithms

Gábor Braun * 1 Sebastian Pokutta * 1 Daniel Zink * 1

Abstract
Conditional gradient algorithms (also often called
Frank-Wolfe algorithms) are popular due to their
simplicity of only requiring a linear optimization
oracle and more recently they also gained significant traction for online learning. While simple
in principle, in many cases the actual implementation of the linear optimization oracle is costly.
We show a general method to lazify various conditional gradient algorithms, which in actual computations leads to several orders of magnitude of
speedup in wall-clock time. This is achieved by
using a faster separation oracle instead of a linear
optimization oracle, relying only on few linear
optimization oracle calls.

1. Introduction
Convex optimization is an important technique both from
a theoretical and an applications perspective. Gradient descent based methods are widely used due to their simplicity
and easy applicability to many real-world problems. We are
interested in solving constraint convex optimization problems of the form
min f (x),
(1)
x∈P

where f is a smooth convex function and P is a polytope,
with access to f being limited to first-order information,
i.e., we can obtain ∇f (v) and f (v) for a given v ∈ P and
access to P via a linear minimization oracle which returns
x = argminv∈P cx for a given linear objective c.
When solving Problem (1) using gradient descent approaches in order to maintain feasibility, typically a projection step is required. This projection back into the feasible
region P is potentially computationally expensive, especially for complex feasible regions in very large dimensions.
As such projection-free methods gained a lot of attention
recently, in particular the Frank-Wolfe algorithm (Frank
1

ISyE, Georgia Institute of Technology, Atlanta, GA. Correspondence to: Daniel Zink <daniel.zink@gatech.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

Algorithm 1 Frank-Wolfe Algorithm (Frank & Wolfe,
1956)
Input: smooth convex f function with curvature C, x1 ∈
P start vertex, LPP linear minimization oracle
Output: xt points in P
1: for t = 1 to T − 1 do
2:
vt ← LPP (∇f (xt ))
2
3:
xt+1 ← (1 − γt )xt + γt vt with γt := t+2
4: end for

& Wolfe, 1956) (also known as conditional gradient descent (Levitin & Polyak, 1966); see also (Jaggi, 2013) for
an overview) and its online version (Hazan & Kale, 2012)
due to their simplicity. We recall the basic Frank-Wolfe
algorithm in Algorithm 1. These methods eschew the projection step and rather use a linear optimization oracle to
stay within the feasible region. While convergence rates
and regret bounds are often suboptimal, in many cases the
gain due to only having to solve a single linear optimization
problem over the feasible region in every iteration still leads
to significant computational advantages (see e.g., (Hazan
& Kale, 2012, Section 5)). This led to conditional gradients algorithms being used for e.g., online optimization
and more generally machine learning and the property that
these algorithms naturally generate sparse distributions over
the extreme points of the feasible region (sometimes also
refereed to as atoms) is often helpful. Further increasing
the relevance of these methods, it was shown recently that
conditional gradient methods can also achieve linear convergence (see e.g., (Garber & Hazan, 2013; Lacoste-Julien
& Jaggi, 2015; Garber & Meshi, 2016)) as well as that the
number of total gradient evaluations can be reduced while
maintaining the optimal number of oracle calls as shown in
(Lan & Zhou, 2014).
Oracle 1 Weak Separation Oracle LPsepP (c, x, Φ, K)
Input: c ∈ Rn linear objective, x ∈ P point, K ≥ 1
accuracy, Φ > 0 objective value;
Output: Either (1) y ∈ P vertex with c(x − y) > Φ/K, or
(2) false: c(x − z) ≤ Φ for all z ∈ P .
Unfortunately, for complex feasible regions even solving the
linear optimization problem might be time-consuming and
as such the cost of solving the LP might be non-negligible.

Lazifying Conditional Gradient Algorithms

This could be the case, e.g., when linear optimization over
the feasible region is a hard problem or when solving largescale optimization problems or learning problems. As such
it is natural to ask the following questions:
(i) Does the linear optimization oracle have to be called
in every iteration?
(ii) Does one need approximately optimal solutions for
convergence?
(iii) Can one reuse information across iteration?
We will answer these questions in this work, showing that (i)
the LP oracle is not required to be called in every iteration,
that (ii) much weaker guarantees are sufficient, and that (iii)
we can reuse information. To significantly reduce the cost
of oracle calls while maintaining identical convergence rates
up to small constant factors, we replace the linear optimization oracle by a (weak) separation oracle (see Oracle 1)
which approximately solves a certain separation problem
within a multiplicative factor and returns improving vertices
(or atoms). We stress that the weak separation oracle is
significantly weaker than approximate minimization, which
has been already considered in (Jaggi, 2013). In fact, if the
oracle returns an improving vertex then this vertex does not
imply any guarantee in terms of solution quality with respect
to the linear minimization problem. It is this relaxation of
the dual guarantees that will provide a significant speedup
as we will see later. At the same time, in case that the oracle
returns false, we directly obtain a dual bound via convexity.
A (weak) separation oracle can be realized by a single call
to a linear optimization oracle, however with two important
differences. It allows for caching and early termination:
Previous solutions are cached, and first it is verified whether
any of the cached solutions satisfy the oracle’s separation
condition. The underlying linear optimization oracle has to
be called, only when none of the cached solutions satisfy
the condition, and the linear optimization can be stopped as
soon as a satisfactory solution with respect to the separation
condition is found. See Algorithm 2 for pseudo-code; early
termination is implicit in line 4.
We call this technique lazy optimization and we will demonstrate significant speedups in wall-clock performance (see
e.g., Figure 1), while maintaining identical theoretical convergence rates.
To exemplify our approach we provide conditional gradient algorithms employing the weak separation oracle for
the standard Frank-Wolfe algorithm as well as the variants
in (Hazan & Kale, 2012; Garber & Meshi, 2016; Garber
& Hazan, 2013), which have been chosen due to requiring modified convergence arguments that go beyond those
required for the vanilla Frank-Wolfe algorithm. Complementing the theoretical analysis we report computational

Oracle 2 LPsepP (c, x, Φ, K) via LP oracle
Input: c ∈ Rn linear objective, x ∈ P point, K ≥ 1
accuracy, Φ > 0 objective value;
Output: Either (1) y ∈ P vertex with c(x − y) > Φ/K, or
(2) false: c(x − z) ≤ Φ for all z ∈ P .
1: if y ∈ P cached with c(x − y) > Φ/K exists then
2:
return y {Cache call}
3: else
4:
compute y ← argmaxx∈P c(x) {LP call}
5:
if c(x − y) > Φ/K then
6:
return y and add y to cache
7:
else
8:
return false
9:
end if
10: end if

results demonstrating effectiveness of our approach via a
significant reduction in wall-clock running time compared
to their linear optimization counterparts.
Related Work
There has been extensive work on Frank-Wolfe algorithms
and conditional gradient descent algorithms and we will be
only able to review work most closely related to ours. The
Frank-Wolfe algorithm was originally introduced in (Frank
& Wolfe, 1956) (also known as conditional gradient descent
(Levitin & Polyak, 1966) and has been intensely studied in
particular in terms of achieving stronger convergence guarantees as well as affine-invariant versions. We demonstrate
our approach for the vanilla Frank-Wolfe algorithm (Frank
& Wolfe, 1956) (see also (Jaggi, 2013)) as an introductory example. We then consider more complicated variants
that require non-trivial changes to the respective convergence proofs to demonstrate the versatility of our approach.
This includes the linearly convergent variant via local linear optimization (Garber & Hazan, 2013) as well as the
pairwise conditional gradient variant of (Garber & Meshi,
2016), which is especially efficient in terms of implementation. However, our technique also applies to the Away-Step
Frank-Wolfe algorithm, the Fully-Corrective Frank-Wolfe
algorithm, as well as the Block-Coordinate Frank-Wolfe algorithm. Recently, in (Freund & Grigas, 2016) guarantees
for arbitrary step-size rules were provided and an analogous analysis can be also performed for our approach. On
the other hand, the analysis of the inexact variants, e.g.,
with approximate linear minimization does not apply to our
case as our oracle is significantly weaker than approximate
minimization as pointed out earlier. For more information,
we refer the interested reader to the excellent overview in
(Jaggi, 2013) for Frank-Wolfe methods in general as well as
(Lacoste-Julien & Jaggi, 2015) for an overview with respect
to global linear convergence.

Lazifying Conditional Gradient Algorithms

It was also recently shown in (Hazan & Kale, 2012) that
the Frank-Wolfe algorithm can be adjusted to the online
learning setting and here we provide a lazy version of this
algorithm. Combinatorial convex online optimization has
been investigated in a long line of work (see e.g., (Kalai
& Vempala, 2005; Audibert et al., 2013; Neu & Bartók,
2013)). It is important to note that our regret bounds hold
in the structured online learning setting, i.e., our bounds
depend on the `1 -diameter or sparsity of the polytope, rather
than its ambient dimension for arbitrary convex functions
(see e.g., (Cohen & Hazan, 2015; Gupta et al., 2016)). We
refer the interested reader to (Hazan, 2016) for an extensive
overview.

(iii) Weak separation through augmentation. We show in
the case of 0/1 polytopes how to implement a weak separation oracle with at most k calls to an augmentation oracle
that on input c ∈ Rn and x ∈ P provides either an improving solution x ∈ P with cx < cx or ensures optimality,
where k denotes the `1 -diameter of P . This is useful when
the solution space is sparse.

A key component of the new oracle is the ability to cache
and reuse old solutions, which accounts for the majority
of the observed speed up. The idea of caching of oracle
calls was already explored in various other contexts such
as cutting plane methods (see e.g., (Joachims et al., 2009))
as well as the Block-Coordinate Frank-Wolfe algorithm in
(Shah et al., 2015; Osokin et al., 2016). Our lazification
approach (which uses caching) is different however in the
sense that our weak separation oracle does not resemble an
approximate linear optimization oracle with a multiplicative
approximation guarantee; see (Osokin et al., 2016, Proof of
Theorem 3. Appendix F) and (Lacoste-Julien et al., 2013)
for comparison to our setup. In fact, our weaker oracle does
not imply any approximation guarantee and differs from
approximate minimization as done e.g., in (Jaggi, 2013)
substantially.

It is important to note that in all cases, we inherit the same
requirements, assumptions, and properties of the baseline
algorithm that we lazify. This includes applicable function classes, norm requirements, as well as smoothness and
(strong) convexity requirements. We also maintain identical
convergence rates up to (small!) constant factors.

Contribution
The main technical contribution of this paper is a new approach, whereby instead of finding the optimal solution,
the oracle is used only to find a good enough solution or a
certificate that such a solution does not exist, both ensuring the desired convergence rate of the conditional gradient
algorithms.
Our contribution can be summarized as follows:

Garber & Meshi, 2016).

(iv) Computational experiments. We demonstrate computational superiority by extensive comparisons of the weak
separation based versions with their original versions. In
all cases we report significant speedups in wall-clock time
often of several orders of magnitude.

Outline
We briefly recall notation and notions in Section 2 and consider conditional gradients algorithms in Section 3. In Section 4 we explain how parameter-free variants of the proposed algorithms can be obtained. Finally, in Section 5
we provide some experimental results. In the supplemental
material we consider two more variants of conditional gradients algorithms (Sections B and C), we show that we can
realize a weak separation oracle with an even weaker oracle
in the case of combinatorial problem (Section D) and we
provide additional computational results (Section E).

2. Preliminaries
∗

(i) Lazifying approach. We provide a general method to
lazify conditional gradient algorithms. For this we replace
the linear optimization oracle with a weak separation oracle,
which allows us to reuse feasible solutions from previous
oracle calls, so that in many cases the oracle call can be
skipped. In fact, once a simple representation of the underlying feasible region is learned no further oracle calls are
needed. We also demonstrate how parameter-free variants
can be obtained.
(ii) Lazified conditional gradient algorithms. We exemplify our approach by providing lazy versions of the vanilla
Frank-Wolfe algorithm as well as of the conditional gradient
methods in (Hazan & Kale, 2012; Garber & Hazan, 2013;

Let k·k be an arbitrary norm on Rn , and let k·k denote
the dual norm of k·k. We will specify the applicable
norm in the later sections. A function f is L-Lipschitz
if |f (y) − f (x)| ≤ Lky − xk for all x, y ∈ dom f . A
convex function f is smooth with curvature at most C if
f (γy + (1 − γ)x) ≤ f (x) + γ∇f (x)(y − x) + Cγ 2 /2 for
all x, y ∈ dom f and 0 ≤ γ ≤ 1. A function f is S-strongly
2
convex if f (y) − f (x) ≥ ∇f (x)(y − x) + S2 ky − xk for
all x, y ∈ dom f . Unless stated otherwise Lipschitz continuity and strong convexity will be measured in the norm
k·k. Moreover, let Br (x) := {y | kx − yk ≤ r} be the ball
around x with radius r with respect to k.k. In the following,
P will denote the feasible region, a polytope and the vertices
of P will be denoted by v1 , . . . , vN .

Lazifying Conditional Gradient Algorithms

3. Lazy Conditional Gradients
We start with the most basic Frank-Wolfe algorithm as a
simple example how a conditional gradient algorithm can
be lazified by means of a weak separation oracle. We will
also use the basic variant to discuss various properties and
implications. We then show how the more complex FrankWolfe algorithms in (Garber & Hazan, 2013) and (Garber &
Meshi, 2016) can be lazified. Throughout this section k·k
denotes the `2 -norm.

guish two cases depending on the return value of the weak
separation oracle in Line 3.
When the oracle returns an improving solution vt , which we
call the positive case, then ∇f (xt )(xt −vt ) ≥ Φt /K, which
is used in the second inequality below. The first inequality
follows by smoothness of f , and the third inequality by the
induction hypothesis:
f (xt+1 ) − f (x∗ )
≤ f (xt ) − f (x∗ ) + γt ∇f (xt )(vt − xt ) +

3.1. Lazy Conditional Gradients: a basic example
We start with lazifying the original Frank-Wolfe algorithm
(arguably the simplest Conditional Gradients algorithm),
adapting the baseline argument from (Jaggi, 2013, Theorem 1). While the vanilla version has suboptimal convergence rate O(1/T ), its simplicity makes it an illustrative
example of the main idea of lazification. The lazy algorithm (Algorithm 2) maintains an upper bound Φt on the
convergence rate, guiding its eagerness for progress when
searching for an improving vertex vt . If the oracle provides
an improving vertex vt we refer to this as a positive call and
we call it a negative call otherwise.
Algorithm 2 Lazy Conditional Gradients (LCG)
Input: smooth convex f function with curvature C, x1 ∈
P start vertex, LPsepP weak linear separation oracle,
accuracy K > 1, initial upper bound Φ0
Output: xt points in P
1: for t = 1 to T − 1 do
2:
3:
4:
5:
6:
7:
8:
9:

Φ

+

Cγt2

2
Φt ← t−1
γ
1+ Kt
vt ← LPsepP (∇f (xt ), xt , Φt , K)
if vt = false then
xt+1 ← xt
else
xt+1 ← (1 − γt )xt + γt vt
end if
end for

Theorem 3.1. Assume f is convex and smooth with cur2(K 2 +1)
vature C. Then Algorithm 2 with γt = K(t+K
2 +2) has
convergence rate
f (xt ) − f (x∗ ) ≤

Φt
Cγt2
+
K
2
Φt
Cγt2
≤ Φt−1 − γt
+
= Φt ,
K
2

≤ f (xt ) − f (x∗ ) − γt

When the oracle returns no improving solution, then in particular ∇f (xt )(xt − x∗ ) ≤ Φt , hence by Line 5 f (xt+1 ) −
f (x∗ ) = f (xt ) − f (x∗ ) ≤ ∇f (xt )(xt − x∗ ) = Φt .
Finally, using the specific values of γt we prove the upper
bound
2 max{C, Φ0 }(K 2 + 1)
Φt−1 ≤
t + K2 + 2
by induction on t. The claim is obvious for t = 1. The induction step is an easy computation relying on the definition
of Φt on Line 2:
Cγ 2

Φt−1 + 2 t
Φt =
≤
1 + γKt

2

2 max{C, Φ0 }(K + 1)
,
t + K2 + 2

where x∗ is a minimum point of f over P .
Proof. We prove by induction that f (xt ) − f (x∗ ) ≤ Φt−1 .
The claim is clear for t = 1 by the choice of Φ0 . Assuming
the claim is true for t, we prove it for t + 1. We distin-

2 max{C,Φ0 }(K 2 +1)
t+K 2 +2

+

max{C,Φ0 }γt2
2

1 + γKt
γt
1 + 2K

= 2 max{C, Φ0 }(K 2 + 1)
1 + γKt (t + K 2 + 2)

≤

The step size γt is chosen to (approximately) minimize Φt
in Line 2; roughly Φt−1 /KC.

Cγt2
2

2 max{C, Φ0 }(K 2 + 1)
.
t + K2 + 3

Here the second equation follows via plugging-in the choice
for γt for one of the γt in the quadratic term and last inequality follows from t ≥ 1 and the concrete choice of
γt .
Remark 3.2 (Discussion of the weak separation oracle). A
few remarks are in order:
(i) Interpretation of weak separation oracle. The weak
separation oracle provides new extreme points (or vertices)
vt that ensure necessary progress to converge at the proposed
rate Φt or it certifies that we are already Φt -close to the
optimal solution. It is important to note that the two cases in
Oracle 1 are not mutually exclusive: the oracle might return
y ∈ P with c(x − y) > Φ/K (positive call: returning a
vertex y with improvement Φ/K), while still c(x − z) ≤ Φ
for all z ∈ P (negative call: certifying that there is no vertex

Lazifying Conditional Gradient Algorithms

z that can improve by Φ). This a desirable property as it
makes the separation problem much easier and the algorithm
works with either answer in the ambiguous case.
(ii) Choice of K. The K parameter can be used to bias
the oracle towards positive calls, i.e., returning improving
directions. We would also like to point out that the algorithm
above as well as those below will also work for K = 1,
however we show in supplemental material (Section D)
that we can use an even weaker oracle to realize a weak
separation oracle if K > 1 and for consistency, we require
K > 1 throughout. In the case K = 1 the two cases in the
oracle are mutually exclusive.
(iii) Effect of caching and early termination. When realizing the weak separation oracle, the actual linear optimization oracle has to be only called if none of the previously
seen vertices (or atoms) satisfies the separation condition.
Moreover, the weak separation oracle has to only produce
a satisfactory solution and not an approximately optimal
one. These two properties are responsible for the observed
speedup (see Figure 1). Moreover, the convex combinations
of vertices of P that represent the solutions xt are extremely
sparse as we reuse (cached) vertices whenever possible.
(iv) Dual certificates. By not computing an approximately
optimal solution, we give up dual optimality certificates. For
a given point x ∈ P , let g(x) := maxv∈P ∇f (x)(x − v)
denote the Wolfe gap. We have f (x) − f (x∗ ) ≤ g(x)
where x∗ = argminx∈P f (x) by convexity. In those
rounds t where we obtain an improving vertex we have
no information about g(xt ). However, if the oracle returns false in round t, then we obtain the dual certificate
f (xt ) − f (x∗ ) ≤ g(xt ) ≤ Φt .
(v) Rate of convergence. A close inspection of the algorithm utilizing the weak separation oracle suggests that the
algorithm converges only at the worst-case convergence rate
that we propose with the Φt sequence. This however is only
an artefact of the simplified presentation for the proof of
the worst-case rate. We can easily adjust the algorithm to
implicitly perform a search over the rate Φt combined with
line search for γ. This leads to a parameter-free variant of
Algorithm 2as given in Section 4 and comes at the expense
of a (small!) constant factor deterioration of the worst-case
rate guarantee; see also Supplementary Material A.(iii) for
an in-depth discussion.
We discuss potential implementation improvements in Supplementary Material A.
3.2. Lazy Pairwise Conditional Gradients
In this section we provide a lazy variant (Algorithm 3) of
the Pairwise Conditional Gradient algorithm from (Garber

& Meshi, 2016), using separation instead of linear optimization. We make identical assumptions: the feasible region
is a 0/1 polytope given in the form P = {x ∈ Rn | 0 ≤
x ≤ 1, Ax = b}, where 1 denotes the all-one vector of
compatible dimension; in particular all vertices of P have
only 0/1 entries.
Algorithm 3 Lazy Pairwise Conditional Gradients (LPCG)
Input: polytope P , smooth and S-strongly convex function
f with curvature C, accuracy K > 1, ηt non-increasing
step-sizes
Output: xt points
1: x1 ∈ P arbitrary and Φ0 ≥ f (x1 ) − f (x∗ )
2: for t = 1, . . . , T do
˜ (xt ) ∈ Rm as follows:
3:
define ∇f
(
˜ (xt )i := ∇f (xt )i if (xt )i > 0
∇f
−∞
if (xt )i = 0
4:
5:
6:

2Φ

+η 2 C

t−1
t
Φt ← 2+
ηt
K∆t


˜ (xt )
ct ← ∇f (xt ), −∇f


Φt
(vt+ , vt− ) ← LPsepP ×P ct , (xt , xt ), ∆
,
K
t

7:
if (vt+ , vt− ) = false then
8:
xt+1 ← xt
9:
else
10:
η̃t ← max{2−δ | δ ∈ Z≥0 , 2−δ ≤ ηt }
11:
xt+1 ← xt + η̃t (vt+ − vt− )
12:
end if
13: end for

Observe that Algorithm 3 calls LPsep on the cartesian product of P with itself. Choosing the objective function as
in Line 5 allows us to simultaneously find an improving
direction and an away-step direction.
Theorem 3.3. Let x∗ be a minimum point of f in P , and Φ0
∗
an
1 :=
q upper bound of f (x1 ) − f (x ). Furthermore, let M
√
M1
S
:= KC/2, κ := min{ 2M2 , 1/ Φ0 },
8 card(x∗ ) , M2
q
p
2 card(x∗ )Φt−1
ηt := κ Φt−1 and ∆t :=
, then AlgoS
rithm 3 has convergence rate

t
1+B
f (xt+1 ) − f (x∗ ) ≤ Φt ≤ Φ0
,
1 + 2B
where B := κ ·

M1
2K .

We recall a technical lemma for the proof.
Lemma 3.4 ((Garber & Meshi, 2016, Lemma 2)). Let
x, y ∈ P . There exists vertices vi of P
such that
P
 x =
Pk
Pk
k
λ
v
and
y
=
(λ
−
γ
)
v
+
γ
i
i
i
i=1 i i
i=1
i=1 i z with
p
Pk
γi ∈ [0, λi ], z ∈ P and i=1 γi ≤ card(y)kx − yk.

Lazifying Conditional Gradient Algorithms

Proof of Theorem 3.3. The feasibility of the iterates xt is
ensured by Line 10 and the monotonicity of the sequence
{ηt }t≥1 with the same argument as in (Garber & Meshi,
2016, Lemma 1 and Observation 2).
We first show by induction that f (xt+1 ) − f (x∗ ) ≤ Φt . For
t = 0 we have Φ0 ≥ f (x1 ) − f (x∗ ). Now assume the statement for some t ≥ 0. In the negative case (Line 8), we use
Φt
the guarantee of Oracle 1 to get ct ((xt , xt )−(z1 , z2 )) ≤ ∆
t
for all z1 , z2 ∈ P , which is equivalent to (as ct (xt , xt ) = 0)
˜ (xt )z2 − ∇f (xt )z1 ≤ Φt and therefore
∇f
∆t
∇f (xt )(z̃2 − z1 ) ≤

Φt
,
∆t

for all z̃2 , z1 ∈ P with supp(z̃2 ) ⊆ supp(xt ). We furPk
ther use Lemma 3.4 to write xt = i=1 λi vi and x∗ =
Pk
Pk
i=1 (λi − γi )vi +
i=1 γi z with γi ∈
q[0, λi ], z ∈ P and
p
Pk
2 card(x∗ )Φt−1
∗
∗
card(x )kxt − x k ≤
=
i=1 γi ≤
S
∆t , using the induction hypothesis and the strong convexity
in the second inequality. Then f (xt+1 ) − f (x∗ ) = f (xt ) −
Pk
f (x∗ ) ≤ ∇f (xt )(xt − x∗ ) = i=1 γi (vi − z) · ∇f (xt ) ≤
Φt , where we used Equation 3.2 for the last inequality.
For the positive case (Lines 10 and 11) we get, using first
smoothness of f , then ηt /2 < η˜t ≤ ηt and ∇f (xt )(vt+ −
vt− ) ≤ −Φt /(∆t K), and finally the definition of Φt :
f (xt+1 ) − f (x∗ ) = f (xt + η˜t (vt+ − vt− )) − f (x∗ )
≤ Φt−1 + η̃t ∇f (xt )(vt+ − vt− ) +
≤ Φt−1 −

η̃t2 C
2

Φt
η2 C
ηt
·
+ t = Φt .
2 ∆t K
2

Plugging in the values of ηt and ∆t to the definition of Φt
gives the desired bound.
1 + κ2 M2 /K
2Φt−1 + ηt2 C
= Φt−1
ηt
2 + K∆t
1 + κM1 /K
t

1+B
1+B
≤ Φ0
.
≤ Φt−1
1 + 2B
1 + 2B

Φt =

4. Parameter-free Conditional Gradients via
Weak Separation
We now provide a parameter-free variant of the Lazy FrankWolfe Algorithm. We stress that the worst-case convergence
rate is identical up to a small constant factor. Here we find a
tight initial bound Φ0 with a single extra LP call, which can
be also done approximately as long as Φ0 is a valid upper
bound. Alternatively, one can perform binary search via the
weak separation oracle as described earlier.
Note that the accuracy parameter K in Algorithm 4 is a
parameter of the oracle and not of the algorithm itself. We

Algorithm 4 Parameter-free Lazy Conditional Gradients
(LCG)
Input: smooth convex function f , x1 ∈ P start vertex,
LPsepP weak linear separation oracle, accuracy K > 1
Output: xt points in P
1: Φ0 ← maxx∈P ∇f (x1 )(x1 − x)/2 {Initial bound}
2: for t = 1 to T − 1 do
3:
vt ← LPsepP (∇f (xt ), xt , Φt−1 , K)
4:
if vt = false then
5:
xt+1 ← xt
{Update Φ}
6:
Φt ← Φt−1
2
7:
else
8:
γt ← argmin0≤γ≤1 f ((1 − γ)xt + γvt )
9:
xt+1 ← (1 − γt )xt + γt vt
{Update iterate}
10:
Φt ← Φt−1
11:
end if
12: end for

will show now that Algorithm 4 converges in the worst-case
at a rate identical to Algorithm 2 (up to a small constant
factor).
Theorem 4.1. Let f be a smooth convex function with curvature C. Algorithm 4 converges at a rate proportional to
1/t. In particular to achieve a bound f (xt ) − f (x∗ ) ≤ ε,
given an initial upper bound f (x1 ) − f (x∗ ) ≤ 2Φ0 , the
number of required steps is upper bounded by
t ≤ dlog Φ0 /εe + 1 + 4Kdlog Φ0 /KCe +

16K 2 C
.
ε

Proof. The main idea of the proof is that while negative
answers to oracle calls halve the dual upper bound 2Φt ,
positive oracle calls significantly decrease the function value
of the current point.
We analyze iteration t of the algorithm. If Oracle 1 in Line 3
returns a negative answer (i.e., false, case (2)), then this
guarantees ∇f (xt )(xt − x) ≤ Φt−1 for all x ∈ P , in
particular, using convexity, f (xt+1 ) − f (x∗ ) = f (xt ) −
f (x∗ ) ≤ ∇f (xt )(xt − x∗ ) ≤ Φt−1 = 2Φt .
If Oracle 1 returns a positive answer (case (1)), then we have
f (xt ) − f (xt+1 ) ≥ γt Φt−1 /K − (C/2)γt2 by smoothness
of f . By minimality of γt , therefore f (xt ) − f (xt+1 ) ≥
min0≤γ≤1 (γΦt−1 /K −(C/2)γ 2 ), which is Φ2t−1 /(2CK 2 )
if Φt−1 < KC, and Φt−1 /K − C/2 ≥ C2 if Φt−1 ≥ KC.
Now we bound the number t0 of consecutive positive oracle
calls immediately following an iteration t with a negative
oracle call. Note that the same argument bounds the number
of initial consecutive positive oracle calls with the choice
t = 0, as we only use f (xt+1 ) − f (x∗ ) ≤ 2Φt below.

Lazifying Conditional Gradient Algorithms

1.0

Note that Φt = Φt+1 = · · · = Φt+t0 . Therefore


2
t0 Φt 2
2CK

≥
t0 Φt−1 −
K

(f (xτ ) − f (xτ +1 ))

if Φt−1 < KC
C
2



if Φt−1 ≥ KC

0.0

2Φt
4KΦt
4KΦt
≤
= 4K.
=
C
2Φ
−
KC
2Φ
− 2
t
t − Φt

Φt
K

Thus iteration t is followed by at most 4K consecutive
positive oracle calls as long as Φt ≥ KC, and 4CK 2 /Φt <
2`+1 · 4K ones for 2−`−1 KC < Φt ≤ 2−` KC with ` ≥ 0.
Adding up the number of oracle calls gives the desired rate:
in addition to the positive oracle calls we also have at most
dlog(Φ0 /ε)e + 1 negative oracle calls, where log(·) is the
binary logarithm and ε is the (additive) accuracy. Thus after
a total of
dlog KC/εe

dlog Φ0 /εe+1+4Kdlog Φ0 /KCe+

0.4
0.2

,

which gives in the case Φt < KC that t0 ≤ 4CK 2 /Φt , and
in the case Φt ≥ KC that
t0 ≤

0.6

X
`=0

≤ dlog Φ0 /εe + 1 + 4Kdlog Φ0 /KCe +

2`+1 ·4K
16K 2 C
ε

iterations (or equivalently oracle calls) we have f (xt ) −
f (x∗ ) ≤ ε.
Remark 4.2. Observe that Algorithm 4 might converge
much faster due to the aggressive halving of the rate. In
fact, Algorithm 4 convergences at a rate that is at most a
factor 4K 2 slower than the rate that the vanilla (non-lazy)
Frank-Wolfe algorithm would realize for the same problem.
In actual wall-clock time Algorithm 4 is much faster though
due to the use of the weaker oracle; see Figure 2 and 4
for a comparison and Section E.1.2 for more experimental
results.
Negative oracle calls tend to be significantly more expensive time-wise than positive oracle calls due to proving dual
bounds. The following corollary is an immediate consequence of the argumentation from above:
Corollary 4.3. Algorithm 4 makes at most dlog Φ0 /εe + 1
negative oracle calls.
If line search is too expensive we can choose γt =
min(1, Φt /KC) in Algorithm 4. In this case an estimate of
the curvature C is required, though no explicit knowledge
of the sequence Φt is needed as compared to the textbook
variant in Section 3.1.

LOCG with cache
LOCG without cache
OCG

0.8
Loss

τ =t+1

0.8
Loss

2Φt ≥ f (xt+1 ) − f (x∗ ) ≥

0
t+t
X

1.0
LOCG with cache
LOCG without cache
OCG

0.6
0.4
0.2

0

2000 4000 6000 8000
Wall-clock time

0.0

0

2000 4000 6000 8000
Oracle time

Figure 1. Performance gain due to caching and early termination
for stochastic optimization over a maximum cut problem with
linear losses. The red line is the OCG baseline, the green one is
the lazy variant using only early termination, and the blue one
uses caching and early termination. Left: loss vs. wall-clock time.
Right: loss vs. total time spent in oracle calls. Time limit was
7200 seconds. Caching allows for a significant improvement in
loss reduction in wall-clock time. The effect is even more obvious
in oracle time as caching cuts out a large number of oracle calls.

5. Experiments
As mentioned before, lazy algorithms have two improvements: caching and early termination. Here we depict the
effect of caching in Figure 1, comparing OCG (no caching,
no early termination), LOCG (caching and early termination) and LOCG (only early termination) (see Algorithm 7).
We did not include a caching-only OCG variant, because
caching without early termination does not make much
sense: in each iteration a new linear optimization problem
has to be solved; previous solutions can hardly be reused as
they are unlikely to be optimal for the new linear optimization problem.
5.1. Effect of K
If the parameter K of the oracle can be chosen, which
depends on the actual oracle implementation, then we can
increase K to bias the algorithm towards performing more
positive calls. At the same time the steps get shorter. As
such there is a natural trade-off between the cost of many
positive calls vs. a negative call. We depict the impact of
the parameter choice for K in Figure 6.

100 200 300
LP calls

LPCG
PCG

LPCG
PCG

0

Dual bound

10 4

10 3

300 600 900 1200
Wall-clock time (s)

0

300 600 900 1200
Wall-clock time (s)

Figure 3. Performance on a large instance of the video colocalization problem using PCG and its lazy variant. We observe that
lazy PCG is significantly better both in terms of function value and
dual bound. Recall that the function value is normalized between
[0, 1].

107
106
105

Function value

2000 4000 6000 8000
Wall-clock time (s)

0

LOCG
OCG

0

300
250
200
150
100
50
0

8000 16000 24000
Iterations

LOCG

0

LPCG
PCG

1.0
0.8
0.6
0.4
0.2
0.0
0.0

LPCG
PCG

0.5

1.0 1.5 2.0
Iterations 1e6

LPCG

15
10
5
0

8000 16000 24000
Iterations

1000 2000 3000 4000
Wall-clock time (s)

0

2

4
6
8
Iterations 1e5

Figure 5. Performance of the two lazified variants LOCG (left
column) and LPCG (right column). The feasible regions are a
cut polytope on the left and the MIPLIB instance air04 on the
right. The objective functions are in both cases quadratic, on the
left randomly chosen in every step. We show the performance
over wall clock time in seconds (first row) and over iterations
(second row). The last row shows the number of call to the linear
optimization oracle. The lazified versions perform significantly
better in wall clock time compared to the non-lazy counterparts.

108
LCG
CG

0

150
300
450
Wall clock time (s)

Function value

Function value

108

1.0
0.8
0.6
0.4
0.2
0.0

1.0
0.8
0.6
0.4
0.2
0.0

LCG
CG

102

102
LCG K=1
LCG K=1.5
LCG K=5
LCG K=10
LCG K=50
LCG K=100

107
106
105

0

10
20
LP calls

30

101

Figure 4. Performance on a matrix completion instance. More
information about this problem can be found in the supplemental
material (Section E). The performance is reported as the objective
function value over wall-clock time in seconds on the left and over
LP calls on the right. In both measures after an initial phase the
function value using LCG is much lower than with the non-lazy
algorithm.

0

150
300
450
Wall clock time (s)

LCG K=1
LCG K=1.5
LCG K=5
LCG K=10
LCG K=50
LCG K=100

Function value

Function value

10 5

1.0
0.8
0.6
0.4
0.2
0.0

0

400

Figure 2. Performance on an instance of the video colocalization
problem. We solve quadratic minimization over a flow polytope
and report the achieved dual bound (or Wolfe-gap) over wall-clock
time in seconds in logscale on the left and over the number of actual
LP calls on the right. We used the parameter-free variant of the
Lazy CG algorithm, which performs in both measures significantly
better than the non-lazy counterpart. The performance difference
is more prominent in the number of LP calls.

LOCG
OCG

Function value

Loss
0

1.0
0.8
0.6
0.4
0.2
0.0

Number of LP calls

150 300 450 600
Wall clock time (s)

LCG
CG

Loss

0

1015
1014
1013
1012
1011
1010
109
108
107
106

Number of LP calls

LCG
CG

Function value

1015
1014
1013
1012
1011
1010
109
108
107
106

Dual bound

Dual bound

Lazifying Conditional Gradient Algorithms

101

0

200 400 600
Iterations

Figure 6. Impact of the oracle approximation parameter K depicted for the Lazy CG algorithm. We can see that increasing
K leads to a deterioration of progress in iterations but improves
performance in wall-clock time. The behavior is similar for other
algorithms.

Lazifying Conditional Gradient Algorithms

Acknowledgements
We are indebted to Alexandre D’Aspremont, Simon LacosteJulien, and George Lan for the helpful discussions and for
providing us with relevant references. Research reported in
this paper was partially supported by NSF CAREER award
CMMI-1452463.

References
Achterberg, Tobias, Koch, Thorsten, and Martin,
Alexander. MIPLIB 2003. Operations Research
Letters, 34(4):361–372, 2006.
doi: 10.1016/
j.orl.2005.07.009.
URL http://www.zib.de/
Publications/abstracts/ZR-05-28/.
Audibert, Jean-Yves, Bubeck, Sébastien, and Lugosi, Gábor.
Regret in online combinatorial optimization. Mathematics of Operations Research, 39(1):31–45, 2013.
Bodic, Pierre Le, Pavelka, Jeffrey W, Pfetsch, Marc E, and
Pokutta, Sebastian. Solving MIPs via scaling-based augmentation. arXiv preprint arXiv:1509.03206, 2015.
Cohen, Alon and Hazan, Tamir. Following the perturbed
leader for online structured learning. In Proceedings of
the 32nd International Conference on Machine Learning
(ICML-15), pp. 1034–1042, 2015.
Dash, Sanjeeb. A note on QUBO instances defined on
Chimera graphs. preprint arXiv:1306.1202, 2013.
Frank, András and Tardos, Éva. An application of simultaneous Diophantine approximation in combinatorial optimization. Combinatorica, 7(1):49–65, 1987.
Frank, Marguerite and Wolfe, Philip. An algorithm for
quadratic programming. Naval research logistics quarterly, 3(1-2):95–110, 1956.
Freund, Robert M. and Grigas, Paul. New analysis and
results for the frank–wolfe method. Mathematical Programming, 155(1):199–230, 2016. ISSN 1436-4646. doi:
10.1007/s10107-014-0841-6. URL http://dx.doi.
org/10.1007/s10107-014-0841-6.
Garber, Dan and Hazan, Elad. A linearly convergent conditional gradient algorithm with applications to online and
stochastic optimization. arXiv preprint arXiv:1301.4666,
2013.
Garber, Dan and Meshi, Ofer. Linear-memory and
decomposition-invariant linearly convergent conditional
gradient algorithm for structured polytopes. arXiv
preprint, arXiv:1605.06492v1, May 2016.
Grötschel, Martin and Lovász, Lászlo. Combinatorial optimization: A survey, 1993.

Gupta, Swati, Goemans, Michel, and Jaillet, Patrick. Solving combinatorial games using products, projections
and lexicographically optimal bases. arXiv preprint
arXiv:1603.00522, 2016.
Gurobi Optimization. Gurobi optimizer reference manual version 6.5, 2016. URL https://www.gurobi.
com/documentation/6.5/refman/.
Hazan, Elad. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3–
4):157–325, 2016. doi: 10.1561/2400000013. URL
http://ocobook.cs.princeton.edu/.
Hazan, Elad and Kale, Satyen. Projection-free online learning. arXiv preprint arXiv:1206.4657, 2012.
Jaggi, Martin. Revisiting Frank–Wolfe: Projection-free
sparse convex optimization. In Proceedings of the 30th
International Conference on Machine Learning (ICML13), pp. 427–435, 2013.
Joachims, Thorsten, Finley, Thomas, and Yu, ChunNam John. Cutting-plane training of structural svms.
Machine Learning, 77(1):27–59, 2009.
Joulin, Armand, Tang, Kevin, and Fei-Fei, Li. Efficient
image and video co-localization with frank-wolfe algorithm. In European Conference on Computer Vision, pp.
253–268. Springer, 2014.
Kalai, Adam and Vempala, Santosh. Efficient algorithms
for online decision problems. Journal of Computer and
System Sciences, 71(3):291–307, 2005.
Koch, Thorsten, Achterberg, Tobias, Andersen, Erling,
Bastert, Oliver, Berthold, Timo, Bixby, Robert E., Danna,
Emilie, Gamrath, Gerald, Gleixner, Ambros M., Heinz,
Stefan, Lodi, Andrea, Mittelmann, Hans, Ralphs, Ted,
Salvagnin, Domenico, Steffy, Daniel E., and Wolter, Kati.
MIPLIB 2010. Mathematical Programming Computation, 3(2):103–163, 2011. doi: 10.1007/s12532-0110025-9. URL http://mpc.zib.de/index.php/
MPC/article/view/56/28.
Lacoste-Julien, Simon and Jaggi, Martin. On the global
linear convergence of Frank–Wolfe optimization
variants. In Cortes, C., Lawrence, N. D., Lee, D. D.,
Sugiyama, M., and Garnett, R. (eds.), Advances in
Neural Information Processing Systems, volume 28, pp.
496–504. Curran Associates, Inc., 2015. URL http:
//papers.nips.cc/paper/5925-on-theglobal-linear-convergence-of-frankwolfe-optimization-variants.pdf.
Lacoste-Julien, Simon, Jaggi, Martin, Schmidt, Mark, and
Pletscher, Patrick. Block-coordinate frank-wolfe optimization for structural svms. In ICML 2013 International
Conference on Machine Learning, pp. 53–61, 2013.

Lazifying Conditional Gradient Algorithms

Lan, Guanghui and Zhou, Yi. Conditional gradient sliding
for convex optimization. Optimization-Online preprint
(4605), 2014.
Levitin, Evgeny S and Polyak, Boris T. Constrained minimization methods. USSR Computational mathematics
and mathematical physics, 6(5):1–50, 1966.
Neu, Gergely and Bartók, Gábor. An efficient algorithm
for learning with semi-bandit feedback. In Algorithmic
Learning Theory, pp. 234–248. Springer, 2013.
Oertel, Timm, Wagner, Christian, and Weismantel, Robert.
Integer convex minimization by mixed integer linear optimization. Oper. Res. Lett., 42(6-7):424–428, 2014.
Osokin, Anton, Alayrac, Jean-Baptiste, Lukasewitz, Isabella, Dokania, Puneet K, and Lacoste-Julien, Simon.
Minding the gaps for block frank-wolfe optimization of
structured svms. ICML 2016 International Conference
on Machine Learning / arXiv preprint arXiv:1605.09346,
2016.
Schulz, Andreas S and Weismantel, Robert. The complexity
of generic primal algorithms for solving general integer
programs. Mathematics of Operations Research, 27(4):
681–692, 2002.
Schulz, Andreas S., Weismantel, Robert, and Ziegler,
Günter M. 0/1-integer programming: Optimization and
augmentation are equivalent. In Algorithms – ESA ’95,
Proceedings, pp. 473–483, 1995.
Shah, Neel, Kolmogorov, Vladimir, and Lampert,
Christoph H. A multi-plane block-coordinate frank-wolfe
algorithm for training structural svms with a costly maxoracle. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2737–2745,
2015.

Lazifying Conditional Gradient Algorithms

A. Implementation Improvements
Note that there are various obvious improvements to Algorithm 2 for actual implementations. These improvements do
not affect the theoretical (worst-case) performance and for the sake of clarity of the exposition we did not include them in
Algorithm 2.
(i) First of all, we can improve the update of Φt , using the actual gap closed, rather than the pessimistic update via the lower
bound on gap closed, i.e., we can update Φt ← Φt − (f (xt ) − f (xt+1 )), whenever we calculated a new point xt+1 .
(ii) Moreover, we can better utilize information from negative oracle calls (i.e., when the oracle returns false): if the oracle
utilizes linear optimization at its core, then a negative oracle call will certify ∇f (xt )(xt − v) ≤ Φt via maximizing ∇f (xt )v
with v ∈ P , i.e., the linear optimization oracle computes g(xt ) and we can reset Φt ← g(xt ). If v ∗ realizes the Wolfe
gap, which is obtained as a byproduct of the above linear maximization, we can further use v ∗ to make a step: rather than
executing line 5, we can execute line 7 with vt = v ∗ . By doing so we maximize the use of information obtained from a
negative oracle call.
(iii) Finally, we can optimize the management of Φt . To obtain a better upper bound Φ0 , we can solve v ∗ :=
argmaxv∈P ∇f (x1 )v at the expense of one extra LP call and set Φ0 := ∇f (x1 )(x1 − v ∗ ) = g(x1 ). Alternatively,
we can perform binary search over Φ0 until the weak separation oracle produces an actual step. If Φ̄ is the value of the
search for which we observe the first step, we can reset Φ0 := 2Φ̄ and we have f (x1 ) − f (x∗ ) ≤ g(x1 ) ≤ 2Φ̄.
Furthermore, we can change the strategy for managing Φt as follows: we keep the value of Φt fixed in line 2 and perform
line search for γ. Whenever, we observe a negative oracle call, we set the current Φt to 12 g(xt ) obtained from the negative
call. As such, we ensure Φt < g(xt ) ≤ 2Φt , which biases the algorithm towards (much cheaper) positive calls. Convergence
is ensured by observing that an LPsepP (·, ·, Φ/2, K) oracle is an LPsepP (·, ·, Φ, 2K) oracle for which the theorem directly
applies. With this strategy we maintain the same theoretical (worst-case) convergence up to a constant factor, however in
case a faster convergence is possible, we adapt to that rate. The parameter-free version in Section 4 utilizes this technique.

B. Lazy Local Conditional Gradients
In this section we provide a lazy version (Algorithm 5) of the conditional gradient algorithm from (Garber & Hazan, 2013).
Let P ⊆ Rn be any polytope, D denote an upper bound on the `2 -diameter of P , and µ ≥ 1 be the affine invariant of
P from (Garber & Hazan, 2013). As the algorithm is not affine invariant by nature, we need a non-invariant version of
smoothness: Recall that a convex function f is β-smooth if
2

f (y) − f (x) ≤ ∇f (x)(y − x) + βky − xk /2.
Algorithm 5 Lazy Local Conditional Gradients (LLCG)
Input: feasible polytope P , β-smooth and S-strongly convex function f , parameters K, S, β, µ; diameter D
Output: xt points
1: x1 ∈ P arbitrary and Φ0 ≥ f (x1 ) − f (x∗ )
S
2: α ← 2Kβnµ
2
3: for t = 1, . . . , T do
4:
5:
6:
7:
8:
9:
10:
11:
12:

Φt ←

2
2 2
2
Φt−1 + β
2 α min{nµ rt ,D }
1+α/K

q
rt ← 2ΦSt−1
pt ← LLPsepP (∇f (xt ), xt , rt , Φt , K)
if pt = false then
xt+1 ← xt
else
xt+1 ← xt + α(pt − xt )
end if
end for

As an intermediary step, we first implement a local weak separation oracle in Algorithm 6, a local version of Oracle 1,
analogously to the local linear optimization oracle in (Garber & Hazan, 2013). To this end, we recall a technical lemma
from (Garber & Hazan, 2013).

Lazifying Conditional Gradient Algorithms

Algorithm 6 Weak Local Separation LLPsepP (c, x, r, Φ, K)
Input: c ∈ Rn linear objective, x ∈ P point,
√ r > 0 radius, Φ > 0 objective value
Output: Eithern(1) y ∈ P
with
kx
−
yk
≤
nµr and c(x − y) > Φ/K, or (2) false: c(x − z) ≤ Φ for all z ∈ P ∩ Br (x).
o
√

1: ∆ ← min

nµ
D r, 1

PM

P

2: Decompose x: x = j=1 λj vj , λj > 0, j λj = 1.
3: Sort vertices: i1 , . . . , iM
cvi1 ≥ · · · ≥ cviM .
Pk
4: k ← min{k : j=1 λij ≥ ∆}



Pk−1
λij vij + ∆ − j=1 λij vik

Φ
v ∗ ← LPsepP c, p∆− , ∆
if v ∗ = false then
return false
else
return y ← x − p− + ∆v ∗
end if

5: p− ←

6:
7:
8:
9:
10:
11:

Pk−1
j=1

n
Lemma B.1.
PN (Garber & Hazan, 2013, Lemma 7) Let P ⊆ R be a polytope and v1 , . . . , vN be its vertices. Let x, y ∈ P
and x = i=1 λi vi a convex combination of the vertices of P . Then there are numbers 0 ≤ γi ≤ λi and z ∈ P satisfying


y−x=−
X
i∈[N ]

γi ≤

X


X

γ i vi + 

i∈[N ]

γi  z

i∈[N ]

√

nµ
kx − yk.
D

Now we prove the correctness of the weak local separation algorithm.
Lemma B.2. Algorithm 6 is correct. In particular LLPsepP (c, x, r, Φ, K)
(i) returns either an y ∈ P with kx − yk ≤

√

nµr and c(x − y) > Φ/K,

(ii) or establishes c(x − z) ≤ Φ for all z ∈ P ∩ Br (x).
Proof. We first consider the case when the algorithm exits in Line 10. Observe that y ∈ P since y is a convex combination
√
PM
PM
of vertices of P . Moreover by construction of y we can write y = j=1 (λij − γj )vij + ∆v ∗ with ∆ = j=1 γj ≤ Dnµ r.
Therefore


X
 X
M
M

∗

kx − yk = 
γj vij − ∆v  ≤
γj kvij − v ∗ k
 j=1
 j=1
√
≤ nµr.
Finally using the guarantee of LPsepP we get
c(x − y) = ∆c


Φ
− v∗ ≥ .
∆
K

p

−

If the algorithm exits in Line 8, we use Lemma B.1 to decompose any y ∈ P ∩ Br (x) in the following way:
N
X
y=
(λi − γi )vi +
i=1

N
X
i=1

!
γi

z,

Lazifying Conditional Gradient Algorithms

PN
with z ∈ P and i=1 γi ≤
PN −
i=1 ηi = ∆. Let

√

nµ
D kx

− yk ≤ ∆. Since

p̃− :=

N
X

PN

i=1

λi = 1 ≥ ∆, there are numbers γi ≤ ηi− ≤ λi with

ηi− vi ,

i=1

p̃+ := y − x + p̃− =

N
N
X
X
(ηi− − γi )vi +
γi z,
i=1

i=1

so that p̃+ /∆ ∈ P . To bound the function value we first observe that the choice of p− in the algorithm assures that cu ≤ cp−
PN
PN
for all u = i=1 ηi vi with i=1 ηi = ∆ and all ηi ≥ 0. In particular, cp̃− ≤ cp− . The function value of the positive part
p̃+ can be bounded with the guarantee of LPsepP :


p−
p̃+
Φ
c
−
≤ ,
∆
∆
∆
i.e., c(p− − p̃+ ) ≤ Φ. Finally combining these bounds gives
c(x − y) = c (p̃− − p̃+ ) ≤ c(p− − p̃+ ) ≤ Φ
as desired.
We are ready to examine the Conditional Gradient Algorithm based on LLPsepP :
Theorem B.3. Algorithm 5 converges with the following rate:
f (xt+1 ) − f (x∗ ) ≤ Φt ≤ Φ0



1 + α/(2K)
1 + α/K

t
.

Proof. The proof is similar to the proof of Theorem 3.3. We prove this rate by induction. For t = 0 the choice of Φ0
guarantees that f (x1 ) − f (x∗ ) ≤ Φ0 . Now assume the theorem holds for t ≥ 0. With strong convexity and the induction
hypothesis we get
2
2
2
kxt − x∗ k ≤ (f (xt ) − f (x∗ )) ≤ Φt−1 = rt2 ,
S
S
i.e., x∗ ∈ P ∩ Brt (xt ). In the negative case, i.e., when pt = false, then case (ii) of Lemma B.2 applies:
f (xt+1 ) − f (x∗ ) = f (xt ) − f (x∗ ) ≤ ∇f (xt )(xt − x∗ ) ≤ Φt .
In the positive case, i.e., when Line 10 is executed, we get the same inequality via:
f (xt+1 ) − f (x∗ ) ≤ Φt−1 + α∇f (xt )(pt − xt ) +

β 2
Φt
β
2
α kx − pt k ≤ Φt−1 − α
+ α2 min{nµ2 rt2 , D2 } = Φt .
2
K
2

Therefore using the definition of α and rt we get the desired bound:



t
Φt−1 + β2 α2 rt2 nµ2
1 + α/(2K)
1 + α/(2K)
Φt ≤
= Φt−1
≤ Φ0
.
1 + α/K
1 + α/K
1 + α/K

C. Lazy Online Conditional Gradients
In this section we lazify the online conditional gradient algorithm of (Hazan & Kale, 2012) over arbitrary polytopes
P = {x ∈ Rn | Ax ≤ b}, resulting in Algorithm 7. We slightly improve constant factors by replacing (Hazan & Kale, 2012,
Lemma 3.1) with a better estimation via solving a quadratic inequality arising from strong convexity. In this section the
norm k·k can be arbitrary.

Theorem C.1. Let 0 ≤ b, s < 1. Let K > 1 be an accuracy parameter. Assume ft is L-Lipschitz, and smooth with
curvature at most Ct−b . Let D := maxy1 ,y2 ∈P ky1 − y2 k denote the diameter of P in norm k·k. Then the following hold for
PT
the points xt computed by Algorithm 7 where x∗T is the minimizer of t=1 ft :

Lazifying Conditional Gradient Algorithms

Algorithm 7 Lazy Online Conditional Gradients (LOCG)
Input: ft functions, x1 ∈ P start vertex, LPsepP weak linear separation oracle, parameters K, C, b, S, s; diameter D
Output: xt points
1: for t = 1 to T − 1 do
2:
∇t ← ∇ft (xt )
3:
if t = 1 then
∗
∗2
4:
h1 ← min{k∇1 k D, 2 k∇1 k /S}
5:
else
r


∗2
k∇t k∗ 2
tk
+
Φ
6:
Kt ← k∇
1−s
1−s
t−1
2St
2St
o
n
∗2
∗
tk
+
2K
7:
ht ← Φt−1 + min k∇t k D, k∇
1−s
t
St
8:
end if
1−b 2
9:

Φt ←

Ct
γt
2(1−b)
γt
1+ K

ht +

Pt

10:
vt ← LPsepP ( i=1 ∇fi (xt ), xt , Φt , K)
11:
if vt = false then
12:
xt+1 ← xt
13:
else
14:
xt+1 ← (1 −
Pγtt )xt + γt vt Pt
15:
Φt ← ht − i=1 fi (xt ) + i=1 fi (xt+1 )
16:
end if
17: end for

(i) With the choice
γt = t−(1−b)/2 ,
the xt satisfy
T
1X
(ft (xT ) − ft (x∗T )) ≤ AT −(1−b)/2 ,
T t=1

where
A :=

CK
+ L(K + 1)D.
2(1 − b)

(ii) Moreover, if all the ft are St−s -strongly convex, then with the choice
γt = t(b+s−2)/3 ,
the xt satisfy
T
1X
(ft (xT ) − ft (x∗T )) ≤ AT −(2(1+b)−s)/3 ,
T t=1

where

(2)



L2
CK
A := 2 (K + 1)(K + 2)
+
.
S
2(1 − b)

PT
Proof. We prove only Claim (ii), as the proof of Claim (i) is similar and simpler. Let FT := t=1 ft . Furthermore, let
hT := AT 1−(2(1+b)−s)/3 be T times the right-hand side of Equation (2). In particular, FT is ST -strongly convex, and
smooth with curvature at most CFT where
T

CFT :=

X
CT 1−b
≥C
t−b ,
1−b
t=1

ST := ST 1−s ≤ S

T
X
t=1

t−s .

Lazifying Conditional Gradient Algorithms

We prove Ft (xt ) − Ft (x∗t ) ≤ ht ≤ ht by induction on t. The case t = 1 is clear. Let Φt denote the value of Φt in Line 9,
while we reserve Φt to denote its value as used in Line 7. We start by showing Ft (xt+1 ) − Ft (x∗t ) ≤ Φt ≤ Φt . We
distinguish two cases depending on vt from Line 10. If vt is false, then Φt = Φt and the weak separation oracle asserts
maxy∈P ∇Ft (xt )(xt − y) ≤ Φt , which combined with the convexity of Ft provides
Ft (xt+1 ) − Ft (x∗t ) = Ft (xt ) − Ft (x∗t ) ≤ ∇Ft (xt )(xt − xt∗ ) ≤ Φt = Φt .
Otherwise vt is a vertex of P , then Line 15 and the induction hypothesis provides Ft (xt+1 ) − Ft (x∗t ) ≤ ht + Ft (xt+1 ) −
Ft (xt ) = Φt . To prove Φt ≤ Φt , we apply the smoothness of Ft followed by the inequality provided by the choice of vt :
Ft (xt+1 ) − Ft (xt ) −

CFt γt2
γ t Φt
≤ ∇Ft (xt )(xt+1 − xt ) = γt ∇Ft (xt )(vt − xt ) ≤ −
.
2
K

Rearranging provides the inequality:
Φt = ht + Ft (xt+1 ) − Ft (xt ) ≤ ht −

CFt γt2
γ t Φt
+
= Φt .
K
2

For later use, we bound the difference between ht and Φt using the value of parameters, ht ≤ ht , and γt ≤ 1:
CFt γt2
2
γt
K

ht +
ht − Φt ≥ ht −
1+

=

ht γt
K

−
1+

CFt γt2
2
γt
K

≥

ht γt
K

−
1+

CFt γt2
2
1
K

=

A−

CK
2(1−b) [2s−(1+b)]/3

t

K +1

.

We now apply Ft (xt+1 ) − Ft (x∗t ) ≤ Φt , together with convexity of ft+1 , and the minimality Ft (x∗t ) ≤ Ft (x∗t+1 ) of x∗t ,
followed by strong convexity of Ft+1 :
Ft+1 (xt+1 ) − Ft+1 (x∗t+1 ) ≤ (Ft (xt+1 ) − Ft (x∗t )) + (ft+1 (xt+1 ) − ft+1 (x∗t+1 ))
∗

(3)

x∗t+1 k

≤ Φt + k∇t+1 k · kxt+1 −
s
2
∗
≤ Φt + k∇t+1 k
(Ft+1 (xt+1 ) − Ft+1 (x∗t+1 )).
St+1

Solving the quadratic inequality provides
∗2

Ft+1 (xt+1 ) −

Ft+1 (x∗t+1 )

k∇t+1 k
≤ Φt +
St+1

v
u
u k∇t+1 k∗ 2
+ 2t
2St+1

∗2

k∇t+1 k
2St+1

!
+ Φt .

(4)

∗

From Equation (3), ignoring the last line, we also obtain Ft+1 (xt+1 ) − Ft+1 (x∗t+1 ) ≤ Φt + k∇t+1 k D via the estimate
kxt+1 − x∗t+1 k ≤ D. Thus Ft+1 (xt+1 ) − Ft+1 (x∗t+1 ) ≤ ht+1 , by Line 7, as claimed.
∗

Now we estimate the right-hand side of Equation (4) by using the actual value of the parameters, the estimate k∇t+1 k ≤ L,
and the inequality s + b ≤ 2. In fact, we estimate a proxy for the right-hand side. Note that A was chosen to satisfy the
second inequality:
s
r
L2
L2
L2
L2
+2
ht ≤ 1−s + 2
ht
St+1
2St+1
St
2St1−s
r
L2 [2s−(1+b)]/3
L2
h
≤
t
+2
1−s t
S
! 2St
r
L2
L2
=
+ 2 A t[2s−(1+b)]/3
S
S
≤

A−

CK
2(1−b) [2s−(1+b)]/3

t
K +1
≤ ht − Φt ≤ ht − Φt .

Lazifying Conditional Gradient Algorithms

In particular,

L2
2St+1

+ Φt ≤ ht hence combining with Equation (4) we obtain
ht+1

L2
+2
≤ Φt +
St+1

s

L2
2St+1

L2
≤ Φt +
+2
St+1

s

L2
ht
2St+1



L2
+ Φt
2St+1



≤ ht ≤ ht+1 .
C.1. Stochastic and Adversarial Versions
Complementing the offline algorithms from Section 3, we will now derive various versions for the online case. The presented
cases here are similar to those in (Hazan & Kale, 2012) and thus we state them without proof.
For stochastic cost functions ft , we obtain bounds from Theorem C.1 (i) similar to (Hazan & Kale, 2012, Theorems 4.1
and 4.3) (with δ replaced by δ/T in the bound to
pcorrect an inaccuracy in the√original argument). The proof is analogous
and hence omitted, but note that ky1 − y2 k2 ≤ ky1 − y2 k1 ky1 − y2 k∞ ≤ k for all y1 , y2 ∈ P .
Corollary C.2. Let ft be convex functions sampled i.i.d. with expectation E [ft ] = f ∗ , and δ > 0. Assume that the ft are
L-Lipschitz in the 2-norm.

(i) If all the ft are smooth with curvature at most C, then Algorithm 7 applied to the ft (with b = 0) yields with probability
1−δ
T
T

 √
X
X
p
f ∗ (xt ) − min
f ∗ (x) ≤ O C T + Lk nT log(nT 2 /δ) log T .
t=1

x∈P

t=1

(ii) Without any smoothness assumption, Algorithm 7 (applied to smoothenings of the ft ) provides with probability 1 − δ
T
X
t=1

f ∗ (xt ) − min

T
X

x∈P

t=1

f ∗ (x) ≤ O

√

nLkT 2/3 + Lk

p


nT log(nT 2 /δ) log T .

Similar to (Hazan & Kale, 2012, Theorem 4.4), from Theorem C.1 (ii) we obtain the following regret bound for adversarial
cost functions with an analogous proof.
Corollary C.3. For any L-Lipschitz convex cost
ft , Algorithm 7 applied to the functions f˜t (x) := ∇ft (xt )x +
√ functions √
2
2L −1/4
√
t
kx
−
x
k
(with
b
=
s
=
1/4,
C
=
L
k,
S
=
L/
k, and Lipschitz constant 3L) achieving regret
1 2
k
T
X
t=1

ft (xt ) − min
x∈P

T
X
t=1

√
ft (x) ≤ O(L kT 3/4 )

with at most T calls to the weak separation oracle.
√
Note that the gradient of the f˜t are easily computed via the formula ∇f˜t (x) = ∇ft (xt ) + 4Lt−1/4 (x − x1 )/ k, particularly
because the gradient of the ft need not be recomputed, so that we obtain a weak separation-based stochastic gradient
descent algorithm, where we only have access to the ft through a stochastic gradient oracle, while retaining all the favorable
properties of the Frank-Wolfe algorithm with a convergence rate O(T −1/4 ) (c.f., (Garber & Hazan, 2013)).

D. Weak Separation through Augmentation
So far we realized the weak separation oracle via lazy optimization. We will now create a (weak) separation oracle for
integral polytopes, employing an even weaker, so-called augmentation oracle, which only provides an improving solution
but provides no guarantee with respect to optimality. We call this approach lazy augmentation. This is especially useful
when a fast augmentation oracle is available or the vertices of the underlying polytope P are particularly sparse. As before
theoretical convergence rates are maintained.

Lazifying Conditional Gradient Algorithms

For simplicity of exposition we restrict to 0/1 polytopes P here. For general integral polytopes, one considers a so-called
directed augmentation oracle, which can be similarly linearized after splitting variables in positive and negative parts; we
refer the interested reader to see (Schulz & Weismantel, 2002; Bodic et al., 2015) for an in-depth discussion.
Let k denote the `1 -diameter of P . Upon presentation with a 0/1 solution x and a linear objective c ∈ Rn , an augmentation
oracle either provides an improving 0/1 solution x̄ with cx̄ < cx or asserts optimality for c:
Oracle 3 Linear Augmentation Oracle AUGP (c, x)
Input: c ∈ Rn linear objective, x ∈ P vertex,
Output: x̄ ∈ P vertex with cx̄ < cx when exists, otherwise x̄ = x
Such an oracle is significantly weaker than a linear optimization oracle but also significantly easier to implement and much
faster; we refer the interested reader to (Grötschel & Lovász, 1993; Schulz et al., 1995; Schulz & Weismantel, 2002) for
an extensive list of examples. While augmentation and optimization are polynomially equivalent (even for convex integer
programming (Oertel et al., 2014)) the current best linear optimization algorithms based on an augmentation oracle are
slow for general objectives. While optimizing an integral objective c ∈ Rn needs O(k logkck∞ ) calls to an augmentation
oracle (see (Schulz et al., 1995; Schulz & Weismantel, 2002; Bodic et al., 2015)), a general objective function, such as the
gradient in Frank–Wolfe algorithms has only an O(kn3 ) guarantee in terms of required oracle calls (e.g., via simultaneous
diophantine approximations (Frank & Tardos, 1987)), which is not desirable for large n. In contrast, here we use an
augmentation oracle to perform separation, without finding the optimal solution. Allowing a multiplicative error K > 1, we
realize an augmentation-based weak separation oracle (see Algorithm 8), which decides given a linear objective function
c ∈ Rn , an objective value Φ > 0, and a starting point x ∈ P , whether there is a y ∈ P with c(x − y) > Φ/K or
c(x − y) ≤ Φ for all y ∈ P . In the former case, it actually provides a certifying y ∈ P , i.e., with c(x − y) > Φ/K. Note
that a constant accuracy K requires a linear number of oracle calls in the diameter k, e.g., K = (1 − 1/e)−1 ≈ 1.582 needs
at most N ≤ k oracle calls.
At the beginning, in Line 2, the algorithm has to replace the input point x with an integral point x0 . If the point x is given as
a convex combination of integral points, then a possible solution is to evaluate the objective c on these integral points, and
choose x0 the first one with cx0 ≤ cx. This can be easily arranged for Frank–Wolfe algorithms as they maintain convex
combinations.
Algorithm 8 Augmenting Weak Separation LPsepP (c, x, Φ, K)
Input: c ∈ Rn linear objective, x ∈ P point, Φ > 0 objective value; K > 1 accuracy
Output: Either (1) y ∈ P vertex with c(x − y) > Φ/K, or (2) false: c(x − z) ≤ Φ for all z ∈ P .
1: N ← dlog(1 − 1/K)/ log(1 − 1/k)e
2: Choose x0 ∈ P vertex with cx0 ≤ cx.
3: for i = 1 to N do
4:
if c(x − xi−1 ) ≥ Φ then
5:
return xi−1
6:
end if
i−1 )
7:
xi ← AUGP (c + Φ−c(x−x
(1 − 2xi−1 ), xi−1 )
k
8:
if xi = xi−1 then
9:
return false
10:
end if
11: end for
12: return xN
Proposition D.1. Assume ky1 − y2 k1 ≤ k for all y1 , y2 ∈ P . Then Algorithm 8 is correct, i.e., it outputs either (1) y ∈ P
with c(x − y) > Φ/K, or (2) false. In the latter case c(x − y) ≤ Φ for all y ∈ P holds. The algorithm calls AUGP at
most N ≤ dlog(1 − 1/K)/ log(1 − 1/k)e many times.
Proof. First note that (1 − 2x)v + kxk1 = kv − xk1 for x, v ∈ {0, 1}n , hence Line 7 is equivalent to xi ← AUGP (c +
Φ−c(x−xi−1 )
k· − xi−1 k1 , xi−1 ).
k

Lazifying Conditional Gradient Algorithms

The algorithm obviously calls the oracle at most N times by design, and always returns a value, so we need to verify only
the correctness of the returned value. We distinguish cases according to the output.
Clearly, Line 5 always returns an xi−1 with c(x − xi−1 ) ≥ Φ > [1 − (1 − 1/k)N ]Φ. When Line 9 is executed, the
augmentation oracle just returned xi = xi−1 , i.e., for all y ∈ P
Φ − c(x − xi−1 )
ky − xi−1 k1
k
Φ − c(x − xi−1 )
k
≤ cy +
k
= c(y − x) + cxi−1 + Φ,

cxi−1 ≤ cy +

so that c(x − y) ≤ Φ, as claimed.
Finally, when Line 12 is executed, the augmentation oracle has found an improving vertex xi at every iteration, i.e.,
Φ − c(x − xi−1 )
kxi − xi−1 k1
k
Φ − c(x − xi−1 )
≥ cxi +
,
k

cxi−1 > cxi +

using kxi − xi−1 k1 ≥ 1 by integrality. Rearranging provides the convenient form


1
[Φ − c(x − xi−1 )],
Φ − c(x − xi ) < 1 −
k
which by an easy induction provides

Φ − c(x − xN ) <
i.e., c(x − xN ) ≥

Φ
K,

1
1−
k

N


[Φ − c(x − x0 )] ≤

1
1−
K


Φ,

finishing the proof.

E. Experiments
We implemented and compared the parameter-free variant of LCG (Algorithm 4) to the standard Frank-Wolf algorithm
(CG). Moreover, we implemented and compared Algorithm 3 (LPCG) to the Pairwise Conditional Gradient algorithm (PCG)
variant of (Garber & Meshi, 2016) as well as implemented and compared Algorithm 7 (LOCG) to the Online Frank-Wolfe
algorithm (OCG) of (Hazan & Kale, 2012). While we did implement the Local Conditional Gradient variant in (Garber &
Hazan, 2013) as well, the very large constants in the original algorithms made it impractical to run.
We have used K = 1.1 and K = 1 as multiplicative factors for the weak separation oracle; for the impact of the choice of
K see Section E.2.1. For the baseline algorithms we use inexact variants, i.e., we solve linear optimization problems only
approximately. This is a significant speedup in favor of non-lazy algorithms at the (potential) cost of accuracy, while neutral
to lazy optimization as it solves an even more relaxed problem anyways. To put things in perspective, the non-lazy baselines
could not complete even a single iteration for a significant fraction of the considered problems in the given time frame if we
were to exactly solve the linear optimization problems.
The linear optimization oracle over P × P for LPCG was implemented by calling the respective oracle over P twice: once
for either component. Contrary to the non-lazy version, the lazy algorithms depend on the initial upper bound Φ0 . For
the instances that need a very long time to solve the (approximate) linear optimization even once, we used for the lazy
algorithms a binary search for Φ0 : starting from a conservative initial value, using the update rule Φ0 ← Φ0 /2 until the
separation oracle returns an improvement for the first time and then we start the algorithm with 2Φ0 , which is an upper
bound on the Wolfe gap and hence also on the primal gap. This initial phase is also included in the reported wall-clock time.
Alternatively, if the linear optimization is less time consuming we used a single (approximate) linear optimization at the
start to obtain an initial bound on Φ0 (see e.g., Section 4).
In some cases, especially when the underlying feasible region has a high dimension and the (approximate) linear optimization
can be solved relatively fast compared to the cost of computing an inner product, we observed that the costs of maintaining

Lazifying Conditional Gradient Algorithms

the cache was very high. In these cases we reduce the cache size every 100 steps by keeping only the 100 points that were
used the most so far. Both, the number of steps and the approximate size of the cache are chosen arbitrarily, however 100 for
both worked very well for all our examples. Of course there are many different strategies for maintaining the cache, which
could be used here and which could lead to further improvements in performance.
The stopping criteria for each of the experiments is a given wall clock time limit in seconds. The time limit was enforced
separately for the main code, and the oracle code so in some cases the actual time used can be larger, when the last oracle
call started before the time limit was reached and took longer than the time left.
We implemented all algorithms in Python 2.7 with critical functions cythonized for performance employing Numpy.
We used these packages from the Anaconda 4.2.0 distribution as well as Gurobi 7.0 (Gurobi Optimization, 2016)
as a black box solver for the linear optimization oracle and the weak separation oracle. The latter was implemented via a
callback function to stop the optimization as soon as a good enough feasible solution has been found. The parameters for
Gurobi were kept at their default settings except for enforcing the time limit of the tests and setting the acceptable duality
gap to 10%, allowing Gurobi to terminate the linear optimization early avoiding the expensive proof of optimality. This is
used to realize the inexact versions of the baseline algorithms. All experiments were performed on a 16-core machine with
Intel Xeon E5-2630 v3 @ 2.40GHz CPUs and 128GB of main memory. While our code does not explicitly use multiple
threads, both Gurobi and the numerical libraries use multiple threads internally.
E.1. Computational results
We performed computational tests on a large variety of different problems that are instances of the three machine learning
tasks video colocalization, matrix completion and structured regression.
Video colocalization. Video colocalization is the problem of identifying objects in a sequence of multiple frames in a
video. In (Joulin et al., 2014) it is shown that video colocalization can be reduced to optimizing a quadratic objective
function over a flow or a path polytope , which is the problem we are going to solve. The quadratic functions are of the
2
form kAx − bk where we choose the non-zero entries in A according to a density parameter at random and then each of
these entries to be [0, 1]-uniformly distributed, while b is chosen as a linear combination of the columns of A with random
2
multipliers from [0, 1]. For some of the instances we also use kx − bk as the objective function with bi ∈ [0, 1] uniformly
at random.
Matrix completion. The formulation of the matrix completion problem we are going to use is the following:
X
2
min
kXi,j − ai,j k
s.t. kXk∗ ≤ R,

(5)

(i,j)∈Ω

√
where k·k∗ denotes the nuclear norm, i.e., kAk∗ = tr( At A). The set Ω, the matrix A with entries ai,j , and R are given
parameters. Similarly to (Lan & Zhou, 2014) we generate the m × n matrix A as the product of AL of size m × r and AR
of size r × n. The entries in AL and AR are chosen from a standard Gaussian. The set of entries Ω is chosen uniformly of
size s = min(5r(m + n − r), d0.99mne). The linear optimization oracle is implemented in this case by a singular value
decomposition of the linear objective function.
2

Structured regression. The structured regression problem consists of solving a quadratic function of the form kAx − bk
over some structured feasible set or a polytope. We construct the objective functions in the same way as for the video
colocalization problem.
We will present in the following two sections the complete set of results for various problems grouped by the different
versions of the considered algorithms. Every figure contains two columns, each containing one experiment. We use different
measures to report performance: the first row reports loss or function value in wall-clock time (including time spent by the
oracle), the second row contains loss or function value in the number of iterations. In some cases we include a row reporting
the loss or function value over the number of linear optimization calls. In some other cases we report in another row the dual
bound or Wolfe gap in wall-clock time. The last row always reports the cumulative number of calls to the linear optimization
oracle for the lazy algorithm. The red line denotes the non-lazy algorithm and the greed line denotes the lazy variants. For
each experiment we also report the cache hit rate, which is the number of oracle calls answered with a point from the cache
over all oracle calls given in percent.

Lazifying Conditional Gradient Algorithms

While we found convergence rates in the number of iterations quite similar (as expected!), we consistently observe a
significant speedup in wall-clock time. In particular for many large-scale or hard combinatorial problems, lazy algorithms
performed several thousand iterations whereas the non-lazy versions completed only a handful of iterations due to the large
time spent approximately solving the linear optimization problem. The observed cache hit rate was at least 90% in most
cases, and often even above 99%.
E.1.1. O NLINE R ESULTS
Additionally to the quadratic objective functions we tested the online version on random linear functions cx + b with
c ∈ [−1, +1]n and b ∈ [0, 1]. For online algorithms, each experiment used a random sequence of 100 different random loss
functions. For online conditional gradient algorithms, in every figure the left column uses linear loss functions, the right one
uses quadratic loss functions of the form as described above over the same polytope.
As an instance of the structured regression problem we used the flow-based formulation for Hamiltonian cycles in graphs,
i.e., the traveling salesman problem (TSP) for graphs with 11 and 16 nodes (Figures 7 and 8). While relatively small,
the oracle problem can be solved in reasonable time for these instances. Another instance of the structured regression
problem uses the standard formulation of the cut polytope for graphs with 23 and 28 nodes as the feasible region (Figures 9
and 10). Another set of feasible regions corresponding to NP-hard problems for the structured regression problem we
tested our algorithm on are the quadratic unconstrained boolean optimization (QUBO) instances defined on Chimera
graphs (Dash, 2013), which are available at http://researcher.watson.ibm.com/researcher/files/ussanjeebd/chimera-data.zip. The instances are relatively hard albeit their rather small size (Figure 11 and 12).
One instance of the video colocalization problem uses a path polytope from http://lime.cs.elte.hu/˜kpeter/
data/mcf/netgen/ that was generated with the netgen graph generator (Figure 13). Most of these instances are very
large-scale minimum cost flow instances with several tens of thousands nodes in the underlying graphs, therefore solving still
takes considerable time despite the problem being in P. We tested on the structured regression problems with the MIPLIB
(Achterberg et al., 2006; Koch et al., 2011)) instances eil33-2 (Figure 14) and air04 (Figure 15) as feasible regions.
Finally for the spanning tree problem, we used the well-known extended formulation with O(n3 ) inequalities for an n-node
graph. We considered graphs with 10 and 25 nodes (Figures 16 and 17).
We observed that while OCG and LOCG converge comparably in the number of iterations, the lazy LOCG performed
significantly more iterations; for hard problems, where linear optimization is costly and convergence requires a large number
of iterations, this led LOCG converging much faster in wall-clock time. In extreme cases OCG could not complete even a
single iteration. This is due to LOCG only requiring some good enough solution, whereas OCG requires a stronger guarantee.
This is reflected in faster oracle calls for LOCG.
E.1.2. O FFLINE R ESULTS
We describe the considered instances in the offline case separately for the vanilla Frank-Wolfe method and the Pairwise
Conditional Gradients method.
Vanilla Frank-Wolfe Method We tested the vanilla Frank-Wolfe algorithm on the six video colocalization instances
with underlying path polytopes from http://lime.cs.elte.hu/˜kpeter/data/mcf/netgen/ (Figures 18,
19 and 20). In these instances we additionally report the dual bound or Wolfe gap in wall clock time. We further tested the
vanilla Frank-Wolfe algorithm on eight instances of the matrix completion problem generated as described above. For these
examples we did not use line search. We give the used parameters for each example in the figures below (Figures 21, 22, 23
and 24). The last tests for this version were performed on three instances of the structured regression problem, two with the
feasible region containing flow-based formulations of Hamiltonian cycles in graphs (Figures 25), two on two different cut
polytope instances (Figure 26) and finally two on two spanning tree instances of different size (Figure 27).
Similarly to the online case, we observe a significant speedup of LCG compared to CG, due to the faster iteration of the lazy
algorithm.
Pairwise Conditional Gradient Algorithm As we inherit structural restrictions of PCG on the feasible region, the
problem repertoire is limited in this case. We tested the Pairwise Conditional Gradient algorithm on the structured
regression problem with feasible regions from the MIPLIB instances eil33-2, air04, eilB101, nw04, disctom,
m100n500k4r1 (Figures 28, 29 and 30).

Lazifying Conditional Gradient Algorithms

Again similarly to the online case and the vanilla Frank-Wolfe algorihtm, we observe a significant improvement in wall-clock
time of LPCG compared to CG, due to the faster iteration of the lazy algorithm.
E.2. Performance improvements, parameter sensitivity, and tuning
E.2.1. PARAMTER - FREE VS . TEXTBOOK VARIANT
For illustrative purposes, we compare the textbook variant of the lazy conditional gradients (Algorithm 2) with its parameterfree counterpart (Algorithm 4) in Figure 31. The parameter-free variant outperforms the textbook variant due to the active
management of Φ combined with line search.
Similar parameter-free variants can be derived for the other algorithms; see discussion in Section 4.

F. Final Remarks
We would like to close with a few final remarks. If a given baseline algorithm works over general compact convex sets P ,
then so does the lazified version. In fact, as the lazified algorithm runs, it produces a polyhedral approximation of the set P
with very few vertices (subject to optimality vs. sparsity tradeoffs; see (Jaggi, 2013, Appendix C)).
Moreover, the weak separation oracle does not need to return extreme points. All algorithms also work with maximal
solutions that are not necessarily extremal (e.g., lying in a higher-dimensional face). However, in that case we lose the
desirable property that the final solution is a sparse convex combination of extreme points (typically vertices in the polyhedral
setup) or atoms.

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

Loss

Loss

Lazifying Conditional Gradient Algorithms

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)
LOCG
OCG

0

LOCG
OCG

0

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

1500 3000 4500 6000
Iterations

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)
LOCG
OCG

0

3000 6000 9000 12000
Iterations

15
10
5
LOCG

0

0

1500 3000 4500 6000
Iterations
cache hit rate: 99.7%

Number of LP calls

Number of LP calls

100
80
60
40
20
0

LOCG

0

3000 6000 9000 12000
Iterations
cache hit rate: 99.0%

Figure 7. LOCG vs. OCG with the TSP polytope for a graph with 11 nodes as the feasible region and with a 500 seconds time limit.
OCG completed only a few iterations, resulting in a several times larger final loss for quadratic loss functions. Notice that with time
LOCG needed fewer and fewer LP calls.

1.0
0.8
0.6
0.4
0.2
0.0

Loss

Loss

Lazifying Conditional Gradient Algorithms

LOCG
OCG

4000 8000 1200016000
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

0

LOCG
OCG

0

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

1.0
0.8
0.6
0.4
0.2
0.0

50 100 150
Iterations

15
30
45
Wall-clock time (s)
LOCG
OCG

0

20 40 60
Iterations

15
10
5
LOCG

0

0

50 100 150
Iterations
cache hit rate: 89.1%

Number of LP calls

Number of LP calls

20
50
40
30
20
10
0

LOCG

0

20 40 60
Iterations
cache hit rate: 20.6%

Figure 8. LOCG vs. OCG on the TSP polytope for a graph with 16 nodes with a time limit of 7200 seconds. OCG was not able to
complete a single iteration and in the quadratic case even LOCG could not complete any more iteration after 50s. The quadratic losses on
the right nicely demonstrate speed improvements (mostly) through early termination of the linear optimization as the cache rate is only
20.6%.

1.0
0.8
0.6
0.4
0.2
0.0

Loss

LOCG
OCG

1.0
0.8
0.6
0.4
0.2
0.0

Number of LP calls

12
10
8
6
4
2
0

LOCG
OCG

0

LOCG
OCG

0

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)

Loss

Loss

0

800 1600 2400
Iterations

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)
LOCG
OCG

0

2000 4000 6000 8000
Iterations

200

LOCG

0

800 1600 2400
Iterations
cache hit rate: 99.6%

Number of LP calls

Loss

Lazifying Conditional Gradient Algorithms

150
100
50
LOCG

0

0

2000 4000 6000 8000
Iterations
cache hit rate: 97.5%

Figure 9. LOCG vs. OCG on the cut polytope for a graph with 23 nodes. Both LOCG and OCG converge to the optimum in a few
iterations for linear losses, while LOCG is remarkably faster for quadratic losses. It demonstrates that the advantage of lazy algorithms
strongly correlates with the difficulty of linear optimization. For linear losses, remarkably LOCG needed no oracle calls after one third of
the time.

1.0
0.8
0.6
0.4
0.2
0.0

Loss

LOCG
OCG

1.0
0.8
0.6
0.4
0.2
0.0

2000 4000 6000 8000
Iterations

20
15
10
5
LOCG

0

0

2000 4000 6000 8000
Iterations
cache hit rate: 99.7%

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

0

LOCG
OCG

0

Number of LP calls

2000 4000 6000 8000
Wall-clock time (s)

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

2000 4000 6000 8000
Wall-clock time (s)
LOCG
OCG

0

Number of LP calls

Loss

Lazifying Conditional Gradient Algorithms

300
250
200
150
100
50
0

8000 16000 24000
Iterations

LOCG

0

8000 16000 24000
Iterations
cache hit rate: 98.6%

Figure 10. LOCG vs. OCG on the cut polytope for a 28-node graph. As for the smaller problem, this also illustrates the advantage of lazy
algorithms when linear optimization is expensive. Again, LOCG needed no oracle calls after a small initial amount of time.

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

Loss

Loss

Lazifying Conditional Gradient Algorithms

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)
LOCG
OCG

0

LOCG
OCG

0

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

2000 4000 6000
Iterations

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)
LOCG
OCG

0

3000 6000 9000 12000
Iterations

20
15
10
5
0

LOCG

0

2000 4000 6000
Iterations
cache hit rate: 99.6%

Number of LP calls

Number of LP calls

25
150
100
50
LOCG

0

0

3000 6000 9000 12000
Iterations
cache hit rate: 98.5%

Figure 11. LOCG vs. OCG on a small QUBO instance. For quadratic losses, both algorithms converged very fast while LOCG still has a
significant edge. For linear losses, LOCG is noticeably faster than OCG.

1.0
0.8
0.6
0.4
0.2
0.0

Loss

LOCG
OCG

1.0
0.8
0.6
0.4
0.2
0.0

10
5
LOCG

0

3000 6000 9000 12000
Iterations
cache hit rate: 99.8%

2000 4000 6000 8000
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0

3000 6000 9000 12000
Iterations

15

0

LOCG
OCG

0

LOCG
OCG

0

Number of LP calls

2000 4000 6000 8000
Wall-clock time (s)

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

0

Number of LP calls

Loss

Lazifying Conditional Gradient Algorithms

4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0

8000 16000 24000
Iterations

LOCG

0

8000 16000 24000
Iterations

cache hit rate: 99.99%

Figure 12. LOCG vs. OCG on a large QUBO instance. Both algorithms converge fast to the optimum. Interestingly, LOCG only performs
4 LP calls.

1.0
0.8
0.6
0.4
0.2
0.0

Loss

LOCG
OCG

1.0
0.8
0.6
0.4
0.2
0.0

Number of LP calls

14
12
10
8
6
4
2
0

150 300
Iterations

0

150 300
Iterations
cache hit rate: 97.0%

1.0
0.8
0.6
0.4
0.2
0.0

450

LOCG

450

LOCG
OCG

0

LOCG
OCG

0

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)

Loss

Loss

0

50
40
30
20
10
0

150 300 450 600
Wall-clock time (s)
LOCG
OCG

0

Number of LP calls

Loss

Lazifying Conditional Gradient Algorithms

150 300 450 600
Iterations

LOCG

0

150 300 450 600
Iterations
cache hit rate: 89.6%

Figure 13. LOCG vs. OCG on a path polytope. Similar convergence rate in the number of iterations, but significant difference in terms of
wall-clock time.

1.0
0.8
0.6
0.4
0.2
0.0

Loss

LOCG
OCG

150 300 450 600
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

Number of LP calls

0
300
250
200
150
100
50
0

LOCG
OCG

0

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

200 400 600
Iterations

150 300 450 600
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

0

500 1000 1500
Iterations

500

LOCG

0

200 400 600
Iterations

cache hit rate: 53.1%

Number of LP calls

Loss

Lazifying Conditional Gradient Algorithms

400
300
200
100
0

LOCG

0

500 1000 1500
Iterations

cache hit rate: 72.2%

Figure 14. LOCG vs. OCG on the MIPLIB instance eil33-2. All algorithms performed comparably, due to fast convergence in this
case.

1.0
0.8
0.6
0.4
0.2
0.0

Loss

LOCG
OCG

1.0
0.8
0.6
0.4
0.2
0.0

0

Number of LP calls

1.0
0.8
0.6
0.4
0.2
0.0

800 1600 2400
Iterations

LOCG

800 1600 2400
Iterations
cache hit rate: 96.8%

LOCG
OCG

0

LOCG
OCG

0
70
60
50
40
30
20
10
0

2500 5000 7500
Wall-clock time (s)

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

8
7
6
5
4
3
2
1
0

2000 4000 6000 8000
Wall-clock time (s)
LOCG
OCG

0

Number of LP calls

Loss

Lazifying Conditional Gradient Algorithms

1500 3000 4500 6000
Iterations

LOCG

0

1500 3000 4500 6000
Iterations
cache hit rate: 99.9%

Figure 15. LOCG vs. OCG on the MIPLIB instance air04. LOCG clearly outperforms OCG as the provided time was not enough for
OCG to complete the necessary number of iterations for entering reasonable convergence.

1.0
0.8
0.6
0.4
0.2
0.0

LOCG
OCG

Loss

Loss

Lazifying Conditional Gradient Algorithms

1.0
0.8
0.6
0.4
0.2
0.0

150 300 450 600
Wall-clock time (s)
LOCG
OCG

0

LOCG
OCG

0

Loss

Loss

0

1.0
0.8
0.6
0.4
0.2
0.0

1.0
0.8
0.6
0.4
0.2
0.0

1000 2000 3000 4000
Iterations

200 400 600
Wall-clock time (s)
LOCG
OCG

0

2500 5000 7500 10000
Iterations

40
30
20
10
0

LOCG

0

1000 2000 3000 4000
Iterations
cache hit rate: 98.8%

Number of LP calls

Number of LP calls

50
15
10
5
LOCG

0

0

2500 5000 7500 10000
Iterations
cache hit rate: 99.8%

Figure 16. LOCG vs. OCG on a spanning tree instance for a 10-node graph. LOCG makes significantly more iterations, few oracle calls,
and converges faster in wall-clock time.

LOCG
OCG

1.0
0.8
0.6
0.4
0.2
0.0

120
100
80
60
40
20
0

1.0
0.8
0.6
0.4
0.2
0.0

800 1600 2400
Iterations

LOCG

0

800 1600 2400
Iterations

cache hit rate: 95.9%

LOCG
OCG

0

LOCG
OCG

0

Number of LP calls

1.0
0.8
0.6
0.4
0.2
0.0

2000 4000 6000 8000
Wall-clock time (s)

Loss

Loss

0

Loss

1.0
0.8
0.6
0.4
0.2
0.0

14
12
10
8
6
4
2
0

2000 4000 6000 8000
Wall-clock time (s)
LOCG
OCG

0

Number of LP calls

Loss

Lazifying Conditional Gradient Algorithms

1500 3000 4500 6000
Iterations

LOCG

0

1500 3000 4500 6000
Iterations
cache hit rate: 99.7%

Figure 17. LOCG vs. OCG on a spanning tree instance for a 25-node graph. On the left, early fluctuation can be observed, bearing no
consequence for later convergence rate. OCG did not get past this early stage. In both cases LOCG converges significantly faster.

Lazifying Conditional Gradient Algorithms

1014

Function value

LCG
CG

1013

1012

0

1013

80
160
240
Wall clock time (s)

1013

0

1014
1013
1012
1011
1010
109
108
107
106

LCG
CG

0

80
160 240
Wall clock time (s)

LCG
CG

1013

1000 2000 3000 4000
Iterations

Dual bound

Dual bound
Number of LP calls

150
300
450
Wall clock time (s)

Function value

LCG
CG

180
160
140
120
100
80
60
40
20
0

0

1014

1014
1013
1012
1011
1010
109
108
107
106

0

500 1000 1500
Iterations
LCG
CG

0

150 300 450 600
Wall clock time (s)

100
LCG

0

1000 2000 3000 4000
Iterations

cache hit rate: 95.72%

Number of LP calls

Function value

1014

1012

LCG
CG

Function value

1014

LCG

80
60
40
20
0

0

500 1000 1500
Iterations

cache hit rate: 94.83%

Figure 18. LCG vs. CG on small netgen instances netgen 08a (left) and netgen 10a (right) with quadratic objective functions. In
both cases both algorithms are able to reduce the function value very fast, however the dual bound or Wolfe gap is reduced much faster by
LCG. Observe that the vertical axis is given with a logscale.

Lazifying Conditional Gradient Algorithms

1016

Function value

LCG
CG

1014

0

Function value

1015

0

100 200 300
Iterations

LCG
CG

0

150 300 450 600
Wall clock time (s)

Function value

1015

1014

400

Dual bound

Function value
Dual bound

1016
1015
1014
1013
1012
1011
1010
109
108
107

0

30
60
90
Iterations

120

LCG
CG

0

150 300 450 600
Wall clock time (s)

60
LCG

50

Number of LP calls

Number of LP calls

150
300
450
Wall clock time (s)
LCG
CG

60

40
30
20
10
0

0

1016
LCG
CG

1015
1014
1013
1012
1011
1010
109
108
107
106

1015

1014

150
300
450
Wall clock time (s)

1015

1014

LCG
CG

0

100 200 300
Iterations

cache hit rate: 86.43%

400

LCG

50
40
30
20
10
0

0

30
60
90
Iterations

120

cache hit rate: 50.00%

Figure 19. LCG vs. CG on medium sized netgen instances netgen 12b (left) and netgen 14a (right) with quadratic objective
functions. The behavior of both versions on these instances is very similar to the small netgen instances (Figure 18), however both in the
function value and the dual bound the difference between the lazy and the non-lazy version is more prominent. Again, we used a logscale
for the vertical axis.

Lazifying Conditional Gradient Algorithms

1017
LCG
CG

1016
1015
1014

0

Function value

Function value

1017

Function value

1015

Dual bound

0

150
300
450
Wall clock time (s)

0

1017
1016
1015
1014
1013
1012
1011
1010
109
108
107
106
150

10
20
30
Iterations

40

LCG
CG

LCG
CG

1016
1015
1014

Dual bound

Function value

1016

300
450
Wall clock time (s)

20

0

1017
1016
1015
1014
1013
1012
1011
1010
109
108
107
150

10
20
30
Iterations

40

LCG
CG

300
450
Wall clock time (s)

20
LCG

Number of LP calls

Number of LP calls

1015

1017
LCG
CG

15
10
5
0

1016

1014

150
300
450
Wall clock time (s)

1017

1014

LCG
CG

0

10
20
30
Iterations

cache hit rate: 48.72%

40

LCG

15
10
5
0

0

10
20
30
Iterations

40

cache hit rate: 50.00%

Figure 20. LCG vs. CG on large netgen instances netgen 16a (left) and netgen 16b (right) with quadratic objective functions. In
both cases the difference in function value between the two versions of the algorithm is large. In the dual bound the performance of the
lazy version is multiple orders of magnitude better than the performance of the non-lazy counterpart. The cache hit rates for these two
instances are lower due to the high dimension of the polytope.

Lazifying Conditional Gradient Algorithms

108
LCG
CG

107
106
105
104

0

Function value

Function value

108

106
5

0

Function value

Function value

LCG
CG

0

150
300
450
Wall clock time (s)

500 1000 1500
Iterations

107
106

0

80
160
Iterations

240

80
60
40
20
0

500 1000 1500
Iterations

cache hit rate: 94.24%

Number of LP calls

35
LCG

0

LCG
CG

105

100
Number of LP calls

106

108

107

104

107

105

150
300
450
Wall clock time (s)

108

10

LCG
CG

LCG

30
25
20
15
10
5
0

0

80
160
Iterations

240

cache hit rate: 84.80%

Figure 21. LCG vs. CG on two matrix completion instances. We solve the problem as given in Equation (5) with the paramters n = 3000,
m = 1000, r = 10 and R = 30000 for the left instance and n = 10000, m = 100, r = 10 and R = 10000 for the right instance. In
both cases the lazy version is slower in interations, however significantly faster in wall clock time.

Lazifying Conditional Gradient Algorithms

109
LCG
CG

Function value

Function value

109
108
107
106

0

250 500 750 1000
Wall clock time (s)

LCG
CG

108
107
106

108
107

Number of LP calls

0

16
14
12
10
8
6
4
2
0

150 300 450
Iterations

200 400 600
Iterations
cache hit rate: 97.50%

LCG
CG

108
107
106

600

LCG

0

Function value

LCG
CG

106

250 500 750 1000
Wall clock time (s)

109

Number of LP calls

Function value

109

0

16
14
12
10
8
6
4
2
0

0

10
20
30
Iterations

40

LCG

0

10
20
30
Iterations

40

cache hit rate: 59.46%

Figure 22. LCG vs. CG on two more matrix completion instances. The parameters for Equation (5) are given by n = 5000, m = 4000,
r = 10 and R = 50000 for the left instance and n = 100, m = 20000, r = 10 and R = 15000 for the right instance. In both of these
cases the performance of the lazy and the non-lazy version are comparable in interations, however in wall clock time the lazy version
reaches lower function values faster.

Lazifying Conditional Gradient Algorithms

108
LCG
CG

108
107
106
105
104
103

0

Function value

Function value

109

LCG
CG

108

Function value

Function value

106

0

150
300
450
Wall clock time (s)

108

107
106
105
104
0

200 400 600
Iterations

800

107
106

0

100 200 300
Iterations

400

70
Number of LP calls

LCG

120
100
80
60
40
20
0

LCG
CG

105

140
Number of LP calls

107

105

150
300
450
Wall clock time (s)

109

103

LCG
CG

0

200 400 600
Iterations
cache hit rate: 80.80%

800

LCG

60
50
40
30
20
10
0

0

100 200 300
Iterations

400

cache hit rate: 82.98%

Figure 23. LCG vs. CG on our fifth and sixth instances of the matrix completion problem. The parameters are n = 5000, m = 100,
r = 10 and R = 15000 for the left instance and n = 3000, m = 2000, r = 10 and R = 10000 for the right instance. The behavior is
very similar to Figure 22. similar performance over iterations however advantages for the lazy version over wall clock time.

Lazifying Conditional Gradient Algorithms

109
LCG
CG

107

106

0

Function value

Function value

108

0

0

150
300
450
Wall clock time (s)

80
160 240
Iterations
cache hit rate: 87.10%

LCG
CG

108
107
106
105

80
160 240
Iterations
LCG

0

Function value

107

Number of LP calls

Function value
Number of LP calls

106

109
LCG
CG

40
35
30
25
20
15
10
5
0

107

105

150
300
450
Wall clock time (s)

108

106

10

LCG
CG

8

80
70
60
50
40
30
20
10
0

0

250 500 750 1000
Iterations
LCG

0

250 500 750 1000
Iterations
cache hit rate: 91.55%

Figure 24. LCG vs. CG on the final two matrix completion instances. The parameters are n = 10000, m = 1000, r = 10 and
R = 1 − 000 for the left instance and n = 5000, m = 1000, r = 10 and R = 30000 for the right instance. On the left in both
measures, instances and wall clock time, the lazy version performs better than the non-lazy counterpart, due to a suboptimal direction at
the beginning with a fairly large step size in the non-lazy version.

Lazifying Conditional Gradient Algorithms

102
LCG
CG

101
100
10−1
10−2
10−3
10−4

0

Function value

Function value

102

100
10−1
10−2
−3

0

1500 3000
Iterations

Function value

Function value

LCG
CG

0

1000 2000 3000
Wall clock time (s)

100
10−1
0

80
160
Iterations

240

140
Number of LP calls

LCG

1200
1000
800
600
400
200
0

10

LCG
CG

1

10−2

4500

1400
Number of LP calls

10−1

102

101

10−4

100

10−2

1000 2000 3000 4000
Wall clock time (s)

102

10

10

LCG
CG

1

0

1500 3000
Iterations
cache hit rate: 69.46%

4500

LCG

120
100
80
60
40
20
0

0

80
160
Iterations

240

cache hit rate: 43.06%

Figure 25. LCG vs. CG on structured regression problems with feasible regions being a TSP polytope over 11 nodes (left) and 12 nodes
(right). In both cases LCG is significantly faster in wall-clock time.

105
104
103
102
101
100
10−1
10−2
10−3

105
LCG
CG

0

Function value

105
104
103
102
101
100
10−1
10−2
10−3

LCG
CG

0

1500 3000
Iterations

102
101
100
0

1000 2000 3000 4000
Wall clock time (s)
LCG
CG

104
103
102
101
100
10−1

4500

0

300 600 900 1200
Iterations

140
LCG

500

Number of LP calls

Number of LP calls

103

105

600

400
300
200
100
0

LCG
CG

104

10−1

1000 2000 3000 4000
Wall clock time (s)

Function value

Function value

Function value

Lazifying Conditional Gradient Algorithms

0

1500 3000
Iterations
cache hit rate: 85.61%

4500

LCG

120
100
80
60
40
20
0

0

300 600 900 1200
Iterations
cache hit rate: 87.48%

Figure 26. LCG vs. CG on structured regression instances using cut polytopes over a graph on 23 nodes (left) and over 28 nodes (right)
as feasible region. In both instances LCG performs significantly better than CG.

Lazifying Conditional Gradient Algorithms

104
LCG
CG

102
101
10

0

10−1
10−2

0

Function value

Function value

103

102
101
100
10−1
10−2

150
300
450
Wall clock time (s)

101
0

10−1

Number of LP calls

90
80
70
60
50
40
30
20
10
0

0

Function value

LCG
CG

102

10−2

0

1000 2000 3000 4000
Wall clock time (s)

104
LCG
CG

103
102
101
100
10−1
10−2

250 500 750 1000
Iterations

0

800
1600
Iterations

2400

100
LCG

0

250 500 750 1000
Iterations
cache hit rate: 90.83%

Number of LP calls

Function value

103

10

LCG
CG

103

LCG

80
60
40
20
0

0

800
1600
Iterations

2400

cache hit rate: 95.59%

Figure 27. LCG vs. CG on structured regression instances with extended formulation of the spanning tree problem on a 10 node graph on
the left and a 15 node graph on the right.

Lazifying Conditional Gradient Algorithms

LPCG
PCG

Number of LP calls

1000 2000 3000 4000
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0
0.0

LPCG
PCG

0.5

6000
5000
4000
3000
2000
1000
0
0.0

LPCG
PCG

0

Function value

Function value

0

1.0
0.8
0.6
0.4
0.2
0.0

LPCG

0.5

1.0 1.5 2.0
Iterations 1e6

cache hit rate: 99.8%

1500 3000 4500
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0

1.0 1.5 2.0
Iterations 1e6

LPCG
PCG

0

Number of LP calls

Function value

1.0
0.8
0.6
0.4
0.2
0.0

air04, 8904 dimensions

Function value

eil33-2, 4516 dimensions

2

4
6
8
1e5
Iterations
LPCG

15
10
5
0

0

2

4
6
8
Iterations 1e5

cache hit rate: 99.999%

Figure 28. LPCG vs. PCG on two MIPLIB instances eil33-2 and air04. LPCG converges very fast, making millions of iterations
with a relatively few oracle calls, while PCG completed only comparably few iterations due to the time-consuming oracle calls. This
clearly illustrates the advantage of lazy methods when the cost of linear optimization is non-negligible. On the left, when reaching
ε-optimality, LPCG performs many (negative) oracle calls to (re-)prove optimality; at that point one might opt for stopping the algorithm.
On the right LPCG needed a rather long time for the initial bound tigthening of Φ0 , before converging significantly faster than PCG.

Lazifying Conditional Gradient Algorithms

1.0
0.8
0.6
0.4
0.2
0.0

nw04, 87482 dimensions

Function value

Function value

eilB101, 2818 dimensions

LPCG
PCG

1.0
0.8
0.6
0.4
0.2
0.0

40
35
30
25
20
15
10
5
0
0.0

LPCG
PCG

1.5
3.0
Iterations

4.5
1e5

cache hit rate: 99.995%

1000 2000 3000 4000
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0
0.0

LPCG
PCG

0.8
1.6
Iterations

2.4
1e5

25

LPCG

1.5
3.0
Iterations

Function value

1.0
0.8
0.6
0.4
0.2
0.0
0.0

0

Number of LP calls

Number of LP calls

Function value

1600 2400 3200 4000
Wall-clock time (s)

LPCG
PCG

4.5
1e5

20
15
10
5

LPCG

0
0.0

0.8
1.6
Iterations

2.4
1e5

cache hit rate: 99.995%

Figure 29. LPCG vs. PCG on MIPLIB instances eilB101 and nw04 with quadratic loss functions. For the eilB101 instance, LPCG
spent most of the time tightening Φ0 , after which it converged very fast, while PCG was unable to complete a single iteration even solving
the problem only approximately. For the nw04 instance LPCG needed no more oracle calls after an initial phase, while significantly
outperforming PCG.

Lazifying Conditional Gradient Algorithms

LPCG
PCG

1000 2000 3000 4000
Wall-clock time (s)

1.0
0.8
0.6
0.4
0.2
0.0

LPCG
PCG

0

Number of LP calls

1.0
0.8
0.6
0.4
0.2
0.0

1

2
Iterations

3
1e5

LPCG

800
600
400
200
0

0

1

2
Iterations

cache hit rate: 99.9%

3
1e5

LPCG
PCG

7500 10000 12500 15000
Wall-clock time (s)

Function value

Function value

0

Function value

1.0
0.8
0.6
0.4
0.2
0.0

m100n500k4r1, 600 dimensions

Number of LP calls

Function value

disctom, 10000 dimensions

1.0
0.8
0.6
0.4
0.2
0.0
0.0

120000
100000
80000
60000
40000
20000
0
0.0

LPCG
PCG

0.3

0.6 0.9 1.2
Iterations 1e5
LPCG

0.3

0.6 0.9 1.2
Iterations 1e5

cache hit rate: 48.4%

Figure 30. LPCG vs. PCG on MIPLIB instances disctom and m100n500k4r1. After very fast convergence, there is a huge increase
in the number of oracle calls for the lazy algorithm LPCG due to reaching ε-optimality as explained before. On the right the initial bound
tightening for Φ0 took a considerable amount of time but then convergence is almost instantaneous.

Lazifying Conditional Gradient Algorithms

LCG param free
LCG

0.8
0.6
0.4
0.2

1.0
Function value

Function value

1.0

0.0
1000 2000 3000 4000
Wall clock time (s)

0.6
0.4
0.2

LCG param free
LCG

0.8
0.6
0.4
0.2

0

1.0
Function value

1.0
Function value

0.8

0.0
0

0.0

2000 4000 6000 8000
Wall clock time (s)
LCG param free
LCG

0.8
0.6
0.4
0.2
0.0

0

40
80
120
Iterations

0

80
60
40
LCG param free
LCG

20
0

200 400 600
Iterations

800

200

0

40
80
120
Iterations

Number of LP calls

100
Number of LP calls

LCG param free
LCG

150
100
50
0

LCG param free
LCG

0

200 400 600
Iterations

800

Figure 31. Comparison of the ‘textbook’ variant of the Lazy CG algorithm (Algorithm 2) vs. the Parameter-free Lazy CG (Algorithm 4)
depicted for two sample instances to demonstrate behavior. The parameter-free variant usually has a slighlty improved behavior in terms of
iterations and a significantly improved behavior in terms of wall-clock performance. In particular, the parameter-free variant can execute
significantly more oracle calls, due to the Φ-halving strategy and the associated bounded number of negative calls (see Theorem 4.3).

