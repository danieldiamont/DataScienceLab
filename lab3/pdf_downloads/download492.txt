On the Expressive Power of Deep Neural Networks

Appendix
Here we include the full proofs from sections in the paper.

A. Proofs and additional results from Section 2.1
Proof of Theorem 2
Proof. We show inductively that FA (x; W ) partitions the input space into convex polytopes via hyperplanes. Consider
(1)
the image of the input space under the first hidden layer. Each neuron vi defines hyperplane(s) on the input space:
(0)
(0)
(0)
letting Wi be the ith row of W (0) , bi the bias, we have the hyperplane Wi x + bi = 0 for a ReLU and hyperplanes
(0)
Wi x + bi = ±1 for a hard-tanh. Considering all such hyperplanes over neurons in the first layer, we get a hyperplane
arrangement in the input space, each polytope corresponding to a specific activation pattern in the first hidden layer.
Now, assume we have partitioned our input space into convex polytopes with hyperplanes from layers ≤ d − 1. Consider
(d)
(d)
vi and a specific polytope Ri . Then the
Pactivation pattern on layers ≤ d − 1 is constant on Ri , and so the input to vi
on Ri is a linear function of the inputs j λj xj + b and some constant term, comprising of the bias and the output of
saturated units. Setting this expression to zero (for ReLUs) or to ±1 (for hard-tanh) again gives a hyperplane equation,
but this time, the equation is only valid in Ri (as we get a different linear function of the inputs in a different region.) So
(d)
the defined hyperplane(s) either partition Ri (if they intersect Ri ) or the output pattern of vi is also constant on Ri . The
theorem then follows.
This implies that any one dimensional trajectory x(t), that does not ‘double back’ on itself (i.e. reenter a polytope it has
previously passed through), will not repeat activation patterns. In particular, after seeing a transition (crossing a hyperplane
to a different region in input space) we will never return to the region we left. A simple example of such a trajectory is a
straight line:
Corollary 1. Transitions and Output Patterns in an Affine Trajectory For any affine one dimensional trajectory x(t) =
x0 + t(x1 − x0 ) input into a neural network FW , we partition R 3 t into intervals every time a neuron transitions. Every
interval has a unique network activation pattern on FW .
Generalizing from a one dimensional trajectory, we can ask how many regions are achieved over the entire input – i.e. how
many distinct activation patterns are seen? We first prove a bound on the number of regions formed by k hyperplanes in
Rm (in a purely elementary fashion, unlike the proof presented in (Stanley, 2011))
Theorem 5. Upper Bound on Regions in a Hyperplane Arrangement Suppose we have k hyperplanes in Rm - i.e. k
equations of form αi x = βi . for αi ∈ Rm , βi ∈ R. Let the number of regions (connected open sets bounded on some sides
by the hyperplanes) be r(k, m). Then
m  
X
k
r(k, m) ≤
i
i=0
Proof of Theorem 5
Proof. Let the hyperplane arrangement be denoted H, and let H ∈ H be one specific hyperplane. Then the number of
regions in H is precisely the number of regions in H − H plus the number of regions in H ∩ H. (This follows from the
fact that H subdivides into two regions exactly all of the regions in H ∩ H, and does not affect any of the other regions.)
In particular, we have the recursive formula
r(k, m) = r(k − 1, m) + r(k − 1, m − 1)
We now induct on k + m to assert the claim. The base cases of r(1, 0) = r(0, 1) = 1 are trivial, and assuming the claim

On the Expressive Power of Deep Neural Networks

for ≤ k + m − 1 as the induction hypothesis, we have

m 
X
k−1

m−1
X


k−1
r(k − 1, m) + r(k − 1, m − 1) ≤
+
i
i
i=0
i=0

 X




d−1
k−1
k−1
k−1
≤
+
+
0
i
i+1
i=0
  m−1
X k 
k
≤
+
0
i+1
i=0
where the last equality follows by the well known identity
  
 

a
a
a+1
+
=
b
b+1
b+1
This concludes the proof.
With this result, we can easily prove Theorem 1 as follows:
Proof of Theorem 1
Proof. First consider the ReLU case. Each neuron has one hyperplane associated with it, and so by Theorem 5, the first
hidden layer divides up the inputs space into r(k, m) regions, with r(k, m) ≤ O(k m ).
Now consider the second hidden layer. For every region in the first hidden layer, there is a different activation pattern in
the first layer, and so (as described in the proof of Theorem 2) a different hyperplane arrangement of k hyperplanes in an
m dimensional space, contributing at most r(k, m) regions.
In particular, the total number of regions in input space as a result of the first and second hidden layers is ≤ r(k, m) ∗
r(k, m) ≤ O(k 2 m). Continuing in this way for each of the n hidden layers gives the O(k m n) bound.
A very similar method works for hard tanh, but here each neuron produces two hyperplanes, resulting in a bound of
O((2k)mn ).

B. Proofs and additional results from Section 2.2
Proof of Theorem 3
B.1. Notation and Preliminary Results
Difference of points on trajectory Given x(t) = x, x(t + dt) = x + δx in the trajectory, let δz (d) = z (d) (x + δx) − z (d) (x)
Parallel and Perpendicular Components: Given vectors x, y, we can write y = y⊥ + yk where y⊥ is the component of y
perpendicular to x, and yk is the component parallel to x. (Strictly speaking, these components should also have a subscript
x, but we suppress it as the direction with respect to which parallel and perpendicular components are being taken will be
explicitly stated.)
This notation can also be used with a matrix W , see Lemma 1.
Before stating and proving the main theorem, we need a few preliminary results.
Lemma 1. Matrix Decomposition Let x, y ∈ Rk be fixed non-zero vectors, and let W be a (full rank) matrix. Then, we
can write
W = k Wk + k W ⊥ + ⊥ Wk + ⊥ W⊥

On the Expressive Power of Deep Neural Networks

such that
k

y

⊥

W⊥ x = 0

T⊥

Wk = 0

y

W⊥ x = 0

T⊥

W⊥ = 0

i.e. the row space of W is decomposed to perpendicular and parallel components with respect to x (subscript on right),
and the column space is decomposed to perpendicular and parallel components of y (superscript on left).
Proof. Let V, U be rotations such that V x = (||x|| , 0..., 0)T and U y = (||y|| , 0...0)T . Now let W̃ = U W V T , and let
W̃ = k W̃k + k W̃⊥ + ⊥ W̃k + ⊥ W̃⊥ , with k W̃k having non-zero term exactly W̃11 , k W̃⊥ having non-zero entries exactly
W̃1i for 2 ≤ i ≤ k. Finally, we let ⊥ W̃k have non-zero entries exactly W̃i1 , with 2 ≤ i ≤ k and ⊥ W̃⊥ have the remaining
entries non-zero.
If we define x̃ = V x and ỹ = U y, then we see that
k

ỹ

⊥

W̃⊥ x̃ = 0

T⊥

W̃k = 0

ỹ

W̃⊥ x̃ = 0

T⊥

W̃⊥ = 0

as x̃, ỹ have only one non-zero term, which does not correspond to a non-zero term in the components of W̃ in the equations.
Then, defining k Wk = U T k W̃k V , and the other components analogously, we get equations of the form
k

W⊥ x = U T k W̃⊥ V x = U T k W̃⊥ x̃ = 0

Observation 1. Given W, x as before, and considering Wk , W⊥ with respect to x (wlog a unit vector) we can express
them directly in terms of W as follows: Letting W (i) be the ith row of W , we have

((W (0) )T · x)x


..
Wk = 

.


((W (k) )T · x)x
i.e. the projection of each row in the direction of x. And of course
W ⊥ = W − Wk
The motivation to consider such a decomposition of W is for the resulting independence between different components, as
shown in the following lemma.
Lemma 2. Independence of Projections Let x be a given vector (wlog of unit norm.) If W is a random matrix with
Wij ∼ N (0, σ 2 ), then Wk and W⊥ with respect to x are independent random variables.
Proof. There are two possible proof methods:
(a) We use the rotational invariance of random Gaussian matrices, i.e. if W is a Gaussian matrix, iid entries N (0, σ 2 ),
and R is a rotation, then RW is also iid Gaussian, entries N (0, σ 2 ). (This follows easily from affine transformation
rules for multivariate Gaussians.)
Let V be a rotation as in Lemma 1. Then W̃ = W V T is also iid Gaussian, and furthermore, W̃k and W̃⊥ partition
the entries of W̃ , so are evidently independent. But then Wk = W̃k V T and W⊥ = W̃⊥ V T are also independent.
(b) From the observation note that Wk and W⊥ have a centered multivariate joint Gaussian distribution (both consist of
linear combinations of the entries Wij in W .) So it suffices to show that Wk and W⊥ have covariance 0. Because
both are centered Gaussians, this is equivalent to showing E(< Wk , W⊥ >) = 0. We have that
E(< Wk , W⊥ >) = E(Wk W⊥T ) = E(Wk W T ) − E(Wk WkT )

On the Expressive Power of Deep Neural Networks

As any two rows of W are independent, we see from the observation that E(Wk W T ) is a diagonal matrix, with the
ith diagonal entry just ((W (0) )T · x)2 . But similarly, E(Wk WkT ) is also a diagonal matrix, with the same diagonal
entries - so the claim follows.

In the following two lemmas, we use the rotational invariance of Gaussians as well as the chi distribution to prove results
about the expected norm of a random Gaussian vector.
Lemma 3. Norm of a Gaussian vector Let X ∈ Rk be a random Gaussian vector, with Xi iid, ∼ N (0, σ 2 ). Then
√ Γ((k + 1)/2)
E [||X||] = σ 2
Γ(k/2)
Proof. We use the fact
√ that if Y is a random Gaussian, and Yi ∼ N (0, 1) then ||Y || follows a chi distribution. This means
that E(||X/σ||) = 2Γ((k + 1)/2)/Γ(k/2), the mean of a chi distribution with k degrees of freedom, and the result
follows by noting that the expectation in the lemma is σ multiplied by the above expectation.
We will find it useful to bound ratios of the Gamma function (as appear in Lemma 3) and so introduce the following
inequality, from (Kershaw, 1983) that provides an extension of Gautschi’s Inequality.
Theorem 6. An Extension of Gautschi’s Inequality For 0 < s < 1, we have


Γ(x + 1)
s 1−s
≤
≤
x+
2
Γ(x + s)


 1 !1−s
1
1 2
x− + s+
2
4

We now show:
Lemma 4. Norm of Projections Let W be a k by k random Gaussian matrix with iid entries ∼ N (0, σ 2 ), and x, y two
given vectors. Partition W into components as in Lemma 1 and let x⊥ be a nonzero vector perpendicular to x. Then
(a)
√


E ⊥ W⊥ x⊥  = ||x⊥ || σ 2

√
Γ(k/2)
≥ ||x⊥ || σ 2
Γ((k − 1)/2



k 3
−
2 4

1/2

(b) If 1A is an identity matrix with non-zeros diagonal entry i iff i ∈ A ⊂ [k], and |A| > 2, then
√


E 1A ⊥ W⊥ x⊥  ≥ ||x⊥ || σ 2

√
Γ(|A|/2)
≥ ||x⊥ || σ 2
Γ((|A| − 1)/2)



|A| 3
−
2
4

1/2

Proof. (a) Let U, V, W̃ be as in Lemma 1. As U, V are rotations, W̃ is alsoiid Gaussian.
Furthermore for any fixed W ,



with ã = V a, by taking inner products, and square-rooting, we see that W̃ ã = ||W a||. So in particular
i
h




E ⊥ W⊥ x⊥  = E ⊥ W̃⊥ x̃⊥ 
But from the definition of non-zero entries of ⊥ W̃⊥ , and the form of x̃⊥ (a zero entry in the first coordinate), it follows
2
that ⊥ W̃⊥ x̃⊥ has exactly k − 1 non zero entries, each a centered Gaussian with variance (k − 1)σ 2 ||x⊥ || . By Lemma
3, the expected norm is as in the statement. We then apply Theorem 6 to get the lower bound.
(b) First note we can view 1A ⊥ W⊥ = ⊥ 1A W⊥ . (Projecting down to a random (as W is random) subspace of fixed size
|A| = m and then making perpendicular commutes with making perpendicular and then projecting everything down
to the subspace.)
So we can view W as a random m by k matrix, and for x, y as in Lemma 1 (with y projected down onto m dimensions),
we can again define U, V as k by k and m by m rotation matrices respectively, and W̃ = U W V T , with analogous

On the Expressive Power of Deep Neural Networks

properties to Lemma 1. Now we can finish as in part (a), except that ⊥ W̃⊥ x̃ may have only m − 1 entries, (depending
2
on whether y is annihilated by projecting down by1A ) each of variance (k − 1)σ 2 ||x⊥ || .

Lemma 5. Norm and Translation Let X be a centered multivariate Gaussian, with diagonal covariance matrix, and µ a
constant vector.
E(||X − µ||) ≥ E(||X||)
Proof. The inequality can be seen intuitively geometrically: as X has diagonal covariance matrix, the contours of the pdf
of ||X|| are circular centered at 0, decreasing radially. However, the contours of the pdf of ||X − µ|| are shifted to be
centered around ||µ||, and so shifting back µ to 0 reduces the norm.
A more formal proof can be seen as follows: let the pdf of X be fX (·). Then we wish to show
Z
Z
||x − µ|| fX (x)dx ≥
||x|| fX (x)dx
x

x

Now we can pair points x, −x, using the fact that fX (x) = fX (−x) and the triangle inequality on the integrand to get
Z
Z
Z
(||x − µ|| + ||−x − µ||) fX (x)dx ≥
||2x|| fX (x)dx =
(||x|| + ||−x||) fX (x)dx
|x|

|x|

|x|

B.2. Proof of Theorem
(d)

We use vi to denote the ith neuron in hidden layer d. We also let x = z (0) be an input, h(d) be the hidden representation
at layer d, and φ the non-linearity. The weights and bias are called W (d) and b(d) respectively. So we have the relations
h(d) = W (d) z (d) + b(d) ,

z (d+1) = φ(h(d) ).

Proof. We first prove the zero bias case. To do so, it is sufficient to prove that

!d+1 
√
i


h
σk


 δz (0) (t)
E δz (d+1) (t) ≥ O  √
σ+k

(1)

(**)

as integrating over t gives us the statement of the theorem.
For ease of notation, we will suppress the t in z (d) (t).
We first write

(d)

(d)

W (d) = W⊥ + Wk

(d)

where the division is done with respect to z (d) . Note that this means h(d+1) = Wk z (d) as the other component annihilates
(maps to 0) z (d) .
(d+1)

We can also define AW (d) = {i : i ∈ [k], |hi

| < 1} i.e. the set of indices for which the hidden representation is not

k

saturated. Letting Wi denote the ith row of matrix W , we now claim that:

1/2 
i
 X
h
 




(d)
(d)
(d)
(d) 2 
EW (d) δz (d+1)  = EW (d) EW (d) 
((W
)
δz
+
(W
)
δz
)
i
i
⊥
k
 
⊥ 
k

i∈A

(*)

(d)
W
k

Indeed, by Lemma 2 we first split the expectation over W (d) into a tower of expectations over the two independent parts
of W to get
i
i
h
h




EW (d) δz (d+1)  = EW (d) EW (d) φ(W (d) δz (d) )
k

⊥

On the Expressive Power of Deep Neural Networks
(d)

But conditioning on Wk

in the inner expectation gives us h(d+1) and AW (d) , allowing us to replace the norm over
k

φ(W (d) δz (d) ) with the sum in the term on the right hand side of the claim.
(d)

(d)

Till now, we have mostly focused on partitioning the matrix W (d) . But we can also set δz (d) = δzk + δz⊥ where the
perpendicular and parallel are with respect to z (d) . In fact, to get the expression in (**), we derive a recurrence as below:
!
√
i
h
σk
 (d) 
√
EW (d) δz⊥ 
σ+k

i
h
 (d+1) 
EW (d) δz⊥
 ≥ O
To get this, we first need to define z̃ (d+1) = 1A

(d)
W
k

h(d+1) - the latent vector h(d+1) with all saturated units zeroed out.
(d+1)

We then split the column space of W (d) = ⊥ W (d) + k W (d) , where the split is with respect to z̃ (d+1) . Letting δz⊥
the part perpendicular to z (d+1) , and A the set of units that are unsaturated, we have an important relation:

be

Claim

 

 (d+1)  ⊥ (d) (d) 
 ≥  W δz 1A 
δz⊥
(where the indicator in the right hand side zeros out coordinates not in the active set.)
To see this, first note, by definition,
(d+1)

δz⊥

= W (d) δz (d) · 1A − hW (d) δz (d) · 1A , ẑ (d+1) iẑ (d+1)

(1)

where the ˆ· indicates a unit vector.
Similarly
⊥

W (d) δz (d) = W (d) δz (d) − hW (d) δz (d) , z̃ˆ(d+1) iz̃ˆ(d+1)

(2)

Now note that for any index i ∈ A, the right hand sides of (1) and (2) are identical, and so the vectors on the left hand side
agree for all i ∈ A. In particular,
(d+1)

δz⊥
· 1A = ⊥ W (d) δz (d) · 1A

 

 (d+1)   (d+1)

Now the claim follows easily by noting that δz⊥
· 1A .
 ≥ δz⊥
(d)

(d)

(d)

Returning to (*), we split δz (d) = δz⊥ + δzk , W⊥
cancellation, we have


i
 X
h



EW (d) δz (d+1)  = EW (d) EW (d) 
⊥ 
k
i∈A

(d)
W
k

(d)

(d)

(d)

= k W⊥ + ⊥ W⊥ (and Wk

analogously), and after some

1/2 

2  
(d)
(d)
(d)
(d)
(d)
(d)
 
(⊥ W⊥ + k W⊥ )i δz⊥ + (⊥ Wk + k Wk )i δzk
 


(d)

(d)

We would like a recurrence in terms of only perpendicular components however, so we first drop the k W⊥ , k Wk (which
can be done without decreasing the norm as they are perpendicular to the remaining terms) and using the above claim, have

i
 X
h
 (d+1) 

EW (d) δz⊥
 ≥ EW (d) EW (d) 
⊥ 
k
i∈A

(d)
W
k

1/2 

2  
(d)
(d)
(d)
(d)
 
(⊥ W⊥ )i δz⊥ + (⊥ Wk )i δzk
 


On the Expressive Power of Deep Neural Networks
(d)

(d)

(d)

But in the inner expectation, the term ⊥ Wk δzk is just a constant, as we are conditioning on Wk . So using Lemma 5
we have


1/2 
1/2 
 X 
 X 
2  
2  


(d)
(d)
(d)
(d)
(d)
(d)
⊥
⊥
⊥
 
 
(
W
)
δz
≥
E
(
W
)
δz
+
(
W
)
δz
EW (d) 
(d) 

i
i
i
⊥
⊥
⊥
⊥
k
k
 
 
W⊥ 
⊥ 

i∈A

i∈A

(d)
W
k

We can then apply Lemma 4 to get

 X

EW (d) 
⊥ 
i∈A

(d)
W
k

(d)
W
k

1/2 
q
2|AW (d) | − 3 h
i

2  
√
σ
k
 (d) 
(d)
(d)
 
√
(⊥ W⊥ )i δz⊥
≥
E δz⊥ 
2
 
2

k

The outer expectation on the right hand side only affects the term in the expectation through the size of the active set of
(d+1)
(d+1)
units. For ReLUs, p = P(hi
> 0) and for hard tanh, we have p = P(|hi
| < 1), and noting that we get a non-zero
norm only if |AW (d) | ≥ 2 (else we cannot project down a dimension), and for |AW (d) | ≥ 2,
k

k

√

q
2|AW (d) | − 3
k

2

2

1 q
|AW (d) |
≥√
k
2

we get


k  
i
i
h
h
p
1 X k j
σ
 (d) 
 (d+1) 
EW (d) δz⊥
p (1 − p)k−j √
j  E δz⊥ 
 ≥ √
2 j=2 j
k
We use the fact that we have the probability mass function for an (k, p) binomial random variable to bound the

√

j term:

 
k  
k  
X
X
k j
k
σ p
σ
k j
σ p
p (1 − p)k−j √
j=−
j
p (1 − p)k−j √
p(1 − p)k−1 √ +
j
1
k
k j=0 j
k
j=2


k
√
σ X 1 k − 1 j−1
√
= −σ kp(1 − p)k−1 + kp · √
p (1 − p)k−j
k j=1 j j − 1
√
But by using Jensen’s inequality with 1/ x, we get


k
X
1 k − 1 j−1
√
p (1 − p)k−j ≥ qP
k
j
−
1
j
j=1

j=1

j

k−1
j−1

1
1
=p

(k − 1)p + 1
pj−1 (1 − p)k−j

where the last equality follows by recognising the expectation of a binomial(k − 1, p) random variable. So putting together,
we get
!
√
i
i
h
h
√
kp
1
 (d+1) 
 (d) 
EW (d) δz⊥
−σ kp(1 − p)k−1 + σ · p
E δz⊥ 
(a)
 ≥ √
2
1 + (k − 1)p
From here, we must analyse the hard tanh and ReLU cases separately. First considering the hard tanh case:
(d+1)

To lower bound p, we first note that as hi

is a normal random variable with variance ≤ σ 2 , if A ∼ N (0, σ 2 )

(d+1)

P(|hi

| < 1) ≥ P(|A| < 1) ≥

1
√
σ 2π

(b)

On the Expressive Power of Deep Neural Networks
2

where the last inequality holds for σ ≥ 1 and follows by Taylor expanding e−x
that p ≤ σ1 .

/2

around 0. Similarly, we can also show

So this becomes



√

k−1
i
i
h
h
√
1
1
1
σk


 E δz (d) 
q
−
k
1
−
E δz (d+1)  ≥  √ 
⊥
σ
2 (2π)1/4 σ √2π + (k − 1)
!
√
i
h
σk
 (d) 
=O √
E δz⊥ 
σ+k
Finally, we can compose this, to get


d+1
√

k−1
i
h
√
1
1
1
σk



q
E δz (d+1)  ≥  √ 
− k 1−
c · ||δx(t)||
σ
2 (2π)1/4 σ √2π + (k − 1)

(c)

with the constant c being the ratio of ||δx(t)⊥ || to ||δx(t)||. So if our trajectory direction is almost orthogonal to x(t)
(which will be the case for e.g. random circular arcs, c can be seen to be ≈ 1 by splitting into components as in Lemma 1,
and using Lemmas 3, 4.)
The ReLU case (with no bias) is even easier. Noting that for random weights, p = 1/2, and plugging in to equation (a), we
get
!
√
√
i
i
h
h
1
−σ k
k
 (d+1) 
 (d) 
EW (d) δz⊥
(d)
+σ· p
E δz⊥ 
 ≥ √
k
2
2
2(k + 1)
√ √
But the expression on the right hand side has exactly the asymptotic form O(σ k/ k + 1), and we finish as in (c).
Result for non-zero bias In fact, we can easily extend the above result to the case of non-zero bias. The insight is to
note that because δz (d+1) involves taking a difference between z (d+1) (t + dt) and z (d+1) (t), the bias term does not enter
at all into the expression for δz (d+1) . So the computations above hold, and equation (a) becomes
i
h
1
 (d+1) 
EW (d) δz⊥
 ≥ √
2
(d+1)

√

√

−σw kp(1 − p)k−1 + σw · p

(d+1) (d)

kp

1 + (k − 1)p

!

i
h
 (d) 
E δz⊥ 

(d+1)

For ReLUs, we require hi
= wi
zi + bi
> 0 where the bias and weight are drawn from N (0, σb2 ) and
2
N (0, σw ) respectively. But with p ≥ 1/4, this holds as the signs for w, b are purely random. Substituting in and working
through results in the same asymptotic behavior as without bias.
(d+1)

For hard tanh, not that as hi

2
is a normal random variable with variance ≤ σw
+ σb2 (as equation (b) becomes
(d+1)

P(|hi

| < 1) ≥ p

2
(σw

1
√
+ σb2 ) 2π

This gives Theorem 3

i
h
 (d+1) 
E δz
 ≥ O 


√
i
h
k
σw
 E δz (d) 
q
·
⊥
p
2 + σ 2 )1/4
(σw
2 + σ2 + k
b
σw
b

On the Expressive Power of Deep Neural Networks

Trajectory length growth with increasing depth

106

w50 scl3
w1000 scl3
w50 scl5
w500 scl5
w100 scl8
w700 scl8
w300 scl10
w700 scl10

105

Trajectory length

104

103

102

101

100

2

4

6

8
Network depth

10

12

14

Figure 12. The figure above shows trajectory growth with different initialization scales as a trajectory is propagated through a fully
connected network for MNIST, with Relu activations. Note that as described by the bound in Theorem 3 we see that trajectory growth
is 1) exponential
p in depth 2) increases with initialization scale and width, 3) increases faster with scale over width, as expected from σw
compared to k/(k + 1) in the Theorem.

Statement and Proof of Upper Bound for Trajectory Growth for Hard Tanh Replace hard-tanh with a linear
(d+1)
coordinate-wise identity map, hi
= (W (d) z (d) )i + bi . This provides an upper bound on the norm. We also then
recover a chi distribution with k terms, each with standard deviation σw1 ,
k2

i √ Γ ((k + 1)/2) σ 

h


w  (d) 
E δz (d+1)  ≤ 2

δz

1
Γ (k/2)
k2
1

k + 1 2  (d) 
≤ σw
δz  ,
k

(2)
(3)

where the second step follows from (Laforgia and Natalini, 2013), and holds for k > 1.
Proof of Theorem 4
Proof. For σb = 0:
Pk
(d)
(d−1) (d−1)
zi
. As we are in the large σ case,
For hidden layer d < n, consider neuron v1 . This has as input i=1 Wi1
(d−1)
(d−1)
(d−1)
we assume that |zi
| = 1. Furthermore, as signs for zi
and Wi1
are both completely random, we can also
(d−1)
(d)
(d−1)
(d−1)
if vi
transitioning (to
assume wlog that zi
= 1. For a particular input, we can define v1 as sensitive to vi
P
(d)
wlog −1) will induce a transition in node v1 . A sufficient condition for this to happen is if |Wi1 | ≥ | j6=i Wj1 |. But
P
X = Wi1 ∼ N (0, σ 2 /k) and j6=i Wj1 = Y 0 ∼ N (0, (k − 1)σ 2 /k). So we want to compute P(|X| > |Y 0 |). For ease of
computation, we instead look at P(|X| > |Y |), where Y ∼ N (0, σ 2 ).
But this is the same as computing P(|X|/|Y | > 1) = P(X/Y < −1) + P(X/Y > 1). But the ratio of two centered
independent
normals with variances σ12 , σ22 follows a Cauchy distribution, with parameter σ1 /σ2 , which in this case is
√
1/ k. Substituting this in to the cdf of the Cauchy distribution, we get that


√
|X|
2
P
> 1 = 1 − arctan( k)
|Y |
π

On the Expressive Power of Deep Neural Networks

Trajectory Length, k =32
σw2
σw2
σw2
σw2
σw2

104
103
102

Trajectory Length, σw2 =4
102

=0.5
=1
=4
=16
=64

Trajectory Length

Trajectory Length

105

101
100

101

100
0

2

4

6
Depth

8

10

12

(a)

0

2

4

6
Depth

8

10

12

(b)

Perturbation Growth

12

6

15
||δxd +1 ||
||δxd ||

||δxd +1 ||
||δxd ||

8
4
2

10

k =2
k =32
k =512

5
0

0
2
0

Perturbation Growth

20

σw2 =1
σw2 =16
σw2 =64

10

(c)

k =1
k =4
k =16
k =64
k =256

100

200

300 400
Width

500

5
0

600

50

100

150

200

250

300

σw2

(d)

Figure 13. The exponential growth of trajectory length with depth, in a random deep network with hard-tanh nonlinearities. A circular
trajectory is chosen between two random vectors. The image of that trajectory is taken at each layer of the network, and its length
2
measured. (a,b) The trajectory length vs. layer, in terms of the network width k and weight variance σw
, both of which determine its
growth rate. (c,d) The average ratio of a trajectory’s length in layer d + 1 relative to its length in layer d. The solid line shows simulated
data, while the dashed lines show upper and lower bounds (Theorem 3). Growth rate is a function of layer width k, and weight variance
2
σw
.

On the Expressive Power of Deep Neural Networks

Finally, using the
√identity arctan(x) + arctan(1/x) and the Laurent series for arctan(1/x), we can evaluate the right hand
side to be O(1/ k). In particular




|X|
1
P
>1 ≥O √
(c)
|Y |
k
√
This means that in expectation, any neuron in layer d will be sensitive to the transitions of k neurons in the layer below.
(d−1)
Using this, and the fact the while vi
might flip very quickly from say −1 to 1, the gradation in the transition ensures
(d−1)
that neurons in layer d sensitive to vi
will transition at distinct times, we get the desired growth rate in expectation as
follows:
(d)

Let T (d) be a random variable denoting the number of transitions in layer d. And let Ti

be a random variable denoting
the
 (d)  P h (d) i
number of transitions of neuron i in layer d. Note that by linearity of expectation and symmetry, E T
= i E Ti
=
h
i
(d)
kE T1
h
i
hP
i
h
i
(d+1)
(d)
(d)
Now, E T1
≥E
1
T
=
kE
1
T
where 1(1,i) is the indicator function of neuron 1 in layer d + 1
(1,i)
(1,1)
1
i
i
being sensitive to neuron i in layer d.
h
i
h
i


(d)
(d)
But by the independence of these two events, E 1(1,1) T1
= E 1(1,1) · E T1 . But the firt time on the right hand
h
i √ h
i
√
(d+1)
(d)
≥ kE T1 .
side is O(1/ k) by (c), so putting it all together, E T1

 √ 

Written in terms of the entire layer, we have E T (d+1) ≥ kE T (d) as desired.
For σb > 0:

√
p
2 ), by noting that Y ∼ N (0, σ 2 + σ 2 ). This results in a growth rate of form
We replace
k with k(1 + σb2 /σw
w
b
q
√
2
σ
O( k/ 1 + σ2b ).
w

B.3. Dichotomies: a natural dual
Our measures of expressivity have mostly concentrated on sweeping the input along a trajectory x(t) and taking measures
of FA (x(t); W ). Instead, we can also sweep the weights W along a trajectory W (t), and look at the consequences (e.g.
binary labels – i.e. dichotomies), say for a fixed set of inputs x1 , ..., xs .
In fact, after random initialization, sweeping the first layer weights is statistically very similar to sweeping the input along
a trajectory x(t). In particular, letting W 0 denote the first layer weights, for a particular input x0 , x0 W 0 is a vector, each
2
coordinate is iid, ∼ N (0, ||x0 ||2 σw
). Extending this observation, we see that (providing norms are chosen appropriately),
0
0
x0 W cos(t) + x1 W sin(t) (fixed x0 , x1 , W ) has the same distribution as x0 W00 cos(t) + x0 W10 sin(t) (fixed x0 , W00 , W10 ).
So we expect that there will be similarities between results for sweeping weights and for sweeping input trajectories, which
we explore through some synthetic experiments, primarily for hard tanh, in Figures 15, 16. We find that the proportionality
of transitions to trajectory length extends to dichotomies, as do results on the expressive power afforded by remaining
depth.
For non-random inputs and non-random functions, this is a well known question upper bounded by the Sauer-Shelah lemma
(Sauer, 1972). We discuss this further in Appendix ??. In the random setting, the statistical duality of weight sweeping
and input sweeping suggests a direct proportion to transitions and trajectory length for a fixed input. Furthermore, if the
xi ∈ S are sufficiently uncorrelated (e.g. random) class label transitions should occur independently for each xi Indeed,
we show this in Figure 14.

C. Addtional Experiments from Section 3
Here we include additional experiments from Section 3

On the Expressive Power of Deep Neural Networks

Dichotomies vs. Remaining Depth
k =2
k =8
k =32

Unique Patterns

104

(a)

k =128
k =512

104

103
102
101
100 0

Dichotomies vs. Width

105
Unique Patterns

105

103
102
101

2

4

100 0

6 8 10 12 14 16 18
Remaining Depth dr

100

200

dr =1
dr =3
dr =5
dr =7
dr =9

300 400
Width k

dr =11
dr =13
dr =15
dr =17
500

(b)

Figure 14. We sweep the weights W of a layer through a trajectory W (t) and count the number of labellings over a set of datapoints.
When W is the first layer, this is statistically identical to sweeping the input through x(t) (see Appendix). Thus, similar results are
observed, with exponential increase with the depth of an architecture, and much slower increase with width. Here we plot the number
of classification dichotomies over s = 15 input vectors achieved by sweeping the first layer weights in a hard-tanh network along a
one-dimensional great circle trajectory. We show this (a) as a function of depth for several widths, and (b) as a function of width for
2
several depths. All networks were generated with weight variance σw
= 8, and bias variance σb2 = 0.

600

On the Expressive Power of Deep Neural Networks

Unique Dichotomies

105
104
103

Dichotomies vs. Remaining Depth
Layer swept = 1
Layer swept = 4
Layer swept = 8
Layer swept = 12
All dichotomies

102
101 0

2

4

6 8 10 12
Remaining Depth dr

14

16

Figure 15. Expressive power depends only on remaining network depth. Here we plot the number of dichotomies achieved by sweeping
the weights in different network layers through a 1-dimensional great circle trajectory, as a function of the remaining network depth.
The number of achievable dichotomies does not depend on the total network depth, only on the number of layers above the layer swept.
2
All networks had width k = 128, weight variance σw
= 8, number of datapoints s = 15, and hard-tanh nonlinearities. The blue dashed
s
line indicates all 2 possible dichotomies for this random dataset.

On the Expressive Power of Deep Neural Networks

Unique Dichotomies

105

Dichotomies vs. Transitions
k =2
k =8
k =32
k =128
k =512

104
103

All dichotomies
Random walk

102
101
100 0
10

101

102
103
Transitions

104

105

Figure 16. Here we plot the number of unique dichotomies that have been observed as a function of the number of transitions the network
has undergone. Each datapoint corresponds to the number of transitions and dichotomies for a hard-tanh network of a different depth,
with the weights in the first layer undergoing interpolation along a great circle trajectory W (0) (t). We compare these plots to a random
walk simulation, where at each transition a single class label is flipped uniformly at random. Dichotomies are measured over a dataset
2
consisting of s = 15 random samples, and all networks had weight variance σw
= 16. The blue dashed line indicates all 2s possible
dichotomies.

On the Expressive Power of Deep Neural Networks

Train Accuracy Against Epoch

0.6

Test Accuracy Against Epoch
lay 2
lay 3
lay 4
lay 5
lay 6
lay 7
lay 8

Accuracy

0.5

0.4

0.3

0.2
0

100

200
300
Epoch Number

400

5000

100

200
300
Epoch Number

400

500

Figure 17. We repeat a similar experiment in Figure 7 with a fully connected network on CIFAR-10, and mostly observe that training
lower layers again leads to better performance, although, as expected, overall performance is impacted by training only a single layer.
2
The networks had width k = 200, weight variance σw
= 1, and hard-tanh nonlinearities. We again only train from the second hidden
layer on so that the number of parameters remains fixed. The theory only applies to training error (the ability to fit a function), and
generalisation accuracy remains low in this very constrained setting.

On the Expressive Power of Deep Neural Networks

Figure 18. Training increases the trajectory length for smaller initialization values of σw . This experiment plots the growth of trajectory
length as a circular interpolation between two MNIST datapoints is propagated through the network, at different train steps. Red indicates
the start of training, with purple the end of training. We see that the training process increases trajectory length, likely to increase the
expressivity of the input-output map to enable greater accuracy.

