Multi-task Learning with Labeled and Unlabeled Tasks
Supplementary Material

Anastasia Pentina 1 Christoph H. Lampert 1

1. Preliminaries
In this section we list a few results from the literature that will be utilized in the proof of Theorem 1.
Proposition 1 (Lemma 1 in (Ben-David et al., 2010)). Let d be the VC dimension of the hypothesis set H and S1 , S2 be
two i.i.d. samples of size n from D1 and D2 respectively. Then for any Î´ > 0 with probability at least 1 âˆ’ Î´:
r
2d log(2n) + log(2/Î´)
.
disc(D1 , D2 ) â‰¤ disc(S1 , S2 )+2
n
Lemma 1 (Theorem 1 in (Maurer, 2006)). Let X1 , . . . , Xn be independent random variables taking values in the set X
and f be a function f : X n â†’ R. For any x = (x1 , . . . , xn ) âˆˆ X n and y âˆˆ X define:
xy,k = (x1 , . . . , xkâˆ’1 , y, xk+1 , . . . , xn )
(inf f )(x) = inf f (xy,k )
k

âˆ†+,f =

yâˆˆX
n
X

(f âˆ’ inf f )2 .

i=1

k

Then for t > 0:

Pr{f âˆ’ E f â‰¥ t} â‰¤ exp

âˆ’t2
2kâˆ†+ kâˆž


.

(1)

Lemma 2 (Corollary 6.10 in (McDiarmid, 1989)). Let W0n be a martingale with respect to a sequence of random variables
(B1 , . . . , Bn ). Let bn1 = (b1 , . . . , bn ) be a vector of possible values of the random variables B1 , . . . , Bn . Let
iâˆ’1
iâˆ’1
= biâˆ’1
= biâˆ’1
ri (biâˆ’1
1 , Bi = bi } âˆ’ inf {Wi : B1
1 , Bi = bi }.
1 ) = sup{Wi : B1
bi

bi

Let r2 (bn1 ) =

Pn

iâˆ’1 2
i=1 (ri (b1 ))

(2)

b2 = supbn r2 (bn ). Then
and R
1
1


22
.
Prn {Wn âˆ’ W0 > } < exp âˆ’
b2
B1
R

(3)

Lemma 3 (Originally (Hoeffding, 1963); in this form Theorem 18 in (Tolstikhin et al., 2014)). Let {U1 , . . . , Um } and
{W1 , . . . , Wm } be sampled uniformly from a finite set of d-dimensional vectors {v1 , . . . , vN } âŠ‚ Rd with and without
replacement respectively. Then for any continuous and convex function F : Rd â†’ R the following holds:
"
!#
"
!#
m
m
X
X
E F
Wi
â‰¤E F
Ui
(4)
i=1
1

i=1

IST Austria. Correspondence to: Anastasia Pentina <apentina@ist.ac.at>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplementary Material

Lemma 4 (Part of Lemma 19 in (Tolstikhin et al., 2014)). Let x = (x1 , . . . , xl ) âˆˆ Rl . Then the following function is
convex:
F (x) = sup xi .
(5)
i=1...l

2. Proof of Theorem 1
We start with bounding the multi-task error by the errors on the source tasks, and transition to empirical quantities while
keeping the effect of random sampling controlled.
Fix a subset of labeled tasks I = {i1 , . . . , ik }, a task hDt , ft i and a weight vector Î± âˆˆ Î›I . Let hâˆ—i âˆˆ arg minhâˆˆH (ert (h) +
eri (h)).1 Writing `(h, h0 ) as shorthand for `(h(x), h0 (x)), we have
X
 X 



Î±i  eri (h) âˆ’ ert (h)
| erÎ± (h) âˆ’ ert (h)| = 
Î±i eri (h) âˆ’ ert (h) â‰¤
â‰¤

(6)

iâˆˆI

iâˆˆI


X 
 
 

Î±i  eri (h) âˆ’ E `(h, hâˆ—i ) +  E `(h, hâˆ—i ) âˆ’ E `(h, hâˆ—i ) +  ert (h) âˆ’ E `(h, hâˆ—i ) = (âˆ—)
xâˆ¼Di

iâˆˆI

xâˆ¼Di

xâˆ¼Dt

xâˆ¼Dt

(7)
We can bound each summand:
| eri (h) âˆ’ E `(h, hâˆ—i )| â‰¤ eri (hâˆ— )
xâˆ¼Di

âˆ—
| E `(h, hi ) âˆ’ E `(h, hâˆ—i ) â‰¤ disc(Di , Dt )
xâˆ¼Di
xâˆ¼Dt

| ert (h) âˆ’ E `(h, hâˆ—i ) â‰¤ ert (hâˆ—i )
xâˆ¼Dt

where the first and the last inequalities hold by the triangular inequality for ` and the second one follows from the definition
of discrepancy. Therefore,
(âˆ—) â‰¤

X

Î±i (eri (hâˆ—i ) + disc(Di , Dt ) + ert (hâˆ—i )) =

iâˆˆI

X

Î±i (Î»it + disc(Di , Dt )).

(8)

iâˆˆI

Consequently, assuming that every task t has its own weights Î±t we obtain that:
T
T
T
T
1X
1X
1 XX t
1 XX t
ert (h) â‰¤
erÎ±t (ht ) +
Î±i disc(Dt , Di ) +
Î±i Î»ti .
T t=1
T t=1
T t=1
T t=1
iâˆˆI

(9)

iâˆˆI

We continue with bounding every expectation on the right hand side of (9) by its empirical counterpart.
2.1. Bound

1
T

PT

t=1

P

iâˆˆI

Î±it disc(Dt , Di )

We apply Proposition 1 to every summand and combine the results using a union bound argument. We obtain that with
probability at least 1 âˆ’ Î´/2 uniformly for all choices of I and Î±1 , . . . , Î±T âˆˆ Î›I :
T
T
1 XX t
1 XX t
Î±i disc(Dt , Di ) â‰¤
Î±i disc(St , Si ) + 2
T t=1
T t=1
iâˆˆI

2.2. Bound

1
T

PT

t=1

iâˆˆI

r

2d log(2n) + log(4T 2 /Î´)
.
n

erÎ±t (ht )

Now we upper-bound the error term in two steps.
1

If the minimum is not attained, the same inequality follows by an argument of arbitrary close approximation.

(10)

Supplementary Material

2.2.1. R ELATE

1
T

PT

t=1

erÎ±t (ht ) TO

1
T

PT

t=1

er
Ëœ Î±t (ht )

We start with relating the multi-task error to the hypothetical empirical error, if the learner would receive labels for all
examples in the selected labeled tasks:
X
(11)
er
Ëœ Î± (h) =
Î±i er
b Siu (h)
iâˆˆI

for
n

er
b Siu (h) =

1X
`(h(xij ), fi (xij )).
n j=1

(12)

Clearly, if m = n this part is not necessary and we can avoid the resulting complexity terms.
Because the choice of the tasks to label, I, their weights, Î± = (Î±1 , . . . , Î±T ), and the predictors, h = (h1 , . . . , hT ), all
depend on the unlabeled data, we aim for a bound that is holds simultaneous for all choices of these quantities, under the
condition that I and Î± depend only on the unlabeled samples, while h can be chosen based also on the labeled subsets.
Our main tool is a refined version of McDiarmidâ€™s inequality, due to Maurer (Maurer, 2006) (Lemma 1), which allows us
to make use of the internal structure of the weights, Î±, while deriving a large deviation bound.
For any S = (S1u , . . . , STu ) define:
Î¨(S) =

sup

sup

sup

I={i1 ,...,ik } Î±1 ,...,Î±n âˆˆÎ›I h1 ,...,hT

T
T
1 XX t
Î± (eri (ht ) âˆ’ er
b Siu (ht )) = sup sup sup g(Î±, h, S)
T t=1 i=1 i
Î±
I
h

(13)

for
g(Î±, h, S) =

T X
n
X
i=1 j=1

!
T
1 X t
i
i
Î± (eri (ht ) âˆ’ `(ht (xj ), ft (xj ))) .
T n t=1 i

(14)

For notational simplicity we will sometimes think
of every Stu as a set of pairs (xti , yit ), where yit = ft (xti ). To apply
P P
Lemma 1 we establish a bound on âˆ†+,Î¨ (S) = i j (Î¨(S) âˆ’ Î¨ij (S))2 , with
Î¨ij (S) = inf sup sup g(Î±, h, S \ {(xij , yji )} âˆª {(x, y)},
(x,y) Î±

(15)

h

i.e. the possible smallest value for Î¨ when changing only the data point (xij , yji ). Let Î±âˆ— , hâˆ— be the point where the sup in
the (13) is attained2 , i.e. Î¨(S) = g(Î±âˆ— , hâˆ— , S). Then:
Î¨ij (S) â‰¥ inf g(Î±âˆ— , hâˆ— , S \ {(xij , yji )} âˆª {(x, y)} )

(16)

(x,y)

and therefore
Î¨(S) âˆ’ Î¨ij (S) â‰¤ g(Î±âˆ— , hâˆ— , S) âˆ’ inf g(Î±âˆ— , hâˆ— , S \ {(xij , yji )} âˆª {(x, y)})

(17)

(x,y)

T

T

1 X âˆ—t
1 X âˆ—t
Î±i (âˆ’`(hâˆ—t (xij ), yji ) + `(hâˆ—t (x), y)) â‰¤
Î± ,
â‰¤ sup
T n t=1 i
(x,y) T n t=1

(18)

where for the last inequality we use that ` is bounded in [0, 1]. Because also Î¨(S) âˆ’ Î¨ij (S) â‰¥ 0, we obtain
n
T X
n
T X
X
X
2
âˆ†+,Î¨ (S) =
(Î¨(S) âˆ’ Î¨ij (S)) â‰¤
i=1 j=1
2

i=1 j=1

1
T 2 n2

T
X
t=1

!2
Î±iâˆ—t

â‰¤

1
T 2n

T X
T
X
i=1 t=1

!2
Î±iâˆ—t

=

1
,
n

If the supremum is not attained the subsequent inequality still follows from an argument of arbitrarily close approximation.

(19)

Supplementary Material

(remember that

P

i

Î±i = 1 for any Î± âˆˆ Î›I ). Therefore, according to Lemma 1 with probability at least 1 âˆ’ Î´/4:
r
2
4
Î¨(S) â‰¤ E Î¨(S) +
log .
n
Î´

(20)

To bound ES Î¨(S) we use symmetrization and Rademacher variables, Ïƒij :
!
T
1 X t
i
i
Î± (eri (ht ) âˆ’ `(ht (xj ), yj ))
E Î¨(S) = E sup
sup
sup
S
S I Î±1 ,...,Î±T âˆˆÎ›I h1 ,...,hT
T n t=1 i
i=1 j=1
!
T
T X
n
X
Ïƒij X t
i
i
â‰¤ 2 E E sup
sup
Î± `(ht (xj ), yj )
sup
S Ïƒ I Î±1 ,...,Î±T âˆˆÎ›I h1 ,...,hT
T n t=1 i
i=1 j=1
T X
n
X

â‰¤ 2EE
S Ïƒ

T
T
T X
n
X
1X
Ïƒij Î±it X
sup
`(ht (xij ), yji )
T t=1 Î±t âˆˆÎ›,ht i=1 j=1 n t=1

â‰¤ 2 E E sup
S Ïƒ

T X
n
X
Ïƒij Î±i

n

Î±,h i=1 j=1

`(h(xij ), y i ),

(21)

(22)

(23)

(24)

where line (23) is obtained from line (22) by dropping the assumption of a common sparsity pattern between the Î±-s. Note
that the function inside the last sup is linear in Î± âˆˆ Î›, therefore supÎ± can be reduced to the sup over the corners of the
simplex, {(1, 0, . . . , 0), . . . , (0, . . . , 0, 1)}. At the same time, by Sauerâ€™s lemma, the number of different choices of h on
d
d
S is bounded by eTdn . Therefore, the total number of different choices in (24) is bounded by T enT
d âˆš . Furthermore,
for any choice of Î± and h, the norm of the T n-vector formed by the summands of (24) is bounded by 1/ n, because
!2
T X
T X
n
n 
n
T
2
X
X
X
X

1
Ïƒij Î±i
1
1
2
`(h(xij ), y i ) = 2
(25)
Î±i `(h(xij ), y i ) â‰¤ 2
Î±i
= .
n
n
n
n
i=1 j=1
i=1 j=1
j=1
i=1
Therefore, by Massartâ€™s lemma:
E sup
Ïƒ

n
T X
X
Ïƒil Î±i

n

Î±,h i=1 j=1

p
`(h(xil ), yli )

â‰¤

2(log T + d log(enT /d))
âˆš
.
n

(26)

Combining (20) and (26) we obtain that with probability at least 1 âˆ’ Î´/4 simultaneously for all choices of tasks to be
labeled, I, weights Î± and hypotheses h:
r
r
T
T
1X
1X
4
8(log T + d log(enT /d))
2
erÎ±t (ht ) â‰¤
er
Ëœ Î±t (ht ) +
+
log .
(27)
T t=1
T t=1
n
n
Î´
2.2.2. R ELATE

1
T

PT

t=1

er
b Î±t (ht ) TO

1
T

PT

t=1

er
Ëœ Î±t (ht )

Fix the unlabeled samples S1u , . . . , STu . This uniquely determines the chosen tasks I and the weights Î±1 , . . . , Î±T âˆˆ Î›I , so
the only remaining source of randomness is the uncertainty which subsets of the selected tasks are labeled.
For notational simplicity we pretend that exactly the first k tasks were selected, i.e. I = {1, . . . , k}. The general case can
be obtained by changing the indices in the proof from 1, . . . , k to i1 , . . . , ik .
To deal with the dependencies between the labeled data points we first note that any random labeled subset Sil =
(sÌ„i1 , . . . , sÌ„im ) can be described as the first m elements of a random permutation Zi = (z1i , . . . , zni ) over n elements that
correspond to the unlabeled sample Siu , i.e. sÌ„ij = (xÌ„ij , yÌ„ji ) = (xizi , yzi i ). With this notation and writing Z = (Z1 , . . . , Zk )
j

j

and `(h, zji ) = `(h(xÌ„ij ), yÌ„ji ) we define the following function
Î¦(Z) =

sup
h1 ,...,hT

T
k
T
n
m

X
1X
1 X t 1 X
1 X
er
Ëœ Î±t (ht ) âˆ’ er
b Î±t (ht ) = sup
Î±i
`(ht , zji ) âˆ’
`(ht , zji ) .
T t=1
n j=1
m j=1
h1 ,...,hT i=1 T t=1

Our main tool is McDiarmidâ€™s inequality (Lemma 2) for martingales.

(28)

Supplementary Material

Construct a martingale sequence
For this, we interpret Z = (z11 , z21 , . . . , znk ) as a sequence of kn dependent variables, z11 , . . . , zkn . For the sake of notational consistency we will keep using double indices, with the convention that the sample index, j = 1, . . . , n, runs
faster than the task index, i = 1, . . . , k. Segments of a sequence will be denoted by upper and lower double indices,
Ä±Ì„ïš¾Ì„
Ä±Ì„ïš¾Ì„
zij
= (zij , zi(j+1) , . . . , zÄ±Ì„ïš¾Ì„ ) for ij â‰¤ Ä±Ì„ïš¾Ì„ and zij
= âˆ… otherwise. We now create a martingale sequence using Doobâ€™s
construction (Doob, 1940):
ij
}.
(29)
Wij = E{Î¦(Z)| z11
Z

where here and in the following when taking expectations over Z it is silently assumed that the expectation is taken only
with respect to variables that are not conditioned on. Note that because of this convention, the expectations in (29) is only
with respect to zi(j+1) , . . . , zkn , so each Wij is a random variable of z11 , . . . , zij . In particular, W00 = EZ Î¦(Z) and
Wkn = Î¦(Z), and the in between sequence is a martingale with respect to z11 , . . . , zkn :


i(jâˆ’1)
i(jâˆ’1)
ij  i(jâˆ’1) 	
} z11
= E{Î¦(Z)|z11
} = Wi(jâˆ’1) .
(30)
E{ Wij |z11
} = E E{Î¦(Z)| z11
Z

Z

Z

Z

b2
Upper-bound R
b2 defined there.
In order to apply Lemma 2 we need an upper bound on the coefficient R
Let i âˆˆ {1, . . . , k} and j âˆˆ {1, . . . , n} be fixed and let Ï€ = (Ï€1 , . . . , Ï€k ) be specific permutations of n elements for which
in
we use the same index conventions as for Z. By Ïƒ and Ï„ will denote elements in Ï€i(j+1)
, i.e. Ïƒ and Ï„ do not occur in any
of the first j positions of the permutation Ï€i . Then
i(jâˆ’1)

rij (Ï€11

)=

i(jâˆ’1)

sup
in
ÏƒâˆˆÏ€i(j+1)

=

sup

{ Wij : z11
sup

h

i(jâˆ’1)

= Ï€11

, zij = Ïƒ} âˆ’

i(jâˆ’1)

E {Î¦(Ï€11

kn
in
in
zi(j+1)
ÏƒâˆˆÏ€i(j+1)
Ï„ âˆˆÏ€i(j+1)

i(jâˆ’1)

inf { Wij : z11

in
ÏƒâˆˆÏ€i(j+1)

i(jâˆ’1)

kn
, Ïƒ, zi(j+1)
)} âˆ’ E {Î¦(Ï€11
kn
zi(j+1)

i(jâˆ’1)

= Ï€11

, zij = Ïƒ}

i
kn
, Ï„, zi(j+1)
)} .

(31)

To analyze (31) further, recall that:
i(jâˆ’1)

E {Î¦(Ï€11

kn
zi(j+1)

X

kn
, Ïƒ, zi(j+1)
)} =

i(jâˆ’1)

Î¦(Ï€11

i(jâˆ’1)

kn
kn
kn
, Ïƒ, Ï€i(j+1)
) Ã— Pr( zi(j+1)
= Ï€i(j+1)
|z11

i(jâˆ’1)

= Ï€11

âˆ§ zij = Ïƒ ),

kn
Ï€i(j+1)

where here and in the following we use the convention that sums over parts of Ï€ run only over values that lead to valid
permutations. Because the permutations of different task are independent, this is equal to
X
i(jâˆ’1)
i(jâˆ’1)
i(jâˆ’1)
kn
in
in
kn
kn
=
Î¦(Ï€11
, Ïƒ, Ï€i(j+1)
Pr( zi(j+1)
= Ï€i(j+1)
|zi1
= Ï€i1
âˆ§ zij = Ïƒ ) Pr(z(i+1)1
= Ï€(i+1)1
)
(32)
kn
Ï€i(j+1)

ij
ij
in
into a
We make the following observation: for any fixed Ï€i1
and any Ï„ 6âˆˆ Ï€i1
, we can rephrase a summation over Ï€i(j+1)
sum over all positions where Ï„ can occur, and a sum over all configuration for the entries that are not Ï„ :

X

in
F (Ï€i(j+1)
)=

in
Ï€i(j+1)

n
X

X

X

i(lâˆ’1)

in
F (Ï€i(j+1) , Ï„, Ï€i(l+1)
)

l=j+1 Ï€ i(lâˆ’1) Ï€ in
i(l+1)
i(j+1)

for any function F . Applying this to the summation in (32), we obtain
X
i(jâˆ’1)
i(jâˆ’1)
i(jâˆ’1)
kn
in
in
= Ï€i(j+1)
|zi1
= Ï€i1
âˆ§ zij = Ïƒ )
Î¦(Ï€11
, Ïƒ, Ï€i(j+1)
) Pr( zi(j+1)
kn
Ï€i(j+1)

kn
kn
Ã— Pr(z(i+1)1
= Ï€(i+1)1
)=

n
X

X

X

l=j+1 Ï€ i(lâˆ’1) Ï€ kn
i(l+1)
i(j+1)

i(jâˆ’1)

Î¦(Ï€11

i(lâˆ’1)

kn
, Ïƒ, Ï€i(j+1) , Ï„, Ï€i(l+1)
)

(33)

Supplementary Material
i(lâˆ’1)

i(lâˆ’1)

i(jâˆ’1)

kn
kn
Ã— Pr( zi(j+1) = Ï€i(j+1) âˆ§ zi(l+1)
= Ï€i(l+1)
|z11
kn
kn
Ã— Pr(z(i+1)1
= Ï€(i+1)1
)=

E

E

n
Z
lâˆ¼Uj+1

i(jâˆ’1)
Î¦(Z|z11

i(jâˆ’1)

= Ï€11

=

i(jâˆ’1)
Ï€11

âˆ§ zij = Ïƒ âˆ§ zil = Ï„ )

âˆ§ zij = Ïƒ âˆ§ zil = Ï„ ),

n
where Uj+1
denotes the uniform distribution over the set {j + 1, . . . , n}. The analogue derivation can be applied to the
quantity in line (31) with Ïƒ and Ï„ exchanged.

For any Z denote by Zijâ†”il the permutation obtained by switching zij and zil . Then, due to the linearity of the expectation:
i(jâˆ’1)

rij (Ï€11

i(jâˆ’1)

) = sup { En E{Î¦(Z) âˆ’ Î¦(Zijâ†”il )|z11
Ïƒ,Ï„

lâˆ¼Uj+1 Z

i(jâˆ’1)

= Ï€11

, zij = Ïƒ, zil = Ï„ ).

(34)

From the definition of Î¦ we see that Î¦(Z) âˆ’ Î¦(Zijâ†”il ) = 0 when j, l âˆˆ {1, . . . , m} or j, l âˆˆ {m + 1 . . . , n}. Since
i(jâˆ’1)
l > j in (34) this implies rij (Ï€11
) = 0 for j âˆˆ {m + 1, . . . , n}. The only remaining cases are j âˆˆ {1, . . . , m} and
l âˆˆ {m + 1, . . . , n}, for which we obtain
Î¦(Z) âˆ’ Î¦(Zijâ†”il ) â‰¤

sup
h1 ,...,hT

T
T
1 X t
1X t1
Î±i (âˆ’`(ht , zji ) + `(ht , zli )) â‰¤
Î±.
T t=1 m
T m t=1 i

where for the first inequality we used that sup F âˆ’ sup G â‰¤ sup(F âˆ’ G) for any F, G, and for the second inequality we
PT
i(jâˆ’1)
1
t
3
used that ` is bounded by [0, 1]. Consequently, rij (Ï€11
) â‰¤ nâˆ’m
t=1 Î±i in this case. Therefore
nâˆ’j T m
b2

R =

n
k X
X

i(jâˆ’1) 2
rij (Ï€11
)

i=1 j=1

m
k
T
1 X  n âˆ’ m 2 X X t
â‰¤ 2 2
Î±i
T m j=1 n âˆ’ j
t=1
i=1

!2

k
T
1 X X t
â‰¤ 2
Î±
T m i=1 t=1 i

!2
.

(35)

Upper-bound EZ Î¦(Z)
The main tool here is Lemma 3. First we rewrite Î¦(Z) in the following way:
Î¦(Z) =

k
T
T
X
1X
1 X
Î±it (er
sup
b Siu (h) âˆ’ er
Î¦t (Z)
b Sil (h)) =
T t=1 h i=1
T m t=1

Î¦t (Z) = sup
h

k
X

m Î±it (er
b Siu (h) âˆ’ er
b Sil (h)).

i=1

Note that even though H can be infinitely large, we can identify a finite subset that represents all possible predictions of
hypothesis in H on S1u âˆª Â· Â· Â· âˆª Sku . We denote their number by L â‰¤ 2kn and the corresponding hypotheses by h1 , . . . , hL .
t
t
Let t âˆˆ {1, . . . , T } be fixed. For every i âˆˆ {1, . . . , k} define a set of n L-dimensional vectors, Vit = {vi1
, . . . , vin
}, where
for every j âˆˆ {1, . . . , n}:
h

i
t
vij
= Î±it er
Ëœ i (h1 ) âˆ’ `(h1 (xij ), yji ) , . . . , Î±it er
Ëœ i (hL ) âˆ’ `(hL (xij ), yji ) .
(36)

With this notation, for every i âˆˆ {1, . . . , k} choosing a random subset Sil âŠ‚ Siu corresponds to sampling m vectors from
Vit uniformly without replacement.
For every i âˆˆ {1, . . . , k}, let Ui = {ui1 , . . . , uim } be sampled from Vit in that way. Then
ï£«
ï£¶
k X
m
X
Î¦t (Z) = F ï£­
uij ï£¸ ,

(37)

i=1 j=1
3

We generously bound

nâˆ’m
nâˆ’j

â‰¤ 1 in this step. By keeping the corresponding factor in the analysis one obtains that the constant B in

the theorem can be improved at least by a factor of

(nâˆ’m)2
.
(nâˆ’0.5)(nâˆ’mâˆ’0.5)

Supplementary Material

where the function F takes as input an L-dimensional vector and returns the value of its maximum component. We now
bound EZ Î¦t (Z) by applying Lemma 3 k times:
E Î¦t (Z) =
Z

E

U1 ,...,Uk

F

k X
m
X

uij



(38)

i=1 j=1

ï£®
=

E

U1 ,...,Ukâˆ’1

ï£¹
m
m
h  kâˆ’1
i

XX
X

ï£°E F
uij +
ukj U1 , . . . , Ukâˆ’1 ï£»
Uk

i=1 j=1

(39)

j=1

By Lemma 4 F (x) is a convex function. Thus F (const + x) is also convex and we can apply Lemma 3 with respect to Uk .
ï£®
â‰¤

ï£«

kâˆ’1
m
XX

h

E

U1 ,...,Ukâˆ’1

ï£°E F ï£­
UÌ‚k

uij +

i=1 j=1

m
X

ï£¹

i

uÌ‚kj ï£¸ U1 , . . . , Ukâˆ’1 ï£»
ï£¶

(40)

j=1

where UÌ‚k = {uki , . . . , ukm } is a set of m vectors sampled from Vkt with replacement.
ï£® ï£«
=

m
kâˆ’1
XX

E

U1 ,...,Ukâˆ’1 ,UÌ‚k

ï£°F ï£­

uij +

m
X

ï£¶ï£¹
uÌ‚kj ï£¸ï£» .

(41)

j=1

i=1 j=1

Repeating the process k times, we obtain

â‰¤ Â·Â·Â· â‰¤

E

UÌ‚1 ,...,UÌ‚k

ï£¶ï£¹
ï£® ï£«
k X
m
X
ï£°F ï£­
uÌ‚ij ï£¸ï£» .

(42)

i=1 j=1

Note that writing the conditioning in the above expressions is just for clarity of presentation, since the U1 , . . . , Uk are
actually independent of each other.
Switching from the U sets by the UÌ‚ sets in Î¦ corresponds to switching from random subsets Sil to random sets SÌƒi consisting
of m points sampled from Siu uniformly with replacement. Therefore we obtain
E Î¦t (Z) =
Z

E

S1l ,...,Skl

Î¦t (S1l , . . . , Skl ) â‰¤

E

Î¦t (SÌƒ1 , . . . , SÌƒk ),

(43)

SÌƒ1 ,...,SÌƒk

which allows us to continue analyzing EZ Î¦t (Z) in the standard way using Rademacher complexities and independent
samples. Applying the common symmetrization trick and introducing Rademacher random variables Ïƒij we obtain
Î¦t (SÌƒ1 , . . . , SÌƒk ) â‰¤ 2 E sup
Ïƒ

h

We can rewrite this using the fact that `(y, y 0 ) = Jy 6= y 0 K =
E sup
Ïƒ

h

k X
m
X
i=1 j=1

Ïƒij Î±it `(h(xij ), yji ) = E sup
Ïƒ

h

k X
m
X
i=1 j=1

k X
m
X

Ïƒij Î±it `(h(xij ), yji ).

i=1 j=1
1âˆ’yy 0
2 :

Ïƒij Î±it

k X
m
X
1 âˆ’ h(xij )yji
1
= E sup
âˆ’Ïƒij yji Î±it h(xij )
2
2 Ïƒ h i=1 j=1

Since âˆ’Ïƒij yji has the same distribution as Ïƒij :

=

k X
m
X
1
E sup
Ïƒij aij (h),
2 Ïƒ a(h)âˆˆA i=1 j=1

Supplementary Material

where aij (h) = Î±it h(xij ) and A = {a(h) : h âˆˆ H}. According to Sauerâ€™s lemma (Corollary 3.3 in (Mohri et al., 2012)):

d
ekm
|A| â‰¤
.
(44)
d
At the same time:

v
v
u k
u k m
X
uX X
âˆš u
i
t
t
2
kak2 =
(Î±i h(xj )) = mt (Î±it )2 .
i=1 j=1

(45)

i=1

Therefore, by Massartâ€™s lemma (Theorem 3.3 in (Mohri et al., 2012)):
v
u k
k X
m
X
X
p
1u
t
i
i
E sup
Ïƒij Î±i `(h(xj ), yj ) â‰¤ t (Î±it )2 Â· 2dm log(ekm/d).
Ïƒ h
2 i=1
i=1 j=1

(46)

By applying this result for all t we obtain:
v
r
T
T
T u
k
uX
X
X
X
2d log(ekm/d)
1
1
1
t
t
2
(Î± ) Â·
E Î¦(Z) =
E Î¦t (Z) â‰¤
E Î¦t (SÌƒ) â‰¤
.
Z
T m t=1 Z
T m t=1 SÌƒ
T t=1 i=1 i
m

(47)

Combining (35) and (47) with Lemma 2 we obtain that for fixed unlabeled samples S1u , . . . , STu with probability at least
1 âˆ’ Î´/4 for all choices of h1 , . . . , hT :
r
r
T
T
1X
1
2d log(ekm/d)
1
log(4/Î´)
1X
er
Ëœ Î±t (ht ) â‰¤
er
b Î±t (ht ) + kÎ±k2,1
+ kÎ±k1,2
.
T t=1
T t=1
T
m
T
2m
By further combining it with (27) we obtain that the following inequality holds uniformly in h1 , . . . , hT âˆˆ H with probability at least 1 âˆ’ Î´/2 over the sampling of the unlabeled training sets, S1u , . . . , STu , and labeled training sets, (Sil )iâˆˆI ,
provided that the subset of labeled tasks, I âŠ‚ {1, . . . , T }, and the task weights, Î±1 , . . . , Î±T âˆˆ Î›I , depend deterministically
on the unlabeled training only.
r
r
T
T
1X
1X
1
1
2d log(ekm/d)
log(4/Î´)
erÎ±t (ht ) â‰¤
er
b Î±t (ht )+ kÎ±k2,1
+ kÎ±k1,2
T t=1
T t=1
T
m
T
2m
r
r
4
8(log T + d log(enT /d))
2
+
+
log .
(48)
n
n
Î´
The statement of Theorem 1 follows by combining (9) with (10) and (48).

References
Ben-David, Shai, Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira, Fernando, and Vaughan, Jennifer Wortman. A
theory of learning from different domains. Machine Learning, 2010.
Doob, Joseph L. Regularity properties of certain families of chance variables. Transactions of the American Mathematical
Society, 47(3), 1940.
Hoeffding, Wassily. Probability inequalities for sums of bounded random variables. Journal of the American Statistical
Association, 1963.
Maurer, Andreas. Concentration inequalities for functions of independent variables. Random Structures and Algorithms,
2006.
McDiarmid, Colin. On the method of bounded differences. In Surveys in Combinatorics, 1989.
Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar, Ameet. Foundations of Machine Learning. The MIT Press, 2012.
Tolstikhin, I., Blanchard, G., and Kloft, M. Localized complexities for transductive learning. In Workshop on Computational Learning Theory (COLT), 2014.

