Appendix A. Some Important Lemmas
In this section, we give several important lemmas which will be used in the proof of the
theorems of this paper.
Lemma 9 If A and B are d×d symmetric positive matrices, and (1−�0 )B � A � (1+�0 )B
where 0 < �0 < 1, then we have
�A1/2 B −1 A1/2 − I� ≤ �0 ,
where I is the identity matrix.
Proof Because A � (1 + �0 )B, we have z T [A − (1 + �0 )B]z ≤ 0 for any nonzero z ∈ Rd .
T Az
≤ 1 + �0 for any z �= 0. Subsequently,
This implies zzT Bz
λmax (B −1 A) =λmax (B −1/2 AB −1/2 )
uT B −1/2 AB −1/2 u
u�=0
uT u
z T Az
= max T
z�=0 z Bz
≤1 + �0 ,
= max

where the last equality is obtained by setting z = B −1/2 u. Similarly, we have λmin (B −1 A) ≥
1 − �0 . Since B −1 A and A1/2 B −1 A1/2 are similar, the eigenvalues of A1/2 B −1 A1/2 are all
between 1 − �0 and 1 + �0 . Therefore, we have
�A1/2 B −1 A1/2 − I� ≤ �0 .

Lemma 10 ([3]) Let X1 , X2 , . . . , Xk be independent, random, symmetric, real matrices
of
�
size d × d with 0 � Xi � LI, where I is the d × d identity matrix. Let Y = ki=1 Xi ,
µmin = λmin (E[Y ]) and µmax = λmax (E[Y ]). Then,
P (λmin (Y ) ≤ (1 − �)µmin ) ≤ d · e−�

2µ

min /L

Lemma 11 ([3]) Given a matrix A ∈ Rm×n , construct an m × n random matrix R such
that
E[R] = A and �R� ≤ L.
Compute the per-sample second moment:
M = max{�E[RRT ]�, E[RT R]�}.
Form the matrix sampling estimator
s

R̄ =

1�
Ri , where each Ri is an independent copy of R.
s
i=1

1

Then, for all t ≥ 0
�

�
P �R̄ − A� ≥ t ≤ (m + n) exp

�

−st2 /2
M + 2Lt/3

�

.

Lemma 12 Assume (9) and (10) hold. Let 0 < δ < 1, 0 < � < 1 and 0 <�
c be given. If we
1
(t)
sample fi ’s uniformly with the sample size |S| and construct H = |S| j∈S ∇2 fj (x(t) ),
then we have the following results:
2
, it holds that
(a) If |S| ≥ 16K clog(2d/δ)
2 �2
�H (t) − ∇2 F (x(t) )� ≤ �c.
(b) If |S| ≥

K log(2d/δ)
,
σ�2

it holds that
λmin (H (t) ) ≥ (1 − �)σ.

�
�
(t)
(t)
Proof Consider |S| i.i.d random matrces Hj , j = 1, . . . , |S| such that P Hj = ∇2 fi (x(t) ) =
(t)

1/n for all i = 1, . . . , n. Then, we have E(Hj ) = ∇2 F (x(t) ) for all j = 1, . . . , |S|. By (9)
(t)

(t)

(t)

and the positive semi-deﬁnite property of Hj , we have λmax (Hj ) ≤ K and λmin (Hj ) ≥ 0.

By Lemma 10, we have that if |S| ≥ K log(d/δ)
, λmin (H (t) ) ≥ (1 − �)σ holds with probability
σ�2
at least 1 − δ.
(t)
We deﬁne random maxtrices Xj = Hj − ∇2 F (x(t) ) for all j = 1, . . . , |S|. We have
E[Xj ] = 0, �Xj � ≤ 2K and �Xj �2 ≤ 4K 2 . By Lemma 11, we have
P(�H (t) − ∇2 F (x(t) )� ≥ �c) ≤ 2d exp−
When |S| ≥

16K 2 log(2d/δ)
,
c2 � 2

c2 �2 |S|
16K 2

.

�H (t) − ∇2 F (x(t) )� ≤ �c holds with probability at least 1 − δ.

Appendix B. Proofs of theorems of Section 3
Proof of Theorem 3 By Assumption 1 and 2, we have that F (x) is µ-strongly convex
and ∇F (x) is L-Lipschitz continuous. Hence, we have
µ ≤ λmin (∇2 F (x)) ≤ λmax (∇2 F (x)) ≤ L.
Hence, for any x in domain, it holds that
κ=

L
≥ κ(∇2 F (x)).
µ
2

By Taylor’s theorem, we obtain
∇F (x(t+1) )
=∇F (x(t) ) + ∇2 F (x(t) )(−p(t) ) +

�

1
0

[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds

=∇F (x(t) ) − ∇2 F (x(t) )[H (t) ]−1 ∇F (x(t) ) + ∇2 F (x(t) )[H (t) ]−1 ∇F (x(t) ) − ∇2 F (x(t) )p(t)
� 1
+
[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds
0

�− 1
��
1
1
2
∇F (x(t) )
I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 ∇2 F (x(t) )
� 1
2
(t)
(t) −1
(t)
(t)
[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds.
+ ∇ F (x )([H ] ∇F (x ) − p ) +

�

= ∇2 F (x(t) )

�1 �
2

0

Hence, we have the following identity
�

∇2 F (x(t) )

�− 1
2

�
�− 1
��
1
1
2
∇F (x(t+1) ) = I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 ∇2 F (x(t) )
∇F (x(t) )
1

+ [∇2 F (x(t) )] 2 ([H (t) ]−1 ∇F (x(t) ) − p(t) )
� 1
1
[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds.
+ [∇2 F (x(t) )]− 2
0

�−1
�
�−1
�
and M ∗ = ∇2 F (x∗ ) . Then we
For notational simplicity, we denote M = ∇2 F (x(t) )
can obtain
�
�
1
1�
�
�∇F (x(t+1) )�M ≤ �I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 � �∇F (x(t) )�M
1

+ �[∇2 F (x(t) )] 2 ��[H (t) ]−1 ∇F (x(t) ) − p(t) �
� 1
2
(t) − 12
�∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )��p(t) �ds.
+ �[∇ F (x )] �
0

We bound the three terms on the right-hand side of the above equation respectively.
For the ﬁrst term, using Lemma 9, we have
�
�
1
1�
�
�I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 � �∇F (x(t) )�M ≤ �0 �∇F (x(t) )�M .

For the second term, by the fact that �AB� ≥ �A�σmin (B) and condition
�∇F (x(t) ) − H (t) p(t) � ≤

�1
�1
�∇F (x(t) )� ≤
�∇F (x(t) )�,
2
κ
κ(∇ F (x(t) ))
3

we obtain
1

�[∇2 F (x(t) )] 2 ��[H (t) ]−1 ∇F (x(t) ) − p(t) �
1

=

�[∇2 F (x(t) )] 2 �

λmin

1

1

([∇2 F (x(t) )]− 2 )

λmin ([∇2 F (x(t) )]− 2 )�[H (t) ]−1 ��∇F (x(t) ) − H (t) p(t) �
1

1
�[∇2 F (x(t) )] 2 �
�1
(t) −1
] �(λmin ([∇2 F (x(t) )]− 2 )�∇F (x(t) )�)
1 �[H
(t)
2
κ(∇ F (x )) λmin ([∇2 F (x(t) )]− 2 )
�1
≤
�∇2 F (x(t) )��[H (t) ]−1 ��∇F (x(t) )�M
κ(∇2 F (x(t) ))
�1
≤
�∇F (x(t) )�M .
1 − �0

≤

For the third term, we bound it for the case that ∇2 F (x) is not Lipschitz continuous
and the case ∇2 F (x) is Lipschitz continuous respectively.
First, we consider the case that ∇2 F (x) is not Lipschitz continuous but is continuous
close to the optimal point x∗ . Because ∇2 F (x) is continuous near x∗ , there exists a suﬃcient
small value γ such that it holds that
�[∇2 F (x∗ )]−1 − [∇2 F (x(t) )]−1 � <

ν(t)
,
L

(14)

and
η(t)µ
�∇2 F (x∗ ) − ∇2 F (x(t) )� < √ ,
κ

(15)

when �x(t) − x∗ � ≤ γ. Therefore, ν(t) and η(t) will go to 0 as x(t) goes to x∗ .
By µ-strong convexity, we have �[∇2 F (xt )]−1 � ≤ µ1 for all x(t) suﬃciently close to x∗ .
Because of Eqn. (2), we have
�[H (t) ]−1 � ≤ (1 + �0 )�[∇2 F (xt )]−1 � ≤

1
.
(1 − �0 )µ

We deﬁne r(t) = ∇F (x(t) ) − H (t) p(t) . Then we have that the direction vector satisﬁes
�p(t) � = �[H (t) ]−1 �(�r(t) � + �∇F (x(t) )�) ≤

2
�∇F (x(t) )�,
(1 − �0 )µ

where the second inequality is because
�r(t) � = �∇F (x(t) ) − H (t) p(t) � ≤
4

�1
�∇F (x(t) )� ≤ �∇F (x(t) )�.
κ

(16)

Hence, with �x − x∗ � ≤ γ, combining condition (15), we have
1

�[∇2 F (x(t) )]− 2 �

�

1

�∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )��p(t) �ds
0
� 1
µη(t) (t)
2
(t) − 12
√ �p �ds
≤ �[∇ F (x )] �
κ
0
1
µη(t)
2
√ �∇F (x(t) )�
≤ �[∇2 F (x(t) )]− 2 �
(1 − �0 )µ κ
1

�[∇2 F (x(t) )]− 2 �
2η(t)
2
(t) − 12
≤
)] )�∇F (x(t) )�
√
1 λmin ([∇ F (x
−
1 − �0 κλmin ([∇2 F (x(t) )] 2 )

≤

2η(t)
�∇F (x(t) )�M .
1 − �0

Therefore, we have
�1
2η(t)
�∇F (x(t) )�M +
�∇F (x(t) )�M
�∇F (x(t+1) )�M ≤�0 �∇F (x(t) )�M +
1 − �0
1 − �0
�
�
�1
2η(t)
�∇F (x(t) )�M .
= �0 +
+
1 − �0 1 − �0
Now, we show the relationship between � · �M and � · �M ∗ . By Eqn. (14), we have
−

ν(t)
ν(t)
uT u ≤ uT ([∇2 F (x∗ )]−1 − [∇2 F (x(t) )]−1 )u ≤
uT u,
λmax (∇2 F (x∗ ))
λmax (∇2 F (x∗ ))

for any nonzero u ∈ Rd , which implies that
(1 − ν(t))uT [∇2 F (x(t) )]−1 u ≤ uT [∇2 F (x∗ )]−1 u ≤ (1 + ν(t))uT [∇2 F (x(t) )]−1 u.
That is,
(1 − ν(t))�u�M ≤ �u�M ∗ ≤ (1 + ν(t))�u�M .
By this relationship between � · �M and � · �M ∗ , we get
�∇F (x

(t+1)

)�

M∗

≤

�

�1
2η(t)
�0 +
+
1 − �0 1 − �0

�

1 + ν(t)
�∇F (x(t) )�M ∗
1 − ν(t)

Second, we consider the case that ∇2 F (x) is Lipschitz continuous with parameter L̂.
We have that the direction vector satisﬁes
�p(t) � ≤

2
�∇F (x(t) )�.
(1 − �0 )λmin (∇2 F (x(t) ))
5

Because ∇2 F (x) is Lipschitz continuous with parameter L̂, we have
� 1
2
(t) − 12
�∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )��p(t) �ds
�[∇ F (x )] �
0
� 1
2
(t) − 12
sL̂�p(t) �2 ds
≤ �[∇ F (x )] �
0

1
1
L̂
2
(t) − 12
= �[∇2 F (x(t) )]− 2 �λ−2
)λ2min ([∇2 F (x(t) )]− 2 )�p(t) �2
min ([∇ F (x )]
2
�
�2
1
2
L̂
2
(t) − 12
≤ �[∇2 F (x(t) )]− 2 �λ−2
([∇
F
(x
)]
)
�∇F (x(t) )�2M
min
2
(1 − �0 )λmin (∇2 F (x(t) ))

=
≤

2L̂λmax (∇2 F (x(t) ))
�
�∇F (x(t) )�2M
2
(t)
(t)
2
2
2
(1 − �0 ) λmin (∇ F (x )) λmin (∇ F (x ))
L̂κ
2
· √ �∇F (x(t) )�2M .
(1 − �0 )2 µ µ

Thus, we have
�
�∇F (x(t+1) )�M ≤ �0 +

�1
1 − �0

�

�∇F (x(t) )�M +

2
L̂κ
· √ �∇F (x(t) )�2M .
2
(1 − �0 ) µ µ

By the Lipschitz continuity of ∇2 F (x) and the condition
�x(t) − x∗ � ≤

µ
L̂κ

≤

λmin (∇2 F (x∗ ))
L̂κ(∇2 F (x(t) ))

,

we obtain
�[∇2 F (x∗ )]−1 − [∇2 F (x(t) )]−1 � ≤�[∇2 F (x∗ )]−1 ��[∇2 F (x(t) )]−1 ��∇2 F (x∗ ) − ∇2 F (x(t) )�
≤L̂�[∇2 F (x∗ )]−1 ��[∇2 F (x(t) )]−1 ��x(t) − x∗ �

≤ν(t)λmin ([∇2 F (x(t) )]−1 ).
Hence, we can obtain that for any u ∈ Rd ,

−ν(t)λmin ([∇2 F (x(t) )]−1 )uT y ≤ y T ([∇2 F (x∗ )]−1 −[∇2 F (x(t) )]−1 )y ≤ ν(t)λmin ([∇2 F (x(t) )]−1 )y T y,
which yields
(1 − ν(t))uT [∇2 F (x(t) )]−1 u ≤ uT [∇2 F (x∗ )]−1 u ≤ (1 + ν(t))uT [∇2 F (x(t) )]−1 u.
That is,
(1 − ν(t))�u�M ≤ �u�M ∗ ≤ (1 + ν(t))�u�M .

Accordingly, we have
�
(t+1)
)�M ∗ ≤ �0 +
�∇F (x

�1
1 − �0

�

1 + ν(t)
2
L̂κ (1 + ν(t))2
�∇F (x(t) )�M ∗ +
�∇F (x(t) )�2M ∗
·
√
1 − ν(t)
(1 − �0 )2 µ µ 1 − ν(t)

6

Appendix C. Proofs of theorems of Section 4
Proof of Theorem 4 If S is an �0 -subspace embedding matrix w.r.t. B(x(t) ), then we
have
(1 − �0 )∇2 F (x(t) ) � [B(x(t) )]T S T SB(x(t) ) � (1 + �0 )∇2 F (x(t) ).

(17)

By simple transformation and omitting �20 , (17) can be transformed into
(1 − �0 )[B(x(t) )]T S T S∇2 B(x(t) ) � ∇2 F (x(t) ) � (1 + �0 )[B(x(t) )]T S T SB(x(t) ).
The convergence rate can be derived directly from Theorem 3.

Appendix D. Proofs of theorems of Section 5
Proof of Theorem 5 By Lemma 12, when |S| ≥
property:

16K 2 log(2d/δ)
,
σ 2 �20

H (t) has the following

�H (t) − ∇2 F (x(t) )� ≤ �0 σ.
The above property implies the following:
|y T (H (t) − ∇2 F (x(t) ))y| ≤ �0 σy T y,

⇒ − �0 σy T y ≤ y T (H (t) − ∇2 F (x(t) ))y ≤ �0 σy T y
⇒ H (t) − �0 σI � ∇2 F (x(t) ) � H (t) + �0 σI

⇒ (1 − �0 )H (t) � ∇2 F (x(t) ) � (1 + �0 )H (t) .
The convergence rate can be derived directly from Theorem 3.
Proof of Theorem 6
By Lemma 12, when |S| ≥

16K 2 log(2d/δ)
,
β2

we have
(t)

�∇2 F (x(t) ) − H|S| � ≤ β,
with probability at least 1 − δ. Hence, we can derive
(t)

|y T (∇2 F (x(t) ) − H|S| )y| ≤ βy T y
(t)

(t)

⇒y T H|S| y − βy T y ≤ y T ∇2 F (x(t) )y ≤ y T H|S| y + βy T y

⇒y T H (t) y − αy T y − βy T y ≤ y T ∇2 F (x(t) )y ≤ y T H (t) y − αy T y + βy T y
(1)

(2)

⇒y T H (t) y − (α + β)y T y ≤ y T ∇2 F (x(t) )y ≤ y T H (t) y + (β − α)y T y.
7

(1)

Now we ﬁrst consider ≤ case, we have
y T H (t) y − (α + β)y T y ≤ y T ∇2 F (x(t) )y

⇒y T H (t) y ≤ y T ∇2 F (x(t) )y + (α + β)y T y
α+β T 2
y ∇ F (x(t) )y
⇒y T H (t) y ≤ y T ∇2 F (x(t) )y +
σ
�
�
α+β
y T ∇2 F (x(t) )y
⇒y T H (t) y ≤ 1 +
σ
�
�
α+β
y T H (t) y ≤ y T ∇2 F (x(t) )y
⇒ 1−
σ+α+β
�
�
α+β
H (t) � F (x(t) ).
⇒ 1−
σ+α+β
(2)

For ≤ case, we consider two cases respectively. The ﬁrst case is β − σ/2 ≤ α ≤ β, and
we have
y T ∇2 F (x(t) )y ≤ y T H (t) y + (β − α)y T y

⇒y T ∇2 F (x(t) )y − (β − α)y T y ≤ y T H (t) y
β−α T 2
y ∇ F (x(t) )y ≤ y T H (t) y
⇒y T ∇2 F (x(t) )y −
σ
�
�
β−α
y T ∇2 F (x(t) )y ≤ y T H (t) y
⇒ 1−
σ
�
�
β−α
T 2
(t)
y T H (t) y
⇒y ∇ F (x )y ≤ 1 +
σ − (β − α)
�
�
β−α
2
(t)
⇒∇ F (x ) � 1 +
H (t) .
σ+α−β
For the case β < α, we can derive
y T ∇2 F (x(t) )y ≤ y T H (t) y + (β − α)y T y ≤ y T H (t) y

⇒∇2 F (x(t) ) � (1 + 0)H (t) .

Hence, for β − σ ≤ α, we have
�
�
�
�
β−α
α+β
(t)
(t)
H � F (x ) � 1 +
H (t) .
1−
σ+α+β
σ+α−β
Therefore, �0 in Theorem 3 can be set as follows:
�
�
β−α
α+β
.
,
�0 = max
σ+α−β σ+α+β
The convergence properties can derived from Theorem 3 directly.

8

Proof of Theorem 7
(t)
We denote the SVD of HS as follows
(t)

T
HS = U Λ̂U T = Ur Λ̂r UrT + U\r Λ̂\r U\r
.

By Lemma 12, when |S| ≥

16K 2 log(2d/δ)
,
β2

we have
(t)

�∇2 F (x(t) ) − H|S| � ≤ β,
with probability at least 1 − δ. Hence, we can derive
(t)

|y T (∇2 F (x(t) ) − H|S| )y| ≤ βy T y
(t)

(t)

⇒y T H|S| y − βy T y ≤ y T ∇2 F (x(t) )y ≤ y T H|S| y + βy T y
(t)

T
y − βy T y ≤ y T ∇2 F (x(t) )y
⇒y T H (t) y + y T U\r (Λ̂\r − λ̂r+1 I)U\r
(t)

T
y + βy T y
≤ y T H (t) y + y T U\r (Λ̂\r − λ̂r+1 I)U\r
�
�
(1)
βI
r
⇒y T H (t) y − y T U
U T y ≤ y T ∇2 F (x(t) )y
(t)
(β + λ̂r+1 )I\r − Λ̂\r
�
�
(2)
βI
r
UT y
≤ y T H (t) y + y T U
(t)
(β − λ̂r+1 )I\r + Λ̂\r
(1)

Now we ﬁrst consider ≤ case, we have
y T H (t) y − y T U

�

βIr
(β +

(t)
λ̂r+1 )I\r
(t)

− Λ̂\r

�

(1)

U T y ≤ y T ∇2 F (x(t) )y

⇒y T H (t) y ≤ y T ∇2 F (x(t) )y + (β + λ̂r+1 )y t y
(t)

β + λ̂r+1 T 2
y ∇ F (x(t) )y
⇒y H y ≤ y ∇ F (x )y +
σ
2β + λ̂r+1 T 2
⇒y T H (t) y ≤ y T ∇2 F (x(t) )y +
y ∇ F (x(t) )y
σ
�
�
(t)
2β + λr+1
y T H (t) y ≤ y T ∇2 F (x(t) )y.
⇒ 1−
(t)
σ + 2β + λr+1
T

(t)

T

2

(t)

Hence we have
�

1+

β
(t)

λr+1 − β

�
9

H (t) � ∇2 F (x).

(2)

Now we ﬁrst consider ≤ case, we have

�

y T ∇2 F (x(t) )y ≤y T H (t) y + y T U
≤y T H (t) y +
�

≤ 1+

β
(t)
λ̂r+1

β
(t)

λr+1 − β

βIr
(t)

(β − λ̂r+1 )I\r + Λ̂\r

�

UT y

y T H (t) y

�

y T H (t) y,

(t)

(t)

where the last inequality is because λr+1 − β ≤ λ̂r+1 . Hence, we have
�
�
β
2
H (t) .
∇ F (x) � 1 + (t)
λr+1 − β
Hence, we have
�0 = max
λ

�

(t)

2β + λr+1

β

,
(t)
(t)
λr+1 − β σ + 2β + λr+1

�

< 1,

(t)

because β ≤ r+1
2 .
The convergence properties can be derived directly by Theorem 3.

Appendix E. Subsampled Hessian and Gradient
In fact, we can also subsample gradient to accelerate the subsampled Newton method. The
detailed procedure is presented in Algorithm 5 [1, 2].
Theorem 13 Let F (x) satisfy the properties described in Theorem 3. We also assume
Eqn. (9) and Eqn. (10) hold and let 0 < δ < 1 and 0 < �0 < 1/2 be given. Let |SH | and |Sg |
be set such that Eqn. (2) holds and it holds that
�g(x(t) ) − ∇F (x(t) )� ≤

�2
�∇F (x(t) )�.
κ

The direction vector p(t) is computed as in Algorithm 5. Then for t = 1, . . . , T , we have the
following convergence properties:
(a) There exists a suﬃcient small value γ, 0 < ν(t) < 1, and 0 < η(t) < 1 such that when
�x(t) − x∗ � ≤ γ, then for each iteration, it holds that
�∇F (x(t+1) )�M ∗ ≤ (�0 + 2�2 + 4η(t))
with probability at least 1 − δ.
10

1 + ν(t)
�∇F (x(t) )�M ∗
1 − ν(t)

Algorithm 5 Subsampled Hessian and Subsampled Gradient.
1: Input: x(0) , 0 < δ < 1, 0 < �0 < 1;
2: Set the sample size |SH | and |Sg |.
3: for t = 0, 1, . . . until termination do
4:
Select a sample set SH , of size |S| and construct H (t) =

Select a sample set Sg of size |Sg | and calculate g(x(t) )

5:

6:
Calculate p(t) = [H (t) ]−1 g(x(t) );
7:
Update x(t+1) = x(t) − p(t) ;
8: end for

�

1
2
(t)
j∈S ∇ fj (x );
|S|
�
1
= |Sg | i∈Sg ∇fi (x(t) ).

(b) If ∇2 F (x(t) ) is also Lipschitz continuous and {x(t) } satisﬁes Eqn. (6), then for each
iteration, it holds that
�∇F (x(t+1) )�M ∗ ≤(�0 + 2�2 )

1 + ν(t)
8L̂κ (1 + ν(t))2
�∇F (x(t) )�M ∗ + √
�∇F (x(t) )�2M ∗ .
1 − ν(t)
µ µ 1 − ν(t)

with probability at least 1 − δ.
In common cases, subsampled gradient g(x(t) ) needs to subsample over 80% of samples
to guarantee convergence of the algorithm. Roosta-Khorasani and Mahoney [2] showed
that it needs |Sg | ≥ G(x(t) )2 κ2 /(ν 2 (t)�∇F (x(t) )�2 ), where G(x(t) ) = maxi �∇fi (x(t) )� for
i = 1, . . . , n. When x(t) is close to x∗ , �∇F (x(t) )� is close to 0. Hence |Sg | will go to n as
iteration goes. This is the reason why the Newton method and variants of the subsampled
Newton method are very sensitive to the accuracy of subsampled gradient.
The proof of Theorem 13 is almost the same with Theorem 3. For completeness, we
give the detailed proof as follows.
Proof By Taylor’s theorem, we obtain
∇F (x(t+1) )
(t)

2

(t)

(t)

=∇F (x ) + ∇ F (x )(−p ) +

�

1
0

[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds

=∇F (x(t) ) − ∇2 F (x(t) )[H (t) ]−1 ∇F (x(t) ) + ∇2 F (x(t) )[H (t) ]−1 ∇F (x(t) ) − ∇2 F (x(t) )p(t)
� 1
+
[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds
0

��
�− 1
1
1
2
I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 ∇2 F (x(t) )
∇F (x(t) )
� 1
[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds.
+ ∇2 F (x(t) )([H (t) ]−1 ∇F (x(t) ) − p(t) ) +

�

= ∇2 F (x(t) )

�1 �
2

0

Hence, we have the following identity
�− 1
�
�− 1
�
��
1
1
2
2
∇F (x(t+1) ) = I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 ∇2 F (x(t) )
∇F (x(t) )
∇2 F (x(t) )
1

+ [∇2 F (x(t) )] 2 ([H (t) ]−1 ∇F (x(t) ) − p(t) )
� 1
2
(t) − 12
[∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )](−p(t) )ds.
+ [∇ F (x )]
0

11

�
�−1
Further more, we deﬁne M = ∇2 F (x(t) ) , we can obtain
�
�
1
1�
�
�∇F (x(t+1) )�M ≤ �I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 � �∇F (x(t) )�M
1

+ �[∇2 F (x(t) )] 2 ��[H (t) ]−1 (∇F (x(t) ) − g(x(t) ))�
� 1
2
(t) − 12
�∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )��p(t) �ds.
+ �[∇ F (x )] �
0

We will bound the three terms on the right hand of above equation seperately.
For the ﬁrst term, using Lemma 9, we have
�
�
1
1�
�
�I − [∇2 F (x(t) )] 2 [H (t) ]−1 [∇2 F (x(t) )] 2 � �∇F (x(t) )�M ≤ �0 �∇F (x(t) )�M .

For the second term, by the fact that �AB� ≥ �A�σmin (B) and condition �g(x(t) ) −
∇F (x(t) )� ≤ �2 �∇F (x(t) )�, we obtain
1

�[∇2 F (x(t) )] 2 ��[H (t) ]−1 (∇F (x(t) ) − g(x(t) ))�
1

=

�[∇2 F (x(t) )] 2 �

λmin

1

1

([∇2 F (x(t) )]− 2 )

λmin ([∇2 F (x(t) )]− 2 )��[H (t) ]−1 ��∇F (x(t) ) − g(x(t) )�

1

≤�2

�[∇2 F (x(t) )] 2 �

1

([∇2 F (x(t) )]− 2 )

λmin
�2
≤
�∇F (x(t) )�M
1 − �0
≤2�2 �∇F (x(t) )�M

�[H (t) ]−1 ��∇F (x(t) )�M

For the third term, we bound it for the case that ∇2 F (x) is not Lipschitz continuous
and the case ∇2 F (x) is Lipschitz continuous respectively.
(a) Now we consider the case that ∇2 F (x) is not Lipschitz continuous but is continuous
close to the optimal point x∗ . Because ∇2 F (x) is continuous near x∗ , there exists a suﬃcient
small value δ such that Eqn. (14) and Eqn. (15) hold when �x(t) − x∗ � ≤ δ.
By µ-strong convexity, we have �[∇2 F (xt )]−1 � ≤ µ1 for all x(t) suﬃciently close to x∗ .
Then we have
1 + �2 /κ
2
�p(t) � = �[H (t) ]−1 ��g(x(t) )� ≤
�∇F (x(t) )� ≤
�∇F (x(t) )�.
(1 − �0 )µ
(1 − �0 )µ

Hence, with �x − x∗ � ≤ δ, combining condition (15), we have
� 1
2
(t) − 12
�∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )��p(t) �ds
�[∇ F (x )] �
0
� 1
1
µη(t) (t)
√ �p �ds
≤�[∇2 F (x(t) )]− 2 �
κ
0
1
µη(t)
2
√ �∇F (x(t) )�
≤�[∇2 F (x(t) )]− 2 �
(1 − �0 )µ κ
1

�[∇2 F (x(t) )]− 2 �
2η(t)
2
(t) − 12
≤
)] )�∇F (x(t) )�
√
1 λmin ([∇ F (x
−
(t)
2
1 − �0 κλmin ([∇ F (x )] 2 )
≤4η(t)�∇F (x(t) )�M ,

12

Therefore, we have
�∇F (x(t+1) )�M ≤�0 �∇F (x(t) )�M + 2�2 �∇F (x(t) )�M + 4η(t)�∇F (x(t) )�M
=(�0 + 2�2 + 4η(t))�∇F (x(t) )�M .

By Eqn. (14), we have
−

ν(t)
ν(t)
y T y ≤ y T ([∇2 F (x∗ )]−1 − [∇2 F (x(t) )]−1 )y ≤
y T y,
λmax (∇2 F (x∗ ))
λmax (∇2 F (x∗ ))

⇒(1 − ν(t))y T [∇2 F (x(t) )]−1 y ≤ y T [∇2 F (x∗ )]−1 y ≤ (1 + ν(t))y T [∇2 F (x(t) )]−1 y

⇒(1 − ν(t))�y�M ≤ �y�M ∗ ≤ (1 + ν(t))�y�M .

By this relationship between � · �M and � · �M ∗ , we get
�∇F (x(t+1) )�M ∗ ≤ (�0 + 2�2 + 4η(t))

1 + ν(t)
�∇F (x(t) )�M ∗
1 − ν(t)

(b) Now we consider the case that ∇2 F (x) is Lipschitz continuous with parameter L̂.
The same to the previous proof, we have
�p(t) � = �[H (t) ]−1 ��g(x(t) )� ≤

1 + �2 /κ
2
�∇F (x(t) )� ≤
�∇F (x(t) )�.
(1 − �0 )µ
(1 − �0 )µ

Because ∇2 F (x) is Lipschitz continuous with parameter L̂, we have
� 1
1
�∇2 F (x(t) + sp(t) ) − ∇2 F (x(t) )��p(t) �ds
�[∇2 F (x(t) )]− 2 �
0
� 1
1
sL�p(t) �2 ds
≤�[∇2 F (x(t) )]− 2 �
0

1
L̂
2
(t) − 12
= �[∇2 F (x(t) )] �λ−2
)λ2min ([∇2 F (x(t) )]− 2 )�p(t) �2
min ([∇ F (x )]
2
�2
�
1
L̂
2
2
(t) − 12
≤ �[∇2 F (x(t) )]− 2 �λ−2
([∇
F
(x
)]
)
�∇F (x(t) )�2M
min
2
(1 − �0 )λmin (∇2 F (x(t) ))

− 12

=

2L̂λmax (∇2 F (x(t) ))
�
�∇F (x(t) )�2M
2
(t)
(t)
2
2
2
(1 − �0 ) λmin (∇ F (x )) λmin (∇ F (x ))

8L̂κ
≤ √ �∇F (x(t) )�2M ,
µ µ

where the last inequality is because �0 ≤ 1/2. Hence, we have
8L̂κ
�∇F (x(t+1) )�M ≤(�0 + 2�2 )�∇F (x(t) )�M + √ �∇F (x(t) )�2M .
µ µ
By the Lipschitz continuity of ∇2 F (x) and the condition
�x(t) − x∗ � ≤

µ
L̂κ

≤
13

λmin (∇2 F (x∗ ))
L̂κ(∇2 F (x(t) ))

,

we obtain
�[∇2 F (x∗ )]−1 − [∇2 F (x(t) )]−1 � ≤�[∇2 F (x∗ )]−1 ��[∇2 F (x(t) )]−1 ��∇2 F (x∗ ) − ∇2 F (x(t) )�
≤L̂�[∇2 F (x∗ )]−1 ��[∇2 F (x(t) )]−1 ��x(t) − x∗ �

≤ν(t)λmin ([∇2 F (x(t) )]−1 ).
Hence, we can derive

− ν(t)λmin ([∇2 F (x(t) )]−1 )y T y ≤ y T ([∇2 F (x∗ )]−1 − [∇2 F (x(t) )]−1 )y ≤ ν(t)λmin ([∇2 F (x(t) )]−1 )y T y,

⇒(1 − ν(t))y T [∇2 F (x(t) )]−1 y ≤ y T [∇2 F (x∗ )]−1 y ≤ (1 + ν(t))y T [∇2 F (x(t) )]−1 y

⇒(1 − ν(t))�y�M ≤ �y�M ∗ ≤ (1 + ν(t))�y�M .
Hence, we have
�∇F (x(t+1) )�M ∗ ≤(�0 + 2�2 )

1 + ν(t)
8L̂κ (1 + ν(t))2
�∇F (x(t) )�M ∗ + √
�∇F (x(t) )�2M ∗
1 − ν(t)
µ µ 1 − ν(t)

Table 2: Datasets Description
Dataset
n
d
source
mushrooms
8124
112
UCI
a9a
32561 123
UCI
Covertype 581012 54
UCI

Appendix F. Unnecessity of Lipschitz continuity of Hessian
In this section, we validate our theoretical results about unnecessity of the Lipschitz continuity condition of ∇2 F (x). We conduct experiment on the primal problem for the linear
SVM which can be written as
n

1
C �
�(bi , �x, ai �)
min F (x) = �x�2 +
x
2
2n
i=1

where (ai , bi ) denotes the training data, x deﬁnes the separating hyperplane, C > 0, and
�(·) is the loss function. In our experiment, we choose Hinge-2 loss as our loss function
whose deﬁnition is
�(b, �x, a�) = max(0, 1 − b�x, a�)2 .
Let SV (t) denote the set of indices of all the support vectors at iteration t, i.e.,
SV (t) = {i : bi �x(t) , ai � < 1}.
14

Then the Hessian matrix of F (x(t) ) can be written as
1 �
ai aTi .
n
(t)

∇2 F (x(t) ) = I +

i∈SV

From the above equation, we can see that ∇2 F (x) is not Lipschitz continuous.
Without loss of generality, we use the Subsampled Newton method (Algorithm 2) in our
experiment. We sample 5% support vectors in each iteration. Our experiments on three
datasets whose detailed description is in Table 2 and report our results in Figure 3.
From Figure 3, we can see that Subsampled Newton converges linearly and the Newton
method converges superlinearly. This matches our theory that the Lipschitz continuity of
∇2 F (x) is not necessary to achieve a linear or superlinear convergence rate.
mushrooms

a9a

5

5

Subsampled Newton
Newton

0

0

-5

-5

-10

-10

covtype

160

Subsampled Newton
Newton

Subsampled Newton
Newton

140
120

-15

80

log(err)

log(err)

log(err)

100

-15

-20

-20

-25

-25

-30

-30

60
40
20
0

-35

-20

-35
5

10

15

iteration

20

25

-40
2

4

6

8

10

12

14

5

iteration

(a) mushrooms.

(b) a9a.

10

15

20

25

30

iteration

(c) covtype.

Figure 3: Convergence properties on diﬀerent datasets.

References
[1] Richard H Byrd, Gillian M Chin, Will Neveitt, and Jorge Nocedal. On the use of
stochastic hessian information in optimization methods for machine learning. SIAM
Journal on Optimization, 21(3):977–995, 2011.
[2] Farbod Roosta-Khorasani and Michael W Mahoney. Sub-sampled newton methods ii:
Local convergence rates. arXiv preprint arXiv:1601.04738, 2016.
[3] Joel A Tropp et al. An introduction to matrix concentration inequalities. Foundations
R in Machine Learning, 8(1-2):1–230, 2015.
and Trends�

15

