Ordinal Graphical Models: A Tale of Two Approaches

Appendix
A. Proof of Theorem 1
Following the notations in Besag (1974); Yang et al. (2012; 2015), we define Q(Y) as
Q(Y) = log P[Y]/P[0] ,
for any Y = (Y1 , . . . , Yp ) 2 {0, 1, . . . , M }p where P[0] denotes the probability that all random variables in Y are identically zero. In this proof, we focus only on the pairwise MRF, however note that even with the higher order dependencies
the theorem still holds. Now, lets consider the most general pairwise form of Q(Y)
X
X
Q(Y) =
Ys Gs (Ys ) +
Ys Yt Gst (Ys , Yt ),
(14)
1sp

1s<tp

where Gs , Gst can be arbitrary functions. In the proof, we will connect this definition of Q(Y) to the node-conditional
distributions P[Ys |Y\s ] and investigate how given P[Ys |Y\s ] effects the forms of Gs (·) and Gst (·) in (14).
Note that Q(Y) and P[Ys |Y\s ] are related as

where Ȳs := (Y1 , . . . , Ys

(15)

Q(Ȳs ) = P[Y ]/P[Ȳs ] = P[Ys |Y\s ]/P[0|Y\s ],

exp Q(Y)

1 , 0, Ys+1 , . . . , Yp ).

The probability in (2) can be represented as
log P[Ys |Y\s ]
⇢
M
⇣X
= log exp
✓s;j I[Ys = j]

µs (Y\s )

j=0

⇢

log 1 + exp

M
⇣X
j=0

✓s;j I[Ys = j]

⌘

exp

M
⇣X

✓s;j

1

j=0

µs (Y\s )

⌘

Substituting this and Equation (14) in Equation (15) we get
X
Ys Gs (Ys ) + Ys
Yt Gst (Ys , Yt )

I[Ys = j]

⇢

log 1 + exp

µs (Y\s )

M
⇣X

✓s;j

j=0

1

⌘

I[Ys = j]

µs (Y\s )

⌘

.

t2\s

= log

⇢

exp

M
⇣X
j=0

⇢

✓s;j I[Ys = j]

log 1 + exp

M
⇣X
j=0

µs (Y\s )

✓s;j I[Ys = j]

⌘

exp

✓s;0 + µs (Y\s ) + log 1 + exp ✓s;0

✓s;j

1

j=0

µs (Y\s )

n

M
⇣X

⌘

µs (Y\s )

o

I[Ys = j]

µs (Y\s )

⇢
M
⇣X
log 1 + exp
✓s;j
j=0

1

⌘

I[Ys = j]

µs (Y\s )

⌘
(16)

.

By setting Yt = 0, 8t 6= s in the above equation, we obtain the first order function Ys Gs (Ys ):
Ys Gs (Ys ) = log
⇢

log 1 + exp
n

⇢

exp

M
⇣X
j=0

M
⇣X
j=0

✓s;j I[Ys = j]

✓s;j I[Ys = j]

✓s;0 + log 1 + exp ✓s;0

o

⌘

⌘

exp

M
⇣X

✓s;j

j=0

⇢

log 1 + exp

M
⇣X
j=0

1 I[Ys = j]

✓s;j

⌘

1 I[Ys = j]

⌘
(17)

Ordinal Graphical Models: A Tale of Two Approaches

where we assume that µs (0) = 0 without loss of generality; if µs (0) = c for some nonzero c, then we simply replace ✓s;j
0
0
with ✓s;j
where ✓s;j
= ✓s;j + c.
Suppose nodes s and t are neighbors in graph G, i.e. Ys Yt Gst (Ys , Yt ) 6= 0. Setting Yr = 0 for all r 62 {s, t}, we obtain
Ys Gs (Ys ) + Ys Yt Gst (Ys , Yt ) =
⇢
M
⇣X
⌘
log exp
✓s;j I[Ys = j] µs (0, . . . , Yt , . . . , 0)

exp

j=0

⇢

log 1 + exp
⇢

log 1 + exp

M
⇣X
j=0

M
⇣X

✓s;j I[Ys = j]
✓s;j

j=0

1

I[Ys = j]

µs (0, . . . , Yt , . . . , 0)

M
⇣X
j=0

⌘

µs (0, . . . , Yt , . . . , 0)

n
o
✓s;0 + µs (0, . . . , Yt , . . . , 0) + log 1 + exp ✓s;0 .

✓s;j

1

I[Ys = j]

µs (0, . . . , Yt , . . . , 0)

⌘

⌘
(18)

Combining (17) and (18) yields

Ys Yt Gst (Ys , Yt ) =
⇣
⌘
⇣
⌘
8
9
M exp ✓
=
<X
µs (0, . . . , Yt , . . . , 0)
exp ✓s;j 1 µs (0, . . . , Yt , . . . , 0)
s;j
log
I[Ys = j]
:
;
exp ✓s;j
exp ✓s;j 1
j=0
⇣
⌘
8
9
M 1 + exp ✓
<X
=
µs (0, . . . , Yt , . . . , 0)
s;j
log
I[Ys = j]
:
;
1 + exp ✓s;j
j=0
⇣
⌘
8
9
M 1 + exp ✓
<X
=
µs (0, . . . , Yt , . . . , 0)
s;j 1
log
I[Ys = j]
:
;
1 + exp ✓s;j 1
j=0
(
)
1 + exp ✓s;0 µs (0, . . . , Yt , . . . , 0)
+ µs (0, . . . , Yt , . . . , 0) + log
.
1 + exp ✓s;0
Similarly, we can also obtain Ys Yt Gst (Ys , Yt ) by considering the difference Q(Y )
(15). Using Q(Y ) Q(Ȳt ), we obtain

Q(Ȳt ), instead of Q(Y )

Ys Yt Gst (Ys , Yt ) =
⇣
⌘
⇣
⌘
8
9
M exp ✓
<X
=
µt (0, . . . , Ys , . . . , 0)
exp ✓t;j 1 µt (0, . . . , Ys , . . . , 0)
t;j
log
I[Yt = j]
:
;
exp ✓t;j
exp ✓t;j 1
j=0
⇣
⌘
8
9
M 1 + exp ✓
<X
=
µt (0, . . . , Ys , . . . , 0)
t;j
log
I[Yt = j]
:
;
1 + exp ✓t;j
j=0
⇣
⌘
8
9
M 1 + exp ✓
<X
=
µt (0, . . . , Ys , . . . , 0)
t;j 1
log
I[Yt = j]
:
;
1 + exp ✓t;j 1
j=0
(
)
1 + exp ✓t;0 µt (0, . . . , Ys , . . . , 0)
+ µt (0, . . . , Ys , . . . , 0) + log
.
1 + exp ✓t;0
At this point, (19) and (20) should be the same for all possible pairs of Ys and Yt .

(19)
Q(Ȳs ) in

(20)

Ordinal Graphical Models: A Tale of Two Approaches

Now, consider the case of Ys = 1 and Yt = 1. For this fixed setting, both µs (0, . . . , 1, . . . , 0) and µt (0, . . . , 1, . . . , 0) are
fixed constants; let us call them c1 and c2 , respectively. Then equating (19) and (20) we get
(

)
(
)
exp ✓s;1 c1
exp ✓s;0 c1
1 + exp ✓s;1 c1
log
log
+ c1
exp ✓s;1
exp ✓s;0
1 + exp ✓s;1
(
)
(
)
1 + exp ✓t;1 c2
exp ✓t;1 c2
exp ✓t;0 c2
= log
log
exp ✓t;1
exp ✓t;0
1 + exp ✓t;1
(
)
(
)
1 + exp ✓t;0 c2
1 + exp ✓s;0 c2
log
+ c2 + log
.
1 + exp ✓t;0
1 + exp ✓s;0

(21)

Trivially, this equality cannot hold for all values of ✓s;0 , ✓s;1 , ✓t;0 , ✓t;1 2 R. This shows that there can’t exist a Q(Y)
that is consistent with node conditional distributions in Equation (2), which in turn entails that there is no consistent joint
distribution for all choices of the parameters.

B. Proof of Theorem 2
The same strategy as in the proof of Theorem 1 can be adopted here. To this end, we derive the form of equation:
exp Q(Y) Q(Ȳs ) = P[Ys |Y\s ]/P[0|Y\s ]

Ys Gs (Ys ) + Ys

X

Yt Gst (Ys , Yt ) =

+ log 1 + exp ✓s;j

log 1 + exp ✓j

j=1

=

M
X1

⌘

✓s;0 + µs (Y\s )

µs (Y\s )
min{Ys ,M

✓s;j

j=0

I[Ys < j]

µs (Y\s )

µs (Y\s ) I[Ys = j]

✓s;j

j=0

t2\s

M
X1

M
X1 ⇣

µs (Y\s ) I[Ys = j]

✓s;0 + µs (Y\s ),

X

1}

log 1 + exp ✓s;j

j=1

µs (Y\s )
(22)

where we used the definition of P[Ys |Y\s ] given in Equation (4).

We can obtain the first order function Ys Gs (Ys ) by setting Yt = 0 for all t 6= s in (22):
Ys Gs (Ys ) =

M
X1
j=0

min{Ys ,M

✓s;j I[Ys = j]

X

1}

log 1 + exp(✓s;j )

✓s;0

(23)

j=1

where we use the fact that µs (0) = 0.
Suppose nodes s and t are neighbors in graph G, i.e. Ys Yt Gst (Ys , Yt ) 6= 0. Setting Yr = 0 for all r 62 {s, t}, we obtain
Ys Gs (Ys ) + Ys Yt Gst (Ys , Yt ) =

M
X1 ⇣
j=0

min{Ys ,M

X
j=1

1}

n
⇣
log 1 + exp ✓s;j

✓s;j

⌘
µs (0, . . . , Yt , . . . , 0) I[Ys = j]

µs (0, . . . , Yt , . . . , 0)

⌘o

✓s;0 + µs (0, . . . , Yt , . . . , 0)

(24)

Ordinal Graphical Models: A Tale of Two Approaches

Combining (23) and (24) yields
Ys Yt Gst (Ys , Yt ) = µs (0, . . . , Yt , . . . , 0) I[Ys = M ]
⇣
⌘9
8
min{Ys ,M 1}
< 1 + exp ✓s;j µs (0, . . . , Yt , . . . , 0) =
X
log
:
;
1 + exp(✓s;j )
j=1

Similarly, we can also obtain Ys Yt Gst (Ys , Yt ) by considering the difference Q(Y )
(15). Using Q(Y ) Q(Ȳt ), we obtain

Q(Ȳt ), instead of Q(Y )

Ys Yt Gst (Ys , Yt ) = µt (0, . . . , Ys , . . . , 0) I[Yt = M ]
⇣
⌘9
8
min{Yt ,M 1}
< 1 + exp ✓t;j µt (0, . . . , Ys , . . . , 0) =
X
log
:
;
1 + exp(✓t;j )
j=1

(25)

Q(Ȳs ) in

(26)

At this point, (25) and (26) should be the same for all possible pairs of Ys and Yt .

As in the proof of Theorem 1, let us consider the case of Ys = 1 and Yt = 1 where M
1. Again, since both
µs (0, . . . , 1, . . . , 0) and µt (0, . . . , 1, . . . , 0) are fixed constants, by the equality of (25) and (26) we have

log

(

1 + exp ✓s;1 c1
1 + exp(✓s;1 )

)

c1 I[1 = M ] = log

(

1 + exp ✓t;1 c2
1 + exp(✓t;1 )

)

If M = 1, then (27) can be reduced as
(
)
(
)
1 + exp ✓s;1 c1
1 + exp ✓t;1 c2
log
= log
+ c1
1 + exp(✓s;1 )
1 + exp(✓t;1 )
and if M > 1, (27) can be reduced as
(
)
(
)
1 + exp ✓s;1 c1
1 + exp ✓t;1 c2
log
= log
1 + exp(✓s;1 )
1 + exp(✓t;1 )

c2 I[1 = M ]

(27)

c2

(28)

(29)

In any case, the equality cannot hold for all values in ✓s;1 , ✓t;1 2 R. This shows that there can’t exist a Q(Y) that
is consistent with node conditional distributions in Equation (4), which in turn entails that there is no consistent joint
distribution for all choices of the parameters.

C. Proof of Theorem 4
˜ from Step 1 concentrates well around
To show that our final estimate of ⌃ is consistent, we first show that our estimate ⌃
⇤
⌃ , by appealing to the result in (Mei et al., 2017).
Lemma 1. Under conditions (C-2), (C-3), there exists some known quantities C1 and C2 depending on L1 , L2 , L3 , M , ↵,
, , and c > 2 such that if n 4C1 log p log n, then
r
⇤
e jk ⌃jk |  C2 log p log n
|⌃
(30)
n
with at least probability 1

p

c

.

Proof. To prove the Lemma we use Theorem 2 of Mei et al. (2017) which shows that, under certain regularity conditions,
there is a one-to-one mapping between the critical points of the empirical risk and the population risk. Moreover Mei et al.
(2017) obtain a bound for the gap between any critical point of the empirical risk and the corresponding critical point of
population risk. We first note that the three required assumptions for Theorem 2 of (Mei et al., 2017) hold for the sample
loss defined in Equation (10), on the domain [ 1 + , 1
]:

Ordinal Graphical Models: A Tale of Two Approaches
0
PM PM
( ;⇥⇤ )
0
The derivative of `jk ( ; ⇥⇤ , Yn ) is equal to a=0 b=0 nnab ab;jk
L1 .
⇤ , hence |` ( )| is upper bounded by
ab;jk ( ;⇥ )
0
0
⇤
2
Therefore, `jk ( ) `jk (⌃jk ) is bounded between [ 2 L1 , 2 L1 ] and sub-Gaussian with a parameter ⇢1 := (2 L1 )2 .

Similarly, the absolute of second derivative |`00jk ( )| is bounded by ( L2 +
Gaussian with a parameter ⇢22 :=
Finally, |`000
jk ( )| 

L3 + 12

2

L2 +

L1 L2 + 2

2 2 2
L1 .
3

2

L21 ), and |`00jk ( )

L31 , hence `00jk (·) is Lipschitz with ⇢3 :=

L3 + 12

`00jk (⌃⇤jk )| is sub2

L1 L2 + 2

3

L31 .

From an application of Theorem 2 of (Mei et al., 2017), it follows that the global maximizer of our empirical log-likelihood
in (10) converges to a corresponding critical point of the expected log-likelihood `¯jk . But since the global maximizer of
our empirical log-likelihood in (10) is precisely the MLE, which is consistent, it follows that the corresponding critical
point of the expected log-likelihood is precisely the true covariance parameter ⌃⇤jk .
Now, we can directly appeal to the recent results on the sparsistency of graphical lasso:
Lemma 2. Suppose that with probability at least 1 c1 p c2 ,
r
⇤
e ⌃ k1  c3 log p .
k⌃
(31)
n
p
Consider our estimator (11) with regularization parameter n = (8c3 /↵) log(p)/n. Then, if the sample size n is lower
n
o2
8
2K ⇤ (1+ ↵
)
3
2
⇤K ⇤, K ⇤K ⇤}
bounded as n c22 max
,
(1+8/↵)d
max{K
(log p+log c1 ), then the inverse of estimate
⌃
⌃
min
b
⌃ satisfies the bound as
r
log p
1
⇤
1
1
b
⌃
(⌃ )
 2c3 (1 + 8↵ )K ⇤
1
n
⇤
1
b
and, moreover, the graph structure of latent Gaussian
q encoded in (⌃ ) is consistently recovered by ⌃
⇤
1
]ij
2c3 (1 + 8↵ 1 )K ⇤ logn p , with probability at least 1 c1 p c2 +2 .
min := minij [(⌃ )

1

as long as

The lemma follows from an application of Theorems 1 and 2 of (Ravikumar et al., 2008).

D. Experiments
D.1. Simulations
Data from Probit Model: Here we present results from simulations when the data is generated from a Probit model
with grid and random graph structures. We first describe the graphs and exact model parameters that were used in these
simulations.
Grid Graph: We select a 10 ⇥ 5 grid graph, with 10 rows and 5 columns. For all the vertical edges we set the
corresponding entries in inverse covariance matrix as 0.25 and for all the horizontal edges we set the corresponding
entries as 0.25. We set the thresholds (✓) at node j as : ✓(j) = [ Inf, 10, 0.7, 0.7, 10, Inf]. Figure 5 presents the
results from this simulation.
Random Graph: We use the same graph generation procedure as (Liu et al., 2012). For each node j in the graph we
associate a bivariate random variable Uj = (U1,j , U2,j ) 2 [0, 1]2 uniformly sampled from a unit square. An edge is
included between (j, k) with probability:
1
p exp
2⇡

kUj

Uk k22
.
0.15

If an edge is added between (j, k) then the corresponding entry in the inverse covariance matrix is set to ! 2 ( 1, 1).
We use the same thresholds (✓) as in grid graph, to convert the latent variables to ordinal variables. Figure 6 presents
the results for ! = 0.8, 0.65.
ROC plots on large n: Figure 7 provides the ROC plots for n = 200, 400, when the data is generated from probit model.

Ordinal Graphical Models: A Tale of Two Approaches

Test Log Likelihood

0.8

0.8

0.6

0.6

0.4

0.2

TPR

TPR

ProbitEMApprox
ProbitEM
ProbitDirect
Oracle
Consec Model
Discrete Model

0.4

0.2

0

0
0

0.2

0.4

0.6

0.8

1

-47

0.7

-48

0.6

-49
-50
-51
-52

0.2

0.4

0.6

0.8

1

0.5
0.4
0.3

15

10

5

0.2

-53
0

FPR

20

Entropy Loss

n = 100

1

Frobenius Loss

n = 50

1

0.1
0

100

200

FPR

300

400

0
0

100

n

200

300

400

0

100

n

200

300

400

n

Figure 5. Comparison of various estimators when the data is generated from a probit model with grid graph structure. The left two plots
show ROC curves for n = 50, 100. The right three plots show performance on test log likelihood, frobenius and entropy losses.

0.6

0.4

0.2

0

0
0

0.2

0.4

0.6

0.8

1

0.2

0.6

0.8

0.6

0.6

0.4

0.2

0

0
0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

0.5

0.4

100

200

300

0.8

1

15

10

5

0.3

0.2
0

400

0
0

100

200

300

400

0

100

n

-42

0.8

-43

0.7

-44
-45
-46
-47

200

300

400

300

400

n
20

0.6
0.5
0.4

15

10

5

0.3

-48
-49

0

FPR

20

0.6

n

TPR

ProbitEMApprox
ProbitEM
ProbitDirect
Oracle
Consec Model
Discrete Model
0

-54

1

Test Log Likelihood

0.8

TPR

0.4

n = 100

1

0.8

0.2

-53

FPR

n = 50

0.4

-52

-55
0

FPR
1

-51

Frobenius Loss

0.2

TPR

TPR

ProbitEMApprox
ProbitEM
ProbitDirect
Oracle
Consec Model
Discrete Model

0.7

-50

Entropy Loss

0.6

-49

Entropy Loss

0.8

Test Log Likelihood

0.8

0.4

n = 100

1

Frobenius Loss

n = 50

1

0.2
0

100

200

FPR

300

400

0
0

n

100

200

300

400

0

n

100

200

n

Figure 6. Comparison of various estimators when the data is generated from a probit model with chain graph structure. Top and bottom
rows correspond to ! = 0.8 and ! = 0.65 respectively. The left two columns show ROC curves for n = 50, 100. The right three
columns show performance on test log likelihood, frobenius and entropy losses.

D.2. Train Time
We now compare the training time of our estimators with ProbitEMApprox. For a comparison of training times of ProbitEM
and ProbitEMApprox see Guo et al. (2015), where the authors show that ProbitEM is ⇡ 1 order of magnitude slower than
ProbitEMApprox.
In this experiment, we fix p = 200 and sample data from a probit model with chain graph structure (with ! in Equation
(13) set to 0.3). Note that the choice of regularization parameter can effect the training time of each of these estimators. So,
we report the training time of these methods averaged over different choices of regularization parameters. Table 1 shows
the results from this experiment1 .

p = 200

n = 100
n = 200
n = 400

ProbitDirect
20.66
19.48
17.80

Consec Model
167.05
196.33
246.65

ProbitEMApprox
81.96
63.81
51.08

Table 1. Training time (in seconds) of ProbitEMApprox, Consec Model, ProbitDirect.

ProbitEMApprox solves glasso in each iteration of EM, whereas ProbitDirect only solves glasso once. As a result ProbitEMApprox is much slower than ProbitDirect. Although Consec Model is slower than other estimators, its training can
be performed in a distributed fashion. So it can be used to learn very large networks.
1
ProbitEMApprox is implemented R and ProbitDirect, Consec Model are implemented in MATLAB. ProbitEMApprox was run for a
maximum of 25 iterations. Step 1 in estimation of ⌃ of ProbitDirect was run only once for different choices of regularization parameter.

Ordinal Graphical Models: A Tale of Two Approaches

0.6

0.4
0.2

0.2

0

0
0.6

0.8

1

0

0.2

0.8

0.6

0.6

TPR

TPR

0.4

0.6

0.8

0.4
0.2

0.4

0
0.8

1

0
0

0.2

0.4

0.6

0.8

1

0.2

0.4

0.6

FPR

0

0.2

0.8

0.6

0.8

1

n = 400

1

0.8

0.8

0.6

0.6

0.4
0.2

1

0.4

FPR

n = 200

ProbitEMApprox
ProbitEM
ProbitDirect
Oracle
Consec Model
Discrete Model

0.4
0.2

0
0

0.4
0.2

1

0
0.6

FPR

ProbitEMApprox
ProbitEM
ProbitDirect
Oracle
Consec Model
Discrete Model

FPR

ProbitEMApprox
ProbitEM
ProbitDirect
Oracle
Consec Model
Discrete Model

0.2

0.4

0.4

1

n = 400

1

0.8

0.2

0.6

FPR

n = 200

0

0.8

0.6

0

FPR

1

0.8

TPR

0.4

n = 400

1

0.2

TPR

0.2

ProbitEMApprox
ProbitEM
ProbitDirect
Oracle
Consec Model
Discrete Model

0.4

n = 200

1

TPR

0.8

0.6

TPR

TPR

0.8

0

n = 400

1

TPR

n = 200

1

0
0

0.2

0.4

0.6

0.8

FPR

1

0

0.2

0.4

0.6

0.8

1

FPR

Figure 7. ROC plots for n = [200, 400], when the data is generated form a probit model. The two plots on the top left are for the chain
graph described in the main part of the paper with ! = 0.3 and the two plots on top right are for the chain graph with ! = 0.9.
The bottom left plots correspond to the random graph described in the appendix with ! = 0.8 and bottom right plots correspond to the
random graph with ! = 0.65.

Figure 8. Summary statistics of the HINTS-FDA dataset.

D.3. HINTS-FDA Study
D.3.1. DATA P REPROCESSING
Missing values: The original data collected through the survey has missing responses for a number of questions. Some
of these missing responses have already been imputed in the data that was made publicly available through the HINTS
website. In our analysis, we impute the rest of the missing responses using median. If a question has more than 50%
missing responses then we don’t use the responses for that question in our analysis.
Categorical Data: Some of the questions in the survey have categorical responses (e.g., Marital Status). We use one hot
encoding technique for such responses to convert them into binary format.
Count Data: For responses which are neither categorical nor ordinal (such as age, how many hours does a person watch
TV etc.,) we binned the responses into a fixed number of categories and converted them into ordinal variables. For example,
for number of hours of TV watched per week we created 5 buckets : <1hr, 2-3hrs, 3-5hrs, 5-10hrs, >10hrs.

Ordinal Graphical Models: A Tale of Two Approaches

Node Name
CigarettesHarmHealth
DailySmokelessHarm

What is the highest grade or level
of schooling you completed?

Education

FewCigarettesHarmHealth
HealthInsurance

IncomeRanges IMP
Mexican
PhoneInHome
Retired
SmokeNow
Student
TobaccoEffects TV
White

Question
How long do you think someone
has to smoke cigarettes
before it harms their health?
How much do you think people harm
themselves when they use
smokeless tobacco every day?

How much do you think people harm
themselves when they smoke
a few cigarettes every day?
Do you have any kind of
health care coverage?
what is the combined annual
income of your family?
Are you a Mexican?
Is there at least one telephone
inside your home?
Occupation Status
Do you now smoke cigarettes every day,
some days or not at all?
Occupation Status
how often have you seen,
heard, or read a message about
the health effects of tobacco
use on TV?
Are you a White?

Possible Responses
1-’< 1 year’, 2-’1 year’
3 - ’5 years’, 4 - ’10 years’
5 - ’20 years or more’
1-No harm, 2-Little harm,
3-Some harm, 4 - A lot of harm
1-’Less than 8 years’, 2-’8 through 11 years’ ,
3-’12 years or completed high school’,
4-’Post high school training’
5-’Some college’, 6-’College graduate’,
7-’Postgraduate’
1-No harm, 2-Little harm,
3-Some harm, 4 - A lot of harm
1-Yes, 2-No
1-’$0-$9,999’, 2-’$10,000-$14,999’ ,
3-’$15,000-$19,999’, 4-’$20,000-$34,999’
5-’$35,000-$49,999’, 6-’$50,000-$74,999’,
7-’$75,000-$99,999’, 8-’$100,000-$199,999’
9- ’$200,000 or more’
1-’Yes’, 2-’No’
1-Yes, 2-No
1-Not Retired, 2-Retired
1-Everyday, 2-Some days, 3-Not at all
1-Not Student, 2-Student
1-’Never’, 2-’A couple of times’, 3-’Lot of times’
1-’Yes’, 2-’No’

Table 2. Table describing the questions corresponding to some of the nodes in Figures 4, 9.

Ordinal Graphical Models: A Tale of Two Approaches

Figure 9. The estimated latent graph structure for variables corresponding to perceptions of smoking risks and SmokeNow. The graph
is generated from the marginal distribution of the corresponding variables. Green edges represent positive partial correlations and red
edges represent negative partial correlations. Edge thickness is proportional to the magnitude of the partial correlation.

