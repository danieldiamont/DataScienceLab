Sparse + Group Sparse Dirty Models

Appendix
A. Proof of Proposition 1
e + e ✓¯ = ↵
e ↵
¯ + e ¯ = + . If (i) either [ ]j or [ ]j is zero, or (ii)
As mentioned in the statement, ✓e ✓ ⇤ = ↵
sign([ ]j ) = sign([ ]j ), then ([ ]j + [ ]j )2 ([ ]j )2 + ([ ]j )2 . Therefore, if this happens for every j, the inequality
(10) holds. When either [↵⇤ ]j 6= 0 or [ ⇤ ]j 6= 0 holds, [ ]j = 0 or [ ]j = 0 is guaranteed by construction (rule 2 or 3
e j and [ e]j (and therefore
above). If both [↵⇤ ]j and [↵⇤ ]j are zero (in case of rule 1), the following lemma ensures that [↵]
¯
¯ j = [ ]j = 0 in this case) have same signs.
[ ]j and [ ]j because [↵]
e and e are always consistent whenever both are not zeros: sign([↵]
e j ) = sign([ e]j ) for all j
Lemma 1. The signs of ↵
e
e j 6= 0 and [ ]j 6= 0.
such that [↵]
e j and [ e]j have opposite signs; say [↵]
e j > 0 and [ e]j < 0. Then [↵]
ej ✏
The proof of Lemma 1 is trivial. Suppose that [↵]
and [ e]j + ✏ with arbitrary small positive ✏ and all others fixed, will have the same loss by L(·), but smaller values in the
regularizers, which violates the stationary condition of local minimum.
Since the sign consistency is always guaranteed (or at least one of them is zero), the decomposability in the statement
trivially holds.

¯ j := ↵
ej
Showing (11) also comes from the definitions of support sets. For any j such that j 2
/ s⇤ but j 2 U , we set ↵
and hence j = 0 by definition. Therefore, the projection does not make any difference. The equality holds for s̄ since
s⇤ ✓ s̄ ✓ U . The same reasoning holds for as well.

B. Proofs for `2 Error Bounds
B.1. Proof of Theorem 1
¯ ¯) by the transformation T (↵⇤ , ⇤ ; ↵,
e e). For notational simplicity, we use ⇤ to denote the error
Recall that we get (↵,
⇤
e
e ↵
¯ and := e ¯. Note that
vector on our estimation: ✓ ✓ . We also define the individual error vectors as
:= ↵
e + e and ✓¯ = ↵
¯ + ¯ by definitions.
⇤ = + since ✓e = ↵
We first show that under (RSC), k⇤k2  1 is guaranteed, and hence the first inequality (12) for the case of k⇤k2  1 only
matters. Toward this, given all the assumptions in the statement, suppose that k⇤k2 1.
Then setting (✓1 , ✓2 ) as (✓ ⇤ , ⇤) in (13), we obtain
2 k⇤k2

⌦
⌧2 k⇤k⌘  rL(✓ ⇤ + ⇤)

↵
rL(✓ ⇤ ), ⇤ .

At the same time, the stationary condition of (14) ensures that any local optimum satisfies
⌦
↵
e + rk✓k
e , ✓ ✓e
rL(✓)
0

(19)

(20)

for any feasible ✓. This is more general than the first-order stationary condition; for some local optima, the first-order
stationary condition does not hold due to the inequality constraint of program (14). (see (Loh & Wainwright, 2014; 2015)
for more details.) Since ✓ ⇤ is feasible by assumption, setting ✓ = ✓ ⇤ yields
⌦
↵
e + rk✓k
e ,⇤  0.
rL(✓)
(21)
Combining (19) and (21) yields

2 k⇤k2
By Hölder’s inequality,
D

⌧2 k⇤k⌘ 

D

rL(✓ ⇤ )

E
e ,⇤ .
rk✓k

E
e , ⇤  rL(✓ ⇤ ) + rk✓k
e
rk✓k
⇣
⌘
⇤
 rL(✓ ⇤ ) ⌘ + ⌘¯ k⇤k⌘
rL(✓ ⇤ )

⇤
⌘

k⇤k⌘
(22)

Sparse + Group Sparse Dirty Models

where we utilize Lemma 3 with defining ⌘¯ := max{ ⌘11 ,
max

⇤

krL(✓ )k1
⌘1

,

⇤

krL(✓ )k1,a⇤
⌘2

 max

1

4⌘1

⌘2 }
2

in the second inequality. Moreover, since rL(✓ ⇤ )

⇤
⌘

=

, 4⌘2 by assumption, we obtain
2

2 k⇤k2

⌧2 k⇤k⌘ 

5¯
⌘
k⇤k⌘ .
4

(23)

Rearranging (23), we have
k⇤k2 

⌘ (i) 2r ⇣ 5¯
⌘ (ii)
k⇤k⌘ ⇣ 5¯
⌘
⌘
+ ⌧2 
+ ⌧2  1
2
4
2 4

e ⌘ + k✓ ⇤ k⌘  2r (since ✓ ⇤ is feasible) and (ii) holds under the conditions that
where (i) follows the fact k⇤k⌘  k✓k
2
2
r  4⌧2 and r¯
⌘ 5.

From now we revisit the stationary condition of the problem (14) and the RSC condition (12) for k⇤k2  1. Recall that s̄
¯ and s̄ is the projection of onto the space w.r.t. s̄ (simply meaning [ s̄ ]j = 0
is the union of supp(↵⇤ ) and supp(↵),
if j 2
/ s̄). b̄ was similarly defined as supp( ⇤ ) [ supp( ¯). Rearranging the stationary condition (21), we obtain
D

E
e ,⇤ 
rL(✓)

(ii)



=

D

e , ✓e
rk✓k

¯ 1
1 k↵k
⇣

2k

+

E (i)
¯
✓¯  k✓k

¯k1,a

¯ 1 + [ ]s̄c 1
k↵k
⇣
+ 2 k ¯k1,a + [ ]b̄c
1

e
k✓k

e
k✓k

(iii)



[ ]s̄c

1,a

1

[ ]b̄c

¯ 1
1 k↵k
e 1
k↵k

1,a

+
⌘

2k

¯k1,a

k ek1,a

⌘

e 1
1 k↵k

2k

ek1,a

where we use (i) the convexity and (iii) the triangular inequality of k · k norm, and we obtain the inequality (ii) since
¯ has the infimal sum of two regularizers by definition. Hence,
k✓k



D

(i)

=

(ii)



=

E
e ,⇤
rL(✓)
⇣
⌘
¯ 1 + [ ]s̄c 1
e 1
[ ]s̄c 1 k↵k
1 k↵k
⇣
⌘
+ 2 k ¯k1,a + [ ]b̄c 1,a
[ ]b̄c 1,a k ek1,a
⇣
⌘
¯ + [ ]s̄c 1
e 1
↵
[ ]s̄c 1 k↵k
1
⇣
⌘
+ 2 ¯ + [ ]b̄c 1,a
[ ]b̄c 1,a k ek1,a
⇣
⌘
¯ + [ ]s̄c + [ ]s̄ 1 + [ ]s̄ 1
e 1
↵
[ ]s̄c 1 k↵k
1
⇣
⌘
+ 2 ¯ + [ ]b̄c + [ ]b̄ 1,a + [ ]b̄ 1,a
[ ]b̄c 1,a k ek1,a
⇣
⌘
⇣
⌘
[ ]s̄ 1
[ ]s̄c 1 + 2 [ ]b̄ 1,a
[ ]b̄c 1,a
1

(24)

where (i) and (ii) hold by the decomposability and triangular inequality of norms, respectively.
¯ , ⇤i|:
We also compute the upper bound for term |hrL(✓)
D

E
¯ ,⇤ =
rL(✓)

since (i) holds from the fact ⇤ =

(ii)

D

E (i)
rL(✓ ⇤ ) , ⇤ =

⌦

rL(✓ ⇤ ),

↵

⌦

rL(✓ ⇤ ),

 krL(✓ ⇤ )k1 k k1 + krL(✓ ⇤ )k1,a⇤ k k1,a

+

↵

by definition, and (ii) does by the two standard Hölder’s inequalities.

(25)

Sparse + Group Sparse Dirty Models

We now combine (12) with (24) and (25), and obtain
⌦
↵
¯ ⇤
1 k⇤k22 ⌧1 k⇤k2⌘  rL(✓¯ + ⇤) rL(✓),
¯ 1 k k1 + krL(✓)k
¯ 1,a⇤ k k1,a
 krL(✓)k
⇣
⌘
⇣
+ 1 [ ]s̄ 1
[ ]s̄c 1 + 2 [ ]b̄ 1,a

[ ]b̄c

1,a

⌘

.

Moreover, since k⇤k⌘  ⌘1 k k1 +⌘2 k k1,a by definition of dirty regularizer k·k⌘ , the above inequality can be rearranged
as
⇣
⌘
⇣
⌘
¯ 1 + 2⌧1 ⌘1 r k k1 + krL(✓)k
¯ 1,a⇤ + 2⌧1 ⌘2 r k k1,a
1 k⇤k22  krL(✓)k
⇣
⌘
⇣
⌘
+ 1 [ ]s̄ 1
[ ]s̄c 1 + 2 [ ]b̄ 1,a
[ ]b̄c 1,a


3

1

2

[ ]s̄

1

+

3

2

2

[ ]b̄

1,a

.

Note that in the last inequality, we utilize the assumption on the setting 1 and 2 in the statement (that is, 4krL(✓ ⇤ )k1 
case) and the fact that = [ ]s̄ + [ ]s̄c . Also note that we dropped minus terms at the end.
1 and 8⌧1 ⌘1 r  1 for

¯
Now, in order to relate the `1 and `2 norms of projected error vector, [ ]s̄ , we need to consider the sparsity level of ↵.
¯ the sparsity level of ↵
¯ might be possibly greater than that of ↵⇤ . However, by (11) of
During the construction of ↵,
Proposition 1, we have s⇤ = s̄ as well as b⇤ = b̄ . Hence,
1 k⇤k22 

where ¯ := max{

1

p

s,

2

p

3
2

1p

3
2

1p

sk k2 +

s [ ]s̄

3
2

2p

2

+

sG

3

2

sG }.

2

2p



sG [ ]b̄

3¯ ⇣
2

2

k k2 + k k2

⌘

(26)

Combining (26) with the result in (10) of Proposition 1, we have
⌘2
⇣
⌘
⌘
1 ⇣
3¯ ⇣
k k2 + k k2  1 k k22 + k k22  1 k⇤k22 
k k2 + k k2 ,
2
2

and finally the upper bound of `2 error can be computed as stated:

k⇤k2  k k2 + k k2 

3 ¯
,
1

which completes the proof.
B.2. Proof of Corollary 1
We first show the loss function of (3) satisfy RSC condition in the following proposition.
Proposition 2. Consider a design matrix X 2 Rn⇥p whose rows are independently sampled from N (0, ⌃). Then, with
probability at least 1 c1 exp( c2 n) for some constants c1 and c2 ,
1
kX✓k22
n
q
log p
0
where ⌘10 =
n and ⌘2 =
depending only on ⌃.

E(k"k1,a⇤ )
p
n

01 k✓k22

⌧10 k✓k2⌘0

2

variables.

(27)

letting a standard normal vector " for ⌘ 0 , and 01 and ⌧10 are some constants

As discussed in (Negahban et al., 2012), for the case of a = 2,
of

for any ✓ 2 Rp .

E(k"k1,2 )
p
n



⇢

pm
n

+

q

3 log q
n

by the standard tail bound

Sparse + Group Sparse Dirty Models

The next step in order to appeal to Theorem 1, is to set the proper regularization parameters. Under the conditions in the
statement, (Negahban et al., 2012) show that
1
= 4 X >w
n

⇤

4krL(✓ )k1
with probability at least 1

2
1 ).

c1 exp( c2 n

4krL(✓ ⇤ )k1,a⇤ = 4
with probability at least 1

r

8

1

log p
n

Moreover,
max

t=1,2,...,q

1 >
X w
n Gt

a⇤

8

⇢

m1 1/a
p
+
n

r

log q
n

2 exp( 2 log q).

3 0
Since

for the values of ⌘ 0 and specified, (12) holds with 1 = 01 and ⌧1 = 64
⌧1 by (27). In addition,
3r 0
(13) holds with 2 = 1 and ⌧2 = 32 ⌧1 , by Lemma 8.
q
81 1
8
Therefore, combining all pieces, for a constant r := min
3⌧ 0 , 5 , 3⌧ 0 , we can guarantee that

k✓k2⌘0

3
2
64 k✓k

1

B.3. Proof of Proposition 2

k✓e

24
max
1

✓ ⇤ k2 

⇢r

s log p
,
n

1

r

sG m
+
n

r

sG log q
.
n

The proof of Proposition 2 is the simple extension of proofs given by (Raskutti et al., 2010; Negahban et al., 2012): with
probability greater than 1 c1 exp( 2 n) for some constants c1 and c2 ,
kX✓k2
p
n

min (⌃

1/2

4

)

k✓k2

3

E kwk⇤⌘
p
k✓k⌘
n

for all ✓ 2 Rp

(28)

where w ⇠ N (0, ⌃), min (⌃1/2 ) is the minimum eigenvalue of ⌃1/2 . Here we use k · k⌘ and its dual norm to arrive
at the statement rather than `1 (and `1 ) as in (Raskutti et al., 2010) or k · k1,a (and its dual) in (Negahban q
et al., 2012).
In particular, (Raskutti et al., 2010) obtains the result in terms of `1 from the fact
E(kwk1,a⇤ )
p
n

(Negahban et al., 2012) does for group lasso case from
standard normal vector, and |||A|||a⇤ := maxk✓ka⇤ =1 kA✓ka⇤ .
1
Recalling E kwk⇤⌘ = E max{ kwk
⌘1 ,

E kwk⇤⌘  E
since both
we have

kwk1
⌘1

and

kwk1,a⇤
⌘2

kwk1,a⇤
⌘2

✓

E(kwk1 )
p
n
1/2

 3(maxi ⌃ii )

E(k"k1,a⇤ )
p
)Gt |||a⇤
n

 maxt=1,...,NG |||(⌃

log p
n

while

where " is a

} ,

kwk1
kwk1,a⇤
+
⌘1
⌘2

◆

=E

✓

kwk1
⌘1

◆

+E

✓

kwk1,a⇤
⌘2

are always greater than or equal to zero. Setting ⌘1 =
⇣
E kwk⇤⌘  3(max ⌃ii ) +
i

max

t=1,...,NG

|||(⌃1/2 )Gt |||a⇤

q

⌘p

log p
n

◆

and ⌘2 =

n

(29)
E(k"k1,a⇤ )
p
n

for ⌘,
(30)

and we can establish the bound
kX✓k2
p
n

min (⌃

1/2

4

)

Finally, (27) can be obtained with constants 1 =
the fact that a

c

b implies 2(a2 + b2 )

⇣
3 3(max ⌃ii ) +

k✓k2

i

min (⌃)

(a + b)

32
2

max

t=1,...,NG

⌘
|||(⌃1/2 )Gt |||a⇤ k✓k⌘ .

(31)

⇣
⌘2
and ⌧1 = 9 3(maxi ⌃ii ) + maxt=1,...,NG |||(⌃1/2 )Gt |||a⇤ from

c2 for positive real a, b and c.

Sparse + Group Sparse Dirty Models

B.4. Proof of Corollary 2
As shown in (Loh & Wainwright, 2015), it can be easily shown that the RSC condition (12) holds with 1 = (|||⇥⇤ |||2 +1) 2
and ⌧1 = 0. Hence by Lemma 7, (13) also holds with 2 = 1 and ⌧2 = 0. Hence, the condition on r in Theorem 1 is
b (⇥⇤ ) 1 , the choices of regularization parameters satisfy the
reduced to r  5(|||⇥⇤ 1|||2 +1)2 . Moreover, since rL(✓ ⇤ ) = ⌃
condition of Theorem 1, and its error bound holds.

C. Proofs for Support Set Recovery Guarantees
Before providing the actual proof of Theorem 2, we briefly review the primal-dual witness (PDW) proof technique (Wainwright, 2009; Jalali et al., 2010; Yang et al., 2015; Loh & Wainwright, 2014) for our setting:
(i) Solve the restricted problem
(32)

minimize L(✓) + R(✓; )

✓2RU , k✓k⌘ r

where U := supp(↵⇤ ) [ supp( ⇤ ) as defined earlier. We set ✓bU as the local minimum of this problem.
⇥
⇤
(ii) Define zb1 2 r ✓
and q (✓) := k✓k
R(✓; ). Choose zb2 such that
U
b
rL(✓)

b + zb = 0
rq (✓)

(33)

b j = 0 at
where zb := (b
z1 , zb2 ), ✓b := (✓bU , 0) and [rk✓k ]j = [rR(✓; )]j = [b
z2 ]j for j 2
/ U so that [rq (✓)]
⇤
b
✓j = 0. Establish strict dual feasibility of kb
z2 k  1
for some 2 (0, 1].

(iii) Show that all stationary points of (17) are supported on U .
C.1. Proof of Theorem 2

Since ✓b has the same sparsity pattern with ✓ ⇤ , we can begin with very similar analysis as the proof of Theorem 1 by
¯ ¯) := T (↵,
b b; ↵,
e e) where (↵,
e e) is the (local) minimizer of
handling ✓b as ✓ ⇤ . Hence, in this proof we re-define (↵,
e
e
¯
b
e + = ✓. Therefore, by construction ✓ = ✓. We also re-use the notation ⇤,
non-convex dirty regularizer such that ↵
b ↵
e ↵
¯ and e ¯ respectively but for newly defined ↵
¯ and ¯.
and to represent again ⇤ := ✓e ✓,
We begin with the first-order stationary condition of (17):
⌦
↵
e + rR(✓;
e ), ⇤  0 ,
rL(✓)

(34)

which is in the form discussed in (21).

As before, we first show that k⇤k2  1 under (RSC). In order to show by contradiction, suppose that k⇤k2
b ⇤) in (13), we obtain
Then setting (✓1 , ✓2 ) = (✓,
2 k⇤k2

Combining (34) and (35) yields
2 k⇤k2
Since we set zb to satisfy (33),

b = rq (✓)
b
rL(✓)

⌦
⌧2 k⇤k⌘  rL(✓b + ⇤)

⌧2 k⇤k⌘ 

and hence by Hölder’s inequality and Lemma 4,

D

b
zb = rk✓k

b
rL(✓)

↵
b ⇤ .
rL(✓),
E
e ), ⇤ .
rR(✓;

b )
rR(✓;

zb =

b ),
rR(✓;

D
E
b ) rR(✓;
e ), ⇤
2 k⇤k2 ⌧2 k⇤k⌘  rR(✓;
✓
◆
b ) ⇤ + rR(✓;
e ) ⇤ k⇤k⌘  2¯
 rR(✓;
⌘ k⇤k⌘
⌘
⌘

1.

(35)

(36)

Sparse + Group Sparse Dirty Models

where ⌘¯ := max{ ⌘11 ,

⌘2 }.
2

Since k⇤k = k✓e

b  2r by the constraint of (17),
✓k

2 k⇤k2  (⌧2 + 2¯
⌘ )k⇤k⌘  2r(⌧2 + 2¯
⌘ ).

Therefore, if 2r(⌧2 + 2¯
⌘ )  1 as assumed, we should have k⇤k2  1.
Now we focus on the RSC condition in (12):

⌦
e
⌧1 k⇤k2⌘  rL(✓)

1 k⇤k22

Let L̄(✓) := L(✓) q (✓). (Recalling q (✓) := k✓k
k✓k ). Then, from (37),
⌦
e
1 k⇤k22 ⌧1 k⇤k2⌘  rL̄(✓)

↵
b ⇤ .
rL(✓),

(37)

R(✓; ), L̄(✓) actually can be rewritten as L(✓) + R(✓; )

↵ ⌦
b ⇤ + rq (✓)
e
rL̄(✓),

↵
b ⇤ .
rq (✓),

(38)

In order to upper-bound the second term in the RHS of (38), we apply the mean-value theorem:
rq

✓e

✓b = r2 q

rq

✓ (✓e

b
✓)

b By the Cauchy-Schwarz inequality
where ✓ is a parameter vector on the line between ✓e and ✓.
⌦
↵
2
e
b ⇤  r2 q (✓)
rq (✓)
rq (✓),
✓e ✓b 2
2
where ||| · |||2 denotes the spectral norm of the matrix.

Combining (38) and (39) with the assumption r2 q (✓)
this paper, as shown in Lemma 5) yields
(1

2

 µ (which holds for non-convex regularizers considered in

⌦
e
⌧1 k⇤k2⌘  rL̄(✓)

µ)k⇤k22

(39)

↵
b ⇤ .
rL̄(✓),

e
At the same time, we represent the stationary condition (34) using the notation q (✓):
⌦
↵ ⌦
↵
e ✓b ✓e + ze, ✓b ✓e
0  rL̄(✓),
⌦

(40)

(41)

↵
✓b = 0, and when combined with (41)

e . From our construction of zb in (33), we have rL̄(✓)
b + zb, ✓e
where ze 2 @k✓k
this implies
⌦
↵ ⌦
↵ ⌦
↵
b
e ✓e ✓b
0  rL̄(✓)
rL̄(✓),
zb, ✓b ✓e
ze, ✓e ✓b
⌦
↵ ⌦
↵
⌦
↵
b
e ✓e ✓b + zb, ✓e
b + ze, ✓b
e .
= rL̄(✓)
rL̄(✓),
k✓k
k✓k

(42)

With (40), this inequality implies

(1

⌦
↵
⌧1 k⇤k2⌘  zb, ✓e
⌦
↵
 zb, ✓e

µ)k⇤k22

⌦
↵
b  k✓k
b by Lemma 3.
where we use the fact ze, ✓b  ke
z k⇤ k✓k

⌦
↵
b + ze, ✓b
k✓k
e
k✓k

e
k✓k

(43)

Assume for now that

k⇤k⌘  max{
for some
have

1

p

s,

2 (0, 1]. Then, defining C := max{
(1

2

1

µ)k⇤k22

p

p

sG } max

s,

2

p

⇢

⌘1 ⌘2
,
1

sG } max

⌧1 k⇤k2⌘

2

n

⌘1

1 k⇤k22

,
1

⌘
p ⇣4
2
+ 2 k⇤k2

(44)

op ⇣
⌘
4
2
+
2
so that k⇤k⌘  Ck⇤k2 , we
2

⌘2

⌧1 C 2 k⇤k22 .

(45)

Sparse + Group Sparse Dirty Models

Therefore, as long as 1 > ⌧1 C 2 ,
⌦
↵
⌧1 k⇤k2⌘  zb, ✓e

µ)k⇤k22

0  (1

e
k✓k

(46)

⌦
↵
⌦
↵
e  zb, ✓e . Actually, since zb, ✓e  kb
e  k✓k
e by Hölder’s inequality and Lemma 3, it should
implying k✓k
z k⇤ k✓k
⌦
↵
e
e
hold that k✓k = zb, ✓ . As discussed in previous works (Wainwright, 2009; Jalali et al., 2010; Yang et al., 2015; Loh &
Wainwright, 2014) using PDW approach, this equality guarantees that for all j 62 U , ✓ej = 0 under the strict dual feasibility.
The remaining procedure is to show (44), which is proved in the following lemma.
Lemma 2. Suppose kb
zU c k⇤  1

for some

2 (0, 1], and 2r⌧1 ⌘1  2 1 and 2r⌧1 ⌘2  2 2 . Then
⇢
⌘
p
⌘1 ⌘2 p ⇣ 4
p
k⇤k⌘  max{ 1 s , 2 sG } max
,
2
+ 2 k⇤k2 .
1

(47)

2

From the first inequality of (42), the following inequality can be easily derived in a similar way of constructing (43):
⌦
↵
⌦
↵
e + zb, ⇤ .
1 µ k⇤k22 ⌧1 k⇤k2⌘  ze, ✓b
k✓k
(48)
|
{z
} | {z }
(I)

(II)

By the same reasoning in (24),
b
(I)  k✓k

e 
k✓k

1

⇣

[ ]s̄

[ ]s̄c

1

1

⌘

+

2

⇣

[ ]b̄

[ ]b̄c

1,a

1,a

⌘

The second term in (48) can be upper bounded as follows: if kb
z2 k⇤  1
as assumed,
⌦
↵ ⌦
↵
(II) = zb1 , ⇤U + zb2 , ⇤U c  kb
z1 k⇤ k⇤U k + kb
z2 k⇤ k⇤U c k
 k⇤U k + (1
)k⇤U c k
⇣
⌘
⇣
 1 [ ]U 1 + (1
) [ ]U c 1 + 2 [ ]U 1,a + (1
⇣
⌘
⇣
= 1 [ ]s̄ 1 + (1
) [ ]s̄c 1 + 2 [ ]b̄ 1,a + (1

) [ ]U c

) [ ]b̄c

1,a

1,a

where the last equality follows the result in Proposition 1.

⌘

.

⌘

(49)

(50)

By (49) and (50),
⌧1 k⇤k2⌘  1 µ k⇤k22 ⌧1 k⇤k2⌘
⇣
⌘
⇣
 1 2 [ ]s̄ 1
[ ]s̄c 1 + 2 2 [ ]b̄

Furthermore,

[ ]b̄c

1,a

1,a

⌘

(51)

.

⇣
⌘
⌧1 k⇤k2⌘  2r⌧1 k⇤k⌘  2r⌧1 ⌘1 k k1 + ⌘2 k k1,a .
Hence, if 2r⌧1 ⌘1 

2

1

and 2r⌧1 ⌘2 
⇣

2

1k

⇣2
 1 2 [ ]s̄

or equivalently
2

⇣

1

[ ]s̄c

1

+

2

as stated, combining 51 and 52 establishes

k1 +
1

2

(52)

⌘
⇣
⌘
k1,a  2r⌧1 ⌘1 k k1 + ⌘2 k k1,a
⌘
⇣
⌘
[ ]s̄c 1 + 2 2 [ ]b̄ 1,a
[ ]b̄c 1,a

2k

[ ]b̄c

1,a

⌘

⇣
⌘⇣
 2+
2

1

[ ]s̄

1

+

2

[ ]b̄

1,a

⌘

.

Sparse + Group Sparse Dirty Models

Finally, we can have
⇣
⌘
k⇤k⌘  ⌘1 k k1 + ⌘2 k k1,a  ⌘¯0 1 k k1 + 2 k k1,a
✓
◆
= ⌘¯0 1 [ ]s̄ 1 + 1 [ ]s̄c 1 + 2 [ ]b̄ 1,a + 2 [ ]b̄c 1,a
⌘⇣
+2
1 [ ]s̄
⌘
p 0 ⇣4
 2¯
⌘¯
+ 2 k⇤k2
 ⌘¯0

⇣4

+
1

[ ]b̄

2

1,a

⌘

 ⌘¯0 ¯

⇣4

+2

⌘⇣

k k2 + k k2

⌘

(53)

p
p
p
where ⌘¯0 := max{ ⌘11 , ⌘22 }, ¯ := max{ 1 s , 2 sG }, and we have k k2 + k k2  2k⇤k2 (since
k k2 )2  k k22 + k k22  k⇤k22 ) by Proposition 1 in the last inequality.

1
2 (k

k2 +

C.2. Proof of Corollary 3
The statement can be shown to hold by combining the result of Theorem 2 and lemmas in (Loh & Wainwright, 2014)
(a) Uniqueness. The proof of Lemma 2 in (Loh & Wainwright, 2014) shows that under (12)
v > r2 L(✓)v

1 kvk22

⌧1 kvk2⌘ ,

8v 2 {v 2 Rp | supp(v) ✓ U, kvk2 = 1} .

By definition of k · k , for any v 2 RU ,

p
p
p
p
kvk⌘  ⌘1 kvS k1 + ⌘2 kvB k1,a  ⌘1 skvS k2 + ⌘2 sG kvB k2  max{⌘1 s, ⌘2 sG }kvk2 .

Hence,
v > r2 L(✓)v

p
p
2
⌧1 max{⌘1 s, ⌘2 sG } kvk22 ,

1 kvk22

8v 2 {v 2 Rp | supp(v) ✓ U, kvk2 = 1} .

which implies
⇣
for all ✓ 2 RU . Therefore if
⇣

r2 L(✓)

1 µ
2

r2 L(✓)

⌘

UU

⇣
⌫ 

⌘
p
p
2
⌧1 max{⌘1 s, ⌘2 sG }
I

p
p
2
⌧1 max{⌘1 s, ⌘2 sG } , it is guaranteed that

⌘
⇣
µ
k✓k22
⌫ 
2
UU

µ

⌘
p
p
p
p
2
2
⌧1 max{⌘1 s, ⌘2 sG }
I ⌫ ⌧1 max{⌘1 s, ⌘2 sG } I

and hence (L(✓) µ2 k✓k22 ) + ( µ2 k✓k22 + R(✓; ) is strictly convex over RU as the sum of strictly convex and convex
function. By Theorem 2, any stationary point ✓e of the program (17) should be in the form of (✓eU , 0U c ). Moreover, the
restricted program is strictly convex as just shown, ✓eU is unique.
b Q(
b ✓b
(b) `1 error bound. By the definition of Q,
(Loh & Wainwright, 2014),



k✓b

✓ ⇤ k1 

bU U
Q

1

bU U
Q

rL(✓ ⇤ )U

b
✓ ⇤ ) = rL(✓)
1

1

rL(✓ ⇤ )U

+ min{

1,

rL(✓ ⇤ ). As shown in the proof of Theorem 2 of
b U + zb1
rq (✓)

2}

b U zb1 k⇤ = max krq (✓)
bU
where we use Lemma 6 so that krq (✓)
b
implying krq (✓)U zb1 k1  min{ 1 , 2 }.

bU U
Q

1

zb1 k1 /

1

(54)

1

1 , krq

bU
(✓)

zb1 k1,a⇤ /

2

 1

Sparse + Group Sparse Dirty Models
⇤
⇤
(c) `1 error bound for (µ, )-amenable regularizers. By the assumption on ✓min
, it is guaranteed |✓bj |
|✓min
|
⇤
b
b
b j or j is larger than max{ 1 , 2 }, and, as a result,
k✓ ✓ k1 2 max{ 1 , 2 } for all j 2 U . Then, at least either ↵

and (54) reduces to k✓b

1

bU U
Q

✓ ⇤ k1 

b U = zb1
rq (✓)

rL(✓ ⇤ )U

1

(55)

.

b as described
In order to show the dual feasibility, we combine the zero-subgradient condition (33) and the definition ofQ
in (Loh & Wainwright, 2014):
"
#"
# "
# " #
⇤
bU
bU U
bU U c
Q
Q
rL(✓ ⇤ )U rq (✓)
zb1
✓bU ✓U
+
+
= 0,
⇤
b
b
b
zb2
0
QU c U QU c U c
rL(✓ )U c rq (✓)U c
b U c = 0, yields
and rearranging it for zb2 with (55) and the fact rq (✓)
Therefore, if krL(✓ ⇤ )U c k⇤ 
holds:

1
2

b U c U (Q
bU U )
rL(✓ ⇤ )U c + Q

zb2 =

b U c U (Q
bU U )
and kQ

kb
z2 k⇤  rL(✓ ⇤ )U c

⇤

C.3. Proof of Corollary 4

1

rL(✓ ⇤ )U k⇤ 

b U c U (Q
bU U )
+ Q

(k)

For the problem, if (j, k) 2 U c , then [b
z2 ] j

⌧
1
(k)
Xj , I
n

1

1

rL(✓ ⇤ )U

rL(✓ ⇤ )U

additionally hold, the strict dual feasibility

1
2

⇤

(56)



1
2

+

1
2

1

.

in (56), can be written as
1 (k) ⇣ 1 ⌦ (k) (k) ↵⌘
X
X , XU k
n Uk n Uk

1

⇣

(k)

X Uk

⌘>

w(k) .

(57)
(k)

In order to show the strict dual feasibility kb
z2 k⇤  1
, we need to show both max(j,k)2U c [b
z2 ]j  (1
) 1 and
P
(k)
maxj (j,k)2U c [b
z2 ]j  (1
) 2.
q
q
(k)
log(pm)
log(pm)
c [b
Setting t = 4
for
(61)
of
Lemma
9
yields
max
z
]

4
with probability at least 1
2
(j,k)2U
j
n
n
q
q
P
(k)
log p+m log 2
log p+m log 2
2 exp( 3 log(pm)). Similarly, setting t = 4
for (62) yields maxj (j,k)2U c [b
z2 ] j  4
n
n

with probability at least 1 2 exp
3(log p + m log 2) . In addition, by the similar reasoning in the proof of Proposition
2, we can easily show that the RSC condition holds w.h.p. for ⌘ such that max{ 1 /⌘1 , 2 /⌘2 } is some constant depending
on (the only difference is a = 1 in this example, but E(|||"|||1 ) for a standard normal vector " ⇠ N (0, Ip⇥p ) scales as
the same rates of 2 by (62) of Lemma 9.).
Therefore, the strict dual feasibility holds for the selection of parameters, and the support set recovery is guaranteed w.h.p.
`1 bound is also trivially derived from the combination
q of (63) of Lemma 9 and Corollary 3 (we can compute the upper
100 log(pm)
1
⇤
b
bound of (QU U ) rL(✓ )U
by setting t =
in (63)).
nCmin

1

D. Useful Lemmas for Proofs

Lemma 3. At any ✓, krk✓k k⇤⌘  max

n

⌘1 , ⌘2
1

2

o

.

Proof. For any fixed ✓, let s be the sub-gradient of k✓k at ✓, rk✓k , in Rp . By definition of the sub-gradient, s satisfies
k✓ + vk

k✓k + hs, vi

for all v 2 Rp .

Sparse + Group Sparse Dirty Models

Taking this inequality the supremum over all v such that kvk = 1, we have
sup k✓ + vk

(58)

k✓k + sup hs, vi .

kvk =1

kvk =1

By rearranging (58),
(i)

ksk⇤ := sup hs, vi  sup k✓ + vk
kvk =1

k✓k

kvk =1

(ii)



sup kvk

kvk =1

where (i) is the definition of dual norm of k · k , and (ii)nfollows the triangular inequality
of the norm k · k . Since
o
krk✓k k1,a
supkvk =1 kvk  1 by definitions, we obtain ksk⇤ = max krk✓k1 k1 ,

1,
finally
implying
2
krk✓k

k⇤⌘

 max

(

Lemma 4. At any ✓, krR(✓; )k⇤⌘  max

n

rk✓k
⌘1

⌘1 , ⌘2
1

2

o

1

,

rk✓k

⌘2

1,a

)

 max

⇢

1

2

,
⌘1 ⌘2

.

.

⇥
⇤
Proof. At any ✓, if we compute the derivative, rR(✓; ) j is upper bounded by max{@⇢ 1 ([✓]j ), @ 2 ,a [✓]gj } where
gj is the group of indices that j belongs to. As shown in Lemma 8 of (Loh & Wainwright, 2014), @⇢ 1 ([✓]j )  @k[✓]j k1 .
Similarly, by definition of , we have @ 2 ,a ([✓]gj )  @k[✓]gj k1,a . Therefore, for every index j, [rR(✓; )]j 
⇤
⇤
[rk✓k ]j , and rR(✓; ) ⌘  rk✓k ⌘ . The final result comes by Lemma 3.
Lemma 5. Consider the non-convex penalty functions in (C1). Then, for any ✓,
r2 q (✓)

2

(59)

 µ.

Proof. For notational simplicity, we define the function F : Rp ! Rp as F (✓; ) := rR(✓; ), hence the i-th coordinate
of F is
Fi :=

@R @↵ @R @
·
+
·
@↵ @✓i
@
@✓i

where we suppress the dependency on ✓ and of F (✓; ) and R(✓; ) for compact presentation. By applying another
chain rule to compute rF , we immediately obtain
@Fi X @ 2 R
@↵k @↵l X @R
@ 2 ↵k
=
·
+
·
@✓j
@↵k @↵l @✓i @✓j
@↵k @✓i @✓j
k,l

k

X @2R
+
@ k@
k,l

l

@ k @ l X @R
@2 k
·
+
·
.
@✓i @✓j
@ k @✓i @✓j
k

Furthermore, given penalty functions defined in (C1), it can be shown that
1.

@2R
@↵k @↵l

=

2. If
=
moreover
@R
@↵k

3. If

@R
@↵k

@

@2R
k@

2

l

= 0 for all k 6= l, and | @@↵R2 |  µ for all k.
k

@ k
@R
k
0 for some k, then @↵
@✓i (and hence @✓i ) can have any value, but if @↵k
@ k
@↵k
@↵k
@✓k can be either 1 ( @✓k should be 0 in this case) or 0 ( @✓k should 1).

6= 0, then

@ 2 ↵k
@✓i @✓j

6= 0 then

@↵k
@✓i

= 0 for i 6= k, and

= 0.

Based on these facts, rF can be reduce to the diagonal matrix whose the maximum absolute element is upper bounded by
µ, implying (59) (since r2 k✓k can be shown to be zero matrix by the same reason above).

Sparse + Group Sparse Dirty Models

Lemma 6.

rk✓k

rR(✓; )

⇤
⌘

 max

n

⌘1 , ⌘2
1

2

o

.

Proof. By definitions of k✓k and R(✓; ), every pair of coordinates (rk✓k , rR(✓; )) has the same signs, and moreover based on Lemma 3 and 4, the statement holds.
Lemma 7 (Lemma 8 of (Loh & Wainwright, 2015)). If L is convex and (12) holds, then (13) holds as well, with 2 = 1
and ⌧2 = 2r⌧1 .
Proof. As shown in the proof of Lemma 8 of (Loh & Wainwright, 2015), for any ✓2 2 Rp such that k✓2 k2
!
⌦
↵
k✓2 k2⌘
rL(✓1 + ✓2 ) rL(✓1 ), ✓2
k✓2 k2 1 ⌧1
k✓2 k22
✓
◆
k✓2 k⌘
k✓2 k2 1 2r⌧1
k✓2 k22
✓
◆
k✓2 k⌘
k✓2 k2 1 2r⌧1
k✓2 k2
1 k✓2 k2

1,

2r⌧1 k✓2 k⌘ .

Lemma 8 (Lemma 9 of (Loh & Wainwright, 2015)). If (12) holds globally for all ✓2 2 Rp , then (13) holds as well, with
2 = 1 and ⌧2 = 2r⌧1 .
Proof. If k✓2 k2

1,

⌦

rL(✓1 + ✓2 )

rL(✓1 ), ✓2

↵

1 k✓2 k22
1 k✓2 k2

⌧1 k✓2 k2⌘

2r⌧1 k✓2 k⌘ .

(k)

Lemma 9 (From the proof of Lemma 9 of (Jalali et al., 2010)). Let Wj be
⌧
1
1 (k) ⇣ 1 ⌦ (k) (k) ↵⌘ 1 ⇣ (k) ⌘>
(k)
Xj , I
X
X , XU k
X Uk
w(k)
n
n Uk n U k
where Ukc denotes the support of k-th column of ⇥⇤ . For all t 0,

⇣
(k)
P max c Wj
t  2 exp
(j,k)2U

and


P max
j

X

(j,k)2U c

(k)

Wj

t  2 exp

⇣

In addition, for all t 0,

⇣1⌦
⌘ 11⇣
⌘>
(k)
(k) ↵
(k)
P
max
X Uk , XUk
X Uk
w(k)
k=1,2,...,m
n
n
⇣
⌘
(k)
where Cmin := mink=1,2,...,m min ⌃Uk Uk > 0.

1

⌘
t2 n
+ log(pm) ,
2
4
⌘
t2 n
+ m log 2 + log p .
2
4

t  2 exp

⇣

⌘
t2 nCmin
+
log(pm)
50 2

(60)

(61)

(62)

(63)

c
Note that (Jalali et al., 2010) originally consider only j 2 \m
k=1 Uk , but (61) and (62) hold by the same reasoning.

E. Additional details on the simulation experiments
Figure 4 provides an example of how the computing time of convex and non-convex dirty models vary with the regularization parameters 1 , 2 . The iteration number needed for each method under the same convergence tolerence exhibit a
similar profile. In many cases non-convex dirty models converge faster and take less computing time.

0.0

0.5

1.0
1.5
−log lam1

Convex DM

2.0

2.0
0.20

0.15

0.0

0.5

1.0
1.5
−log lam1

2.0

Non-convex DM (SCAD)

−log lam2
0.5 1.0 1.5

0.15

0.25

0.0

2.0
0.20

−log lam2
0.5 1.0 1.5

0.25

0.0

0.0

−log lam2
0.5 1.0 1.5

2.0

Sparse + Group Sparse Dirty Models

0.25

0.20

0.15

0.0

0.5

1.0
1.5
−log lam1

2.0

Non-convex DM (MCP)

Figure 4. Timing for comparison methods for varying penalty parameters.

