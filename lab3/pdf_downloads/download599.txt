Supplementary Material for Magnetic Hamiltonian Monte Carlo

1

Section 3 and 4 Proofs

Here we provide proofs for results discussed in Section 3 of the main text regarding non-canonical dynamics.
Lemma 1. The map ΦA
τ,H (θ, p) defined by integrating the non-canonical Hamiltonian system


d θ(t)
= A∇θ,p H(θ(t), p(t))
dt p(t)

(1)

with initial conditions (θ, p) for time τ , where A ∈ M2n×2n is any invertible, antisymmetric matrix induces
a flow on the coordinates (θ, p) that is still energy-conserving (∂τ H(ΦA
τ,H (θ, p)) = 0) and symplectic with
respect to A ([∇θ,p Φτ,H (θ, p)]> A−1 [∇θ,p Φτ,H (θ, p)] = A−1 ) which also implies volume-preservation of the
flow.
Proof. The proofs of both results simply uses the antisymmetry of A.
Energy-Conservation – Simply, we have that:
∂τ H(Φτ,H (θ, p)) = ∇θ,p H(Φτ,H (θ, p))∂τ Φτ,H (θ, p) =
>

∇θ,p H(Φτ,H (θ, p)) A∇θ,p H(Φτ,H (θ, p)) = 0

(2)
(3)

using the antisymmetry of A and symmetry of ∇θ,p H(Φτ,H (θ, p))∇θ,p H(Φτ,H (θ, p))> .
Symplecticness – We must show that the Jacobian of the flow generated by the dynamics preserves the
non-canonical structure matrix A, which amounts to showing:
[∇θ,p Φτ,H (θ, p)]> A−1 [∇θ,p Φτ,H (θ, p)] = A−1
|
{z
}
|
{z
}
F (τ )>

(4)

F (τ )

where we define F (τ ) = ∇θ,p Φτ,H (θ, p) as the time-evolving Jacobian of the flow. First, note that F (τ ) can
be equivalently described as the solution to the differential equation:
d
F (τ ) = A∇θ,p ∇θ,p H(Φτ,H (θ, p))F (τ )
dτ

(5)

with the initial condition F (0) = I2d (the Jacobian for the identity map at t = 0). Trivially, we have:
F (0)A−1 F (0) = A−1

(6)

Then note that:
d
(F (τ )> A−1 F (τ )) =
dτ
F (τ )> A−1 A∇θ,p ∇θ,p H(Φτ,H (θ, p))F (τ ) + F (τ )> ∇θ,p ∇θ,p H(Φτ,H (θ, p))A> A−1 F (τ )
= F (τ )> ∇θ,p ∇θ,p H(Φτ,H (θ, p))F (τ ) − F (τ )> ∇θ,p ∇θ,p H(Φτ,H (θ, p))F (τ ) = 0
as desired by simply using A> = −A.
Time-Reversibility – However, crucially it is not the case that the Hamilton equations are time-reversible
in the traditional sense of canonical Hamiltonian dynamics.
1

Lemma 2. If (θ(t), p(t)) is a solution to the non-canonical dynamics:

 


d θ(t)
E
F ∇θ H(θ(t), p(t))
=
−F> G ∇p H(θ(t), p(t))
dt p(t)
|
{z
}

(7)

A

e p
e (t)) = (θ(−t), −p(−t)) is a solution to the modified non-canonical dynamics:
then (θ(t),
#
"

 
e p(t))
e
∇e
H(
θ(t),
d θ(t)
−E
F
θ
=
e p
−F> −G ∇e
e (t)
dt p
e (t))
H(θ(t),
p
|
{z
}
e
A

(8)

e which reduces to the traditional time-reversal
if H(θ, p) = H(θ, −p). In particular if E = G = 0 then A = A,
symmetry of canonical Hamiltonian dynamics.
Proof. A direct calculation yields

 

  d
d θ̃(t)
−E∇θ H(θ(−t)) − F∇p H(θ(−t))
− θ(−t)
=
= ddt
−F> ∇θ H(θ(−t)) + G∇p H(θ(−t))
dt p̃(t)
dt p(−t)
 



−E
F
−E∇θ̃ H(θ̃(t)) + F∇p̃ H(θ̃(t))
∇θ̃ H(θ̃(t))
=
=
−F> −G ∇p̃ H(θ̃(t))
−F> ∇θ̃ H(θ̃(t)) − G∇p̃ H(θ̃(t))
|
{z
}
Ã

1.1

Non-Canonical Dynamics Variable Augmentation

As remarked in the main text it is necessary to flip the E and G matrices at the end of a deterministic
simulation of the Hamiltonian dynamics in order to render the proposal time-reversible which is in turn
necessary to satisfy detailed balance. This is crucial for the correctness of the algorithm especially when an
approximate simulation of the dynamics is used (as is always often the case).
In particular, say that we wish to use ΦA
τ,H (θ, p) as a transition kernel with fixed, non-zero values of E = E0
and G = G0 . We first augment the state-space by placing a symmetric, binary distribution independently over
E and G such that p(E = E0 ) = p(E = −E0 ) = 1/2 and p(G = G0 ) = p(G = −G0 ) = 1/2, independently of
θ, p:
ρ(θ, p, E, G) ∝ e−H(θ,p) p(E)p(G).

(9)

Importantly, this augmentation leaves the distribution over θ, p intact when E and G are marginalized out.
Just as applying the momentum flip operator, Φp : (θ, p, E, G) → (θ, −p, E, G), is a deterministic, energypreserving, volume-preserving transformation, the E, G flip operators, ΦE : (θ, p, E, G) → (θ, p, −E, G)
and ΦG : (θ, p, E, G) → (θ, p, E, −G), are also deterministic, energy-preserving, volume-preserving transformations that leave (9) invariant for this particular augmentation with p(E) and p(G). We can now
e A (θ, p), composed of simulating the Hamiltonian flow as ΦA (θ, p) plus
build a self-inverse operator Φ
τ,H
τ,H
ΦE ◦ ΦG ◦ Φp , a flip of p, E, G, as:
e A (θ, p) = ΦE ◦ ΦG ◦ Φp ◦ ΦA (θ, p)
Φ
τ,H
τ,H

(10)

e A (θ, p). Φ
e A (θ, p) can now be used as the
Now we have constructed a deterministic, self-inverse map Φ
τ,H
τ,H
proposal for a reversible MCMC algorithm.
An important point to note is that our choice of variable augmentation strategy, namely augmenting with
binary distribution, is certainly not unique. However, it is perhaps the most natural and simplest choice
which avoids the repetitive computation of matrix exponentials/diagonalizations since the approximate flow
detailed in Section 2.2 will only need to compute matrix exponentials once upfront for ±G.
2

1.2

Mass Preconditioning Proofs

A common variation on standard HMC dynamics is to set the kinetic energy term in the Hamiltonian H(θ, p)
to 12 p> M−1 p for some symmetric positive-definite matrix M, and sample the initial momentum variable
p from the corresponding distribution N (0, M). However, we can contextualize preconditioning using a
non-canonical A matrix in the following manner:
Lemma 3. i) Preconditioned HMC with momentum variable p ∼ N (0, M) in the (θ, p) coordinates, is
exactly equivalent to simulating non-canonical HMC with p0 = M−1/2 p ∼ N (0, I) and the non-canonical
matrix:


0
M1/2
A=
−(M1/2 )>
0
and then transforming back to (θ, p) coordinates using p = M1/2 p0 . Here M1/2 is a Cholesky factor for M.
ii) On the other hand if we apply a change of basis (via an invertible matrix F) to our coordinates
θ0 = F−1 θ, simulate HMC in the (θ0 , p) coordinates, and transform back to the original basis using F, this is
exactly equivalent to non-canonical HMC with


0
F
A=
−F> 0
Proof. We first prove the equivalence regarding the change of basis in momentum space. Under the M mass
matrix variant of HMC, p is drawn from a N (0, M) distribution, and the dynamics of θ, p are then given by
  

d θ
M−1 p
=
−∇θ U (θ)
dt p
Denoting the upper-triangular Cholesky factor of M−1 by M−1/2 , and introducing a new variable p0 =
M−1/2 p, we obtain the following dynamics for the joint variable (θ, p0 ):
 

 
 


d θ
d
θ
(M−1/2 )> p0
0
(M−1/2 )> ∇θ U (θ)
=
=
=
p0
−M−1/2 ∇θ U (θ)
−M−1/2
0
dt p0
dt M−1/2 p
Further, note that if the marginal distribution of p is N (0, M), then under this change of variables p0 has the
marginal distribution N (0, I). Thus, simulating canonical HMC with a non-identity mass matrix is equivalent
to making a change of basis in momentum space, simulating non-canonical HMC with a particular choice of
non-canonical A matrix, and finally reverting back to the original basis.
We now prove the equivalence regarding the change of basis in θ space, which follows similarly. Consider
non-canonical HMC on the state-momentum pair (θ, p), with the antisymmetric matrix A taking the particular
form


0
F
A=
−F> 0
The states θ, p obtained from this algorithm are equal to those obtained by first changing basis to θ0 = F−1 θ,
then simulating standard HMC dynamics for the pair (θ0 , p) with respect to the Hamiltonian
1
H 0 (θ0 , p) = U 0 (θ0 ) + p> p
2
1
= U (Fθ) + p> p
2
and then reverting to the original basis as θ = Fθ0 . To see this, first note that if we denote the distribution
on the coordinates θ corresponding to the potential U by π(θ) = e−U (θ) , then the corresponding distribution
on the coordinates θ0 is given by π 0 , where
π 0 (θ0 ) = det(F)π(Fθ0 )
The corresponding potential U 0 is therefore given by
U 0 (θ0 ) = U (Fθ0 )
3

Running canonical HMC dynamics targeting the Hamiltonian H 0 yields the dynamics:
  

d θ0
p
=
−∇θ0 U (θ0 )
dt p
But note then that the dynamics of the original coordinates are then given by:
 

 
 

 
d Fθ0
d θ
Fp
Fp
Fp
=
=
=
=
−∇θ0 U (θ0 )
−∇θ0 U (Fθ)
−(F> )∇θ U (θ)
dt p
dt p



0
F ∇θ U (θ)
=
p
−F> 0
which are exactly a special case of non-canonical HMC dynamics described above.

2

Magnetic HMC (MHMC)

Here we provide proofs related to the dynamics of magnetic HMC and it’s symplectic integration scheme.

2.1

Non-Canonical Dynamics and Magnetism

We first establish the connection between the particular subcase of non-canonical Hamiltonian dynamics
where


0
I
A=
−I G
and Newton’s law for a charged particle coupled to a magnetic field.
Lemma 4. In 3-dimensions the non-canonical Hamiltonian dynamics, with Hamiltonian H(θ, p) = U (θ) +
1 >
2 p p, correspond to simulating the differential equations:
 


  

d θ
∇θ H
0
I ∇θ U (θ)
0
I
≡
(11)
=
−I G
p
−I G ∇p H
dt p
| {z }
| {z }
A

where

A



−b3
0
b1

0
G =  b3
−b2


b2
−b1 
0

are equivalent to the Newtonian
mechanics of a charged particle (with unit mass and charge) coupled to a

b1
~ = b2  which take the form:
magnetic field B
b3
d2 θ
dθ ~
= −∇θ U (θ) +
×B
2
dt
dt
where θ is simply a 3-dimensional vector and × the cross-product.

(12)

Proof. If we let θ and p denote our position and momentum coordinates in 3 dimensions then Newton’s law
for a charged particle in a magnetic field (with m = q = 1) is:
d2 θ
dθ ~
= −∇θ U (θ) +
×B
2
dt
dt

(13)

Defining momentum canonically as dθ
dt = p we have:
  
 
 


d θ
p
p
0
I ∇θ U (θ)
=
=
≡
~
−∇θ U (θ) + Gp
−I G
p
−∇θ U (θ) + dθ
dt p
dt × B
| {z }
A

4

(14)

We now show that the dynamics used in magnetic HMC cannot be reproduced by simply choosing a
different smooth Hamiltonian, H 0 (θ, p) and using the canonical A matrix:


0 I
A=
−I 0
to generate the dynamics.
Lemma 5. The non-canonical Hamiltonian dynamics with magnetic A and Hamiltonian H(θ, p) = U (θ) +
1 >
2 p p cannot be obtained using canonical Hamiltonian dynamics for any choice of smooth Hamiltonian.
Proof. Consider the ODEs corresponding to non-canonical dynamics with magnetic A and H(θ, p) =
U (θ) + 12 p> p:
  

 

d θ
0
I ∇θ U (θ)
p
=
=
.
(15)
−I G
p
−∇θ U (θ) + Gp
dt p
Assume, to obtain a contradiction, that these canonical Hamiltonian dynamics can be reproduced for some
choice of smooth H 0 (θ, p) and canonical A matrix (i.e. E = G = 0, F = I):
  

 

d θ
0 I ∇θ H 0 (θ, p)
p
=
=
.
(16)
−I 0 ∇p H 0 (θ, p)
−∇θ U (θ) + Gp
dt p
This implies:

 


 

∇p H 0 (θ, p)
p
∇θ ∇p H 0 (θ, p)
0
=
=⇒
=
.
∇θ H 0 (θ, p)
∇θ U (θ) − Gp
∇p ∇θ H 0 (θ, p)
−G

(17)

However, as long as the 2nd-order mixed partial derivatives are continuous they must be equal; so the
conclusion follows.

2.2

Symplectic Integrator for Magnetic Dynamics

We begin by considering the symmetric splitting:
H(θ, p) = U (θ)/2 + pT p/2 + U (θ)/2
| {z } | {z } | {z }
H1 (θ)

H2 (p)

(18)

H1 (θ)

The corresponding non-canonical dynamics for the sub-Hamiltonians H1 (θ) and H2 (p) are:
  

 

d θ
E
F ∇θ U (θ)/2
E∇θ U (θ)/2
=
=
0
−F> G
−F> ∇θ U (θ)/2
dt p
{z
}
|

(19)

A

and
  
  

d θ
E
F 0
Fp
=
=
.
Gp
−F> G p
dt p
|
{z
}

(20)

A

A
We denote the corresponding flows by ΦA
,H1 (θ) and Φ,H2 (p) respectively. The flow in (19) is generally not
explicitly tractable unless we take E = 0 – in which case it is solved by an Euler translation as for standard
Hamiltonian dynamics.
Crucially, the flow in (20) is a linear differential equation and hence analytically integrable. Without loss
of generality, we restrict ourselves to the case F = I (the case for general F follows similarly). The dynamics
associated with the flow H2 (p) introduced in Lemma 4 become

 

d θ(t)
p(t)
=
Gp(t)
dt p(t)

5

with initial condition (θ0 , p0 ). Using the power series representation of the matrix exponential, the second
differential equation for p may be integrated analytically to yield the following flow in p-space:
p(t) = exp(Gt)p0
Substituting this result into the differential equation for θ yields
dθ
= exp(Gt)p0
dt
If G is invertible then once again using the power series representation of the matrix exponential and
rearranging yields the solution
  

θ
θ + G−1 (exp(G) − I)p
Φ,H2 (p)
=
p
exp(G)p
If G is not invertible, then slightly more care must be taken to first diagonalize G and separate its
invertible/singular components. Since G is strictly antisymmetric it can be written as iH where H is a
Hermitian matrix. Thus it can be diagonalized over C as:

  >

 Λ 0 UΛ
G = UΛ U0
0 0 U0>
 >


UΛ
where Λ is a diagonal submatrix consisting of the nonzero eigenvalues of G. UΛ U0 and
are unitary
U0>
matrices where the columns of UΛ are the eigenvectors of G corresponding to its nonzero eigenvalues while
the columns of U0 are the eigenvectors of G corresponding to its zero eigenvalues. Even if G is not invertible
we still have:
p(t) = exp(Gt)p0
However it is more convenient to represent the matrix exponential as:

  >

 exp(Λt) 0 UΛ
exp(Gt) = UΛ U0
0
I U0>
Substituting this result into the differential equation for θ, this representation of exp(Gt) implies the
non-identity block can be handled as in the invertible case while the I block follows trivially to give:

  >

 Λ−1 (exp(Λt) − I) 0 UΛ
U
U
θ(t) = θ0 + Λ
p
0
0
tI U0> 0
Note that if G = 0 then the flow map will simply reduce to an Euler translation as in ordinary HMC. We
can also combine the ideas of Section 1.2 to obtain a preconditioned, magnetic HMC algorithm corresponding
to a general A-matrix of the form


0
F
−F> G
Dealing with a non-zero E becomes more subtle, since the corresponding sub-Hamiltonian is no longer
exactly integrable under the splitting construction. In order to exactly integrate this sub-block a more costly
implicit integrator is needed.

2.3

Integration Error of Magnetic HMC

Since we are using a symmetric, leapfrog splitting scheme for magnetic HMC that exactly integrates each
sub-Hamiltonian we obtain identical error scaling to the leapfrog integrator applied to canonical HMC. Indeed,
symplectic integrators are well-known to have many nice error properties in general and so perhaps this result
is not so surprising [3].

6

Lemma 6. The symplectic leapfrog-like integrator for magnetic HMC will have the same local (∼ O(3 ))
and global (∼ O(2 )) error scaling (over τ ∼ L steps), as the canonical leapfrog integrator of standard HMC
if the Hamiltonian is separable.
Proof. Note that for the parametrization of A corresponding to magnetic HMC the Hamiltonian vector field
~ = ∇p H∇θ + (−∇θ + G∇p H)∇p ≡ A
~ will generate the exact flow corresponding to exactly simulating
~ +B
H
3
the dynamics. We obtain an O( ) local error by simply exploiting the separability of the Hamiltonian.
The leapfrog integration scheme splits the Hamiltonian as: H(θ, p) = H1 (θ) + H2 (p) + H1 (θ) and exactly
integrates each sub-Hamiltonian so:
 
 


~ ◦ exp A
~ ◦ exp  B
~
Φfrog
=
Φ
◦
Φ
◦
Φ
=
exp
B
(21)
,H
(θ)
,H
(p)
,H
(θ)
1
2
1
,H
2
2
(22)
Via repeated applications of the Baker-Campbell-Hausdorff formula [3] obtain:


 
2
~ ◦ exp A
~ + B
~ +  [A,
~ B]
~ + O(3 ) =
exp
B
2
2
2


2
2
~
~ + B
~ +  [A,
~ B]
~ + 1 [  B,
~ A
~ + B
~ +  [A,
~ B]]
~
exp
B + A
+ O(3 ) =
2
2
2
2 2
2
2


 
2
2
2
~ B]
~ +  [B,
~ A]
~ +  [B,
~ B]
~ + O(3 ) = exp H
~ + O(3 )
~ +  [A,
exp H
4
4
8

(23)
(24)
(25)

where we have used the antisymmetry of the commutator. The global error scaling, for an integration time of
τ = L follows straightforwardly:

 
 

L
~ ◦ exp A
~ ◦ exp  B
~
Φfrog
B
τ,H = exp
2
2

 
L
3
~ + O( )
= exp H


~ + LO(2 )
= exp LH
 
~ + τ O(2 )
= exp τ H
 
~ + O(2 )
= exp τ H

(26)
(27)
(28)
(29)
(30)

as desired.

3

Section 6 Experimental Details

Here we provide relevant experimental details for some of the Experiments presented in the main text.

3.1

Gaussians

In both experiments the reported autocorrelation measures are averaged over all coordinates as well as over
100 independent runs of the HMC/MHMC chains.
3.1.1

2D Gaussian

For the uncorrelated, ill-conditioned 2D Gaussian experiment presented in the main text the magnetic G
component only has one non-zero parameter which was set to g = .2.

7

3.1.2

10D Gaussian

For the uncorrelated, ill-conditioned 10D Gaussian experiment presented in the main text, the G matrix was
set to encourage the flow of momentum between the directions of large marginal variance with covariance
eigenvalues 106 and the remaining 8 directions of directions of small marginal variance with covariance
eigenvalues of 1. We denote the directions of large marginal variance as x1 , x2 , and the other 8 directions
of directions of small marginal variance as xi . G was set such that G1i = G2i = g, Gi1 = Gi2 = −g and
G12 = G21 = 0 for g = .2.

3.2

Mixture of Gaussians

The superior mixing of MHMC relative to HMC in this example holds true for a wide range of (, L) settings
as we can see by looking at the maximum mean discrepancy as a function of the number of samples in both
Figures 1 and 2.
30

30

g = 0.0
g = 0.05
g = 0.1
g = 0.15

g = 0.0
g = 0.05
g = 0.1
g = 0.15

25

Maximum Mean Discrepancy

Maximum Mean Discrepancy

25

20

15

10

5

0

20

15

10

5

0
2000

4000

6000

8000

10000 12000 14000

2000

Number of Samples

4000

6000

8000

10000 12000 14000

Number of Samples

Figure 1: Left: MMD vs. Number of Samples for ( = 1.5, L = 33). The acceptance rate was ∼ .74 for all g.
Right: MMD vs. Number of Samples for ( = 1.9, L = 40). The acceptance rate was ∼ .43 for all g = 0 and
∼ .34 for all non-zero g. In both diagrams g denotes the non-zero component of the magnetic field.

30

30

g = 0.0
g = 0.05
g = 0.1
g = 0.15

g = 0.0
g = 0.05
g = 0.1
g = 0.15

25

Maximum Mean Discrepancy

Maximum Mean Discrepancy

25

20

15

10

5

0

20

15

10

5

0
2000

4000

6000

8000

10000 12000 14000

2000

Number of Samples

4000

6000

8000

10000 12000 14000

Number of Samples

Figure 2: Left: MMD vs. Number of Samples for ( = 1.0, L = 50). The acceptance rate was ∼ .87 for all g.
Right: MMD vs. Number of Samples for ( = 0.5, L = 110). The acceptance rate was ∼ .95 for all g. In both
diagrams g denotes the non-zero component of the magnetic field.
We found that tuning the parameters (, L) via Bayesian optimization often resulted in worse performance
for ordinary HMC since the values found for (, L) were too conservative to encourage exploration between
both modes. Moreover, more aggressive choices for (, L) for ordinary HMC led to a sharp drop in acceptance
rate and significantly worse performance.

8

3.3

Gaussian Funnel

In this additional experiment, we consider the Gaussian funnel of [4] with density
p(x, v) = Πni=1 N (xi |0, e−v )N (v|0, 32 )
in 10+1 dimensions (i.e. n = 10). This density illustrates the pathological correlation present in many
hierarchical models between x, a vector of low-level parameters, and v, a hyperparameter controlling their
variability. As noted in [1, 5], Riemannian HMC methods, which incorporate local curvature information of
the target, are well-suited to this problem as they help the dynamics traverse the energy surface which rapidly
changes as a v varies. HMC (as well as MHMC) do not exploit curvature information and will have more
difficulty exploring the v direction due to the rapid variation in density – see [1] for a detailed discussion.
Despite this difficulty, we might intuitively expect that introducing a “curl” term into the entries of G
which couple each xi and v could increase exploration of the dynamics since these variables are nonlinearly
correlated. In order to encourage the periodic flow of momentum between the marginal direction v and the
coordinates xi the G matrix was set such that Gvi = g, Giv = −g, Gij = 0 with g = .2. To investigate this,
we generated 10000 samples from both HMC and MHMC, discarding 1000 burn-in samples and computed
the minimum effective sample size across x and v and bias in the moments of the v parameter similar to the
set-up in [5] for various , L (see Table 1). We report results averaged over 100 different runs of the Markov
chains.
Table 1: Comparison of HMC and MHMC targeting the Gaussian funnel for a variety of leapfrog steps
algorithm

settings

time (s)

min ESS(x, v)

min ESS(x, v)/s

MSE(E[v], E[v 2 ])

HMC

 = 0.05, L = 100

225

414, 85

1.84, 0.38

.59, 1.57

MHMC

 = 0.05, L = 100

270

463, 97

1.71, 0.36

.29, 1.17

HMC

 = 0.05, L = 300

705

1342, 118

1.90, 0.17

.35, 1.15

MHMC

 = 0.05, L = 300

837

1554, 122

1.86, 0.15

.15, 1.05

We find that adding the magnetic field component decreases the bias in the moments and marginally
increases the ESS, although both samplers struggle to explore the full target density, as the relatively low
ESS figures indicate. Further details and experiments are provided in the Appendix.
Recall the density of the Gaussian funnel p(x, v) = Πni=1 N (xi |0, e−v )N (v|0, 32 ). In order to encourage
the periodic flow of momentum between the marginal direction v and the coordinates xi the G matrix was
set such that Gvi = g, Giv = −g, Gij = 0 with g = .2. Moreover the reported results were averaged over 100
different runs of the Markov chains.

4

MHMC Proposals and Dynamics

In this section, we provide illustrations of the proposal distributions of MHMC in simple low-dimensional
settings, to aid intuition and demonstrate the divergence of its behaviour from standard HMC.

4.1

Gaussian Densities

We first consider the case of an isotropic Gaussian target, and illustrate the proposal distribution

 of standard
E F
HMC, as well as MHMC with a variety of settings for the skew-symmetric matrix A =
- see Figure
F G
3. As in previous sections, we denote the off-diagonal element of the G matrix by g. We also provide proposal
plots for an anisotropic Gaussian target distribution - see Figure 4.

4.2

Banana Density

We also provide proposal illustrations for the banana density of [2], as shown in Figure 5.

9

(a) Standard HMC (g=0)

(b) MHMC (g=0.5)

(c) MHMC (g=1.0)

(d) MHMC (g=2.0)

(e) MHMC (g=3.0)

(f) MHMC (g=4.0)

Figure 3: HMC and MHMC proposals for an isotropic Gaussian target.

(a) Standard HMC (g=0)

(b) MHMC (g=0.5)

(c) MHMC (g=1.0)

(d) MHMC (g=2.0)

(e) MHMC (g=3.0)

(f) MHMC (g=4.0)

Figure 4: HMC and MHMC proposals for an anisotropic Gaussian target.

10

(a) Standard HMC (g=0)

(b) MHMC (g=0.1)

(c) MHMC (g=0.2)

(d) MHMC (g=0.3)

(e) MHMC (g=0.4)

(f) MHMC (g=0.5)

Figure 5: HMC and MHMC proposals for the banana density target.

11

References
[1] Michael Betancourt and Mark Girolami. Hamiltonian Monte Carlo for Hierarchical Models. CRC Press,
Boca Raton, FL, 2015.
[2] Heikki Haario, Eero Saksman, and Johanna Tamminen. Adaptive proposal distribution for random walk
metropolis algorithm. Computational Statistics, 14(3), 1999.
[3] Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich. Geometric Numerical Integration.
Oberwolfach Reports, pages 805–882, 2006.
[4] Radford M. Neal. Slice Sampling: Rejoinder, 2003.
[5] Yichuan Zhang and Charles Sutton. Semi-separable Hamiltonian Monte Carlo for Inference in Bayesian
Hierarchical Models. In Advances in Neural Information Processing Systems, pages 10–18, 2014.

12

