Maximum Selection and Ranking under Noisy Comparisons

Moein Falahatgar 1 Alon Orlitsky 1 Venkatadheeraj Pichapati 1 Ananda Theertha Suresh 2

Abstract
We consider (, Œ¥)-PAC maximum-selection and
ranking using pairwise comparisons for general
probabilistic models whose comparison probabilities satisfy strong stochastic transitivity and
stochastic triangle inequality. Modifying the
popular knockout tournament, we propose a
simple maximum-selection
algorithm that uses

O n2 1 + log 1Œ¥ comparisons, optimal up to a
constant factor. We then derive a general framework that uses noisy binary search to speed up
many ranking algorithms, and combine it with
merge sort to obtain a ranking
algorithm that uses

O n2 log n(log log n)3 comparisons for Œ¥ = n1 ,
optimal up to a (log log n)3 factor.

1. Introduction
1.1. Background
Maximum selection and sorting using pairwise comparisons are computer-science staples taught in most introductory classes and used in many applications. In fact, sorting,
also known as ranking, was once claimed to utilize 25% of
all computer cycles, e.g., (Mukherjee, 2011).
In many applications, the pairwise comparisons produce
only random outcomes. In sports, tournaments rank teams
based on pairwise matches whose outcomes are probabilistic in nature. For example, Microsoft‚Äôs TrueSkill (Herbrich
et al., 2006) software matches and ranks thousands of Xbox
gamers based on individual game results. And in online advertising, out of a myriad of possible ads, each web page
may display only a few, and a user will typically select at
most one. Based on these random comparisons, ad companies such as Google, Microsoft, or Yahoo, rank the ads‚Äô appeal (Radlinski & Joachims, 2007; Radlinski et al., 2008).
These and related applications have brought about a resur1

University of California, San Diego 2 Google Research.
Correspondence to:
Venkatadheeraj Pichapati <dheerajpv7@ucsd.edu>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

gence of interest in maximum selection and ranking using noisy comparisons. Several probabilistic models
were considered, including the popular Bradley-TerryLuce (Bradley & Terry, 1952) and its Plackett-Luce (PL)
generalization (Plackett, 1975; Luce, 2005). Yet even for
such specific models, the number of pairwise comparisons
needed, or sample complexity, of maximum selection and
ranking was known only to within a log n factor. We consider a significantly broader class of models and yet propose algorithms that are optimal up to a constant factor for
maximum selection and up to (log log n)3 for ranking.
1.2. Notation
Noiseless comparison assumes an unknown underlying
ranking r(1), . . . ,r(n) of the elements in {1, . . . ,n} such
that if two elements are compared, the higher-ranked one
is selected. Similarly for noisy comparisons, we assume
an unknown ranking of the elements, but now if two elements i and j are compared, i is chosen with some unknown probability p(i, j) and j is chosen with probability p(j, i) = 1 ‚àí p(i, j), where if i is higher-ranked, then
p(i, j) ‚â• 21 . Repeated comparisons are independent of
each other.
Let pÃÉ(i, j) = p(i, j) ‚àí 12 reflect the additional probability
by which i is preferable to j. Note that pÃÉ(j, i) = ‚àípÃÉ(i, j)
and pÃÉ(i, j) ‚â• 0 if r(i) > r(j). |pÃÉ(i, j)| can also be seen as
a measure of dissimilarity between i and j. Following (Yue
& Joachims, 2011), we assume that two natural properties, satisfied for example by the PL model, hold whenever
r(i) > r(j) > r(k): Strong Stochastic Transitivity (SST),
pÃÉ(i, k) ‚â• max(pÃÉ(i, j), pÃÉ(j, k)), and Stochastic Triangle Inequality (STI), pÃÉ(i, k) ‚â§ pÃÉ(i, j) + pÃÉ(j, k).
Two types of algorithms have been proposed for maximum selection and ranking under noisy comparisons: nonadaptive or offline (Rajkumar & Agarwal, 2014; Negahban
et al., 2012; 2016; Jang et al., 2016) where the comparison pairs are chosen in advance, and adaptive or online
where the comparison pairs are selected sequentially based
on previous comparison results. We focus on the latter.
We specify the desired output via the (, Œ¥)-PAC
paradigm (Yue & Joachims, 2011; SzoÃàreÃÅnyi et al., 2015)
that requires the output to likely closely approximate the
intended outcome. Specifically, given , Œ¥ > 0, with prob-

Maximum Selection and Ranking under Noisy Comparisons

ability ‚â• 1 ‚àí Œ¥, maximum selection must output an maximum element i such that for all j, p(i, j) ‚â• 12 ‚àí .
Similarly, with probability ‚â• 1 ‚àí Œ¥, the ranking algorithm
must output an -ranking r0 (1), . . . ,r0 (n) such that whenever r0 (i) > r0 (j), p(i, j) ‚â• 12 ‚àí .
1.3. Outline
In Section 2 we review past work and summarize our
contributions. In Section 3 we describe and analyze our
maximum-selection algorithm. In Section 4 we propose
and evaluate the ranking algorithm. In Section 5 we experimentally compare our algorithms with existing ones. In
Section 6 we mention some future directions.

2. Old and new results

‚Ä¢ Maximum-selection
algorithm with sample complexity O n2 1 + log 1Œ¥ , optimal up to a constant factor.

‚Ä¢ Ranking algorithm with O n2 (log n)3 log nŒ¥ sample
complexity.
‚Ä¢ General framework that converts any ranking algo-
rithm with sample complexity O n2 (log n)x log nŒ¥
into a ranking algorithm that for Œ¥ ‚â• n1 has sample
complexity O n2 log n(log log n)x .
‚Ä¢ Using the above framework, a ranking algorithm
with

sample complexity O n2 log n(log log n)3 for Œ¥ = n1 .

‚Ä¢ An ‚Ñ¶ n2 log nŒ¥ lower bound on the sample complexity of any PAC ranking algorithm, matching our algorithm‚Äôs sample complexity up to a (log log n)3 factor.

2.1. Related work
Several researchers studied algorithms that with probability 1 ‚àí Œ¥ find the exact maximum and ranking. (Feige
et al., 1994) considered a simple model where the elements are ranked, and pÃÉ(i, j) =  whenever r(i) > r(j).
(Busa-Fekete et al., 2014a) considered comparison probabilities p(i, j) satisfying the Mallows model (Mallows,
1957). And (Urvoy et al., 2013; Busa-Fekete et al., 2014b;
Heckel et al., 2016) considered general comparison probabilities, without an underlying ranking assumption, and derived rankings based on Copeland- and Borda-counts, and
random-walk procedures. As expected, when the comparison probabilities approach half, the above algorithms require arbitrarily many comparisons.
To achieve finite complexity even with near-half comparison probabilities, researchers adopted the PAC paradigm.
For the PAC model with SST and STI constraints, (Yue &
Joachims, 2011) derived a maximum-selection
algorithm

n
and used it to bound
with sample complexity O n2 log Œ¥
the regret of the problem‚Äôs dueling-bandits variant. Related
results appeared in (Syrgkanis et al., 2016). For the PL
model, (SzoÃàreÃÅnyi et al., 2015) derived a PAC ranking algon
rithm with sample complexity O( n2 log n log Œ¥
).
Deterministic adversarial versions of the problem were
considered by (Ajtai et al., 2015), and by (Acharya et al.,
2014a; 2016) who were motivated by density estimation (Acharya et al., 2014b).
2.2. New results
We consider (, Œ¥)-PAC adaptive maximum selection and
ranking using pairwise comparisons under SST and STI
constraints. Note that when  ‚â• 21 or Œ¥ ‚â• 1 ‚àí 1/n for
maximum selection and Œ¥ ‚â• 1 ‚àí 1/n2 for ranking, any
output is correct. We show for  < 1/4, Œ¥ < 12 and any n:

3. Maximum selection
3.1. Algorithm outline
We propose a simple maximum-selection algorithm based
on Knockout tournaments. Knockout tournaments are used
to find a maximum element under non-noisy comparisons.
Knockout tournament of n elements runs in dlog ne rounds
where in each round it randomly pairs the remaining elements and proceeds the winners to next round.
Our
algorithm,
given
in
K NOCKOUT
uses

O n2 1 + log 1Œ¥
comparisons and O(n) memory
to find an -maximum.
(Yue & Joachims, 2011) uses

n
O n2 log Œ¥
comparisons and O(n2 ) memory to find an
-maximum. Hence we get log n-factor improvement in
the number of comparisons and also we use linear memory
compared to quadratic memory. From (Zhou & Chen,
2014) it can be inferred that the best PAC 
maximum selection algorithm requires ‚Ñ¶ n2 1 + log 1Œ¥
comparisons,
hence up to constant factor, K NOCKOUT is optimal.
(Yue & Joachims, 2011; SzoÃàreÃÅnyi et al., 2015) eliminate elements one by one until only -maximums are remaining.
Since they potentially need n ‚àí 1 eliminations, in order
to appply union bound they had to ensure that each eliminated element is not an -maximum w.p. 1 ‚àí Œ¥/n, requiring
O(log(n/Œ¥)) comparisons for each eliminated element and
hence a superlinear sample complexity O(n log(n/Œ¥)).
In contrast, K NOCKOUT eliminates elements in log n
rounds. Since in Knockout tournaments, number of elements decrease exponentially with each round, we afford
to endure more error in the initial rounds and less error
in the latter rounds by repeating comparison between each
pair more times in latter rounds. Specifically, let bi be the
highest-ranked element (according to the unobserved underlying ranking) at the beginning of round i. K NOCKOUT
makes sure that w.p. ‚â• 1‚àí 2Œ¥i , pÃÉ(bi , bi+1 ) ‚â§ i by repeating

Maximum Selection and Ranking under Noisy Comparisons

comparison between each pair in round i for O
c
2i/3



1
2i

log

2i
Œ¥



times. Choosing i =
with c = 2 ‚àí1, we make sure

that comparison complexity is O n2 1 + log 1Œ¥ and by
union bound and STI, w.p. ‚â• 1 ‚àí Œ¥, pÃÉ(b1 , bdlog ne+1 ) ‚â§
Pdlog ne+1 c
‚â§ .
i=1
2i/3
For Œ≥ ‚â• 1, a relaxed notion of SST, called Œ≥-stochastic
transitivity (Yue & Joachims, 2011), requires that if r(i) >
r(j) > r(k), then max(pÃÉ(i, j), pÃÉ(j, k)) ‚â§ Œ≥ ¬∑ pÃÉ(i, k). Our
results apply to this general notion of Œ≥-stochastic transitivity and the analysis of K NOCKOUT
under
 4 is presented

1
1
+
log
comthis model. K NOCKOUT uses O nŒ≥
2
Œ¥
parisons.
Remark 1. (Yue & Joachims, 2011) considered a different definition of -maximum as an element i that is at most
 dissimilar to true maximum i.e., for j with r(j) = n,
pÃÉ(j, i) ‚â§ . Note that this definition is less restrictive than
ours, hence requires fewer comparisons.
this
 defi Under
nŒ≥ 6
n
nition, (Yue & Joachims, 2011) used O 2 log Œ¥ comparisons to find an -maximum whereas
 2 a simple modifica
1
1
+
log
tion of K NOCKOUT shows that O nŒ≥
com2

Œ¥
parisons suffice. Hence we also get a significant improvement in the exponent of Œ≥.
To simplify the analysis, we assume that n is a power of
2, otherwise we can add 2dlog ne ‚àí n dummy elements that
lose to every original element with probability 1. Note that
all -maximums will still be from the original set.
3.2. Algorithm
We start with a subroutine C OMPARE that compares two
elements. It compares two elements i, j and maintains empirical probability pÃÇi , a proxy for p(i, j). It also maintains a
confidence value cÃÇ s.t., w.h.p., pÃÇi ‚àà (p(i, j)‚àí cÃÇ, p(i, j)+ cÃÇ).
C OMPARE stops if it is confident about the winner or if it
reaches its comparison budget m. It outputs the element
with more wins breaking ties randomly.
Algorithm 1 C OMPRARE
Input: element i, element j, bias , confidence Œ¥.
Initialize: pÀÜi = 12 , cÃÇ = 12 , m = 212 log 2Œ¥ , r = 0, wi = 0.
1. while (|pÀÜi ‚àí 12 | ‚â§ cÃÇ ‚àí  and r ‚â§ m)
(a) Compare i and j. if i wins wi = wi + 1.
q
2
1
(b) r = r + 1, pÀÜi = wri , cÃÇ = 2r
log 4rŒ¥ .
if pÀÜi ‚â§

1
2

Lemma 2. If pÃÉ(i, j) ‚â• , then

1/3

Output: j. else Output: i.

We show that C OMPARE w.h.p., outputs the correct winner
if the elements are well seperated.

P r(C OMPARE(i, j, , Œ¥) 6= i) ‚â§ Œ¥.
Note that instead of using fixed number of comparisons,
C OMPARE stops the comparisons adaptively if it is confident about the winner. If |pÃÉ(i, j)|  , C OMPARE stops
much before comparison budget 212 log 2Œ¥ and hence works
better in practice.
Now we present the subroutine K NOCKOUT-ROUND that
we use in main algorithm K NOCKOUT.
3.2.1. K NOCKOUT-ROUND
K NOCKOUT-ROUND takes a set S and outputs a set of size
|S|/2. It randomly pairs elements, compares each pair using C OMPARE, and returns the set of winners. We will later
show that maximum element in the output set will be comparable to maximum element in the input set.
Algorithm 2 K NOCKOUT-ROUND
Input: Set S, bias , confidence Œ¥.
Initialize: Set O = ‚àÖ.
1. Pair elements in S randomly.
2. for every pair (i, j):
Add C OMPARE(i, j, , Œ¥) to O.
Output: O
Note that comparisons between each pair can be handled
by a different processor and hence this algorithm can be
easily parallelized.
S can have several maximum elements. Comparison probabilities corresponding to all maximum elements will be
essentially same because of STI. We define max(S) to be
the maximum element with the least index, namely,


def
max(S) = S min{i : pÃÉ(S(i), S(j)) ‚â• 0 ‚àÄj} .
Lemma 3. K NOCKOUT-ROUND(S, , Œ¥) uses
comparisons and with probability ‚â• 1 ‚àí Œ¥,



pÃÉ max(S), max K NOCKOUT-ROUND(S, , Œ¥)

|S|
42

log

2
Œ¥

!
‚â§ Œ≥.

3.2.2. K NOCKOUT
Now we present the main algorithm K NOCKOUT. K NOCK OUT takes an input set S and runs log n rounds of
K NOCKOUT-ROUND halving the size of S at the end of
each round. Recall that K NOCKOUT-ROUND makes sure
that maximum element in the output set is comparable to

Maximum Selection and Ranking under Noisy Comparisons

maximum element in the input set. Using this, K NOCK OUT makes sure that the output element is comparable to
maximum element in the input set.
Since the size of S gets halved after each round, K NOCK OUT compares each pair more times in the latter rounds.
Hence the bias between maximum element in input set and
maximum element in output set is small in latter rounds.
Algorithm 3 K NOCKOUT
Input: Set S, bias , confidence Œ¥, stochasticity Œ≥.
Initialize: i = 1, S = set of all elements, c = 21/3 ‚àí 1.
while |S| > 1


Œ¥
1. S = K NOCKOUT-ROUND S, Œ≥ 2c
,
i
i/3
2 .
2
2. i = i + 1.
Output: the unique element in S.
Note that K NOCKOUT uses only memory of set S and
hence O(n) memory suffices.
Theorem 4 shows that K NOCKOUT outputs an -maximum
with probability ‚â• 1 ‚àí Œ¥. It also bounds the number of
comparisons used by the algorithm.
Theorem
4.  K NOCKOUT(S, , Œ¥)
uses
 4

Œ≥ |S|
1
O 2 1 + log Œ¥
comparisons and with probability at least 1 ‚àí Œ¥, outputs an -maximum.

4. Ranking

algorithm that works for general model.
The main algorithm B INARY-S EARCH -R ANKING randomly selects (lognn)x elements (anchors) and rank them
using R ANK -x . The algorithm has then effectively created (lognn)x bins, each between two successively ranked
anchors. Then for each element, the algorithm identifies
the bin it belongs to using a noisy binary search algorithm.
The algorithm then ranks the elements within each bin using R ANK -x .
We first present M ERGE -R ANK, a R ANK -3 algorithm.
4.1. Merge Ranking
We present
ranking
algorithm M ERGE -R ANK that

 a simple
n(log n)3
n
uses O
log Œ¥ comparisons, O(n) memory and
2
with probability ‚â• 1 ‚àí Œ¥ outputs an -ranking. Thus
M ERGE -R ANK is a R ANK -x algorithm for x = 3.
Similar to Merge Sort, M ERGE -R ANK divides the elements
into two sets of equal size, ranks them separately and combines the sorted sets. Due to the noisy nature of comparisons, M ERGE -R ANK compares two elements i, j sufficient times, so that the comparison output is correct with
high probability when |pÃÉ(i, j)| ‚â• log n . Put differently,
M ERGE -R ANK is same as the typical Merge Sort, except it
uses C OMPARE as the comparison function. Due to lack of
space, M ERGE -R ANK is presented in Appendix A.
Let‚Äôs define the error of an ordered set S as the maximum
distance between two wrongly ordered items in S, namely,
def

err(S) =
We propose a ranking
algorithm that

 with probability at
n log n(log log n)3
1
least 1‚àí n uses O
comparisons and out2
puts an -ranking.


n
comparisons for Œ¥ = n1
Notice that we use only OÃÉ n log
2

where as (SzoÃàreÃÅnyi et al., 2015) uses O n(log n)2 /2
comparisons even for constant error probability Œ¥. Furthermore (SzoÃàreÃÅnyi et al., 2015) provided these guarantees
only under Plackett-Luce model which is more restrictive
compared to ours. Also, their algorithm uses O(n2 ) memory compared to O(n) memory requirement of ours.
Our main algorithm B INARY-S EARCH -R ANKING assumes
the existence of a ranking algorithm R ANK -x that
 with
probability at least 1 ‚àí Œ¥ uses O n2 (log n)x log nŒ¥ comparisons and outputs an -ranking for any Œ¥ > 0,  > 0 and
some x > 1. We also present a R ANK -x algorithm with
x = 3.
Observe that we need R ANK -x algorithm to work for any
model that satisfies SST and STI. (SzoÃàreÃÅnyi et al., 2015)
showed that their algorithm works for Plackett-Luce model
but not for more general model. So we present a R ANK -x

max

pÃÉ(S(i), S(j)).

1‚â§i‚â§j‚â§|S|

We show that when we merge two ordered sets, the error of
the resulting ordered set will be at most log n more than the
maximum of errors of individual ordered sets.
Observe that M ERGE -R ANK is a recursive algorithm and
the error of a singleton set is 0. Two singleton sets each
containing a unique element from the input set merge to
2
form a set with two elements with an error at most log
n,
then two sets with two elements merge to form a set with
3
four elements with an error of at most log
n and henceforth.
Thus the error of the output ordered set is bounded by .
Lemma 5 shows that M ERGE -R ANK can output an ranking of S with probability ‚â• 1 ‚àí Œ¥. It also bounds the
number of comparisons used by the algorithm.


Œ¥
Lemma
5. M ERGE -R ANK S, log|S| , |S|
takes
2


|S|(log |S|)3
|S|
O
log Œ¥ comparisons and with probability
2
‚â• 1 ‚àí Œ¥, outputs an -ranking. Hence, M ERGE -R ANK is a
R ANK -3 algorithm.
Now we present our main ranking algorithm.

Maximum Selection and Ranking under Noisy Comparisons

4.2. B INARY-S EARCH -R ANKING
We first sketch the algorithm outline below. We then provide a proof outline.
4.2.1. A LGORITHM OUTLINE
Our algorithm is stated in B INARY-S EARCH -R ANKING. It
can be summarized in three major parts.
Creating anchors: (Steps 1 to 3) B INARY-S EARCH R ANKING first selects a set S 0 of (lognn)x random elements
(anchors) and ranks them using R ANK -x . At the end of
this part, there are (lognn)x ranked anchors. Equivalently,
the algorithm creates (lognn)x ‚àí 1 bins, each bin between
two successively ranked anchors.
Coarse ranking: (Step 4) After forming the bins, the algorithm uses a random walk on a binary search tree, to find
which bin each element belongs to. I NTERVAL -B INARYS EARCH is similar to the noisy binary search algorithm
in (Feige et al., 1994). It builds a binary search tree with the
bins as the leaves and it does a random walk over this tree.
Due to lack of space the algorithm I NTERVAL -B INARYS EARCH is presented in Appendix B but more intuition is
given later in this section.
Ranking within each bin: (Step 5) For each bin, we
show that the number of elements far from both anchors
is bounded. The algorithm checks elements inside a bin
whether they are close to any of the bin‚Äôs anchors. For
the elements that are close to anchors, the algorithm ranks
them close to the anchor. And for the elements that are
away from both anchors the algorithm ranks them using
R ANK -x and outputs the resulting ranking.
4.2.2. A NALYSIS OF B INARY-S EARCH -R ANKING
Creating anchors In Step 1 of the algorithm we select
n/(log n)x random elements. Since these are chosen uniformly random, they lie nearly uniformly in the set S. This
intuition is formalized in the next lemma.
Lemma 6. Consider a set S of n elements. If we select
n
(log n)x elements uniformly randomly from S and build an
ordered set S 0 s.t. pÃÉ(S 0 (i), S 0 (j)) ‚â• 0 ‚àÄi > j , then with
probability ‚â• 1 ‚àí n14 , for any  > 0 and all k,

Algorithm 4 B INARY-S EARCH -R ANKING
Input: Set S, bias .
o
Initialize: 0 = /16, 00 = /15,
j and S k = ‚àÖ. Sj = ‚àÖ,

Cj = ‚àÖ and Bj = ‚àÖ, for 1 ‚â§ j ‚â§

n
(log n)x

+ 2.

k
j
1. Form a set S 0 with (lognn)x random elements from
S. Remove these elements from S.

2. Rank S 0 using R ANK -x S 0 , 0 , n16 .
3. Add dummy element S
a at the beginning of S 0 such that
p(a, e) = 0 ‚àÄe ‚àà S S 0 . Add dummy element
b at
S
the end of S 0 such that p(b, e) = 1 ‚àÄe ‚àà S S 0 .
4. for e ‚àà S:
(a) k = I NTERVAL -B INARY-S EARCH(S 0 , e, 00 ).
(b) Insert e in Sk .
j
k
5. for j = 1 to (lognn)x + 2:
(a) for e ‚àà Sj :
i. if
2(e,S 0 (j), 1000‚àí2 log n) ‚àà
 1 C OMPARE
00 1
00
, insert e in Cj .
2 ‚àí 6 , 2 + 6
0
ii. else
if
C OMPARE
+

 1 2(e,00S 1(j
00
00‚àí2
‚àí
6
,
+
6
,
1), 10
log n) ‚àà
2
2
then insert e in Cj+1 .
iii. else insert e in Bj .

(b) Rank Bj using R ANK -x Bj , 00 , n14 .
(c) Append S 0 (j), Cj , Bj in order at the end of S o .
Output: S o

anchor and a right anchor . We say that an element belongs
to a bin if it wins over the bin‚Äôs left anchor with probability
‚â• 12 and wins over the bin‚Äôs right anchor with probability ‚â§ 12 . Notice that some elements might win over S 0 (1)
with probability < 12 and thus not belong to any bin. So in
0
Step 3, we add a dummy element a at
Sthe0 beginning of S
where a loses to every element in S S with probability
1. For similar reasons we add a dummy
S element b to the
end of S 0 where every element in S S 0 loses to b with
probability 1.

Coarse Ranking Note that S 0 (i) and S 0 (i + 1) are respec|{e ‚àà S : pÃÉ(e, S 0 (k)) > , pÃÉ(S 0 (k+1), e) > }| ‚â§ 5(log n)x+1 .
tively the left and right anchors of the bin Si .
In Step 2, we use R ANK -x to rank S 0 . Lemma 7 shows the
guarantee of ranking S 0 .
Lemma 7. After Step 2 of the B INARY-S EARCH R ANKING with probability ‚â• 1 ‚àí n16 , S 0 is 0 -ranked.
At the end of Step 2, we have (lognn)x ‚àí 1 bins, each between two successively ranked anchors. Each bin has a left

Algorithm 5 C OMPARE 2
Input: element i, element j, number of comparisons m.
1. Compare i and j for m times and return the fraction
of times i wins over j.

Maximum Selection and Ranking under Noisy Comparisons

Since S 0 is 0 -ranked and the comparisons are noisy, it is
hard to find a bin Si for an element e such that p(e, S 0 (i)) ‚â•
1
1
0
00
2 and p(S (i + 1), e) ‚â• 2 . We call a bin Si a  ‚àínearly
1
0
correct bin for an element e if p(e, S (i)) ‚â• 2 ‚àí 00 and
p(S 0 (i + 1), e) ‚â• 12 ‚àí 00 for some 00 > 0 .

S EARCH ran for at most 30 log n steps, Q can have at most
60 log n elements and hence B INARY-S EARCH can search
effectively by repeating each comparison O(log n) times
to maintain high confidence. Next paragraph explains how
B INARY-S EARCH finds such an element f .

In Step 4, for each element we find an 00 -nearly correct bin
using I NTERVAL -B INARY-S EARCH . Next we describe an
outline of I NTERVAL -B INARY-S EARCH.

B INARY-S EARCH first compares e with the middle element m of Q for O(log n) times. If the fraction of
wins for e is between 12 ‚àí 300 and 21 + 300 , then w.h.p.
|pÃÉ(e, m)| ‚â§ 400 and hence B INARY-S EARCH outputs m.
If the fraction of wins for e is less than 21 ‚àí 300 , then w.h.p.
pÃÉ(e, m) ‚â§ ‚àí200 and hence it eliminates all elements to the
right of m in Q. If the fraction of wins for e is more than
1
00
00
2 + 3 , then w.h.p. pÃÉ(e, m) ‚â• 2 and hence it eliminates
all elements to the left of m in Q. It continues this process
until it finds an element f such that the fraction of wins for
e is between 21 ‚àí 300 and 12 + 300 .

I NTERVAL -B INARY-S EARCH first builds a binary search
tree of intervals (see Appendix B) as follows: the root node
is the entire interval between the first and the last elements
in S 0 . Each non-leaf node interval I has two children corresponding to the left and right halves of I. The leaves of the
tree are the bins between two successively ranked anchors.
To find an 00 -nearly correct bin for an element e, the algorithm starts at the root of the binary search tree and at every
non-leaf node corresponding to interval I, it checks if e belongs to I or not by comparing e with I‚Äôs left and right
anchors. If e loses to left anchor or wins against the right
anchor, the algorithm backtracks to current node‚Äôs parent.
If e wins against I‚Äôs left anchor and loses to its right one,
the algorithm checks if e belongs to the left or right child
by comparing e with the middle element of I and moves
accordingly.
When at a leaf node, the algorithm checks if e belongs to
the bin by maintaining a counter. If e wins against the bin‚Äôs
left anchor and loses to the bin‚Äôs right anchor, it increases
the counter by one or otherwise it decreases the counter by
one. If the counter is less than 0 the algorithm backtracks
to the bin‚Äôs parent. By repeating each comparison several
times, the algorithm makes a correct decision with proba19
.
bility ‚â• 20
Note that there could be several 00 -nearly correct bins for
e and even though at each step the algorithm moves in the
direction of one of them, it could end up moving in a loop
and never reaching one of them. We thus run the algorithm
for 30 log n steps and terminate.
If the algorithm is at a leaf node by 30 log n steps and the
counter is more than 10 log n we show that the leaf node bin
is a 00 -nearly correct bin for e and the algorithm outputs
the leaf node. If not, the algorithm puts in a set Q all the
anchors visited so far and orders Q according to S 0 .
We select 30 log n steps to ensure that if there is only one
nearly correct bin, then the algorithm outputs that bin w.p.
‚â• 1 ‚àí n16 . Also we do not want too many steps so as to
bound the size of Q.
By doing a simple binary search in Q using B INARYS EARCH (see Appendix B) we find an anchor f ‚àà Q
such that |pÃÉ(e, f )| ‚â§ 400 . Since I NTERVAL -B INARY-

In next Lemma, we show that I NTERVAL -B INARYS EARCH achieves to find a 500 -nearly correct bin for every
element.
Lemma 8. For any element e ‚àà S, Step 4 of
B INARY-S EARCH -R ANKING places e in bin Sl such that
pÃÉ(e, S 0 (l)) > ‚àí500 and pÃÉ(S 0 (l + 1), e) > ‚àí500 with probability ‚â• 1 ‚àí n15 .
Ranking within each bin Once we have identified the bins,
we rank the elements inside each bin. By Lemma 6, inside
each bin all elements are close to the bin‚Äôs anchors except
at most 5(log n)x+1 of them.
The algorithm finds the elements close to anchors in Step
5a by comparing each element in the bin with the bin‚Äôs
anchors. If an element in bin Sj is close to bin‚Äôs anchors
S 0 (j) or S 0 (j + 1) , the algorithm moves it to the set Cj
or Cj+1 accordingly and if it is far away from both, the algorithm moves it to the set Bj . The following two lemmas
state that this separating process happens accurately with
high probability. The proofs of these results follow from
the Chernoff bound and hence omitted.
Lemma 9. At the end of Step 5a, for all j, ‚àÄe ‚àà Cj ,
|pÃÉ(e, S 0 (j))| < 700 with probability ‚â• 1 ‚àí n13 .
Lemma 10. At the end of Step 5a, for all j, ‚àÄe ‚àà Bj ,
min(pÃÉ(e, S 0 (j)), pÃÉ(S 0 (j + 1), e)) > 500 with probability
‚â• 1 ‚àí n13 .
Combining Lemmas 6, 7 and 10 next lemma shows that the
size of Bj is bounded for all j.
Lemma 11. At the end of Step 5a, |Bj | ‚â§ 5(log n)x+1 for
all j, with probability ‚â• 1 ‚àí n33 .
Since all the elements in Cj are already close to an anchor, they need not be ranked. By Lemma 11 with probability ‚â• 1 ‚àí n33 the number of elements in Bj is at most
5(log n)x+1 . We use R ANK -x to rank each Bj and output
the final ranking.

Maximum Selection and Ranking under Noisy Comparisons

Lemma 12 shows that all Bj ‚Äôs are 00 -ranked at the end of
Step 5b. Proof follows from properties of R ANK -x and
union bound.

Combining the above set of results yields our main result.
Theorem 13. Given access to R ANK -x, B INARYS EARCH
-R ANKING with probability ‚â• 1 ‚àí n1 , uses

x

log n)
O n log n(log
2
ranking.

KNOCKOUT
MalllowsMPI
AR
BTM-PAC

Sample complexity

Lemma 12. At the end of Step 5b, all Bj s are 00 -ranked
with probability ‚â• 1 ‚àí n13 .

10 6

comparisons and outputs an -

10 5

10 4

10 3

n=7

n=10

n=15

Number of elements

Using M ERGE -R ANK as a R ANK -x algorithm with x = 3
leads to the following corollary.

Figure 1. Comparison of sample complexity for small input sizes,
with  = 0.05, and Œ¥ = 0.1

Corollary
14. B INARY-S EARCH -R ANKING
uses

log n)3
comparisons
and
outputs
an
O n log n(log
2

Using PALPAC-AMPRR (SzoÃàreÃÅnyi et al., 2015) as a
R ANK -x algorithm with x = 1 leads to the following corollary over PL model.
Corollary 15. Over PL model, B INARY-S EARCH R ANKING
with probability ‚â•
1 ‚àí n1 uses

log n
comparisons and outputs an -ranking.
O n log nlog
2

KNOCKOUT
10

Sample complexity

ranking with probability ‚â• 1 ‚àí

1
n.

10 9

8

MallowsMPI
AR

10 7

10

6

10 5

10

4

n = 50

n = 100

n = 200

n = 500

Number of elements

It is well known that to rank a set of n values under the
noiseless setting, ‚Ñ¶(n log n) comparisons are necessary.

We show that under the noisy model, ‚Ñ¶ n2 log nŒ¥ samples
are necessary to output an -ranking and hence our algorithm is near-optimal.
1
4,

1
2,

Theorem 16. For  ‚â§ Œ¥ ‚â§ there exists a noisy model
that satisfies SST and STI such that to output
an -ranking

with probability ‚â• 1 ‚àí Œ¥, ‚Ñ¶ n2 log nŒ¥ comparisons are
necessary.

5. Experiments
We compare the performance of our algorithms with that
of others over simulated data. Similar to (Yue & Joachims,
2011), we consider the stochastic model where p(i, j) =
0.6 ‚àÄi < j. Note that this model satisfies both SST and STI.
We find 0.05-maximum with error probability Œ¥ = 0.1. Observe that i = 1 is the only 0.05-maximum. We compare
the sample complexity of K NOCKOUT with that of BTMPAC (Yue & Joachims, 2011), MallowsMPI (Busa-Fekete
et al., 2014a), and AR (Heckel et al., 2016). BTM-PAC is
an (, Œ¥)-PAC algorithm for the same model considered in
this paper. MallowsMPI finds a Condorcet winner which
exists under our general model. AR finds the maximum according to Borda scores. We also tried PLPAC (SzoÃàreÃÅnyi
et al., 2015), developed originally for PL model but the algorithm could not meet guarantees of Œ¥ = 0.1 under this

Figure 2. Comparison of sample complexity for large input size,
with  = 0.05, and Œ¥ = 0.1

model and hence omitted. Note that in all the experiments
the reported numbers are averaged over 100 runs.
In Figure 1, we compare the sample complexity of algorithms when there are 7, 10 and 15 elements. Our algorithm outperforms all the others. BTM-PAC performs
much worse in comparison to others because of high constants in the algorithm. Further BTM-PAC allows comparing an element with itself since the main objective in
(Yue & Joachims, 2011) is to reduce the regret. We exclude
BTM-PAC for further experiments with higher number of
elements.
In Figure 2, we compare the algorithms when there are 50,
100, 200 and 500 elements. Our algorithm outperforms
others for higher number of elements too. Performance of
AR gets worse as the number of elements increases since
Borda scores of the elements get closer to each other and
hence AR takes more comparisons to eliminate an element.
Notice that number of comparisons is in logarithmic scale
and hence the performance of MallowsMPI appears to be
close to that of ours.
As noted in (SzoÃàreÃÅnyi et al., 2015), sample complexity of
MallowsMPI gets worse as pÃÉ(i, j) gets close to 0. To

Maximum Selection and Ranking under Noisy Comparisons

KNOCKOUT

0.09
0.05
0.01
0.001
0.0001
0.00001

12

Sample complexity

MallowsMPI

Sample complexity

10 6

14

10 8

10 7

10 6

10
8
6
4
2

10

5

0.01

0.005

0

0.001

0

200

400

600

800

1000

Number of elements

Figure 3. Sample complexity of K NOCKOUT and MallowsMPI
for different values of qÃÉ, with  = 0.05 and Œ¥ = 0.1
10 8

Sample complexity

MallowsMPI
KNOCKOUT

10 6

10 4

10 2
0

0.2

0.4

0.6

0.8

1

Figure 4. Sample complexity of K NOCKOUT and MallowsMPI
under Mallows model for various values of œÜ

show the pronounced effect, we use the stochastic model
p(1, j) = 0.6 ‚àÄj > 1, p(i, j) = 0.5 + qÃÉ ‚àÄj > i, i > 1
where qÃÉ < 0.1, and the number of elements is 15. Here too
we find 0.05-maximum with Œ¥ = 0.1. Note that i = 1 is
the only 0.05-maximum in this stochastic model. In Figure 3, we compare the algorithms for different values of
qÃÉ: 0.01, 0.005 and 0.001. As discussed above, the performance of MallowsMPI gets much worse whereas our
algorithm‚Äôs performance stays unchanged. The reason is
that MallowsMPI finds the Condorcet winner using successive elimination technique and as qÃÉ gets closer to 0,
MallowsMPI takes more comparisons for each elimination. Our algorithm tries to find an alternative which defeats Condorcet winner with probability ‚â• 0.5 ‚àí 0.05 and
hence for alternatives that are very close to each other, our
algorithm declares either one of them as winner after comparing them for certain number of times.
Next we evaluate K NOCKOUT on Mallows model which
does not satisfy STI. Mallows is a parametric model which
is specified by single parameter œÜ. As in (Busa-Fekete
et al., 2014a), we consider n = 10 elements and various
values for œÜ: 0.03, 0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95 and 0.99.
Here again we seek to find 0.05-maximum with Œ¥ = 0.05.

Figure 5. Sample complexity of M ERGE -R ANK for different 

As we can see in Figure 4, sample complexity of K NOCK OUT and MallowsMPI is essentially same under small values of œÜ but K NOCKOUT outperforms MallowsMPI as œÜ
gets close to 1 since comparison probabilities grow closer
to 1/2. Surprisingly, for all values of œÜ except for 0.99,
K NOCKOUT returned Condorcet winner in all runs. For
œÜ = 0.99, K NOCKOUT returned second best element in 10
runs out of 100. Note that pÃÉ(1, 2) = 0.0025 and hence
K NOCKOUT still outputed a 0.05-maximum. Even though
we could not show theoretical guarantees of K NOCKOUT
under Mallows model, our simulations suggest that it can
perform well even under this model.
For the stochastic model p(i, j) = 0.6 ‚àÄi < j, we run
our M ERGE -R ANK algorithm to find an -ranking with
Œ¥ = 0.1. Figure 5 shows that sample complexity does not
increase a lot with decreasing . We attribute this to the
subroutine C OMPARE that finds the winner faster when the
elements are more dissimilar.
Some more experiments are provided in Appendix G.

6. Conclusion
We studied maximum selection and ranking using noisy
comparisons for broad comparison models satisfying SST
and STI. For maximum selection we presented a simple
algorithm with linear, hence optimal, sample complexity.
For ranking we presented a framework that improves the
performance of many ranking algorithms and applied it to
merge ranking to derive a near-optimal algorithm.
We conducted several experiments showing that our algorithms perform well and out-perform existing algorithms
on simulated data.
The maximum-selection experiments suggest that our algorithm performs well even without STI. It would be of interest to extend our theoretical guarantees to this case. For
ranking, it would be interesting to close the (log log n)3 ratio between the upper- and lower- complexity bounds.

Maximum Selection and Ranking under Noisy Comparisons

7. Acknowledgements
We thank Yi Hao and Vaishakh Ravindrakumar for very
helpful discussions and suggestions, and NSF for supporting this work through grants CIF-1564355 and CIF1619448.

References
Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and Suresh,
Ananda Theertha. Sorting with adversarial comparators and
application to density estimation. In ISIT, pp. 1682‚Äì1686.
IEEE, 2014a.
Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and Suresh,
Ananda Theertha. Near-optimal-sample estimators for spherical gaussian mixtures. NIPS, 2014b.

Negahban, Sahand, Oh, Sewoong, and Shah, Devavrat. Iterative
ranking from pair-wise comparisons. In NIPS, pp. 2474‚Äì2482,
2012.
Negahban, Sahand, Oh, Sewoong, and Shah, Devavrat. Rank centrality: Ranking from pairwise comparisons. Operations Research, 2016.
Plackett, Robin L. The analysis of permutations. Applied Statistics, pp. 193‚Äì202, 1975.
Radlinski, Filip and Joachims, Thorsten. Active exploration for
learning rankings from clickthrough data. In Proceedings of
the 13th ACM SIGKDD, pp. 570‚Äì579. ACM, 2007.
Radlinski, Filip, Kurup, Madhu, and Joachims, Thorsten. How
does clickthrough data reflect retrieval quality? In Proceedings of the 17th ACM conference on Information and knowledge management, pp. 43‚Äì52. ACM, 2008.

Acharya, Jayadev, Falahatgar, Moein, Jafarpour, Ashkan, Orlitsky, Alon, and Suresh, Ananda Theertha. Maximum selection
and sorting with adversarial comparators and an application to
density estimation. arXiv preprint arXiv:1606.02786, 2016.

Rajkumar, Arun and Agarwal, Shivani. A statistical convergence
perspective of algorithms for rank aggregation from pairwise
data. In Proc. of the ICML, pp. 118‚Äì126, 2014.

Ajtai, MikloÃÅs, Feldman, Vitaly, Hassidim, Avinatan, and Nelson, Jelani. Sorting and selection with imprecise comparisons.
ACM Transactions on Algorithms (TALG), 12(2):19, 2015.

Syrgkanis, Vasilis, Krishnamurthy, Akshay, and Schapire,
Robert E. Efficient algorithms for adversarial contextual learning. arXiv preprint arXiv:1602.02454, 2016.

Bradley, Ralph Allan and Terry, Milton E. Rank analysis of incomplete block designs: I. the method of paired comparisons.
Biometrika, 39(3/4):324‚Äì345, 1952.
Busa-Fekete, RoÃÅbert, HuÃàllermeier, Eyke, and SzoÃàreÃÅnyi, BalaÃÅzs.
Preference-based rank elicitation using statistical models: The
case of mallows. In Proc. of the ICML, pp. 1071‚Äì1079, 2014a.
Busa-Fekete, RoÃÅbert, SzoÃàreÃÅnyi, BalaÃÅzs, and HuÃàllermeier, Eyke.
Pac rank elicitation through adaptive sampling of stochastic
pairwise preferences. In AAAI, 2014b.
Feige, Uriel, Raghavan, Prabhakar, Peleg, David, and Upfal, Eli.
Computing with noisy information. SIAM Journal on Computing, 23(5):1001‚Äì1018, 1994.
Heckel, Reinhard, Shah, Nihar B, Ramchandran, Kannan, and
Wainwright, Martin J. Active ranking from pairwise comparisons and when parametric assumptions don‚Äôt help. arXiv
preprint arXiv:1606.08842, 2016.
Herbrich, Ralf, Minka, Tom, and Graepel, Thore. Trueskill: a
bayesian skill rating system. In Proceedings of the 19th International Conference on Neural Information Processing Systems, pp. 569‚Äì576. MIT Press, 2006.
Jang, Minje, Kim, Sunghyun, Suh, Changho, and Oh, Sewoong.
Top-k ranking from pairwise comparisons: When spectral
ranking is optimal. arXiv preprint arXiv:1603.04153, 2016.
Luce, R Duncan. Individual choice behavior: A theoretical analysis. Courier Corporation, 2005.
Mallows, Colin L. Non-null ranking models. i. Biometrika, 44
(1/2):114‚Äì130, 1957.
Mukherjee, Sudipta. Data structures using C: 1000 problems and
solutions. McGraw Hill Education, 2011.

SzoÃàreÃÅnyi, BalaÃÅzs, Busa-Fekete, RoÃÅbert, Paul, Adil, and
HuÃàllermeier, Eyke. Online rank elicitation for plackett-luce:
A dueling bandits approach. In NIPS, pp. 604‚Äì612, 2015.
Urvoy, Tanguy, Clerot, Fabrice, FeÃÅraud, Raphael, and Naamane,
Sami. Generic exploration and k-armed voting bandits. In
Proc. of the ICML, pp. 91‚Äì99, 2013.
Yue, Yisong and Joachims, Thorsten. Beat the mean bandit. In
Proc. of the ICML, pp. 241‚Äì248, 2011.
Zhou, Yuan and Chen, Xi. Optimal pac multiple arm identification
with applications to crowdsourcing. 2014.

Maximum Selection and Ranking under Noisy Comparisons

A. Merge Ranking
We first introduce a subroutine that is used by M ERGE -R ANK. It
merges two ordered sets in the presence of noisy comparisons.

A.1. M ERGE
M ERGE takes two ordered sets S1 and S2 and outputs an ordered
set Q by merging them. M ERGE starts by comparing the first elements in each set S1 and S2 and places the loser in the first position of Q. It compares the two elements sufficient times to make
sure that output is near-accurate. Then it compares the winner
and the element right to loser in the corresponding set. It continues this process until we run out of one of the sets and then adds
the remaining elements to the end of Q and outputs Q.

Algorithm 7 M ERGE -R ANK
Input: Set S, bias , confidence Œ¥.
1. S1 = M ERGE -R ANK(S(1 : b|S|/2c), , Œ¥).
2. S2 = M ERGE -R ANK(S(b|S|/2c + 1 : |S|), , Œ¥).
Output: M ERGE(S1 , S2 , , Œ¥).

B. Algorithms for Ranking

Algorithm 6 M ERGE
Input: Sets S1 , S2 , bias , confidence Œ¥.
Initialize: i = 1, j = 1 and O = ‚àÖ.
1. while i ‚â§ |S1 | and j ‚â§ |S2 |.
(a) if S1 (i) = C OMPARE(S1 (i), S2 (j), , Œ¥), then
append S2 (j) at the end of O and j = j + 1.
(b) else append S1 (i) at the end of O and i = i + 1.

Algorithm 8 I NTERVAL -B INARY-S EARCH
Input: Ordered array S, search element e, bias 

2. if i ‚â§ |S1 |, then append S1 (i : |S1 |) at the end of O.
1. T = B UILD -B INARY-S EARCH -T REE(|S|).
3. if j ‚â§ |S2 |, then append S2 (j : |S2 |) at the end of O.
Output: O.

2. Initialize set Q = ‚àÖ, node Œ± = root(T ), and count
c = 0.
3. repeat for 30 log n times

We show that when we merge two ordered sets using M ERGE, the
error of resulting ordered set is not high compared to the maximum of errors of individual ordered sets.
Lemma 17. With probability ‚â• 1 ‚àí (|S1 | + |S2 |)Œ¥, error of
M ERGE(S1 , S2 , , Œ¥) is at most  more than the maximum of errors of S1 and S2 . Namely, with probability ‚â• 1 ‚àí (|S1 | + |S2 |)Œ¥,
err(M ERGE(S1 , S2 , , Œ¥)) ‚â§ max (err(S1 ), err(S2 )) + .

A.2. M ERGE -R ANK
Now we present the algorithm M ERGE -R ANK. M ERGE -R ANK
partitions the input set S into two sets S1 and S2 each of size
|S|/2. It then orders S1 and S2 separately using M ERGE R ANK and combines the ordered sets using M ERGE. Notice that
M ERGE -R ANK is a recursive algorithm. The singleton sets each
containing an unique element in S are merged first. Two singleton sets are merged to form a set with two elements, then the sets
with two elements are merged to form a set with four elements
and henceforth. By Lemma 17, each merge with bound parameter
0 adds at most 0 to the error. Since error of singleton sets is 0
and each element takes part in log n merges, the error of the output set is at most 0 log n. Hence with bound parameter / log n,
the error of the output set is less than .

(a) if Œ±2 ‚àí Œ±1 > 1,


2
i. Add Œ±1 , Œ±2 and Œ±1 +Œ±
to Q.
2
ii. if C OMPARE 2(S(Œ±1 ), e, 10
2
 ) > 1/2 or C OM PARE 2(e, S(Œ±2 ), 10
)
>
1/2
then go back to
2

the parent, Œ± = parent(Œ±).
iii. else


2
), e, 10
‚Ä¢ if C OMPARE 2(S( Œ±1 +Œ±
2
2 ) > 1/2
go to the left child,Œ± = left(Œ±).
‚Ä¢ else go to the right child, Œ± = right(Œ±).
(b) else
i. if C OMPARE 2(e, S(Œ±1 ), 10
2 ) > 1/2 and
C OMPARE 2(S(Œ±2 ), e, 10
)
2 > 1/2,
c = c + 1.
ii. else
A. if c = 0, Œ± = parent(Œ±).
B. else c = c ‚àí 1.
4. (a) if c > 10 log n, Output: Œ±1 .
(b) else
i. Sort Q.
ii. Output: B INARY-S EARCH(S, Q, e, ).

Maximum Selection and Ranking under Noisy Comparisons

Algorithm 9 B UILD -B INARY -S EARCH -T REE
Input: size n.
// Recall that each node m in the tree is an interval between
left end m1 and right end m2 .
0

1. Initialize set T = ‚àÖ.

C. Some tools for proving lemmas
We first prove an auxilliary result that we use in the future analysis.
Lemma 18. Let W = C OMPARE(i, j, , Œ¥) and L be the other
element. Then with probability ‚â• 1 ‚àí Œ¥,
p(W, L) ‚â•

2. Initialize the tree T with the root node (1, n).
m = (1, n)

1
‚àí .
2

where m1 = 1 and m2 = n,

root(T ) = m

Proof. Note that if |pÃÉ(i, j)| < , then p(i, j) >
p(j, i) > 12 ‚àí . Hence, p(W, L) ‚â• 12 ‚àí .

3. Add m to T 0 .

1
2

‚àí  and

If |pÃÉ(i, j)| ‚â• , without loss of generality, assume that i is a better
element i.e., pÃÉ(i, j) ‚â• . By Lemma 2, with probability atleast
1 ‚àí Œ¥, W = i. Hence


1
P r p(W, L) ‚â• ‚àí  = P r(W = i) ‚â• 1 ‚àí Œ¥.
2

0

4. while T is not empty

(a) Consider a node i in T 0 .
(b) if i2 ‚àí i1 > 1, create a left child and right child
to i and set their parents as i.


 
 
We now prove a Lemma that follows from SST and STI that we
i1 + i2
i1 + i2
,
Œ≤=
, i2 , will use in future analysis.
Œ± = i1 ,
2
2
left(i) = Œ±,
parent(Œ±) = i,

right(i) = Œ≤,
parent(Œ≤) = i.

and add nodes Œ± and Œ≤ to T 0 .
(c) Remove node i from T 0 .
Output: T .

Lemma 19. If pÃÉ(i, j) ‚â§ 1 , pÃÉ(j, k) ‚â§ 2 , then pÃÉ(i, k) ‚â§ 1 + 2 .
Proof. We will divide the proof into four cases based on whether
pÃÉ(i, j) > 0 and pÃÉ(j, k) > 0.
If pÃÉ(i, j) ‚â§ 0 and pÃÉ(j, k) ‚â§ 0, then by SST, pÃÉ(i, k) ‚â§ 0 ‚â§
1 + 2 .
If 0 < pÃÉ(i, j) ‚â§ 1 and 0 < pÃÉ(j, k) ‚â§ 2 , then by STI, pÃÉ(i, k) ‚â§
1 + 2 .
If pÃÉ(i, j) < 0 and 0 < pÃÉ(j, k) ‚â§ 2 , then by SST, pÃÉ(i, k) ‚â§ 2 ‚â§
1 + 2 .
If 0 < pÃÉ(i, j) ‚â§ 1 and pÃÉ(j, k) < 0, then by SST, pÃÉ(i, k) ‚â§ 1 ‚â§
1 + 2 .

Algorithm 10 B INARY-S EARCH
Input: Ordered array S, ordered array Q, search item e,
bias .
Initialize: l = 1, h = |Q|.
1. while h ‚àí l > 0


 10 log n 
(a) t = C OMPARE 2 e, S(Q( l+h
), 2
.
2
1

1
(b) if t ‚àà
2 ‚àí 3, 2 + 3 , then Output:
 l+h 
Q( 2 ).
(c) else if t < 12 ‚àí 3, then move to the left.


l+h
h=
.
2
(d) else move to the right.


l+h
l=
.
2
Output: Q(h).

D. Proofs of Section 3
Proof of Lemma 2
Proof. Let pÃÇri and cÃÇr denote pÃÇi and cÃÇ respectively after r number
of comparisons. Output of C OMPARE(i, j, , Œ¥) will not be i only
if pÃÇri < 21 +  ‚àí cÃÇr for any r < m = 212 log 2Œ¥ or if pÀÜi < 12 for
r = m. We will show that the probability of each of these events
happening is bounded by 2Œ¥ . Hence by union bound, Lemma follows.
After r comparisons, by Hoeffding‚Äôs inequality,
P r(pÃÇri <

r 2
4r 2
1
Œ¥
+  ‚àí cÃÇr ) ‚â§ e‚àí2r(cÃÇ ) = e‚àí log Œ¥ = 2 .
2
4r

Using union bound,
P r(‚àÉr s.t. pÃÇri ‚â§
After m =

1
22

log

2
Œ¥

1
Œ¥
+  ‚àí cÃÇr ) ‚â§
2
2

rounds, by Hoeffding‚Äôs inequality,

P r(pÃÇm
i <

2
1
Œ¥
) ‚â§ e‚àí2m = .
2
2

Maximum Selection and Ranking under Noisy Comparisons

E. Proofs of Section 4.1

Proof of Lemma 3
|S|
2

1
22

2
Œ¥

pairs is compared at most
Proof. Each of the
log
|S|
‚àó
2
times, hence the total comparisons is ‚â§ 4
.
Let
k
=
log
2
Œ¥
‚àó
max(K NOCKOUT-ROUND(S, , Œ¥)) and s = max(S). Let a be
the element paired with s‚àó . There are two cases: pÃÉ(s‚àó , a) ‚â• 
and pÃÉ(s‚àó , a) < .
If pÃÉ(s‚àó , a) ‚â• , by Lemma 2 with probability ‚â• 1 ‚àí Œ¥, s‚àó will
win and hence by definitions of s‚àó and k‚àó , pÃÉ(s‚àó , k‚àó ) = 0 ‚â§ Œ≥.
Alternatively, if pÃÉ(s‚àó , a) < , let winner(s‚àó , a) denote the winner
between s‚àó and a. Then,
(a)

(b)

(c)

r(a) ‚â§ r(winner(s‚àó , a)) ‚â§ r(k‚àó ) ‚â§ r(s‚àó )
where (a) follows from r(a) ‚â§ r(s‚àó ), (b) and (c) follow from the
definitions of s‚àó and k‚àó respectively. From stochastic tranisitivity
on a, k‚àó and s‚àó , pÃÉ(s‚àó , k‚àó ) ‚â§ Œ≥ pÃÉ(s‚àó , a) ‚â§ Œ≥.

Proof of Lemma 17
Proof. Let Q = M ERGE(S1 , S2 , , Œ¥). We will show that for every k, w.p. ‚â• 1 ‚àí Œ¥, pÃÉ(Q(k), Q(l)) ‚â§ max(err(S1 ), err(S2 )) +
 ‚àÄl > k. Note that if this property is true for every element
then err(Q) ‚â§ max(err(S1 ), err(S2 )) + . Since there are
|S1 | + |S2 | elements in the final merged set, the Lemma follows
by union bound.
If S1 (i) and S2 (j) are compared in M ERGE algorithm, without
loss of generality, assume that S1 (i) loses i.e., S1 (i) appears before S2 (j) in T . The elements that appear to the
S right of S1 (i) in
Q belong to set Q‚â•S1 (i) = {S1 (k) : k > i} {S2 (k) : k ‚â• j}.
We will show that w.p. ‚â• 1 ‚àí Œ¥, ‚àÄe ‚àà Q‚â•S1 (i) , pÃÉ(S1 (i), e) ‚â§
max (err(S1 ), err(S2 )) + .
By definition of error of an ordered set,
pÃÉ(S1 (i), S1 (k)) ‚â§ err(S1 )
pÃÉ(S2 (j), S2 (k)) ‚â§ err(S2 )

Proof of Theorem 4
Proof. We first bound the number of comparisons. Let ni =
|S|
be the number of elements in the set at the beginning of
2i‚àí1
round i. The number of comparisons at round i is
ni Œ≥ 4 22i/3
2i+1
‚â§
¬∑
¬∑ log
.
2
2c2 2
Œ¥

‚àÄk > i
‚àÄk ‚â• j.

(2)
(3)

By Lemma 18, w.p. ‚â• 1 ‚àí Œ¥,
pÃÉ(S1 (i), S2 (j)) ‚â§ .

(4)

Hence by Equations 3, 4 and Lemma 19, w.p.
pÃÉ(S1 (i), S2 (k)) ‚â§  + err(S2 ) ‚àÄk ‚â• j.

‚â• 1 ‚àí Œ¥,

Hence the number of comparisons in all rounds is
log |S|



‚àû
X |S| Œ≥ 4 22i/3
|S|Œ≥ 4 X 1
2
2i+1
i
+
log
¬∑
¬∑
log
‚â§
2i
2c2 2
Œ¥
2c2 2 i=1 2i/3
Œ¥
i=1
 1/3

4
|S|Œ≥
2
2
1
= 2 2
+ log
2c 
c2
c
Œ¥



4
|S|Œ≥
1
=O
1 + log
.
2
Œ¥

We now show that with probability ‚â• 1‚àíŒ¥, the output of K NOCK i/3
i
OUT is an -maximum. Let i = c/(Œ≥2 ) and Œ¥i = Œ¥/2 . Note
that i and Œ¥i are bias and confidence values used in round i. Let
bi be a maximum element in the set S before round i. Then by
Lemma 3, with probability ‚â• 1 ‚àí Œ¥i ,
pÃÉ(bi , bi+1 ) ‚â§ i

(1)

By union bound, the probability that Equation 1 does not hold for
some round 1 ‚â§ i ‚â§ log |S| is
log |S|

‚â§

X
i=1

X Œ¥
‚â§ Œ¥.
Œ¥i =
2i
i=1

log |S|

X
i=1

pÃÉ(bi , bi+1 ) ‚â§

‚àû
X
i=1

Proof. We first bound the total comparisons. Let C(Q, 0 , Œ¥ 0 ) be
the number of comparisons that the M ERGE -R ANK uses on a set
Q. Since M ERGE -R ANK is a recursive algorithm,
C(Q, 0 , Œ¥ 0 ) ‚â§C(Q[1 : b|Q|/2c], 0 , Œ¥ 0 )
+ C(Q[b|Q|/2c : |Q|], 0 , Œ¥ 0 ) +
From
this one  can obtain

|S|
O |S| log
log Œ¥10 . Hence,
02

C |S|,

Œ¥

,
log |S| |S|2




=O

that

|Q|
2
log 0 .
202
Œ¥

C(S, 0 , Œ¥ 0 )

=


|S| log3 |S|
|S|2
.
log
2
Œ¥

Now we bound the error. By Lemma 17, with probability ‚â• 1 ‚àí
|Q|Œ¥,
err(M ERGE -R ANK(Q, 0 , Œ¥ 0 )) ‚â§

log |S|


max{err M ERGE -R ANK Q[1 : b|Q|/2c], 0 , Œ¥ 0 ,

err M ERGE -R ANK T [b|Q|/2c + 1 : |Q|], 0 , Œ¥ 0 } + 0 . (5)

With probability ‚â• 1 ‚àí Œ¥, Equation 1 holds for all i and by
stochastic triangle inequality,
pÃÉ(b1 , blog |S|+1 ) ‚â§

Proof of Lemma 5

c
= /Œ≥.
Œ≥2i/3

We now show that if pÃÉ(b1 , e) ‚â§ /Œ≥, e is an -maximum, namely
pÃÉ(f, e) ‚â§  ‚àÄf ‚àà S. Note that b1 is a maximum element in the
original set S and hence r(b1 ) = n. If r(f ) ‚â• r(e), then by
Œ≥-stochastic transitivity, pÃÉ(f, e) ‚â§ Œ≥ pÃÉ(b1 , e) ‚â§  and if r(f ) ‚â§
r(e), then pÃÉ(f, e) ‚â§ 0 ‚â§ .

We can bound the total times M ERGE is called in a single instance of M ERGE -R ANK(S, 0 , Œ¥ 0 ). M ERGE combines the singleton sets and forms the sets with two elements, it combines
the sets with two elements and forms the sets with four elements
henceforth. Hence the total times M ERGE is called
P and
|S| |S|
is log
‚â§ |S|. Therefore, the probability that Equation 5
i=1
2i
holds every time when two ordered sets are merged in M ERGE R ANK(S, 0 , Œ¥ 0 ) is ‚â§ |S| ¬∑ |S|Œ¥ 0 = |S|2 Œ¥ 0 .
If Equation 5 holds every time M ERGE is called, then error of
P |S| 0
M ERGE -R ANK(S, 0 , Œ¥ 0 ) is at most log
 ‚â§ 0 log |S|. This
i=1

Maximum Selection and Ranking under Noisy Comparisons
is because err(S) is 0 if S has only one element. And a singleton
set participates in log n merges before becoming the final output
set.
2 0

Therefore, w.p. ‚â• 1 ‚àí |S| Œ¥ ,
err(M ERGE -R ANK(S, 0 , Œ¥ 0 )) ‚â§ log |S|0 .
Hence with probability ‚â• 1 ‚àí Œ¥,


err M ERGE -R ANK S,

Œ¥

,
log |S| |S|2


‚â§ .

F. Proofs for Section 4.2
Proof of Lemma 6
Proof. Let set S be ordered s.t. pÃÉ(S(i), S(j)) ‚â• 0 ‚àÄi > j. Let
Sk00 = {S(l) : k ‚â§ l ‚â§ k + 5(log n)x+1 ‚àí 1). The probability
that none of the elements in Sk00 is selected for a given k is
‚â§


n/(log n)x
5(log n)x+1
1
1‚àí
< 5.
n
n

Therefore by union bound, the probability that none of the elements in Sk00 is selected for any k is
‚â§n¬∑

1
1
= 4.
n5
n

Proof of Lemma 8
We prove Lemma 8 by dividing it into smaller lemmas. We refer
to |pÃÉ(e, f )| as a measure of distance between elements e and f .
We divide all elements in S into two sets based on distance from
anchors. First set contains all elements that are far away from all
anchors and the second set contains all elements which are close
to atleast one of the anchors. I NTERVAL -B INARY-S EARCH acts
differently on both sets.
We first show that for elements in the first set, I NTERVAL B INARY-S EARCH places them in between the right anchors by
using just the random walk subroutine.
For elements in the second set, I NTERVAL -B INARY-S EARCH
might fail to find the right anchors just by using the random walk
subroutine. But we show that I NTERVAL -B INARY-S EARCH visits
a close anchor during random walk and B INARY-S EARCH finds a
close anchor from the set of visited anchors using simple binary
search.
We first prove Lemma 8 for the elements of first set.
Lemma 20. For 00 > 0 , consider an 0 -ranked S 0 . If an element
e is such that |pÃÉ(e, S 0 (j))| > 00 ‚àÄj, then with probability ‚â• 1 ‚àí
1
step 4a of I NTERVAL -B INARY-S EARCH(S 0 , e, 00 ) outputs
n6
the index y such that pÃÉ(e, S 0 (y)) > 00 and pÃÉ(S 0 (y + 1), e) > 00 .

Proof. We first show that there is an unique y s.t. pÃÉ(e, S 0 (y)) >
00 and pÃÉ(S 0 (y + 1), e) > 00 .
Let i be the largest index such that pÃÉ(e, S 0 (i)) > 00 . By
Lemma 19, pÃÉ(e, S 0 (j)) > 00 ‚àí 0 > 0 ‚àÄj < i. Hence by
the assumption on e, pÃÉ(e, S 0 (j)) > 00 ‚àÄj < i. Let k be the

smallest index such that pÃÉ(S 0 (k), e) > 00 . By a similar argument
as previously, we can show that pÃÉ(S 0 (j), e) > 00 ‚àÄj > k.
Hence by the above arguments and the fact that |pÃÉ(e, S 0 (j))| >
00 ‚àÄj, there exists only one y such that pÃÉ(e, S 0 (y)) > 00 and
pÃÉ(S 0 (y + 1), e) > 00 .
Thus in the tree T , there is only one leaf node w such that
pÃÉ(e, S 0 (w1 )) > 00 and pÃÉ(S 0 (w2 ), e) > 00 .
Consider some node m which is not an ancestor of w. Then either
pÃÉ(S 0 (m1 ), e) > 00 or pÃÉ(S 0 (m2 ), e) < ‚àí00 . Since we compare
e with S 0 (m1 ) and S 0 (m2 ) 10
002 times, we move to the parent of
m with probability atleast 19
.
20
Consider some node m which is an ancestor of w. Then
pÃÉ(S 0 (m
‚àí00 , pÃÉ(S 0 (m2 ), e)
>
00 , and
1 ), e)  <
m
0
00
1 +m2
|pÃÉ(S (
), e)| >  . Therefore we move in direction of
2
.
w with probability atleast 19
20
Therefore if we are not at w, then we move towards w with proba19
bility atleast 20
and if we are at w then the count c increases with
probability atleast 19
.
20
Since we start at most log n away from w if we move towards
w for 21 log n then the algorithm will output y. The probability that we will move towards w less than 21 log n times is
21 19
‚â§ e‚àí30 log nD( 30 || 20 ) ‚â§ n16 .
To prove Lemma 8 for the elements of the second set, we first
show that the random walk subroutine of algorithm I NTERVAL B INARY-S EARCH placing an element in wrong bin is highly unlikely.
Lemma 21. For 00 > 0 , consider an 0 -ranked set S 0 . Now
consider an element e and y such that either pÃÉ(S 0 (y), e) > 00
or pÃÉ(S 0 (y + 1), e) < ‚àí00 , then step 4a of I NTERVAL -B INARYS EARCH(S 0 , e, 00 ) will not output y with probability ‚â• 1 ‚àí n17 .

Proof. Recall that step 4a of I NTERVAL -B INARY-S EARCH outputs y if we are at the leaf node (y, y +1) and the count c is atleast
10 log n.
Since either pÃÉ(S 0 (y), e) > 00 or pÃÉ(S 0 (y + 1), e) < ‚àí00 ,
when we are at leaf node (y, y + 1), the count decreases with
. Hence the probability that I NTERVAL probability atleast 19
20
B INARY-S EARCH is at (y, y + 1) and the count is greater
P30 log n
‚àíi¬∑D( i‚àí102ilog n || 19
)
20
than 10 log n is at most
<
i=10 log n e
1

19

20 log ne‚àí10 log nD( 3 || 20 ) ‚â§

1
.
n7

We now show that for an element of the second set, the random
walk subroutine either places it in correct bin or visits a close
anchor.
Lemma 22. For 00 > 0 , consider an 0 -ranked set S 0 . Now
consider an element e that is close to an element in S 0 i.e.,
‚àÉg : |pÃÉ(S 0 (g), e)| < 00 . With probability ‚â• 1 ‚àí n16 , step
4a of I NTERVAL -B INARY-S EARCH(S 0 , e, 00 ) will either output
the right index y such that pÃÉ(S 0 (y), e) < 00 and pÃÉ(S 0 (y +
1), e) > ‚àí00 or I NTERVAL -B INARY-S EARCH visits S 0 (h) such
that |pÃÉ(S 0 (h), e)| < 200 .
Proof. By Lemma 21, step 4a of I NTERVAL -B INARY-S EARCH
does not output a wrong interval with probability 1 ‚àí n17 . Hence
we just need to show that w.h.p., e visits a close anchor.

Maximum Selection and Ranking under Noisy Comparisons
Let i be the largest index such that pÃÉ(e, S 0 (i)) > 200 . Then
‚àÄj < i, by Lemma 19, pÃÉ(e, S(j)) > 200 ‚àí 0 > 00 .
Let k be the smallest index such that pÃÉ(S 0 (k), e) > 200 . Then
‚àÄj > k, by Lemma 19, pÃÉ(S 0 (j), e) > 00 .
Therefore
for
u
<
v
such
that
min(|pÃÉ(S 0 (u), e)|, |pÃÉ(S 0 (v), e)|) ‚â• 200 only one of three
sets {x : x < u},{x : u < x < v} and {x : x > v} contains an
index z such that |pÃÉ(S 0 (z), e)| < 00 .
2
Let a node Œ± be s.t. for some c ‚àà {Œ±1 , Œ±2 , d Œ±1 +Œ±
e},
2
|pÃÉ(S 0 (c), e)| ‚â§ 200 . If I NTERVAL -B INARY-S EARCH reaches
such a node Œ± then we are done.

So assume that I NTERVAL -B INARY-S EARCH is at a node Œ≤ s.t.
2
‚àÄc ‚àà {Œ≤1 , Œ≤2 , d Œ≤1 +Œ≤
e}, |pÃÉ(S 0 (c), e)| > 200 . Note that only
2
one of three sets {x : x < Œ≤1 or x > Œ≤2 }, {x : Œ≤1 < x <
2
2
d Œ≤1 +Œ≤
e} and {x : d Œ≤1 +Œ≤
e < x < Œ≤2 } contains an index z
2
2
such that |pÃÉ(S 0 (z), e)| < 00 and I NTERVAL -B INARY-S EARCH
19
moves towards that set with probability 20
. Hence the probability
that we never visit an anchor that is less than 200 away is at most
15.5 19
e‚àí30 log nD( 30 || 20 ) ‚â§ n17 .

10/2 . So total comparisons in step 3 is O(log n/2 ). The
size of Q is upper bounded by 90 log n and B INARY-S EARCH
does a simple binary search over Q by repeating each comparison
10 log n/2 . Hence total comparisons used by B INARY-S EARCH
is O(log n log log n/2 )
Combining Lemmas 7, 21, 22, 23, 24 yields the result.

Proof of Lemma 11
Proof. Combining Lemmas 7, 10 and using union bound, at the
end of step 5a ,w.p. ‚â• 1 ‚àí n23 , S 0 is 0 -ranked and ‚àÄj, e ‚àà Bj ,
min(pÃÉ(e, S 0 (j)), pÃÉ(S 0 (j + 1), e)) > 500 . Hence by Lemma 19,
‚àÄj, k < j, e ‚àà Bj , pÃÉ(e, S 0 (k)) > 500 ‚àí 0 > 400 . Similarly,
‚àÄj, k > j, e ‚àà Bj , pÃÉ(S 0 (k), e) > 500 ‚àí 0 > 400 .
If |Bj | > 0, then pÃÉ(e, S 0 (k)) > 400 for e ‚àà Bj , k ‚â§ j,
pÃÉ(S 0 (l), e) > 400 for e ‚àà Bj , l ‚â• j + 1. Hence by stochastic transitivity, pÃÉ(S 0 (l), S 0 (k)) > 400 for l > j ‚â• k. Therefore
there exists k, l s.t. pÃÉ(S 0 (l), f ) > 0 ‚àÄf ‚àà {S 0 (y) : y ‚â§ j},
pÃÉ(S 0 (k), S 0 (l)) > 0 and pÃÉ(f, S 0 (k)) > 0 ‚àÄf ‚àà {S 0 (y) : y > j}.
Now by Lemma 6, w.p. ‚â• 1 ‚àí n14 , size of all such sets Bj is less
than 5(log n)x+1 .

We now complete the proof by showing that for an element e from
the second set, if Q contains an index y of an anchor that is close
to e, B INARY-S EARCH will output one such index.

Lemma follows by union bound.

Lemma 23. For 00 > 0 , consider ordered sets S 0 , Q s.t.
p(S 0 (Q(i)), S 0 (Q(j))) > 21 ‚àí 0 ‚àÄi > j. For an element e s.t.,
‚àÉg : |pÃÉ(S 0 (Q(g)), e)| < 200 , B INARY-S EARCH(S 0 , Q, e, 00 )
will return y such that |pÃÉ(S 0 (Q(y)), e)| < 400 with probability
‚â• 1 ‚àí n16 .

Proof of Theorem 13
We first bound the running time of B INARY-S EARCH -R ANKING
algorithm.
Theorem 25. B INARY-S EARCH -R ANKING terminates after
n)x
O( n(loglog
log n) comparisons with probability ‚â• 1 ‚àí n12 .
2

Proof. At any stage of B INARY-S EARCH, there are three possibilities that can happen . Consider the case when we are comparing
e with S 0 (Q(i)).

Proof. Step 2 R ANK -x(S 0 , 0 , n16 ) terminates after O( n2 log n)
comparisons with probability ‚â• 1 ‚àí n16 .

1. |pÃÉ(S 0 (Q(i)), e)| < 200 . Probability that the fraction of
wins for e is not between 21 ‚àí 300 and 12 + 300 is less than

By Lemma 8, for each element e, the step 4a I NTERVAL log n
)
B INARY-S EARCH(S 0 , e, 00 ) terminates after O( log n log
2
n log n log log n
)
comcomparisons. Hence step 4 takes at most O(
2
parisons.

e

n 002
‚àí 10 log
002 


‚â§

1
.
n10

Hence B INARY-S EARCH outputs Q(i).

2. pÃÉ(S 0 (Q(i)), e) > 200 . Probability that the fraction of wins for
‚àí 10 log n 002

‚â§ n110 . So B INARYe is more than 21 is less than e 002
S EARCH will not move right. Also notice that pÃÉ(S 0 (Q(j)), e) >
200 ‚àí 0 > 00 ‚àÄj > i.
3.

pÃÉ(S 0 (Q(i)), e) > 400 .

Probability that the fraction of
‚àí 10 log n 002

wins for e is more than 21 ‚àí 300 is less than e 002
‚â§
1
.
Hence
B
INARY
-S
EARCH
will
move
left.
Also
notice
that
10
n
pÃÉ(S 0 (Q(j)), e) > 400 ‚àí 0 > 00 ‚àÄj > i.
We can show similar results for pÃÉ(S 0 (Q(i)), e) < ‚àí200 and
pÃÉ(S 0 (Q(i)), e) < ‚àí400 . Hence if |pÃÉ(S 0 (Q(i)), e)| < 200 then
B INARY-S EARCH outputs Q(i), and if 200 < |pÃÉ(S 0 (Q(i)), e)| <
400 then either B INARY-S EARCH outputs Q(i) or moves in the
correct direction and if |pÃÉ(S 0 (Q(i)), e)| > 400 , then B INARYS EARCH moves in the correct direction.
Lemma 24. I NTERVAL -B INARY-S EARCH(S, e, ) terminates in
log n
O( log n log
) comparisons for any set S of size O(n).
2
Proof. Step 3 of I NTERVAL -B INARY-S EARCH runs for 30 log n
iterations. In each iteration, I NTERVAL -B INARY-S EARCH compares e with at most 3 anchors and repeats each comparison for

Comparing each element with the anchors in steps 5a takes at
n
most O( log
) comparisons.
2
With probability ‚â• 1 ‚àí n14 step 5b R ANK - X(Bi , 00 , n14 )
x
i |)
terminates after O(|Bi | (log |B
log n) comparisons.
By
2
Lemma 11, |Bi | ‚â§ 5(log n)x+1 for all i w.p.
‚â•
Hence, w.p.
‚â• 1 ‚àí n33 , total comparisons
1 ‚àí n33 .
P
(log |Bi |)x
to rank all Bi s is at most
log n) ‚â§
i O(|Bi |
2
P
|Bi | log n(log(5(log n)x+1 ))x
n log n(log log n)x
) = O(
).
i O(
2
2
Therefore, by summing comparisons over all steps, with
probability ‚â• 1 ‚àí n12 total comparisons is at most


log n)x
O n log n(log
.
2

Now we show that B INARY-S EARCH -R ANKING outputs an ranking with high probability.
Theorem 26. B INARY-S EARCH -R ANKING produces an ranking with probability at least 1 ‚àí n12 .
Proof. By combining Lemmas 7, 9, 10, 12 and using union
bound, w.p. ‚â• 1 ‚àí n12 , at the end of step 5b,

Maximum Selection and Ranking under Noisy Comparisons
‚Ä¢ S 0 is 0 -ranked.
‚Ä¢ Each Ci has elements such that |pÃÉ(Ci (j), S(i))| < 700 for
all j.
‚Ä¢ Each Bi has elements such that pÃÉ(S 0 (i), Bi (j)) < ‚àí500
and pÃÉ(S 0 (i + 1), Bi (j)) > 500 for all j.
‚Ä¢ All Bi s are 00 -ranked.
S
S
S
S
For j ‚â• i, e ‚àà Bi‚àí1 S 0 (i) Ci , f ‚àà S 0 (j) Cj Bj ,
0
0
0
0
pÃÉ(e, f ) ‚â§ pÃÉ(e, S (i)) + pÃÉ(S (i), S (j)) + pÃÉ(S (j), f ) ‚â§ 700 +
0 + 700 < 1500 = . Combining the above results proves the
Theorem.
Combining Theorems 25, 26 yields the result.

Proof Sketch for Theorem 16
Proof sketch. Consider a stochastic model where there is an inherent ranking r and for any two consecutive elements p(i, i +
1) = 21 ‚àí 2. Suppose there is a genie that knows the true ranking r up to the sets {r(2i ‚àí 1), r(2i)} for all i i.e., for each i,
genie knows {r(2i ‚àí 1), r(2i)} but it does not know the ranking
between these two elements. Since consecutive elements have
(i, i + 1) = 2 > , to find an -ranking, the genie has to correctly identify the ranking within all the n/2 pairs. Using Fano‚Äôs
inequality from information
 theory, it can be shown that the genie
needs at least ‚Ñ¶ n2 log nŒ¥ comparisons to identify the ranking of
the consecutive elements with probability 1 ‚àí Œ¥.

Sample complexity

10 7
KNOCKOUT
IR-BTM-PAC
R-BTM-PAC
BTM-PAC

10 6

10 5

10 4

10 3

7

10

15

Number of elements

Figure 6. Sample complexity comparison of K NOCKOUT and
variations of BTM-PAC for different input sizes, with  = 0.05
and Œ¥ = 0.1

G. Additional Experiments
As we mentioned in Section 5, BTM-PAC allows comparison of
an element with itself. It is not beneficial when the goal is to find
-maximum. So we modify their algorithm by not allowing such
comparisons. We refer to this restricted version as R-BTM-PAC.
As seen in figure, performance of BTM-PAC does not increase
by much by restricting the comparisons.
We further reduce the constants in R-BTM-PAC. We change
Equations
(7) and (8) in (Yue & Joachims, 2011) to cŒ¥ (t) =
q
log

n3 N
Œ¥

and N = d 12 log

n3 N
Œ¥

We believe the same guarantees hold even with the updated constants. We refer to this improved restricted version as IR-BTMPAC. Here too we consider the stochastic model where p(i, j) =
0.6‚àÄ i < j and we find 0.05-maximum with error probability
Œ¥ = 0.1.
In Figure 6 we compare the performance of K NOCKOUT and all
variations of BTM-PAC. As the figure suggests, the performance
of IR-BTM-PAC improves a lot but K NOCKOUT still outperforms it significantly.

5

KNOCKOUT(0.09)
KNOCKOUT(0.05)
KNOCKOUT(0.01)
KNOCKOUT(0.001)
KNOCKOUT(0.0001)
KNOCKOUT(0.00001)
MallowsMPI

12

e, respectively.

Sample complexity

1
t

10

14

10
8
6
4
2
0

0

200

400

600

800

1000

Number of elements

In Figure 7, we consider the stochastic model where p(i, j) =
0.6 ‚àÄi < j and find -maximum for different values of . Similar
to previous experiments, we use Œ¥ = 0.1. As we can see the
number of comparisons increases almost linearly with n. Further
the number of comparisons does not increase significantly even
when  decreases. Also the number of comparisons seem to be
converging as  goes to 0. K NOCKOUT outperforms MallowsMPI
even for the very small  values. We attribute this to the subroutine
C OMPARE that finds the winner faster when the distance between
elements are much larger than .

Figure 7. Sample complexity of K NOCKOUT for different values
of n and 

