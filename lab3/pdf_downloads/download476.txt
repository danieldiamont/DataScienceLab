Supplementary material

1

Introduction

This paper contains supplementary material to the main paper “A birth-death
process for feature allocation”.

2

Proofs

This section provides the proofs to theorems and equations provided in the
paper. Before providing the actual proofs, we present some useful propositions
that will help the reader understand the material that follows.
Poisson approximation to the Binomial distribution
Proposition 1. (Poisson approximation) Suppose Xn is a random variable following the Binomial distribution with number of trials as n and success ratio as
pn , denoted by Binomial(n, pn ), such that n → ∞, pn → 0 and npn → λ > 0.
Then, for k = 0, 1, 2, . . . ,
Xn → Poisson(λ),

in distribution.

In other words,
P (Xn = k) →

λk e−λ
,
k!

as n → ∞.

Normal approximation to the Poisson distribution
Proposition 2. (Normal approximation) Suppose X is a random variable following the Poisson distribution with mean λ such that X ∼ Poisson(λ). Then,
for λ → ∞,
 √ 
X → N λ, λ ,
in distribution.
√
,where λ is the standard deviation.

1

Dirac approximation to Normal distribution
Proposition 3. (Dirac approximation) Suppose X is a random variable following the Normal distribution with mean 1 and standard deviation σ = λ1 such
that X ∼ N (1, σ). Then, for σ → 0,
X → δ(1),

in distribution.

In other words,
X → 1,

as σ → 0.

Proof of proposition (1) in the main paper:
Assume that z(t) is a realization of the BDFP (Z(t)) over the finite interval
[0, T ], T > 0 and we write (z(t))0≤t≤T . With probability one the sample path
(z(t))0≤t≤T will only contain a finite number of jump events, each of which is
either a birth or a death event. We write B and Q to denote the set of the
features created or turned off by birth or death events respectively. We denote
as t1 , . . . , tJ the times when the chain jumps, where J = |B| + |Q|.
The probability of observing a sample (z(t))0≤t≤T can be written as the
product of three factors; the probability of the initial state, the probability of
each jump (event) and the probability of the interarrival times between the
events. More specifically,
• Probability of the initial state at time t = 0:
At time t = 0, the feature allocation z(0) follows an IBP distribution.
Kz(0)+
Y (N − mk )!(mk − 1)!
αKz(0)+
P(z(0)) = QH
exp (−αHN )
z(0)
N!
k=1
h=1 Kh !

(1)

where Kz(0)+ and Hz(0) are the number of total and distinct nonzero
features correspondingly in state z(0) and mk is the number of objects
that possess feature k.
• Probability of all the interarrival times between events:
The probability that the chain will not jump in the interval [tl , tl+1 ] can
be derived as follows, by dividing the time from tl to tl+1 into k intervals
of duration tl+1k−tl , and letting k → ∞:
!




k−1
Y
i(tl+1 − tl ) (tl+1 − tl )
no jump
P in [t , t ) = lim
1 − q tl +
l l+1
k→∞
k
k
i=0



!
k−1
X
i(tl+1 − tl ) (tl+1 − tl )
= exp lim
log 1 − q tl +
k→∞
k
k
i=0
 Z tl+1

= exp −
q(t)dt = exp(−qz(tl ) (tl+1 − tl ))
tl

2

where we wrote q(t) = qz(tl ) and we also considered that the rate qz(tl )
is constant in the interval tl+1 − tl . The interarrival times in [0, T ] are
independent of each other, so the probability of all the interarrival times
is a product as follows:
|B|+|D|

Y
l=0


 |B|+|Q|
 Z tl+1

Y
no jump
P in [t , t ) =
exp −
q(τ )dτ
l l+1
tl
l=0

 Z T
q(t)dt
= exp −
0
|B|+|Q|



= exp −

X

(tl+1 − tl )qz(tl )



(2)

l=0

where t0 = 0 and t|B|+|Q|+1 = T . Note that the above product includes
the probability of not transiting from the last jump time to T .
• Probability of all the events taking place during the interval [0, T ]:
The probability density that the chain jumps at time tl to a new state
is qz(tl −)z(tl ) , where tl − denotes an infinitesimal time prior to tl . The
probability density of all the events taking place is:
|B|+|Q|

Y

qz(tl −)z(tl )

(3)

l=1

To compute the probability of the path we take the product of Equations (1),(2)
and (3).
Proof of corollary (1) in the main paper:
The IBP is defined as the limit of the corresponding distribution over matrices
with M columns as M → ∞. The finite model, which gives the IBP in the limit
as M → ∞ (Griffiths & Ghahramani, 2011) is
α
ωk |α ∼ Beta
,
M
Znk |ωk ∼ Bernoulli(ωk )
(4)
for k = 1, . . . , M and n = 1, . . . , N .
The expected number of features at any time t ∈ T is
Z
Z
Z
µ0 (dx)
K
E[Nf ] =
νt (dωdx) =
ρ(dω)
= , K→∞
D
D
[0,1]⊗X
[0,1]
X
We introduced K → ∞ because

R

(5)

ρ(dω) = ∞. Since the expected number

[0,1]
by K
D,

of features at any t ∈ T is given
the substitution of M = K
D for K → ∞
R
and D = α , results in the generative model in Equation (12) in the main paper.
3

Proof of proposition (4) in the main paper:
For what follows, we use NB (δ) to denote the number of birth events (actual
appearance of a feature with at least one member in it) that is NB (δ) = |{B =
PN
{fk } : tkb ∈ δ, n=1 znk (t) ≥ 1}| and NF (δ) to denote the number of feature
events tb at time interval δ, that is NF (δ) = |{F = {fk } : tkb ∈ δ}|. Note here
the difference between a birth and a feature event. Not all feature events are
birth events.
• The number of features present in a time interval [t, t+δ] follows a Poisson
R
R
R t+δ
R∞
distribution with intensity ν(δ) = [0,1] ρ(dω) X µ0 (dx) t dtb 0 dtω =
Kδ for K → ∞, i.e. Nf (δ) ∼ Poisson(Kδ). Since K → ∞, use of Proposition (2) and Proposition (3) result in
Nf (δ) → Kδ, as K → ∞.

(6)

• The probability of activating a feature fk given that it is created in the
time interval δ is:
π=

P (NB (δ) = 1|NF (δ) = 1)
Z
P (NB (δ) = 1, ωk |NF (δ) = 1)p(ωk )dωk

=

(7)


R
, 1 . Morewhere, based on Equation (12) in the main paper, ωk ∼ Beta K
over, P (NB (δ) = 1, ωk |NF (δ) = 1) is the probability that at time interval
δ at least one object out of N grants membership of the feature given that
only one feature is created at the interval δ, i.e
P (NB (δ) = 1, ωk |NF (δ) = 1) = P (n ≥ 1) = 1 − P (n < 1) = 1 − (1 − ωk )N
where we used the fact that P (n ≤ m) is given by the Binomial cumulative
Pbmc 
distribution function form F(m; N, ωk ) = P (n ≤ m) = i=0 Ni ωki (1 −
ωk )N −i . Here bmc is the “floor” under m, i.e. the greatest integer less
than or equal to m. Finally, Equation (7) becomes


Z
R
π =
(1 − (1 − ωk )N )Beta ωk ; , 1 dωk
K
R
Z
−1
(1 − ωkN )N ωkK
dωk
= 1−
R
B( K
, 1)
=

1−

R
B( K
, N + 1)
R
B( K
, 1)

(8)

where B(·) is the Beta function.
In order to compute the birth rate at [t, t+δ] we need to compute the probability
of having one only birth event at the time interval δ, i.e.
Z
P (NB (δ) = 1) = P (NB (δ) = 1|NF (δ))P (NF (δ))dNF .
(9)
4

The number of actual births given the number of features present follows a
Binomial distribution, i.e. NB (δ)|NF (δ) ∼ Binomial(NB (δ); NF (δ), π). Since
NF (δ) → Kδ → ∞ and π → 0 as K → ∞, following Proposition (1), this
distribution can be approximated by
NB (δ)|NF (δ) ∼ Poisson(NB (δ); πKδ), K → ∞

(10)

Based on Equation (10), Equation (9) now becomes
Z
P (NB (δ) = 1) =
P (NB (δ) = 1|NF (δ))P (NF (δ))dNF
=

Poisson(1; πKδ)

=

πKδe−πKδ

(11)

We need to consider the case when K → ∞ and we will focus on the product
term πKδ.
lim πKδ

K→∞

=
1
β= K

=

L’Hopital

=

β→0

=

=

R

, N + 1) 
B( K
lim δK 1 −
R
K→∞
B( K
, 1)
B(βR, 1) − B(βR, N + 1)
)
lim δ(
β→0
βB(βR, 1)
RB(βR, 1)(ψ(βR) − ψ(βR + 1)) − RB(βR, N + 1)(ψ(βR) − ψ(βR + N + 1))
lim
β→0
βRB(βR, 1)(ψ(βR) − ψ(βR + 1)) + B(βR, 1)
R(ψ(0) − ψ(1)) − R(ψ(0) − ψ(N + 1))
δ
1
N
X
1
δR(ψ(N + 1) − ψ(1)) = δR
= δRHN
(12)
n
n=1

where we used the definition of the derivative of the Beta function dB(x,y)
=
dx
B(x, y)(ψ(x) − ψ(x + y)). Considering again the probability in Equation (11)
and taking its limit as K → ∞ and δ → 0 we have
δ→0

P (NB (δ) = 1) = δRHN e−δRHN = δRHN

(13)

If we divide the result by δ, then the resulting rate is equal to the overall birth
rate RHN in BDFP. The probability of only one feature out of the NF dying in
the time interval δ is trivial NF (δ) R
α equal to the death rate in BDF process.
We assumed that the probability of observing more than one events in δ → 0 is
negligible.
Proof of proposition (5) in the main paper:
In the finite model, at any t ∈ T, the constrained projection is computed as an
integral over the space defined by the updated set of constraints t − tω < tb < t,
0 < tω < ∞ and 0 < tb < T which can be summarised as t − tω < tb < t and

5

0 < tω < t. As such,
Z tZ

t

g(dtb )β(dtω )

νt (dωdx) = ρ(dω)µ0 (dx)
0

t−tω


1 − e−Dt 
= ρ(dω)µ0 (dx) − te−Dt +
(14)
D
The exponential terms add a dependency on the index t ∈ T. This is a result
of constraining tb ∈ [0, T ]; the number of features present near the origin t = 0
is smaller than the number of features present later in time, since no features
1
are allowed to be born in tb < 0. This effect diminishes as t  D
since then
e−Dt → 0 and νt (dωdx) coincides with the restricted projection in the infinite
(dx)
.
case, i.e. νt (dωdx) = ρ(dω) µ0 D
In the same fashion, in the finite model, the expected number of features
R1R
−Dt
present at any t ∈ T is 0 X νt (dωdx) = K(−te−Dt + 1−eD ). Again, for
1
, the expected number of features at any t approaches K
t D
D and K → ∞ in
the infinite case. This is confirmed in Figure 1(a). It takes some time until the
empirical mean number of features converges to the true expected value under
the infinite model. In order to make sure that at any random time point in the
range considered the process has IBP marginals, we have to make sure that the
expected number of features at any time is equal to the true expected value,
Kα
that is K
D = R . The discussion above ensures this considering the process at
1
t  D , where the D is the death rate. In this case, we only allow data to live
at the time range where IBP marginals are satisfied.

3

Posterior Simulation

The BDF process is a continuous-time Markov process and use of the forwardbackward algorithm would facilitate inference. However, the exponentially large
space of feature allocations makes inference hard. However, the equivalent BEP
simplifies inference by allowing the application of a simple, efficient Gibbs sampling approach. For what follows, we present the posterior distributions for the
linear-Gaussian likelihood model in Equation 15 and presented in Figure 3.
Likelihood term p(Y|S, tb , tw , A, σ ) Given all the unknown parameters,
the computation of the likelihood is straightforward. More precisely, given the
values of S, tb and tw the computation of the feature allocation matrices Zt for
t = 1, . . . , L is deterministic, as presented earlier, and as such, observing the
data matrix
PKT Yt is equal in distribution to observing matrix Yt − Zt A for which
ytnd − k=1 ztnk Akd = tnd ∼ N (0, σ ) holds. Consequently,
p(Y|S, tb , tw , A, σ )

=

L
Y

p(Yt |S, tb , tw , A, σ )

t=1

=

L Y
N Y
D
Y
t=1 n d=1

6

N ytnd −

KT
X
k=1

!
ztnk Akd ; 0, σ

(15)

140

120

number of features present

100

80

60

40

20

0
0

10

20

30

40

50

60

70

80

90

100

time
Figure 1: Traceplot and running mean plot of number of features present as
a function of time for a sample from the finite BEP model. The plots were
produced using a dataset of N = 3 and 5000 feature allocation matrices were
drawn from the process over a period of [0, 100]. The number of features present
is plotted over time. The values of hyperparameters chosen is α = 2, R = 20
and K = 1000. Note the convergence to the mean µ = Kα
R = 100 as indicated
by the red traceplot; it takes some time until the number of features present
converge to the expected mean.

7

Sample the parameters α, R, tb and tw . Due to lack of conjugacy, the
posterior of these parameters has no closed form and exact computation is not
feasible. For that reason, we used slice sampling (Neal, 2003). For completeness,
we provide the conditional posteriors.
p(α|tkw , R)

∝ p(tw |α, R)p(α)

=

Gamma(α; κα , θα )

KT
Y
k=1

p(R|ω, tw , α) ∝

p(ω|R, α)p(tw |R, α)p(R)
= Gamma(R; κR , θR )

p(tkb |Y, S, t¬k
b , tw , A, σ )

∝

p(tkw |Y, R, S, t¬k
w , tb , A, α, σ ) ∝



k R
Exponential tw ;
α

KT
Y





R
R
Beta ωk ; , 1 Exponential tkw ;
K
α

k
k
p(Y|S, tb , tw , A, σ )p(tb )
!
D
N Y
L Y
X
Y
N ytnd −
ztnk Akd ; 0, σ
= U(tkb ; 0, T )
t=1 n d=1
k
p(Y|S, tw , tb , A, σ )p(tkw |R, α)
!

Y
L Y
N Y
D
X
k R
= Exponential tw ;
N ytnd −
ztnk Akd ; 0, σ
α t=1 n=1
d=1
k

Sample the noise standard deviation σ We put a Gamma prior over
the precision, that is τ ∼ Gamma(a , β ), where a , β are the shape and rate
hyperparameters. The posterior over the precision is then,
p(τ |Y, S, tb , tw , A) ∝ p(Y|S, tb , tw , A, τ )p(τ |a , β )
The Gamma prior is conjugate to the Gaussian likelihood and thus, the conditional posterior is a Gamma distribution too. More precisely, the posterior has
the form
!
D
N X
L X
X
2tnd
LN D
p(τ |Y, S, tb , tw , A) = Gamma a +
, β +
2
2
t=1 n=1
d=1

where tnd = ytnd −
as σ = √1τ .

PKT

k=1 ztnk Akd .

Finally, we compute the standard deviation

Sample the
Beta prior wk ∼
 weights ω Gibbs sampling is simple since the
QKT
R
, 1 is conjugate to the Binomial likelihood p(S|ω) = k=1 Binomial(mk ; N, ωk )
Beta K

R
resulting in the conditional posterior distribution wk ∼ Beta |mk | + K
, N − |mk | + 1 , i =
1, . . . , KT where |mk | is the number of objects that have feature fk in their potential, i.e. Snk = 1.
p(ωk |S) ∝ p(S(:, k)|ωk )p(ωk )

R
∝ Binomial(mk ; N, ωk )Beta
,1
K


8


∝ Beta


R
+ mk , 1 + N − mk ,
K

for k = 1, . . . , KT .
Sample the feature potential matrix S The prior over the feature potential matrix S is a product of Bernoulli distributed parameters. More precisely,
p(S|ω) =

N KT
Y
Y

p(Snk |ω) =

n=1 k=1

N KT
Y
Y

Bernoulli(Snk ; ωk )

n=1 k=1

The posterior over each matrix element Snk is given by
p(Snk |Y, ω, S¬nk , tb , tw , A, σ ) ∝ p(Y|S, ω, tb , tw , A, σ )p(Snk |ω)
We only need to consider Snk ∈ {0, 1}, so we evaluate the right hand side for
Snk = 0 and Snk = 1, normalize, and sample Snk from the resulting Bernoulli
posterior.

3.1

Getting it right

To validate our sampling alorithm for BEP we follow the joint distribution
testing methodology of Geweke (2004). There are two ways to sample from
the joint distribution, P (Y, θ) over parameters θ = {R, ω, S, tb , tw , A, α, σ },
and data, Y defined by a probabilistic model such as BEP. The first we will
refer to as “marginal-conditional” sampling, shown in Algorithm 1. Both steps
here are straightforward: sampling from the prior followed by sampling from
the likelihood model. The second way, referred to as “successive-conditional”
sampling is shown in Algorithm 2, where Q represents a single (or multiple)
iteration(s) of our MCMC sampler. To validate our sampler we can then check,
either informally or using hypothesis tests, whether the samples drawn from the
joint P (Y, θ) in these two different ways appear to have come from the same
distribution.
We apply this method to our sampler with just N = 10, D = 2 and |F| = 5,
two time points and all hyperparameters fixed as follows: For the shape and scale
of α we set κα = 4, θα = 1, for the shape and rate of  we set α = 2, β = 2,
for the shape and scale of R we chose κR = 4, θR = 1 and finally for the mean
and standard deviation of A we set µA = 0, σA = 1.
We draw 80K samples using both the marginal-conditional and successiveconditional procedures. We look at various characteristics of the samples, including the number of features at every time point, the mean of the factor
loading matrix A.
The distribution of the number of features under the successive-conditional
sampler matches that under the marginal-conditional sampler almost perfectly
as shown in Figure 2. Both the histogram and the quantile-quantile plot show
the similarity of the two distributions, with the straight line in the later indicating an almost perfect match. The deviation from a straight line in the upper
9

corner of the qq-plot is a result of there being fewer samples available to estimate these quantiles accurately. Under the successive-conditional sampler the
average number of features is 0.85, 0.95 for the two locations while under the
marginal-conditional is 0.83, 0.96 respectively with standard deviations 0.91,
1.01 and 0.92, 1.02 respectively: a hypothesis test did not reject the null hypothesis that the means of the two distributions are equal. While this cannot
completely guarantee correctness of the algorithm and code, 80K samples is a
large number for such a small model and thus provides strong evidence that our
algorithm is correct. Figure 3 provides the same evidence.
4

4

4

x 10

4

3

3

2

2

1

1

0

0

1

2

3

4

0

5

4

4

0

1

2

3

4

5

0

1

2

3

4

5

4

x 10

4

3

3

2

2

1

1

0

0

1

2

3

4

6

4

4

2

2

1

2

3

4

5

x 10

0

5

6

0
0

x 10

0
0

6

(a) First time location

1

2

3

4

5

6

(b) Second time location

Figure 2: Comparing the distribution of the number of features under the
marginal-conditional and successive conditional samplers. Figures in column (a)
show the empirical distribution over the number of features under the marginalconditional (first row) and successive conditional (second row) for the first time
location. Respectively for the second time location in column (b). Figure in the
last row show the qq-plots of the two empirical distributions for the two time
locations. The agreement of the two distributions is evidence for the correctness
of our MCMC sampler for the finite model.

4

Experiments

In the main paper, we presented results on real world dataset. To complete the
analysis, we provide here further experiments on synthetic dataset.

10

25

25

20

20

35

30

25

15

15

10

10

20

15

10
5

5
5

0
0

20

40

0
0

20

(a) α

40

(b) R

0
0

20

40

(c) σ

Figure 3: Comparing the distribution α, R and σ under the marginalconditional and successive conditional samplers. Figure shows the qq-plots of
the two empirical distributions. The agreement of the two distributions is evidence for the correctness of our MCMC sampler for the finite model.

4.1

Synthetic dataset

We first explored the ability of our model to recover underlying structure using synthetic data. We generated observations Yt from the BEP at 7 distinct time points, t = 1, 2, . . . , 7. More specifically, we assumed N = 20 datapoints and 4 features with birth times, tb = [0.1, 0.4, 0.8, 1.2] and life spans
tw = [0.8, 0.8, 0.8, 0.8]. We also assumed a potential matrix S and derived the
feature allocation matrices Zt as determined by the BEP and shown in Figure 4.
We used the four 6 × 6 images shown in Figure 4 (middle row, left) as features
and collected them to construct the feature loading matrix A of size 4 × 36.
Each row of this matrix corresponds to one of the 4 features. The synthetic
data at each time point is then generated by superimposing the images using
the linear Gaussian likelihood, i.e. Yt = Zt A + , where  is the noise term
which we take as Gaussian with standard deviation 0.5.
We ran both models, that is the BEP and a set of 7 independent IBP models
(one at each time location). To evaluate predictive performance, we held out
10% of the data (elements in each Yt ). For inference, we ran the BEP sampler
derived in Section 3 for 1000 MCMC iterations, which appeared sufficient for
burnin. The total number of features is KT = 12, where we took K = 6 and T =
2. For the independent IBP model, we considered the same Gaussian likelihood

11

Algorithm 1: Marginal conditional
1: for m = 1 to M do
2:
θ(m) ∼ P (θ)
3:
Y (m) ∼ P (Y |θ(m) )
4: end for

Train error
Test error
Train log likelihood
Test log likelihood

Algorithm 2: Successive conditional
1: θ (1) ∼ P (θ)
2: Y (1) ∼ P (Y |θ (1) )
3: for m = 2 to M do
4:
θ(m) ∼ Q(θ|θ(m−1) , Y (m−1) )
5:
Y (m) ∼ P (Y |θ(m) )
6: end for

BEP
3.3934 ± 0.0714
3.4229 ± 0.1771
−3, 348 ± 9.7123
−381.7972 ± 4.4620

independent IBP
3.3428 ± 0.0738
4.7367 ± 0.4283
−3, 311 ± 33.8831
−605.6935 ± 68.9909

Table 1: Results for synthetic Gaussian superimposition data

as for the BEP but with independent A and noise variance at each location. The
quantitative results are presented in Table 4.1 where the likelihood and the error
are averaged over the last 200 MCMC samples. Figure 4 shows the solutions
found by the two models. The MCMC sample with the highest log probability
under the posterior was used. The BEP successfully finds the true features and
Z matrix, whereas the independent IBP model finds a solution where additional
features are used in all of the 7 locations (see second and third row of Figure
4). The clean solution provided by BEP shows that leveraging the dependence
among consecutive feature allocations greatly improves performance. Table 4.1
confirms this quantitatively: the BEP model performs considerably better both
in terms of test error and likelihood. While the independent IBP looks good
in terms of training error/likelihood, the big difference in the performance in
terms of the test likelihood suggests this is overfitting.
Next, we explore the ability of BEP to recover latent features in a synthetic
time series network data. We hand-constructed a set of six square binary matrices which encode the friendship links among N = 20 people evolving through
time, as shown in Figure 5. People form groups which determine the links and
non-links between them. As time passes, the partitioning of people changes:
new friendship links are created while others break. The closer in time two
snapshots are, the more similar we expect the related partitions to be. We ran
BEP using the network likelihood model in Equation (16) for 2000 MCMC iterations keeping the last 400 samples for estimation and holding 10% of the data
out for prediction. For comparison, we used independent LFRM models at each
timepoint. The BEP model outperforms the independent LFRM in terms of
both test error and likelihood (Table 4.1) while, analogously to the linear Gaussian setting, the independent LFRM seems to overfit yielding “better” values in

12

True Y

inferred Y for BEP

True datapoint

Inferred Y for IBP

Inferred datapoint for BEP

Inferred datapoint for static IBP

1

1

1

2

2

2

3

3

3

4

4

4

5

5

5

6

6

6

True Features

Inferred Features for BEP

3

3

True Z

3

2

0

0

0

0

0

0

0

Inferred Features for IBP

0

inferred Z for BEP

Inferred Z for IBP

Inferred Z for BEP

123456789
10
11
12
t1

123456789
10
11
12

123456789
10
11
12

t2

t3

123456789
10
11
12
t4

Inferred Z over covariate for static IBP

123456789
10
11
12
t5

123456789
10
11
12
t6

123456789
10
11
12
t7

1

2

3

4

1
t1

2

3

4

123456789
10
11
12
t2

t3

123456789
10
11
12
t4

123456789

1 2 3 4 5

t5

Figure 4: Linear Gaussian synthetic data experiment. Top row: the true
and reconstructed observations for one datapoint (n = 16th) at the 7 different
time locations. Middle row: the true and inferred features. The inferred
features for BEP have been plotted along with a number on top indicating how
often they are used, that is the total number of time locations at which they
are active. Note that for the independent IBP model there are different factor
loading matrices for each time location. Bottom row: true and inferred feature
allocation matrices for the different time locations.

13

1
t6

2

3

4
t7

0

10

20

30

40

50

Figure 5: Synthetic network data.

Train error
Test error
Train log likelihood
Test log likelihood

BEP
0.0688 ± 0.0148
0.0745 ± 0.0132
−209.2373 ± 5.8391
−9.2547 ± 1.7358

independent LFRM
0.0968 ± 0.0286
0.1211 ± 0.0194
−203.8954 ± 13.7807
−19.4331 ± 5.3122

Table 2: Results for synthetic link data
the train likelihood. Figures 6 and 7 offer a qualitative overview of the solution.
Figure 6 shows the features found in the sample with the highest log probability
under the posterior. Both show figures the slow evolution of the feature allocation dictated by the BEP; once objects are allocated to a feature, the feature
membership remains the same until the feature dies. For instance, moving from
time location 1 to 2 explains the data by keeping feature 17 alive (with the same
10 members) and introducing features 12 and 15. The LFRM is overly flexible
since it assigns objects to features at each location independently. As such, it
explains the observations at the second time location using two features created
independently from the ones in the previous location.

4.2

ChIP-seq dataset

shows genomic annotations, from ChromHMM (Ernst et al., 2011) for the region
we model in the main paper. The inferred solution for both the BEP and the
independent IBP’s are shown in Figure 9 and Figure 10 respectively. The BEP
model allows for a smooth evolution of the latent feature allocation inferred as
opposed to the independe IBP’s where the latent structure is explained with
rapid changing allocations and with considerable differences in the number of

14

30

25

20

15

10

5

0

0

1

2

3

4

5

6

7

8

Figure 6: Inferred features using the BEP in he synthetic network data. Time
is denoted on the x-axis along with the 6 time locations (dasehd red line), i.e
[3 4 4.3 6 6.3 7]. The total number of features is |F| = 30. Red colour is used
to indicated the features that are alive for more than one location. The features
that cross the vertical grey line at each time location are the ones present at
that time.

15

2

2

2

4

4

4

6

6

6

8

8

8

10

10

10

12

12

12

14

14

14

16

16

16

18

18

20

20
5

10

15

20

25

30

18

20
5

10

15

20

25

30

2

2

2

4

4

4

6

6

6

8

8

8

10

10

10

12

12

12

14

14

14

16

16

16

18

18

20
10

15

20

25

30

10

15

20

25

30

2

2

4

4

4

6

6

6

8

8

8

10

10

10

12

12

12

14

14

14

16

16

16

18

18

0.5

1.5

2

2.5

3

3.5

0.5

1.5

2

2.5

3

3.5

0.5

2

2

4

4

4

6

6

6

8

8

8

10

10

10

12

12

12

14

14

14

16

16

16

18

18

20

20
1

1.5

2

2.5

0.5

25

30

5

10

15

20

25

30

20
1

2

0.5

20

18

20
1

15

20
5

2

20

10

18

20
5

5

1

1.5

2

2.5

3

3.5

4

4.5

18

20
1

1.5

2

2.5

0.5

0.6

0.7

0.8

0.9

1

1.1

1.2

1.3

1.4

1.5

Figure 7: Inferred feature allocation matrices for the six locations (from left
to right) in the synthetic link dataset. First two rows: Feature allocation
matrices inferred by BEP. Last two rows: Feature allocation matrices inferred
by independent LFRM.

16

features found at each location. As such, covariate dependence over the allocation is a better modelling approach.
11000

14000

17000

20000

23000

26000

29000

32000

35000

38000

41000

44000

47000

50000

53000

56000

59000

HMEC chromatin state
HUVEC chromatin state
K562 chromatin state
HEPG2 chromatin state
H1ES chromatin state
GM12878 chromatin state
NHEK chromatin state
NHLF chromatin state
HSMM chromatin state

OR4F5

Assay

p36.33
chr1

FAM138A
WASH7P
FAM138F

Sample

RefSeq genes

Active promoter
Weak promoter
Inactive/poised promoter
Strong enhancer
Weak/poised enhancer
Insulator
Transcriptional transition
Weak transcribed
Polycomb repressed
Heterochrom; low signal
Repetitive/CNV

Figure 8: ChIP-seq data: Chromatin states for the genomic region we model.
From the BEP reconstruction in Figure 5(b) we see the promoter region around
18kb-19kb with high H3K27ac, the transcribed region of the WASH7P gene from
8kb-18kb, and the repressive H3K29me3 and H3K9me3 marks further downstream, corresponding to polycomb repression and heterochromatin.

4.3

van de Bunt’s dataset

In van de Bunt et al. (1999), 32 university freshman students in a given discipline
at a Dutch university were surveyed at seven time points about who in their class
they considered as friends. Initially, i.e. t1 , most of the students were unknown
to each other. The first four time points are three weeks apart, whereas the last
three time points are six weeks apart as showin in Figure 11.

References
Ernst, Jason, Kheradpour, Pouya, Mikkelsen, Tarjei S, Shoresh, Noam, Ward,
Lucas D, Epstein, Charles B, Zhang, Xiaolan, Wang, Li, Issner, Robbyn,
Coyne, Michael, et al. Mapping and analysis of chromatin state dynamics in
nine human cell types. Nature, 473(7345):43–49, 2011. 14
Geweke, John. Getting it right: Joint distribution tests of posterior simulators.
Journal of the American Statistical Association, 99:799–804, 2004. 9
Griffiths, Thomas L. and Ghahramani, Zoubin. The indian buffet process: An
introduction and review. Journal of Machine Learning Research, 12:1185–
1224, July 2011. 3
Neal, R M. Slice sampling. The Annals of Statistics, 31(3):705–741, 2003. 8
van de Bunt, Gerhard G, Van Duijn, Marijtje AJ, and Snijders, Tom AB. Friendship networks through time: An actor-oriented dynamic statistical network
model. Computational & Mathematical Organization Theory, 5:167–192, 1999.
17

17

2

2

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

14

12

14
2

4

6

8

10

12

14

16

18

20

12

14
2

4

6

8

10

12

14

16

18

20

12

14
2

4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

2

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

14

12

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

14

12

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

10

10

10

10

12

12

12

12

12

14
4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

10

10

10

10

12

12

12

12

12

14
4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

12

12

12

14
2

4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

12

12

12

14
2

4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

14

12

14
2

4

6

8

10

12

14

16

18

20

12

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

14

12

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

14

12

14
2

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

16

18

20

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

2

4

6

8

10

12

14

16

18

20

12

14
2

18

6

12

14
2

16

14
2

2

12

14

12

14
2

12

6

12

14
2

10

14
2

2

12

8

12

14
2

6

14
2

2

12

4

14
2

2

14

2

14
2

2

14

20

6

8

10

2

18

14
2

2

14

16

6

8

10

2

14

14
2

2

14

12

12

14
2

10

6

12

14
2

8

14
2

2

12

6

12

14
2

4

6

12

14
2

2

14
2

4

6

8

10

12

14

16

18

20

Figure 9: ChIP-seq dataset: Inferred feature allocation matrices for the BEP
model. The allocation matrices for 50 locations (out of the 500) from left to
right are shown. Each pair of adjacent matrices correspond to locations with 10
18
bins distance, i.e. 103 base pairs.

2

2

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

14

12

14
2

4

6

8

10

12

14

12

14
1

2

3

4

5

6

7

8

9

10

12

14

11

2

4

6

8

10

12

14

14

16

2

4

6

8

10

12

14

16

18

5

2

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

14

12

14
2

4

6

8

10

12

14

16

18

20

5

10

15

20

2

4

6

8

10

12

14

5

10

15

20

25

5

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

14

12

14
5

10

15

20

25

4

6

8

10

12

14

16

18

20

5

10

15

20

4

6

8

10

12

14

16

18

20

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

10

10

10

10

12

12

12

12

12

14
10

15

20

25

14

30

10

20

30

40

50

60

14

70

10

20

30

40

50

60

70

80

10

20

30

40

50

60

70

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

10

10

10

10

12

12

12

12

12

14
10

15

20

25

14
2

4

6

8

10

12

14

16

18

20

14

22

5

10

15

5

10

15

20

25

2

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

12

12

12

14
5

10

15

20

14

25

5

10

15

14

20

5

10

15

20

5

10

15

20

25

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

12

12

12

14
4

6

8

10

12

14

16

14
2

4

6

8

10

12

14

16

18

14

20

2

4

6

8

10

12

14

5

10

15

20

2

2

2

4

4

4

4

4

6

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

14
5

10

15

20

12

14

25

5

10

15

20

2

4

6

8

10

12

14

2

4

6

8

10

12

14

16

18

2

2

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

14

12

14
2

4

6

8

10

12

14

16

18

4

6

8

10

12

14

16

18

2

4

6

8

10

12

4

6

8

10

12

14

16

18

20

22

2

2

2

4

4

4

4

4

6

6

6

6

8

8

8

8

8

10

10

10

10

10

12

12

14
2

4

6

8

10

12

14

16

18

20

22

4

6

8

10

12

14

16

2

3

4

5

6

7

8

9

10

11

60

20

25

30

4

6

8

10

12

14

16

18

20

4

6

8

10

12

14

4

6

8

10

12

14

16

18

20

22

10

15

20

25

30

12

14
1

15

50

6

12

14
2

10

5

2

14

5

40

14
2

2

12

30

30

12

14

20

20

25

6

12

14
2

10

20

14

16

2

12

15

12

14

25

10

2

2

14

50

14

16

2

12

5

2

2

2

45

14

25

2

14

40

14

20

2

14

35

6

8

10

5

30

14

90

2

14

25

6

8

10

5

20

14
2

2

14

15

12

14

22

10

6

12

14
2

25

14

16

2

12

20

12

14

25

15

6

12

14

22

10

14
2

4

6

8

10

12

14

2

4

6

8

10

12

14

16

18

20

Figure 10: ChIP-seq dataset: Inferred feature allocation matrices for the independent IBP model. The allocation matrices for 50 locations (out of the 500)
from left to right are shown. Each pair of adjacent matrices correspond to
3
locations with 10 bins distance, i.e. 1019
base pairs.

Figure 11: van de Bunt network data.

20

