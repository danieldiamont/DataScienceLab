ProtoNN: kNN for Resource-scarce Devices

8. Appendix
For the entirety of this section, assume W = I and Z = I2 .
8.1. Derivation of gradient of the loss with respect to prototypes
In this subsection, we will derive the gradient of the expected loss function with respect to B under the general mixture of
Gaussian model. That is, x âˆ¼ 12 N (Âµ+ , Î£+ ) + 21 N (Âµâˆ’ , Î£âˆ’ ).
The loss function with 2 prototypes and rbf-kernel (Î³ 2 = 21 ) is the following:
n

Remp =

	

	 

1 X
kyi âˆ’ e1 exp âˆ’ 12 kb+ âˆ’ xi k2 âˆ’ e2 exp âˆ’ 12 kbâˆ’ âˆ’ xi k2 k2
n i=1

In the infinite sample case (n â†’ âˆž), with points being draw from mixture of Gaussian, we have
	 


R = E[Remp ] =0.5[Exâˆ¼N (Âµ+ ,Î£+ ) (1 âˆ’ exp âˆ’ 21 kb+ âˆ’ xk2 )2


	 
+ 0.5Exâˆ¼N (Âµâˆ’ ,Î£âˆ’ ) (exp âˆ’ 12 kb+ âˆ’ xk2 )2 ]


	 
0.5[Exâˆ¼N (Âµâˆ’ ,Î£âˆ’ ) (1 âˆ’ exp âˆ’ 21 kbâˆ’ âˆ’ xk2 )2


	 
+ 0.5Exâˆ¼N (Âµ+ ,Î£+ ) (exp âˆ’ 21 kbâˆ’ âˆ’ xk2 )2 ]
Notice that the loss function decomposes into independent terms with respect to b+ and bâˆ’ . Given this observation, we
constrain our focus on the analysis with respect to only b+ . All theorems follow analogously for bâˆ’ .
We introduce some notation that we will use in the rest of the section.
â€¢ âˆ†+ := (b+ âˆ’ Âµ+ ), âˆ†âˆ’ := (b+ âˆ’ Âµâˆ’ ), ÂµÌ„ := (Âµ+ âˆ’ Âµâˆ’ ).
2

â€¢ For p.s.d. matrix M , vector v, kvkM = vT M v.
n
o
2
0
:= exp âˆ’ 21 kâˆ†+ kÎ£0 .
â€¢ Î£0+ := (I + Î£+ )âˆ’1 , g+
+
n
o
2
00
â€¢ Î£00+ := (I + 2Î£+ )âˆ’1 , g+
:= exp âˆ’kâˆ†+ kÎ£00 .
+
o
n
2
0
:= exp âˆ’kâˆ†âˆ’ kÎ£0 .
â€¢ Î£0âˆ’ := (I + 2Î£âˆ’ )âˆ’1 , gâˆ’
âˆ’
n
o
2
00
â€¢ Î£00âˆ’ := (I + 2Î£âˆ’ )âˆ’1 , gâˆ’
:= exp âˆ’kâˆ†âˆ’ kÎ£00 .
âˆ’

Theorem 3 (Gradient of the loss). In the infinite sample case:


0
00
00
âˆ‡b+ R = Î£0+ |Î£0+ |g+
âˆ’ Î£00+ |Î£00+ |g+
âˆ†+ âˆ’ Î£00âˆ’ |Î£00âˆ’ |gâˆ’
âˆ†âˆ’ .
Proof. The loss function decomposes as a sum over datapoints. Hence, using the fact that the expectation and gradient
operators both distribute over sums, we can write down the gradient as,


dR(x)
âˆ‡b+ R = Ex
=
db+
Z
n
o

	

	
T
=âˆš 1
(b+ âˆ’ x) exp âˆ’ 21 k(b+ âˆ’ x)k2 (1 âˆ’ exp âˆ’ 12 k(b+ âˆ’ x)k2 ) exp âˆ’ 21 (x âˆ’ Âµ+ ) (Î£+ )âˆ’1 (x âˆ’ Âµ+ ) dx
|2Ï€Î£+ |
Z
n
o

	

	
T
âˆ’âˆš 1
(b+ âˆ’ x) exp âˆ’ 12 k(b+ âˆ’ x)k2 (exp âˆ’ 12 k(b+ âˆ’ x)k2 ) exp âˆ’ 21 (x âˆ’ Âµâˆ’ ) (Î£âˆ’ )âˆ’1 (x âˆ’ Âµâˆ’ ) dx
|2Ï€Î£âˆ’ |
Z

	
2
T âˆ’1
=âˆš 1
(b+ âˆ’ x) exp âˆ’ 21 (xT (Î£+ Î£0+ )âˆ’1 x âˆ’ 2xT (b+ + Î£âˆ’1
+ Âµ+ ) + kb+ k + Âµ+ Î£+ Âµ+ ) dx
|2Ï€Î£+ |
Z

	
+âˆš 1
(b+ âˆ’ x) exp âˆ’ 12 (xT (Î£+ Î£00+ )âˆ’1 x âˆ’ 2xT (2b+ + Î£âˆ’1
Âµ+ ) + 2kb+ k2 + Âµ+ T Î£âˆ’1
Âµ+ ) dx
+
+
|2Ï€Î£+ |
Z

	
2
T âˆ’1
âˆ’âˆš 1
(b+ âˆ’ x) exp âˆ’ 12 (xT (Î£âˆ’ Î£00âˆ’ )âˆ’1 x âˆ’ 2xT (2b+ + Î£âˆ’1
âˆ’ Âµâˆ’ ) + 2kb+ k + Âµâˆ’ Î£âˆ’ Âµâˆ’ ) dx.
|2Ï€Î£âˆ’ |

ProtoNN: kNN for Resource-scarce Devices

The last equality follows by completing the square and separating out constants with respect to x. We thus have Gaussians
with the following means,
âˆ’1
âˆ’1
00
00
00
00
Âµ0+ := Î£+ Î£0+ (b+ + Î£âˆ’1
+ Âµ+ ), Âµ+ := Î£+ Î£+ (2b+ + Î£+ Âµ+ ), Âµâˆ’ := Î£âˆ’ Î£âˆ’ (2b+ + Î£âˆ’ Âµâˆ’ ),

The following constants come out as factors,
o
o
n
n
o
n
2
2
2
00
0
00
= exp âˆ’kâˆ†âˆ’ kÎ£00 .
g+
= exp âˆ’ 12 kâˆ†+ kÎ£0 , g+
= exp âˆ’kâˆ†+ kÎ£00 , gâˆ’
+

+

âˆ’

Then, the expression for the gradient can be re-written as:

Z
n
o 
T
0
0
âˆ’1
0
0
1
1
âˆ‡b+ R = âˆš
(b+ âˆ’ x) exp âˆ’ 2 (x âˆ’ Âµ+ ) (Î£+ Î£+ ) (x âˆ’ Âµ+ ) dx g+
|2Ï€Î£+ |

Z
n
o 
T
00
00
âˆ’1
00
00
1
1
(b+ âˆ’ x) exp âˆ’ 2 (x âˆ’ Âµ+ ) (Î£+ Î£+ ) (x âˆ’ Âµ+ ) dx g+
âˆ’ âˆš
|2Ï€Î£+ |

Z
n
o 
T
00
00
âˆ’1
00
00
1
1
(b+ âˆ’ x) exp âˆ’ 2 (x âˆ’ Âµâˆ’ ) (Î£âˆ’ Î£âˆ’ ) (x âˆ’ Âµâˆ’ ) dx gâˆ’
âˆ’ âˆš
.
|2Ï€Î£âˆ’ |

Using expectation of Gaussians, we get:

 0
 00
 00 
âˆ‡b+ R = (b+ âˆ’ Âµ0+ ) |Î£0+ | g+
âˆ’ (b+ âˆ’ Âµ00+ ) |Î£00+ | g+
âˆ’ (b+ âˆ’ Âµ00âˆ’ ) |Î£00âˆ’ | gâˆ’


0
00
00
= Î£0+ |Î£0+ |g+
âˆ’ Î£00+ |Î£00+ |g+
âˆ†+ âˆ’ Î£00âˆ’ |Î£00âˆ’ |gâˆ’
âˆ†âˆ’ .
To establish the last equality, we claim that (b+ âˆ’ Âµ0+ ) = Î£0+ âˆ†+ , (b+ âˆ’ Âµ00+ ) = Î£00+ âˆ†+ , and (b+ âˆ’ Âµ00âˆ’ ) = Î£00âˆ’ âˆ†âˆ’ as a
consequence of their definitions. We show the first of these three equalities and the other two proofs are similar. Note that
0 âˆ’1
Î£+ , Î£0+ , Î£âˆ’1
commute, when Î£+ is non-singular. Therefore
+ and (Î£+ )
(b+ âˆ’ Âµ0+ ) = Î£0+ (Î£0+ )âˆ’1 (b+ âˆ’ Î£+ Î£0+ (b+ + Î£âˆ’1
+ Âµ+ ))
= Î£0+ ((Î£0+ )âˆ’1 âˆ’ Î£+ )b+ âˆ’ Î£0+ (Î£0+ )âˆ’1 Î£+ Î£0+ Î£âˆ’1
+ Âµ+
= Î£0+ ((I + Î£+ ) âˆ’ Î£+ )b+ âˆ’ Î£0+ Âµ+
= Î£0+ (b+ âˆ’ Âµ+ )
= Î£0+ âˆ†+ .

Corollary 1. Let x be sampled from a mixture of 2 spherical Gaussians, i.e., Î£+ = Î£âˆ’ = I and x âˆ¼
1
2 N (Âµâˆ’ , I). Then, the gradient of the loss is given by,






1
1
1
0
00
00
âˆ‡b+ R =
g
âˆ’
g
âˆ’
gâˆ’
+
+
2d/2+1
3d/2+1
3d/2+1

1
2 N (Âµ+ , I)

+

where,
0
g+








1
1
1
2
00
2
00
2
= âˆ†+ exp âˆ’ kâˆ†+ k , g+ = exp âˆ’ kâˆ†+ k , gâˆ’ = exp âˆ’ kâˆ†âˆ’ k .
4
3
3

For the rest of this section, unless otherwise stated, assume Î£+ = Î£âˆ’ = I.
8.2. Lemmas
2
Lemma 1. Let assumptions of Theorem 1 hold. In particular, let âˆ†+ T ÂµÌ„ â‰¥ âˆ’ (1âˆ’Î´)
2 kÂµÌ„k , for some small constant Î´ > 0,
then:


Î´kÂµÌ„k2
0
00
,
gâˆ’
â‰¤ g+
exp âˆ’
4
n
o
n
o
2
2
00
0
where gâˆ’
:= exp âˆ’ kâˆ†3âˆ’ k , g+
:= exp âˆ’ kâˆ†4+ k .

ProtoNN: kNN for Resource-scarce Devices

Proof.
00
gâˆ’

(
)





kâˆ†âˆ’ k2
kâˆ†âˆ’ k2
kâˆ†+ + ÂµÌ„k2
kâˆ†+ k2 + kÂµÌ„k2 + 2âˆ†+ T ÂµÌ„
= exp âˆ’
â‰¤ exp âˆ’
= exp âˆ’
= exp âˆ’
3
4
4
4








2
2
2
2
2
Î¶1
kâˆ†+ k + kÂµÌ„k âˆ’ (1 âˆ’ Î´)kÂµÌ„k
kâˆ†+ k
Î´kÂµÌ„k
Î´kÂµÌ„k2
0
â‰¥ exp âˆ’
= exp âˆ’
exp âˆ’
= g+ exp âˆ’
.
4
4
4
4


2
Here, Î¶1 follows by replacing âˆ†+ T ÂµÌ„ with its lower bound âˆ’ (1âˆ’Î´)
2 kÂµÌ„k as specified in the assumption.
2
Lemma 2. Let assumptions
of Theorem 1 hold. In particular, if for some fixed Î´ â‰¥ 0, âˆ†+ T ÂµÌ„ â‰¥ âˆ’ (1âˆ’Î´)
2 kÂµÌ„k ,and
o
n
Î±kÂµÌ„k2
for some fixed Î± > 0. Also, let d â‰¥ 8(Î± âˆ’ Î´)kÂµÌ„k2 . Then we have:
kâˆ†+ k â‰¥ 8kÂµÌ„k exp âˆ’ 4




d
dR
Î±kÂµÌ„k2
0
> 0.1 Â· ( 21 ) 2 +1 g+
> 0,
kâˆ†+ kkâˆ†âˆ’ k exp âˆ’
db+
4
o
n
o
n
2
2
0
00
:= exp âˆ’ kâˆ†4+ k .
where where gâˆ’
:= exp âˆ’ kâˆ†3âˆ’ k , g+
âˆ†+ T E



Proof. Using âˆ†âˆ’ = âˆ†+ + ÂµÌ„ and triangle inequality, we have:
kâˆ†+ k
kâˆ†+ k
kâˆ†+ k
=
â‰¥
.
kâˆ†âˆ’ k
kâˆ†+ + ÂµÌ„k
kâˆ†+ k + kÂµÌ„k
n
o
2
The above quantity is monotonically increasing in âˆ†+ . Hence, for all kâˆ†+ k â‰¥ 8kÂµÌ„k exp âˆ’ Î±kÂµÌ„k
, we have:
4
n
o
Î±kÂµÌ„k2


8
exp
âˆ’
4
Î±kÂµÌ„k2
kâˆ†+ k
n
o
â‰¥
â‰¥
4
exp
âˆ’
.
2
kâˆ†âˆ’ k
4
8 exp âˆ’ Î±kÂµÌ„k
+
1
4
00
0
, we have
â‰¥ g+
Using Lemma 1 and the fact that g+







d
d
d
dR
0
00
00
âˆ†+ T E
= âˆ†+ T
( 12 ) 2 +1 g+
âˆ’ ( 31 ) 2 +1 g+
âˆ†+ âˆ’ ( 13 ) 2 +1 gâˆ’
âˆ†âˆ’
db+
 




d
d
d
Î´kÂµÌ„k2
0
00
0
âˆ’ âˆ†+ T âˆ†âˆ’ ( 13 ) 2 +1 g+
âˆ’ ( 13 ) 2 +1 g+
exp âˆ’
= kâˆ†+ k2 ( 21 ) 2 +1 g+
4


  


d
d
d
Î´kÂµÌ„k2
0
0
â‰¥ kâˆ†+ k2 ( 21 ) 2 +1 âˆ’ ( 13 ) 2 +1 g+
âˆ’ âˆ†+ T âˆ†âˆ’ ( 13 ) 2 +1 g+
exp âˆ’
4


  


d
d
d
Î´kÂµÌ„k2
0
0
â‰¥ kâˆ†+ k2 ( 12 ) 2 +1 âˆ’ ( 13 ) 2 +1 g+
âˆ’ kâˆ†+ kkâˆ†âˆ’ k( 13 ) 2 +1 g+
exp âˆ’
4





2
d
d
d
kâˆ†+ k
Î´kÂµÌ„k
0
= g+
kâˆ†+ kkâˆ†âˆ’ k
( 12 ) 2 +1 âˆ’ ( 31 ) 2 +1 âˆ’ ( 13 ) 2 +1 exp âˆ’
.
kâˆ†âˆ’ k
4







Î¶1
d
d
d
Î±kÂµÌ„k2
Î´kÂµÌ„k2
0
1 2 +1
1 2 +1
1 2 +1
â‰¥ g+ kâˆ†+ kkâˆ†âˆ’ k 4 exp âˆ’
(2)
âˆ’ (3)
âˆ’ (3)
exp âˆ’
,
4
4







d
d
d
Î±kÂµÌ„k2
(Î´ âˆ’ Î±)kÂµÌ„k2
0
1 2 +1
1 2 +1
1 2 +1
4 (2)
,
= g+ kâˆ†+ kkâˆ†âˆ’ k exp âˆ’
âˆ’ (3)
âˆ’ (3)
exp âˆ’
4
4





d
d
Î±kÂµÌ„k2
(Î´ âˆ’ Î±)kÂµÌ„k2
0
â‰¥ g+
kâˆ†+ kkâˆ†âˆ’ k exp âˆ’
( 12 ) 2 +1 âˆ’ ( 31 ) 2 +1 exp âˆ’
,
4
4

where Î¶1 follows from (3) and the last inequality follows by simple calculations.
Lemma now follows by using d â‰¥ 8(Î± âˆ’ Î´)kÂµÌ„k2 and d â‰¥ 1.

(3)

ProtoNN: kNN for Resource-scarce Devices

8.3. Proof of Theorem 1
Proof. Note that âˆ‡b+ R = c1 âˆ†+ âˆ’ c2 âˆ†âˆ’ , where c1 =
1
exp(âˆ’ 31 kâˆ†âˆ’ k2 ).
3d/2+1

1
2d/2+1

exp(âˆ’ 14 kâˆ†+ k2 ) âˆ’

1
3d/2+1

exp(âˆ’ 13 kâˆ†+ k2 ), c2 =

Let b+ 0 = b+ âˆ’ Î·âˆ‡b+ R. Then, b+ 0 âˆ’ Âµ+ = âˆ†+ âˆ’ Î·âˆ‡b+ R.
kb+ 0 âˆ’ Âµ+ k2 = kâˆ†+ k2 + Î· 2 kâˆ‡b+ Rk2 âˆ’ 2Î·âˆ†+ T âˆ‡b+ R.

(4)

Note that âˆ†+ T âˆ‡b+ R > 0. Hence, setting Î· appropriately, we get:
0

2

2

kb+ âˆ’ Âµ+ k â‰¤ kÂµÌ„k

(âˆ†+ T âˆ‡b+ R)2
1âˆ’
kâˆ†+ kkâˆ‡b+ Rk2

!
.

(5)

Using Lemma 2, we have:
oï£¶
n
2
0 2
0.01 Â· ( 12 )d+2 Â· (g+
) kâˆ†âˆ’ k2 Â· exp âˆ’ Î±kÂµÌ„k
2
ï£¸.
kb+ 0 âˆ’ Âµ+ k2 â‰¤ kâˆ†+ k2 ï£­1 âˆ’
2c21 kâˆ†+ k2 + 2c22 kâˆ†âˆ’ k2
ï£«

(6)

Using âˆ†âˆ’ = âˆ†+ + ÂµÌ„ and 2âˆ†+ T ÂµÌ„ â‰¥ âˆ’(1 âˆ’ Î´)kÂµÌ„k2 , we have: kâˆ†âˆ’ k2 = kâˆ†+ + ÂµÌ„k2 = kâˆ†+ k2 + kÂµÌ„k2 + 2âˆ†+ T ÂµÌ„ â‰¥
kâˆ†+ k2 + Î´kÂµÌ„k2 . Using monotonicity of the above function wrt kâˆ†âˆ’ k, and using kâˆ†âˆ’ k â‰¥ kdistpk, we have:
n
oï£¶
ï£«
2
0 2
) Â· ( 21 )d+2 exp âˆ’ Î±kÂµÌ„k
0.01 Â· (g+
4
ï£¸.
kb+ 0 âˆ’ Âµ+ k2 â‰¤ kâˆ†+ k2 ï£­1 âˆ’
(7)
2
2
2c1 + 2c2
0 2 1 d+2
) (2)
Using Lemma 1, we have: c22 â‰¤ c21 . Moreover, using (g+
â‰¥ 4c21 , we get:



Î±kÂµÌ„k2
kb+ 0 âˆ’ Âµ+ k2 â‰¤ kâˆ†+ k2 1 âˆ’ 0.01 exp âˆ’
.
4

(8)

o
n
2
.
That is, kb+ 0 âˆ’ Âµ+ k2 decreases geometrically until kb+ 0 k â‰¤ 8 exp âˆ’ Î±kÂµÌ„k
4
8.4. Proof of Theorem 2
Proof. We wish to analyze the Hessian âˆ‡2b+ R. The loss function decomposes as a sum over datapoints. Hence, using the
fact that the expectation and Hessian operators both distribute over sums, we can write down the Hessian as,

 2
d R(x)
2
0
00
00
âˆ‡b+ R = Ex
= c1 g+
(I âˆ’ 12 âˆ†+ âˆ†+ T ) âˆ’ c2 g+
(I âˆ’ 32 âˆ†+ âˆ†+ T ) âˆ’ c2 gâˆ’
(I âˆ’ 23 âˆ†âˆ’ âˆ†âˆ’ T ).
db+ 2
d

d

Here, c1 = ( 21 ) 2 +1 , c2 = ( 31 ) 2 +1 . Now,

 00

h
i
00 
+ gâˆ’
c2 g+
T
0
0
00
00
0
âˆ‡2b+ R < c1 g+
I âˆ’ c1 g+
âˆ†+ âˆ†+ T + c2 g+
I + c2 gâˆ’
I < c1 g+
1âˆ’
I
âˆ’
âˆ†
âˆ†
.
+ +
0
c1
g+
o
n
d/2+1
2
00
0
. Also, let c := cc21 = 32
. It can be seen that if d â‰¥ 1, c â‰¤ 0.6. Thus,
From lemma 1, gâˆ’
â‰¤ g+
exp âˆ’ Î´kÂµÌ„k
4
âˆ‡2b+ R




00
g+
Î´kÂµÌ„k2
T
<
1 âˆ’ 0.6 0 âˆ’ 0.6 exp âˆ’
I âˆ’ âˆ†+ âˆ†+
g+
4






kâˆ†+ k2
Î´kÂµÌ„k2
T
0
= c1 g+ 1 âˆ’ 0.6 exp âˆ’
âˆ’ 0.6 exp âˆ’
I âˆ’ âˆ†+ âˆ†+
12
4
0
c1 g+



ProtoNN: kNN for Resource-scarce Devices

Figure 5. Iteration vs. Test Accuracy plot on mnist binary dataset. The legend shows the variables that are optimized (e.g., Z and W
corresponds to case where we fix B and optimize over Z, W ).



n
o
4
2
Î´ ( (ln 0.1)Î´
)
From assumption, exp âˆ’ Î´kÂµÌ„k
â‰¤
exp
âˆ’
= 0.1. Thus,
4
4
âˆ‡2b+ R

<

0
c1 g+





kâˆ†+ k2
T
I âˆ’ âˆ†+ âˆ†+
0.9 âˆ’ 0.6 exp âˆ’
12

For kâˆ†+ k2 â‰¤ 0.5, the following facts can be seen by simple one-dimensional arguments:



1
kâˆ†+ k2
.
âˆ’ kâˆ†+ k2 â‰¥
0.9 âˆ’ 0.6 exp âˆ’
12
20



kâˆ†+ k2
0.9 âˆ’ 0.6 exp âˆ’
â‰¤1
12
2
value of âˆ†+ âˆ†+ T , and all eigen values of the scaled identity matrix are
oi
hkâˆ†+ k is the
n only 2eigen
+k
. Thus the ratio of the largest eigen value to the smallest eigen value of âˆ‡2b+ R is smaller
0.9 âˆ’ 0.6 exp âˆ’ kâˆ†12
than 20. Thus the condition number is bounded by 20, and the theorem follows.

9. Experiments
9.1. Joint training of Z, B, and W
A major reason ProtoNN achieves state-of-the-art performance is because of the joint optimization problem over Z, B, W
that ProtoNN solves. Instead of limiting ourselves to a projection matrix thatâ€™s fixed beforehand on the basis of some
unknown objective function (LMNN, SLEEC), we incorporate it into our objective and learn it along with the prototypes
and the label vectors. To show that this joint optimization in fact helps improve the accuracy of ProtoNN, we conducted
the following experiment, where we donâ€™t optimize one or more of Z, B, W in our algorithm and instead fix them to their
initial values. We use the following hyper parameters for ProtoNN: d = 10, sW = 0.1, sZ = sB = 1.0 and m = 20.
We initialize ProtoNN using LMNN. If W is not begin trained, then we sparsify it immediately at the beginning of the
experiment.
Figure 5 shows the results from this experiment on mnist binary dataset. The X-axis denotes iterations of alternating
minimization. One iteration denotes 20 epochs each over each of the e parameters W, B, Z. From the plots, we can see
that if W is fixed to its initial value, then the performance of ProtoNN drops significantly.
9.2. Datasets

ProtoNN: kNN for Resource-scarce Devices

Dataset
cifar
character recognition
eye
mnist
usps
ward
letter-26
mnist-10
usps-10
curet-61
aloi
mediamill
delicious
eurlex

n
50000
4397
456
60000
7291
4503
19500
4397
7291
4209
97200
30993
12920
15539

d
400
400
8192
784
256
1000
16
784
256
610
128
101
500
5000

L
2
2
2
2
2
2
26
10
10
61
1000
120
983
3993

Links
http://manikvarma.org/
https://www.kaggle.com/
https://rd.springer.com/chapter/10.1007/978-3-540-25976-3 23
http://manikvarma.org/
http://manikvarma.org/
https://www.kaggle.com/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
http://www.manikvarma.org/
https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/multiclass.html
http://manikvarma.org/downloads/XC/XMLRepository.html
http://manikvarma.org/downloads/XC/XMLRepository.html
http://manikvarma.org/downloads/XC/XMLRepository.html

Table 3. Dataset statistics and links

