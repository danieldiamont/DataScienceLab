Second-Order Kernel Online Convex Optimization with Adaptive Sketching

Daniele Calandriello 1 Alessandro Lazaric 1 Michal Valko 1

Abstract
Kernel online convex optimization (KOCO) is a
framework combining the expressiveness of nonparametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require
only O(t) time and space per iteration, and, when
the only information on the losses is their
âˆš convexity, achieve a minimax optimal O( T ) regret. Nonetheless, many common losses in kernel problems, such as squared loss, logistic loss,
and squared hinge loss posses stronger curvature
that can be exploited. In this case, second-order
KOCO methods achieve O(log(Det(K))) regret,
which we show scales as O(deff log T ), where
deff is the effective dimension of the
âˆš problem and
is usually much smaller than O( T ). The main
drawback of second-order methods is their much
higher O(t2 ) space and time complexity. In this
paper, we introduce kernel online Newton step
(KONS), a new second-order KOCO method that
also achieves O(deff log T ) regret. To address the
computational complexity of second-order methods, we introduce a new matrix sketching algorithm for the kernel matrix Kt , and show that for
a chosen parameter Î³ â‰¤ 1 our Sketched-KONS
reduces the space and time complexity by a factor of Î³ 2 to O(t2 Î³ 2 ) space and time per iteration,
while incurring only 1/Î³ times more regret.

1. Introduction
Online convex optimization (OCO) (Zinkevich, 2003) models the problem of convex optimization over Rd as a game
over t âˆˆ {1, . . . , T } time steps between an adversary and
the player. In its linear version, that we refer to as linearOCO (LOCO), the adversary chooses a sequence of arbitrary convex losses `t and points xt , and a player chooses
weights wt and predicts xTt wt . The goal of the player is to
1
SequeL team, INRIA Lille - Nord Europe. Correspondence
to: Daniele Calandriello <daniele.calandriello@inria.fr>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

minimize the regret, defined as the difference between the
losses of the predictions obtained using the weights played
by the player and the best fixed weight in hindsight given
all points and losses.
Gradient descent. For this setting, Zinkevich (2003)
showed that simple gradient descent (GD), combined with
a smart choiceâˆš
for the stepsize Î·t of the gradient updates,
achieves a O( dT ) regret with a O(d) space and time
cost per iteration. When the only assumption on the losses
is simple convexity, this upper bound matches the corresponding lower bound (Luo et al., 2016), thus making
first-order methods (e.g., GD) essentially unimprovable in
a minimax sense. Nonetheless, when the losses have additional curvature properties, Hazan et al. (2006) show that
online Newton step (ONS), an adaptive method that exploits second-order (second derivative) information on the
losses, can achieve a logarithmic regret O(d log T ). The
downside of this adaptive method is the larger O(d2 ) space
and per-step time complexity, since second-order updates
require to construct, store, and invert Ht , a preconditioner
matrix related to the Hessian of the losses used to correct
the first-order updates.
Kernel gradient descent. For linear models, such as the
ones considered in LOCO, a simple way to create more expressive models is to map them in some high-dimensional
space, the feature space, and then use the kernel trick
(SchoÌˆlkopf & Smola, 2001) to avoid explicitly computing their high-dimensional representation. Mapping to a
larger space allows the algorithm to better fit the losses
chosen by the adversary and reduce its cumulative loss.
As a drawback, the Kernel OCO (KOCO) problem1 is
fundamentally harder than LOCO, due to 1) the fact that
an infinite parametrization makes regret bounds scaling
with the dimension d meaningless and 2) the size of the
model, and therefore time and space complexities, scales
with t itself, making these methods even less performant
than LOCO algorithms. Kernel extensions of LOCO algorithms have been proposed for KOCO, such as functional
GD (e.g.,
âˆš NORMA, Kivinen et al., 2004) which achieves
a O( T ) regret with a O(t) space and time cost per iteration. For second-order methods, the Second-Order Per1
This setting is often referred to as online kernel learning or
kernel-based online learning in the literature.

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

ceptron (Cesa-Bianchi et al., 2005) or NAROW (Orabona
& Crammer, 2010) for generic curved losses and Recursive Kernel Least Squares (Zhdanov & Kalnishkan, 2010)
or Kernel AAR (Gammerman et al., 2004) for the specific
case of `2 losses provide bounds that scale with the logdeterminant of the kernel-matrix. As we show, this quantity is closely related to the effective dimension dTeff of the
of the points xt , and scales as O(dTeff log T ), playing a similar role as the O(d log T ) bound from LOCO.
Approximate GD. To trade off between computational
complexity smaller than O(d2 ) and improved regret
(close to O(d log T )), several methods try approximate
second-order updates, replacing Ht with an approxie t that can be efficiently stored and inverted. Adamate H
Grad (Duchi et al., 2011) and ADAM (Kingma & Ba, 2015)
reweight the gradient updates on a per-coordinate basis use t , but these methods ultimately only iming a diagonal H
âˆš
prove the regret dependency on d and leave the T component unchanged. Sketched-ONS, by Luo et al. (2016),
uses matrix sketching to approximate Ht with a r-rank
e t , that can be efficiently stored and updated in
sketch H
2
O(dr ) time and space, close to the O(d) complexity of diagonal approximations. More importantly, Sketched-ONS
achieves a much smaller regret compared to diagonal approximations: When the true Ht is of low-rank r, it recovers a O(r log T ) regret bound logarithmic in T . Unfortunately, due to the sketch approximation, a new term appears
in the bound that scales with the spectra of Ht , and in some
cases can grow much larger than O(log T ).
Approximate kernel GD. Existing approximate GD methods for KOCO focus only on first-order updates, trying to
reduce the O(t) per-step complexity. Budgeted methods,
such as Budgeted-GD (Wang et al., 2012) and budgeted
variants of the perceptron (Cavallanti et al., 2007; Dekel
et al., 2008; Orabona et al., 2008) explicitly limit the size
of the model, using some destructive budget maintenance
procedure (e.g., removal, projection) to constrain the natural model growth over time. Alternatively, functional approximation methods in the primal (Lu et al., 2016) or dual
(Le et al., 2016) use non-linear embedding techniques, such
as random feature expansion (Le et al., 2013), to reduce
the KOCO problem to a LOCO problem âˆš
and solve it efficiently. Unfortunately, to guarantee O( T ) regret using less than O(t) space and time per round w.h.p., all
of these methods require additional assumptions, such as
points xt coming from a distribution or strong convexity
on the losses. Moreover, as approximate first-order
methâˆš
ods, they can at most hope to match the O( T ) regret of
exact GD, and among second-order kernel methods, no approximation scheme has been proposed that can provably
maintain the same O(log T ) regret as exact GD. In addition, approximating Ht is harder for KOCO, since we cannot directly access the matrix representation of Ht in the

feature-space, making diagonal approximation impossible,
and low-rank sketching harder.
Contributions In this paper, we introduce Kernel-ONS, an
extension to KOCO of the ONS algorithm. As a secondorder method, KONS achieves a O(dteff log T ) regret on
a variety of curved losses, and runs in O(t2 ) time and
space. To alleviate the computational complexity, we propose S KETCHED -KONS, the first approximate secondorder KOCO methods, that approximates the kernel matrix
with a low-rank sketch. To compute this sketch we propose a new online kernel dictionary learning, kernel online
row sampling, based on ridge leverage scores. By adaptively increasing the size of its sketch, S KETCHED -KONS
provides a favorable regret-performance trade-off, where
for a given factor Î³ â‰¤ 1, we can increase the regret by
a linear 1/Î³ factor to O(dteff log(T )/Î³) while obtaining a
quadratic Î³ 2 improvement in runtime, thereby achieving
O(t2 Î³ 2 ) space and time cost per iteration.

2. Background
In this section, we introduce linear algebra and RKHS
notation, and formally state the OCO problem in an
RKHS (SchoÌˆlkopf & Smola, 2001).
Notation. We use upper-case bold letters A for matrices,
lower-case bold letters a for vectors, lower-case letters a
for scalars. We denote by [A]ij and [a]i the (i, j) element
of a matrix and i-th element of a vector respectively. We
denote by IT âˆˆ RT Ã—T , the identity matrix of dimension T
and by Diag(a) âˆˆ RT Ã—T , the diagonal matrix with the vector a âˆˆ RT on the diagonal. We use eT,i âˆˆ RT to denote
the indicator vector of dimension T for element i. When
the dimension of I and ei is clear from the context, we omit
the T . We also indicate with I the identity operator. We use
A  B to indicate that A âˆ’ B is a positive semi-definite
(PSD) matrix. With k Â· k we indicate the operator `2 -norm.
Finally, the set of integers between 1 and T is denoted by
[T ] := {1, . . . , T }.
Kernels. Given an arbitrary input space X and a positive
definite kernel function K : X Ã—X â†’ R, we indicate the reproducing kernel Hilbert space (RKHS) associated with K
as H. We choose to represent our Hilbert space H as a feature space where, given K, we can find an associated feature map Ï• : X â†’ H, such that K(x, x0 ) can be expressed
as an inner product K(x, x0 ) = hÏ•(x), Ï•(x0 )iH . With a
slight abuse of notation, we represent our feature space
as an high-dimensional vector space, or in other words
H âŠ† RD , where D is very large or potentially infinite.
With this notation, we can write the inner product simply as
K(x, x0 ) = Ï•(x)T Ï•(x0 ), and for any function fw âˆˆ H, we
can represent it as a (potentially infinite) set of weights w
such that fw (x) = Ï•(x)T w. Given points {xi }ti=1 , we

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

shorten Ï•(xi ) = Ï†i and define the feature matrix Î¦t =
[Ï†1 , . . . , Ï†t ] âˆˆ RDÃ—t . Finally, to denote the inner product
between two arbitrary subsets a and b of columns of Î¦T
we use Ka,b = Î¦Ta Î¦b . With this notation, we can write
the empirical kernel matrix as Kt = K[t],[t] = Î¦Tt Î¦t , the
vector with all the similarities between a new point and the
old ones as k[tâˆ’1],t = Î¦Ttâˆ’1 Ï†t , and the kernel evaluated at
a specific point as kt,t = Ï†Tt Ï†t . Throughout the rest of the
paper, we assume that K is normalized and Ï†Tt Ï†t = 1.
Kernelized online convex optimization. In the general
OCO framework with linear prediction, the optimization
process is a game where at each time step t âˆˆ [T ] the player
1 receives an input xt âˆˆ X from the adversary,
2 predicts ybt = fwt (xt ) = Ï•(xt )T wt = Ï†Tt wt ,
3 incurs loss `t (b
yt ), with `t a convex and differentiable
function chosen by the adversary,
yt ).
4 observes the derivative gÌ‡t = `0t (b
T

Since the player uses a linear combination Ï†t wt to compute ybt , having observed gÌ‡t , we can compute the gradient,

Algorithm 1 One-shot KONS
Input: Feasible parameter C, stepsizes Î·t , regulariz. Î±
1: Initialize w0 = 0, g0 = 0, b0 = 0, A0 = Î±I
2: for t = {1, . . . , T } do
3:
receive xt
4:
compute bs as in Lem.P2
tâˆ’1
5:
compute ut = Aâˆ’1
tâˆ’1 (
s=0 bs gs )
T
6:
compute y t = Ï•(xt ) ut
7:
predict ybt = Ï•(xt )T wt = y t âˆ’ h(y t )
8:
observe gt , update At = Atâˆ’1 + Î·t gt gtT
9: end for
Assumption 1. The loss function `t satisfies |`0t (y)| â‰¤ L
whenever y â‰¤ C.
Note that this is equivalent to assuming Lipschitzness of the
the loss w.r.t. y and it is weaker than assuming something
on the norm of the gradient kgt k, since kgt k = |gÌ‡t |kÏ†t k.
Assumption 2. There exists Ïƒt â‰¥ 0 such that for all
u, w âˆˆ S , lt (w) = `t (Ï†Tt w) is lower-bounded by
lt (w) â‰¥ lt (u) + âˆ‡lt (u)T (wâˆ’u) +

T

gt = âˆ‡`t (b
yt ) = gÌ‡t âˆ‡(Ï†t wtâˆ’1 ) = gÌ‡t Ï†t .
After t timesteps, we indicate with Dt = {xi }ti=1 , the
dataset containing the points observed so far. In the
rest of the paper we consider the problem of kernelized
OCO (KOCO) where H is arbitrary and potentially nonparametric. We refer to the special parametric case H =
Rd and Ï†t = xt as linear OCO (LOCO).
In OCO, the goal is to design an algorithm that returns a solution that performs almost as well as the best-in-class, thus
we must first define our comparison class. We define the
feasible set as St = {w : |Ï†Tt w| â‰¤ C} and S = âˆ©Tt=1 St .
This comparison class contains all functions fw whose output is contained (clipped) in the interval [âˆ’C, C] on all
points x1 , . . . , xT . Unlike the often used constraint on
kwkH (Hazan et al., 2006; Zhu & Xu, 2015), comparing
against clipped functions (Luo et al., 2016; Gammerman
et al., 2004; Zhdanov & Kalnishkan, 2010) has a clear interpretation even when passing from Rd to H. Moreover, S is
invariant to linear transformations of H and suitable for
practical problems where it is often easier to choose a reasonable interval for the predictions ybt rather than a bound
on the norm of a (possibly non-interpretable) parametrization w. We can now define the regret as
RT (w) =

XT
t=1

`t (Ï†Tt wt ) âˆ’ `t (Ï†Tt w)

and denote with RT = RT (wâˆ— ), the regret w.r.t. wâˆ— =
PT
arg minwâˆˆS t=1 `t (Ï†Tt w), i.e., the best fixed function
in S. We work with the following assumptions on the
losses.

Ïƒt
(âˆ‡lt (u)T (wâˆ’u))2 .
2

This condition is weaker than strong convexity and it is satisfied by all exp-concave losses (Hazan et al., 2006). For
example, the squared loss lt (w) = (yt âˆ’ xTt w)2 is not
strongly convex but satisfies Asm. 2 with Ïƒt = 1/(8C 2 )
when w âˆˆ S.

3. Kernelized Online Newton Step
The online Newton step algorithm, originally introduced
by Hazan et al. (2006), is a projected gradient descent that
uses the following update rules
ut = wtâˆ’1 âˆ’ Aâˆ’1
tâˆ’1 gtâˆ’1 ,
A

wt = Î Sttâˆ’1 (ut ),
A

where Î Sttâˆ’1 (ut ) = arg minwâˆˆSt kut âˆ’ wkAtâˆ’1 is an
oblique projection on a set St with matrix Atâˆ’1 . If St is
the set of vectors with bounded prediction in [âˆ’C, C] as
by Luo et al. (2016), then the projection reduces to
A

wt = Î Sttâˆ’1 (ut ) = ut âˆ’

h(Ï†Tt ut ) âˆ’1
Atâˆ’1 Ï†t ,
Ï†Tt Aâˆ’1
tâˆ’1 Ï†t

(1)

where h(z) = sign(z) max{|z| âˆ’ C, 0} computes how
much z is above or below the interval [âˆ’C, C]. When
At = I/Î·t , ONS is equivalent to vanilla projected
graâˆš
dient descent, which in LOCO achieves O( dT ) regret
(Zinkevich, 2003). In the sameP
setting, Hazan et al. (2006)
t
T
shows that choosing At =
s=1 Î·s gs gs + Î±I makes
ONS an efficient reformulation of follow the approximate

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

leader (FTAL). While traditional follow-the-leader
Ptâˆ’1 algorithms play the weight wt = arg minwâˆˆSt s=1 lt (w),
FTAL replaces the loss lt with a convex approximation using Asm. 2, and plays the minimizer of the surrogate function. As a result, under Asm. 1-2 and when Ïƒt â‰¥ Ïƒ > 0,
FTAL achieves a logarithmic O(d log T ) regret. FTALâ€™s
solution path can be computed in O(d2 ) time using ONS
updates, and further speedups were proposed by Luo et al.
(2016) using matrix sketching.
Unfortunately, in KOCO, vectors Ï†t and weights wt cannot be explicitly represented, and most of the quantities
used in vanilla ONS (Eq. 1) cannot be directly computed.
Instead, we derive a closed form alternative (Alg. 1) that
can be computed in practice. Using a rescaled variant
âˆš
âˆš
of our feature vectors Ï†t , Ï†t = gÌ‡t Î·t Ï†t = Î·t gt
T
and Î¦t = [Ï†1 , . . . , Ï†t ], we can rewrite At = Î¦t Î¦t +
T
Î±I and Î¦t Î¦t = Kt , where the empirical kernel matrix Kt is computed using the rescaled kernel K(xi , xj ) =
âˆš
âˆš
gÌ‡i Î·i gÌ‡j Î·j K(xi , xj ) instead of the original K, or equivâˆš
alently Kt = Dt Kt Dt with Dt = Diag({gÌ‡i Î·i }ti=1 ) the
rescaling diagonal matrix. We begin by noting that
!
h(Ï†Tt ut ) âˆ’1
T
T
ybt = Ï†t wt = Ï†t ut âˆ’ T âˆ’1
Atâˆ’1 Ï†t
Ï†t Atâˆ’1 Ï†t
= Ï†Tt ut âˆ’ h(Ï†Tt ut )

Ï†Tt Aâˆ’1
tâˆ’1 Ï†t
Ï†Tt Aâˆ’1
tâˆ’1 Ï†t

= y t âˆ’ h(y t ).

As a consequence, if we can find a way to compute y t , then
we can obtain ybt without explicitly computing wt . Before
that, we first derive a non-recursive formulation of ut .
Lemma 1. In Alg. 1 we introduce
âˆš
bi = [bt ]i = gÌ‡i Î·i

ybi âˆ’

h(y i )
T

Ï†i Aâˆ’1
iâˆ’1 Ï†i

!

Then, we can compute
yt =

1 T
Dtâˆ’1 (btâˆ’1 âˆ’ (Ktâˆ’1 + Î±I)âˆ’1 Ktâˆ’1 btâˆ’1 ).
k
Î± [tâˆ’1],t

Since Alg. 1 is equivalent to ONS (Eq. 1), existing regret
bounds for ONS directly applies to its kernelized version.
Proposition 1 (Luo et al., 2016). For any sequence of
losses `t satisfying Asm. 1-2, the regret RT of Alg. 1 is
bounded by RT â‰¤ Î±kwâˆ— k2 + RG + RD with
RG :=

T
X

gtT Aâˆ’1
t gt =

t=1

RD :=

T
X

T
X

T

T

Ï†t (Î¦t Î¦t + Î±I)âˆ’1 Ï†t /Î·t

t=1

(wt âˆ’ wâˆ— )T (At âˆ’ Atâˆ’1 âˆ’Ïƒt gt gtT )(wt âˆ’ wâˆ— )

t=1

=

T
X
(Î·t âˆ’ Ïƒt )gÌ‡t2 (Ï†Tt (wt âˆ’ wâˆ— ))2 .
t=1

In the d-dimensional
LOCO, choosing a decreasing stepp
d/(C 2 L2 t) allows ONS to achieve a
size Î·âˆš
=
t
O(CL dT ) regret for the cases where Ïƒt = 0. When
Ïƒt â‰¥ Ïƒ > 0 (e.g., when the functions are exp-concave) we
can set Î·t = Ïƒt and improve the regret to O(d log(T )). Unfortunately, these quantities hold little meaning
for KOCO
âˆš
with D-dimensional features, since a O( D) regret can be
very large or even infinite. On the other hand, we expect
the regret of KONS to depend on quantities that are more
strictly related to the kernel Kt and its complexity.
Definition 1. Given a kernel function K, a set of points
Dt = {xi }ti=1 and a parameter Î± > 0, we define the Î±ridge leverage scores (RLS) of point i as
Ï„t,i = eTt,i KTt (Kt +Î±I)â€“1 et,i = Ï†Ti (Î¦t Î¦Tt +Î±I)â€“1 Ï†i , (2)

1
âˆ’âˆš
Î·i

and compute ut as
ut = Aâˆ’1
tâˆ’1 Î¦tâˆ’1 btâˆ’1 .
Then, ut is equal to the same quantity in Eq. 1 and the sequence of predictions ybt is the same in both algorithms.
While the definition of bt and ut still requires performing operations in the (possibly infinitely dimensional) feature space, in the following we show that bt and the prediction y t can be conveniently computed using only inner
products.
Lemma 2. All the components bi = [bt ]i of the vector
introduced in Lem. 1 can be computed as


Î±h(y i )
1
âˆš
gÌ‡i Î·i ybi âˆ’
âˆ’
.
T
Î·i
ki,i âˆ’ k[iâˆ’1],i (Kiâˆ’1 + Î±I)âˆ’1 k[iâˆ’1],i

and the effective dimension of Dt as
dteff (Î±) =

t
X


Ï„t,i = Tr Kt (Kt + Î±It )âˆ’1 .

(3)

i=1

In general, leverage scores have been used to measure
the correlation between a point i w.r.t. the other t âˆ’ 1
points, and therefore how essential it is in characterizing the dataset (Alaoui & Mahoney, 2015). As an example, if Ï†i is completely orthogonal to the other points,
Ï„t,i = Ï†Ti (Ï†i Ï†Ti + Î±I)âˆ’1 Ï†i â‰¤ 1/(1 + Î±) and its RLS is
maximized, while in the case where all the points xi are
identical, Ï„t,i = Ï†Ti (tÏ†i Ï†Ti + Î±I)âˆ’1 Ï†i â‰¤ 1/(t + Î±) and its
RLS is minimal. While the previous definition is provided
for a generic kernel function K, we can easily instantiate it
on K and obtain the definition of Ï„ t,i . By recalling the first
regret term in the decomposition of Prop. 1, we notice that
RG =

T
X
t=1

T

T

Ï†t (Î¦t Î¦t + Î±I)âˆ’1 Ï†t /Î·t =

T
X
t=1

Ï„ t,t /Î·t ,

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

which reveals a deep connection between the regret of
KONS and the cumulative sum of the RLS. In other words,
the RLS capture how much the adversary can increase
the regret by picking orthogonal directions that have not
been seen before. While in LOCO, this can happen at
most d times (hence the dependency on d in the final regret,
which is mitigated by a suitable choice of Î·t ), in KOCO,
RG can grow linearly with time, since large H can have
infinite near-orthogonal directions. Nonetheless, the actual
growth rate is now directly related to the complexity of the
sequence of points chosen by the adversary and the kernel
function K. While the effective dimension dteff (Î±) is related to the capacity of the RKHS H on the points in Dt
and it has been shown to characterize the generalization error in batch linear regression (Rudi et al., 2015), we see
that RG is rather related to the online effective dimension
P
t
donl (Î±) =
i Ï„ i,i . Nonetheless, we show that the two
quantities are also strictly related to each other.
Lemma 3. For any dataset DT , any Î± > 0 we have
T

donl (Î±) :=

T
X
t=1

T

â‰¤ deff (Î±)(1 + log(kKT k/Î± + 1)).
T

We first notice that in the first inequality we relate donl (Î±)
to the log-determinant of the kernel matrix KT . This quantity appears in a large number of works on online linear
prediction (Cesa-Bianchi et al., 2005; Srinivas et al., 2010)
where they were connected to the maximal mutual information gain in Gaussian processes. Finally, the second
inequality shows that in general the complexity of online
learning is only a factor log T (in the worst case) away
from the complexity of batch learning. At this point, we
can generalize the regret bounds of LOCO to KOCO.
Theorem 1. For any sequence of losses `t satisfying
Asm.âˆš
1-2, let Ïƒ = mint Ïƒt . If Î·t â‰¥ Ïƒ â‰¥ 0 for all t and
Î± â‰¤ T , the regret of Alg. 1 is upper-bounded as
T
X

for KT and that it is smaller than the rank r for any Î±.2 For
exp-concave functions (i.e., Ïƒ > 0), we slightly improve
over the bound of Luo et al. (2016) from O(d log T ) down
to O(dTeff (Î±) log T ) â‰¤ O(r log T ), where r is the (unknown) rank
p of the dataset. Furthermore, when
âˆš Ïƒ = 0, settingâˆšÎ·t = 1/(L2 C 2 t) gives us a regret O( T dTeff (Î±))
âˆš â‰¤
O( T r), which is potentially much smaller than O( T d).
Furthermore, if an oracle
q provided us in advance with

dTeff (Î±), setting Î·t =
dTeff (Î±)/(L2 C 2 t) gives a regret
q
âˆš
O( dTeff (Î±)T ) â‰¤ O( rT ).

Ï„ t,t â‰¤ log(Det(KT /Î± + I))

RT â‰¤ Î±kwâˆ— k2 + dTonl (Î±)/Î·T + 4C 2 L2

Algorithm 2 Kernel Online Row Sampling (KORS)
Input: Regularization Î±, accuracy Îµ, budget Î²
1: Initialize I0 = âˆ…
2: for t = {0, . . . , T âˆ’ 1} do
3:
receive Ï†t
4:
construct temporary dictionary I t := Itâˆ’1 âˆª (t, 1)
5:
compute pet = min{Î²e
Ï„t,t , 1} using I t and Eq. 4
6:
draw zt âˆ¼ B(e
pt ) and if zt = 1, add (t, 1/e
pt ) to It
7: end for

(Î·t âˆ’ Ïƒ).

t=1

In particular, if for all t we have Ïƒt â‰¥ Ïƒ > 0, setting
Î·t = Ïƒ we obtain
 log(2ÏƒL2 T )
,
RT â‰¤ Î±kwâˆ— k2 + 2dTeff Î±/(ÏƒL2 )
Ïƒ
âˆš
otherwise, Ïƒ = 0 and setting Î·t = 1/(LC t) we obtain
âˆš
RT â‰¤ Î±kwâˆ— k2 + 4LC T dTeff (Î±/L2 ) log(2L2 T ).
Comparison to LOCO algorithms. We first notice that
the effective dimension dTeff (Î±) can be seen as a soft rank

Comparison to KOCO algorithms. Simple functional
gradient descent
âˆš (e.g., NORMA, Kivinen et al., 2004)
achieves a O( T ) regret when properly tuned (Zhu &
Xu, 2015), regardless of the loss function. For the special case of squared loss, Zhdanov & Kalnishkan (2010)
show that Kernel Ridge Regression achieves the same
O(log(Det(KT /Î± + I))) regret as achieved by KONS for
general exp-concave losses.

4. Kernel Online Row Sampling
Although KONS achieves a low regret, storing and inverting the K matrix requires O(t2 ) space and O(t3 ) time,
which becomes quickly unfeasible as t grows. To improve
space and time efficiency, we replace Kt with an accurate
e t , constructed using a carefully
low-rank approximation K
chosen dictionary It of points from Dt . We extend the online row sampling (ORS) algorithm of Cohen et al. (2016)
to the kernel setting and obtain Kernel-ORS (Alg. 2). There
are two main obstacles to overcome in the adaptation of
ORS: From an algorithmic perspective we need to find a
computable estimator for the RLS, since Ï†t cannot be accessed directly, while from an analysis perspective we must
prove that our space and time complexity does not scale
with the dimension of Ï†t (as Cohen et al. 2016), as it can
potentially be infinite.
We define a dictionary It as a collection of (index, weight)
tuples (i, 1/e
pi ) and the associated selection matrix St âˆˆ
This can be easily seen as dTeff (Î±) =
Î»t are the eigenvalues of KT .
2

P

t

Î»t /(Î»t + Î±), where

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

p
RtÃ—t as a diagonal matrix with 1/ pei for all i âˆˆ It and 0
T
elsewhere. We also introduce AIt t = Î¦t St STt Î¦t +Î±I as an
approximation of At constructed using the dictionary It .
At each time step, KORS temporarily adds t with weight 1
to the dictionary Itâˆ’1 and constructs the temporary dictionary It,âˆ— and the corresponding selection matrix St,âˆ— and
I
approximation At t,âˆ— . This augmented dictionary can be
effectively used to compute the RLS estimator,
I

Ï„et,i = (1 + Îµ)Ï†t At t,âˆ—
=

1+Îµ
Î±

T

âˆ’1

Ï†t

T

k t,t âˆ’ k[t],t St,âˆ— (St,âˆ— Kt St,âˆ— +

Î±I)âˆ’1 STt,âˆ— k[t],t

(4)

.

While we introduced a similar estimator before (Calandriello et al., 2017), here we modified it so that Ï„et,i is
an overestimate of the actual Ï„ t,i . Note that all rows and
columns for which St,âˆ— is zero (all points outside the temporary dictionary It,âˆ— ) do not influence the estimator, so
they can be excluded from the computation. As a consequence, denoting by |It,âˆ— | the size of the dictionary, Ï„et,i can
be efficiently computed in O(|It,âˆ— |2 ) space and O(|It,âˆ— |2 )
time (using an incremental update of Eq. 4). After computing the RLS, KORS randomly chooses whether to include
a point in the dictionary using a coin-flip with probability
pet = min{Î²e
Ï„t,t , 1} and weight 1/e
pt , where Î² is a parameter. The following theorem gives us at each step guarantees
on the accuracy of the approximate matrices AIt t and of
estimates Ï„et,t , as well as on the size |It | of the dictionary.
Theorem 2. Given parameters 0 < Îµ â‰¤ 1, 0 < Î±,
1+Îµ
and run Algorithm 2 with
0 < Î´ < 1, let Ï = 1âˆ’Îµ
2
Î² â‰¥ 3 log(T /Î´)/Îµ . Then w.p. 1 âˆ’ Î´, for all steps t âˆˆ [T ],
(1) (1 âˆ’ Îµ)At  AIt t  (1 + Îµ)At .
Pt
(2) The dictionaryâ€™s size |It | = s=1 zs is bounded by
t
X
s=1

zs â‰¤ 3

t
X

2

pes â‰¤ dtonl (Î±)

s=1

6Ï log
3ÏÎ²
â‰¤ dteff (Î±)
2
Îµ
Îµ2

2T
Î´

Moreover, the algorithm runs in
e t (Î±)2 ) time per iteration.
space, and O(d
eff

performance and regret. Alg. 3 runs KORS as a black-box
estimating RLS Ï„et , that are then used
to sketch the original
e t = Pt Î·t zt gt gT , where at
matrix At with a matrix A
t
s=1
each step we add the current gradient gt gtT only if the coin
et
flip zt succeeded. Unlike KORS, the elements added to A
are not weighted, and the probabilities pet used for the coins
zt are chosen as the maximum between Ï„et,t , and a parameter 0 â‰¤ Î³ â‰¤ 1. Let Rt be the unweighted counterpart of
St , that is [Rt ]i,j = 0 if [St ]i,j = 0 and [Rt ]i,j = 1 if
[St ]i,j 6= 0. Then we can efficiently compute the coefficients ebt and predictions yet as follows.
Lemma 4. Let Et = RTt Kt Rt +Î±I be an auxiliary matrix,
e t ]i used in Alg. 3 can be
then all the components ebi = [b
computed as


1
Î±h(yÌ†i )
âˆš
gÌ‡i Î·i yei âˆ’
âˆ’
.
T
Î·i
ki,i âˆ’ k[iâˆ’1],i Riâˆ’1 Eâˆ’1
iâˆ’1 Riâˆ’1 k[iâˆ’1],i
Then we can compute


.

(3) Satisfies Ï„t,t â‰¤ Ï„et,t â‰¤ ÏÏ„t,t .
O(dteff (Î±)2

Algorithm 3 S KETCHED -KONS
Input: Feasible parameter C, stepsizes Î·t , regulariz. Î±
e 0 = Î±I
1: Initialize w0 = 0, g0 = 0, b0 = 0, A
2: Initialize independent run of KORS
3: for t = {1, . . . , T } do
4:
receive xt
e âˆ’1 (Ptâˆ’1 ebs gs )
et = A
5:
compute u
tâˆ’1
s=0
et
6:
compute yÌ†t = Ï•(xt )T u
e t = yÌ†t âˆ’ h(yÌ†t ), observe gt
7:
predict yet = Ï•(xt )T w
8:
compute Ï„et,t using KORS (Eq. 4)
9:
compute pet = max{min{Î²e
Ï„t,t , 1}, Î³}
10:
draw zt âˆ¼ B(e
pt )
et = A
e tâˆ’1 + Î·t zt gt gT
11:
update A
t
12: end for

4

log (T ))

The most interesting aspect of this result is that the dictionary It generated by KORS allows to accurately approxT
imate the At = Î¦t Î¦t + Î±I matrix up to a small (1 Â± Îµ)
multiplicative factor with a small time and space complexity, which makes it a natural candidate to sketch KONS.

5. Sketched ONS
Building on KORS, we now introduce a sketched variant of
KONS that can efficiently trade off between computational

yÌ†t =

1 T
k
Dtâˆ’1 btâˆ’1
Î± [tâˆ’1],t

âˆ’ kT[tâˆ’1],t Dtâˆ’1 Rtâˆ’1 Eâˆ’1
tâˆ’1 Rtâˆ’1 Ktâˆ’1 btâˆ’1 .

Note that since the columns in Rt are selected without
weights, (RTt Kt Rt + Î±I)âˆ’1 can be updated efficiently
e t changes.
using block inverse updates, and only when A
While the specific reason for choosing the unweighted
e t instead of the weighted version AIt used in
sketch A
t
KORS is discussed further in Sect. 6, the following corole t is as accurate as AIt in approximating
lary shows that A
t
At up to the smallest sampling probability peÎ³t .
Corollary 1. Let peÎ³min = minTt=1 peÎ³t . Then w.h.p., we have
e t.
(1 âˆ’ Îµ)e
pmin At  pemin AIt t  A
We can now state the main result of this section. Since
for S KETCHED -KONS we are interested not only in regret

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

minimization, but also in space and time complexity, we
do not consider the case Ïƒ = 0, because when the function
does not have any curvature,
âˆš standard GD already achieves
the optimal regret of O( T ) (Zhu & Xu, 2015) while requiring only O(t) space and time per iteration.
Theorem 3. For any sequence of losses `t satisfying
T
Asm. 1-2, let Ïƒ = mint Ïƒt and
âˆš Ï„ min = mint=1 Ï„ t,t 2. When
Î·t â‰¥ Ïƒ > 0 for all t, Î± â‰¤ T , Î² â‰¥ 3 log(T /Î´)/Îµ , if we
set Î·t = Ïƒ then w.p. 1 âˆ’ Î´ the regret of Alg. 3 satisfies

dTeff Î±/(ÏƒL2 ) log(2ÏƒL2 T )
âˆ— 2
e
, (5)
RT â‰¤ Î±kw k + 2
Ïƒ max{Î³, Î²Ï„ min }
and the algorithm runs in O(dteff (Î±)2 + t2 Î³ 2 ) time and
O(dteff (Î±)2 + t2 Î³ 2 ) space complexity for each iteration t.
Proof sketch: Given these guarantees, we need to bound
RG and RD . Bounding RD is straightforward, since
by construction S KETCHED -KONS adds at most Î·t gt gtT
e t at each step. To bound RG instead, we must take
to A
e t = Î¦t Rt RT Î¦T + Î±I
into account that an unweighted A
t t
T
can be up to pemin distant from the weighted Î¦t St STt Î¦t
for which we have guarantees. Hence the max{Î³, Î²Ï„ min }
term appearing at the denominator.

6. Discussion
Regret guarantees. From Eq. 5 we can see that when Ï„ min
is not too small, setting Î³ = 0 we recover the guarantees of
exact KONS. Since usually we do not know Ï„ min , we can
choose to set Î³ > 0, and as long as Î³ â‰¥ 1/ polylog T , we
preserve a (poly)-logarithmic regret.
Computational speedup. The time required to compute
k[tâˆ’1],t , kt,t , and kT[tâˆ’1],t Dtâˆ’1 btâˆ’1 gives a minimum O(t)
per-step complexity. Note that Ktâˆ’1 btâˆ’1 can also be computed incrementally in O(t) time. Denoting the size of the
e eff (Î±)t + tÎ³), computing
dictionary at time t as Bt = O(d
âˆ’1
e t ]i and kT
[b
D
R
E
[tâˆ’1],t tâˆ’1 tâˆ’1 tâˆ’1 Rtâˆ’1 Ktâˆ’1 btâˆ’1 requires
an additional O(Bt2 ) time. When Î³ â‰¤ dteff (Î±)/t, each iteration takes O(dteff (Î±)2 ) to compute Ï„et,t incrementally using
e âˆ’1 and O(dt (Î±)2 )
KORS, O(dteff (Î±)2 ) time to update A
t
eff
time to compute [bt ]t . When Î³ > dteff (Î±)/t, each iteration still takes O(dteff (Î±)2 ) to compute Ï„et,t using KORS
and O(t2 Î³ 2 ) time to update the inverse and compute [bt ]t .
Therefore, in the case when Ï„ min is not too small, our runtime is of the order O(dteff (Î±)2 + t), which is almost as
small as the O(t) runtime of GD but with the advantage
of a second-order method logarithmic regret. Moreover,
when Ï„ min is small and we set a large Î³, we can trade off a
1/Î³ increase in regret for a Î³ 2 decrease in space and time
complexity when compared to exact KONS (e.g., setting
Î³ = 1/10 would correspond to a tenfold increase in regret,
but a hundred-fold reduction in computational complexity).

Asymptotic behavior. Notice however, that space and time
complexity, grow roughly with a term â„¦(t mints=1 pes ) âˆ¼
â„¦(t max{Î³, Î²Ï„ min }), so if this quantity does not decrease
over time, the computational cost of S KETCHED -KONS
will remain large and close to exact KONS. This is to be
expected, since S KETCHED -KONS must always keep an
accurate sketch in order to guarantee a logarithmic regret
bound. Note that Luo et al. (2016) took an opposite approach for LOCO, where they keep a fixed-size sketch but
possibly pay in regret, if this fixed size happens to be too
small. Since a non-logarithmic regret is achievable simply
running vanilla GD, we rather opted for an adaptive sketch
at the cost of space and time complexity. In batch optimization, where `t does not change over time, another possibility is to stop updating the solution once Ï„ min becomes
too small. When Hs is the Hessian of ` in ws , then the
quantity gtT Hâˆ’1
t gt , in the context of Newtonâ€™s method, is
called Newton decrement and it corresponds up to constant
factors to Ï„ min . Since a stopping condition based on Newtonâ€™s decrement is directly related to the near-optimality of
the current wt (Nesterov & Nemirovskii, 1994), stopping
when Ï„ min is small also provides guarantees about the quality of the solution.
Sampling distribution. Note that although Î³ > 0 means
that all columns have a small uniform chance of being see t , this is not equivalent to unilected for inclusion in A
formly sampling columns. It is rather a combination of a
RLS-based sampling to ensure that columns important to
reconstruct At are selected and a threshold on the probabilities to avoid too much variance in the estimator.
Biased estimator and results in expectation. The rane t is biased, since E[Î¦t Rt RT Î¦T ] =
dom approximation A
t t
T
T
Î¦t Diag({Ï„ t,t })Î¦t 6= Î¦t Î¦t . Another option would be
e0
to
Ptuse a weightedT and unbiased approximation At =
ps gs gs used in KORS and a common choice
s=1 Î·s zs /e
in matrix approximation methods, see e.g., Alaoui & Mahoney, 2015. Due to its unbiasedness, this variant would
automatically achieve the same logarithmic regret as exact KONS in expectation (similar to the result obtained
by Luo et al., 2016, using Gaussian random projection in
LOCO). While any unbiased estimator, e.g., uniform sampling of gt , would achieve this result, RLS-based sampling
already provides strong reconstruction guarantees sufficient to bound RG . Nonetheless, the weights 1/e
ps may
e
cause large variations in At over consecutive steps, thus
leading to a large regret RD in high probability.
Limitations of dictionary learning approaches and open
problems. From the discussion above, it appears that
a weighted, unbiased dictionary may not achieve highprobability logarithmic guarantee because of the high variance coming from sampling. On the other hand, if we want
to recover the regret guarantee, we may have to pay for it

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

with a large dictionary. This may actually be due to the
analysis, the algorithm, or the setting. An important property of the dictionary learning approach used in KORS is
that it can only add but not remove columns and potentially
re-weight them. Notice that in the batch setting (Alaoui &
Mahoney, 2015; Calandriello et al., 2017), the sampling of
columns does not cause any issue and we can have strong
learning guarantees in high probability with a small dictionary. Alternative sketching methods such as Frequent Directions (FD, Ghashami et al., 2016a) do create new atoms
as learning progresses. By restricting to composing dictionaries from existing columns, we only have the degree
of freedom of the weights of the columns. If we set the
weights to have an unbiased estimate, we achieve an accurate RG but suffer a huge regret in RD . On the other hand,
we can store the columns unweighted to have small RD but
large RG . This could be potentially fixed if we knew how
to remove less important columns from dictionary to gain
some slack in RD .
We illustrate this problem with following simple scenario.
The adversary always presents to the learner the same
point x (with associated Ï†), but for the loss it alternates
between `2t (wt ) = (C âˆ’ Ï†T wt )2 on even steps and
`2t+1 (wt ) = (âˆ’C âˆ’ Ï†T wt )2 on odd steps. Then, Ïƒt =
Ïƒ = 1/(8C 2 ), and we have a gradient that always points in
the same Ï† direction, but switches sign at each step. The
optimal solution in hindsight is asymptotically w = 0 and
let this be also our starting point w0 . We also set Î·t = Ïƒ,
since this is what ONS would do, and Î± = 1 for simplicity.
For this scenario, we can compute several useful quantities
in closed form, in particular, RG and RD ,
RG â‰¤

T
X

T

X
gÌ‡t2
C2
â‰¤ O(log T ),
â‰¤
2
2
C Ïƒt + Î±
s=1 gÌ‡s Ïƒ + Î±
t=1

Pt
t=1

RD =

Xt
s=1

(Î·t âˆ’ Ïƒ)(wtT gt )2 = 0.

Note that although the matrix At is rank 1 at each time step,
vanilla ONS does not take advantage of this easy data, and
would store it all with a O(t2 ) space in KOCO.
As for the sketched versions of ONS, sketching using
FD (Luo et al., 2016) would adapt to this situation, and
only store a single copy of gt = g, achieving the desired regret with a much smaller space. Notice that in
this example, the losses `t are effectively strongly convex,
and even basic gradient descent with a stepsize Î·t = 1/t
would achieve logarithmic regret (Zhu & Xu, 2015) with
even smaller space. On the other hand, we show how the
dictionary-based sketching has difficulties in minimizing
the regret bound from Prop. 1 in our simple scenario. In
particular, consider an arbitrary (possibly randomized) algorithm that is allowed only to reweight atoms in the dictionary and not to create new ones (as FD). In our example, this translates to choosing a schedule of weights ws

e t = Pt ws Ï† Ï† = Wt Ï†Ï† with total weight
and set A
s s
PTs=1
W = WT = s=1 ws and space complexity equal to the
number of non-zero weights B = |{ws 6= 0}|. We can
show that there is no schedule for this specific class of algorithms with good performance due to the following three
conflicting goals.
Pt
(1) To mantain RG small, s=1 ws should be as large as
possible, as early as possible.
(2) To mantain RD small, we should choose weights
wt > 1 as few times as possible, since we accumulate max{wt âˆ’ 1, 0} regret every time.
(3) To mantain the space complexity small, we should
choose only a few wt 6= 0.
To enforce goal (3), we must choose a schedule with no
more than B non-zero entries. Given the budget B, to satisfy goal (2) we should use all the B budget in order to
exploit as much as possible the max{wt âˆ’ 1, 0} in RD , or
in other words we should use exactly B non-zero weights,
and none of these should be smaller P
than 1. Finally, to mint
imize RG we should raise the sum s=1 ws as quickly as
possible, settling on a schedule where w1 = W âˆ’ B and
ws = 1 for all the other B weights. It easy to see that if we
want logarithmic RG , W needs to grow as T , but doing so
with a logarithmic B would make RD = T âˆ’ B = â„¦(T ).
Similarly, keeping W = B in order to reduce RD would
increase RG . In particular notice, that the issue does not
go away even if we know the RLS perfectly, because the
same reasoning applies. This simple example suggests that
dictionary-based sketching methods, which are very successful in batch scenarios, may actually fail in achieving
logarithmic regret in online optimization.
This argument raises the question on how to design alternative sketching methods for the second-order KOCO. A first
approach, discussed above, is to reduce the dictionary size
dropping columns that become less important later in the
process, without allowing the adversary to take advantage
of this forgetting factor. Another possibility is to deviate
from the ONS approach and RD + RG regret decomposition. Finally, as our counterexample in the simple scenario hints, creating new atoms (either through projection
or merging) allows for better adaptivity, as shown by FD
(Ghashami et al., 2016a) based methods in LOCO. However, the kernelization of FD does not appear to be straighforward. The most recent step in this direction (in particular, for kernel PCA) is only able to deal with finite feature
expansions (Ghashami et al., 2016b) and therefore its application to kernels is limited.
Acknowledgements The research presented was supported by
French Ministry of Higher Education and Research, NordPas-de-Calais Regional Council and French National Research
Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and
BoB (n.ANR-16-CE23-0003)

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

References
Alaoui, Ahmed El and Mahoney, Michael W. Fast randomized kernel methods with statistical guarantees. In
Neural Information Processing Systems, 2015.
Calandriello, Daniele, Lazaric, Alessandro, and Valko,
Michal. Distributed sequential sampling for kernel matrix approximation. In International Conference on Artificial Intelligence and Statistics, 2017.
Cavallanti, Giovanni, Cesa-Bianchi, Nicolo, and Gentile,
Claudio. Tracking the best hyperplane with a simple
budget perceptron. Machine Learning, 69(2-3):143â€“167,
2007.
Cesa-Bianchi, Nicolo, Conconi, Alex, and Gentile, Claudio. A second-order perceptron algorithm. SIAM Journal on Computing, 34(3):640â€“668, 2005.
Cohen, Michael B, Musco, Cameron, and Pachocki, Jakub.
Online row sampling. International Workshop on Approximation, Randomization, and Combinatorial Optimization, 2016.
Dekel, Ofer, Shalev-Shwartz, Shai, and Singer, Yoram. The
forgetron: A kernel-based perceptron on a budget. SIAM
Journal on Computing, 37(5):1342â€“1372, 2008.
Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research,
12:2121â€“2159, 2011.
Gammerman, Alex, Kalnishkan, Yuri, and Vovk, Vladimir.
On-line prediction with kernels and the complexity approximation principle. In Uncertainty in Artificial Intelligence, 2004.
Ghashami, Mina, Liberty, Edo, Phillips, Jeff M, and
Woodruff, David P. Frequent directions: Simple and deterministic matrix sketching. SIAM Journal on Computing, 45(5):1762â€“1792, 2016a.
Ghashami, Mina, Perry, Daniel J, and Phillips, Jeff.
Streaming kernel principal component analysis. In International Conference on Artificial Intelligence and Statistics, 2016b.
Hazan, Elad, Kalai, Adam, Kale, Satyen, and Agarwal,
Amit. Logarithmic regret algorithms for online convex
optimization. In Conference on Learning Theory, 2006.

Le, Quoc, SarloÌs, TamaÌs, and Smola, Alex J. Fastfood Approximating kernel expansions in loglinear time. In
International Conference on Machine Learning, 2013.
Le, Trung, Nguyen, Tu, Nguyen, Vu, and Phung, Dinh.
Dual Space Gradient Descent for Online Learning. In
Neural Information Processing Systems, 2016.
Lu, Jing, Hoi, Steven C.H., Wang, Jialei, Zhao, Peilin, and
Liu, Zhi-Yong. Large scale online kernel learning. Journal of Machine Learning Research, 17(47):1â€“43, 2016.
Luo, Haipeng, Agarwal, Alekh, Cesa-Bianchi, Nicolo, and
Langford, John. Efficient second-order online learning
via sketching. Neural Information Processing Systems,
2016.
Nesterov, Yurii and Nemirovskii, Arkadii. Interior-point
polynomial algorithms in convex programming. Society
for Industrial and Applied Mathematics, 1994.
Orabona, Francesco and Crammer, Koby. New adaptive algorithms for online classification. In Neural Information
Processing Systems, 2010.
Orabona, Francesco, Keshet, Joseph, and Caputo, Barbara.
The projectron: a bounded kernel-based perceptron. In
International Conference on Machine learning, 2008.
Rudi, Alessandro, Camoriano, Raffaello, and Rosasco,
Lorenzo. Less is more: NystroÌˆm computational regularization. In Neural Information Processing Systems,
2015.
SchoÌˆlkopf, Bernhard and Smola, Alexander J. Learning
with kernels: Support vector machines, regularization,
optimization, and beyond. MIT Press, 2001.
Srinivas, Niranjan, Krause, Andreas, Seeger, Matthias, and
Kakade, Sham M. Gaussian process optimization in the
bandit setting: No regret and experimental design. In
International Conference on Machine Learning, 2010.
Tropp, Joel Aaron. Freedmanâ€™s inequality for matrix martingales. Electronic Communications in Probability, 16:
262â€“270, 2011.
Wang, Zhuang, Crammer, Koby, and Vucetic, Slobodan.
Breaking the curse of kernelization: Budgeted stochastic
gradient descent for large-scale svm training. Journal of
Machine Learning Research, 13(Oct):3103â€“3131, 2012.
Zhdanov, Fedor and Kalnishkan, Yuri. An identity for kernel ridge regression. In Algorithmic Learning Theory,
2010.

Kingma, Diederik and Ba, Jimmy. Adam: A method for
stochastic optimization. In International Conference on
Learning Representations, 2015.

Zhu, C. and Xu, H. Online gradient descent in function
space. ArXiv:1512.02394, 2015.

Kivinen, J., Smola, A.J., and Williamson, R.C. Online
Learning with Kernels. IEEE Transactions on Signal
Processing, 52(8), 2004.

Zinkevich, Martin. Online convex programming and generalized infinitesimal gradient ascent. In International
Conference on Machine Learning, 2003.

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

A. Preliminary results
We begin with a generic linear algebra identity that is be used throughout our paper.
Proposition 2. For any X âˆˆ RnÃ—m matrix and Î± > 0,
XXT (XXT + Î±I)âˆ’1 = X(XT X + Î±I)âˆ’1 XT
and
1
Î±I(XXT + Î±I)âˆ’1
Î±
1
= (XXT âˆ’ XXT + Î±I)(XXT + Î±I)âˆ’1
Î±
1
= (I âˆ’ XXT (XXT + Î±I)âˆ’1 )
Î±
1
= (I âˆ’ X(XT X + Î±I)âˆ’1 XT ).
Î±

(XXT + Î±I)âˆ’1 =

Proposition 3. For any matrix or linear operator X, if a selection matrix S satisfies
k(XXT + Î±I)âˆ’1/2 (XXT âˆ’ XSST XT )(XXT + Î±I)âˆ’1/2 k â‰¤ Îµ,
we have
(1 âˆ’ Îµ)Xt XTt âˆ’ ÎµÎ±I  Xt St STt XTt  (1 + Îµ)Xt XTt + ÎµÎ±I.
Proposition 4. Let Kt = UÎ›UT and Î¦t = VÎ£UT , then
k(Î¦t Î¦Tt + Î±I)âˆ’1/2 Î¦t (I âˆ’ Ss STs )Î¦Tt (Î¦t Î¦Tt + Î±I)âˆ’1/2 k
= k(Î£Î£T + Î±I)âˆ’1/2 Î£UT (I âˆ’ Ss STs )UÎ£T (Î£Î£T + Î±I)âˆ’1/2 k
= k(Î› + Î±I)âˆ’1/2 Î›1/2 UT (I âˆ’ Ss STs )U(Î›1/2 )T (Î› + Î±I)âˆ’1/2 k
1/2

1/2

= k(Kt + Î±I)âˆ’1/2 Kt (I âˆ’ Ss STs )Kt (Kt + Î±I)âˆ’1/2 k.
We also use the following concentration inequality for martingales.
Proposition 5 (Tropp, 2011, Thm. 1.2). Consider a matrix martingale {Yk : k = 0, 1, 2, . . . } whose values are selfadjoint matrices with dimension d and let {Xk : k = 1, 2, 3, . . . } be the difference sequence. Assume that the difference
sequence is uniformly bounded in the sense that
kXk k2 â‰¤ R

for k = 1, 2, 3, . . . .

almost surely

Define the predictable quadratic variation process of the martingale as
Wk :=

k

h
i
X

E X2j  {Xs }jâˆ’1
s=0 ,

for k = 1, 2, 3, . . . .

j=1

Then, for all Îµ â‰¥ 0 and Ïƒ 2 > 0,
P âˆƒk â‰¥ 0 : kYk k2 â‰¥ Îµ âˆ© kWk k â‰¤ Ïƒ

2



Îµ2 /2
â‰¤ 2d Â· exp âˆ’ 2
Ïƒ + RÎµ/3



.

Proposition 6 (Calandriello et al., 2017, App. D.4). P
Let {zs }ts=1 be independent Bernoulli random variables, each with
t
success probability ps , and denote their sum as d = s=1 ps â‰¥ 1. Then,3
!
t
X
P
zs â‰¥ 3d â‰¤ exp{âˆ’3d(3d âˆ’ (log(3d) + 1))} â‰¤ exp{âˆ’2d}
s=1
3

This is a simple variant of Chernoff bound where the Bernoulli random variables are not identically distributed.

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

B. Proofs for Section 3
Proof of Lem. 1. We begin by applying the definition of ut+1 and collecting Aâˆ’1
t , which can always be done since, for
Î± > 0, At is invertible,
âˆ’1
ut+1 = wt âˆ’ Aâˆ’1
t gt = At (At wt âˆ’ gt ).

We focus now on the last term and use the definition of At ,
At wt âˆ’ gt = Atâˆ’1 wt + Î·t gt gtT wt âˆ’ gt
âˆš
âˆš
= Atâˆ’1 ut âˆ’ Atâˆ’1 rt + ( Î·t gtT wt âˆ’ 1/ Î·t )Ï†t .
Looking at Atâˆ’1 rt and using the assumption gÌ‡t 6= 0,
Atâˆ’1 rt =

h(Ï†Tt ut )
Atâˆ’1 Aâˆ’1
tâˆ’1 Ï†t
Ï†Tt Aâˆ’1
Ï†
t
tâˆ’1

h(Ï†Tt ut ) gÌ‡t2 Î·t
2 Ï†t
Ï†Tt Aâˆ’1
tâˆ’1 Ï†t gÌ‡t Î·t
âˆš
gÌ‡t Î·t h(Ï†Tt ut )
Ï†t .
=
T
Ï†t Aâˆ’1
tâˆ’1 Ï†t
=

Putting together all three terms, and using the fact that gtT wt = gÌ‡t Ï†t wt = gÌ‡t ybt and denoting bt = [bt ]t we have
ut+1 = Aâˆ’1
t (At wt âˆ’ gt )
= Aâˆ’1
t (Atâˆ’1 ut + bt Ï†t )
âˆ’1
= Aâˆ’1
t (Atâˆ’1 (wtâˆ’1 âˆ’ Atâˆ’1 gtâˆ’1 ) + bt Ï†t )

= Aâˆ’1
t (Atâˆ’1 wtâˆ’1 âˆ’ gtâˆ’1 + bt Ï†t )
= Aâˆ’1
t (Atâˆ’2 wtâˆ’2 âˆ’ gtâˆ’2 + btâˆ’1 Ï†tâˆ’1 + bt Ï†t )
Xt
= Aâˆ’1
bs Ï†s ).
t (A0 w0 +
s=1

Proof of Lem. 2. Throughout this proof, we make use of the linear algebra identity from Prop. 2. We begin with the
reformulation of [bt ]t . In particular, the only term that we need to reformulate is
T

âˆ’1
Ï†t Aâˆ’1
Ï†t
tâˆ’1 Ï†t = Ï†t (Î¦tâˆ’1 Î¦tâˆ’1 + Î±I)
1
T
T
= Ï†t (I âˆ’ Î¦tâˆ’1 (Î¦tâˆ’1 Î¦tâˆ’1 + Î±I)âˆ’1 Î¦tâˆ’1 )Ï†t
Î±
1 T
T
T
T
= (Ï†t Ï†t âˆ’ Ï†t Î¦tâˆ’1 (Î¦tâˆ’1 Î¦tâˆ’1 + Î±I)âˆ’1 Î¦tâˆ’1 Ï†t )
Î±
1
T
= (kt,t âˆ’ k[tâˆ’1],t (Ktâˆ’1 + Î±I)âˆ’1 k[tâˆ’1],t ).
Î±

For y t , we have
y t = Ï†Tt ut = Ï†Tt Aâˆ’1
tâˆ’1 Î¦tâˆ’1 btâˆ’1
T

= Ï†Tt (Î¦tâˆ’1 Î¦tâˆ’1 + Î±I)âˆ’1 Î¦tâˆ’1 btâˆ’1
1
T
T
= Ï†Tt (I âˆ’ Î¦tâˆ’1 (Î¦tâˆ’1 Î¦tâˆ’1 + Î±I)âˆ’1 Î¦tâˆ’1 )Î¦tâˆ’1 btâˆ’1
Î±
1
= Ï†Tt Î¦tâˆ’1 Dtâˆ’1 (btâˆ’1 âˆ’ (Ktâˆ’1 + Î±I)âˆ’1 Ktâˆ’1 btâˆ’1 )
Î±
1
= kT[tâˆ’1],t Dtâˆ’1 (btâˆ’1 âˆ’ (Ktâˆ’1 + Î±I)âˆ’1 Ktâˆ’1 btâˆ’1 ).
Î±

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

Proof of Lem. 3. We prove the lemma for a generic kernel K and kernel matrix KT . Then, Lem. 3 simply follows by
applying the proof to K and KT . From the definition of Ï„t,t we have
T
X

Ï„t,t =

t=1

T
X

Ï†Tt (Î¦t Î¦Tt + Î±I)âˆ’1 Ï†t =

T
X
âˆš
âˆš
âˆ’1
(Ï†Tt / Î±) (Î¦t Î¦Tt /Î± + I) (Ï†t / Î±) â‰¤ log(Det(Î¦T Î¦TT /Î± + I)),
t=1

t=1

where the last passage is proved by Hazan et al. (2006). Using Sylvesterâ€™s determinant identity,
Det(Î¦T Î¦TT /Î± + I) = Det(Î¦TT Î¦T /Î± + I) =

T
Y

(Î»t /Î± + 1),

t=1

where Î»t are the eigenvalues of Î¦TT Î¦T = KT . Then,
T
X

Ï„t,t â‰¤ log

Y



T
t=1

t=1

(Î»t /Î± + 1)

=

XT
t=1

log(Î»t /Î± + 1).

We can decompose this as
T
X

log(Î»t /Î± + 1) =

t=1

T
X


log(Î»t /Î± + 1)

t=1

=

T
X

Î»t /Î± + 1
Î»t /Î± + 1



T

log(Î»t /Î± + 1)

t=1

â‰¤ log(kKT k/Î± + 1)

X log(Î»t /Î± + 1)
Î»t /Î±
+
Î»t /Î± + 1 t=1 Î»t /Î± + 1
T
X
t=1

T

X log(Î»t /Î± + 1)
Î»t
+
Î»t + Î± t=1 Î»t /Î± + 1

â‰¤ log(kKT k/Î± + 1)dTeff (Î±) +
= log(kKT k/Î± + 1)dTeff (Î±) +

T
X
(Î»t /Î± + 1) âˆ’ 1

Î»t /Î±
t=1
dTeff (Î±),

+1

where the first inequality is due to kKT k â‰¥ Î»t for all t and the monotonicity of log(Â·), and the second inequality is due to
log(x) â‰¤ x âˆ’ 1.
Proof of Thm 1. We need to bound RT (wâˆ— ), and we use Prop. 1. For RD nothing changes from the parametric case, and
we use Asm. 1 and the definition of the set S to bound
RD =

XT
t=1

(Î·t âˆ’ Ïƒt )gÌ‡t2 (Ï†Tt (wt âˆ’ w))2 â‰¤

XT
t=1

(Î·t âˆ’ Ïƒ)L2 (|Ï†Tt wt | + |Ï†Tt w|)2 â‰¤ 4L2 C 2

XT
t=1

(Î·t âˆ’ Ïƒ).

For RG , we reformulate
XT
t=1

T

XT 1 T
Î·t T âˆ’1
gt At gt =
Ï†t Aâˆ’1
t Ï†t
t=1 Î·t
t=1 Î·t
T
1 XT
1 XT
deff (Î±)
T
â‰¤
Ï†t Aâˆ’1
Ï†
=
Ï„
=
d
(Î±)/Î·
â‰¤
(1 + log(kKT k/Î± + 1)),
t,t
onl
T
t
t
t=1
t=1
Î·T
Î·T
Î·T

gtT Aâˆ’1
t gt =

XT

where deff (Î±) and KT are computed using the rescaled kernel K.

âˆš
Let us remind ourselves the definition D = Diag {gÌ‡t Î·t }Tt=1 . Since Î·t 6= 0 and gÌ‡t 6= 0 for all t, D is invertible and we
have Î»min (Dâˆ’2 ) = minTt=1 1/(gÌ‡t2 Î·t ) â‰¥ 1/(L2 Î·1 ). For simplicity, we assume Î·1 = Ïƒ, leaving the case Î·1 = 1/1 = 1 as a

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

special case. We derive
T

deff (Î±) = Tr(KT (KT + Î±I)âˆ’1 )
= Tr(DKT D(DKT D + Î±DDâˆ’2 D)âˆ’1 )
= Tr(DKT DDâˆ’1 (KT + Î±Dâˆ’2 )âˆ’1 Dâˆ’1 )
= Tr(KT I(KT + Î±Dâˆ’2 )âˆ’1 Dâˆ’1 D)
= Tr(KT (KT + Î±Dâˆ’2 )âˆ’1 )
â‰¤ Tr(KT (KT + Î±Î»min (Dâˆ’2 )I)âˆ’1 )




Î± âˆ’1
I
= dTeff Î±/(ÏƒL2 ) .
â‰¤ Tr KT KT +
2
ÏƒL
Similarly,
log(kKT k/Î± + 1) â‰¤ log(Tr(KT )/Î± + 1) â‰¤ log(ÏƒL2 Tr(Kt )/Î± + 1) â‰¤ log(ÏƒL2 T /Î± + 1) â‰¤ log(2ÏƒL2 T /Î±),
since Tr(Kt ) =

PT

t=1

kt,t =

PT

t=1

Ï†Tt Ï†t â‰¤

PT

t=1

1 = T.

C. Proofs for Section 4
Proof of Thm. 2. We derive the proof for a generic K with its induced Ï†t = Ï•(xt ) and Kt . Then, S KETCHED -KONS
(Alg. 3) applies this proof to the rescaled Ï†t and Kt .
Our goal is to prove that Alg. 2 generates accurate and small dictionaries at all time steps t âˆˆ [T ]. More formally, a
dictionary Is is Îµ-accurate w.r.t. Dt when
1/2

1/2

k(Î¦t Î¦Tt + Î±I)âˆ’1/2 Î¦t (I âˆ’ Ss STs )Î¦Tt (Î¦t Î¦Tt + Î±I)âˆ’1/2 k = k(Kt + Î±I)âˆ’1/2 Kt (I âˆ’ Ss STs )Kt (Kt + Î±I)âˆ’1/2 k â‰¤ Îµ,
where we used Prop. 4 to move from feature to primal space.
We also introduce the projection operators,
vt,i :=((Kt + Î±I)âˆ’1 Kt )1/2 et,i
1/2

1/2

Pt :=(Kt + Î±I)âˆ’1/2 Kt Kt (Kt + Î±I)âˆ’1/2 =

t
X

T
vt,s vt,s
= Vt VtT

s=1

e t :=(Kt + Î±I)âˆ’1/2 K1/2 St ST K1/2 (Kt + Î±I)âˆ’1/2 =
P
t
t
t

t
X
zt
T
= Vt St STt VtT ,
vt,s vt,s
p
e
t
s=1

where the zt variables are the {0, 1} random variables sampled by Alg. 2. Note that with this notation we have
T
kvt,i vt,i
k = k((Kt + Î±I)âˆ’1 Kt )1/2 et,i eTt,i (Kt (Kt + Î±I)âˆ’1 )1/2 k

= eTt,i ((Kt + Î±I)âˆ’1 Kt )1/2 (Kt (Kt + Î±I)âˆ’1 )1/2 et,i = eTt,i (Kt + Î±I)âˆ’1 Kt et,i = Ï„t,i .
We can now formalize the event â€œsome of the guarantees of Alg. 2 do not holdâ€ and bound the probability of this event. In
particular, let
e t âˆ’ Pt =
Yt := P

t 
X
zt
s=1

pet


T
âˆ’ 1 vt,s vt,s
.

We want to show


Xt
t
P âˆƒt âˆˆ [T ] : kYt k â‰¥ Îµ âˆª
zt â‰¥ 3Î²donl (Î±) â‰¤ Î´,
| {z }
}
| s=1 {z
At

Bt

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

where event At refers to the case when the intermediate dictionary It fails to accurately approximate Kt at some step
t âˆˆ [T ] and event Bt considers the case when the memory requirement is not met (i.e., too many columns are kept in a
dictionary It at a certain time t âˆˆ [T ]).
Step 1: Splitting the problem. We can conveniently decompose the previous joint (negative) event into two separate
conditions as
(T
(T
(T
)!
)!
) (T
)!
[

T
[
[
[
[

P
At âˆª B t
=P
At
+P
Bt
âˆ’P
At âˆ©
Bt
t=1

t=1

(
=P

t=1

ï£«(

)!

T
[

At

+ Pï£­

t=1

(
=P

(
âˆ©

Bt

t=1

)!

T
[

T
[

)

At

+P

t=1

T
[

T
[

t=1

t=1

){ ï£¶

(

At

ï£¸=P

t=1

(

(
Bt âˆ©

T
[

(

)!
At

+P

t=1

T
[

(

)
Bt

T
\

âˆ©

t=1

)!
A{t

t=1

))!

T
\

A{t0

.

t0 =1

t=1

Applying this reformulation and a union bound, we obtain


Xt
t
P âˆƒt âˆˆ [T ] : kYt k â‰¥ Îµ âˆª
zt â‰¥ 3Î²donl (Î±)
s=1

â‰¤

T
X

P (kYt k â‰¥ Îµ) +

T
X

P

t=1

t=1

t
X

!
zs â‰¥ 3Î²dtonl (Î±) âˆ© {âˆ€ t0 âˆˆ {1, . . . , t} : kYt k â‰¤ Îµ} .

s=1

To conclude the proof, we show in Step 2 and 3, that each of the failure events happens with probability less than

Î´
2T

.

Step 2: Bounding the accuracy. We first point out that dealing with Yt is not trivial since the process {Yt }Tt=1 is
composed by matrices of different size, that cannot be directly compared. Denote with Sts the matrix constructed by (1)
taking Ss and adding t âˆ’ s rows of zeros to its bottom to extend it, and (2) adding t âˆ’ s indicator columns et,i for all i > s.
We begin by reformulating Yt as a random process Y0t , Y1t , . . . , Ytt with differences Xts defined as
Xts =




zs
T
,
âˆ’ 1 vt,s vt,s
pes

Ykt =

k
X
s=1

Xts =

k 
X
zs
s=1

pes


T
= Vt (Stk (Stk )T âˆ’ I)VtT .
âˆ’ 1 vt,s vt,s

We introduce the freezing probabilities
t
t
ps = pes Â· I{kYsâˆ’1
k < Îµ} + 1 Â· I{kYsâˆ’1
k â‰¥ Îµ}
t

and the associated process Ys based on the coin flips z s performed using ps instead of the original pes as in Alg. 2. In
other words, this process is such that if at any time s âˆ’ 1 the accuracy condition is not met, then for all steps from s on the
t
algorithm stops updating the dictionary. We also define Yt = Yt . Then we have

P (kYt k â‰¥ Îµ) â‰¤ P kYt k â‰¥ Îµ ,
so we can simply bound the latter to bound the former. To show the usefulness of the freezing process, consider the step s
t
where the process froze, or more formally define s as the step where kYst k < Îµ and kYs+1
k â‰¥ Îµ. Then for all s â‰¤ s, we
t
can combine Prop. 3, the definition of Vt , and the guarantee that kYs k < Îµ to obtain
Î¦s Ss STs Î¦s  Î¦t Sts (Sts )T Î¦t  Î¦t Î¦Tt + Îµ(Î¦t Î¦Tt + Î³I),
where in the first inequality we used the fact that Sts is simply obtained by bordering Ss . Applying the definition of pes ,
when pes < 1 we have
ps = pes = Î²e
Ï„s,s = Î²(1 + Îµ)Ï†Ts (Î¦s Ss STs Î¦Ts + Î³I)âˆ’1 Ï†s
â‰¥ Î²(1 + Îµ)Ï†Ts (Î¦s Î¦Ts + Îµ(Î¦t Î¦Tt + Î³I) + Î³I)âˆ’1 Ï†s
1
= Î²(1 + Îµ)
Ï†T (Î¦t Î¦Tt + Î³I)âˆ’1 Ï†s , = Î²Ï„t,s ,
1+Îµ s

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

which shows that our estimates of RLS are upper bounds on the true values.
t

t

From this point onwards we focus on a specific t, and omit the index from Ys , Xs and vt,s . We can now verify that Ys is
a martingale, by showing that Xs is zero mean. Denote with Fk = {Xs }ks=1 the filtration of the process. When ps = 1,
either because Î² pes â‰¥ 1 or because the process is frozen, we have Xs = 0 and the condition is satisfied. Otherwise, we
have


 








E [z s | Fsâˆ’1 ]
zs
ps
T 
T

E Xs Fsâˆ’1 = E
âˆ’ 1 vs vs  Fsâˆ’1 =
âˆ’ 1 vs vs =
âˆ’ 1 vs vsT = 0,
ps
ps
ps
where we use the fact that ps is fixed conditioned on Fsâˆ’1 and its the (conditional) expectation of z s . Since Yt is a
martingale, we can use Prop. 5. First, we find R. Again, when ps = 1 we have Xs = 0 and R â‰¥ 0. Otherwise,

 



 zs
  zs
Ï„t,s
1
1
T
T



 p âˆ’ 1 vs vs  â‰¤  p âˆ’ 1  kvs vs k â‰¤ p Ï„t,s â‰¤ Î²Ï„t,s = Î² := R.
s
s
s
For the total variation, we expand
Wt :=

t
X
s=1

h 2
E Xs

"
#
2 
t

i X
zs


E
âˆ’ 1  Fsâˆ’1 vs vsT vs vsT
 Fsâˆ’1 =

p
s
s=1






t   2
X
z s 
z s 
=
E 2  Fsâˆ’1 âˆ’ E 2  Fsâˆ’1 + 1 vs vsT vs vsT
ps
ps
s=1








t
t  
X
X
z s 
z s 
T
T
E 2  Fsâˆ’1 âˆ’ 1 vs vs vs vs =
E 2  Fsâˆ’1 âˆ’ 1 vs vsT vs vsT
=
p
ps
s
s=1
s=1



t 
t  T
t 
X
X
X
1
vs vs
Ï„t,s
âˆ’ 1 vs vsT vs vsT =
âˆ’ vsT vs vs vsT =
âˆ’ Ï„t,s vs vsT ,
=
ps
ps
ps
s=1
s=1
s=1

where we used the fact that z 2s = z s and E[z s |Fsâˆ’1 ] = ps . We can now bound this quantity as

 
 
 


t
t 
t
t
 X
 X

h 2
i X

 X
Ï„t,s
Ï„t,s
Ï„t,s

 


T
T
T
Wt  = 
E Xs  Fsâˆ’1  = 
âˆ’ Ï„t,s vs vs  â‰¤ 
vs vs  â‰¤ 
vs vs 


 
 
 

ps
ps
Î²Ï„t,s
s=1
s=1
s=1
s=1
 t


1
1
1
1
X

= 
vs vsT  = kVt VtT k = kPt k â‰¤ := Ïƒ 2 .
 Î²
Î² s=1
Î²
Î²
Therefore, if we let Ïƒ 2 = 1/Î² and R = 1/Î², we have



P (kYt k â‰¥ Îµ) â‰¤ P kYt k â‰¥ Îµ = P kYt k â‰¥ Îµ âˆ© kYt k â‰¤ Ïƒ 2 + P kYt k â‰¥ Îµ âˆ© kYt k â‰¥ Ïƒ 2
(
)
 2 


Îµ2
1
Îµ Î²
2
2
â‰¤ P kYt k â‰¥ Îµ âˆ© kYt k â‰¤ Ïƒ + P kYt k â‰¥ Ïƒ â‰¤ 2t exp âˆ’ 1
+ 0 â‰¤ 2t exp âˆ’
Â·
2 Î² (1 + Îµ/3)
3
Step 3: Bounding the space. We want to show that
P

t
X

!
zs â‰¥

3Î²dtonl (Î±)

0

âˆ© {âˆ€ t âˆˆ {1, . . . , t} : kYt k â‰¤ Îµ} .

s=1

Assume, without loss of generality, that for all s âˆˆ [t] we have Î²Ï„s,s â‰¤ 1, and introduce the independent Bernoulli random
variables zbs âˆ¼ B(Î²Ï„s,s ). Thanks to the intersection with the event {âˆ€ t0 âˆˆ {1, . . . , t} : kYt k â‰¤ Îµ}, we know that all
dictionaries Is are Îµ-accurate, and therefore for all s we have pes â‰¤ Î²e
Ï„s,s â‰¤ Î²Ï„s,s . Thus zbs stochastically dominates zs and
we have
!
!
t
t
X
X
t
0
t
P
zs â‰¥ 3Î²donl (Î±) âˆ© {âˆ€ t âˆˆ {1, . . . , t} : kYt k â‰¤ Îµ} â‰¤ P
zbs â‰¥ 3Î²donl (Î±) .
s=1

s=1

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

Applying Prop. 6 to

P

t
X

Pt

bs
s=1 z

and knowing that

Pt

s=1

Pt

pt =

s=1

Î²Ï„s,s = Î²dtonl (Î±), we have

!
zbs â‰¥

3Î²dtonl (Î±)

â‰¤ exp{âˆ’3Î²dtonl (Î±)(3Î²dtonl (Î±) âˆ’ (log(3Î²dtonl (Î±)) + 1))} â‰¤ exp{âˆ’2Î²dtonl (Î±)}.

s=1

Assuming dtonl (Î±) â‰¥ 1, we have that exp{âˆ’2Î²dtonl (Î±)} â‰¤ exp{âˆ’2Î²} â‰¤ exp{âˆ’ log((T /Î´)2 )} â‰¤ Î´ 2 /T 2 â‰¤ Î´/(2T ) as
long as 2Î´ â‰¤ 2 â‰¤ T .

D. Proofs for Section 5
Proof of Lemma 4. Through this proof, we make use of the linear algebra identity from Prop. 2. We begin with the refore t ]i . In particular, the only term that we need to reformulate is
mulation of ebi = [b
T

âˆ’1
e âˆ’1 Ï† = Ï† (Î¦tâˆ’1 Rtâˆ’1 RT Î¦
Ï†t A
Ï†t
t
tâˆ’1 tâˆ’1 + Î±I)
tâˆ’1 t
1
T
T
= Ï†t (I âˆ’ Î¦tâˆ’1 Rtâˆ’1 (RTtâˆ’1 Î¦tâˆ’1 Î¦tâˆ’1 Rtâˆ’1 + Î±I)âˆ’1 RTtâˆ’1 Î¦tâˆ’1 )Ï†t
Î±
1 T
T
T
T
= (Ï†t Ï†t âˆ’ Ï†t Î¦tâˆ’1 Rtâˆ’1 (RTtâˆ’1 Î¦tâˆ’1 Î¦tâˆ’1 Rtâˆ’1 + Î±I)âˆ’1 RTtâˆ’1 Î¦tâˆ’1 Ï†t )
Î±
1
T
= (kt,t âˆ’ k[tâˆ’1],t Rtâˆ’1 (RTtâˆ’1 Ktâˆ’1 Rtâˆ’1 + Î±I)âˆ’1 RTtâˆ’1 k[tâˆ’1],t ).
Î±

For yÌ†t , we have
e tâˆ’1
e âˆ’1 Î¦tâˆ’1 b
e t = Ï†Tt A
yÌ†t = Ï†Tt u
tâˆ’1
T
e tâˆ’1
= Ï†Tt (Î¦tâˆ’1 Rtâˆ’1 RTtâˆ’1 Î¦tâˆ’1 + Î±I)âˆ’1 Î¦tâˆ’1 b


1
T
T
e tâˆ’1
= Ï†Tt I âˆ’ Î¦tâˆ’1 Rtâˆ’1 (RTtâˆ’1 Î¦tâˆ’1 Î¦tâˆ’1 Rtâˆ’1 + Î±I)âˆ’1 RTtâˆ’1 Î¦tâˆ’1 Î¦tâˆ’1 b
Î±


1
e tâˆ’1 âˆ’ Rtâˆ’1 (RT Ktâˆ’1 Rtâˆ’1 + Î±I)âˆ’1 RT Ktâˆ’1 b
e tâˆ’1
= Ï†Tt Î¦tâˆ’1 Dtâˆ’1 b
tâˆ’1
tâˆ’1
Î±


1
e tâˆ’1 âˆ’ Rtâˆ’1 (RT Ktâˆ’1 Rtâˆ’1 + Î±I)âˆ’1 RT Ktâˆ’1 b
e tâˆ’1 .
= kT[tâˆ’1],t Dtâˆ’1 b
tâˆ’1
tâˆ’1
Î±

Proof of Theorem 3. Since the only thing that changed is the formulation of the At matrix, the bound from Prop. 1 still
eT of Alg. 3 is bounded as
applies. In particular, we have that the regret R
e
R(w)
â‰¤Î±kwk2A0 +

XT
t=1

e âˆ’1 gt +
gtT A
t

XT
t=1

et âˆ’ A
e tâˆ’1 âˆ’ Ïƒt gt gT )(wt âˆ’ w).
(wt âˆ’ w)T (A
t

From Thm. 2, we have that KORS succeeds with high probability. In particular, using the guarantees of the Îµ-accuracy (1),
we can bound for the case Î·t = Ïƒ as
e âˆ’1 gt =
gtT A
t
=
â‰¤
â‰¤
=

Î·t T
1 T
T
T
gt (Î¦t Rt RTt Î¦t + Î±I)âˆ’1 gt = Ï†t (Î¦t Rt RTt Î¦t + Î±I)âˆ’1 Ï†t
Î·t
Î·t

âˆ’1
1 pemin T
1 1
1
T
T
T
Ï†t (Î¦t Rt RTt Î¦t + Î±I)âˆ’1 Ï†t =
Ï†t
Î¦t Rt RTt Î¦t + Î±I
Ï†t
Î·t pemin
Î·t pemin
pemin

âˆ’1
1 1
T
T
Ï†t Î¦t St STt Î¦t + Î±I
Ï†t
Î·t pemin
1
T
T
Ï† ((1 âˆ’ Îµ)Î¦t Î¦t âˆ’ ÎµÎ±I + Î±I)âˆ’1 Ï†t
pemin Î·t t
1
Ï„ t,t
T
T
,
Ï†t (Î¦t Î¦t + Î±I)âˆ’1 Ï†t =
(1 âˆ’ Îµ)Ïƒe
pmin
(1 âˆ’ Îµ)Ïƒe
pmin

Second-Order Kernel Online Convex Optimization with Adaptive Sketching

p
p
where in the first inequality we used the fact that the weight matrix St contains weights such that 1/ pemin â‰¥ 1/ pet , in
the second inequality we used the Îµ-accuracy, and finally, we used Î·t = Ïƒ and the definition of Ï„ t,t . Therefore,
RG =

XT
t=1

e âˆ’1 gt â‰¤
gtT A
t

XT
1
donl (Î±)
Â·
Ï„ t,t â‰¤
t=1
(1 âˆ’ Îµ)Ïƒe
pmin
(1 âˆ’ Îµ)Ïƒ max{Î²Ï„ min , Î³}

To bound RD , we have
XT
t=1

et âˆ’ A
e tâˆ’1 âˆ’ Ïƒt gt gT )(wt âˆ’ w) =
(wt âˆ’ w)T (A
t
â‰¤

XT
t=1
XT
t=1

(wt âˆ’ w)T (Î·t zt gt gtT âˆ’ Ïƒt gt gtT ) (wt âˆ’ w)
(Ïƒ âˆ’ Ïƒt )(gtT (wt âˆ’ w))2 â‰¤ 0.

