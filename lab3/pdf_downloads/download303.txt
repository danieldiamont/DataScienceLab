How to Escape Saddle Points Efficiently

A. Detailed Proof of Main Theorem
In this section, we give detailed proof for the main theorem. We will first state two key lemmas that show how the algorithm
can make progress when the gradient is large or near a saddle point, and show how the main theorem follows from the two
lemmas. Then we will focus on the novel technique in this paper: how to analyze gradient descent near saddle point.
A.1. General Framework
In order to prove the main theorem, we need to show that the algorithm will not be stuck at any point that either has a large
gradient or is a saddle point. This idea is similar to previous works (e.g.(Ge et al., 2015)). We first state a standard Lemma
that shows if the current gradient is large, then we make progress in function value.
Lemma 12. Assume f (·) satisfies A1, then for gradient descent with stepsize η < 1` , we have:
η
f (xt+1 ) ≤ f (xt ) − k∇f (xt )k2
2
Proof. By Assumption A1 and its property, we have:
`
f (xt+1 ) ≤f (xt ) + ∇f (xt )> (xt+1 − xt ) + kxt+1 − xt k2
2
η
η2 `
k∇f (xt )k2 ≤ f (xt ) − k∇f (xt )k2
=f (xt ) − ηk∇f (xt )k2 +
2
2

The next lemma says that if we are “close to a saddle points”, i.e., we are at a point where the gradient is small, but the
Hessian has a reasonably large negative eigenvalue. This is the main difficulty in the analysis. We show a perturbation
followed by small (polylog) number of standard gradient descent steps can also make the function value decrease with high
probability.
Lemma 13. There exist absolute constant cmax , for f (·) satisfies A1, and any c ≤ cmax , and χ ≥ 1.
η, r, gthres , fthres , tthres calculated same way as in Algorithm 2. Then, if x̃t satisfies:
√
k∇f (x̃t )k ≤ gthres
and
λmin (∇2 f (x̃t )) ≤ − ρ

Let

Let xt = x̃t + ξt where ξt comes from the uniform distribution over B0 (r), and let xt+i be the iterates of gradient descent
from xt with stepsize η, then with at least probability 1 − √d`ρ e−χ , we have:
f (xt+tthres ) − f (x̃t ) ≤ −fthres
The proof of this lemma is deferred to Section A.2. Using this Lemma, we can then prove the main Theorem.
2

Theorem 3. There exist absolute constant cmax such that: if f (·) satisfies A1, then for any δ > 0,  ≤ `ρ , ∆f ≥ f (x0 )−f ? ,
and constant c ≤ cmax , with probability 1 − δ, the output of PGD(x0 , `, ρ, , c, δ, ∆f ) will be −second order stationary
point, and terminate in iterations:



`(f (x0 ) − f ? )
d`∆f
4
O
log
2
2 δ
Proof. Denote c̃max to be the absolute constant allowed in Theorem 13. In this theorem, we let cmax = min{c̃max , 1/2},
and choose any constant c ≤ cmax .
In this proof, we will actually achieve some point satisfying following condition:
√
c
√
k∇f (x)k ≤ gthres = 2 · ,
λmin (∇2 f (x)) ≥ − ρ
χ
√

Since c ≤ 1, χ ≥ 1, we have

c
χ2

≤ 1, which implies any x satisfy Eq.(3) is also a -second-order stationary point.

Starting from x0 , we know if x0 does not satisfy Eq.(3), there are only two possibilities:

(3)

How to Escape Saddle Points Efficiently

1. k∇f (x0 )k > gthres : In this case, Algorithm 2 will not add perturbation. By Lemma 12:
η 2
c2 2
f (x1 ) − f (x0 ) ≤ − · gthres
=− 4 ·
2
2χ
`
2. k∇f (x0 )k ≤ gthres : In this case, Algorithm 2 will add a perturbation of radius r, and will perform gradient descent
(without perturbations) for the next tthres steps. Algorithm 2 will then check termination condition. If the condition is
not met, we must have:
s
3
c
f (xtthres ) − f (x0 ) ≤ −fthres = − 3 ·
χ
ρ
This means on average every step decreases the function value by
c3 2
f (xtthres ) − f (x0 )
≤− 4 ·
tthres
χ
`
In case 1, we can repeat this argument for t = 1 and in case 2, we can repeat this argument for t = tthres . Hence, we can
3
2
conclude as long as algorithm 2 has not terminated yet, on average, every step decrease function value by at least χc 4 · ` .
However, we clearly can not decrease function value by more than f (x0 ) − f ? , where f ? is the function value of global
minima. This means algorithm 2 must terminate within the following number of iterations:
f (x0 ) − f ?
χ4 `(f (x0 ) − f ? )
=
·
=O
2
3

c
c3
2
χ4 · `



`(f (x0 ) − f ? )
log4
2



d`∆f
2 δ



Finally, we would like to ensure when Algorithm 2 terminates, the point it finds is actually an -second-order stationary
point. The algorithm can only terminate when the gradient is small, and the function value does not decrease after a
perturbation and tthres iterations. We shall show every time when we add perturbation to iterate x̃t , if λmin (∇2 f (x̃t )) <
√
− ρ, then we will have f (xt+tthres ) − f (x̃t ) ≤ −fthres . Thus, whenever the current point is not an -second-order
stationary point, the algorithm cannot terminate.
According to Algorithm 2, we immediately know k∇f (x̃t )k ≤ gthres (otherwise we will not add perturbation at time t).
By Lemma 13, we know this event happens with probability at least 1 − √d`ρ e−χ each time. On the other hand, during one
entire run of Algorithm 2, the number of times we add perturbations is at most:
1
tthres

·

χ3
χ4 `(f (x0 ) − f ? )
·
=
3
2
c

c

√

ρ(f (x0 ) − f ? )
2

By union bound, for all these perturbations, with high probability Lemma 13 is satisfied. As a result Algorithm 2 works
correctly. The probability of that is at least
d`
χ3
1 − √ e−χ ·
ρ
c
Recall our choice of χ = 3 max{log(

d`∆f
c2 δ

√

ρ(f (x0 ) − f ? )
χ3 e−χ d`(f (x0 ) − f ? )
=1−
·
2

c
2

), 4}. Since χ ≥ 12, we have χ3 e−χ ≤ e−χ/3 , this gives:

d`(f (x0 ) − f ? )
χ3 e−χ d`(f (x0 ) − f ? )
·
≤ e−χ/3
≤δ
2
c

c2
which finishes the proof.

How to Escape Saddle Points Efficiently

A.2. Main Lemma: Escaping from Saddle Points Quickly
Now we prove the main lemma, which shows near a saddle point, a small perturbation followed by a small number of
gradient descent steps will decrease the function value with high probability. This is the main step where we need new
analysis, as the analysis previous works (e.g.(Ge et al., 2015)) do not work when the step size and perturbation do not
depend polynomially in dimension d.
Intuitively, after adding a perturbation, the current point of the algorithm is a uniform distribution over a d-dimensional ball
centered at x̃, which we call perturbation ball. After a small number of gradient steps, some points in this ball (which we
call the escaping region) will significantly decrease the function; other points (which we call the stuck region) does not
see a significant decrease in function value. We hope to show that the escaping region constitutes at least 1 − δ fraction of
the volume of the perturbation ball.
However, we do not know the exact form of the function near the saddle point, so the escaping region does not have a
clean analytic description. Explicitly computing its volume can be very difficult. Our proof rely on a crucial observation:
although we do not know the shape of the stuck region, we know the “width” of it must be small, therefore it cannot have
a large volume. We will formalize this intuition later in Lemma 15.
The proof of the main lemma requires carefully balancing between different quantities including function value, gradient,
parameter space and number of iterations. For clarify, we define following scalar quantities, which serve as the “units” for
function value, gradient, parameter space, and time (iterations). We will use these notations throughout the proof.
Let the condition number be the ratio of the smoothness parameter (largest eigenvalue of Hessian) and the negative eigenvalue γ: κ = `/γ ≥ 1, we define the following units:
F := η`

dκ
γ3
· log−3 ( ),
ρ2
δ

S :=

η`

p

p γ2
dκ
η` · log−2 ( )
ρ
δ
dκ
log( δ )
T :=
ηγ
G :=

γ
dκ
· log−1 ( ),
ρ
δ

Intuitively, if we plug in our choice of step size η` = O(1) (which we will prove later) and hide the logarithmic de2
3
pendences, we have F = Õ( γρ2 ), G = Õ( γρ ), S = Õ( γρ ), which is the only way to correctly discribe the units of
function value, gradient, parameter space by just γ and ρ. Moreover, these units are closely related, in particular, we know
q
F ·log( dκ
δ )
γ

=

G ·log( dκ
δ )
γ

= S.

For simplicity of later proofs, we first restate Lemma 13 into a slightly more general form as follows. Lemma 13 is directly
implied following lemma.
Lemma 14 (Lemma 13 restated). There exists universal constant cmax , for f (·) satisfies A1, for any δ ∈ (0, dκ
e ], suppose
we start with point x̃ satisfying following conditions:
k∇f (x̃)k ≤ G

and

λmin (∇2 f (x̃)) ≤ −γ

Let x0 = x̃ + ξ where ξ come from the uniform distribution over ball with radius S /(κ · log( dκ
δ )), and let xt be the iterates
of gradient descent from x0 . Then, when stepsize η ≤ cmax /`, with at least probability 1 − δ, we have following for any
1
T ≥ cmax
T:
f (xT ) − f (x̃) ≤ −F
Lemma 14 is almost the same as Lemma 13. It is easy to verify that by substituting η = c` , γ =
Lemma 14, we immediately obtain Lemma 13.

√

ρ and δ =

√d` e−χ
ρ

into

Now we will formalize the intuition that the “width” of stuck region is small.
Lemma 15. There exists a universal constant cmax , for any δ ∈ (0, dκ
e ], let f (·), x̃ satisfies the conditions in Lemma 14,
and without loss of generality let e1 be the minimum eigenvector of ∇2 f (x̃). Consider two gradient descent sequences
{ut }, {wt } with initial points u0 , w0 satisfying: (denote radius r = S /(κ · log( dκ
δ )))
√
ku0 − x̃k ≤ r, w0 = u0 + µ · r · e1 , µ ∈ [δ/(2 d), 1]
Then, for any stepsize η ≤ cmax /`, and any T ≥

1
cmax T

, we have:

min{f (uT ) − f (u0 ), f (wT ) − f (w0 )} ≤ −2.5F

How to Escape Saddle Points Efficiently

Intuitively, lemma 15 claims for any two points u0 , w0 inside the perturbation ball,
√ if u0 − w0 lies in the direction of
minimum eigenvector of ∇2 f (x̃), and ku0 − w0 k is greater than threshold δr/(2 d), then at least one of two sequences
{ut }, {wt } will “efficiently escape saddle point”. In other words, if u0 is a point √
in the stuck region, consider any point
w0 that is on a straight line along direction of e1 . As long as w0 is slightly far (δr/ d) from u0 , it must be in the escaping
region. This is what we mean by the “width” of the stuck region being small.Now we prove the main Lemma using this
observation:
Proof of Lemma 14. By adding perturbation, in worst case we increase function value by:
S
S
1
`
3
f (x0 ) − f (x̃) ≤ ∇f (x̃)> ξ + kξk2 ≤ G (
) + `(
)2 ≤ F
dκ
dκ
2
2 κ · log( δ )
2
κ · log( δ )
On the other hand, let radius r =

S
.
κ·log( dκ
δ )

We know x0 come froms uniform distribution over Bx̃ (r). Let Xstuck ⊂ Bx̃ (r)

denote the set of bad starting points so that if x0 ∈ Xstuck , then f (xT ) − f (x0 ) > −2.5F (thus stuck at a saddle point);
otherwise if x0 ∈ Bx̃ (r) − Xstuck , we have f (xT ) − f (x0 ) ≤ −2.5F .
δ
, 1].
By applying Lemma 15, we know for any x0 ∈ Xstuck , it is guaranteed that (x0 ± µre1 ) 6∈ Xstuck where µ ∈ [ 2√
d

Denote IXstuck (·) be the indicator function of being inside set Xstuck ; and vector x = (x(1) , x(−1) ), where x(1) is the
component along e1 direction, and x(−1) is the remaining d − 1 dimensional vector. Recall B(d) (r) be d-dimensional ball
with radius r; By calculus, this gives an upper bound on the volumn of Xstuck :
Z
Vol(Xstuck ) =
dx · IXstuck (x)
(d)

Bx̃ (r)

√

Z

(−1)

=

Z

x̃(1) +

dx
(d−1)
Bx̃
(r)

Z
≤

√

x̃(1) −

dx

(−1)

(d−1)
Bx̃
(r)



r 2 −kx̃(−1) −x(−1) k2

dx(1) · IXstuck (x)
r 2 −kx̃(−1) −x(−1) k2

δ
· 2· √ r
2 d



(d−1)

= Vol(B0

δr
(r)) × √
d

Then, we immediately have the ratio:
δr
√
d

(d−1)

× Vol(B0

r
δ Γ( d2 + 1)
δ
d 1
≤
=√
≤√ ·
+ ≤δ
d
1
(d)
(d)
2 2
πd Γ( 2 + 2 )
πd
Vol(Bx̃ (r))
Vol(B0 (r))
q
Γ(x+1)
The second last inequality is by the property of Gamma function that Γ(x+1/2)
< x + 12 as long as x ≥ 0. Therefore,
with at least probability 1 − δ, x0 6∈ Xstuck . In this case, we have:
Vol(Xstuck )

(r))

f (xT ) − f (x̃) =f (xT ) − f (x0 ) + f (x0 ) − f (0)
≤ − 2.5F + 1.5F ≤ −F
which finishes the proof.
A.3. Bounding the Width of Stuck Region
In order to prove Lemma 15, we do it in two steps:
1. We first show if gradient descent from u0 does not decrease function value, then all the iterates must lie within a small
ball around u0 (Lemma 16).
2. If gradient descent starting from a point u0 stuck in a small ball around a saddle point, then gradient descent from w0
(moving u0 along e1 direction for at least a certain distance), will decreases the function value (Lemma 17).
Recall we assumed without loss of generality e1 is the minimum eigenvector of ∇2 f (x̃). In this context, we denote
H := ∇2 f (x̃), and for simplicity of calculation, we consider following quadratic approximation:
1
f˜y (x) := f (y) + ∇f (y)> (x − y) + (x − y)> H(x − y)
2

(4)

How to Escape Saddle Points Efficiently

Now we are ready to state two lemmas formally:
Lemma 16. For any constant ĉ ≥ 3, there exists absolute constant cmax : for any δ ∈ (0, dκ
e ], let f (·), x̃ satisfies the
)),
define:
condition in Lemma 14, for any initial point u0 with ku0 − x̃k ≤ 2S /(κ · log( dκ
δ
n
n
o
o
T = min inf t|f˜u0 (ut ) − f (u0 ) ≤ −3F , ĉT
t

then, for any η ≤ cmax /`, we have for all t < T that kut − x̃k ≤ 100(S · ĉ).
Lemma 17. There exists absolute constant cmax , ĉ such that: for any δ ∈ (0, dκ
e ], let f (·), x̃ satisfies the condition in
Lemma 14, and sequences {ut }, {wt } satisfy the conditions in Lemma 15, define:
n
o
o
n
T = min inf t|f˜w0 (wt ) − f (w0 ) ≤ −3F , ĉT
t

then, for any η ≤ cmax /`, if kut − x̃k ≤ 100(S · ĉ) for all t < T , we will have T < ĉT .
Note the conclusion T < ĉT in Lemma 17 equivalently means:
n
o
inf t|f˜w0 (wt ) − f (w0 ) ≤ −3F < ĉT
t

That is, for some T < ĉT , {wt } sequence “escape the saddle point” in the sense of sufficient function value decrease
f˜w0 (wt ) − f (w0 ) ≤ −3F . Now, we are ready to prove Lemma 15.
(2)

Proof of Lemma 15. W.L.O.G, let x̃ = 0 be the origin. Let (cmax , ĉ) be the absolute constant so that Lemma 17 holds,
(1)
also let cmax be the absolute constant to make Lemma 16 holds based on our current choice of ĉ. We choose cmax ≤
(2)
(1)
min{cmax , cmax } so that our step size η ≤ cmax /` is small enough which make both Lemma 16 and Lemma 17 hold. Let
? :=
T
ĉT and define:
n
o
T 0 = inf t|f˜u0 (ut ) − f (u0 ) ≤ −3F
t

Let’s consider following two cases:
Case T 0 ≤ T ? : In this case, by Lemma 16, we know kuT 0 −1 k ≤ O(S ), and therefore
kuT 0 k ≤kuT 0 −1 k + ηk∇f (uT 0 −1 )k ≤ kuT 0 −1 k + ηk∇f (x̃)k + η`kuT 0 −1 k ≤ O(S )
By choosing cmax small enough and η ≤ cmax /`, this gives:
1
ρ
f (uT 0 ) − f (u0 ) ≤∇f (u0 )> (uT 0 − u0 ) + (uT 0 − u0 )> ∇2 f (u0 )(uT 0 − u0 ) + kuT 0 − u0 k3
2
6
ρ
ρ
2
3
˜
≤fu0 (ut ) − f (u0 ) + ku0 − x̃kkuT 0 − u0 k + kuT 0 − u0 k
2
6
p
≤ − 3F + O(ρS 3 ) = −3F + O( η` · F ) ≤ −2.5F
By choose cmax ≤ min{1, 1ĉ }. We know η < 1` , by Lemma 12, we know gradient descent always decrease function value.
1
Therefore, for any T ≥ cmax
T ≥ ĉT = T ? ≥ T 0 , we have:
f (uT ) − f (u0 ) ≤ f (uT ? ) − f (u0 ) ≤ f (uT 0 ) − f (u0 ) ≤ −2.5F
Case T 0 > T ? : In this case, by Lemma 16, we know kut k ≤ O(S ) for all t ≤ T ? . Define
n
o
T 00 = inf t|f˜w0 (wt ) − f (w0 ) ≤ −2F
t

By Lemma 17, we immediately have T 00 ≤ T ? . Apply same argument as in first case, we have for all T ≥
f (wT ) − f (w0 ) ≤ f (wT ? ) − f (w0 ) ≤ −2.5F .
Next we finish the proof by proving Lemma 16 and Lemma 17.

1
cmax T

that

How to Escape Saddle Points Efficiently

A.3.1. P ROOF OF L EMMA 16
In Lemma 16, we hope to show if the function value did not decrease, then all the iterations must be constrained in a
small ball. We do that by analyzing the dynamics of the iterations, and we decompose the d-dimensional space into
two subspaces: a subspace S which is the span of significantly negative eigenvectors of the Hessian and its orthogonal
compliment.
Recall notation H := ∇2 f (x̃) and quadratic approximation f˜y (x) as defined in Eq.(4). Since δ ∈ (0, dκ
e ], we always have
)
≥
1.
W.L.O.G,
set
u
=
0
to
be
the
origin,
by
gradient
descent
update
function,
we
have:
log( dκ
0
δ
ut+1 =ut − η∇f (ut )
Z
=ut − η∇f (0) − η

1


∇2 f (θut )dθ ut

0

=ut − η∇f (0) − η(H + ∆t )ut
=(I − ηH − η∆t )ut − η∇f (0)

(5)

R1
Here, ∆t := 0 ∇2 f (θut )dθ − H. By Hessian Lipschitz, we have k∆t k ≤ ρ(kut k + kx̃k), and by smoothness of the
gradient, we have k∇f (0)k ≤ k∇f (x̃)k + `kx̃k ≤ G + ` · 2S /(κ · log( dκ
δ )) ≤ 3G .
We will now compute the projections of ut in different eigenspaces of H. Let S be the subspace spanned by all eigenvectors
of H whose eigenvalue is less than − ĉ log(γ dκ ) . S c denotes the subspace of remaining eigenvectors. Let αt and βt denote
δ
the projections of ut onto S and S c respectively i.e., αt = PS ut , and βt = PS c ut . We can decompose the update
equations Eq.(5) into:
αt+1 =(I − ηH)αt − ηPS ∆t ut − ηPS ∇f (0)

(6)

βt+1 =(I − ηH)βt − ηPS c ∆t ut − ηPS c ∇f (0)

(7)

By definition of T , we know for all t < T :
γ kαt k2
1
1
>
+ βt> Hβt
−3F < f˜0 (ut ) − f (0) =∇f (0)> ut − u>
t Hut ≤ ∇f (0) ut −
2
2 ĉ log( dκ
2
)
δ
Combined with the fact kut k2 = kαt k2 + kβt k2 , we have:


2ĉ log( dκ
1
δ )
3F + ∇f (0)> ut + βt> Hβt + kβt k2
kut k2 ≤
γ
2
)
(
dκ
dκ
>
F ĉ log( dκ
G ĉ log( δ )
2
δ ) βt Hβt ĉ log( δ )
kut k,
,
, kβt k
≤14 · max
γ
γ
γ
where last inequality is due to k∇f (0)k ≤ 3G . This gives:


s
s
 G ĉ log( dκ )

dκ
> Hβ ĉ log( dκ )
F
ĉ
log(
)
β
t
t
δ
δ
δ
kut k ≤ 14 · max
,
,
, kβt k


γ
γ
γ

(8)

Now, we use induction to prove that
kut k ≤ 100(S · ĉ)

(9)

Clearly Eq.(9) is true for t = 0 since u0 = 0. Suppose Eq.(9) is true for all τ ≤ t. We will now show that Eq.(9) holds for
t + 1 < T . Note that by the definition of S , F and G , we only need to bound the last two terms of Eq.(8) i.e., kβt+1 k
>
and βt+1
Hβt+1 .
By update function of βt (Eq.(7)), we have:
βt+1 ≤(I − ηH)βt + ηδt

(10)

How to Escape Saddle Points Efficiently

and the norm of δt is bounded as follows:
kδt k ≤ k∆t kkut k + k∇f (0)k
≤ ρ (kut k + kx̃k) kut k + k∇f (0)k
dκ
≤ ρ · 100ĉ(100ĉ + 2/(κ · log( )))S 2 + G
δ
p
= [100ĉ(100ĉ + 2) η` + 1]G ≤ 2G
The last step follows by choosing small enough constant cmax ≤

1
100ĉ(100ĉ+2)

(11)

and stepsize η < cmax /`.

Bounding kβt+1 k: Combining Eq.(10), Eq.(11) and using the definiton of S c , we have:
kβt+1 k ≤ (1 +

ηγ
)kβt k + 2ηG
ĉ log( dκ
δ )

Since kβ0 k = 0 and t + 1 ≤ T , by applying above relation recurrsively, we have:
kβt+1 k ≤

t
X

2(1 +

τ =0

ηγ
)τ ηG ≤ 2 · 3 · T ηG ≤ 6(S · ĉ)
ĉ log( dκ
δ )

The second last inequality is because T ≤ ĉT by definition, so that (1 +
>
Bounding βt+1
Hβt+1 :

ηγ
)T
ĉ log( dκ
δ )

(12)

≤ 3.

Using Eq.(10), we can also write the update equation as:
βt =

t−1
X

(I − ηH)τ δτ

τ =0

Combining with Eq.(11), this gives
>
βt+1
Hβt+1 =η 2

t
t
X
X

δτ>1 (I − ηH)τ1 H(I − ηH)τ2 δτ2

τ1 =0 τ2 =0

≤η 2

t
t
X
X

kδτ1 kk(I − ηH)τ1 H(I − ηH)τ2 kkδτ2 k

τ1 =0 τ2 =0
t
t
X
X

≤4η 2 G 2

k(I − ηH)τ1 H(I − ηH)τ2 k

τ1 =0 τ2 =0

Let the eigenvalues of H to be {λi }, then for any τ1 , τ2 ≥ 0, we know the eigenvalues of (I − ηH)τ1 H(I − ηH)τ2 are
{λi (1 − ηλi )τ1 +τ2 }. Let gt (λ) := λ(1 − ηλ)t , and setting its derivative to zero, we obtain:
∇gt (λ) = (1 − ηλ)t − tηλ(1 − ηλ)t−1 = 0
We see that λ?t =

1
(1+t)η

is the unique maximizer, and gt (λ) is monotonically increasing in (−∞, λ?t ]. This gives:

k(I − ηH)τ1 H(I − ηH)τ2 k = max λi (1 − ηλi )τ1 +τ2 ≤ λ̂(1 − η λ̂)τ1 +τ2 ≤
i

1
(1 + τ1 + τ2 )η

where λ̂ = min{`, λ?τ1 +τ2 }. Therefore, we have:
>
βt+1
Hβt+1 ≤4η 2 G 2

t
t
X
X

k(I − ηH)τ1 H(I − ηH)τ2 k

τ1 =0 τ2 =0

≤4ηG 2

t
t
X
X
τ1 =0 τ2

1
dκ
≤ 8ηT G 2 ≤ 8S 2 γĉ · log−1 ( )
1 + τ1 + τ2
δ
=0

(13)

How to Escape Saddle Points Efficiently

The second last inequality is because by rearrange summation:
t
t
X
X
τ1 =0 τ2

2t

X
1
1
=
≤ 2t + 1 < 2T
min{1 + τ, 2t + 1 − τ } ·
1
+
τ
+
τ
1
+
τ
1
2
τ =0
=0

Finally, substitue Eq.(12) and Eq.(13) into Eq.(8), this gives:


s
s

 G ĉ log( dκ )
dκ
dκ
>
F ĉ log( δ )
βt Hβt ĉ log( δ )
δ
,
,
, kβt k
kut+1 k ≤14 · max


γ
γ
γ
≤100(S · ĉ)
This finishes the induction as well as the proof of the lemma.
A.3.2. P ROOF OF L EMMA 17
In this Lemma we try to show if all the iterates from u0 are constrained in a small ball, iterates from w0 must be able
to decrease the function value. In order to do that, we keep track of vector v which is the difference between u and w.
Similar as before, we also decompose v into different eigenspaces. However, this time we only care about the projection
of v on the direction e1 and its orthognal subspace.
Again, recall notation H := ∇2 f (x̃), e1 as minimum eigenvector of H and quadratic approximation f˜y (x) as defined in
dκ
Eq.(4). Since δ ∈ (0, dκ
e ], we always have log( δ ) ≥ 1. W.L.O.G, set u0 = 0√to be the origin. Define vt = wt − ut , by
assumptions in Lemma 17, we have v0 = µ[S /(κ · log( dκ
δ ))]e1 , µ ∈ [δ/(2 d), 1]. Now, consider the update equation
for wt :
ut+1 + vt+1 = wt+1 =wt − η∇f (wt )
=ut + vt − η∇f (ut + vt )
Z
=ut + vt − η∇f (ut ) − η

1


∇2 f (ut + θvt )dθ vt

0

=ut + vt − η∇f (ut ) − η(H + ∆0t )vt
=ut − η∇f (ut ) + (I − ηH − η∆0t )vt
R1
where ∆0t := 0 ∇2 f (ut + θvt )dθ − H. By Hessian Lipschitz, we have k∆0t k ≤ ρ(kut k + kvt k + kx̃k). This gives the
dynamic for vt satisfy:
vt+1 = (I − ηH − η∆0t )vt
(14)
Since kw0 − x̃k ≤ ku0 − x̃k + kv0 k ≤ S /(κ · log( dκ
δ )), directly applying Lemma 16, we know wt ≤ 100(S · ĉ) for all
t ≤ T . By condition of Lemma 17, we know kut k ≤ 100(S · ĉ) for all t < T . This gives:
kvt k ≤ kut k + kwt k ≤ 200(S · ĉ) for all t < T

(15)

This in sum gives for t < T :
k∆0t k ≤ ρ(kut k + kvt k + kx̃k) ≤ ρ(300ĉS + S /(κ · log(

dκ
))) ≤ ρS (300ĉ + 1)
δ

On the other hand, denote ψt be the norm of vt projected onto e1 direction, and ϕt be the norm of vt projected onto
remaining subspace. Eq.(14) gives us:
q
ψt+1 ≥(1 + γη)ψt − µ ψt2 + ϕ2t
q
ϕt+1 ≤(1 + γη)ϕt + µ ψt2 + ϕ2t

How to Escape Saddle Points Efficiently

where µ = ηρS (300ĉ + 1). We will now prove via induction that for all t < T :
ϕt ≤ 4µt · ψt

(16)

By hypothesis of Lemma 17, we know ϕ0 = 0, thus the base case of induction holds. Assume Eq.(16) is true for τ ≤ t,
For t + 1 ≤ T , we have:


q
2
2
4µ(t + 1)ψt+1 ≥4µ(t + 1) (1 + γη)ψt − µ ψt + ϕt
q
ϕt+1 ≤4µt(1 + γη)ψt + µ ψt2 + ϕ2t
From above inequalities, we see that we only need to show:
q
(1 + 4µ(t + 1)) ψt2 + ϕ2t ≤ 4(1 + γη)ψt
By choosing

√

cmax ≤

1
300ĉ+1

1
, 1 }, and η ≤ cmax /`, we have
min{ 2√
2 4ĉ

p
4µ(t + 1) ≤ 4µT ≤ 4ηρS (300ĉ + 1)ĉT = 4 η`(300ĉ + 1)ĉ ≤ 1
This gives:
q
q
4(1 + γη)ψt ≥ 4ψt ≤ 2 2ψt2 ≥ (1 + 4µ(t + 1)) ψt2 + ϕ2t
which finishes the induction.

Now, we know ϕt ≤ 4µt · ψt ≤ ψt , this gives:
ψt+1 ≥ (1 + γη)ψt −
where the last step follows from µ = ηρS (300ĉ + 1) ≤

√

√

2µψt ≥ (1 +

γη
)ψt
2

cmax (300ĉ + 1)γη · log−1 ( dκ
δ )<

(17)
γη
√ .
2 2

Finally, combining Eq.(15) and (17) we have for all t < T :
γη t
) ψ0
2
γη t S
dκ
γη t δ S
dκ
=(1 +
) c0
log−1 ( ) ≥ (1 +
) √
log−1 ( )
2
κ
δ
2 2 d κ
δ

200(S · ĉ) ≥kvt k ≥ ψt ≥ (1 +

This implies:

√

√

log(400 κ δ d · ĉ log( dκ
1 log(400 κ δ d · ĉ log( dκ
δ ))
δ ))
T <
≤
≤ (2 + log(400ĉ))T
γη
2
log(1 + 2 )
γη
dκ
The last inequality is due to δ ∈ (0, dκ
e ] we have log( δ ) ≥ 1. By choosing constant ĉ to be large enough to satisfy
2 + log(400ĉ) ≤ ĉ, we will have T < ĉT , which finishes the proof.

B. Improve Convergence by Local Structure
In this section, we show if the objective function has nice local structure (e.g. satisfies Assumptions A3.a or A3.b), then it
is possible to combine our analysis with the local analysis in order to get very fast convergence to a local minimum.
In particular, we prove Theorem 5.
Theorem 5. There exist absolute constant cmax such that: if f (·) satisfies A1, A2, and A3.a (or A3.b), then for any
δ > 0,  > 0, ∆f ≥ f (x0 ) − f ? , constant c ≤ cmax , let ˜ = min(θ, γ 2 /ρ), with probability 1 − δ, the output of
PGDli(x0 , `, ρ, ˜, c, δ, ∆f , β) will be -close to X ? in iterations:




`(f (x0 ) − f ? )
d`∆f
β
ζ
4
O
log
+
log
˜2
˜2 δ
α


How to Escape Saddle Points Efficiently

Proof. Theorem 5 runs PGDli(x0 , `, ρ, ˜, c, δ, ∆f , β). According to algorithm 3, we know it calls PGD(x0 , `, ρ, , c, δ, ∆f )
first (denote its output as x̂), then run standard gradient descent with step size β1 starting from x̂.
By Corollary 4, we know x̂ is already in the ζ-neighborhood of X ? , where X ? is the set of local minima. Therefore, to
prove this theorem, we only need to show prove following two claims:
1. Suppose {xt } is the sequence of gradient descent starting from x0 = x̂ with step size
ζ-neighborhood of X ? .

1
β,

then xt is always in the

β
2. Local structure (assumption A3.a or A3.b) allows iterates to converge to points -close to X ? within O( α
log ζ )
iterations.

We will focus on Assumption A3.b (as we will later see Assumption A3.a is a special case of Assumption A3.b). Assume
xt is in ζ-neighborhood of X ? , by gradient updates and the definition of projection, we have:
kxt+1 − PX ? (xt+1 )k2 ≤kxt+1 − PX ? (xt )k2 = kxt − η∇f (xt ) − PX ? (xt )k2
=kxt − PX ? (xt )k2 − 2ηh∇f (xt ), xt − PX ? (xt )i + η 2 k∇f (xt )k2
η
≤kxt − PX ? (xt )k2 − ηαkxt − PX ? (xt )k2 + (η 2 − )k∇f (x)k2
β
α
2
≤(1 − )kxt − PX ? (xt )k
β
The second last inequality is due to (α, β)-regularity condition. The last inequality is because of the choice η =

1
β.

There are two consequences of this calculation. First, it shows kxt+1 − PX ? (xt+1 )k2 ≤ kxt − PX ? (xt )k2 . As a result if
xt in ζ-neighborhood of X ? , xt+1 is also in this ζ-neighborhood. Since x0 is in the ζ-neighborhood by Corollary 4, by
induction we know all later iterations are in the same neighborhood.
Now, since we know all the points xt are in the neighborhood, the equation also shows linear convergence rate (1 − α
β ).
The initial distance is bounded by kx0 − PX ? (x0 )k ≤ ζ, therefore to converge to points -close to X ? , we only need the
following number of iterations:
β
ζ
log(/ζ)
= O( log ).
log(1 − α/β)
α

This finishes the proof under Assumption A3.b.
Finally, we argue assumption A3.a implies A3.b. First, notice that if a function is locally strongly convex, then its local
minima are isolated: for any two points x, x0 ∈ X ? , the local region Bx (ζ) and Bx0 (ζ) must be disjoint (otherwise function
f (x) is strongly convex in connected domain Bx (ζ) ∪ Bx0 (ζ) but has two distinct local minima, which is impossible).
Therefore, W.L.O.G, it suffices to consider one perticular disjoint region, with unique local minimum we denote as x? ,
clearly, for all x ∈ Bx? (ζ) we have PX ? (x) = x? .
Now by α-strong convexity:
f (x? ) ≥ f (x) + h∇f (x), x? − xi +

α
kx − x? k2
2

(18)

On the other hand, for any x in this ζ-neighborhood, we already proved x − β1 ∇f (x) also in this ζ-neighborhood. By
β-smoothness, we also have:
1
1
f (x − ∇f (x)) ≤ f (x) −
k∇f (x)k2
(19)
β
2β
Combining Eq.(18) and Eq.(19), and using the fact f (x? ) ≤ f (x − β1 ∇f (x)), we get:
h∇f (x), x − x? i ≥
which finishes the proof.

α
1
kx − x? k2 +
k∇f (x)k2
2
2β

How to Escape Saddle Points Efficiently

C. Geometric Structures of Matrix Factorization Problem
In this Section we investigate the global geometric structures of the matrix factorization problem. These properties are
summarized in Lemmas 6 and 7. Such structures allow us to apply our main Theorem and get fast convergence (as shown
in Theorem 8).
Note that our main results Theorems 3 and 5 are proved for functions f (·) whose input x is a vector. For the current
function in 2, though the input U ∈ Rd×r is a matrix, we can always vectorize it to be a vector in Rdr and apply our
results. However, for simplicity of presentation, we still write everything in matrix form (without explicit vectorization),
while the reader should keep in mind the operations are same if one vectorizes everything first.
Recall for vectors we use k·k to denote the 2-norm, and for matrices we use k·k and k·kF to denote spectral norm, and
Frobenius norm respectively. Furthermore, we always use σi (·) to denote the i-th largest singular value of the matrix.
We first show how the geometric properties (Lemma 6 and Lemma 7) imply a fast convergence (Theorem 8).
Theorem 8. There exists an absolute constant cmax such that the following holds.
For matrix factorization (2), for any δ > 0 and constant c ≤ cmax , let Γ1/2 := 2 max{kU0 k, 3(σ1? )1/2 }, suppose we run
2
(σ ? )2
PGDli(U0 , 8Γ, 12Γ1/2 , 108Γr 1/2 , c, δ, rΓ2 , 10σ1? ), then:
1. With probability 1, the iterates satisfy kUt k ≤ Γ1/2 for every t ≥ 0.
2. With probability 1 − δ, the output will be -close to global minima set X ? in
!
 4


σr?
σ1?
dΓ
Γ
4
O r
log
+ ? log
σr?
δσr?
σr

iterations.
Proof of Theorem 8. Denote c̃max to be the absolute constant allowed in Theorem 5. In this theorem, we let cmax =
min{c̃max , 21 }, and choose any constant c ≤ cmax .
Theorem 8 consists of two parts. In part 1 we show that the iterations never bring the matrix to a very large norm, while in
part 2 we apply our main Theorem to get fast convergence. We will first prove the bound on number of iterations assuming
the bound on the norm. We will then proceed to prove part 1.
Part 2: Assume part 1 of the theorem is true i.e., with probability 1, the iterates satisfy kUt k ≤ Γ1/2 for every t ≥ 0. In
this case, although we are doing unconstrained optimization, we can still use the geometric properties that hold inside this
region. .
By Lemma 6 and 7, we know objective function Eq.(2) is 8Γ-smooth, 12Γ1/2 -Lipschitz Hessian,
1
( 24
(σr? )3/2 , 13 σr? , 13 (σr? )1/2 )-strict saddle, and holds ( 23 σr? , 10σ1? )-regularity condition in 13 (σr? )1/2 neighborhood of
local minima (also global minima) X ? . Furthermore, note f ? = 0 and recall Γ1/2 = 2 max{kU0 k, 3(σ1? )1/2 }, then, we
have:
? 2
>
? 2
f (U0 ) − f ? = kU0 U>
0 − M kF ≤ 2rkU0 U0 − M k ≤

Thus, we can choose ∆f =
1/2

PGDli(U0 , 8Γ, 12Γ
in iterations:

,

rΓ2
2 .

rΓ2
.
2

Substituting the corresponding parameters from Theorem 5, we know by running

2
(σr? )2
, c, δ, rΓ2 , 10σ1? ),
108Γ1/2

with probability 1 − δ, the output will be -close to global minima set X ?
!
 4


Γ
dΓ
σ1?
σr?
4
+ ? log
O r
log
.
σr?
δσr?
σr


Part 1: We will now show part 1 of the theorem. Recall PGDli (Algorithm 3) runs PGD (Algorithm 2) first, and then runs
gradient descent within 13 (σr? )1/2 neighborhood of X ? . It is easy to verify that 31 (σr? )1/2 neighborhood of X ? is a subset
of {U|kUk2 ≤ Γ}. Therefore, we only need to show that first phase PGD will not leave the region. Specifically, we now
use induction to prove the following for PGD:

How to Escape Saddle Points Efficiently

1. Suppose at iteration τ we add perturbation, and denote Ũτ to be the iterate before adding perturbation (i.e., Uτ =
Ũτ + ξτ , and Ũτ = Uτ −1 − η∇f (Uτ −1 )). Then, kŨτ k ≤ 12 Γ, and
2. kUt k ≤ Γ for all t ≥ 0.
By choice of parameters of Algorithm 2, we know η =

c
8Γ .

First, consider gradient descent step without perturbations:

?
kUt+1 k =kUt − η∇f (Ut )k = kUt − η(Ut U>
t − M )Ut k
?
≤kUt − ηUt U>
t Ut k + ηkM Ut k

≤ max[σi (Ut ) − ησi3 (Ut )] + ηkM? Ut k
i

√
For the first term, we know function f (t) = t − ηt3p
is monotonically√increasing in [0, 1/ 3η]. On the other hand, by
induction assumption, we also know kUt k ≤ Γ1/2 ≤ 8Γ/(3c) = 1/ 3η. Therefore, the max is taken when i = 1:
kUt+1 k ≤kUt k − ηkUt k3 + ηkM? Ut k
≤kUt k − η(kUt k2 − σ1? )kUt k.

(20)

We seperate our discussion into following cases.
Case kUt k > 12 Γ1/2 : In this case kUt k ≥ max{kU0 k, 3(σ1? )1/2 }. Recall Γ1/2 = 2 max{kU0 k, 3(σ1? )1/2 }. Clearly,
Γ ≥ 36σ1? , we know:
1
c 1
( Γ − σ1? ) Γ1/2
8Γ 4
2
c 1/2
= kUt k − Γ .
72

kUt+1 k ≤kUt k − η(kUt k2 − σ1? )kUt k ≤ kUt k −
≤kUt k −

c 1
1
1
( Γ − Γ) Γ1/2
8Γ 4
36 2

This means that in each iteration, the spectral norm would decrease by at least

c 1/2
.
72 Γ

Case kUt k ≤ 12 Γ1/2 : From (20), we know that as long as kUt k2 ≥ kM? k, we will always have kUt+1 k ≤ kUt k ≤
1 1/2
. For kUt k2 ≤ kM? k, we have:
2Γ
kUt+1 k ≤kUt k − η(kUt k2 − σ1? )kUt k = kUt k +

c ?
(σ − kUt k2 )kUt k
8Γ 1

c
((σ ? )1/2 + kUt k)kUt k
8Γ 1
cσ ?
≤kUt k + ((σ1? )1/2 − kUt k) × 1 ≤ (σ1? )1/2
4Γ

≤kUt k + ((σ1? )1/2 − kUt k) ×

Thus, in this case, we always have kUt+1 k ≤ 21 Γ1/2 .
In conclusion, if we don’t add perturbation in iteration t, we have:
• If kUt k > 21 Γ1/2 , then kUt+1 k ≤ kUt k −

c 1/2
.
72 Γ

• If kUt k ≤ 12 Γ1/2 , then kUt+1 k ≤ 21 Γ1/2 .
Now consider the iterations where we add perturbation. By the choice of radius of perturbation in Algorithm 2 , we increase
spectral norm by at most :
√
c
(σr? )2
1
kξt k ≤ kξt kF ≤ 2
≤ Γ1/2
χ 108Γ1/2 · 8Γ
2
The first inequality is because χ ≥ 1 and c ≤ 1. That is, if before perturbation we have kŨt k ≤
kŨt + ξt k ≤ Γ1/2 .

1 1/2
,
2Γ

then kUt k =

How to Escape Saddle Points Efficiently

On the other hand, according to Algorithm 2, once we add perturbation, we will not add perturbation for next tthres =
χ·24Γ
24
48
1 1/2
}, tthres }:
c2 σ ? ≥ c2 ≥ c iterations. Let T = min{inf i {Ut+i |kUt+i k ≤ 2 Γ
r

kUt+T −1 k ≤ kUt k −

c 1/2
c(T − 1)
Γ (T − 1) ≤ Γ1/2 (1 −
)
72
72

48
This gives T ≤ 36
c < c ≤ tthres . Let τ > t be the next time when we add perturbation (τ ≥ t + tthres ), we immediately
know kUT +i k ≤ 21 Γ1/2 for 0 ≤ i < τ − T and kŨτ k ≤ 21 Γ1/2 .

Finally, kU0 k ≤
theorem.

1 1/2
2Γ

by definition of Γ, so the initial condition holds. This finishes induction and the proof of the

In the next subsections we prove the geometric structures.
C.1. Smoothness and Hessian Lipschitz
Before we start proofs of lemmas, we first state some properties about gradient and Hessians. The gradient of the objective
function f (U) is
∇f (U) = 2(UU> − M? )U.
Furthermore, we have the gradient and Hessian satisfy for any Z ∈ Rd×r :
h∇f (U), Zi = 2h(UU> − M? )U, Zi, and
2

>

∇ f (U)(Z, Z) = kUZ +

ZU> k2F

>

(21)
?

>

+ 2hUU − M , ZZ i.

(22)

Lemma 6. For any Γ ≥ σ1? , inside the region {U|kUk2 < Γ}, f (U) defined in Eq.(2) is 8Γ-smooth and 12Γ1/2 -Hessian
Lipschitz.
Proof. Denote D = {U|kUk2 < Γ}, and recall Γ ≥ σ1? .
Part 1: For any U, V ∈ D, we have:
k∇f (U) − ∇f (V)kF =2k(UU> − M? )U − (VV> − M? )VkF


≤2 kUU> U − VV> VkF + kM? (U − V)kF
≤2 [3 · ΓkU − VkF + σ1? kU − VkF ] ≤ 8Γ · kU − VkF .
The last line is due to following decomposition and triangle inequality:
UU> U − VV> V = UU> (U − V) + U(U − V)> V + (U − V)V> V.

Part 2: For any U, V ∈ D, and any Z ∈ Rd×r , according to Eq.(22), we have:
|∇2 f (U)(Z, Z) − ∇2 f (V)(Z, Z)| = kUZ> + ZU> k2F − kVZ> + ZV> k2F + 2hUU> − VV> , ZZ> i .
{z
} |
{z
}
|
A

B

For term A, we have:
A =hUZ> + ZU> , (U − V)Z> + Z(U − V)> i + h(U − V)Z> + Z(U − V)> , VZ> + ZV> i
≤kUZ> + ZU> kF k(U − V)Z> + Z(U − V)> kF + k(U − V)Z> + Z(U − V)> kF kVZ> + ZV> kF
≤4kUkkZk2F kU − VkF + 4kVkkZk2F kU − VkF ≤ 8Γ1/2 kZk2F kU − VkF .

How to Escape Saddle Points Efficiently

For term B, we have:
B ≤ 2kUU> − VV> kF kZZ> kF ≤ 4Γ1/2 kZk2F kU − VkF .
The inequality is due to following decomposition and triangle inequality:
UU> − VV> = U(U − V)> + (U − V)V> .
Therefore, in sum we have:
max |∇2 f (U)(Z, Z) − ∇2 f (V)(Z, Z)| ≤ max 12Γ1/2 kZk2F kU − VkF

Z:kZkF ≤1

Z:kZkF ≤1

≤12Γ1/2 kU − VkF .

C.2. Strict-Saddle Property and Local Regularity
Recall the gradient and Hessian of objective function is calculated as in Eq.(21) and Eq.(22). We first prove an elementary
inequality regarding to the trace of product of two symmetric PSD matrices. This lemma will be frequently used in the
proof.
Lemma 18. For A, B ∈ Rd×d both symmetric PSD matrices, we have:
σmin (A)tr(B) ≤ tr(AB) ≤ kAktr(B)
Proof. Let A = VDV> be the eigendecomposition of A, where D is diagonal matrix, and V is orthogonal matrix. Then
we have:
d
X
tr(AB) = tr(DV> BV) =
Dii (V> BV)ii .
i=1
>

Since B is PSD, we know V BV is also PSD, thus the diagonal entries are non-negative. That is, (V> BV)ii ≥ 0 for
all i = 1, . . . , d. Finally, the lemma follows from the fact that σmin (A) ≤ Dii ≤ kAk and tr(V> BV) = tr(BVV> ) =
tr(B).
Now, we are ready to prove Lemma 7.
Lemma 7. For f (U) defined in Eq.(2), all local minima are global minima. The set of global minima is X ? =
{V? R|RR> = R> R = I}. Furthermore, f (U) satisfies:
1
1. ( 24
(σr? )3/2 , 13 σr? , 31 (σr? )1/2 )-strict saddle property, and

2. ( 23 σr? , 10σ1? )-regularity condition in 13 (σr? )1/2 neighborhood of X ? .
Proof. Let us denote the set X ? := {V? R|RR> = R> R = I}, in the end of proof, we will show this set is the set of all
local minima (which is also global minima).
Throughout the proof of this lemma, we always focus on the first-order and second-order property for one matrix U. For
simplicity of calculation, when it is clear from the context, we denote U? = PX ? (U) and ∆ = U−PX ? (U). By definition
of X ? , we know U? = V? RU and ∆ = U − V? RU , where
RU =

argmin
R:RR> =R> R=I

kU − V? Rk2F

We first prove following claim, which will used in many places across this proof:
U> U? = U> V? RU is a symmetric PSD matrix.

(23)

How to Escape Saddle Points Efficiently

This because by expanding the Frobenius norm, and letting the SVD of V?> U be ADB> , we have:
argmin
R:RR> =R> R=I

=

argmin

kU − V? Rk2F =
−tr(U> V? R) =

R:RR> =R> R=I

argmin

−hU, V? Ri

R:RR> =R> R=I

argmin

−tr(DA> RB)

R:RR> =R> R=I

Since A, B, R are all orthonormal matrix, we know A> RB is also orthonormal matrix. Moreover for any orthonormal
matrix T, we have:
X
X
tr(DT) =
Dii Tii ≤
Dii
i

i

The last inequality is because Dii is singular value thus non-negative, and T is orthonormal, thus Tii ≤ 1. This means
the maximum of tr(DT) is achieved when T = I, i.e., the minimum of −tr(DA> RB) is achieved when R = AB> .
Therefore, U> V? RU = BDA> AB> = BDB> is symmetric PSD matrix.
Part 1: In order to show the strict saddle property, we only need to show that for any U satisfying k∇f (U)kF ≤
and k∆kF = kU − U? kF ≥ 13 (σr? )1/2 , we always have σmin (∇2 f (U)) ≤ − 31 σr? .

1
? 3/2
24 (σr )

Let’s consider Hessian ∇2 (U) in the direction of ∆ = U − U? . Clearly, we have:
UU> − M? = UU> − (U − ∆)(U − ∆)> = (U∆> + ∆U> ) − ∆∆>
and by (21):
h∇f (U), ∆i =2h(UU> − M? )U, ∆i = hUU> − M? , ∆U> + U∆> i
=hUU> − M? , UU> − M? + ∆∆> i
Therefore, by Eq.(22) and above two equalities, we have:
∇2 f (U)(∆, ∆) =kU∆> + ∆U> k2F + 2hUU> − M? , ∆∆> i
=kUU> − M? + ∆∆> k2F + 2hUU> − M? , ∆∆> i
=k∆∆> k2F − 3kUU> − M? k2F + 4hUU> − M? , UU> − M? + ∆∆> i
=k∆∆> k2F − 3kUU> − M? k2F + 4h∇f (U), ∆i
Consider the first two terms, by expanding, we have:
3kUU> − M? k2F − k∆∆> k2F = 3k(U? ∆> + ∆U?> ) + ∆∆> k2F − k∆∆> k2F

=3 · tr 2U?> U? ∆> ∆ + 2(U?> ∆)2 + 4U?> ∆∆> ∆ + (∆> ∆)2 − tr((∆> ∆)2 )

=tr 6U?> U? ∆> ∆ + 6(U?> ∆)2 + 12U?> ∆∆> ∆ + 2(∆> ∆)2
√
√
√
=tr((4 3 − 6)U?> U? ∆> ∆ + (12 − 4 3)U?> (U? + ∆)∆> ∆ + 2( 3U?> ∆ + ∆> ∆)2 )
√
√
≥(4 3 − 6)tr(U?> U? ∆> ∆) ≥ (4 3 − 6)σr? k∆k2F
where the second last inequality is because U?> (U? + ∆)∆> ∆ = U?> U∆> ∆ is the product of two symmetric PSD
matrices (thus its trace is non-negative); the last inequality is by Lemma 18.
Finally, in case we have k∇f (U)kF ≤

1
? 3/2
24 (σr )

and k∆kF = kU − U? kF ≥ 13 (σr? )1/2

√
h∇f (U), ∆i
1
∇2 f (U)(∆, ∆) ≤ −(4 3 − 6)σr? + 4
2
k∆kF
k∆k2F
√
√
k∇f (U)kF
1
≤ −(4 3 − 6.5)σr? ≤ − σr?
≤ − (4 3 − 6)σr? + 4
k∆kF
3

σmin (∇2 f (U)) ≤

How to Escape Saddle Points Efficiently

Part 2: In 13 (σr? )1/2 neigborhood of X ? , by definition, we know,
k∆k2F = kU − U? k2F ≤
Clearly, by Weyl’s inequality, we have kUk ≤ kU? k + k∆k ≤
Moreover, since U?> U is symmetric matrix, we have:

1 ?
σ .
9 r

4
? 1/2
,
3 (σ1 )

and σr (U) ≥ σr (U? ) − k∆k ≥

2
? 1/2
.
3 (σr )


1
σr (U> U? + U?> U)
2

1
≥ σr (U> U + U?> U? ) − k(U − U? )> (U − U? )k
2

1
≥ σr (U> U) + σr (U?> U? ) − k∆k2F
2
4 1
2
1
≥ (1 + − )σr? = σr? .
2
9 9
3

σr (U?> U) =

At a highlevel, we will prove (α, β)-regularity property (1) by proving that:
1. h∇f (x), x − PX ? (x)i ≥ αkx − PX ? (x)k2 , and
2. h∇f (x), x − PX ? (x)i ≥

1
2
β k∇f (x)k .

According to (21), we know:
h∇f (U), U − PX ? (U)i =2h(UU> − M? )U, ∆i = 2hU∆> + ∆U?> , ∆U> i
=2(tr(U∆> U∆> ) + tr(∆U?> U∆> ))
=2(k∆> Uk2F + tr(U?> U∆> ∆)).

(24)

The last equality is because ∆> U is symmetric matrix. Since U?> U is symmetric PSD matrix, and recall σr (U?> U) ≥
2 ?
3 σr , by Lemma 18 we have:
h∇f (U), U − PX ? (U)i ≥ σr (U?> U)tr(∆> ∆) ≥

2 ?
σ k∆k2F .
3 r

(25)

On the other hand, we also have:
k∇f (U)k2F =4h(UU> − M? )U, (UU> − M? )Ui
=4h(U∆> + ∆U?> )U, (U∆> + ∆U?> )Ui




=4 tr[(∆> UU> ∆)U> U] + 2tr[∆> UU> U? ∆> U] + tr(U?> UU> U? ∆> ∆) .
{z
} |
{z
}
|
A

B

>

For term A, by Lemma 18, and ∆ U being a symmetric matrix, we have:
A ≤ kU> Ukk∆> Uk2F + 2kU> U? kk∆> Uk2F ≤ (

16 8 ? > 2
+ )σ1 k∆ UkF ≤ 5σ1? k∆> Uk2F
9
3

For term B, by Eq.(23) we can denote C = U?> U = U> U? which is symmetric PSD matrix, by Lemma 18, we have:
B =tr(C2 ∆> ∆) = tr(C(C1/2 ∆> ∆C1/2 ))
≤kCktr(C1/2 ∆> ∆C1/2 ) = kCktr(C∆> ∆) ≤

4 ?
σ tr(U?> U∆> ∆).
3 1

Combining with (24) we have:
k∇f (U)k2F ≤ σ1? (20k∆> Uk2F +

16
tr(U?> U∆> ∆)) ≤ 10σ1? h∇f (U), U − PX ? (U)i.
3

(26)

How to Escape Saddle Points Efficiently

Combining (25) and (26), we have:
h∇f (U), U − PX ? (U)i ≥

1 ?
1
k∇f (U)k2F .
σ kU − PX ? (U)k2F +
3 r
20σ1?

