How to Escape Saddle Points Efficiently

A. Detailed Proof of Main Theorem
In this section, we give detailed proof for the main theorem. We will first state two key lemmas that show how the algorithm
can make progress when the gradient is large or near a saddle point, and show how the main theorem follows from the two
lemmas. Then we will focus on the novel technique in this paper: how to analyze gradient descent near saddle point.
A.1. General Framework
In order to prove the main theorem, we need to show that the algorithm will not be stuck at any point that either has a large
gradient or is a saddle point. This idea is similar to previous works (e.g.(Ge et al., 2015)). We first state a standard Lemma
that shows if the current gradient is large, then we make progress in function value.
Lemma 12. Assume f (Â·) satisfies A1, then for gradient descent with stepsize Î· < 1` , we have:
Î·
f (xt+1 ) â‰¤ f (xt ) âˆ’ kâˆ‡f (xt )k2
2
Proof. By Assumption A1 and its property, we have:
`
f (xt+1 ) â‰¤f (xt ) + âˆ‡f (xt )> (xt+1 âˆ’ xt ) + kxt+1 âˆ’ xt k2
2
Î·
Î·2 `
kâˆ‡f (xt )k2 â‰¤ f (xt ) âˆ’ kâˆ‡f (xt )k2
=f (xt ) âˆ’ Î·kâˆ‡f (xt )k2 +
2
2

The next lemma says that if we are â€œclose to a saddle pointsâ€, i.e., we are at a point where the gradient is small, but the
Hessian has a reasonably large negative eigenvalue. This is the main difficulty in the analysis. We show a perturbation
followed by small (polylog) number of standard gradient descent steps can also make the function value decrease with high
probability.
Lemma 13. There exist absolute constant cmax , for f (Â·) satisfies A1, and any c â‰¤ cmax , and Ï‡ â‰¥ 1.
Î·, r, gthres , fthres , tthres calculated same way as in Algorithm 2. Then, if xÌƒt satisfies:
âˆš
kâˆ‡f (xÌƒt )k â‰¤ gthres
and
Î»min (âˆ‡2 f (xÌƒt )) â‰¤ âˆ’ Ï

Let

Let xt = xÌƒt + Î¾t where Î¾t comes from the uniform distribution over B0 (r), and let xt+i be the iterates of gradient descent
from xt with stepsize Î·, then with at least probability 1 âˆ’ âˆšd`Ï eâˆ’Ï‡ , we have:
f (xt+tthres ) âˆ’ f (xÌƒt ) â‰¤ âˆ’fthres
The proof of this lemma is deferred to Section A.2. Using this Lemma, we can then prove the main Theorem.
2

Theorem 3. There exist absolute constant cmax such that: if f (Â·) satisfies A1, then for any Î´ > 0,  â‰¤ `Ï , âˆ†f â‰¥ f (x0 )âˆ’f ? ,
and constant c â‰¤ cmax , with probability 1 âˆ’ Î´, the output of PGD(x0 , `, Ï, , c, Î´, âˆ†f ) will be âˆ’second order stationary
point, and terminate in iterations:



`(f (x0 ) âˆ’ f ? )
d`âˆ†f
4
O
log
2
2 Î´
Proof. Denote cÌƒmax to be the absolute constant allowed in Theorem 13. In this theorem, we let cmax = min{cÌƒmax , 1/2},
and choose any constant c â‰¤ cmax .
In this proof, we will actually achieve some point satisfying following condition:
âˆš
c
âˆš
kâˆ‡f (x)k â‰¤ gthres = 2 Â· ,
Î»min (âˆ‡2 f (x)) â‰¥ âˆ’ Ï
Ï‡
âˆš

Since c â‰¤ 1, Ï‡ â‰¥ 1, we have

c
Ï‡2

â‰¤ 1, which implies any x satisfy Eq.(3) is also a -second-order stationary point.

Starting from x0 , we know if x0 does not satisfy Eq.(3), there are only two possibilities:

(3)

How to Escape Saddle Points Efficiently

1. kâˆ‡f (x0 )k > gthres : In this case, Algorithm 2 will not add perturbation. By Lemma 12:
Î· 2
c2 2
f (x1 ) âˆ’ f (x0 ) â‰¤ âˆ’ Â· gthres
=âˆ’ 4 Â·
2
2Ï‡
`
2. kâˆ‡f (x0 )k â‰¤ gthres : In this case, Algorithm 2 will add a perturbation of radius r, and will perform gradient descent
(without perturbations) for the next tthres steps. Algorithm 2 will then check termination condition. If the condition is
not met, we must have:
s
3
c
f (xtthres ) âˆ’ f (x0 ) â‰¤ âˆ’fthres = âˆ’ 3 Â·
Ï‡
Ï
This means on average every step decreases the function value by
c3 2
f (xtthres ) âˆ’ f (x0 )
â‰¤âˆ’ 4 Â·
tthres
Ï‡
`
In case 1, we can repeat this argument for t = 1 and in case 2, we can repeat this argument for t = tthres . Hence, we can
3
2
conclude as long as algorithm 2 has not terminated yet, on average, every step decrease function value by at least Ï‡c 4 Â· ` .
However, we clearly can not decrease function value by more than f (x0 ) âˆ’ f ? , where f ? is the function value of global
minima. This means algorithm 2 must terminate within the following number of iterations:
f (x0 ) âˆ’ f ?
Ï‡4 `(f (x0 ) âˆ’ f ? )
=
Â·
=O
2
3

c
c3
2
Ï‡4 Â· `



`(f (x0 ) âˆ’ f ? )
log4
2



d`âˆ†f
2 Î´



Finally, we would like to ensure when Algorithm 2 terminates, the point it finds is actually an -second-order stationary
point. The algorithm can only terminate when the gradient is small, and the function value does not decrease after a
perturbation and tthres iterations. We shall show every time when we add perturbation to iterate xÌƒt , if Î»min (âˆ‡2 f (xÌƒt )) <
âˆš
âˆ’ Ï, then we will have f (xt+tthres ) âˆ’ f (xÌƒt ) â‰¤ âˆ’fthres . Thus, whenever the current point is not an -second-order
stationary point, the algorithm cannot terminate.
According to Algorithm 2, we immediately know kâˆ‡f (xÌƒt )k â‰¤ gthres (otherwise we will not add perturbation at time t).
By Lemma 13, we know this event happens with probability at least 1 âˆ’ âˆšd`Ï eâˆ’Ï‡ each time. On the other hand, during one
entire run of Algorithm 2, the number of times we add perturbations is at most:
1
tthres

Â·

Ï‡3
Ï‡4 `(f (x0 ) âˆ’ f ? )
Â·
=
3
2
c

c

âˆš

Ï(f (x0 ) âˆ’ f ? )
2

By union bound, for all these perturbations, with high probability Lemma 13 is satisfied. As a result Algorithm 2 works
correctly. The probability of that is at least
d`
Ï‡3
1 âˆ’ âˆš eâˆ’Ï‡ Â·
Ï
c
Recall our choice of Ï‡ = 3 max{log(

d`âˆ†f
c2 Î´

âˆš

Ï(f (x0 ) âˆ’ f ? )
Ï‡3 eâˆ’Ï‡ d`(f (x0 ) âˆ’ f ? )
=1âˆ’
Â·
2

c
2

), 4}. Since Ï‡ â‰¥ 12, we have Ï‡3 eâˆ’Ï‡ â‰¤ eâˆ’Ï‡/3 , this gives:

d`(f (x0 ) âˆ’ f ? )
Ï‡3 eâˆ’Ï‡ d`(f (x0 ) âˆ’ f ? )
Â·
â‰¤ eâˆ’Ï‡/3
â‰¤Î´
2
c

c2
which finishes the proof.

How to Escape Saddle Points Efficiently

A.2. Main Lemma: Escaping from Saddle Points Quickly
Now we prove the main lemma, which shows near a saddle point, a small perturbation followed by a small number of
gradient descent steps will decrease the function value with high probability. This is the main step where we need new
analysis, as the analysis previous works (e.g.(Ge et al., 2015)) do not work when the step size and perturbation do not
depend polynomially in dimension d.
Intuitively, after adding a perturbation, the current point of the algorithm is a uniform distribution over a d-dimensional ball
centered at xÌƒ, which we call perturbation ball. After a small number of gradient steps, some points in this ball (which we
call the escaping region) will significantly decrease the function; other points (which we call the stuck region) does not
see a significant decrease in function value. We hope to show that the escaping region constitutes at least 1 âˆ’ Î´ fraction of
the volume of the perturbation ball.
However, we do not know the exact form of the function near the saddle point, so the escaping region does not have a
clean analytic description. Explicitly computing its volume can be very difficult. Our proof rely on a crucial observation:
although we do not know the shape of the stuck region, we know the â€œwidthâ€ of it must be small, therefore it cannot have
a large volume. We will formalize this intuition later in Lemma 15.
The proof of the main lemma requires carefully balancing between different quantities including function value, gradient,
parameter space and number of iterations. For clarify, we define following scalar quantities, which serve as the â€œunitsâ€ for
function value, gradient, parameter space, and time (iterations). We will use these notations throughout the proof.
Let the condition number be the ratio of the smoothness parameter (largest eigenvalue of Hessian) and the negative eigenvalue Î³: Îº = `/Î³ â‰¥ 1, we define the following units:
F := Î·`

dÎº
Î³3
Â· logâˆ’3 ( ),
Ï2
Î´

S :=

Î·`

p

p Î³2
dÎº
Î·` Â· logâˆ’2 ( )
Ï
Î´
dÎº
log( Î´ )
T :=
Î·Î³
G :=

Î³
dÎº
Â· logâˆ’1 ( ),
Ï
Î´

Intuitively, if we plug in our choice of step size Î·` = O(1) (which we will prove later) and hide the logarithmic de2
3
pendences, we have F = OÌƒ( Î³Ï2 ), G = OÌƒ( Î³Ï ), S = OÌƒ( Î³Ï ), which is the only way to correctly discribe the units of
function value, gradient, parameter space by just Î³ and Ï. Moreover, these units are closely related, in particular, we know
q
F Â·log( dÎº
Î´ )
Î³

=

G Â·log( dÎº
Î´ )
Î³

= S.

For simplicity of later proofs, we first restate Lemma 13 into a slightly more general form as follows. Lemma 13 is directly
implied following lemma.
Lemma 14 (Lemma 13 restated). There exists universal constant cmax , for f (Â·) satisfies A1, for any Î´ âˆˆ (0, dÎº
e ], suppose
we start with point xÌƒ satisfying following conditions:
kâˆ‡f (xÌƒ)k â‰¤ G

and

Î»min (âˆ‡2 f (xÌƒ)) â‰¤ âˆ’Î³

Let x0 = xÌƒ + Î¾ where Î¾ come from the uniform distribution over ball with radius S /(Îº Â· log( dÎº
Î´ )), and let xt be the iterates
of gradient descent from x0 . Then, when stepsize Î· â‰¤ cmax /`, with at least probability 1 âˆ’ Î´, we have following for any
1
T â‰¥ cmax
T:
f (xT ) âˆ’ f (xÌƒ) â‰¤ âˆ’F
Lemma 14 is almost the same as Lemma 13. It is easy to verify that by substituting Î· = c` , Î³ =
Lemma 14, we immediately obtain Lemma 13.

âˆš

Ï and Î´ =

âˆšd` eâˆ’Ï‡
Ï

into

Now we will formalize the intuition that the â€œwidthâ€ of stuck region is small.
Lemma 15. There exists a universal constant cmax , for any Î´ âˆˆ (0, dÎº
e ], let f (Â·), xÌƒ satisfies the conditions in Lemma 14,
and without loss of generality let e1 be the minimum eigenvector of âˆ‡2 f (xÌƒ). Consider two gradient descent sequences
{ut }, {wt } with initial points u0 , w0 satisfying: (denote radius r = S /(Îº Â· log( dÎº
Î´ )))
âˆš
ku0 âˆ’ xÌƒk â‰¤ r, w0 = u0 + Âµ Â· r Â· e1 , Âµ âˆˆ [Î´/(2 d), 1]
Then, for any stepsize Î· â‰¤ cmax /`, and any T â‰¥

1
cmax T

, we have:

min{f (uT ) âˆ’ f (u0 ), f (wT ) âˆ’ f (w0 )} â‰¤ âˆ’2.5F

How to Escape Saddle Points Efficiently

Intuitively, lemma 15 claims for any two points u0 , w0 inside the perturbation ball,
âˆš if u0 âˆ’ w0 lies in the direction of
minimum eigenvector of âˆ‡2 f (xÌƒ), and ku0 âˆ’ w0 k is greater than threshold Î´r/(2 d), then at least one of two sequences
{ut }, {wt } will â€œefficiently escape saddle pointâ€. In other words, if u0 is a point âˆš
in the stuck region, consider any point
w0 that is on a straight line along direction of e1 . As long as w0 is slightly far (Î´r/ d) from u0 , it must be in the escaping
region. This is what we mean by the â€œwidthâ€ of the stuck region being small.Now we prove the main Lemma using this
observation:
Proof of Lemma 14. By adding perturbation, in worst case we increase function value by:
S
S
1
`
3
f (x0 ) âˆ’ f (xÌƒ) â‰¤ âˆ‡f (xÌƒ)> Î¾ + kÎ¾k2 â‰¤ G (
) + `(
)2 â‰¤ F
dÎº
dÎº
2
2 Îº Â· log( Î´ )
2
Îº Â· log( Î´ )
On the other hand, let radius r =

S
.
ÎºÂ·log( dÎº
Î´ )

We know x0 come froms uniform distribution over BxÌƒ (r). Let Xstuck âŠ‚ BxÌƒ (r)

denote the set of bad starting points so that if x0 âˆˆ Xstuck , then f (xT ) âˆ’ f (x0 ) > âˆ’2.5F (thus stuck at a saddle point);
otherwise if x0 âˆˆ BxÌƒ (r) âˆ’ Xstuck , we have f (xT ) âˆ’ f (x0 ) â‰¤ âˆ’2.5F .
Î´
, 1].
By applying Lemma 15, we know for any x0 âˆˆ Xstuck , it is guaranteed that (x0 Â± Âµre1 ) 6âˆˆ Xstuck where Âµ âˆˆ [ 2âˆš
d

Denote IXstuck (Â·) be the indicator function of being inside set Xstuck ; and vector x = (x(1) , x(âˆ’1) ), where x(1) is the
component along e1 direction, and x(âˆ’1) is the remaining d âˆ’ 1 dimensional vector. Recall B(d) (r) be d-dimensional ball
with radius r; By calculus, this gives an upper bound on the volumn of Xstuck :
Z
Vol(Xstuck ) =
dx Â· IXstuck (x)
(d)

BxÌƒ (r)

âˆš

Z

(âˆ’1)

=

Z

xÌƒ(1) +

dx
(dâˆ’1)
BxÌƒ
(r)

Z
â‰¤

âˆš

xÌƒ(1) âˆ’

dx

(âˆ’1)

(dâˆ’1)
BxÌƒ
(r)



r 2 âˆ’kxÌƒ(âˆ’1) âˆ’x(âˆ’1) k2

dx(1) Â· IXstuck (x)
r 2 âˆ’kxÌƒ(âˆ’1) âˆ’x(âˆ’1) k2

Î´
Â· 2Â· âˆš r
2 d



(dâˆ’1)

= Vol(B0

Î´r
(r)) Ã— âˆš
d

Then, we immediately have the ratio:
Î´r
âˆš
d

(dâˆ’1)

Ã— Vol(B0

r
Î´ Î“( d2 + 1)
Î´
d 1
â‰¤
=âˆš
â‰¤âˆš Â·
+ â‰¤Î´
d
1
(d)
(d)
2 2
Ï€d Î“( 2 + 2 )
Ï€d
Vol(BxÌƒ (r))
Vol(B0 (r))
q
Î“(x+1)
The second last inequality is by the property of Gamma function that Î“(x+1/2)
< x + 12 as long as x â‰¥ 0. Therefore,
with at least probability 1 âˆ’ Î´, x0 6âˆˆ Xstuck . In this case, we have:
Vol(Xstuck )

(r))

f (xT ) âˆ’ f (xÌƒ) =f (xT ) âˆ’ f (x0 ) + f (x0 ) âˆ’ f (0)
â‰¤ âˆ’ 2.5F + 1.5F â‰¤ âˆ’F
which finishes the proof.
A.3. Bounding the Width of Stuck Region
In order to prove Lemma 15, we do it in two steps:
1. We first show if gradient descent from u0 does not decrease function value, then all the iterates must lie within a small
ball around u0 (Lemma 16).
2. If gradient descent starting from a point u0 stuck in a small ball around a saddle point, then gradient descent from w0
(moving u0 along e1 direction for at least a certain distance), will decreases the function value (Lemma 17).
Recall we assumed without loss of generality e1 is the minimum eigenvector of âˆ‡2 f (xÌƒ). In this context, we denote
H := âˆ‡2 f (xÌƒ), and for simplicity of calculation, we consider following quadratic approximation:
1
fËœy (x) := f (y) + âˆ‡f (y)> (x âˆ’ y) + (x âˆ’ y)> H(x âˆ’ y)
2

(4)

How to Escape Saddle Points Efficiently

Now we are ready to state two lemmas formally:
Lemma 16. For any constant cÌ‚ â‰¥ 3, there exists absolute constant cmax : for any Î´ âˆˆ (0, dÎº
e ], let f (Â·), xÌƒ satisfies the
)),
define:
condition in Lemma 14, for any initial point u0 with ku0 âˆ’ xÌƒk â‰¤ 2S /(Îº Â· log( dÎº
Î´
n
n
o
o
T = min inf t|fËœu0 (ut ) âˆ’ f (u0 ) â‰¤ âˆ’3F , cÌ‚T
t

then, for any Î· â‰¤ cmax /`, we have for all t < T that kut âˆ’ xÌƒk â‰¤ 100(S Â· cÌ‚).
Lemma 17. There exists absolute constant cmax , cÌ‚ such that: for any Î´ âˆˆ (0, dÎº
e ], let f (Â·), xÌƒ satisfies the condition in
Lemma 14, and sequences {ut }, {wt } satisfy the conditions in Lemma 15, define:
n
o
o
n
T = min inf t|fËœw0 (wt ) âˆ’ f (w0 ) â‰¤ âˆ’3F , cÌ‚T
t

then, for any Î· â‰¤ cmax /`, if kut âˆ’ xÌƒk â‰¤ 100(S Â· cÌ‚) for all t < T , we will have T < cÌ‚T .
Note the conclusion T < cÌ‚T in Lemma 17 equivalently means:
n
o
inf t|fËœw0 (wt ) âˆ’ f (w0 ) â‰¤ âˆ’3F < cÌ‚T
t

That is, for some T < cÌ‚T , {wt } sequence â€œescape the saddle pointâ€ in the sense of sufficient function value decrease
fËœw0 (wt ) âˆ’ f (w0 ) â‰¤ âˆ’3F . Now, we are ready to prove Lemma 15.
(2)

Proof of Lemma 15. W.L.O.G, let xÌƒ = 0 be the origin. Let (cmax , cÌ‚) be the absolute constant so that Lemma 17 holds,
(1)
also let cmax be the absolute constant to make Lemma 16 holds based on our current choice of cÌ‚. We choose cmax â‰¤
(2)
(1)
min{cmax , cmax } so that our step size Î· â‰¤ cmax /` is small enough which make both Lemma 16 and Lemma 17 hold. Let
? :=
T
cÌ‚T and define:
n
o
T 0 = inf t|fËœu0 (ut ) âˆ’ f (u0 ) â‰¤ âˆ’3F
t

Letâ€™s consider following two cases:
Case T 0 â‰¤ T ? : In this case, by Lemma 16, we know kuT 0 âˆ’1 k â‰¤ O(S ), and therefore
kuT 0 k â‰¤kuT 0 âˆ’1 k + Î·kâˆ‡f (uT 0 âˆ’1 )k â‰¤ kuT 0 âˆ’1 k + Î·kâˆ‡f (xÌƒ)k + Î·`kuT 0 âˆ’1 k â‰¤ O(S )
By choosing cmax small enough and Î· â‰¤ cmax /`, this gives:
1
Ï
f (uT 0 ) âˆ’ f (u0 ) â‰¤âˆ‡f (u0 )> (uT 0 âˆ’ u0 ) + (uT 0 âˆ’ u0 )> âˆ‡2 f (u0 )(uT 0 âˆ’ u0 ) + kuT 0 âˆ’ u0 k3
2
6
Ï
Ï
2
3
Ëœ
â‰¤fu0 (ut ) âˆ’ f (u0 ) + ku0 âˆ’ xÌƒkkuT 0 âˆ’ u0 k + kuT 0 âˆ’ u0 k
2
6
p
â‰¤ âˆ’ 3F + O(ÏS 3 ) = âˆ’3F + O( Î·` Â· F ) â‰¤ âˆ’2.5F
By choose cmax â‰¤ min{1, 1cÌ‚ }. We know Î· < 1` , by Lemma 12, we know gradient descent always decrease function value.
1
Therefore, for any T â‰¥ cmax
T â‰¥ cÌ‚T = T ? â‰¥ T 0 , we have:
f (uT ) âˆ’ f (u0 ) â‰¤ f (uT ? ) âˆ’ f (u0 ) â‰¤ f (uT 0 ) âˆ’ f (u0 ) â‰¤ âˆ’2.5F
Case T 0 > T ? : In this case, by Lemma 16, we know kut k â‰¤ O(S ) for all t â‰¤ T ? . Define
n
o
T 00 = inf t|fËœw0 (wt ) âˆ’ f (w0 ) â‰¤ âˆ’2F
t

By Lemma 17, we immediately have T 00 â‰¤ T ? . Apply same argument as in first case, we have for all T â‰¥
f (wT ) âˆ’ f (w0 ) â‰¤ f (wT ? ) âˆ’ f (w0 ) â‰¤ âˆ’2.5F .
Next we finish the proof by proving Lemma 16 and Lemma 17.

1
cmax T

that

How to Escape Saddle Points Efficiently

A.3.1. P ROOF OF L EMMA 16
In Lemma 16, we hope to show if the function value did not decrease, then all the iterations must be constrained in a
small ball. We do that by analyzing the dynamics of the iterations, and we decompose the d-dimensional space into
two subspaces: a subspace S which is the span of significantly negative eigenvectors of the Hessian and its orthogonal
compliment.
Recall notation H := âˆ‡2 f (xÌƒ) and quadratic approximation fËœy (x) as defined in Eq.(4). Since Î´ âˆˆ (0, dÎº
e ], we always have
)
â‰¥
1.
W.L.O.G,
set
u
=
0
to
be
the
origin,
by
gradient
descent
update
function,
we
have:
log( dÎº
0
Î´
ut+1 =ut âˆ’ Î·âˆ‡f (ut )
Z
=ut âˆ’ Î·âˆ‡f (0) âˆ’ Î·

1


âˆ‡2 f (Î¸ut )dÎ¸ ut

0

=ut âˆ’ Î·âˆ‡f (0) âˆ’ Î·(H + âˆ†t )ut
=(I âˆ’ Î·H âˆ’ Î·âˆ†t )ut âˆ’ Î·âˆ‡f (0)

(5)

R1
Here, âˆ†t := 0 âˆ‡2 f (Î¸ut )dÎ¸ âˆ’ H. By Hessian Lipschitz, we have kâˆ†t k â‰¤ Ï(kut k + kxÌƒk), and by smoothness of the
gradient, we have kâˆ‡f (0)k â‰¤ kâˆ‡f (xÌƒ)k + `kxÌƒk â‰¤ G + ` Â· 2S /(Îº Â· log( dÎº
Î´ )) â‰¤ 3G .
We will now compute the projections of ut in different eigenspaces of H. Let S be the subspace spanned by all eigenvectors
of H whose eigenvalue is less than âˆ’ cÌ‚ log(Î³ dÎº ) . S c denotes the subspace of remaining eigenvectors. Let Î±t and Î²t denote
Î´
the projections of ut onto S and S c respectively i.e., Î±t = PS ut , and Î²t = PS c ut . We can decompose the update
equations Eq.(5) into:
Î±t+1 =(I âˆ’ Î·H)Î±t âˆ’ Î·PS âˆ†t ut âˆ’ Î·PS âˆ‡f (0)

(6)

Î²t+1 =(I âˆ’ Î·H)Î²t âˆ’ Î·PS c âˆ†t ut âˆ’ Î·PS c âˆ‡f (0)

(7)

By definition of T , we know for all t < T :
Î³ kÎ±t k2
1
1
>
+ Î²t> HÎ²t
âˆ’3F < fËœ0 (ut ) âˆ’ f (0) =âˆ‡f (0)> ut âˆ’ u>
t Hut â‰¤ âˆ‡f (0) ut âˆ’
2
2 cÌ‚ log( dÎº
2
)
Î´
Combined with the fact kut k2 = kÎ±t k2 + kÎ²t k2 , we have:


2cÌ‚ log( dÎº
1
Î´ )
3F + âˆ‡f (0)> ut + Î²t> HÎ²t + kÎ²t k2
kut k2 â‰¤
Î³
2
)
(
dÎº
dÎº
>
F cÌ‚ log( dÎº
G cÌ‚ log( Î´ )
2
Î´ ) Î²t HÎ²t cÌ‚ log( Î´ )
kut k,
,
, kÎ²t k
â‰¤14 Â· max
Î³
Î³
Î³
where last inequality is due to kâˆ‡f (0)k â‰¤ 3G . This gives:
ï£±
ï£¼
s
s
ï£² G cÌ‚ log( dÎº )
ï£½
dÎº
> HÎ² cÌ‚ log( dÎº )
F
cÌ‚
log(
)
Î²
t
t
Î´
Î´
Î´
kut k â‰¤ 14 Â· max
,
,
, kÎ²t k
ï£³
ï£¾
Î³
Î³
Î³

(8)

Now, we use induction to prove that
kut k â‰¤ 100(S Â· cÌ‚)

(9)

Clearly Eq.(9) is true for t = 0 since u0 = 0. Suppose Eq.(9) is true for all Ï„ â‰¤ t. We will now show that Eq.(9) holds for
t + 1 < T . Note that by the definition of S , F and G , we only need to bound the last two terms of Eq.(8) i.e., kÎ²t+1 k
>
and Î²t+1
HÎ²t+1 .
By update function of Î²t (Eq.(7)), we have:
Î²t+1 â‰¤(I âˆ’ Î·H)Î²t + Î·Î´t

(10)

How to Escape Saddle Points Efficiently

and the norm of Î´t is bounded as follows:
kÎ´t k â‰¤ kâˆ†t kkut k + kâˆ‡f (0)k
â‰¤ Ï (kut k + kxÌƒk) kut k + kâˆ‡f (0)k
dÎº
â‰¤ Ï Â· 100cÌ‚(100cÌ‚ + 2/(Îº Â· log( )))S 2 + G
Î´
p
= [100cÌ‚(100cÌ‚ + 2) Î·` + 1]G â‰¤ 2G
The last step follows by choosing small enough constant cmax â‰¤

1
100cÌ‚(100cÌ‚+2)

(11)

and stepsize Î· < cmax /`.

Bounding kÎ²t+1 k: Combining Eq.(10), Eq.(11) and using the definiton of S c , we have:
kÎ²t+1 k â‰¤ (1 +

Î·Î³
)kÎ²t k + 2Î·G
cÌ‚ log( dÎº
Î´ )

Since kÎ²0 k = 0 and t + 1 â‰¤ T , by applying above relation recurrsively, we have:
kÎ²t+1 k â‰¤

t
X

2(1 +

Ï„ =0

Î·Î³
)Ï„ Î·G â‰¤ 2 Â· 3 Â· T Î·G â‰¤ 6(S Â· cÌ‚)
cÌ‚ log( dÎº
Î´ )

The second last inequality is because T â‰¤ cÌ‚T by definition, so that (1 +
>
Bounding Î²t+1
HÎ²t+1 :

Î·Î³
)T
cÌ‚ log( dÎº
Î´ )

(12)

â‰¤ 3.

Using Eq.(10), we can also write the update equation as:
Î²t =

tâˆ’1
X

(I âˆ’ Î·H)Ï„ Î´Ï„

Ï„ =0

Combining with Eq.(11), this gives
>
Î²t+1
HÎ²t+1 =Î· 2

t
t
X
X

Î´Ï„>1 (I âˆ’ Î·H)Ï„1 H(I âˆ’ Î·H)Ï„2 Î´Ï„2

Ï„1 =0 Ï„2 =0

â‰¤Î· 2

t
t
X
X

kÎ´Ï„1 kk(I âˆ’ Î·H)Ï„1 H(I âˆ’ Î·H)Ï„2 kkÎ´Ï„2 k

Ï„1 =0 Ï„2 =0
t
t
X
X

â‰¤4Î· 2 G 2

k(I âˆ’ Î·H)Ï„1 H(I âˆ’ Î·H)Ï„2 k

Ï„1 =0 Ï„2 =0

Let the eigenvalues of H to be {Î»i }, then for any Ï„1 , Ï„2 â‰¥ 0, we know the eigenvalues of (I âˆ’ Î·H)Ï„1 H(I âˆ’ Î·H)Ï„2 are
{Î»i (1 âˆ’ Î·Î»i )Ï„1 +Ï„2 }. Let gt (Î») := Î»(1 âˆ’ Î·Î»)t , and setting its derivative to zero, we obtain:
âˆ‡gt (Î») = (1 âˆ’ Î·Î»)t âˆ’ tÎ·Î»(1 âˆ’ Î·Î»)tâˆ’1 = 0
We see that Î»?t =

1
(1+t)Î·

is the unique maximizer, and gt (Î») is monotonically increasing in (âˆ’âˆ, Î»?t ]. This gives:

k(I âˆ’ Î·H)Ï„1 H(I âˆ’ Î·H)Ï„2 k = max Î»i (1 âˆ’ Î·Î»i )Ï„1 +Ï„2 â‰¤ Î»Ì‚(1 âˆ’ Î· Î»Ì‚)Ï„1 +Ï„2 â‰¤
i

1
(1 + Ï„1 + Ï„2 )Î·

where Î»Ì‚ = min{`, Î»?Ï„1 +Ï„2 }. Therefore, we have:
>
Î²t+1
HÎ²t+1 â‰¤4Î· 2 G 2

t
t
X
X

k(I âˆ’ Î·H)Ï„1 H(I âˆ’ Î·H)Ï„2 k

Ï„1 =0 Ï„2 =0

â‰¤4Î·G 2

t
t
X
X
Ï„1 =0 Ï„2

1
dÎº
â‰¤ 8Î·T G 2 â‰¤ 8S 2 Î³cÌ‚ Â· logâˆ’1 ( )
1 + Ï„1 + Ï„2
Î´
=0

(13)

How to Escape Saddle Points Efficiently

The second last inequality is because by rearrange summation:
t
t
X
X
Ï„1 =0 Ï„2

2t

X
1
1
=
â‰¤ 2t + 1 < 2T
min{1 + Ï„, 2t + 1 âˆ’ Ï„ } Â·
1
+
Ï„
+
Ï„
1
+
Ï„
1
2
Ï„ =0
=0

Finally, substitue Eq.(12) and Eq.(13) into Eq.(8), this gives:
ï£¼
ï£±
s
s
ï£½
ï£² G cÌ‚ log( dÎº )
dÎº
dÎº
>
F cÌ‚ log( Î´ )
Î²t HÎ²t cÌ‚ log( Î´ )
Î´
,
,
, kÎ²t k
kut+1 k â‰¤14 Â· max
ï£¾
ï£³
Î³
Î³
Î³
â‰¤100(S Â· cÌ‚)
This finishes the induction as well as the proof of the lemma.
A.3.2. P ROOF OF L EMMA 17
In this Lemma we try to show if all the iterates from u0 are constrained in a small ball, iterates from w0 must be able
to decrease the function value. In order to do that, we keep track of vector v which is the difference between u and w.
Similar as before, we also decompose v into different eigenspaces. However, this time we only care about the projection
of v on the direction e1 and its orthognal subspace.
Again, recall notation H := âˆ‡2 f (xÌƒ), e1 as minimum eigenvector of H and quadratic approximation fËœy (x) as defined in
dÎº
Eq.(4). Since Î´ âˆˆ (0, dÎº
e ], we always have log( Î´ ) â‰¥ 1. W.L.O.G, set u0 = 0âˆšto be the origin. Define vt = wt âˆ’ ut , by
assumptions in Lemma 17, we have v0 = Âµ[S /(Îº Â· log( dÎº
Î´ ))]e1 , Âµ âˆˆ [Î´/(2 d), 1]. Now, consider the update equation
for wt :
ut+1 + vt+1 = wt+1 =wt âˆ’ Î·âˆ‡f (wt )
=ut + vt âˆ’ Î·âˆ‡f (ut + vt )
Z
=ut + vt âˆ’ Î·âˆ‡f (ut ) âˆ’ Î·

1


âˆ‡2 f (ut + Î¸vt )dÎ¸ vt

0

=ut + vt âˆ’ Î·âˆ‡f (ut ) âˆ’ Î·(H + âˆ†0t )vt
=ut âˆ’ Î·âˆ‡f (ut ) + (I âˆ’ Î·H âˆ’ Î·âˆ†0t )vt
R1
where âˆ†0t := 0 âˆ‡2 f (ut + Î¸vt )dÎ¸ âˆ’ H. By Hessian Lipschitz, we have kâˆ†0t k â‰¤ Ï(kut k + kvt k + kxÌƒk). This gives the
dynamic for vt satisfy:
vt+1 = (I âˆ’ Î·H âˆ’ Î·âˆ†0t )vt
(14)
Since kw0 âˆ’ xÌƒk â‰¤ ku0 âˆ’ xÌƒk + kv0 k â‰¤ S /(Îº Â· log( dÎº
Î´ )), directly applying Lemma 16, we know wt â‰¤ 100(S Â· cÌ‚) for all
t â‰¤ T . By condition of Lemma 17, we know kut k â‰¤ 100(S Â· cÌ‚) for all t < T . This gives:
kvt k â‰¤ kut k + kwt k â‰¤ 200(S Â· cÌ‚) for all t < T

(15)

This in sum gives for t < T :
kâˆ†0t k â‰¤ Ï(kut k + kvt k + kxÌƒk) â‰¤ Ï(300cÌ‚S + S /(Îº Â· log(

dÎº
))) â‰¤ ÏS (300cÌ‚ + 1)
Î´

On the other hand, denote Ïˆt be the norm of vt projected onto e1 direction, and Ï•t be the norm of vt projected onto
remaining subspace. Eq.(14) gives us:
q
Ïˆt+1 â‰¥(1 + Î³Î·)Ïˆt âˆ’ Âµ Ïˆt2 + Ï•2t
q
Ï•t+1 â‰¤(1 + Î³Î·)Ï•t + Âµ Ïˆt2 + Ï•2t

How to Escape Saddle Points Efficiently

where Âµ = Î·ÏS (300cÌ‚ + 1). We will now prove via induction that for all t < T :
Ï•t â‰¤ 4Âµt Â· Ïˆt

(16)

By hypothesis of Lemma 17, we know Ï•0 = 0, thus the base case of induction holds. Assume Eq.(16) is true for Ï„ â‰¤ t,
For t + 1 â‰¤ T , we have:


q
2
2
4Âµ(t + 1)Ïˆt+1 â‰¥4Âµ(t + 1) (1 + Î³Î·)Ïˆt âˆ’ Âµ Ïˆt + Ï•t
q
Ï•t+1 â‰¤4Âµt(1 + Î³Î·)Ïˆt + Âµ Ïˆt2 + Ï•2t
From above inequalities, we see that we only need to show:
q
(1 + 4Âµ(t + 1)) Ïˆt2 + Ï•2t â‰¤ 4(1 + Î³Î·)Ïˆt
By choosing

âˆš

cmax â‰¤

1
300cÌ‚+1

1
, 1 }, and Î· â‰¤ cmax /`, we have
min{ 2âˆš
2 4cÌ‚

p
4Âµ(t + 1) â‰¤ 4ÂµT â‰¤ 4Î·ÏS (300cÌ‚ + 1)cÌ‚T = 4 Î·`(300cÌ‚ + 1)cÌ‚ â‰¤ 1
This gives:
q
q
4(1 + Î³Î·)Ïˆt â‰¥ 4Ïˆt â‰¤ 2 2Ïˆt2 â‰¥ (1 + 4Âµ(t + 1)) Ïˆt2 + Ï•2t
which finishes the induction.

Now, we know Ï•t â‰¤ 4Âµt Â· Ïˆt â‰¤ Ïˆt , this gives:
Ïˆt+1 â‰¥ (1 + Î³Î·)Ïˆt âˆ’
where the last step follows from Âµ = Î·ÏS (300cÌ‚ + 1) â‰¤

âˆš

âˆš

2ÂµÏˆt â‰¥ (1 +

Î³Î·
)Ïˆt
2

cmax (300cÌ‚ + 1)Î³Î· Â· logâˆ’1 ( dÎº
Î´ )<

(17)
Î³Î·
âˆš .
2 2

Finally, combining Eq.(15) and (17) we have for all t < T :
Î³Î· t
) Ïˆ0
2
Î³Î· t S
dÎº
Î³Î· t Î´ S
dÎº
=(1 +
) c0
logâˆ’1 ( ) â‰¥ (1 +
) âˆš
logâˆ’1 ( )
2
Îº
Î´
2 2 d Îº
Î´

200(S Â· cÌ‚) â‰¥kvt k â‰¥ Ïˆt â‰¥ (1 +

This implies:

âˆš

âˆš

log(400 Îº Î´ d Â· cÌ‚ log( dÎº
1 log(400 Îº Î´ d Â· cÌ‚ log( dÎº
Î´ ))
Î´ ))
T <
â‰¤
â‰¤ (2 + log(400cÌ‚))T
Î³Î·
2
log(1 + 2 )
Î³Î·
dÎº
The last inequality is due to Î´ âˆˆ (0, dÎº
e ] we have log( Î´ ) â‰¥ 1. By choosing constant cÌ‚ to be large enough to satisfy
2 + log(400cÌ‚) â‰¤ cÌ‚, we will have T < cÌ‚T , which finishes the proof.

B. Improve Convergence by Local Structure
In this section, we show if the objective function has nice local structure (e.g. satisfies Assumptions A3.a or A3.b), then it
is possible to combine our analysis with the local analysis in order to get very fast convergence to a local minimum.
In particular, we prove Theorem 5.
Theorem 5. There exist absolute constant cmax such that: if f (Â·) satisfies A1, A2, and A3.a (or A3.b), then for any
Î´ > 0,  > 0, âˆ†f â‰¥ f (x0 ) âˆ’ f ? , constant c â‰¤ cmax , let Ëœ = min(Î¸, Î³ 2 /Ï), with probability 1 âˆ’ Î´, the output of
PGDli(x0 , `, Ï, Ëœ, c, Î´, âˆ†f , Î²) will be -close to X ? in iterations:




`(f (x0 ) âˆ’ f ? )
d`âˆ†f
Î²
Î¶
4
O
log
+
log
Ëœ2
Ëœ2 Î´
Î±


How to Escape Saddle Points Efficiently

Proof. Theorem 5 runs PGDli(x0 , `, Ï, Ëœ, c, Î´, âˆ†f , Î²). According to algorithm 3, we know it calls PGD(x0 , `, Ï, , c, Î´, âˆ†f )
first (denote its output as xÌ‚), then run standard gradient descent with step size Î²1 starting from xÌ‚.
By Corollary 4, we know xÌ‚ is already in the Î¶-neighborhood of X ? , where X ? is the set of local minima. Therefore, to
prove this theorem, we only need to show prove following two claims:
1. Suppose {xt } is the sequence of gradient descent starting from x0 = xÌ‚ with step size
Î¶-neighborhood of X ? .

1
Î²,

then xt is always in the

Î²
2. Local structure (assumption A3.a or A3.b) allows iterates to converge to points -close to X ? within O( Î±
log Î¶ )
iterations.

We will focus on Assumption A3.b (as we will later see Assumption A3.a is a special case of Assumption A3.b). Assume
xt is in Î¶-neighborhood of X ? , by gradient updates and the definition of projection, we have:
kxt+1 âˆ’ PX ? (xt+1 )k2 â‰¤kxt+1 âˆ’ PX ? (xt )k2 = kxt âˆ’ Î·âˆ‡f (xt ) âˆ’ PX ? (xt )k2
=kxt âˆ’ PX ? (xt )k2 âˆ’ 2Î·hâˆ‡f (xt ), xt âˆ’ PX ? (xt )i + Î· 2 kâˆ‡f (xt )k2
Î·
â‰¤kxt âˆ’ PX ? (xt )k2 âˆ’ Î·Î±kxt âˆ’ PX ? (xt )k2 + (Î· 2 âˆ’ )kâˆ‡f (x)k2
Î²
Î±
2
â‰¤(1 âˆ’ )kxt âˆ’ PX ? (xt )k
Î²
The second last inequality is due to (Î±, Î²)-regularity condition. The last inequality is because of the choice Î· =

1
Î².

There are two consequences of this calculation. First, it shows kxt+1 âˆ’ PX ? (xt+1 )k2 â‰¤ kxt âˆ’ PX ? (xt )k2 . As a result if
xt in Î¶-neighborhood of X ? , xt+1 is also in this Î¶-neighborhood. Since x0 is in the Î¶-neighborhood by Corollary 4, by
induction we know all later iterations are in the same neighborhood.
Now, since we know all the points xt are in the neighborhood, the equation also shows linear convergence rate (1 âˆ’ Î±
Î² ).
The initial distance is bounded by kx0 âˆ’ PX ? (x0 )k â‰¤ Î¶, therefore to converge to points -close to X ? , we only need the
following number of iterations:
Î²
Î¶
log(/Î¶)
= O( log ).
log(1 âˆ’ Î±/Î²)
Î±

This finishes the proof under Assumption A3.b.
Finally, we argue assumption A3.a implies A3.b. First, notice that if a function is locally strongly convex, then its local
minima are isolated: for any two points x, x0 âˆˆ X ? , the local region Bx (Î¶) and Bx0 (Î¶) must be disjoint (otherwise function
f (x) is strongly convex in connected domain Bx (Î¶) âˆª Bx0 (Î¶) but has two distinct local minima, which is impossible).
Therefore, W.L.O.G, it suffices to consider one perticular disjoint region, with unique local minimum we denote as x? ,
clearly, for all x âˆˆ Bx? (Î¶) we have PX ? (x) = x? .
Now by Î±-strong convexity:
f (x? ) â‰¥ f (x) + hâˆ‡f (x), x? âˆ’ xi +

Î±
kx âˆ’ x? k2
2

(18)

On the other hand, for any x in this Î¶-neighborhood, we already proved x âˆ’ Î²1 âˆ‡f (x) also in this Î¶-neighborhood. By
Î²-smoothness, we also have:
1
1
f (x âˆ’ âˆ‡f (x)) â‰¤ f (x) âˆ’
kâˆ‡f (x)k2
(19)
Î²
2Î²
Combining Eq.(18) and Eq.(19), and using the fact f (x? ) â‰¤ f (x âˆ’ Î²1 âˆ‡f (x)), we get:
hâˆ‡f (x), x âˆ’ x? i â‰¥
which finishes the proof.

Î±
1
kx âˆ’ x? k2 +
kâˆ‡f (x)k2
2
2Î²

How to Escape Saddle Points Efficiently

C. Geometric Structures of Matrix Factorization Problem
In this Section we investigate the global geometric structures of the matrix factorization problem. These properties are
summarized in Lemmas 6 and 7. Such structures allow us to apply our main Theorem and get fast convergence (as shown
in Theorem 8).
Note that our main results Theorems 3 and 5 are proved for functions f (Â·) whose input x is a vector. For the current
function in 2, though the input U âˆˆ RdÃ—r is a matrix, we can always vectorize it to be a vector in Rdr and apply our
results. However, for simplicity of presentation, we still write everything in matrix form (without explicit vectorization),
while the reader should keep in mind the operations are same if one vectorizes everything first.
Recall for vectors we use kÂ·k to denote the 2-norm, and for matrices we use kÂ·k and kÂ·kF to denote spectral norm, and
Frobenius norm respectively. Furthermore, we always use Ïƒi (Â·) to denote the i-th largest singular value of the matrix.
We first show how the geometric properties (Lemma 6 and Lemma 7) imply a fast convergence (Theorem 8).
Theorem 8. There exists an absolute constant cmax such that the following holds.
For matrix factorization (2), for any Î´ > 0 and constant c â‰¤ cmax , let Î“1/2 := 2 max{kU0 k, 3(Ïƒ1? )1/2 }, suppose we run
2
(Ïƒ ? )2
PGDli(U0 , 8Î“, 12Î“1/2 , 108Î“r 1/2 , c, Î´, rÎ“2 , 10Ïƒ1? ), then:
1. With probability 1, the iterates satisfy kUt k â‰¤ Î“1/2 for every t â‰¥ 0.
2. With probability 1 âˆ’ Î´, the output will be -close to global minima set X ? in
!
 4


Ïƒr?
Ïƒ1?
dÎ“
Î“
4
O r
log
+ ? log
Ïƒr?
Î´Ïƒr?
Ïƒr

iterations.
Proof of Theorem 8. Denote cÌƒmax to be the absolute constant allowed in Theorem 5. In this theorem, we let cmax =
min{cÌƒmax , 21 }, and choose any constant c â‰¤ cmax .
Theorem 8 consists of two parts. In part 1 we show that the iterations never bring the matrix to a very large norm, while in
part 2 we apply our main Theorem to get fast convergence. We will first prove the bound on number of iterations assuming
the bound on the norm. We will then proceed to prove part 1.
Part 2: Assume part 1 of the theorem is true i.e., with probability 1, the iterates satisfy kUt k â‰¤ Î“1/2 for every t â‰¥ 0. In
this case, although we are doing unconstrained optimization, we can still use the geometric properties that hold inside this
region. .
By Lemma 6 and 7, we know objective function Eq.(2) is 8Î“-smooth, 12Î“1/2 -Lipschitz Hessian,
1
( 24
(Ïƒr? )3/2 , 13 Ïƒr? , 13 (Ïƒr? )1/2 )-strict saddle, and holds ( 23 Ïƒr? , 10Ïƒ1? )-regularity condition in 13 (Ïƒr? )1/2 neighborhood of
local minima (also global minima) X ? . Furthermore, note f ? = 0 and recall Î“1/2 = 2 max{kU0 k, 3(Ïƒ1? )1/2 }, then, we
have:
? 2
>
? 2
f (U0 ) âˆ’ f ? = kU0 U>
0 âˆ’ M kF â‰¤ 2rkU0 U0 âˆ’ M k â‰¤

Thus, we can choose âˆ†f =
1/2

PGDli(U0 , 8Î“, 12Î“
in iterations:

,

rÎ“2
2 .

rÎ“2
.
2

Substituting the corresponding parameters from Theorem 5, we know by running

2
(Ïƒr? )2
, c, Î´, rÎ“2 , 10Ïƒ1? ),
108Î“1/2

with probability 1 âˆ’ Î´, the output will be -close to global minima set X ?
!
 4


Î“
dÎ“
Ïƒ1?
Ïƒr?
4
+ ? log
O r
log
.
Ïƒr?
Î´Ïƒr?
Ïƒr


Part 1: We will now show part 1 of the theorem. Recall PGDli (Algorithm 3) runs PGD (Algorithm 2) first, and then runs
gradient descent within 13 (Ïƒr? )1/2 neighborhood of X ? . It is easy to verify that 31 (Ïƒr? )1/2 neighborhood of X ? is a subset
of {U|kUk2 â‰¤ Î“}. Therefore, we only need to show that first phase PGD will not leave the region. Specifically, we now
use induction to prove the following for PGD:

How to Escape Saddle Points Efficiently

1. Suppose at iteration Ï„ we add perturbation, and denote UÌƒÏ„ to be the iterate before adding perturbation (i.e., UÏ„ =
UÌƒÏ„ + Î¾Ï„ , and UÌƒÏ„ = UÏ„ âˆ’1 âˆ’ Î·âˆ‡f (UÏ„ âˆ’1 )). Then, kUÌƒÏ„ k â‰¤ 12 Î“, and
2. kUt k â‰¤ Î“ for all t â‰¥ 0.
By choice of parameters of Algorithm 2, we know Î· =

c
8Î“ .

First, consider gradient descent step without perturbations:

?
kUt+1 k =kUt âˆ’ Î·âˆ‡f (Ut )k = kUt âˆ’ Î·(Ut U>
t âˆ’ M )Ut k
?
â‰¤kUt âˆ’ Î·Ut U>
t Ut k + Î·kM Ut k

â‰¤ max[Ïƒi (Ut ) âˆ’ Î·Ïƒi3 (Ut )] + Î·kM? Ut k
i

âˆš
For the first term, we know function f (t) = t âˆ’ Î·t3p
is monotonicallyâˆšincreasing in [0, 1/ 3Î·]. On the other hand, by
induction assumption, we also know kUt k â‰¤ Î“1/2 â‰¤ 8Î“/(3c) = 1/ 3Î·. Therefore, the max is taken when i = 1:
kUt+1 k â‰¤kUt k âˆ’ Î·kUt k3 + Î·kM? Ut k
â‰¤kUt k âˆ’ Î·(kUt k2 âˆ’ Ïƒ1? )kUt k.

(20)

We seperate our discussion into following cases.
Case kUt k > 12 Î“1/2 : In this case kUt k â‰¥ max{kU0 k, 3(Ïƒ1? )1/2 }. Recall Î“1/2 = 2 max{kU0 k, 3(Ïƒ1? )1/2 }. Clearly,
Î“ â‰¥ 36Ïƒ1? , we know:
1
c 1
( Î“ âˆ’ Ïƒ1? ) Î“1/2
8Î“ 4
2
c 1/2
= kUt k âˆ’ Î“ .
72

kUt+1 k â‰¤kUt k âˆ’ Î·(kUt k2 âˆ’ Ïƒ1? )kUt k â‰¤ kUt k âˆ’
â‰¤kUt k âˆ’

c 1
1
1
( Î“ âˆ’ Î“) Î“1/2
8Î“ 4
36 2

This means that in each iteration, the spectral norm would decrease by at least

c 1/2
.
72 Î“

Case kUt k â‰¤ 12 Î“1/2 : From (20), we know that as long as kUt k2 â‰¥ kM? k, we will always have kUt+1 k â‰¤ kUt k â‰¤
1 1/2
. For kUt k2 â‰¤ kM? k, we have:
2Î“
kUt+1 k â‰¤kUt k âˆ’ Î·(kUt k2 âˆ’ Ïƒ1? )kUt k = kUt k +

c ?
(Ïƒ âˆ’ kUt k2 )kUt k
8Î“ 1

c
((Ïƒ ? )1/2 + kUt k)kUt k
8Î“ 1
cÏƒ ?
â‰¤kUt k + ((Ïƒ1? )1/2 âˆ’ kUt k) Ã— 1 â‰¤ (Ïƒ1? )1/2
4Î“

â‰¤kUt k + ((Ïƒ1? )1/2 âˆ’ kUt k) Ã—

Thus, in this case, we always have kUt+1 k â‰¤ 21 Î“1/2 .
In conclusion, if we donâ€™t add perturbation in iteration t, we have:
â€¢ If kUt k > 21 Î“1/2 , then kUt+1 k â‰¤ kUt k âˆ’

c 1/2
.
72 Î“

â€¢ If kUt k â‰¤ 12 Î“1/2 , then kUt+1 k â‰¤ 21 Î“1/2 .
Now consider the iterations where we add perturbation. By the choice of radius of perturbation in Algorithm 2 , we increase
spectral norm by at most :
âˆš
c
(Ïƒr? )2
1
kÎ¾t k â‰¤ kÎ¾t kF â‰¤ 2
â‰¤ Î“1/2
Ï‡ 108Î“1/2 Â· 8Î“
2
The first inequality is because Ï‡ â‰¥ 1 and c â‰¤ 1. That is, if before perturbation we have kUÌƒt k â‰¤
kUÌƒt + Î¾t k â‰¤ Î“1/2 .

1 1/2
,
2Î“

then kUt k =

How to Escape Saddle Points Efficiently

On the other hand, according to Algorithm 2, once we add perturbation, we will not add perturbation for next tthres =
Ï‡Â·24Î“
24
48
1 1/2
}, tthres }:
c2 Ïƒ ? â‰¥ c2 â‰¥ c iterations. Let T = min{inf i {Ut+i |kUt+i k â‰¤ 2 Î“
r

kUt+T âˆ’1 k â‰¤ kUt k âˆ’

c 1/2
c(T âˆ’ 1)
Î“ (T âˆ’ 1) â‰¤ Î“1/2 (1 âˆ’
)
72
72

48
This gives T â‰¤ 36
c < c â‰¤ tthres . Let Ï„ > t be the next time when we add perturbation (Ï„ â‰¥ t + tthres ), we immediately
know kUT +i k â‰¤ 21 Î“1/2 for 0 â‰¤ i < Ï„ âˆ’ T and kUÌƒÏ„ k â‰¤ 21 Î“1/2 .

Finally, kU0 k â‰¤
theorem.

1 1/2
2Î“

by definition of Î“, so the initial condition holds. This finishes induction and the proof of the

In the next subsections we prove the geometric structures.
C.1. Smoothness and Hessian Lipschitz
Before we start proofs of lemmas, we first state some properties about gradient and Hessians. The gradient of the objective
function f (U) is
âˆ‡f (U) = 2(UU> âˆ’ M? )U.
Furthermore, we have the gradient and Hessian satisfy for any Z âˆˆ RdÃ—r :
hâˆ‡f (U), Zi = 2h(UU> âˆ’ M? )U, Zi, and
2

>

âˆ‡ f (U)(Z, Z) = kUZ +

ZU> k2F

>

(21)
?

>

+ 2hUU âˆ’ M , ZZ i.

(22)

Lemma 6. For any Î“ â‰¥ Ïƒ1? , inside the region {U|kUk2 < Î“}, f (U) defined in Eq.(2) is 8Î“-smooth and 12Î“1/2 -Hessian
Lipschitz.
Proof. Denote D = {U|kUk2 < Î“}, and recall Î“ â‰¥ Ïƒ1? .
Part 1: For any U, V âˆˆ D, we have:
kâˆ‡f (U) âˆ’ âˆ‡f (V)kF =2k(UU> âˆ’ M? )U âˆ’ (VV> âˆ’ M? )VkF


â‰¤2 kUU> U âˆ’ VV> VkF + kM? (U âˆ’ V)kF
â‰¤2 [3 Â· Î“kU âˆ’ VkF + Ïƒ1? kU âˆ’ VkF ] â‰¤ 8Î“ Â· kU âˆ’ VkF .
The last line is due to following decomposition and triangle inequality:
UU> U âˆ’ VV> V = UU> (U âˆ’ V) + U(U âˆ’ V)> V + (U âˆ’ V)V> V.

Part 2: For any U, V âˆˆ D, and any Z âˆˆ RdÃ—r , according to Eq.(22), we have:
|âˆ‡2 f (U)(Z, Z) âˆ’ âˆ‡2 f (V)(Z, Z)| = kUZ> + ZU> k2F âˆ’ kVZ> + ZV> k2F + 2hUU> âˆ’ VV> , ZZ> i .
{z
} |
{z
}
|
A

B

For term A, we have:
A =hUZ> + ZU> , (U âˆ’ V)Z> + Z(U âˆ’ V)> i + h(U âˆ’ V)Z> + Z(U âˆ’ V)> , VZ> + ZV> i
â‰¤kUZ> + ZU> kF k(U âˆ’ V)Z> + Z(U âˆ’ V)> kF + k(U âˆ’ V)Z> + Z(U âˆ’ V)> kF kVZ> + ZV> kF
â‰¤4kUkkZk2F kU âˆ’ VkF + 4kVkkZk2F kU âˆ’ VkF â‰¤ 8Î“1/2 kZk2F kU âˆ’ VkF .

How to Escape Saddle Points Efficiently

For term B, we have:
B â‰¤ 2kUU> âˆ’ VV> kF kZZ> kF â‰¤ 4Î“1/2 kZk2F kU âˆ’ VkF .
The inequality is due to following decomposition and triangle inequality:
UU> âˆ’ VV> = U(U âˆ’ V)> + (U âˆ’ V)V> .
Therefore, in sum we have:
max |âˆ‡2 f (U)(Z, Z) âˆ’ âˆ‡2 f (V)(Z, Z)| â‰¤ max 12Î“1/2 kZk2F kU âˆ’ VkF

Z:kZkF â‰¤1

Z:kZkF â‰¤1

â‰¤12Î“1/2 kU âˆ’ VkF .

C.2. Strict-Saddle Property and Local Regularity
Recall the gradient and Hessian of objective function is calculated as in Eq.(21) and Eq.(22). We first prove an elementary
inequality regarding to the trace of product of two symmetric PSD matrices. This lemma will be frequently used in the
proof.
Lemma 18. For A, B âˆˆ RdÃ—d both symmetric PSD matrices, we have:
Ïƒmin (A)tr(B) â‰¤ tr(AB) â‰¤ kAktr(B)
Proof. Let A = VDV> be the eigendecomposition of A, where D is diagonal matrix, and V is orthogonal matrix. Then
we have:
d
X
tr(AB) = tr(DV> BV) =
Dii (V> BV)ii .
i=1
>

Since B is PSD, we know V BV is also PSD, thus the diagonal entries are non-negative. That is, (V> BV)ii â‰¥ 0 for
all i = 1, . . . , d. Finally, the lemma follows from the fact that Ïƒmin (A) â‰¤ Dii â‰¤ kAk and tr(V> BV) = tr(BVV> ) =
tr(B).
Now, we are ready to prove Lemma 7.
Lemma 7. For f (U) defined in Eq.(2), all local minima are global minima. The set of global minima is X ? =
{V? R|RR> = R> R = I}. Furthermore, f (U) satisfies:
1
1. ( 24
(Ïƒr? )3/2 , 13 Ïƒr? , 31 (Ïƒr? )1/2 )-strict saddle property, and

2. ( 23 Ïƒr? , 10Ïƒ1? )-regularity condition in 13 (Ïƒr? )1/2 neighborhood of X ? .
Proof. Let us denote the set X ? := {V? R|RR> = R> R = I}, in the end of proof, we will show this set is the set of all
local minima (which is also global minima).
Throughout the proof of this lemma, we always focus on the first-order and second-order property for one matrix U. For
simplicity of calculation, when it is clear from the context, we denote U? = PX ? (U) and âˆ† = Uâˆ’PX ? (U). By definition
of X ? , we know U? = V? RU and âˆ† = U âˆ’ V? RU , where
RU =

argmin
R:RR> =R> R=I

kU âˆ’ V? Rk2F

We first prove following claim, which will used in many places across this proof:
U> U? = U> V? RU is a symmetric PSD matrix.

(23)

How to Escape Saddle Points Efficiently

This because by expanding the Frobenius norm, and letting the SVD of V?> U be ADB> , we have:
argmin
R:RR> =R> R=I

=

argmin

kU âˆ’ V? Rk2F =
âˆ’tr(U> V? R) =

R:RR> =R> R=I

argmin

âˆ’hU, V? Ri

R:RR> =R> R=I

argmin

âˆ’tr(DA> RB)

R:RR> =R> R=I

Since A, B, R are all orthonormal matrix, we know A> RB is also orthonormal matrix. Moreover for any orthonormal
matrix T, we have:
X
X
tr(DT) =
Dii Tii â‰¤
Dii
i

i

The last inequality is because Dii is singular value thus non-negative, and T is orthonormal, thus Tii â‰¤ 1. This means
the maximum of tr(DT) is achieved when T = I, i.e., the minimum of âˆ’tr(DA> RB) is achieved when R = AB> .
Therefore, U> V? RU = BDA> AB> = BDB> is symmetric PSD matrix.
Part 1: In order to show the strict saddle property, we only need to show that for any U satisfying kâˆ‡f (U)kF â‰¤
and kâˆ†kF = kU âˆ’ U? kF â‰¥ 13 (Ïƒr? )1/2 , we always have Ïƒmin (âˆ‡2 f (U)) â‰¤ âˆ’ 31 Ïƒr? .

1
? 3/2
24 (Ïƒr )

Letâ€™s consider Hessian âˆ‡2 (U) in the direction of âˆ† = U âˆ’ U? . Clearly, we have:
UU> âˆ’ M? = UU> âˆ’ (U âˆ’ âˆ†)(U âˆ’ âˆ†)> = (Uâˆ†> + âˆ†U> ) âˆ’ âˆ†âˆ†>
and by (21):
hâˆ‡f (U), âˆ†i =2h(UU> âˆ’ M? )U, âˆ†i = hUU> âˆ’ M? , âˆ†U> + Uâˆ†> i
=hUU> âˆ’ M? , UU> âˆ’ M? + âˆ†âˆ†> i
Therefore, by Eq.(22) and above two equalities, we have:
âˆ‡2 f (U)(âˆ†, âˆ†) =kUâˆ†> + âˆ†U> k2F + 2hUU> âˆ’ M? , âˆ†âˆ†> i
=kUU> âˆ’ M? + âˆ†âˆ†> k2F + 2hUU> âˆ’ M? , âˆ†âˆ†> i
=kâˆ†âˆ†> k2F âˆ’ 3kUU> âˆ’ M? k2F + 4hUU> âˆ’ M? , UU> âˆ’ M? + âˆ†âˆ†> i
=kâˆ†âˆ†> k2F âˆ’ 3kUU> âˆ’ M? k2F + 4hâˆ‡f (U), âˆ†i
Consider the first two terms, by expanding, we have:
3kUU> âˆ’ M? k2F âˆ’ kâˆ†âˆ†> k2F = 3k(U? âˆ†> + âˆ†U?> ) + âˆ†âˆ†> k2F âˆ’ kâˆ†âˆ†> k2F

=3 Â· tr 2U?> U? âˆ†> âˆ† + 2(U?> âˆ†)2 + 4U?> âˆ†âˆ†> âˆ† + (âˆ†> âˆ†)2 âˆ’ tr((âˆ†> âˆ†)2 )

=tr 6U?> U? âˆ†> âˆ† + 6(U?> âˆ†)2 + 12U?> âˆ†âˆ†> âˆ† + 2(âˆ†> âˆ†)2
âˆš
âˆš
âˆš
=tr((4 3 âˆ’ 6)U?> U? âˆ†> âˆ† + (12 âˆ’ 4 3)U?> (U? + âˆ†)âˆ†> âˆ† + 2( 3U?> âˆ† + âˆ†> âˆ†)2 )
âˆš
âˆš
â‰¥(4 3 âˆ’ 6)tr(U?> U? âˆ†> âˆ†) â‰¥ (4 3 âˆ’ 6)Ïƒr? kâˆ†k2F
where the second last inequality is because U?> (U? + âˆ†)âˆ†> âˆ† = U?> Uâˆ†> âˆ† is the product of two symmetric PSD
matrices (thus its trace is non-negative); the last inequality is by Lemma 18.
Finally, in case we have kâˆ‡f (U)kF â‰¤

1
? 3/2
24 (Ïƒr )

and kâˆ†kF = kU âˆ’ U? kF â‰¥ 13 (Ïƒr? )1/2

âˆš
hâˆ‡f (U), âˆ†i
1
âˆ‡2 f (U)(âˆ†, âˆ†) â‰¤ âˆ’(4 3 âˆ’ 6)Ïƒr? + 4
2
kâˆ†kF
kâˆ†k2F
âˆš
âˆš
kâˆ‡f (U)kF
1
â‰¤ âˆ’(4 3 âˆ’ 6.5)Ïƒr? â‰¤ âˆ’ Ïƒr?
â‰¤ âˆ’ (4 3 âˆ’ 6)Ïƒr? + 4
kâˆ†kF
3

Ïƒmin (âˆ‡2 f (U)) â‰¤

How to Escape Saddle Points Efficiently

Part 2: In 13 (Ïƒr? )1/2 neigborhood of X ? , by definition, we know,
kâˆ†k2F = kU âˆ’ U? k2F â‰¤
Clearly, by Weylâ€™s inequality, we have kUk â‰¤ kU? k + kâˆ†k â‰¤
Moreover, since U?> U is symmetric matrix, we have:

1 ?
Ïƒ .
9 r

4
? 1/2
,
3 (Ïƒ1 )

and Ïƒr (U) â‰¥ Ïƒr (U? ) âˆ’ kâˆ†k â‰¥

2
? 1/2
.
3 (Ïƒr )


1
Ïƒr (U> U? + U?> U)
2

1
â‰¥ Ïƒr (U> U + U?> U? ) âˆ’ k(U âˆ’ U? )> (U âˆ’ U? )k
2

1
â‰¥ Ïƒr (U> U) + Ïƒr (U?> U? ) âˆ’ kâˆ†k2F
2
4 1
2
1
â‰¥ (1 + âˆ’ )Ïƒr? = Ïƒr? .
2
9 9
3

Ïƒr (U?> U) =

At a highlevel, we will prove (Î±, Î²)-regularity property (1) by proving that:
1. hâˆ‡f (x), x âˆ’ PX ? (x)i â‰¥ Î±kx âˆ’ PX ? (x)k2 , and
2. hâˆ‡f (x), x âˆ’ PX ? (x)i â‰¥

1
2
Î² kâˆ‡f (x)k .

According to (21), we know:
hâˆ‡f (U), U âˆ’ PX ? (U)i =2h(UU> âˆ’ M? )U, âˆ†i = 2hUâˆ†> + âˆ†U?> , âˆ†U> i
=2(tr(Uâˆ†> Uâˆ†> ) + tr(âˆ†U?> Uâˆ†> ))
=2(kâˆ†> Uk2F + tr(U?> Uâˆ†> âˆ†)).

(24)

The last equality is because âˆ†> U is symmetric matrix. Since U?> U is symmetric PSD matrix, and recall Ïƒr (U?> U) â‰¥
2 ?
3 Ïƒr , by Lemma 18 we have:
hâˆ‡f (U), U âˆ’ PX ? (U)i â‰¥ Ïƒr (U?> U)tr(âˆ†> âˆ†) â‰¥

2 ?
Ïƒ kâˆ†k2F .
3 r

(25)

On the other hand, we also have:
kâˆ‡f (U)k2F =4h(UU> âˆ’ M? )U, (UU> âˆ’ M? )Ui
=4h(Uâˆ†> + âˆ†U?> )U, (Uâˆ†> + âˆ†U?> )Ui
ï£«

ï£¶

=4 ï£­tr[(âˆ†> UU> âˆ†)U> U] + 2tr[âˆ†> UU> U? âˆ†> U] + tr(U?> UU> U? âˆ†> âˆ†)ï£¸ .
{z
} |
{z
}
|
A

B

>

For term A, by Lemma 18, and âˆ† U being a symmetric matrix, we have:
A â‰¤ kU> Ukkâˆ†> Uk2F + 2kU> U? kkâˆ†> Uk2F â‰¤ (

16 8 ? > 2
+ )Ïƒ1 kâˆ† UkF â‰¤ 5Ïƒ1? kâˆ†> Uk2F
9
3

For term B, by Eq.(23) we can denote C = U?> U = U> U? which is symmetric PSD matrix, by Lemma 18, we have:
B =tr(C2 âˆ†> âˆ†) = tr(C(C1/2 âˆ†> âˆ†C1/2 ))
â‰¤kCktr(C1/2 âˆ†> âˆ†C1/2 ) = kCktr(Câˆ†> âˆ†) â‰¤

4 ?
Ïƒ tr(U?> Uâˆ†> âˆ†).
3 1

Combining with (24) we have:
kâˆ‡f (U)k2F â‰¤ Ïƒ1? (20kâˆ†> Uk2F +

16
tr(U?> Uâˆ†> âˆ†)) â‰¤ 10Ïƒ1? hâˆ‡f (U), U âˆ’ PX ? (U)i.
3

(26)

How to Escape Saddle Points Efficiently

Combining (25) and (26), we have:
hâˆ‡f (U), U âˆ’ PX ? (U)i â‰¥

1 ?
1
kâˆ‡f (U)k2F .
Ïƒ kU âˆ’ PX ? (U)k2F +
3 r
20Ïƒ1?

