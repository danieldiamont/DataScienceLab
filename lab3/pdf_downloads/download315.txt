Supplementary Material for:
Recursive Partitioning for Personalization using Observational Data

Nathan Kallus 1

Omitted Proofs
Proof of Theorem 1. By Asn. 1, we have
E [Y | X = x, T = t] = E [Y (T ) | X = x, T = t]
= E [Y (t) | X = x, T = t]
= E [Y (t) | X = x]

(definition of Y = Y (T ))
(conditioned on T = t)
(Asn. 1).

Consider a realization of the data and X = x where convergence occurs for all t âˆˆ [m]. Let

(x) = inf{Î¶ : s âˆˆ [m], Î¶ = E [Y | X = x, T = s] âˆ’ mintâˆˆ[m] E [Y | X = x, T = t] > 0},
where inf(âˆ…) = âˆž. By assumption of convergence at this realization of the data and X = x, we have that eventually for all t âˆˆ [m], |ÂµÌ‚t,nt (x) âˆ’ E [Y | X = x, T = t]| < (x)/2, at which point we must necessarily also have
Ï„Ì‚n (x) âˆˆ arg mintâˆˆ[m] E [Y | X = x, T = t] = arg mintâˆˆ[m] E [Y (t) | X = x]. By assumption of pointwise consistency
and because the intersection of finitely many a.s. events is a.s., the set of such realization of the data and X = x have
probability 1.
Proof of Theorem 2. First note that, given any x with P (T = t | X = x) > 0, we have
h
i
h
i
h
i
Y I[T =t]
Y I[T =t]
Y I[T =t]
I[T =t]|X=x]
E [Y | X = x, T = t] = E[Y
=
E
|
X
=
x
=
E
|
X
=
x
=
E
|
X
=
x
.
P(T =t|X=x)
Ï†(t,x)
Ï†(T,X)
Q
Therefore, since P (T = t | X) > 0 almost surely,
R(Ï„ ) = E [Y (Ï„ (X))] = E [E [Y (Ï„ (X)) | X]]
= E [E [Y (Ï„ (X)) | X, T = Ï„ (X)]]
= E [E [Y | X, T = Ï„ (X)]]
= E [E [Y I [T = Ï„ (X)]/Q | X]]
= E [Y I [T = Ï„ (X)]/Q]

(iterated expectations)
(Asn. 1)
(definition of Y )
(above observation)
(iterated expectations) .

Proof of Theorem 4. We start with 1vA. Restrict to x such that Ï†(s, x) > 0 âˆ€s (almost everywhere). Let Âµ(t, x) =
E [Y (t) | X = x]. Under Asn. 1,
Î´ tvA (x) = E [Y | X = x, T = t] âˆ’ P
E [Y | X = x, T 6= t]
= E [Y | X = x, T = t] âˆ’ s6=t E [Y | X = x, T = s] P (T = s | X = x, T 6= t)
P
P
= Âµ(t, x) âˆ’ s6=t Ï†(s, x)Âµ(s, x)/ s6=t Ï†(s, x).
1

School of Operations Research and Information Engineering and Cornell Tech, Cornell University. Correspondence to: Nathan
Kallus <kallus@cornell.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplementary Material for: Recursive Partitioning for Personalization using Observational Data

Since Ï†(s, x) > 0, itâ€™s clear that Î´ tvA (x) â‰¤ Î´ svA (x) âˆ€s if and only if Âµ(t, x) â‰¤ Âµ(s, x) âˆ€s. The rest of the proof for 1vA
follows the same
 way as Thm. 1, showing that, under the assumption of pointwise consistent estimation, the estimation


gap suptâˆˆ[m] Î´Ì‚ntvA (x) âˆ’ Î´ tvA (x) is eventually smaller than half the decision gap, 1vA (x) = inf{Î¶ : s âˆˆ [m], Î¶ =

Î´ svA (x) âˆ’ mintâˆˆ[m] Î´ tvA (x) > 0}, a.s. and for almost everywhere x.
Next, we deal with 1v1-A. Fix x. Fix any tm âˆˆ arg maxtâˆˆ[m] Âµ(t, x). Let Î´ tvmin (x) = mins6=t Î´ tvs (x). If t, s 6= tm , then
Î´ tvmin (x)âˆ’Î´ svmin (x) = Âµ(t, x)âˆ’Âµ(s, x). On the other hand, for any t âˆˆ [m], we always have both Âµ(t, x)âˆ’Âµ(tm , x) â‰¤ 0
and Î´ tvmin (x) âˆ’ Î´ tm vmin (x) â‰¤ 0. Therefore, we have
t âˆˆ arg mintâˆˆ[m] Âµ(t, x) â‡â‡’ Âµ(t, x) âˆ’ Âµ(s, x) â‰¤ 0 âˆ€s 6= t â‡â‡’ Âµ(t, x) âˆ’ Âµ(s, x) â‰¤ 0 âˆ€s 6= t, tm
â‡â‡’ Î´ tvmin (x) âˆ’ Î´ svmin (x) â‰¤ 0 âˆ€s 6= t, tm â‡â‡’ Î´ tvmin (x) âˆ’ Î´ svmin (x) â‰¤ 0 âˆ€s 6= t
â‡â‡’ t âˆˆ arg mintâˆˆ[m] Î´ tvmin (x).


 tvmin

tvmin
â‰¤
Î´Ì‚
(x)
âˆ’
Î´
(x)
Let
Î´Ì‚ntvmin (x)
=
mins6=t Î´Ì‚ntvs
(x)
and
note
that
sup


tâˆˆ[m] n
t +ns



 tvs
suptâˆˆ[m],sâˆˆ[m] Î´Ì‚nt +ns (x) âˆ’ Î´ tvs (x), which converges to zero under pointwise consistency. The rest of the proof
for 1v1-A follows as above, showing that this estimation gap is eventually smaller than half the decision gap,
1v1-A (x) = inf{Î¶ : s âˆˆ [m], Î¶ = Î´ svmin (x) âˆ’ mintâˆˆ[m] Î´ tvmin (x) > 0}, a.s. and for almost everywhere x.
Next,
we deal with 1v1-B.
Fix x and a realization of the data where convergence holds for all t 6= hs. Then, eventually


i

 tvs
tvs
tvs
(x) < 0 =
(x)
âˆ’
Î´
(x)
â‰¤
|Î´
(x)| /2 for all t 6= s such that Î´ tvs (x) 6= 0. That is, eventually I Î´Ì‚ntvs
Î´Ì‚

 nt +ns
t +ns
P
I [Î´ tvs (x) < 0] for all t 6= s such that Î´ tvs (x) 6= 0. Restrict to such large enough n. Let kt (x) = t6=s I [Î´ tvs (x) < 0],
i
h


P
kÌ‚t (x) = t6=s I Î´Ì‚ntvs
(x) < 0 , and kmin (x) = arg mintâˆˆ[m] Âµ(t, x). Then, t âˆˆ arg mintâˆˆ[m] Âµ(t, x) â‡â‡’ kt (x) =
t +ns
i
h
P
(x)
<
0
.
m âˆ’ kmin (x) â‡â‡’ kÌ‚t (x) â‰¥ m âˆ’ kmin (x) â‡= t âˆˆ arg maxtâˆˆ[m] s6=t I Î´Ì‚ntvs
t +ns
Proof of Theorem 5. By random sampling, (Xij , Tij , Yij (1), . . . , Yij (m)) are distributed iid as (X, T, Y (1), . . . , Y (m))
is in population. For j âˆˆ [ntest ], let ijt be ij â€™s match for treatment t, or ij if Tij = t. Under exact matching,
Y
Pijtm(1), . . . , Yijt (m) | Xji is distributed the same as Yij (1), . . . , Yij (m) | Xji , Tji = t. By writing YÌ‚ij t = Yijt =
s=1 I [t = s] Yijs (s), we see that
 Pm 


E[YÌ‚ij Ï„ (Xij ) ] = E E
s=1 I s = Ï„ (Xij ) Yijs (s) | Xij





Pm
= s=1 E I s = Ï„ (Xij ) E Yijs (s) | Xji



Pm
= s=1 E I [s = Ï„ (Xi )] E Yij (s) | Xi , Ti = s



Pm
= s=1 E I [s = Ï„ (Xi )] E Yij (s) | Xi
 Pm

=E E
s=1 I [s = Ï„ (Xi )] Yij (s) | Xi

= E Yij (Ï„ (Xij ))

(iterated expectation)
(linearity)
(exact matching)
(Asn. 1)
(linearity)
(iterated expectation)

