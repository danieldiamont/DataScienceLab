Supplementary Material for:
Recursive Partitioning for Personalization using Observational Data

Nathan Kallus 1

Omitted Proofs
Proof of Theorem 1. By Asn. 1, we have
E [Y | X = x, T = t] = E [Y (T ) | X = x, T = t]
= E [Y (t) | X = x, T = t]
= E [Y (t) | X = x]

(definition of Y = Y (T ))
(conditioned on T = t)
(Asn. 1).

Consider a realization of the data and X = x where convergence occurs for all t ∈ [m]. Let

(x) = inf{ζ : s ∈ [m], ζ = E [Y | X = x, T = s] − mint∈[m] E [Y | X = x, T = t] > 0},
where inf(∅) = ∞. By assumption of convergence at this realization of the data and X = x, we have that eventually for all t ∈ [m], |µ̂t,nt (x) − E [Y | X = x, T = t]| < (x)/2, at which point we must necessarily also have
τ̂n (x) ∈ arg mint∈[m] E [Y | X = x, T = t] = arg mint∈[m] E [Y (t) | X = x]. By assumption of pointwise consistency
and because the intersection of finitely many a.s. events is a.s., the set of such realization of the data and X = x have
probability 1.
Proof of Theorem 2. First note that, given any x with P (T = t | X = x) > 0, we have
h
i
h
i
h
i
Y I[T =t]
Y I[T =t]
Y I[T =t]
I[T =t]|X=x]
E [Y | X = x, T = t] = E[Y
=
E
|
X
=
x
=
E
|
X
=
x
=
E
|
X
=
x
.
P(T =t|X=x)
φ(t,x)
φ(T,X)
Q
Therefore, since P (T = t | X) > 0 almost surely,
R(τ ) = E [Y (τ (X))] = E [E [Y (τ (X)) | X]]
= E [E [Y (τ (X)) | X, T = τ (X)]]
= E [E [Y | X, T = τ (X)]]
= E [E [Y I [T = τ (X)]/Q | X]]
= E [Y I [T = τ (X)]/Q]

(iterated expectations)
(Asn. 1)
(definition of Y )
(above observation)
(iterated expectations) .

Proof of Theorem 4. We start with 1vA. Restrict to x such that φ(s, x) > 0 ∀s (almost everywhere). Let µ(t, x) =
E [Y (t) | X = x]. Under Asn. 1,
δ tvA (x) = E [Y | X = x, T = t] − P
E [Y | X = x, T 6= t]
= E [Y | X = x, T = t] − s6=t E [Y | X = x, T = s] P (T = s | X = x, T 6= t)
P
P
= µ(t, x) − s6=t φ(s, x)µ(s, x)/ s6=t φ(s, x).
1

School of Operations Research and Information Engineering and Cornell Tech, Cornell University. Correspondence to: Nathan
Kallus <kallus@cornell.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplementary Material for: Recursive Partitioning for Personalization using Observational Data

Since φ(s, x) > 0, it’s clear that δ tvA (x) ≤ δ svA (x) ∀s if and only if µ(t, x) ≤ µ(s, x) ∀s. The rest of the proof for 1vA
follows the same
 way as Thm. 1, showing that, under the assumption of pointwise consistent estimation, the estimation


gap supt∈[m] δ̂ntvA (x) − δ tvA (x) is eventually smaller than half the decision gap, 1vA (x) = inf{ζ : s ∈ [m], ζ =

δ svA (x) − mint∈[m] δ tvA (x) > 0}, a.s. and for almost everywhere x.
Next, we deal with 1v1-A. Fix x. Fix any tm ∈ arg maxt∈[m] µ(t, x). Let δ tvmin (x) = mins6=t δ tvs (x). If t, s 6= tm , then
δ tvmin (x)−δ svmin (x) = µ(t, x)−µ(s, x). On the other hand, for any t ∈ [m], we always have both µ(t, x)−µ(tm , x) ≤ 0
and δ tvmin (x) − δ tm vmin (x) ≤ 0. Therefore, we have
t ∈ arg mint∈[m] µ(t, x) ⇐⇒ µ(t, x) − µ(s, x) ≤ 0 ∀s 6= t ⇐⇒ µ(t, x) − µ(s, x) ≤ 0 ∀s 6= t, tm
⇐⇒ δ tvmin (x) − δ svmin (x) ≤ 0 ∀s 6= t, tm ⇐⇒ δ tvmin (x) − δ svmin (x) ≤ 0 ∀s 6= t
⇐⇒ t ∈ arg mint∈[m] δ tvmin (x).


 tvmin

tvmin
≤
δ̂
(x)
−
δ
(x)
Let
δ̂ntvmin (x)
=
mins6=t δ̂ntvs
(x)
and
note
that
sup


t∈[m] n
t +ns



 tvs
supt∈[m],s∈[m] δ̂nt +ns (x) − δ tvs (x), which converges to zero under pointwise consistency. The rest of the proof
for 1v1-A follows as above, showing that this estimation gap is eventually smaller than half the decision gap,
1v1-A (x) = inf{ζ : s ∈ [m], ζ = δ svmin (x) − mint∈[m] δ tvmin (x) > 0}, a.s. and for almost everywhere x.
Next,
we deal with 1v1-B.
Fix x and a realization of the data where convergence holds for all t 6= hs. Then, eventually


i

 tvs
tvs
tvs
(x) < 0 =
(x)
−
δ
(x)
≤
|δ
(x)| /2 for all t 6= s such that δ tvs (x) 6= 0. That is, eventually I δ̂ntvs
δ̂

 nt +ns
t +ns
P
I [δ tvs (x) < 0] for all t 6= s such that δ tvs (x) 6= 0. Restrict to such large enough n. Let kt (x) = t6=s I [δ tvs (x) < 0],
i
h


P
k̂t (x) = t6=s I δ̂ntvs
(x) < 0 , and kmin (x) = arg mint∈[m] µ(t, x). Then, t ∈ arg mint∈[m] µ(t, x) ⇐⇒ kt (x) =
t +ns
i
h
P
(x)
<
0
.
m − kmin (x) ⇐⇒ k̂t (x) ≥ m − kmin (x) ⇐= t ∈ arg maxt∈[m] s6=t I δ̂ntvs
t +ns
Proof of Theorem 5. By random sampling, (Xij , Tij , Yij (1), . . . , Yij (m)) are distributed iid as (X, T, Y (1), . . . , Y (m))
is in population. For j ∈ [ntest ], let ijt be ij ’s match for treatment t, or ij if Tij = t. Under exact matching,
Y
Pijtm(1), . . . , Yijt (m) | Xji is distributed the same as Yij (1), . . . , Yij (m) | Xji , Tji = t. By writing Ŷij t = Yijt =
s=1 I [t = s] Yijs (s), we see that
 Pm 


E[Ŷij τ (Xij ) ] = E E
s=1 I s = τ (Xij ) Yijs (s) | Xij





Pm
= s=1 E I s = τ (Xij ) E Yijs (s) | Xji



Pm
= s=1 E I [s = τ (Xi )] E Yij (s) | Xi , Ti = s



Pm
= s=1 E I [s = τ (Xi )] E Yij (s) | Xi
 Pm

=E E
s=1 I [s = τ (Xi )] Yij (s) | Xi

= E Yij (τ (Xij ))

(iterated expectation)
(linearity)
(exact matching)
(Asn. 1)
(linearity)
(iterated expectation)

