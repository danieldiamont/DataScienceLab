Appendix
A. Details of the Proof
Proof of Theorem 2 We assume the optimization starts with an initialized weights w0 . t is denoted as the iteration index.
Let wgt and wst be the model parameter updated by our omniscient teacher and SGD, respectively. We first consider the
case where t = 1. For SGD, the first gradient update ws1 is



âˆ‚`( w0 , xs , ys )
1
0
.
(10)
w s = w âˆ’ Î·t
âˆ‚w0
Then we compute the difference between ws1 and wâˆ— :

2

 0 

2 
 1
âˆ‚`(
w
,
x
,
y)


0
âˆ—
âˆ—
ws âˆ’ w  = w âˆ’ Î·t
âˆ’
w

2


âˆ‚w0
2
(11)

2
*
+




 0 


0
 0
2
âˆ‚`(
w
,
x
,
y)
âˆ‚`(
w
,
x
,
y)


âˆ—
2
0
âˆ—
= w âˆ’ w 2 + Î·t 
 âˆ’ 2Î·t w âˆ’ w ,


âˆ‚w0
âˆ‚w0
2

Because the omniscient teacher is to minimize last two term, so we are guaranteed to have


 1

wg âˆ’ wâˆ— 2 â‰¤ ws1 âˆ’ wâˆ— 2 .
(12)
2
2




2
2
So with the same initialization wg0 = ws0 , wg1 âˆ’ wâˆ— 2 â‰¤ ws1 âˆ’ wâˆ— 2 is always true. Then we consider the case where
t = k, k â‰¥ 1. We first compute the difference between wgk+1 and wâˆ— :

2




 k+1
2 
âˆ‚`( wgk , x , y)


âˆ—
k
âˆ—
wg âˆ’ w  = wg âˆ’ Î·t
âˆ’
w

k+1
2


âˆ‚w
2
2

*
+

 k 

 k 
 

 k

,
x
,
y)
âˆ‚`(
w
,
x
,
y)
âˆ‚`(
w
2


g
g
= wg âˆ’ wâˆ— 2 + min Î·t2 
 âˆ’ 2Î·t wgk âˆ’ wâˆ— ,


âˆ‚wgk
âˆ‚wgk
{x,y}
(13)
2


*
+






2
 âˆ‚`( wk , xk , y k ) 
2

âˆ‚`( wgk , xkâˆ— , yâˆ—k )

g
âˆ—
âˆ— 
k
âˆ—
= wgk âˆ’ wâˆ— 2 + Î·t2 
âˆ’
2Î·
w
âˆ’
w
,

t
g


âˆ‚wgk
âˆ‚wgk
2
 k

2
= wg âˆ’ wâˆ— 2 âˆ’ T V (wgk )
where xkâˆ— , yâˆ—k is the sample selected by the omniscient teacher in the k-th iteration. Using the given conditions, we can
bound the difference between wsk+1 and wâˆ— from below:

2

 k s s


 k+1

,
x
,
y
)
âˆ‚`(
w
2

s
âˆ—
ws âˆ’ wâˆ—  = wsk âˆ’ Î·t
âˆ’
w

2


âˆ‚wsk
2

*

 k k k 

 k k k +

2
(14)
 k

âˆ— 2
2  âˆ‚`( ws , xs , ys ) 
k
âˆ— âˆ‚`( ws , xs , ys )

= ws âˆ’ w 2 + Î·t 
 âˆ’ 2Î·t ws âˆ’ w ,
k
k


âˆ‚ws
âˆ‚ws
2
 k

2
â‰¥ ws âˆ’ wâˆ— 2 âˆ’ T V (wsk )
where xks , ysk is the sample selected by the random teacher in the k-th iteration. Comparing Eq. 13 and Eq. 14 and using

2 
2
the condition in the theorem, the following inequality always holds under the condition wgk âˆ’ wâˆ— 2 â‰¤ wsk âˆ’ wâˆ— 2 :
 k+1







ws âˆ’ wâˆ— 2 = wsk âˆ’ wâˆ— 2 âˆ’ T V (wsk ) â‰¥ wgk âˆ’ wâˆ— 2 âˆ’ T V (wgk ) = wgk+1 âˆ’ wâˆ— 2 .
(15)
2
2
2
2
 1




2
2
2
Further because we already know that wg âˆ’ wâˆ— 2 â‰¤ ws1 âˆ’ wâˆ— 2 , using induction we can conclude that wgt âˆ’ wâˆ— 2
2

will be always not larger than kwst âˆ’ wâˆ— k2 (t can be any iteration). Therefore, in each iteration the omniscient teacher can
always converge not slower than random teacher (SGD).

Iterative Machine Teaching

Proof of Proposition 3 Consider the square loss `(hw, xi , y) = (hw, xi âˆ’ y)2 , we have âˆ‚`(hw,xi,y)
= 2(hw, xi âˆ’ y)x.
âˆ‚w
2
2
Suppose we are given two initializations w1 , w2 satisfying kw1 âˆ’ wâˆ— k2 â‰¤ kw2 âˆ’ wâˆ— k2 . For square loss, we first write out
2

2

kw1 âˆ’ wâˆ— k âˆ’ T V (w1 ) = kw1 âˆ’ wâˆ— k + min {Î·t2 T1 (x, y|w1 ) âˆ’ 2Î·t T2 (x, y|w1 )}
xâˆˆX ,yâˆˆY
2


 
âˆ—
âˆ—


âˆ— âˆ‚`(hw1 , x i , y )
âˆ— 2
2  âˆ‚`(hw1 , xi , y) 
= kw1 âˆ’ w k + min Î·t 
 âˆ’ 2Î·t w1 âˆ’ w ,
âˆ‚w1
âˆ‚w1
{x,y}
2
(
R
R
2
âˆ— 2
âˆ—
2( kw1 âˆ’wâˆ— k ) kw1 âˆ’ w k (w1 âˆ’ w ), if kw1 âˆ’wâˆ— k < Î·1t
2
= kw1 âˆ’ wâˆ— k +
2
R
1
âˆ’ kw1 âˆ’ wâˆ— k , if kw1 âˆ’w
âˆ—k â‰¥ Î·
t

(16)

Similarly for w2 , we have
2

kw2 âˆ’ wâˆ— k âˆ’ T V (w2 )
(
R
2
âˆ— 2
âˆ—
2( kw2 âˆ’w
âˆ— k ) kw2 âˆ’ w k (w2 âˆ’ w ), if
âˆ— 2
= kw2 âˆ’ w k +
2
R
1
âˆ’ kw2 âˆ’ wâˆ— k , if kw2 âˆ’w
âˆ—k â‰¥ Î·
t

R
kw2 âˆ’wâˆ— k

<

1
Î·t

(17)

There will be three scenarios to consider: (1) RÎ·t â‰¤ kw1 âˆ’ wâˆ— k â‰¤ kw2 âˆ’ wâˆ— k; (2) kw1 âˆ’ wâˆ— k â‰¤ RÎ·t â‰¤ kw2 âˆ’ wâˆ— k; (3)
kw1 âˆ’ wâˆ— k â‰¤ kw2 âˆ’ wâˆ— k â‰¤ RÎ·t . It is easy to verify that under all three scenarios, we have
2

kw1 âˆ’ wâˆ— k âˆ’ T V (w1 ) â‰¤

2

kw2 âˆ’ wâˆ— k âˆ’ T V (w2 )

(18)

To simplify notations, we denote Î²(hw,xi,y) = âˆ‡hw,xi ` (hw, xi , y) for a loss function `(Â·, Â·) in the following proof. For
omniscient teacher, (xÌ‚, yÌ‚) denotes a specific construction of (x, y). Notice that (xÌƒ, yÌƒ) will not be used in omniscient teacher
case to avoid ambiguity, since the student and the teacher use the same representation space.
Proof of Theorem 4

At t-step, the omniscient teacher selects the samples via optimization


 




 
min Î· 2 kâˆ‡wt ` wt , x , y k2 âˆ’ 2Î· wt âˆ’ wâˆ— , âˆ‡wt ` wt , x , y .
xâˆˆX ,yâˆˆY

We denote xÌ‚ = Î³ (w âˆ’ wâˆ— ) and yÌ‚ âˆˆ Y, since Î³ (w âˆ’ wâˆ— ) âˆˆ X , we have


 




 
min Î· 2 kâˆ‡wt ` wt , x , y k2 âˆ’ 2Î· wt âˆ’ wâˆ— , âˆ‡wt ` wt , x , y
xâˆˆX ,yâˆˆY


2
2
â‰¤
Î· 2 Î²(hw
kwt âˆ’ wâˆ— k22 .
t ,xÌ‚i,yÌ‚) Î³ âˆ’ 2Î·Î²(hw t ,xÌ‚i,yÌ‚) Î³
t

(19)
(20)

Plug Eq. (19) into the recursion Eq. (3), we have

2
 t
 t+1


âˆ‚`(hw, xi , y)
âˆ— 2
âˆ—

w
âˆ’ w 2 = min w âˆ’ Î·
âˆ’w 
xâˆˆX ,yâˆˆY
âˆ‚w
2

2


t
t
 t


âˆ‚`(hw , xi , y) 
2
 âˆ’ 2Î· wt âˆ’ wâˆ— , âˆ‚`(hw , xi , y)
= w âˆ’ wâˆ— 2 + min Î· 2 


xâˆˆX ,yâˆˆY
âˆ‚wt
âˆ‚wt
2


2
2
2
â‰¤ 1 + Î· 2 Î²(hw
kwt âˆ’ wâˆ— k22 = 1 âˆ’ Î·Î²(hwt ,Î³(wt âˆ’wâˆ— )i,yÌ‚) Î³ kwt âˆ’ wâˆ— k22 .
t ,xÌ‚i,yÌ‚) Î³ âˆ’ 2Î·Î²(hw t ,xÌ‚i,yÌ‚) Î³
(21)
First we let Î½(Î³) = minw,y Î³âˆ‡hw,Î³(wâˆ’wâˆ— )i ` (hw, Î³ (w âˆ’ wâˆ— )i , y). Then we have the condition 0 < Î½(Î³) â‰¤
Î³Î²(hw,Î³(wâˆ’wâˆ— )i,yÌ‚) â‰¤ Î·1 < âˆž for any w, y, so we can obtain
0 â‰¤ 1 âˆ’ Î³Î·Î²(hw,Î³(wâˆ’wâˆ— )i,yÌ‚) â‰¤ 1 âˆ’ Î·Î½(Î³),
after simplifying Î½(Î³) to Î½, we therefore have the following inequality from Eq. (21):
 t+1
2
2
2
w
âˆ’ wâˆ— 2 â‰¤ (1 âˆ’ Î·Î½) wt âˆ’ wâˆ— 2 ,
Thus we can have the exponential convergence:
 t



w âˆ’ wâˆ—  â‰¤ (1 âˆ’ Î·Î½)t w0 âˆ’ wâˆ—  ,
2
2

âˆ’1
0
âˆ—
k
1
log kw âˆ’w
samples to achieve an -approximation of wâˆ— .
in other words, the student needs log 1âˆ’Î·Î½


Iterative Machine Teaching

Proof of Proposition 5 Because ` (hw, xi , y) is Î¶1 -strongly convex w.r.t. w, we have


2
2
2
kxk , âˆ€ {x, y} âˆˆ X Ã— Y,
Î¶1 ` (hw, xi , y) âˆ’ min ` (hw, xi , y) â‰¤ kâˆ‡w ` (hw, xi , y)k = Î²(hw,xi,y)
w

	
where X = x âˆˆ Rd , kxk â‰¤ R . Using xÌ‚ = Î³(w âˆ’ wâˆ— ), Î³ â‰¥ 0, we have
r 

Î¶1 ` (hw, Î³(w âˆ’ wâˆ— )i , y) âˆ’ min ` (hw, Î³(w âˆ’ wâˆ— )i , y) â‰¤ Î²(hw,Î³(wâˆ’wâˆ— )i,y) Î³kw âˆ’ wâˆ— k.
w

We assume the loss function is always non-negative, i.e., ` (hw, xi , y) â‰¥ 0. Therefore we have
p
Î¶1 (` (hw, Î³(w âˆ’ wâˆ— )i , y)) â‰¤ Î²(hw,Î³(wâˆ’wâˆ— )i,y) Î³kw âˆ’ wâˆ— k.
Because ` (hw, xi , y) is Î¶-strongly convex w.r.t. w, it is also Î¶2 -strongly convex w.r.t. hw, xi. Then we perform Taylor
expansion to ` (hw, Î³(w âˆ’ wâˆ— )i , y) w.r.t. hw, xi at the point hwâˆ— , xi and obtain
Î¶2
` (hw, Î³(w âˆ’ wâˆ— )i , y) â‰¥ ` (hw, Î³(wâˆ— âˆ’ wâˆ— )i , y) + âˆ‡hw,xi ` (hw, Î³(wâˆ— âˆ’ wâˆ— )i , y) (w âˆ’ wâˆ— )T x + k(w âˆ’ wâˆ— )T xk2
2
which leads to
Î¶2
` (hw, Î³(w âˆ’ wâˆ— )i , y) â‰¥ Î³ 2 kw âˆ’ wâˆ— k4
2
Combining pieces, we have
r
Î¶1 Î¶2
Î³kw âˆ’ wâˆ— k â‰¤ Î²(hw,Î³(wâˆ’wâˆ— )i,y) Î³.
2
q 2
	
1
1
R
âˆ—
Then if we set Î³ = min
Î¶1 Î¶2 kwâˆ’wâˆ— kÎ· , kwâˆ’wâˆ— k , we can have Î· â‰¤ Î²(hw,Î³(wâˆ’w )i,y) Î³. Because ` (hw, xi , y) is
Lipschitz smooth w.r.t. hw, xi with parameter L, we have


Î²(hw,xi,y) âˆ’ Î²(hwâˆ— ,xi,y)  â‰¤ LR kw âˆ’ wâˆ— k
Because Î²(hwâˆ— ,xi,y) = 0, we have the following inequality:


Î²(hw,xi,y)  â‰¤ LR kw âˆ’ wâˆ— k
If we multiply both side with Î³, we can have
Î²(hw,xi,y) Î³ â‰¤ LR kw âˆ’ wâˆ— k Î³
By setting Î³ as

1
LRÎ·kwâˆ’wâˆ— k ,

we arrive at Î²(hw,xi,y) Î³ < Î·1 . Combining pieces, as long as we set
r

1
R
1
2
,
,
Î³ = min
,
Î¶1 Î¶2 Î·kw âˆ’ wâˆ— k kw âˆ’ wâˆ— k LRÎ· kw âˆ’ wâˆ— k

then we can have

1
.
Î·
where c is a non-zero positive constant. Therefore, we achieve the condition for the exponential synthesis-based teaching.
0 < c â‰¤ Î²(hw,Î³ xÌ‚i,yÌ‚) Î³ â‰¤

By the Proposition 5, the absolute loss and sqaure loss are exponentially teachable in synthesis-based case, and we can
obtain Î³ by plugging into the general form. We will tighten the Î³ up by analyzing absolute loss and square loss separately.
Besides that, we also show the commonly used loss functions for classification, e.g., hinge loss and logistic loss, are also
exponentially teachable in synthesis-based teaching if kwâˆ— k can be bounded.
Proposition 9 Absolute loss is exponentially teachable in synthesis-based teaching.
Proof To show one loss function is exponentially teachable in synthesis-based case, we just need to find the appropriate Î³
such that the learning intensity is bounded below and above, according to Theorem 4. For the absolute loss, i.e.,
` (hw, xi , y) = |hw, xi âˆ’ y| ,
its sub-gradient is
âˆ‡w `(hw, xi , y) = sign(hw, xi âˆ’ y)x,
and thus, the learning intensity Î²(hw,xi,y) = sign (hw, xi âˆ’ y). For w 6= wâˆ— , plugging xÌ‚ = Î³ (w âˆ’ wâˆ— ) and

Iterative Machine Teaching

yÌ‚ = hwâˆ— , Î³ (w âˆ’ wâˆ— )i into the learning intensity, we have

Î²Î³hw,xÌ‚i,yÌ‚ Î³ = sign Î³ 2 hw âˆ’ wâˆ— , w âˆ’ wâˆ— i Î³ = Î³.
Recall that Î³ 6= 0, |Î³| â‰¤

R
kwt âˆ’wâˆ— k ,

âˆ€t âˆˆ N, we have
Î³ â‰¤ min

tâˆˆN kw t

R
:= C.
âˆ’ wâˆ— k

Set Î³ = min{C, Î·1 }, we have Î½ = min{C, Î·1 }. Therefore, we obtain the exponential decay. In fact, since the kwt âˆ’ wâˆ— k
R
decreases in every step, we have C = kw0 âˆ’w
âˆ— k . In following proof, we will follow the same argument to use this fact.

Proposition 10 Square loss is exponentially teachable in synthesis-based teaching.
Proof For square loss, i.e.,
2

` (hw, xi , y) = (hw, xi âˆ’ y) ,
its gradient is
âˆ‡w ` (hw, xi , y) = 2 (hw, xi âˆ’ y) x,
and thus, the learning intensity Î²hw,xi,y = 2 (hw, xi âˆ’ y). For w 6= wâˆ— , plugging xÌ‚ = Î³ (w âˆ’ wâˆ— ) and yÌ‚ =
hwâˆ— , Î³ (w âˆ’ wâˆ— )i into the learning intensity, we have
2

Î²(hw,xÌ‚i,yÌ‚) Î³ = 2Î³ 2 kw âˆ’ wâˆ— k .
o
R
Set Î³ = min âˆš2Î·kw1t âˆ’wâˆ— k , kwt âˆ’w
, we achieve the exponential teachable condition.
âˆ—k
n

Proposition 11 Hinge loss is exponentially teachable in synthesis-based teaching if kwâˆ— k â‰¤ 1.
Proof For hinge loss, i.e.,
` (hw, xi , y) = max (1 âˆ’ y hw, xi , 0) ,
as long as 1 âˆ’ y hw, xi > 0, its subgradient will be
âˆ‡w ` (hw, xi , y) = âˆ’yx.
âˆ—

Denote xÌ‚ = Î³ (w âˆ’ w ), we have Î²hw,xÌ‚i,yÌ‚ = âˆ’yÌ‚ where yÌ‚ âˆˆ {âˆ’1, 1}. To satisfy the exponential teachable condition, we
need to select yÌ‚ and Î³ such that
ï£±
ï£±
ï£±
âˆ—
âˆ—
ï£´
ï£´
ï£´
ï£²yÌ‚Î³ hw, w âˆ’ w i < 1
ï£²hw, w âˆ’ w i > âˆ’1
ï£²1 âˆ’ yÌ‚ hw, xÌ‚i > 0
0 < âˆ’yÌ‚Î³ â‰¤ Î·1
â‡’ âˆ’ Î·1 â‰¤ yÌ‚Î³ < 0
â‡’ âˆ’ Î·1 â‰¤ yÌ‚Î³ < 0
.
ï£´
ï£´
ï£´
ï£³
ï£³
ï£³
R
R
R
|Î³| â‰¤ kwâˆ’wâˆ— k
|Î³| â‰¤ kwâˆ’wâˆ— k
|Î³| â‰¤ kwâˆ’w
âˆ—k
If kwâˆ— k â‰¤ 1, we can show
2

hw, wâˆ— i â‰¤ kwk kwâˆ— k â‰¤ kwk < 1 + kwk ,
where the last inequality comes from the fact 1 + a2 âˆ’ a > 0, and thus, we have hw, w âˆ’ wâˆ— i > âˆ’1. Therefore, we select
any configuration of yÌ‚ and Î³ satisfying
1
R
âˆ’ â‰¤ yÌ‚Î³ < 0, and |Î³| â‰¤
.
Î·
kw âˆ’ wâˆ— k
o
n
R
.
Particularly, we set yÌ‚ = âˆ’1 and Î³ = min Î·1 , kw0 âˆ’w
âˆ—k

Proposition 12 Logistic loss is exponentially teachable in synthesis-based teaching if kwâˆ— k â‰¤ 1.
Proof For the logistic loss, i.e.,
` (hw, xi , y) = log (1 + exp(âˆ’y hw, xi)) ,

Iterative Machine Teaching

its gradient is

yx
.
1 + exp(y hw, xi)
yÌ‚
where yÌ‚ âˆˆ {âˆ’1, 1}. To satisfy the exponential teachable
Denote xÌ‚ = Î³ (w âˆ’ wâˆ— ), we have Î²hw,xÌ‚i,yÌ‚ = âˆ’ 1+exp(yÌ‚hw,xÌ‚i)
condition, we need to select yÌ‚ and Î³ such that
(
yÌ‚Î³
0 < âˆ’ 1+exp(yÌ‚hw,xÌ‚i)
â‰¤ Î·1
.
R
|Î³| â‰¤ kwâˆ’w
âˆ—k
âˆ‡w ` (hw, xi , y) = âˆ’

Particularly, we set yÌ‚ = âˆ’1, we can fix the Î³ by
Î³
Î³
1
0<
<
â‰¤Î³â‰¤ ,
1 + exp(Î³)
1 + exp(yÌ‚ hw, xÌ‚i)
Î·
The

Î³
1+exp(Î³)

<

we can choose Î³

and

|Î³| â‰¤

Î³
is obtained by the monotonicity of exp(Â·) and hw, w
1+exp(yÌ‚hw,xÌ‚i)
o
n
Î³
1
R
, and thus, the lower bound Î½ = 1+exp(Î³)
= min Î· , kw0 âˆ’w
.
âˆ—k

R
.
kw âˆ’ wâˆ— k

âˆ’ wâˆ— i > âˆ’1 when kwâˆ— k. Therefore,

Proof of Corollary 6 In each update, given the training sample x âˆˆ span (X ), we have wt+1 = wt âˆ’ Î·Î²hw,xi,y x,
0
therefore, the âˆ†t+1 w := wt+1 âˆ’ w
(X ). If w0 âˆ’ wâˆ— âˆˆ span (X ), wt+1 âˆ’ wâˆ— âˆˆ span (X ), which means by linear
Pnâˆˆ span
t
combination, we can construct Î³Ì‚ i=1 Î±i xi = Î³ (wt âˆ’ wâˆ— ). With the condition that the loss function is exponentially
synthesis-based teachable, we achieve the conclusion that the combination-based omniscient teacher will converge at least
exponentially with the same rate to the synthesis-based teaching.

Proof of Theorem 8 The proof is similar to the synthesis-based case. However, we introduce the consideration of the
effect of pool-based teaching. Specifically, we first obtain a virtual training sample in full space, and then, we generate the
sample from the candidate pool to mimic the virtual sample.
With the condition w0 âˆ’ v âˆ— âˆˆ span (D), as we discussed in the proof of Corollary 6, in every iteration, wt âˆ’ v âˆ— âˆˆ span (D).
Therefore, we only need to consider in the space of span (D). Meanwhile, since the teacher can rescale the sample, without
loss of generality, we assume if x âˆˆ X , then âˆ’x âˆˆ X to make the rescaling is always positive.
At t-step, as the loss is exponentially synthesis-based teachable with Î³, therefore, we have the virtually constructed sample
{xv , yv } where xv = Î³ (wt âˆ’ wâˆ— ) with Î³ satisfying the condition of exponentially teachable in synthesis-based settings,
we first rescale the candidate pool X such that


âˆ€x âˆˆ X , Î³x kxk = kxv k = Î³ wt âˆ’ wâˆ—  .
We denote the rescaled candidate pool as Xt , under the condition of rescalable pool-based teachability, there is a sample
{xÌ‚, yÌ‚} âˆˆ X Ã— Y with scale factor Î³Ì‚ such that


 




 
min
Î· 2 kâˆ‡wt ` wt , x , y k2 âˆ’ 2Î· wt âˆ’ wâˆ— , âˆ‡wt ` wt , x , y
(x,y)âˆˆXt Ã—Y

2

2
t
âˆ—
â‰¤ Î· 2 Î²hw
t ,Î³Ì‚ xÌ‚i,yÌ‚ kxÌ‚k âˆ’ 2Î·Î²hw t ,Î³Ì‚ xÌ‚i,yÌ‚ hw âˆ’ w , Î³Ì‚ xÌ‚i.

We decompose the Î³Ì‚ xÌ‚ = axv + xv âŠ¥ with a =
min

(x,y)âˆˆXt Ã—Y

Î· 2 kâˆ‡wt `

hÎ³Ì‚ xÌ‚,xv i
.
kxv k2




and xv âŠ¥ = Î³Ì‚ xÌ‚ âˆ’ axv . Then, we have






 
wt , x , y k2 âˆ’ 2Î· wt âˆ’ wâˆ— , âˆ‡wt ` wt , x , y
2

2
t
âˆ—
â‰¤ Î· 2 Î²hw
t ,Î³Ì‚ xÌ‚i,yÌ‚ kxÌ‚k âˆ’ 2Î·Î²hw t ,Î³Ì‚ xÌ‚i,yÌ‚ hw âˆ’ w , Î³Ì‚ xÌ‚i
2

2
2
âˆ—
t
âˆ—
= Î· 2 Î²hw
t ,Î³Ì‚ xÌ‚i,yÌ‚ Î³ kw âˆ’ w k âˆ’ 2Î·Î²hw t ,Î³Ì‚ xÌ‚i,yÌ‚ hw âˆ’ w , axv + xv âŠ¥ i


2
2
âˆ— 2
âˆ— 2
 t
= Î· 2 Î²hw
.
t ,Î³Ì‚ xÌ‚i,yÌ‚ Î³ kw âˆ’ w k âˆ’ 2Î·Î²hw t ,Î³Ì‚ xÌ‚i,yÌ‚ Î³a w âˆ’ w

Under the condition
2V(X )
,
Î·
we denote Î½ (Î³) = minw,xÌ‚âˆˆX ,yÌ‚âˆˆY Î³Î²hw,Î³ wâˆ’wâˆ— i,yÌ‚ > 0 and Âµ (Î³) = maxw,xÌ‚âˆˆX ,yÌ‚âˆˆY Î³Î²hw,Î³ wâˆ’wâˆ— i,yÌ‚ <
xÌ‚
xÌ‚
0 < Î³Î²hw,Î³ wâˆ’wâˆ— i,yÌ‚ <
xÌ‚

2V(X )
.
Î·

Iterative Machine Teaching

we have the recursion

2

2
 t+1
w
âˆ’ wâˆ— 2 â‰¤ r(Î·, Î³) wt âˆ’ wâˆ— 2 ,

n
o
2
2
with r(Î·, Î³, V(X )) := max 1 + Î· 2 Âµ (Î³) âˆ’ 2Î·Âµ (Î³) V(X ), 1 + Î· 2 Î½ (Î³) âˆ’ 2Î·Î½ (Î³) V(X ) and 0 â‰¤ r(Î·, Î³) < 1. Therefore, the algorithm converges exponentially
 t



w âˆ’ wâˆ—  â‰¤ r (Î·, Î³)t/2 w0 âˆ’ wâˆ—  ,
2
2

âˆ’1
0
âˆ—
k
1
in other words, the student needs 2 log r(Î·,Î³,V(X
log kw âˆ’w
samples to achieve an -approximation of wâˆ— . For
))

âˆ’1

Î·,Î³,V(X )
1
clearity, we define the constant term as C2
= 2 log r(Î·,Î³,V(X
.
))

Iterative Machine Teaching

B. Detailed Experimental Setting
Layer
Conv1.x
Pool1
Conv2.x
Pool2
Conv3.x
Pool3
FC1

CNN-6
[3Ã—3, 16]Ã—2

CNN-9
CNN-12
[3Ã—3, 16]Ã—3 [3Ã—3, 16]Ã—4
2Ã—2 Max, Stride 2
[3Ã—3, 32]Ã—2 [3Ã—3, 32]Ã—3 [3Ã—3, 32]Ã—4
2Ã—2 Max, Stride 2
[3Ã—3, 64]Ã—2 [3Ã—3, 64]Ã—3 [3Ã—3, 64]Ã—4
2Ã—2 Max, Stride 2
32
32
32

Table 1. Our standard CNN architectures for CIFAR-10. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain
multiple convolution layers. E.g., [3Ã—3, 16]Ã—3 denotes 3 cascaded convolution layers with 16 filters of size 3Ã—3. The CNNs learning
ends at 20K iterations with multi-step rate decay.

General Settings We have used three linear models in the experiments. In specific, the formulation of ridge regression
(RR) is
n
1X1 T
Î»
2
min
(w xi + b âˆ’ yi )2 + kwk
d
2
2
wâˆˆR ,bâˆˆR n
i=1
The formulation of logistic regression (LR) is
n
Î»
1X
2
log(1 + exp{âˆ’yi (wT xi + b)}) + kwk
min
2
wâˆˆRd ,bâˆˆR n
i=1
The formulation of support vector machine (SVM) is
n
Î»
1X
2
max(1 âˆ’ yi (wT xi + b), 0) + kwk
min
2
wâˆˆRd ,bâˆˆR n
i=1
Comparison of different teaching strategies We use a linear regression model (ridge regression with Î» = 0) for this
experiment. We set R as 1 and uniformly generate 30 data points as our knowledge pool for the teacher. In this first case,
we set the feature dimension as 2, while in the second case, feature dimension is 70. The learning rate is set as 0.0001 for
pool-based teaching, same as BGD and SGD.
Experiments on Gaussian data Specifically, RR is run on training data (xi , y) where each entry in xi is Gaussian
distributed and y = hwâˆ— , xi i + . LR and SVM are run on {X1 , +1} and {X2 , âˆ’1} where xi âˆˆ X1 is Gaussian distributed
in each entry and +1, âˆ’1 are the labels. Specifically, we use the 10-dimension data that is Gaussian distributed with
(0.5, Â· Â· Â· , 0.5) (label +1) and (âˆ’0.5, Â· Â· Â· , âˆ’0.5) (label âˆ’1) as mean and identity matrix as covariance matrix. We generate
1000 training data points for each class. Learning rate for the same feature space is 0.0001, while learning rate for different
feature spaces are 0.00001. Î» is set as 0.00005.
Experiments on uniform spherical data We first generate the training data that are uniformly distributed on a unit
sphere kxi k2 = 1. Then we set the data points on half of the sphere ((0, Ï€]) as label +1 and the other half ((Ï€, 2Ï€]) as label
âˆ’1. All the generated data points are 2D. For the scenario of different features, we use a random orthogonal projection
matrix to generate the teacherâ€™s feature space from studentâ€™s. Learning rate for the same feature space is 0.001, while
learning rate for different feature spaces are 0.0001. Î» is set as 0.00005.
Experiments on MNIST dataset We use 24D random features (projected by a random matrix R784Ã—24 ) for the MNIST
dataset. The learning rate for all the compared methods are 0.001. Note that, we generate the teacherâ€™s features using a
random projection matrix (R24Ã—24 ) from the original 24D studentâ€™s features. Î» is set as 0.00005.
Experiments on CIFAR-10 dataset The learning rate for all the compared methods are 0.001. Î» is set as 0.00005. The
goal is to learn the R32Ã—10 fully connected layer, which is also the classifiers for 10 classes. The three network we use in
the experiments are shown as follows:

Iterative Machine Teaching

Experiments on infant ego-centric dataset We manually crop and label all the objects that the child is holding for this
experiments. For feature extraction, we use VGG-16 network that is pre-trained on Imagenet dataset. Then we use PCA
to reduce the 4096 dimension to 64 dimension. We train a multi-class logistic regression to classify the objects. Note that,
the omniscient teacher is also applied to train the logistic regression model. The learning rate is set to 0.001 for both SGD
and omniscient teacher.

Iterative Machine Teaching

C. Comparison of different teaching strategies
10

Batch gradient descent
Stochastic gradient descent
Synthesis-based teaching
Pool-based teaching
Rescalable pool-based teaching
Combination-based teaching

1.2
1

9
Difference between w t and w*

Difference between w t and w*

1.4

0.8
0.6
0.4
0.2

8
7

Batch gradient descent
Stochastic gradient descent
Synthesis-based teaching
Pool-based teaching
Rescalable pool-based teaching
Combination-based teaching

6
5
4
3
2
1

0

100

200
300
Iteration Number

400

(a) Dimension of the feature space
is smaller than the number of samples

500

0

0

100

200
300
Iteration Number

400

500

(b) Dimension of the feature space
is greater than the number of samples

Figure 8. Comparison of different teaching strategies.

We first compare four different teaching strategies for the omniscient teacher. We consider two scenarios. One is that
the dimension of feature space is smaller than the number of samples (the given features are sufficient to represent the
entire feature), and the other is that the feature dimension is greater than the number of samples (the given features are not
sufficient to represent the entire feature). In these two scenarios, we find that synthesis-based teaching usually works the
best and always achieves exponential convergence. The combination-based teaching is exactly the same as the synthesisbased teaching in the first scenario, but it is much worse than synthesis in the second scenario. Rescalable pool-based
teaching is also better than pool-based teaching. Empirically, the experiment verifies our theoretical findings: the more
flexible the teaching strategy is, the more convergence gain we may obtain.

Iterative Machine Teaching

D. More experiments on MNIST dataset
We provide more experimental results on MNIST dataset. Fig. 9 shows the selected examples from 7/9 binary digit
classification. The results further verify the teacher models tend to select easy examples at first and gradually shift their
focuses to difficult examples, very much resembling the human learning. Fig. 10 shows the difference between the
current model parameter and the optimal model parameter over iterations. It also shows that our teachers achieve faster
convergence.

Iteration 1-40

Iteration 601-640 Iteration 1201-1240

(a) Omniscient Teacher

Iteration 1-40

Iteration 601-640 Iteration 1201-1240

(b) Imitation Teacher

Figure 9. Selected training examples during iteration. (7/9 classification)

Batch gradient descent
Stochastic gradient descent
Omniscient Teachter
Surrogate Teacher (same space)
Surrogate Teacher (different space)
Imitation Teacher

0.3
0.28
0.26
0.24
0.22
0.2
0.18
0.16

0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02

0.14
0.12
0

0.18

Difference between w t and w*

Difference between wt and w*

0.32

200

400

600

800

Iteration Number

1000

1200

0

0

Difference between

200

wt

and w*

400

600

800

Iteration Number

1000

Figure 10. Teaching logistic regression on MNIST dataset. Left column: 0/1 classification. Right column: 3/5 classification

Iterative Machine Teaching

E. Teaching linear models on uniform spherical data
In this experiment, we use a different data distribution to further evaluate the teacher models. We will examine LR and
SVM by classifying uniform spherical data.
Teaching in the same feature space. From Fig. 11, one can observe that the convergence is consistently improved
while using omniscient teacher to provide examples to learners. We find that the significance of improvement is related
to the training data distribution and loss function, as indicated by our theoretical results. The surrogate teacher produces
less convergence gain in SVM, because the convexity lower bound becomes very loose in this case. Overall, omniscient
teacher still presents strong teaching capability. More interestingly, we use simple SGD run on the sample set selected by
the omniscient teacher and also get faster convergence, showing that the selected example set is better than the entire set in
terms of convergence.

0.7

2.2

0.65

1.8
1.6
1.4

0.55
0.5
0.45

1.2

0.4

1

0.35

0.8

0

1000

2000

3000

4000

Iteration Number

5000

0.3

6000

0

1000

2000

3000

4000

Iteration Number

5000

1.1

Objective Value

1.5

1

0.8

Difference between w t and w*

0.9

2

0.7
0.6
0.5
0.4
0.3

0.5

0.2
0

0

500

1000

1500

Iteration Number

2000

2500

0.1

0

500

1000

1500

Iteration Number

(c) Teaching support vector machine in the same feature space

2000

1.5
1

2500

0.55
0.5
0.45
0.4
0.35
0.3

0.5

0.25
0

500

1000

1500

2000

Iteration Number

2500

0.2

3000

0

1000

1500

2000

Iteration Number

2500

3000

1.1

2

1

1.8

0.9

1.6
1.4
1.2
1

Batch gradient descent
Stochastic gradient descent
Surrogate teacher (pool)
Imitation teacher (pool)

0.8
0.7
0.6
0.5
0.4

0.8

0.3

0.6
0.4

500

(b) Teaching logistic regression in different feature spaces

2.2

Batch gradient descent
Stochastic gradient descent
SGD on selected set
Omniscient Teacher
Surrogate teacher

1

0.6

2

0

6000

(a) Teaching logistic regression in the same feature space

2.5

Difference between w t and w*

0.6

Batch gradient descent
Stochastic gradient descent
Surrogate teacher (pool)
Imitation teacher (pool)

0.65
2.5

Objective Value

2.4

Batch gradient descent
Stochastic gradient descent
SGD on selected set
Omniscient Teacher
Surrogate teacher

Objective Value

0.75

Difference between wt and w*

0.8

2.6

2

0.7

3

2.8

Objective Value

Difference between w t and w*

Teaching in different feature spaces. While the teacher and student use different feature spaces, one can observe from
Fig. 11 that the surrogate teacher performs very poorly, even worse than the original SGD and BGD. The imitation teacher
works much better and achieves consistent and significant convergence speedup, showing its superiority while the teacher
and the student use different features.

0

200

400

600

Iteration Number

800

1000

0.2

0

200

400

600

Iteration Number

(d) Teaching support vector machine in different feature spaces

Figure 11. Convergence results on uniform spherical data.

800

1000

Iterative Machine Teaching

F. Object learning experiment on childrenâ€™s ego-centric visual data
We experiment with a dataset capturing children and parents interacting with toys in a naturalistic setting (Yurovsky et al.,
2013). These interactions are recorded for around 10.5 minutes with a camera worn low on the childâ€™s forehead. The headcameraâ€™s visual field was 90 degrees wide, providing a broad view of objects visible to the infant. The camera was attached
to a headband that was tightened so that it did not move once set on the child. To calibrate the camera, the experimenter
noted when the child focused on an object and adjusted the camera until the object was in the center of the image in the
control monitor.
For our experiments, we selected interactions of 4 one year old infants. For each parent-child dyad, we annotated the
bounding box location and category of the toy attended to by the infant at each frame. There are 10 objects in total: doll
(34 frames), toy (53 frames), duck (335 frames), frog (2108 frames), helicopter (169 frames), horse (42 frames), mickey
(472 frames), phone (394 frames), sheep (119 frames) and tiger (266 frames). We use a VGG-16 network that is pre-trained
on Imagenet dataset as our feature extraction. We first extract the 4096D features from these images and then use PCA to
reduce the dimension to 64D. Finally, we run our omniscient teacher on these ego-centric data.
One can observe from Fig. 12 that our omniscient teacher achieves faster convergence than the random teacher. Moreover,
we give part of the selected training examples of random teacher and omniscient teacher in Fig. 14 and Fig. 15, respectively.
We visualize the selected samples every 50 iterations from the first iteration to the 10000th iteration. Interestingly, we find
that the training samples that are selected by the omniscient teacher consist of contiguous bouts of experience with the
same object instance, unlike the random teacher. The adjacent samples are similar and the object changes in a smooth way.
These inputs are qualitatively similar in their ordering to the actual visual experiences of infants in our study, as illustrated
in Fig. 13. This can be seen as partial algorithmic confirmation of the desirable structural properties of childrenâ€™s natural
learning environment, which emphasizes a smooth and continuous evolution of visual experience, in sharp contrast to
random sample selection.
2.4
SGD
Omniscient teacher

1.3

Objective Value

Objective Value

1.35
1.25
1.2
1.15
1.1
1.05

2.2
2.1
2
1.9

0

2000

4000

6000

Iteration

8000

(a) Video of infant 1

10000

1.3

1.25

1.8

1.6

2.2
SGD
Omniscient Teacher

1.35

2.18

SGD
Omniscient Teacher

2.16
2.14
2.12
2.1
2.08
2.06

1.7

1
0.95

1.4
SGD
Omniscient Teacher

2.3

Objective Value

1.4

Objective Value

1.45

1.2
0

2000

4000

6000

Iteration

8000

(b) Video of infant 2

10000

2.04
0

2000

4000

6000

Iteration

8000

10000

(c) Video of infant 3

0

2000

4000

6000

Iteration

8000

(d) Video of infant 4

Figure 12. Convergence comparison on infant ego-centric visual data.

Figure 13. Training examples corresponding to the natural sequence of objects experienced by a single infant in our study.

10000

Iterative Machine Teaching

Figure 14. Training examples selected by the random teacher (Stochastic gradient descent).

Figure 15. Training examples selected by the omniscient teacher.

