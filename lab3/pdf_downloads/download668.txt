On The Projection Operator to A Three-view Cardinality Constrained Set

Supplementary Material
In the supplementary materials, we include all the proofs for the proposed theorems and the detailed derivations for the
formulation of the crowdsourcing task assignment problem.

1. Proof of Lemma 1
Firstly we show how to convert the projection problem (2) to a support set selection problem. For any vector w, let vector
x 2 {0, 1}p indicate the nonzero positions of w, then we can claim that
kv

vx k2  kv

wk2

where vx is a vector having same dimension with v, and it keeps elements at positions where x has “1”, and fills zeros at
positions where x has “0”. In addition, vector w 2 ⌦(G, s) if and only if its support set indicator vector x satisfies Ax  s,
given A is defined in (4).
So the problem (2) can be converted to integer programming:
min

x2{0,1}p

vx k2

kv

(9)

subject to Ax  s
and the objective can be further simplified:
kv

vx k2 = hv2 , 1
2

= kvk

xi
hv2 , xi

Since v is constant here, then the problem (9) is equivalent to the ILP (3), which means problem (2) is equivalent to ILP (3).
Then we complete the proof.

2. Proof of Theorem 2
To prove Theorem 2, we use the concept of totally unimodular matrix.
Definition 2. Totally Unimodular (TU) Matrix. An integer matrix is TU, if the determinant of any square submatrices3
is in the set { 1, 0, 1}.
Proposition 1. If A is TU, then A> is TU, and their concatenations with identity matrices (i.e. [A, I], [A> , I]> ) are still
TU.
Proof. Since transposing matrix will not change the determinant, so it is obvious A> is TU.
Then we prove stacking with identity matrix I preserves TU property. We prove it by induction. Firstly, we show that
submatrix with size 1 always has determinant in { 1, 0, 1}, because any element from I is either 1 or 0. Now consider
a submatrix with size k having determinant in { 1, 0, 1}, then submatrix with size k + 1 will still have determinant in
{ 1, 0, 1}. To show this, we only need to prove that adding a new row/column from I will not change the determinant
out of set { 1, 0, 1}. Since any row/column from I only has one nonzero element “1”, we can eliminate other elements in
the same position by subtracting a multiple of this row/column to other rows/columns. After that, we can remove this row
and column, and the determinant can only change its sign. So we know that submatrix with size k + 1 has determinant in
{ 1, 0, 1} if submatrix with size k has determinant in { 1, 0, 1}.
Lemma 4. If A is TU, s is an integer vector, then all vertices of the following polytope are integer points:

3

{x | Ax  s, x 2 [0, 1]p }

Submatrix here is a square smaller matrix obtained by removing certain rows and columns

(10)

On The Projection Operator to A Three-view Cardinality Constrained Set

Proof. We can have an equivalent form of this polytope:
⇢ 

A
s
x
x
,x
I
1
From Proposition 1, we know that matrix



0

(11)

A
I

is TU if A is TU, so this meets the case in Theorem 13.2 (see Papadimitriou & Steiglitz, 1982, chap 13). Then we complete
the proof.
Lemma 5. If C is the matrix whose each row is the indicator vector of a group g 2 G1 [ G2 in our TVCS model, then C is
a TU matrix.
Proof. Since C’s rows are the indicator vectors of groups in G, so Cij 2 {0, 1}. From the definition 1, we know that there
are at most two “1”s in each column. For the column which has two “1”s, the two corresponding groups are from G1 and
G2 respectively. (Because we know that groups within G1 or G2 do not overlap.)
By this way, our matrix C meets the case in Theorem 13.3 (see Papadimitriou & Steiglitz, 1982, chap 13), and it is a TU
matrix.
Lemma 6. Recall the matrix A in equation (4):
A=



1>
C

If it is constructed from the TVCS model, then A is a TU matrix.
Proof. From Lemma 5, we know C is a TU matrix for any G of our TVCS model. In other words, any submatrix restricted
in C has determinant 1, 0, or 1. Therefore, we only need to consider the submatrix of A0 has overlaps with the first row
1> . There are only three possible forms of such submatrix S. We will show all of their determinants are in { 1, 0, 1}.
1) At least one column of S has a single “1”, so it must appear in the first row 1> . By exchanging such column with the
last column (which can only influence the sign of determinant), we can transform it with form:
 >
1
1
C̄ 0
where C̄ is any submatrix of C. From the matrix determinant property, we have |S| = ±|C̄| 2 { 1, 0, 1}. Therefore,
submatrix S in such form have determinants in { 1, 0, 1}.
2) All columns of S have three “1” elements (the last row has “1” for every column). For the rows which are from C,
we can sum all the rows to a certain row (this will not change the determinant). By this way we transform S to the
following form:
2 >3
1
4 2> 5
C̄
where C̄ is a submatrix of C. In this case, S is not full rank, so its determinant is 0.

3) Each column in S contains at least two “1” elements, and there exists one column which has exactly two “1”s. By
exchanging it to the last column, we can transform it to be:
2 >
3
1
1
6· · · 07
6
7
4 C̄i 1 5
··· 0

On The Projection Operator to A Three-view Cardinality Constrained Set

This means that one “1” is in the first row, and another is in the row from C, let us say it’s C̄i . Since subtracting one
row from another row will not change the determinant, we can let the first row subtract C̄i :
2 >
3
2 >
3
1
1
1
C̄i 0
6 · · · 07
6
07
6
7 ! 6 ···
7
4 C̄i 1 5
4 C̄i
15
··· 0
···
0

Now the last column only has a single “1” in the i-th row. We can generate a smaller matrix S 0 by removing the i-th
row and the last column, and if S 0 has determinant in { 1, 0, 1}, so does S.
2

3
2 >
3
0
1
C̄i
7
07
! S0 = 4 · · · 5
15
···
0

1> C̄i
6 ···
6
4 C̄i
···

If C̄i 6= 0> , then there are some positions (including j-th column) in the first row will become zeros. For any column
of matrix S 0 which has “0” element in the first row, there are two cases:
(a) This column only contains zeros, i.e. S 0 has zero determinant.
(b) This column contains a single “1”, we can generate a smaller matrix S 00 by removing this column and the row
where this “1” sits. If S 00 has determinant in { 1, 0, 1}, so does S 0 .

Notice that it is impossible for the case that such column has two “1”s. (Since each column can have at most three
“1”s, and we already remove the “1” in the first row by subtraction, and discard another “1” by removing C̄i .) In
the above case (b), we can repeat removing columns and rows until we get a degenerate matrix (has 0 determinant),
or a matrix whose first row does not have zeros. For the later situation, we can process it by same procedures as the
original matrix S unless it only has one row and one column, i.e. a matrix having single element “1” (has determinant
1).
If C̄i = 0> , we can also process it by same procedures as the original matrix S.
Therefore, we have proved that any square submatrix S in A0 has determinant in { 1, 0, 1}, which means A0 is TU, and
hence A is TU.
Applying Lemma 4 and Lemma 6, we complete the proof of Theorem 2.

3. Proof of Theorem 3
To prove Theorem 3, we start with several lemmas.
Lemma 7. Formulate the feasibility problem as problem (7), let f be the objective of the formulation in Theorem 3. If
f ⇤ = 0, there exists a such that
f (z)

f⇤

2

kz

Pz⇤ (z)k2 , 8z 2 ⌦

where Pz⇤ (z) is the optimal point which is closet to z.
Proof. Since f ⇤ = 0, there exists at least an z⇤ such that
Az⇤

a0

Bz⇤ = b
Cz⇤  c

From Hoffman’s Theorem (Hoffman, 2003), we know that there exists a
2

kz

Pz⇤ (z)k2  k[Az

a]+ k2 + kBz

> 0, such that
bk2 + k[Cz

c]+ k2

On The Projection Operator to A Three-view Cardinality Constrained Set

Therefore, we know for any z in ⌦,
Cz  c
and
2

Pz⇤ (z)k2  k[Az

kz

a]+ k2 + kBz

bk2

Using the lemma above, we can prove the Theorem 3 now.
Proof. Denote by

t

:= kzt
t+1

Pz⇤ (zt )k2 . We have
= kzt+1
 kzt+1

Pz⇤ (zt )k2

 kzt

Pz⇤ (zt ) + zt+1

=

t



kzt+1

t

= kzt

Let T = hrf (zt ), zt+1

Pz⇤ (zt+1 )k2
zt k2

Pz⇤ (zt )k2 + kzt+1

zt k2 + 2hzt+1

zt k2 + 2hzt+1

kzt+1

zt k2

zt , zt+1

2 hrf (zt ), zt+1

zt , zt

Pz⇤ (zt )i

Pz⇤ (zt )i

Pz⇤ (zt )i

Pz⇤ (zt )i. Then we have
T = hrf (zt ), zt+1

Pz⇤ (zt )i

= hrf (zt ), zt

Pz⇤ (zt )i + hrf (zt ), zt+1 zt i
L t+1
f ⇤ + f (zt ) + f (zt+1 ) f (zt )
kz
zt k2
2
L t+1
= f (zt+1 ) f ⇤
kz
z t k2
2

where L is the Lipschitz continuous gradient constant. Back to the original inequality, we have
kzt+1

Pz⇤ (zt+1 )k2 

kzt+1

t

zt k2



t

(1

L )kzt+1



t

(1

L )kzt+1

2 hrf (zt ), zt+1

Pz⇤ (zt )i

z t k2

2 (f (zt+1 )

z t k2

2

2

kzt+1

f ⇤)
Pz⇤ (zt+1 )k2

where the last inequality comes from Lemma 7.
Let

=

1
L

and we have
(1 +

which shows the linear convergence rate ↵ =

1
1+ L

L

)

t+1



t

t+1



1
1+

t
L

, then it completes the proof.

4. Formulation of the Expected Accuracy in Crowdsourcing Task Assignment
In crowdsourcing task assignment problem, recall the objective function of problem (8):
m

1 X
Eacc (Q·,j , X·,j )
m j=1

On The Projection Operator to A Three-view Cardinality Constrained Set

For the j-th task, Eacc (Q·,j , X·,j ) is defined in the following:
Eacc (Q·,j , X·,j ) =P(ŷj = 1|yj = 1)P(yj = 1) + P(ŷj = 0|yj = 0)P(yj = 0)
=EŶ⌦

j ,j

|yj =1 [I(ŷj

= 1)]P(yj = 1) + EŶ⌦

j ,j

|yj =0 [I(ŷj

= 0)]P(yj = 0)

(12)

where I(·) is the indicator function. We can further specify this formulation by considering the equivalent forms for ŷj = 1
and ŷj = 0:
ŷj = 1
,P(yj = 1|Ŷ⌦j ,j )
,

P(Ŷ⌦j ,j |yj = 1)

P(yj = 0|Ŷ⌦j ,j )
P(yj = 0)
P(yj = 1)

P(Ŷ⌦j ,j |yj = 0)
Y ✓ Qij ◆2Ŷi,j 1
P(yj = 0)
,
1 Qij
P(yj = 1)
i2⌦j
✓
◆
✓
◆
X
Qij
P(yj = 0)
,
(2Ŷi,j 1) log
log
1 Qij
P(yj = 1)
i2⌦j
◆
✓
◆
✓
n
X
Qij
P(yj = 0)
,
Xij (2Ŷi,j 1) log
log
1 Qij
P(yj = 1)
i=1

0

Similar derivation can be applied to ŷj = 0 (change “ ” to “<”). Here we substitute the indicator function I(t 0) as
1
sigmoid function S(t) = 1+exp(
1) log (Qij /(1 Qij ))
t) to obtain a smooth approximation. Denote by Zij := (2Ŷi,j
and rj := log (P(yj = 0)/P(yj = 1)) for short. The (smooth) objective turns out to be:
"
!#
"
!#
!
m
n
n
X
X
1 X
EŶ |yj =1 S
Zij Xij rj
P(yj = 1) + EŶ |yj =0 S rj
Zij Xij
P(yj = 0)
m j=1
i=1
i=1
and its stochastic gradient is:
g(X)·,j

1
= P(yj = 1)(1
m

S(

1
P(yj = 0)(1
m

S(

n
X
i=1
n
X
i=1

y =1
Zijj Xij

y =0

Zijj

Xij

rj ))S(
rj ))S(

n
X
i=1
n
X

y =1

Xij

rj )Z·,jj

y =0

Xij

rj )Z·,jj

Zijj
Zijj

i=1

where Z yj =1 (or Z yj =0 ) is generated by sampling Ŷ given yj = 1 (or yj = 0).

y =1

y =0

+

