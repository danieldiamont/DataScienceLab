Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Supplementary Materials
A. Proof of Theorem 1
We first recall the following lemma.
Lemma 1 (Lemma 1, (Gong et al., 2013)). Under Assumption 1.{3}. For any Î· > 0 and any x, y âˆˆ Rd such that
x = proxÎ·g (y âˆ’ Î·âˆ‡f (y)), one has that
1
F (x) â‰¤ F (y) âˆ’ ( 2Î·
âˆ’

L
2 )kx

âˆ’ yk2 .

Applying Lemma 1 with x = xk , y = yk , we obtain that
1
F (xk ) â‰¤ F (yk ) âˆ’ ( 2Î·
âˆ’

L
2 )kxk

âˆ’ yk k2 .

(12)

Since Î· < L1 , it follows that F (xk ) â‰¤ F (yk ). Moreover, the update rule of APGnc guarantees that F (yk+1 ) â‰¤ F (xk ). In
summary, for all k the following inequality holds:
F (yk+1 ) â‰¤ F (xk ) â‰¤ F (yk ) â‰¤ F (xkâˆ’1 ).

(13)

Combing further with the fact that F (xk ), F (yk ) â‰¥ inf F > âˆ’âˆ for all k, we conclude that {F (xk )}, {F (yk )} converge
to the same limit F âˆ— , i.e.,
lim F (xk ) = lim F (yk ) = F âˆ— .

kâ†’âˆ

kâ†’âˆ

(14)

On the other hand, by induction we conclude from eq. (13) that for all k
F (yk ) â‰¤ F (x0 ),

F (xk ) â‰¤ F (x0 ).

Combining with Assumption 1.1 that F has bounded sublevel set, we conclude that {xk } and {yk } are bounded and thus
have bounded limit points. Now combining eq. (12) and eq. (13) yields
1
âˆ’
( 2Î·

L
2 )kyk

âˆ’ xk k2 â‰¤ F (yk ) âˆ’ F (xk )
â‰¤ F (yk ) âˆ’ F (yk+1 ),

(15)

which, after telescoping over k and letting k â†’ âˆ, becomes
âˆ
X

1
âˆ’
( 2Î·

L
2 )kyk

âˆ’ xk k2 â‰¤ F (y1 ) âˆ’ inf F < âˆ.

(16)

k=1

This further implies that kyk âˆ’ xk k â†’ 0, and hence {xk } and {yk } share the same set of limit points â„¦. Note that â„¦ is
closed (it is a set of limit points) and bounded, we conclude that â„¦ is compact in Rd .
By optimality condition of the proximal gradient step of APGnc, we obtain that
âˆ’âˆ‡f (yk ) âˆ’ Î·1 (xk âˆ’ yk ) âˆˆ âˆ‚g(xk )
â‡” âˆ‡f (xk ) âˆ’ âˆ‡f (yk ) âˆ’ Î·1 (xk âˆ’ yk ) âˆˆ âˆ‚F (xk ),
|
{z
}

(17)

uk

which further implies that
kuk k = kâˆ‡f (xk ) âˆ’ âˆ‡f (yk ) âˆ’ Î·1 (xk âˆ’ yk )k
â‰¤ (L + Î·1 )kyk âˆ’ xk k â†’ 0.

(18)

Consider any limit point z0 âˆˆ â„¦, and w.l.o.g we write xk â†’ z0 , yk â†’ z0 by restricting to a subsequence. By the definition
of the proximal map, the proximal gradient step of APGnc implies that
2
1
2Î· kyk âˆ’ xk k + g(xk )
1
yk i + 2Î·
kz0 âˆ’ yk k2 + g(z0 ).

hâˆ‡f (yk ), xk âˆ’ yk i +
â‰¤ hâˆ‡f (yk ), z0 âˆ’

(19)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Taking lim sup on both sides and note that xk âˆ’ yk â†’ 0, yk â†’ z0 , we obtain that lim supkâ†’âˆ g(xk ) â‰¤ g(z0 ). Since
g is lower semicontinuous and xk â†’ z0 , it follows that lim supkâ†’âˆ g(xk ) â‰¥ g(z0 ). Combining both inequalities, we
conclude that limkâ†’âˆ g(xk ) = g(z0 ). Note that the continuity of f yields limkâ†’âˆ f (xk ) = f (z0 ), we then conclude that
limkâ†’âˆ F (xk ) = F (z0 ). Since limkâ†’âˆ F (xk ) = F âˆ— by eq. (14), we conclude that
F (z0 ) â‰¡ F âˆ— ,

âˆ€z0 âˆˆ â„¦.

(20)

Hence, F remains constant on the compact set â„¦. To this end, we have established xk â†’ z0 , F (xk ) â†’ F (z0 ) and that
âˆ‚F (xk ) 3 uk â†’ 0. Recall the definition of limiting sub-differential, we conclude that 0 âˆˆ âˆ‚F (z0 ) for all z0 âˆˆ â„¦.

B. Proof of Theorem 2
Throughout the proof we assume that F (xk ) 6= F âˆ— for all k because otherwise the algorithm terminates and the conclusions
hold trivially. We also denote k0 as a sufficiently large positive integer.
Combining eq. (12) and eq. (13) yields that
L
2 )kyk+1

1
âˆ’
F (xk+1 ) â‰¤ F (xk ) âˆ’ ( 2Î·

âˆ’ xk+1 k2 .

(21)

Moreover, eq. (17) and eq. (18) imply that
distâˆ‚F (xk ) (0) â‰¤ (L + Î·1 )kyk âˆ’ xk k.

(22)

We have shown in Appendix A that F (xk ) â†“ F âˆ— , and it is also clear that distâ„¦ (xk ) â†’ 0. Thus, for any , Î´ > 0 and all
k â‰¥ k0 , we have
xk âˆˆ {x | distâ„¦ (x) â‰¤ , F âˆ— < F (x) < F âˆ— + Î´}.
Since â„¦ is compact and F is constant on it, the uniformized KL property implies that for all k â‰¥ k0
Ï•0 (F (xk ) âˆ’ F âˆ— )distâˆ‚F (xk ) (0) â‰¥ 1.

(23)

Recall that rk := F (xk ) âˆ’ F âˆ— . Then eq. (23) is equivalent to
2
1 â‰¤ Ï•0 (rk ) distâˆ‚F (xk ) (0)
2

(i)
2
â‰¤ (Ï•0 (rk )) Î·1 + L kyk âˆ’ xk k2


(ii)

2

â‰¤ (Ï•0 (rk ))

2
1
Î· +L
1
L [F
2Î· âˆ’ 2

(xkâˆ’1 ) âˆ’ F (xk )]

2

â‰¤ d1 (Ï•0 (rk )) (rkâˆ’1 âˆ’ rk ) ,
where (i) is due to eq. (22), (ii) follows from eq. (21), and d1 =



1
Î·

+L

2 
1
/ 2Î·
âˆ’

L
2



. Since Ï• (t) = Î¸c tÎ¸ , we have that

Ï•0 (t) = ctÎ¸âˆ’1 . Thus the above inequality becomes
1 â‰¤ d1 c2 rk2Î¸âˆ’2 (rkâˆ’1 âˆ’ rk ) .

(24)

It has been shown in (Frankel et al., 2015; Li & Lin, 2015) that sequence {rk } satisfying the above inductive property
converges to zero at different rates according to Î¸ as stated in the theorem.

C. Proof of Theorem 3
g non-convex, k = 0: In this setting, we first prove the following inexact version of Lemma 1.
Lemma 2. Under Assumption 1.3. For any Î· > 0 and any x, y âˆˆ Rd such that x = proxÎ·g (y âˆ’ Î·(âˆ‡f (y) + e)), one has
that
F (x) â‰¤ F (y) + ( L2 âˆ’

1
2Î· )kx

âˆ’ yk2 + kx âˆ’ ykkek.

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Proof. By Assumption 1.3 we have that
f (x) â‰¤ f (y) + hx âˆ’ y, âˆ‡f (y)i +

L
2 kx

âˆ’ yk2 .

Also, by the definition of proximal map, the proximal gradient step implies that
g(x) +

1
2Î· kx

âˆ’ y + Î·(âˆ‡f (y) + e)k2 â‰¤ g(y) +

1
2Î· kÎ·(âˆ‡f (y)

+ e)k2 ,

which, after simplifications becomes that
g(x) â‰¤ g(y) âˆ’

1
2Î· kx

âˆ’ yk2 âˆ’ hx âˆ’ y, (âˆ‡f (y) + e)i.

Combine the above two inequalities further gives that
F (x) â‰¤ F (y) + ( L2 âˆ’

1
2Î· )kx

âˆ’ yk2 + kx âˆ’ ykkek.

Using Lemma 2 with x = xk , y = yk , e = ek and notice the fact that kek k â‰¤ Î³kxk âˆ’ yk k, we obtain that
F (xk ) â‰¤ F (yk ) + (Î³ +

L
2

âˆ’

1
2Î· )kxk

âˆ’ yk k2 .

(25)

Moreover, the optimality condition of the proximal gradient step with gradient error gives that By optimality condition of
the proximal gradient step of APGnc, we obtain that
âˆ‡f (xk ) âˆ’ âˆ‡f (yk ) âˆ’ ek âˆ’ Î·1 (xk âˆ’ yk ) âˆˆ âˆ‚F (xk ),
which further implies that
distâˆ‚F (xk ) (0) â‰¤ (Î³ + L + Î·1 )kyk âˆ’ xk k.

(26)

Notice that eq. (25) and eq. (26) are parallel to the key inequalities eq. (21) and eq. (22) in the analysis of exact APGnc.
1
1
Thus, by choosing Î· < 2Î³+L
and redefining d1 = ( Î·1 + L + Î³)2 /( 2Î·
âˆ’ L2 âˆ’ Î³), all the statements in Theorem 1 remain
true and the convergence rates in Theorem 2 remain the same order with a worse constant.
g convex: We first present the following lemma.
Lemma 3. For any x, v âˆˆ Rd , let u0 âˆˆ âˆ‚ g(x) such that âˆ‡f (x) + u0 has minimal norm. Denote Î¾ := distâˆ‚g(x) (u0 ), then
we have
distâˆ‚F (x) (0) â‰¤ distâˆ‡f (x)+âˆ‚ g(x) (0) + Î¾.

(27)

Proof. We observe the following
distâˆ‚F (x) (0) = min kâˆ‡f (x) + uk
uâˆˆâˆ‚g(x)

= min kâˆ‡f (x) + u0 + u âˆ’ u0 k, âˆ€u0 âˆˆ âˆ‚ g(x)
uâˆˆâˆ‚g(x)

â‰¤ kâˆ‡f (x) + u0 k + min ku âˆ’ u0 k, âˆ€u0 âˆˆ âˆ‚ g(x)
uâˆˆâˆ‚g(x)

â‰¤ min
g(x)kâˆ‡f (x) + u0 k + Î¾
0
u âˆˆâˆ‚

= distâˆ‡f (x)+âˆ‚ g(x) (0) + Î¾.

(28)

Recall that we have two inexactness, i.e., xk = proxÎ·gk (yk âˆ’ Î·(âˆ‡f (yk ) + ek )). Following a proof similar to that of
Lemma 2 and notice that k â‰¤ Î´kxk âˆ’ yk k2 , we can obtain that
2
L
1
2 âˆ’ 2Î· )kxk âˆ’ yk k + k
1
(Î³ 0 + L2 âˆ’ 2Î·
)kxk âˆ’ yk k2

F (xk ) â‰¤ F (yk ) + (Î³ +
â‰¤ F (yk ) +

(29)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

for some Î³ 0 > Î³ > 0. Since g is convex, by Lemma 2 in (Schmidt et al., 2011) one can exhibit vk with kvk k â‰¤
such that
1
Î· [yk âˆ’ xk âˆ’ Î·(âˆ‡f (yk ) + ek ) âˆ’ vk ] âˆˆ âˆ‚k g(xk ).

âˆš

2Î·k

This implies that
1
Î·

distâˆ‡f (xk )+âˆ‚k g(xk ) (0) â‰¤ (Î³ +

+ L)kxk âˆ’ yk k +

q

2k
Î· .

Apply Lemma 3 and notice that k â‰¤ Î´kxk âˆ’ yk k2 , Î¾k â‰¤ Î»kxk âˆ’ yk k, we obtain that
distâˆ‚F (xk ) (0) â‰¤ (Î³ 0 +

1
+ L)kxk âˆ’ yk k
Î·

(30)

for some Î³ 0 > Î³ > 0. Now eq. (29) and eq. (30) are parallel to the key inequalities eq. (21) and eq. (22) in the analysis
1
of exact APGnc. Thus, by choosing Î· < 2Î³ 01+L and redefining d1 = ( Î·1 + L + Î³ 0 )2 /( 2Î·
âˆ’ L2 âˆ’ Î³ 0 ), all the statements in
Theorem 1 remain true and the convergence rates in Theorem 2 remain the same order with a worse constant.

D. Proof of Theorem 4
We first define the following quantities for the convenience of the proof.
2

1
) + Î·L2 , cm = 0,
ct = ct+1 (1 + m


Rkt := E F (xtk ) + ct kxtk âˆ’ x0k k2 ,

(31)

xÌ„t+1
k

(33)

=

proxÎ·g (xtk

âˆ’

(32)

Î·âˆ‡f (xtk )).

Note that xÌ„t+1
is a reference sequence introduced for the convenience of analysis, and is not being computed in the
k
implementation of the algorithm. Then it has been shown in the proof of Theorem 5 of (Reddi et al., 2016b) that
 


1
E kxÌ„t+1
âˆ’ xtk k2 .
(34)
Rkt+1 â‰¤ Rkt + L âˆ’ 2Î·
k
Telescoping eq. (34) over t from t = 1 to t = m âˆ’ 1, we obtain
"
E[F (xm
k )]

â‰¤E

F (xÌ„1k )

+

c1 kxÌ„1k

âˆ’

x0k k2

+

mâˆ’1
X

Lâˆ’

1
2Î·



#
kxÌ„t+1
k

âˆ’

xtk k2

.

(35)

t=1

Following from eq. (31), a simple induction shows that ct â‰¤ Î·L2 m. Setting Î· <
eq. (35) further implies that

1
2L

and recalling that F (yk ) â‰¤ F (xm
kâˆ’1 ).,

1
2
1
0 2
E[F (yk+1 )] â‰¤ E[F (xm
k )] â‰¤ E[F (xÌ„k )] + Î·L mE[kxÌ„k âˆ’ xk k ].

(36)

Now telescoping eq. (34) again over t from t = 0 to t = m âˆ’ 1 and applying eq. (36), we obtain
E[F (xm
k )] â‰¤ E[F (yk )] +

mâˆ’1
X

(L âˆ’

1
2Î· )E

 t+1

kxÌ„k âˆ’ xtk k2 .

(37)

t=0

Combining all the above facts, we conclude that for Î· <

1
2L

E[F (yk )] â‰¤ E[F (ykâˆ’1 )] â‰¤ . . . â‰¤ F (y0 ).

(38)

Since E[F (Â·)] is bounded below, E[F (yk )] decreases to a finite limit, say, F âˆ— . Define rk = E [F (yk ) âˆ’ F âˆ— ], and assume
rk > 0 for all k (since otherwise rk = 0 and the algorithm terminates). Applying the KÅ property with Î¸ = 1/2, we obtain
1
c (F (x)

1

âˆ’ F âˆ— ) 2 â‰¤ distâˆ‚F (x) (0).

(39)

Setting x = xÌ„1k , we further obtain
1
1
c2 (F (xÌ„k )


2
âˆ’ F âˆ— ) â‰¤ dist2âˆ‚F (xÌ„1 ) (0) â‰¤ L + Î·1 kxÌ„1k âˆ’ yk k2 ,
k

(40)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

where the last inequality is due to eq. (33). Taking expectation over both sides and using eq. (36), we obtain
2 
 

 1
Î·L2 m
m
âˆ—
0 2
1
1
â‰¤
L
+
E[F
(x
)
âˆ’
F
]
âˆ’
E
kxÌ„
âˆ’
x
k
E kxÌ„1k âˆ’ yk k2 .
k
k
k
c2
c2
Î·

(41)

Noting that x0k = yk and EF (yk+1 ) â‰¤ EF (xm
k ), we then rearrange the above inequality and obtain


2


Î·L2 m
âˆ—
m
âˆ—
1
1
1
E[F
(y
)
âˆ’
F
]
â‰¤
E[F
(x
)
âˆ’
F
]
â‰¤
L
+
+
E kxÌ„1k âˆ’ yk k2
2
2
2
k+1
k
c
c
Î·
c

(42)

2

â‰¤

(L+ Î·1 )

2

+ Î·Lc2m

1
2Î· âˆ’L

(E[F (yk )] âˆ’ E[F (yk+1 )]) ,

(43)

which can be further rewritten as
rk+1 â‰¤ d (rk âˆ’ rk+1 ) ,

(44)

2

where d =

c2 (L+ Î·1 ) +Î·L2 m
.
1
2Î· âˆ’L

Then a simple induction yields that
rk+1 â‰¤



d
d+1

k+1

(F (y0 ) âˆ’ F âˆ— ) .

(45)

E. Proof of Theorem 5
We first introduce some auxiliary lemmas.
Lemma 4. Consider the convex function g and x, y âˆˆ Rd such that y = proxÎ·g (x) for some  > 0. Then, there exists
âˆš
kik â‰¤ 2Î· that satisfies the following inequality for all z âˆˆ Rd .
g(y) +

1
2Î· ky

âˆ’ xk2 â‰¤ g(z) +

1
2Î· kz

âˆ’ xk2 âˆ’

Proof. By Lemma 2 in (Schmidt et al., 2011), there exists kik â‰¤
1
Î·

âˆš

1
2Î· ky

âˆ’ zk2 + hy âˆ’ z, Î·1 ii + .

(46)

2Î· such that

(x âˆ’ y âˆ’ i) âˆˆ âˆ‚ g(y).

(47)

Then, the definition of -subdifferential implies that
g(z) âˆ’ g(y) â‰¥ hz âˆ’ y, âˆ‚ g(y)i âˆ’  = hz âˆ’ y, Î·1 (x âˆ’ y âˆ’ i)i âˆ’ , âˆ€ z âˆˆ Rd .

(48)

The desired result follows by rearranging the above inequality.
Lemma 5. Consider the convex function g and x, y, d âˆˆ Rd such that y = proxÎ·g (x âˆ’ Î·d) for some  > 0. Then, there
âˆš
exists kik â‰¤ 2Î· that satisfies the following inequality for all z âˆˆ Rd .


1
g(y) = hy âˆ’ z, d âˆ’ Î·1 ii â‰¤ g(z) + 2Î·
kz âˆ’ xk2 âˆ’ ky âˆ’ zk2 âˆ’ ky âˆ’ xk2 + .
(49)
Proof. By Lemma 4, we obtain the following inequality for all z âˆˆ Rd .
Î·
âˆ’ xk2 + kdk2
2
1
1
â‰¤ g(z) + 2Î·
kz âˆ’ x + Î·dk2 âˆ’ 2Î·
ky âˆ’ zk2 + hy âˆ’ z, Î·1 ii + 

g(y) + hy âˆ’ x, di +

1
2Î· ky

1
= g(z) + hz âˆ’ x, di 2Î·
kz âˆ’ xk2 + Î·2 kdk2 âˆ’

1
2Î· ky

âˆ’ zk2 + hy âˆ’ z, Î·1 ii + .

(50)

The desired result follows by rearranging the above inequality.
Lemma 6. Consider the convex function g and x, y, d âˆˆ Rd such that y = proxÎ·g (x âˆ’ Î·d) for some  > 0. Then, there
âˆš
exists kik â‰¤ 2Î· that satisfies the following inequality for all z âˆˆ Rd .




1
1
1
F (y) + hy âˆ’ z, d âˆ’ Î·1 i âˆ’ âˆ‡f (x)i â‰¤ F (z) + L2 âˆ’ 2Î·
ky âˆ’ xk2 + L2 + 2Î·
kz âˆ’ xk2 âˆ’ 2Î·
ky âˆ’ zk2 + . (51)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Proof. By Lipschitz continuity of âˆ‡f , we obtain
f (y) â‰¤ f (x) + hâˆ‡f (x), y âˆ’ xi +
f (x) â‰¤ f (z) + hâˆ‡f (x), x âˆ’ zi +

2
L
2 ky âˆ’ xk ,
2
L
2 kx âˆ’ zk .

(52)
(53)

Adding the above inequalities together yields
f (y) â‰¤ f (z) + hâˆ‡f (x), y âˆ’ zi +

L
2




ky âˆ’ xk2 + kz âˆ’ xk2 .

(54)

Combining with Lemma 5, we then obtain the desired result.
t
Recall the reference sequence xÌ„t+1
= proxÎ·g (xtk âˆ’ Î·âˆ‡f (xtk )). Applying Lemma 6 with  = 0, y = xÌ„t+1
k
k , z = xk , and
d = âˆ‡f (xtk ) and taking expectation on both sides, we obtain

i
h

t+1
t+1
t 2
t 2
t
1
1
L
kxÌ„
âˆ’
)
âˆ’
x
k
âˆ’
kxÌ„
.
(55)
E[F (xÌ„t+1
âˆ’
x
k
)]
â‰¤
E
F
(x
)
+
k
k
k
k
k
k
2
2Î·
2Î·
t+1
t
Similarly, applying Lemma 6 with  = tk , y = xt+1
k , z = xÌ„k , d = vk and taking expectation on both sides, we obtain
h

t+1
t
t
1
E[F (xt+1
)]
â‰¤
E
F (xÌ„t+1
âˆ’ xÌ„t+1
k
k ) + hxk
k , âˆ‡f xk âˆ’ vk + Î· ik i



i

2
t
1
1
1
+ L2 âˆ’ 2Î·
kxÌ„t+1
âˆ’ xt+1
kxt+1
âˆ’ xtk k2 + L2 + 2Î·
kxÌ„t+1
âˆ’ xtk k2 âˆ’ 2Î·
(56)
k
k k + k .
k
k

Adding eq. (55) and eq. (56) together yields


h

t+1
t
1
âˆ’ xtk k2 + L2 âˆ’
E[F (xt+1
k )] â‰¤ E F (xk ) + L âˆ’ 2Î· kxÌ„k
t
t
where T = hxt+1
âˆ’ xÌ„t+1
k
k , âˆ‡f (xk ) âˆ’ vk +

ik
Î·i

1
2Î·



kxt+1
âˆ’ xtk k2 âˆ’
k

t+1
1
2Î· kxÌ„k

2
âˆ’ xt+1
k k +T

i

+ tk . Now we bound E[T ] as follows.

i
 t+1
 Î· h

ik 2
2
t
t
kxk âˆ’ xt+1
k
+
E
kâˆ‡f
x
k
+ tk
âˆ’
v
+
k
k
k
2
Î·
i
h





2
1
â‰¤ 2Î·
E kxt+1
âˆ’ xt+1
+ Î·E kâˆ‡f xtk âˆ’ vkt k2 + Î·E k iÎ·k k2 + tk
k
k k





2
1
E kxt+1
âˆ’ xt+1
+ Î·E kâˆ‡f xtk âˆ’ vkt k2 + 3tk .
â‰¤ 2Î·
k
k k

E[T ] â‰¤

(57)

1
2Î· E

(58)
(59)
(60)





By Lemma 3 of (Reddi et al., 2016b), it holds that E kâˆ‡f (xtk ) âˆ’ vkt k2 â‰¤ L2 E kxtk âˆ’ x0k k2 . Combining with the
above inequality, we further obtain that




2
1
E[T ] â‰¤ 2Î·
E kxt+1
âˆ’ xt+1
+ Î·L2 E kxtk âˆ’ x0k k2 + 3tk .
(61)
k
k k
Substituting the above result into eq. (57), we obtain
h



t+1
t 2
t
1
L
E[F (xt+1
)]
â‰¤
E
F
(x
)
+
L
âˆ’
kxÌ„
âˆ’
x
k
+
k
k
k
k
2Î·
2 âˆ’

1
2Î·



i
t 2
2
t
0 2
t
kxt+1
âˆ’
x
k
+
Î·L
kx
âˆ’
x
k
+
3
k
k
k
k .
k

(62)



mâˆ’t
Recalling that Rkt := E F (xtk ) + ct kxtk âˆ’ x0k k2 , where ct = Î·L2 (1+Î²)Î² âˆ’1 with Î² > 0. Then, we can upper bound
Rkt+1 as


t+1
Rkt+1 =E F (xt+1
âˆ’ xtk + xtk âˆ’ x0k k2
(63)
k ) + ct+1 kxk


t+1
t+1
t+1
t 2
t
0 2
t
t
0
=E F (xk ) + ct+1 kxk âˆ’ xk k + kxk âˆ’ xk k + 2hxk âˆ’ xk , xk âˆ’ xk i
(64)
h


i
t+1
1
(65)
â‰¤E F (xt+1
âˆ’ xtk k2 + ct+1 (1 + Î²) kxtk âˆ’ x0k k2
k ) + ct+1 1 + Î² kxk
h


h


i
1
1
â‰¤E F (xtk ) + L âˆ’ 2Î·
kxÌ„t+1
âˆ’ xtk k2 + ct+1 1 + Î²1 + L2 âˆ’ 2Î·
kxt+1
âˆ’ xtk k2
(66)
k
k



2
t
0 2
t
+ ct+1 (1 + Î²) + Î·L kxk âˆ’ xk k + 3k .
(67)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Setting Î² = 1/m in ct and observe that
mâˆ’t

ct = Î·L2 (1+Î²)Î²

âˆ’1


= Î·L2 m (1 + Î²)mâˆ’t âˆ’ 1 â‰¤ Î·L2 m (e âˆ’ 1) â‰¤ 2Î·L2 m,

(68)

which further implies that


ct+1 1 + Î²1 +

L
2

â‰¤ 2Î·L2 m(1 + m) â‰¤ 4Î·L2 m2 +

L
2

= 4ÏLm2 +

L
2

â‰¤

1
2Î· .

(69)

Also note that ct = ct+1 (1 + Î²) + Î·L2 . Collecting all these facts, Rkt+1 can be further upper bounded by

i
h
1
kxÌ„t+1
âˆ’ xtk k2 + 3tk .
Rkt+1 â‰¤ Rkt + E L âˆ’ 2Î·
k
Telescoping eq. (70) from t = 1 to t = m âˆ’ 1, we obtain
"
E[F (xm
k )]

â‰¤E

F (xÌ„1k )

+

c1 kxÌ„1k

âˆ’

x0k k2

+

mâˆ’1
X

1
2Î·

Lâˆ’



kxÌ„t+1
k

âˆ’

xtk k2

+

t=1

mâˆ’1
X

(70)

#
3tk

.

(71)

t=1

Again, telescoping eq. (70) from t = 0 to t = m âˆ’ 1 we obtain
E[F (yk+1 )] â‰¤ E[F (xm
k )] â‰¤ E[F (yk )] +

mâˆ’1
X

1
2Î· )E

(L âˆ’

mâˆ’1
X  
 t+1

kxÌ„k âˆ’ xtk k2 + 3
E tk .

Assume
that 3

mâˆ’1
P

t=0
mâˆ’1
P
t=0

(72)

t=0

t=0



E kxÌ„t+1
âˆ’ xtk k2 > 0, because otherwise the algorithm is terminated. Assume that there exists Î± > 0 such
k
mâˆ’1
P

E [tk ] â‰¤ Î±

t=0



E kxÌ„t+1
âˆ’ xtk k2 and
k

1
2Î·

âˆ’ L âˆ’ Î± > 0. Then eq. (72) further implies that

E[F (yk+1 )] â‰¤ E[F (xm
k )] â‰¤ E[F (yk )] +

mâˆ’1
X

(L âˆ’

1
2Î·



+ Î±)E kxÌ„t+1
âˆ’ xtk k2 .
k

(73)

t=0

That is, we have E[F (yk )] â‰¤ E[F (ykâˆ’1 )] â‰¤ . . . â‰¤ F (y0 ), and hence E[F (yk )] â†“ F âˆ— . We can further upper bound
eq. (71) as
"
#
mâˆ’1
mâˆ’1

X
X
1
1
0 2
1
E[F (xm
L âˆ’ 2Î·
3tk
kxÌ„t+1
âˆ’ xtk k2 +
k )] â‰¤E F (xÌ„k ) + c1 kxÌ„k âˆ’ xk k +
k
t=1

"
F (xÌ„1k )

â‰¤E

+

c1 kxÌ„1k

âˆ’

x0k k2



t=1

âˆ’ Lâˆ’

1
2Î·



kxÌ„1k

âˆ’

x0k k2

+

mâˆ’1
X

Lâˆ’

1
2Î·



kxÌ„t+1
k

t=0

"


â‰¤E F (xÌ„1k ) + c1 +

1
2Î·



kxÌ„1k âˆ’ x0k k2 +

mâˆ’1
X

Lâˆ’

1
2Î·



âˆ’

xtk k2

+

mâˆ’1
X

#
3tk

t=0

#

+ Î± kxÌ„t+1
âˆ’ xtk k2
k

t=0

h


â‰¤E F (xÌ„1k ) + E 2Î·L2 m +

1
2Î·



kxÌ„1k

i
âˆ’ x0k k2 .

(74)

Define rk = E [F (yk ) âˆ’ F âˆ— ], and suppose rk > 0 for all k (otherwise the algorithm terminates in finite steps). Applying
the KÅ condition with Î¸ = 1/2, we obtain
1
c (F (x)

1

âˆ’ F âˆ— ) 2 â‰¤ distâˆ‚F (x) (0).

(75)

Setting x = xÌ„1k , we obtain

2
1
2
1
âˆ—
1
(F
(xÌ„
)
âˆ’
F
)
â‰¤
dist
L
+
kxÌ„1k âˆ’ yk k2 .
1 ) (0) â‰¤
k
âˆ‚F
(xÌ„
Î·
k
c2

(76)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Taking expectation on both sides and using the result from eq. (74), we obtain
1
âˆ—
E[F (xm
k )âˆ’F ]âˆ’
c2

1
2Î·L2 m+ 2Î·
c2

2 


 
E kxÂ¯k 1 âˆ’ x0k k2 â‰¤ L + Î·1 E kxÌ„1k âˆ’ yk k2 .

Note that x0k = yk . Then rearranging the above inequality yields

2
1
âˆ—
m
âˆ—
1
1
E[F
(y
)
âˆ’
F
E[F
(x
+
]
â‰¤
)
âˆ’
F
]
â‰¤
L
+
2
k+1
k
c
Î·
c2
2

â‰¤

(L+ Î·1 )

+

1
2Î·L2 m+ 2Î·
c2

2Î·L2 m+ 1
2Î·
c2

1
2Î· âˆ’Lâˆ’Î±





E kxÌ„1k âˆ’ yk k2

(E[F (yk )] âˆ’ E[F (yk+1 )]) ,

(77)

(78)
(79)

2

which can be rewritten as rk+1 â‰¤ d (rk âˆ’ rk+1 ) with d =

rk+1

d
â‰¤
rk â‰¤
d+1



1
c2 (L+ Î·1 ) +2Î·L2 m+ 2Î·
1
2Î· âˆ’Lâˆ’Î±

d
d+1

k+1

. Then, induction yields that

(F (y0 ) âˆ’ F âˆ— ) .

(80)

