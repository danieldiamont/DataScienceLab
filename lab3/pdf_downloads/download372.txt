Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Supplementary Materials
A. Proof of Theorem 1
We first recall the following lemma.
Lemma 1 (Lemma 1, (Gong et al., 2013)). Under Assumption 1.{3}. For any η > 0 and any x, y ∈ Rd such that
x = proxηg (y − η∇f (y)), one has that
1
F (x) ≤ F (y) − ( 2η
−

L
2 )kx

− yk2 .

Applying Lemma 1 with x = xk , y = yk , we obtain that
1
F (xk ) ≤ F (yk ) − ( 2η
−

L
2 )kxk

− yk k2 .

(12)

Since η < L1 , it follows that F (xk ) ≤ F (yk ). Moreover, the update rule of APGnc guarantees that F (yk+1 ) ≤ F (xk ). In
summary, for all k the following inequality holds:
F (yk+1 ) ≤ F (xk ) ≤ F (yk ) ≤ F (xk−1 ).

(13)

Combing further with the fact that F (xk ), F (yk ) ≥ inf F > −∞ for all k, we conclude that {F (xk )}, {F (yk )} converge
to the same limit F ∗ , i.e.,
lim F (xk ) = lim F (yk ) = F ∗ .

k→∞

k→∞

(14)

On the other hand, by induction we conclude from eq. (13) that for all k
F (yk ) ≤ F (x0 ),

F (xk ) ≤ F (x0 ).

Combining with Assumption 1.1 that F has bounded sublevel set, we conclude that {xk } and {yk } are bounded and thus
have bounded limit points. Now combining eq. (12) and eq. (13) yields
1
−
( 2η

L
2 )kyk

− xk k2 ≤ F (yk ) − F (xk )
≤ F (yk ) − F (yk+1 ),

(15)

which, after telescoping over k and letting k → ∞, becomes
∞
X

1
−
( 2η

L
2 )kyk

− xk k2 ≤ F (y1 ) − inf F < ∞.

(16)

k=1

This further implies that kyk − xk k → 0, and hence {xk } and {yk } share the same set of limit points Ω. Note that Ω is
closed (it is a set of limit points) and bounded, we conclude that Ω is compact in Rd .
By optimality condition of the proximal gradient step of APGnc, we obtain that
−∇f (yk ) − η1 (xk − yk ) ∈ ∂g(xk )
⇔ ∇f (xk ) − ∇f (yk ) − η1 (xk − yk ) ∈ ∂F (xk ),
|
{z
}

(17)

uk

which further implies that
kuk k = k∇f (xk ) − ∇f (yk ) − η1 (xk − yk )k
≤ (L + η1 )kyk − xk k → 0.

(18)

Consider any limit point z0 ∈ Ω, and w.l.o.g we write xk → z0 , yk → z0 by restricting to a subsequence. By the definition
of the proximal map, the proximal gradient step of APGnc implies that
2
1
2η kyk − xk k + g(xk )
1
yk i + 2η
kz0 − yk k2 + g(z0 ).

h∇f (yk ), xk − yk i +
≤ h∇f (yk ), z0 −

(19)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Taking lim sup on both sides and note that xk − yk → 0, yk → z0 , we obtain that lim supk→∞ g(xk ) ≤ g(z0 ). Since
g is lower semicontinuous and xk → z0 , it follows that lim supk→∞ g(xk ) ≥ g(z0 ). Combining both inequalities, we
conclude that limk→∞ g(xk ) = g(z0 ). Note that the continuity of f yields limk→∞ f (xk ) = f (z0 ), we then conclude that
limk→∞ F (xk ) = F (z0 ). Since limk→∞ F (xk ) = F ∗ by eq. (14), we conclude that
F (z0 ) ≡ F ∗ ,

∀z0 ∈ Ω.

(20)

Hence, F remains constant on the compact set Ω. To this end, we have established xk → z0 , F (xk ) → F (z0 ) and that
∂F (xk ) 3 uk → 0. Recall the definition of limiting sub-differential, we conclude that 0 ∈ ∂F (z0 ) for all z0 ∈ Ω.

B. Proof of Theorem 2
Throughout the proof we assume that F (xk ) 6= F ∗ for all k because otherwise the algorithm terminates and the conclusions
hold trivially. We also denote k0 as a sufficiently large positive integer.
Combining eq. (12) and eq. (13) yields that
L
2 )kyk+1

1
−
F (xk+1 ) ≤ F (xk ) − ( 2η

− xk+1 k2 .

(21)

Moreover, eq. (17) and eq. (18) imply that
dist∂F (xk ) (0) ≤ (L + η1 )kyk − xk k.

(22)

We have shown in Appendix A that F (xk ) ↓ F ∗ , and it is also clear that distΩ (xk ) → 0. Thus, for any , δ > 0 and all
k ≥ k0 , we have
xk ∈ {x | distΩ (x) ≤ , F ∗ < F (x) < F ∗ + δ}.
Since Ω is compact and F is constant on it, the uniformized KL property implies that for all k ≥ k0
ϕ0 (F (xk ) − F ∗ )dist∂F (xk ) (0) ≥ 1.

(23)

Recall that rk := F (xk ) − F ∗ . Then eq. (23) is equivalent to
2
1 ≤ ϕ0 (rk ) dist∂F (xk ) (0)
2

(i)
2
≤ (ϕ0 (rk )) η1 + L kyk − xk k2


(ii)

2

≤ (ϕ0 (rk ))

2
1
η +L
1
L [F
2η − 2

(xk−1 ) − F (xk )]

2

≤ d1 (ϕ0 (rk )) (rk−1 − rk ) ,
where (i) is due to eq. (22), (ii) follows from eq. (21), and d1 =



1
η

+L

2 
1
/ 2η
−

L
2



. Since ϕ (t) = θc tθ , we have that

ϕ0 (t) = ctθ−1 . Thus the above inequality becomes
1 ≤ d1 c2 rk2θ−2 (rk−1 − rk ) .

(24)

It has been shown in (Frankel et al., 2015; Li & Lin, 2015) that sequence {rk } satisfying the above inductive property
converges to zero at different rates according to θ as stated in the theorem.

C. Proof of Theorem 3
g non-convex, k = 0: In this setting, we first prove the following inexact version of Lemma 1.
Lemma 2. Under Assumption 1.3. For any η > 0 and any x, y ∈ Rd such that x = proxηg (y − η(∇f (y) + e)), one has
that
F (x) ≤ F (y) + ( L2 −

1
2η )kx

− yk2 + kx − ykkek.

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Proof. By Assumption 1.3 we have that
f (x) ≤ f (y) + hx − y, ∇f (y)i +

L
2 kx

− yk2 .

Also, by the definition of proximal map, the proximal gradient step implies that
g(x) +

1
2η kx

− y + η(∇f (y) + e)k2 ≤ g(y) +

1
2η kη(∇f (y)

+ e)k2 ,

which, after simplifications becomes that
g(x) ≤ g(y) −

1
2η kx

− yk2 − hx − y, (∇f (y) + e)i.

Combine the above two inequalities further gives that
F (x) ≤ F (y) + ( L2 −

1
2η )kx

− yk2 + kx − ykkek.

Using Lemma 2 with x = xk , y = yk , e = ek and notice the fact that kek k ≤ γkxk − yk k, we obtain that
F (xk ) ≤ F (yk ) + (γ +

L
2

−

1
2η )kxk

− yk k2 .

(25)

Moreover, the optimality condition of the proximal gradient step with gradient error gives that By optimality condition of
the proximal gradient step of APGnc, we obtain that
∇f (xk ) − ∇f (yk ) − ek − η1 (xk − yk ) ∈ ∂F (xk ),
which further implies that
dist∂F (xk ) (0) ≤ (γ + L + η1 )kyk − xk k.

(26)

Notice that eq. (25) and eq. (26) are parallel to the key inequalities eq. (21) and eq. (22) in the analysis of exact APGnc.
1
1
Thus, by choosing η < 2γ+L
and redefining d1 = ( η1 + L + γ)2 /( 2η
− L2 − γ), all the statements in Theorem 1 remain
true and the convergence rates in Theorem 2 remain the same order with a worse constant.
g convex: We first present the following lemma.
Lemma 3. For any x, v ∈ Rd , let u0 ∈ ∂ g(x) such that ∇f (x) + u0 has minimal norm. Denote ξ := dist∂g(x) (u0 ), then
we have
dist∂F (x) (0) ≤ dist∇f (x)+∂ g(x) (0) + ξ.

(27)

Proof. We observe the following
dist∂F (x) (0) = min k∇f (x) + uk
u∈∂g(x)

= min k∇f (x) + u0 + u − u0 k, ∀u0 ∈ ∂ g(x)
u∈∂g(x)

≤ k∇f (x) + u0 k + min ku − u0 k, ∀u0 ∈ ∂ g(x)
u∈∂g(x)

≤ min
g(x)k∇f (x) + u0 k + ξ
0
u ∈∂

= dist∇f (x)+∂ g(x) (0) + ξ.

(28)

Recall that we have two inexactness, i.e., xk = proxηgk (yk − η(∇f (yk ) + ek )). Following a proof similar to that of
Lemma 2 and notice that k ≤ δkxk − yk k2 , we can obtain that
2
L
1
2 − 2η )kxk − yk k + k
1
(γ 0 + L2 − 2η
)kxk − yk k2

F (xk ) ≤ F (yk ) + (γ +
≤ F (yk ) +

(29)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

for some γ 0 > γ > 0. Since g is convex, by Lemma 2 in (Schmidt et al., 2011) one can exhibit vk with kvk k ≤
such that
1
η [yk − xk − η(∇f (yk ) + ek ) − vk ] ∈ ∂k g(xk ).

√

2ηk

This implies that
1
η

dist∇f (xk )+∂k g(xk ) (0) ≤ (γ +

+ L)kxk − yk k +

q

2k
η .

Apply Lemma 3 and notice that k ≤ δkxk − yk k2 , ξk ≤ λkxk − yk k, we obtain that
dist∂F (xk ) (0) ≤ (γ 0 +

1
+ L)kxk − yk k
η

(30)

for some γ 0 > γ > 0. Now eq. (29) and eq. (30) are parallel to the key inequalities eq. (21) and eq. (22) in the analysis
1
of exact APGnc. Thus, by choosing η < 2γ 01+L and redefining d1 = ( η1 + L + γ 0 )2 /( 2η
− L2 − γ 0 ), all the statements in
Theorem 1 remain true and the convergence rates in Theorem 2 remain the same order with a worse constant.

D. Proof of Theorem 4
We first define the following quantities for the convenience of the proof.
2

1
) + ηL2 , cm = 0,
ct = ct+1 (1 + m


Rkt := E F (xtk ) + ct kxtk − x0k k2 ,

(31)

x̄t+1
k

(33)

=

proxηg (xtk

−

(32)

η∇f (xtk )).

Note that x̄t+1
is a reference sequence introduced for the convenience of analysis, and is not being computed in the
k
implementation of the algorithm. Then it has been shown in the proof of Theorem 5 of (Reddi et al., 2016b) that
 


1
E kx̄t+1
− xtk k2 .
(34)
Rkt+1 ≤ Rkt + L − 2η
k
Telescoping eq. (34) over t from t = 1 to t = m − 1, we obtain
"
E[F (xm
k )]

≤E

F (x̄1k )

+

c1 kx̄1k

−

x0k k2

+

m−1
X

L−

1
2η



#
kx̄t+1
k

−

xtk k2

.

(35)

t=1

Following from eq. (31), a simple induction shows that ct ≤ ηL2 m. Setting η <
eq. (35) further implies that

1
2L

and recalling that F (yk ) ≤ F (xm
k−1 ).,

1
2
1
0 2
E[F (yk+1 )] ≤ E[F (xm
k )] ≤ E[F (x̄k )] + ηL mE[kx̄k − xk k ].

(36)

Now telescoping eq. (34) again over t from t = 0 to t = m − 1 and applying eq. (36), we obtain
E[F (xm
k )] ≤ E[F (yk )] +

m−1
X

(L −

1
2η )E

 t+1

kx̄k − xtk k2 .

(37)

t=0

Combining all the above facts, we conclude that for η <

1
2L

E[F (yk )] ≤ E[F (yk−1 )] ≤ . . . ≤ F (y0 ).

(38)

Since E[F (·)] is bounded below, E[F (yk )] decreases to a finite limit, say, F ∗ . Define rk = E [F (yk ) − F ∗ ], and assume
rk > 0 for all k (since otherwise rk = 0 and the algorithm terminates). Applying the KŁ property with θ = 1/2, we obtain
1
c (F (x)

1

− F ∗ ) 2 ≤ dist∂F (x) (0).

(39)

Setting x = x̄1k , we further obtain
1
1
c2 (F (x̄k )


2
− F ∗ ) ≤ dist2∂F (x̄1 ) (0) ≤ L + η1 kx̄1k − yk k2 ,
k

(40)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

where the last inequality is due to eq. (33). Taking expectation over both sides and using eq. (36), we obtain
2 
 

 1
ηL2 m
m
∗
0 2
1
1
≤
L
+
E[F
(x
)
−
F
]
−
E
kx̄
−
x
k
E kx̄1k − yk k2 .
k
k
k
c2
c2
η

(41)

Noting that x0k = yk and EF (yk+1 ) ≤ EF (xm
k ), we then rearrange the above inequality and obtain


2


ηL2 m
∗
m
∗
1
1
1
E[F
(y
)
−
F
]
≤
E[F
(x
)
−
F
]
≤
L
+
+
E kx̄1k − yk k2
2
2
2
k+1
k
c
c
η
c

(42)

2

≤

(L+ η1 )

2

+ ηLc2m

1
2η −L

(E[F (yk )] − E[F (yk+1 )]) ,

(43)

which can be further rewritten as
rk+1 ≤ d (rk − rk+1 ) ,

(44)

2

where d =

c2 (L+ η1 ) +ηL2 m
.
1
2η −L

Then a simple induction yields that
rk+1 ≤



d
d+1

k+1

(F (y0 ) − F ∗ ) .

(45)

E. Proof of Theorem 5
We first introduce some auxiliary lemmas.
Lemma 4. Consider the convex function g and x, y ∈ Rd such that y = proxηg (x) for some  > 0. Then, there exists
√
kik ≤ 2η that satisfies the following inequality for all z ∈ Rd .
g(y) +

1
2η ky

− xk2 ≤ g(z) +

1
2η kz

− xk2 −

Proof. By Lemma 2 in (Schmidt et al., 2011), there exists kik ≤
1
η

√

1
2η ky

− zk2 + hy − z, η1 ii + .

(46)

2η such that

(x − y − i) ∈ ∂ g(y).

(47)

Then, the definition of -subdifferential implies that
g(z) − g(y) ≥ hz − y, ∂ g(y)i −  = hz − y, η1 (x − y − i)i − , ∀ z ∈ Rd .

(48)

The desired result follows by rearranging the above inequality.
Lemma 5. Consider the convex function g and x, y, d ∈ Rd such that y = proxηg (x − ηd) for some  > 0. Then, there
√
exists kik ≤ 2η that satisfies the following inequality for all z ∈ Rd .


1
g(y) = hy − z, d − η1 ii ≤ g(z) + 2η
kz − xk2 − ky − zk2 − ky − xk2 + .
(49)
Proof. By Lemma 4, we obtain the following inequality for all z ∈ Rd .
η
− xk2 + kdk2
2
1
1
≤ g(z) + 2η
kz − x + ηdk2 − 2η
ky − zk2 + hy − z, η1 ii + 

g(y) + hy − x, di +

1
2η ky

1
= g(z) + hz − x, di 2η
kz − xk2 + η2 kdk2 −

1
2η ky

− zk2 + hy − z, η1 ii + .

(50)

The desired result follows by rearranging the above inequality.
Lemma 6. Consider the convex function g and x, y, d ∈ Rd such that y = proxηg (x − ηd) for some  > 0. Then, there
√
exists kik ≤ 2η that satisfies the following inequality for all z ∈ Rd .




1
1
1
F (y) + hy − z, d − η1 i − ∇f (x)i ≤ F (z) + L2 − 2η
ky − xk2 + L2 + 2η
kz − xk2 − 2η
ky − zk2 + . (51)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Proof. By Lipschitz continuity of ∇f , we obtain
f (y) ≤ f (x) + h∇f (x), y − xi +
f (x) ≤ f (z) + h∇f (x), x − zi +

2
L
2 ky − xk ,
2
L
2 kx − zk .

(52)
(53)

Adding the above inequalities together yields
f (y) ≤ f (z) + h∇f (x), y − zi +

L
2




ky − xk2 + kz − xk2 .

(54)

Combining with Lemma 5, we then obtain the desired result.
t
Recall the reference sequence x̄t+1
= proxηg (xtk − η∇f (xtk )). Applying Lemma 6 with  = 0, y = x̄t+1
k
k , z = xk , and
d = ∇f (xtk ) and taking expectation on both sides, we obtain

i
h

t+1
t+1
t 2
t 2
t
1
1
L
kx̄
−
)
−
x
k
−
kx̄
.
(55)
E[F (x̄t+1
−
x
k
)]
≤
E
F
(x
)
+
k
k
k
k
k
k
2
2η
2η
t+1
t
Similarly, applying Lemma 6 with  = tk , y = xt+1
k , z = x̄k , d = vk and taking expectation on both sides, we obtain
h

t+1
t
t
1
E[F (xt+1
)]
≤
E
F (x̄t+1
− x̄t+1
k
k ) + hxk
k , ∇f xk − vk + η ik i



i

2
t
1
1
1
+ L2 − 2η
kx̄t+1
− xt+1
kxt+1
− xtk k2 + L2 + 2η
kx̄t+1
− xtk k2 − 2η
(56)
k
k k + k .
k
k

Adding eq. (55) and eq. (56) together yields


h

t+1
t
1
− xtk k2 + L2 −
E[F (xt+1
k )] ≤ E F (xk ) + L − 2η kx̄k
t
t
where T = hxt+1
− x̄t+1
k
k , ∇f (xk ) − vk +

ik
ηi

1
2η



kxt+1
− xtk k2 −
k

t+1
1
2η kx̄k

2
− xt+1
k k +T

i

+ tk . Now we bound E[T ] as follows.

i
 t+1
 η h

ik 2
2
t
t
kxk − xt+1
k
+
E
k∇f
x
k
+ tk
−
v
+
k
k
k
2
η
i
h





2
1
≤ 2η
E kxt+1
− xt+1
+ ηE k∇f xtk − vkt k2 + ηE k iηk k2 + tk
k
k k





2
1
E kxt+1
− xt+1
+ ηE k∇f xtk − vkt k2 + 3tk .
≤ 2η
k
k k

E[T ] ≤

(57)

1
2η E

(58)
(59)
(60)





By Lemma 3 of (Reddi et al., 2016b), it holds that E k∇f (xtk ) − vkt k2 ≤ L2 E kxtk − x0k k2 . Combining with the
above inequality, we further obtain that




2
1
E[T ] ≤ 2η
E kxt+1
− xt+1
+ ηL2 E kxtk − x0k k2 + 3tk .
(61)
k
k k
Substituting the above result into eq. (57), we obtain
h



t+1
t 2
t
1
L
E[F (xt+1
)]
≤
E
F
(x
)
+
L
−
kx̄
−
x
k
+
k
k
k
k
2η
2 −

1
2η



i
t 2
2
t
0 2
t
kxt+1
−
x
k
+
ηL
kx
−
x
k
+
3
k
k
k
k .
k

(62)



m−t
Recalling that Rkt := E F (xtk ) + ct kxtk − x0k k2 , where ct = ηL2 (1+β)β −1 with β > 0. Then, we can upper bound
Rkt+1 as


t+1
Rkt+1 =E F (xt+1
− xtk + xtk − x0k k2
(63)
k ) + ct+1 kxk


t+1
t+1
t+1
t 2
t
0 2
t
t
0
=E F (xk ) + ct+1 kxk − xk k + kxk − xk k + 2hxk − xk , xk − xk i
(64)
h


i
t+1
1
(65)
≤E F (xt+1
− xtk k2 + ct+1 (1 + β) kxtk − x0k k2
k ) + ct+1 1 + β kxk
h


h


i
1
1
≤E F (xtk ) + L − 2η
kx̄t+1
− xtk k2 + ct+1 1 + β1 + L2 − 2η
kxt+1
− xtk k2
(66)
k
k



2
t
0 2
t
+ ct+1 (1 + β) + ηL kxk − xk k + 3k .
(67)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Setting β = 1/m in ct and observe that
m−t

ct = ηL2 (1+β)β

−1


= ηL2 m (1 + β)m−t − 1 ≤ ηL2 m (e − 1) ≤ 2ηL2 m,

(68)

which further implies that


ct+1 1 + β1 +

L
2

≤ 2ηL2 m(1 + m) ≤ 4ηL2 m2 +

L
2

= 4ρLm2 +

L
2

≤

1
2η .

(69)

Also note that ct = ct+1 (1 + β) + ηL2 . Collecting all these facts, Rkt+1 can be further upper bounded by

i
h
1
kx̄t+1
− xtk k2 + 3tk .
Rkt+1 ≤ Rkt + E L − 2η
k
Telescoping eq. (70) from t = 1 to t = m − 1, we obtain
"
E[F (xm
k )]

≤E

F (x̄1k )

+

c1 kx̄1k

−

x0k k2

+

m−1
X

1
2η

L−



kx̄t+1
k

−

xtk k2

+

t=1

m−1
X

(70)

#
3tk

.

(71)

t=1

Again, telescoping eq. (70) from t = 0 to t = m − 1 we obtain
E[F (yk+1 )] ≤ E[F (xm
k )] ≤ E[F (yk )] +

m−1
X

1
2η )E

(L −

m−1
X  
 t+1

kx̄k − xtk k2 + 3
E tk .

Assume
that 3

m−1
P

t=0
m−1
P
t=0

(72)

t=0

t=0



E kx̄t+1
− xtk k2 > 0, because otherwise the algorithm is terminated. Assume that there exists α > 0 such
k
m−1
P

E [tk ] ≤ α

t=0



E kx̄t+1
− xtk k2 and
k

1
2η

− L − α > 0. Then eq. (72) further implies that

E[F (yk+1 )] ≤ E[F (xm
k )] ≤ E[F (yk )] +

m−1
X

(L −

1
2η



+ α)E kx̄t+1
− xtk k2 .
k

(73)

t=0

That is, we have E[F (yk )] ≤ E[F (yk−1 )] ≤ . . . ≤ F (y0 ), and hence E[F (yk )] ↓ F ∗ . We can further upper bound
eq. (71) as
"
#
m−1
m−1

X
X
1
1
0 2
1
E[F (xm
L − 2η
3tk
kx̄t+1
− xtk k2 +
k )] ≤E F (x̄k ) + c1 kx̄k − xk k +
k
t=1

"
F (x̄1k )

≤E

+

c1 kx̄1k

−

x0k k2



t=1

− L−

1
2η



kx̄1k

−

x0k k2

+

m−1
X

L−

1
2η



kx̄t+1
k

t=0

"


≤E F (x̄1k ) + c1 +

1
2η



kx̄1k − x0k k2 +

m−1
X

L−

1
2η



−

xtk k2

+

m−1
X

#
3tk

t=0

#

+ α kx̄t+1
− xtk k2
k

t=0

h


≤E F (x̄1k ) + E 2ηL2 m +

1
2η



kx̄1k

i
− x0k k2 .

(74)

Define rk = E [F (yk ) − F ∗ ], and suppose rk > 0 for all k (otherwise the algorithm terminates in finite steps). Applying
the KŁ condition with θ = 1/2, we obtain
1
c (F (x)

1

− F ∗ ) 2 ≤ dist∂F (x) (0).

(75)

Setting x = x̄1k , we obtain

2
1
2
1
∗
1
(F
(x̄
)
−
F
)
≤
dist
L
+
kx̄1k − yk k2 .
1 ) (0) ≤
k
∂F
(x̄
η
k
c2

(76)

Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization

Taking expectation on both sides and using the result from eq. (74), we obtain
1
∗
E[F (xm
k )−F ]−
c2

1
2ηL2 m+ 2η
c2

2 


 
E kx¯k 1 − x0k k2 ≤ L + η1 E kx̄1k − yk k2 .

Note that x0k = yk . Then rearranging the above inequality yields

2
1
∗
m
∗
1
1
E[F
(y
)
−
F
E[F
(x
+
]
≤
)
−
F
]
≤
L
+
2
k+1
k
c
η
c2
2

≤

(L+ η1 )

+

1
2ηL2 m+ 2η
c2

2ηL2 m+ 1
2η
c2

1
2η −L−α





E kx̄1k − yk k2

(E[F (yk )] − E[F (yk+1 )]) ,

(77)

(78)
(79)

2

which can be rewritten as rk+1 ≤ d (rk − rk+1 ) with d =

rk+1

d
≤
rk ≤
d+1



1
c2 (L+ η1 ) +2ηL2 m+ 2η
1
2η −L−α

d
d+1

k+1

. Then, induction yields that

(F (y0 ) − F ∗ ) .

(80)

