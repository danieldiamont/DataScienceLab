Supplementary material:
Bayesian Optimization with Tree-structured Dependencies

Rodolphe Jenatton 1 Cedric Archambeau 1 Javier Gonzalez 2 Matthias Seeger 1

Abstract
In this supplementary material, we provide additional experimental results along with some details about the inference in our model and its nonlinear extension.

1. Experiments
In this section, we provide complementary experimental
results.
1.1. Optimization of synthetic tree-structured functions
In addition to the results presented in the core of the paper, we include simulations with the non-linear extension of
tree, denoted by tree-nonlinear, where the shared
parameters zp = [rv ]v∈Vp are modeled in a non-linear fashion (see details in Section 3 of the supplementary material).
Moreover, we consider as well settings where the shared
variables have a quadratic dependency in the leaf objectives,
on both balanced and unbalanced binary trees; see Figure 1
and Figure 2.
The trees on Figure 1 are referred to as small balanced,
while those on Figure 2 are referred to as small
unbalanced. We consider also higher-dimensional versions of those, with a depth of 4, resulting in respectively 8 and 9 leaves whose constant shifts are respectively
{a × 0.1}8a=1 and {a × 0.1}9a=1 (they are referred to as
large balanced and large unbalanced ).
As defined in the core paper, all the non-shared continuous
variables xj ’s are defined in [−1, 1], while the shared ones
are in [0, 1]. This implies that the best function value will
always be 0.1.
Amazon, Berlin, Germany. 2 Amazon, Cambridge, United
Kingdom. Correspondence to: Rodolphe Jenatton <jenatton@amazon.de>, Cedric Archambeau <cedrica@amazon.de>,
Javier Gonzalez <gojav@amazon.co.uk>, Matthias Seeger
<matthias@amazon.de>.

We can see that the observations made in the core of the paper are still valid for the unbalanced trees. In particular, with
linearly-dependent shared variables, tree-nonlinear
converges more slowly than the linear version tree (middle panels of Figures 3 and 4), which may be the price
to pay for having higher-dimensional latent variables c.
Moreover, in presence of quadratically-dependent shared
variables (right panels of Figures 3 and 4), we observe
that tree fails to model adequately the non-linearities,
while tree-nonlinear, as expected, can. In absence of
shared variables (left panels of Figures 3 and 4), tree and
tree-nonlinear are simply equivalent, which explains
why their curves are superimposed. Finally, as dimension
increases (i.e., going from the small to large settings),
the performance of independent worsens, while that of
tree and tree-nonlinear are barely affected.
1.2. Multi-layer perceptron tuning
We report in Figure 5 the results of all methods including the
non-linear extension of tree. The setting is identical to that
described in the core of the paper. We can see that the linear
version tree performs better than tree-nonlinear.
This conclusion, perhaps surprising, is in good agreement
with the recent observations from Zhang et al. (2016) where
simple linear models outperformed more sophisticated, nonlinear competing methods within the context of the optimization of data analytic pipelines.

2. Details about the Tree-structured
semi-parametric Gaussian process
regression model
We start by providing details about the posterior inference.
2.1. Posterior Inference

1

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

The joint distribution P (y, g, c) of our model is given by

Q

P (c)

p N (gp ; bp , Kp )N (yp ; gp

2
+ Z>
p c, σ Inp ), (1)

where Kp = [Kp (xi , xj )]i,j∈Ip are kernel matrices, with
the prior P (c) = N (c; 0, Σc ). Our goal is to obtain the posterior process P (gp (·)|c, yp ) and the posterior distribution

Supplementary material: Bayesian Optimization with Tree-structured Dependencies

The second expression has precisely the form of the posterior for a standard GP regression model: the first factor is
the GP prior P (gp ), while the second is the Gaussian likelihood with observations yp − Z>
p c and noise covariance
σ 2 Inp . Following Rasmussen & Williams (2006), we obtain
the posterior GP distribution as:

gp (·)|c, yp ∼ GP mp (·), Sp (·, ·) ,

x1
0
x2

0
x24

0

1
x25

+ 0.1

1

x26

+ 0.2

x3

1
x27 + 0.4

+ 0.3

x1
0

1

x2 , r 8
0

x3 , r9

x24 + 0.1+r8

1

0

1
x25 + 0.2+r8

x26 + 0.3+r9

x27 + 0.4+r9

>
where mp (x) = kp (x)> M−1
p (yp − Zp c − bp ) + bp and
0
0
>
−1
Sp (x, x ) = Kp (x, x ) − kp (x) Mp kp (x0 ). Next, to
obtain the posterior P (c|y), we note that

x1
0

1

x2 , r8

x3 , r 9

0
x24 + 0.1+(r8 − 0.5)2
x25

>

Q

P (c|y) ∝ N (c; 0, Σc )

1
0

+ 0.2+(r8 − 0.5)

p N (yp ; Zp

c + bp , Mp ).

1

2

x26 + 0.3+(r9 − 0.5)2
x27 + 0.4+(r9 − 0.5)2

Figure 1. Three examples of functions with tree-structured conditional relationships. Each inner node is a binary variable, and a
path in those trees represents successive binary decisions. The
leaves contain univariate quadratic functions that are shifted by
different constant terms. Top: Setting without shared variables.
Middle: r8 and r9 are shared variables that are common to the
functions at the leaves of their respective subtrees. In this example,
the shared variables have a linear dependency in the leaf objectives.
Bottom: Similar to the middle tree, except that r8 and r9 have a
quadratic dependency in the leaf objectives.

After some algebra, we can also re-arrange this expres>
1 >
sion into a quadratic
form P (c|y) ∝ e− 2 c Λc c+fc c ,
P
−1
−1
where fc =
+
p Zp Mp (yp − bp ) and Λc = Σc
P
−1 >
p Zp Mp Zp . Hence, we see that the posterior P (c|y) is
−1
the Gaussian distribution N (Λ−1
c fc , Λc ). Expressions
depending on this distribution are computed using the
Cholesky decomposition of Λc .
We next give details about the computation of the marginal
likelihood.
2.2. Marginal Likelihood
Recall the full joint from (1). Using (2) and dividing out
common terms leads to

Q

P (y)P (c|y) = N (c; 0, Σc )
P (c|y). This can be done by invoking standard multivariate Gaussian identities (Petersen & Pedersen, 2012) or by
directly reading of the posterior parameters after rewriting
the joint distribution into the following form:

>

p N (yp ; Zp

c + bp , Mp ).

Hence, we obtain the following expression for the logmarginal likelihood log P (y):

P

>

p log N (yp ; Zp c + bp , Mp ) + log P (c)− logP (c|y).

Q

P (y)P (c|y)

p P (gp |c, yp ).

3. Details about the non-linear extension of
tree

First, we can decompose the joint P (yp , gp |c):
P (gp |c, yp )N (yp ; Z>
p c + bp , Mp ),

(2)

where Mp = Kp + σ 2 Inp . In the sequel, we compute expressions such as M−1
p and log |Mp | by using the Cholesky
decomposition Mp = Lp L>
p . The second factor in (2) is
the distribution P (yp |c), obtained by integrating out gp :
Z
N (yp ; gp + Z>
p c, Mp )N (gp ; bp , Kp )dgp .
gp

It will play a role shortly. The first factor in (2) is the
posterior over the latent functions values:
2
P (gp |c, yp ) ∝ N (gp ; bp , Kp )N (yp ; gp + Z>
p c, σ Inp )
2
∝ N (gp ; bp , Kp )N (yp − Z>
p c; gp , σ Inp ).

For space limitation reasons, we have omitted in the core
paper the details describing how we use in practice the
random Fourier features (Rahimi et al., 2007).
For a given vertex v ∈ V with feature representation rv ∈
RDv , we associate the new Rv -dimensional representation
p

φv (rv ) , 2/Rv · cos Wv rv + qv
where cos(·) operates element-wise, Wv ∈ RRv ×Dv has entries identically and independently distributed from N (0, γ1 )
while qv ∈ RRv has its components generated uniformly in
[0, 2π]. Those randomly generated features can be shown
to approximate a RBF kernel with bandwidth γ 2 (Rahimi
et al., 2007).

Supplementary material: Bayesian Optimization with Tree-structured Dependencies
x1

x1

0

0

1

x2
x25 + 0.3

x4
x28

0

1

+ 0.1

x29

1

0

1

0

1

x2 , r10

x3
x26 + 0.4

x27 + 0.5

x25 + 0.3+r10

x4
0
x28

+ 0.2

x3 , r11
1

0

0
x26 + 0.4+r11

1
x27 + 0.5+r11

1
x29

+ 0.1+r10

+ 0.2+r10

x1
0

1

x2 , r10

x3 , r11
1

0
0

x4

1

x26 + 0.4+(r11 − 0.5)2

1
x25

0

x27 + 0.5+(r11 − 0.5)2

+ 0.3+(r10 − 0.5)2

x29 + 0.2+(r10 − 0.5)2
x28 + 0.1+(r10 − 0.5)2

Figure 2. Three examples of functions with tree-structured conditional relationships. Each inner node is a binary variable, and a path in
those trees represents successive binary decisions. The leaves contain univariate quadratic functions that are shifted by different constant
terms. Compared with Figure 2, the trees are not balanced anymore. Top left: Setting without shared variables. Top right: r10 and r11 are
shared variables that are common to the functions at the leaves of their respective subtrees. In this example, the shared variables have a
linear dependency in the leaf objectives. Bottom: Similar to the middle tree, except that r10 and r11 have a quadratic dependency in the
leaf objectives.

In practice, to address the problem of the selection of γ,
we consider a strategy consisting of approximating a sum
of RBF kernels for different values of γ. Our experiments
showed that taking γ ∈ {0.1, 1, 10} provided good results.
Moreover, we fix the dimension Rv = 3R for all vertices, where R corresponds to the dimension of φγv (rv ) ,
p
(γ)
2/Rv · cos(Wv rv + qv ) for each γ ∈ {0.1, 1, 10} with
(γ)
Wv having its entries drawn from N (0, γ1 ). The final representation φv (rv ) = [φγv (rv )]γ∈{0.1,1,10} is obtained via a
concatenation of the φγv ’s. In practice, we take R = 25.

References
Petersen, K. B. and Pedersen, M. S. The matrix cookbook.
Technical report, Technical University of Denmark, nov
2012. Version 20121115.
Rahimi, Ali, Recht, Benjamin, et al. Random features for
large-scale kernel machines. In Advances in Neural Information Processing Systems, volume 3, pp. 5, 2007.
Rasmussen, Carl and Williams, Chris. Gaussian Processes
for Machine Learning. MIT Press, 2006.
Zhang, Yuyu, Bahadori, Mohammad Taha, Su, Hang, and
Sun, Jimeng. Flash: Fast bayesian optimization for data
analytic pipelines. In Krishnapuram, Balaji, Shah, Mohak,
Smola, Alexander J., Aggarwal, Charu, Shen, Dou, and

Rastogi, Rajeev (eds.), KDD, pp. 2065–2074. ACM, 2016.
ISBN 978-1-4503-4232-2.

Supplementary material: Bayesian Optimization with Tree-structured Dependencies

1
2

1

0

3

1

small balanced - linearly shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

1

0
log10(Distance to optimum)

log10(Distance to optimum)

0

small balanced - no shared variables
arc
gp-baseline
independent
marginal
random
smac
tree
tree-nonlinear

log10(Distance to optimum)

1

2

3

1

small balanced - quad. shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

2

3

4

log10(Distance to optimum)

0
1
2

20

30
40
Iterations

50

60

4
0

70

large balanced - no shared variables
arc
gp-baseline
independent
marginal
random
smac
tree
tree-nonlinear

1

0

3

1

10

20

30
40
Iterations

50

60

4
0

70

large balanced - linearly shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

1

0
log10(Distance to optimum)

1

10

log10(Distance to optimum)

50

2

3

1

10

20

30
40
Iterations

50

30
40
Iterations

50

60

70

large balanced - quad. shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

2

3

4
50

10

20

30
40
Iterations

50

60

70

4
0

10

20

30
40
Iterations

50

60

70

4
0

10

20

60

70

Figure 3. Optimization performance over synthetic tree-structured functions, as measured by the log10 distance to the (known) minimum
versus the number of iterations. Top: Results for the small balanced binary trees displayed in Figure 1, without (left), with linearlydependent (middle) and quadratically-dependent (right) shared variables. Bottom: Results for larger balanced binary trees with depth 4
and a total of 8 leaves, without (left), with linearly-dependent (middle) and quadratically-dependent (right) shared variables. Best seen in
color.

Supplementary material: Bayesian Optimization with Tree-structured Dependencies

1
2

1

0

3

1

small unbalanced - linearly shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

1

0
log10(Distance to optimum)

log10(Distance to optimum)

0

small unbalanced - no shared variables
arc
gp-baseline
independent
marginal
random
smac
tree
tree-nonlinear

log10(Distance to optimum)

1

2

3

1

small unbalanced - quad. shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

2

3

4

1

large unbalanced - no shared variables
arc
gp-baseline
independent
marginal
random
smac
tree
tree-nonlinear

log10(Distance to optimum)

0
1
2

20

30
40
Iterations

50

60

4
0

70

1

0

3

1

10

20

30
40
Iterations

50

60

4
0

70

large unbalanced - linearly shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

1

0
log10(Distance to optimum)

10

log10(Distance to optimum)

50

2

3

1

10

20

30
40
Iterations

50

30
40
Iterations

50

60

70

large unbalanced - quad. shared variables
arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

2

3

4
50

10

20

30
40
Iterations

50

60

70

4
0

10

20

30
40
Iterations

50

60

70

4
0

10

20

60

70

Figure 4. Optimization performance over synthetic tree-structured functions, as measured by the log10 distance to the (known) minimum
versus the number of iterations. Top: Results for the small unbalanced binary trees displayed in Figure 2, without (left), with linearlydependent (middle) and quadratically-dependent (right) shared variables. Bottom: Results for the large unbalanced binary trees with depth
4 and a total of 9 leaves, without (left), with linearly-dependent (middle) and quadratically-dependent (right) shared variables. Best seen
in color.

Supplementary material: Bayesian Optimization with Tree-structured Dependencies

Rank across all datasets (shared topology)

7

6

Mean rank

5

arc
gp-baseline
independent
random
smac
tree
tree-nonlinear

4

3

2

1
0

10

20

30

40

Iterations

50

60

70

80

90

Figure 5. Tuning of a multi-layer perceptron for binary classification. Average rank aggregated over 45 datasets versus the number of
iterations (lower is better; see details in the main article). Best seen in color.

