Clustering High Dimensional Dynamic Data Streams

A

Proofs of Section 3

Proof of Lemma 2.2. Fix an i and consider a grid Gi . For each center zj , denote Xj,α the indicator random variable for the
event that the distance to the boundary in dimension α of grid Gi is at most ∆/(2i+1 d). Since in each dimension, if the
center is close to a boundary, it contributes a factor at most 2 to the total number of close cells. It follows that the number
of cells that have distance at most ∆/(2i+1 d) to zj is at most,
Pd

N =2
Xj,α

Defining Yj,α = 2

α=1

Xj,α

.

, we obtain,
"
E[N ] = E

#

d
Y

Yj,α =

α=1

d
Y

E[Yj,α ].

α=1

We have that P r[Xj,α = 1] ≤ 1/d and so we get,
E[Yj,α ] ≤ E [1 + Xj,α ] = 1 + E[Xj,α ] ≤ 1 + 1/d.
Qd

Thus E(N ) = α=1 E[Yj,α ] ≤ (1+1/d)d ≤ e. Thus the expected number of center cells is at most (1+1/d)d |Z| ≤ e|Z|.
By Markov’s inequality, the probability that we have more than e|Z|(L+1)/ρ center cells in each grid is at most ρ/(L+1).
By a union bound, the probability that in any grid we have more than e|Z|(L + 1)/ρ center cells is at most ρ.
√
i
Proof of Lemma 3.4. Let L0 = L + 1. Note that for each point p ∈ P , |d(cip , Z) − d(ci−1
p , Z)| ≤ ∆ d/2 . Denote
P
P
i+1
i+1
i
i
Â = p∈S (d(cp , Z) − d(cp , Z))/πi and A = p∈∪{C∈C} (d(cp , Z) − d(cp , Z)). We have that E(Â) = A. Let
Xp := Ip∈S (d(cip , Z) − d(ci+1
p , Z))/πi ,
where
is the indicator function that p ∈ S. Then we have that V ar(Xp ) ≤ ∆2 d/(4i πi ) and b := maxp |Xp | ≤
√ Ip∈S
i
∆ d/(2 πi ). By Bernstein’s inequality,
h
i
t2
−
P r |Â − A| > t ≤ 2e 2|P |∆2 d/(4i πi )+2bt/3
−

3×2i−1 t2 πi

t
≤ 2e (βOPT+ 3 )∆

√
d

.

(7)

0

By setting t = OPT/L , we have that


2L0 ∆dk
OPT
ρ
P r |Â − A| >
≤ 2e− ln ρ ≤ 0 dk .
0
L
L∆
Thus with probability 1−ρ/(L0 ∆dk ), Â is an OPT/L0 additive approximation to the sum

B

(8)
P

p∈P

d(cip , Z)−d(ci+1
p , Z).

Proof of Theorem 3.6

Before we prove this theorem, we first present Lemma B.1 and Lemma B.2. In Algorithm 1, for each level i ∈ [0, L], let Hi
be the set of cells in Gi whose frequencies are returned by HEAVY-HITTER in the RetrieveFrequency procedure.
c be the returned frequency of C. Let H 0 be the set of cells in Gi whose frequencies are returned by
For each C ∈ Hi , let |C|
i
a K-set in the RetrieveFrequency procedure. Then Hi and Hi0 are complements in Gi .
Lemma B.1. Let L0 = L + 1. Fix , ρ ∈ (0, 1/2). Let Z ∗ ⊂ [∆]d be a set of optimal k-centers for the k-median problem of
the input point set. For each i ∈ [0, L], if at most ekL0 /ρ cells C in Gi satisfy d(C, Z ∗ ) ≤ ∆/(2i+1 d), then with probability
1 − ρ/L0 , the following two statements hold:
P


d
c − |C|) d(c(C), Z) − d(c(C P ), Z) ≤ OPT
1.  C∈Hi (|C|
2L0 for every Z ⊂ [∆] .
2.

P

C∈Hi0

|C|diam(C) ≤ β OPT for β = 3d3/2

Proof of Lemma B.1. Let L0 = L + 1. Fix a value i ∈ [0, L] and then W = ∆/2i is the width of a cell in Gi . Since at most
ekL0 /ρ cells in Gi satisfy d(C, Z ∗ ) ≤ W/(2d), then of the remaining cells, at most 2kL0 /ρ cells can contain more than
OPT W
ρOPT
ρdOPT/(W kL0 ) points. This is because each such cells contribute at least ρd
W kL0 2d = 2kL0 to the cost which sums to
0
0
OPT. Therefore, at most (e + 2)L k/ρ cells contain more than ρdOPT/(W kL ) points.
The number of cells in grid Gi is at most N = (1 + 2i )d (and perhaps as few as 2id , depending on the random vector
v), so HEAVY-HITTER receives cells of at most N types. Enumerating all cells C ∈ Gi such that |Cj | ≥ |Cj+1 |, define
fj = |Cj |. Algorithm 1 sets k 0 = (e + 2)L0 k/ρ, and the additive error of the estimator of fi of HEAVY-HITTER is given

Clustering High Dimensional Dynamic Data Streams

qP
PN
N
0
0
2
by 0
j=k0 +1 fj ≤
j=k0 +1 fj . We know that for all j > k the value fj ≤ ρdOPT/(W kL ). Moreover, the sum
∗
2dOPT/W because each point is at distance at least W/(2d) to a point of Z . Under these two restraints, the grouping of
maximal error is p
with fj = ρdOPT/(W kL0 ) for k 0 < j ≤ k 0 + 2kL0 /ρ and fj = 0 for j > k 0 + 2kL0 /ρ. Then the additive
0
error becomes  2ρ/(kL0 )dOPT/W .
√
The error from
a single cell Cj is at most |fj − fˆj | dW , and HEAVY-HITTER gaurantees with probability 1 − δ that
p
|fj − fˆj | ≤ 0q 2ρ/(kL0 )dOPT/W for every j. Therefore to ensure total error over all k 0 cells is bounded by OPT/(2L0 ),
we set 0 ≤  8(2+e)ρ2 kd3 L03 . Setting δ = ρ/L0 , the above bound holds with probability at least 1 − ρ/L0 .
P
For the second claim, we must bound C∈H 0 |C|. Hi consists of the top k 0 cells when ordered by value of fˆj . This
i
may differ from the toppk 0 cells when ordered by value of fj , but if j and j 0 change orders between these two orderings
then |fj − fj 0 | ≤ 20 2ρ/(kL0 )dOPT/W . qSince the sum may swap up to k 0 indices, the difference is bounded by
p
ρ
2k 0 0 2ρ/(kL0 )dOPT/W . By setting 0 ≤
8(2+e)2 dkL0 , we can ensure that the difference is at most dOPT/W . We
√
PN
P
know that j=k0 +1 fj ≤ 2dOPT/W , and so C∈H 0 |C| ≤ 3dOPT/W . For all cells C ∈ Gi , diam(C) = dW . Therefore
i
P
3/2
OPT.
C∈H 0 |C|diam(C) ≤ 3d
i

Lemma B.2. Let L0 = L + 1. In Algorithm 1, fixing , ρ ∈ (0, 1/2), o ∈ O and i ∈ [0, L], if OPT/2 ≤ o ≤ OPT, then
0
4 03
k
with probability 1 − ρ/(L0 ∆kd ), at most (2+e)L
+ 24d2L k ln ρ1 cells of Gi contain a point of Si,o .
ρ
Proof. Similar to the proof of Lemma B.1, there are at most k 0 = (2+e)L0 k/ρ cells C in Gi that satisfy |C| ≥ ρdOPT/(W k)
and/or d(C, Z ∗ ) ≤ W/(2d). Considering the other cells, together they contain at most 2dOPT/W points. So by a Chernoff
bound, with probability 1 − ρ/(L0 ∆kd ) at most O(2dπi,o OPT/(W ρ)) ≤ 24d4 L03 k ln ρ1 /2 points are sampled. The claim
follows since each non-empty cell must contain at least one point.
Proof of Theorem 3.6. Let L0 = L + 1. W.l.o.g. we assume ρ ≥ ∆−d , since otherwise we store the entire set of points and
the theorem is proved. By Lemma 2.2, with probability at least 1 − ρ, for every level i ∈ [0, L], at most ekL0 /ρ cells C in
Gi satisfy d(C, Z ∗ ) ≤ ∆/(2i+1 d). Conditioning on this event, we will show 1) in the query phase, if o∗ ≤ OPT, then with
probability at least 1 − 4ρ, S is the desired coreset; 2) there exists o ≤ OPT in the guesses O = {1, 2, 4, . . . , ∆d+1 } such
that with probability 1 − 4ρ, none of the K-set structures return Nil. 1) and 2) guarantee the correctness of the algorithm.
Note that one can always rescale ρ to ρ/9 to achieve the correct probability bound. Finally, we will bound the space, update
time and query time of the algorithm.
To show 1), we first note that the coreset size is at most O(KL) as desired. Then by Lemma 3.2, we only need to show
that with probability at least 1 − 4ρ, for any k-set Z ⊂ [∆]d and any level i ∈ [−1, L],
d i , Z)| ≤ OPT ,
|cost(Gi , Z) − cost(G
L0
c is returned by RetrieveFrequency. For each level i, we denote Ci as the set of cells
where the value of each |C|
that gets frequency from a HEAVY-HITTER instances in the RetrieveFrequency procedure, and Si = {p ∈ C :
C 6∈ Ci , ho∗ ,i (p) = 1} be the set of points sampled in the rest of cells. Since KSo∗ ,i does not return Fail, then for each
c = |Si ∩ C|/πi (o∗ ). Fix a k-set Z ⊂ [∆]d , we rewrite the cost as,
C ∈ Gi \Ci , |C|
X
X
∗
c d(c(C), Z) − d(c(C P ), Z) +
d i , Z) =
cost(G
|C|
(d(cip , Z) − d(ci−1
p , Z)))/πi (o ),
C∈Ci

p∈Si

where C is the parent cell of C in grid Gi−1 . By Lemma B.1 we have that, with probability at least 1 − ρ/L0 , for every
Z ⊂ [∆]d ,


X
 OPT
X




c d(c(C), Z) − d(c(C P ), Z) −
|C|
|C| d(c(C), Z) − d(c(C P ), Z)  ≤
,



2L0
P

C∈Ci

P

C∈Ci

3/2

and that, C∈Gi \Ci |C|diam(C) ≤ 3d OPT. Conditioning on this event, by Lemma 3.4, with probability at least 1 −
ρ/(L0 ∆kd ),


X

X

 OPT

i
i−1
P

|C| d(c(C), Z) − d(c(C ), Z)  ≤
(d(cp , Z) − d(cp , Z)))/πi −
.

2L0
p∈Si

C∈Gi \Ci

Clustering High Dimensional Dynamic Data Streams

By a union bound, we show with probability at least 1 − 4ρ, for any k-set Z ⊂ [∆]d and any level i ∈ [−1, L],
d i , Z)| ≤ OPT ,
|cost(Gi , Z) − cost(G
L0
as desired.
To show 2), we will consider some OPT/2 ≤ o ≤ OPT. By Lemma B.2 with probablity at least 1 − ρ/∆kd , the total
0
4 03
k
number of cells occupied by sample points in each level is upper bounded by K = (2+e)L
+ 24d2L k ln ρ1 . Thus by the
ρ
guarantee of the K-Set structure, with probability at least 1 − ρ, none of the KSo,0 , KSo,1 . . . , KSo,L will return Fail.
2
The memory requirement of the algorithm is determined by the L instances of HEAVY-HITTER and
 the dL instances
N
1
0
of K-set. By Theorem 3.5, each instance of HEAVY-HITTER requires O (k + 02 ) log δ log m bits of space. Here
N ≤ (1 + ∆/W )d ≤ ∆d and m is the maximum number of elements active in the stream. Since we require that at most
one point
location at the same time, we have that m ≤ N . The parameters
are setto k 0 = (2 + e)Lk/ρ,
 qexists at each 

0 =



ρ
8(2+e)2 kd3 L3

, and δ = ρ/L. This translates to a space bound of O dL + log

1
ρ

d4 L5 k
ρ2

bits. For each

K-Set data structure, it requires

O(KdL log(KL/ρ)) = O

d5 L4 k dkL2
+
2
ρ


log

dkL
ρ


 6 6
2
4
log dkL
bits of space. In total, there are O(dL2 ) K-Set instances and thus all K-Set instances cost O d L2 k + d kL
ρ
ρ
bits of space. By the same argument as in the offline case, the last paragraph of the proof of Theorem 3.3, the size of the
coreset is at most O((k 0 + K)L) = O(d4 kL4 −2 + kL2 /ρ) points. Finally, to derandomize the fully random functions, we
use Nissan’s pseudorandom generator (Nisan, 1992) in a similar way used in (Indyk, 2000b). But our pseudo-random bits
only need to fool the sampling part of the algorithm rather than whole algorithm. We consider an augmented streaming algorithm A that does exactly the same as in CoreSet but with all the HEAVY-HITTER operations removed. Thus all K-set
instances will have identical distribution with the ones in algorithm CoreSet. A uses O(KdL log(KL/ρ)) bits of space.
To fool this algorithm, using Nissan’s pseudo-random generator,
of random
 seed to generate the hash functions
 7 7the length
d3 kL5
dkL
d kL
d
+ ρ
log ρ . This random seed is thus sufficient
we need is of size O(KdL log(KL/ρ) log(|O|∆ )) = O
2
 7 7


3
5
d5 kL6
d kL
to be used in Algorithm CoreSet. Thus the total space used in the algorithm is O
+ d kL
log dkL
2
ρ
ρ + 2 ρ
bits.
Regarding the update time, for the HEAVY-HITTER operations, it requires O(L log N ) = O(dL2 ) time. For the Kset operations, it requires |O|LO(log(KL/ρ)) = dL2 log(dkL/(ρ)) time. The de-randomized hash operation takes
O(dL) more time per update. The final query time is dominated by the HEAVY-HITTER data structure, which requires
poly(d, k, L, 1/) time.

C

Full Construction of Positively Weighted Coreset

In this section, we will introduce a modification to our previous coreset construction, which leads to a coreset with all
positively weighted points. The high level idea is as follows. When considering the estimate of the number of points in
a cell, the estimate is only accurate when it truly contains a large number of points. However, in the construction of the
previous section, we sample from each cell of each level, even though some of the cells contain a single point. For those
cells, we cannot adjust their weights from negative to positive, since doing so would introduce large error. In this section,
we introduce an ending level to each point. In other words, the number of points of a cell is estimated by sampling only if
it contains many points. Thus, the estimates will be accurate enough and allow us to rectify the weights to be all positive.
This section is organized as follows. We reformulate the telescope sum in Subsection 4.1, provide a different construction
(still with negative weights) in Subsection 4.2, modify our different construction to output non-negative weights in Subsection 4.3, and move this construction into to the streaming setting in Subsection 4.4. For simplicity of presentation, we
will use λ1 , λ2 , . . . to denote some fixed positive universal constants.
C.1 Reformulation of the Telescope Sum
Definition C.1. A heavy cell identification scheme H is a map H : G → {heavy, non-heavy} such that, h(C−1 ) =heavy
and for cell C ∈ Gi for i ∈ [0, L]
1. if |C| ≥

2i ρdOPT
k(L+1)∆

then H(C) = heavy;

2. If H(C) = non-heavy, then H(C 0 ) = non-heavy for every subsell C 0 of C.
3. For every cell C in level L, H(C) = non-heavy.

Clustering High Dimensional Dynamic Data Streams

4. For each i ∈ [0, L], |{C ∈ Gi : H(C) = heavy}| ≤

λ1 kL
ρ ,

where λ1 ≤ 10 is a positive universal constant.

The output for a cell not specified by the above conditions can be arbitrary. We call a cell heavy if it is identified heavy
by H. Note that a heavy cell does not necessarily contain a large number of points, but the total number of these cells is
always bounded.
In the sequel, heavy cells are defined by an arbitrary fixed identification scheme unless otherwise specified.
Definition C.2. Fix a heavy cell identification scheme H. For level i ∈ [−1, L], let C(p, i) ∈ Gi be the cell in Gi containing
p. The ending level l(p) of a point p ∈ P is the largest level i such that H(C(p, i)) =heavy, and H(C(p, i+1)) =non-heavy.
Note that the ending level is uniquely defined if a heavy cell identification scheme is fixed. We now rewrite the telescope
sum for p as follows,
l(p)
X

l(p)
p=
cip − ci−1
+ cL
p
p − cp ,
i=0

c−1
p

= 0 and
= p. For arbitrary k-centers Z ⊂ [∆]d , we write,

Pl(p)
l(p)
L
d(p, Z) = i=0 d(cip , Z) − d(ci−1
p , Z) + d(cp , Z) − d(cp , Z) + d(0, Z)
Let Pl be all the points with ending level l(p) = l. We now present the following lemmas.
where

cL
p

Lemma C.3. Let Pi be the set of points with ending level i. Let Z ∗ ⊂ [∆]d be a set of optimal k-centers for the kmedian problem of the input point set. Assume that for each i ∈ [−1, L], at most ek(L + 1)/ρ cells C in Gi satisfy
d(C, Z ∗ ) ≤ ∆/(2i+1 d). Then
√
∆ d
≤ λ2 d3/2 OPT,
|Pi | ·
2i
where λ2 > 0 is a universal constant.
Before we prove this lemma, we first introduce the following lemmas to bound the cells with a large number of points.
Lemma C.4. Assume that for each i ∈ [0, L], at most ek(L + 1)/ρ cells C in Gi satisfy d(C, Z ∗ ) ≤ ∆/(2i+1 d). Then for
2i ρdOPT
any r > 0 there are at most (e+2r)k(L+1)
cells that satisfy |C| ≥ rk(L+1)∆
.
ρ
Proof of Lemma C.4. Let L0 = L + 1. Fix a value i ∈ [0, L] and then W = ∆/2i is the width of a cell in Gi . Since at most
OPT W
ρOPT
ekL/ρ cells in Gi satisfy d(C, Z ∗ ) ≤ W/(2d), then of the remaining cells, each contribute at least ρd
rW kL0 2d = 2rkL0 to the
0
∗
cost, and the cost of these cells is at most OPT. Therefore there can be at most 2rL k/ρ cells such that d(C, Z ) > W/(2d).
Along with the at most ekL0 /ρ cells (by the assumption) such that d(C, Z ∗ ) ≤ W/(2d), there are at most (e + 2r)L0 k/ρ
cells that contain at least ρdOPT/(rW kL0 ) points.
Lemma C.5. Assume that for each i ∈ [0, L], at most ek(L + 1)/ρ cells C in Gi satisfy d(C, Z ∗ ) ≤ ∆/(2i+1 d). Then for
i ∈ [−1, L], the points of Pi can be partitioned to at most k 0 = 2(e+6)k(L+1)
groups, G1 , G2 , . . . , Gk0 , such that for each
ρ
i−1

ρdOPT
.
j ∈ [k 0 ], there exists a C ∈ Gi , such that Gj ∈ C, |Gj | < 5 2 k(L+1)∆

Proof of Lemma C.5. Let L0 = L + 1. For each heavy cell in Gi , if the number of points falling into its non-heavy subcells
i−1
ρdOPT
(in Gi+1 ) is less than 2 kL
, we group all these subcells into a single group. Let the groups formed this way be called
0∆
type I, and by Property 4 of Definition 4.1 there are at most (e + 4)kL0 /ρ type I groups.
For each of the remaining
cells in Gi , we group
its subcells into groups such that each group contains a number
h heavy

2i−1 ρdOPT
2i−1 ρdOPT
of points in the interval
, 5 kL0 ∆
. This can be done since each non-heavy subcell contains less than
kL0 ∆
2i+1 ρdOPT
kL0 ∆

i−1

i−1

ρdOPT
ρdOPT
= 4 2 kL
points, and the total number of points contained in them is at least 2 kL
(otherwise we
0∆
0∆
would have formed a type I group). Let the groups formed this way be called type II. By the assumption of of Lemma C.3,
0
∆
∗
at most ekL
ρ of these non-heavy subcells are within distance 2i+2 d from an optimal center of Z . Since each type II group
i−1

ρdOPT
contains at least 2 kL
points, by the same argument as in Lemma C.4, the number of type II groups further than
0∆
0
∆
distance 2i+2 d from an optimal center is at most 8kL
ρ . We conclude that,

k0 ≤

(e + 4)kL0
(e + 8)kL0
+
.
ρ
ρ

Clustering High Dimensional Dynamic Data Streams

Proof of Lemma C.3. Let L0 = L+1. Fix a value i ∈ [−1, L] and then W = ∆/2i is the
upper bound of the width of√a cell
√
P
P
i+1
ρdOPT ∆ d
in Gi . Let G1 , G2 , . . . Gk0 be group of points satisfying Lemma C.5. Thus, p∈Pl ∆2i d ≤ j∈[k0 ] 2 kL
· 2i ≤
0∆
λ0 kL0
ρ

·

√
2i+1 ρdOPT ∆ d
kL0 ∆
2i

≤ λ2 d3/2 OPT for some universal constants λ0 and λ2 .

Proof of Proposition C.10. First notice that the weighted set satisfies the about condition is an -coreset. If we replace each
ci | by the exact number of points in |Ci |, then the new weighted set is an (/2)-coreset. For each C ∈ G, let bC be the new
|C
value returned by the algorithm, and bq is the new value of a point q ∈ S. The error of the cost introduced is at most,
 √


L
X
X  1
X
 ∆ d
c − bC | +



.
A=
||C|
 πi−1 − bp 
2i
i=0
C∈Gi : heavy

p∈Si−1

By the procedure, the new value of a cell is always smaller than its original value, thus

 √

L
L
X
X
X  1
∆ d X
c − bC +

A=
|C|
− bp  i =
gi ,
πi−1
2
i=0
i=0
C∈Gi : heavy

where

p∈Si−1


gi = 


X

c − bC +
|C|

C∈Gi :heavy

Let

X
p∈Si−1


fi = 




c  +
|C| − |C|

X

C∈Gi :heavy

X
C 0 ∈Gi−1 :heavy

1
πi−1

− bp 

√
∆ d
.
2i

 √



 |Si−1 ∩ C 0 |
∆ d

− |Pi−1 ∩ C 0 | i .
 πi−1
2

Thus
fi ≤ OPT/L by choosing
appropriate λ6 .
Now consider heavy cell C ∈


P

|Si ∩C| 
c
bC − C∈Gi+1 :heavy |C| − πi . Then,





X


|S
∩
C|
i
c + |C|
c − |C| −
c0 | − |C 0 |) −
sC = bC − |C|
(|C
− |Pi ∩ C| 
πi


C 0 ∈Gi+1 :heavy



 


  |S ∩ C|
X


 c0

i
c  + |C|
c − |C| +
≤ bC − |C|
− |Pi ∩ C| .
|C | − |C 0 | + 
πi
0

Gi , let sC

=

(9)

C ∈Gi+1 :heavy

Then
gi =

X
C∈Gi−1


 √
√
∆ d  X
1
1
1
∆ d
sC i +
− bp  i ≤ gi−1 + fi−1 + fi .
2
πi−1
2
2
2

(10)

p∈Si−1

Since g−1 = f−1 = 0, thus
gi ≤ fi + 3

i−1
X
j=0

2j−i fj , and

L
X
i=0

gi ≤

L
X
i=0

fi (1 +

i
L
X
X
3
)
≤
4
fi ≤ 4OPT.
2j
j=1
i=1

Remark C.6. The multiset of centers of heavy cells with each assigned a weight of the number of points in the cell is a
l(p)
O(d3/2 )-coreset. This can be easily seen by removing the term of d(cL
p , Z) − d(cp , Z) from Equation (C.1) together with
Lemma C.3, which bounds the error introduced by this operation.
C.2 The New Construction (with arbitrary weights)
For these heavy cells, we use HEAVY-HITTER algorithms to obtain accurate estimates of the number of points in these
cells, thus providing a heavy cell identification scheme. For the non-heavy cells, we only need to sample points from the
bottom level, GL , but with a different probability for points with different ending levels. We present the following lemma
that governs the correctness of sampling from the last level.
√
Lemma C.7. If a set of points Pi ⊂ P satisfies |Pi |∆ d/(2i ) ≤ β OPT for some β ≥ 2/(3(L + 1)), let Si be an

Clustering High Dimensional Dynamic Data Streams

independent sample from Pi such that p ∈ Si with probability
!
√
3a(L + 1)2 ∆ dβ 2∆kd (L + 1)
πi ≥ min
ln
,1
2i 2 o
ρ
where 0 < o ≤ aOPT for some a > 0. Then for a fixed set Z ⊂ [∆]d , with probability at least 1 − ρ/((L + 1)Deltakd ),
P
P
OPT .
| p∈Si (d(cip , Z) − d(p, Z))/πi − p∈Pi (d(cip , Z) − d(p, Z))| ≤ L+1
Proof. The proof is identical to that of Lemma 3.4.
√
βρ
i
Lemma C.8. Consider a set of sets {Pi }L
i=0 which satisfies |Pi |∆ d/(2 ) ≤ k(L+1) OPT for some β ≥ /(3(L + 1)).
For each i ∈ [0, L], let Si be an independent sample from Pi with sampling probability
!
√
4aβk(L + 1)3 ∆ d
2
πi ≥ min
log , 1
2i 2 ρo
δ
where 0 < o ≤ aOPT for some a > 0, then with probability at least 1 − δ,

 √
 |Si ∩ Pi |
∆ d
ρOPT

.
− |Pi | i ≤
 πi
2
k(L + 1)2
Proof of Lemma C.8. The proof is simply by Bernstein inequality. Let t =

i

√ 2 ρOPT ,
dk(L+1)2 ∆

Xp := Ip∈Si /πi , then we have

that V ar(Xp ) ≤ 1/πi and b := maxp |Xp | ≤ 1/πi . By Bernstein’s inequality, for any j ∈ [k 0 ],



 |Pj ∩ Si |

t2
−


Pr 
− |Pj | > t ≤ 2e 2|C|/πi +2bt/3 ≤ δ.
πi
We now describe the new construction. This essentially has the same gaurantee as the simpler construction from the
previous section, however the benefit here is that (as shown in the next subsection) it can be modified to output only
c are given as a blackbox. In proposition C.9 we specify the
positive weights. In the following paragraph, the estimations |C|
conditions these estimations must satisfy.
Non-Negatively Weighted Construction Fix an arbitrary heavy cell identification scheme H. Let Pl be all the points
c be an estimation of number of points of |C|, we also call
with ending level l(p) = l. For each heavy cell C, let |C|
0
c
c
|C| the value of cell C. For each non-heavy cell C , let |C 0 | = 0. Let S be a set samples of P constructed as follows:
S = S−1 ∪ S0 ∪ S1 , ∪ . . .∪ SL , where 
Sl is a set
 of i.i.d samples from Plwith probability πl . Here πl for l ∈ [−1, L]
2

2

dk

2

3

2

∆
log 2L∆
+ λ42di kL
log 30kL
is redefined as,πl = min λ32dl ∆L
2 ρo
2o
ρ
ρ2 , 1 where λ3 > 0 and λ4 > 0 are universal
constants. Our coreset S is composed by all the sampled points in S and the cell centers of heavy cells, with each point p
assigned a weight 1/πl(p) and for each cell center c of a heavy cell C ∈ Gi , the weight is,
X
c−
c0 | − |Si ∩ C| .
(11)
wt(c) = |C|
|C
πi
0 0
0
C :C ∈Gi+1 ,C ⊂C,
C 0 is heavy

For each non-heavy cell C except for those in the bottom level, wt(c(C)) = 0. The weight of each point from S is the value
of the corresponding cell in the bottom level.
We now state the following proposition for a coreset construction, which immediately serves as an offline coreset construction.
Proposition C.9. Let H be an arbitrary heavy cell identification scheme. Fix Ω(∆−d ) ≤ ρ < 1 and for each heavy C ∈ Gi
i

c is an estimation of number of points in C with additive error at most
in level i, |C|
· 2 ρdOPT , where λ5 > 0 is a
λ5 Ld3/2

kL∆

universal constant. Let Sl be the set of i.i.d. samples of Pl with probability πl (o). If 0 < o ≤ OPT, then with probability
at least 1 − 4ρ, for every k-set Z ⊂ [∆]d ,


X

X



wt(q)d(q, Z) −
d(p, Z) ≤ OPT.

q∈S

p∈P

Clustering High Dimensional Dynamic Data Streams

And the coreset size |S| is

O

d3 L4 k
2




1
kL OPT
d + log
.
ρ
ρ
o

Proof of Proposition C.9. Fix a k-set Z ⊂ [∆]d . First notice that,
X
d
cost(Z)
=
wt(q)d(q, Z)
q∈S

=

L−1
X





i=−1






X

C∈Gi :C heavy


c
|C| −


X
C 0 :C 0 ∈Gi+1 ,C 0 ⊂C,
C 0 is heavy




X d(p, Z) 

c0 | − |Si ∩ C| 
|C
 d(c(C), Z) +

πi 
πi 
p∈Si




X d(p, Z) − d(cip , Z)
c

,
=
|C|(d(c(C),
Z) − d(c(C P ), Z)) +
(12)
πi
i=−1 C∈Gi :C heavy
p∈Si
P
P
where we denote d(c(C−1
), Z) = 0 for convenience. Let cost(Z) = p∈P d(p, Z). Note that we can also write the true
cost of Z as


L−1
X
X
X

cost(Z) =
|C|(d(c(C), Z) − d(c(C P , Z))) +
d(p, Z) − d(cip , Z) .
L−1
X

X

i=−1

C∈Gi :C heavy

p∈Pi

We have that,
d
cost(Z)
− cost(Z) = A1 + A2 ,
where
A1 =

L−1
X

X


i=−1

and





c − |C|)(d(c(C), Z) − d(c(C P , Z)))
(|C|

C∈Gi :C heavy


X d(p, Z) − d(cip , Z)
X

−
d(p, Z) − d(cip , Z) .
A2 =
π
i
i=−1
L−1
X



p∈Si

p∈Pi

Let Z ∗ ⊂ [∆]d be a set of optimal k-centers for the k-median problem of the input point set. By Lemma 2.2, with
∗
i+1
probability at most 1 − ρ, for each i ∈ [0, L], if at most ek(L + 1)/ρ cells C in
 Gisatisfy d(C, Z ) ≤ ∆/(2 d).
kL
0
Conditioning on this event, we have that, by Lemma C.4 there are at most k = O ρ heavy cells per level. Since for


c

2i ρdOPT

each C ∈ Gi , |C|
− |C| ≤ λ5 Ld
3/2 ·
kL∆ , by choosing appropriate constant λ5 > 0 we have



L−1
X  X

P
c

|A1 | ≤
(|C| − |C|)(d(c(C), Z) − d(c(C , Z)))


i=−1 C∈Gi :C heavy
√
2i ρdOPT ∆ d
OPT

·
·
≤
.
(13)
≤ L · k0 ·
i
3/2
kL∆
2
2
λ5 Ld
For A2 , let


X d(p, Z) − d(cip , Z)
X
A2i = 
−
d(p, Z) − d(cip , Z) .
πi
p∈Si

p∈Pi

√

By Lemma C.3, for each i ∈ [−1, L − 1], |Pi |∆ d/(2i ) = λ2 (d3/2 OPT). Thus by Lemma C.7, and choosing appropriate
OPT
constants, with probability at least 1 − ρ/(L + 1)∆dk , |A2i | ≤ 2(L+1)
. By the union bound, with probability at least 1 − ρ,
OPT
for every level i, and every k-set Z ⊂ [∆]d , |A2i | ≤ 2(L+1)
. Thus |A2 | ≤ OPT/2. In total, with probability at least
d
1 − 3ρ, |A1 + A2 | ≤ OPT for any k-set Z ⊂ [∆] .
The coreset size is the number of heavy cells plus the number of sampled points. The number of heavy cells is O(kL2 /ρ).

Clustering High Dimensional Dynamic Data Streams

The expected number of sampled points per level is at most,
 4 3
 
d L k d3 L2 k
kL
OPT
+
|Si | = O
log
.
2
2 ρ
ρ
o
By a Chernoff bound, with probability at least 1 − ρ/∆dk , for every level i ∈ [0, L], the number of sampled points is,
 
 4 3
kL
d L k d3 L3 k
OPT
+
log
|Si | ≤ O
.
2
ρ2
ρ
o
Thus the size of the coreset S is,
 3 4 


d L k
1
kL OPT
|S| ≤ O
d
+
log
.
2
ρ
ρ
o

C.3 Ensuring Non-Negative Weights
In this section, we will provide a procedure to rectify all the weights for the coreset constructed in the last sub-section. The
idea is similar to the method used in (Indyk & Price, 2011). The procedure is shown in Algorithm 4.3.
Proposition C.10. Let S be a weighted set constructed using the Non-Negatively Weighted Construction, i.e. each heavy
c and the set of sampled points S = S−1 ∪ S0 . . . ∪ SL with each point in Sl has weight 1/πl . If for each
cell C has value |C|
i
OPT for some universal constant λ > 0 and for each i ∈ [−1, L] and any

c − |C|| ≤
heavy cell C ∈ Gi , ||C|
· 2 ρd
6
kL∆
λ6 Ld3/2
k-set Z ⊂ [∆]d ,


X

X


i
i

wt(p)(d(cp , Z) − d(p, Z)) −
(d(cp , Z) − d(p, Z))

p∈S

p∈Pi
≤

OPT
,
2L

and
X
C∈Gi : heavy


 √
 |Si ∩ C|
∆ d
OPT

.
− |Pi ∩ C| i ≤
 πi
2
L

0
d
d
d
Then on input |C
1 |, |C2 |, . . . , |Ck0 | and S, where k is the number of heavy cells, the coreset output by Algorithm 4.3 is a
(4)-coreset.

C.4 The Streaming Algorithm
C.4.1 S AMPLING F ROM S PARSE C ELLS
For the streaming algorithm, we can still use HEAVY-HITTER algorithms to find the heavy cells. The major challenge
is to do the sampling for each point from its ending level. We do this using a combination of hash functions and K-Set.
In Algorithm C.4.1, we provide a procedure that recovers the set of points from cells with a small number of points and
ignore all the heavy cells. The guarantee is,
Theorem C.11. Given as input a set of dynamically updating streaming points P ⊂ [N ], a set of mutually disjoint cells
C ⊂ [M ], whose union covers the region of P . Algorithm C.4.1 outputs all the points in cells with less than β points or
output Fail. If with the promise that at most α cells from C contain a point of P , then the algorithm outputs Fail with
probability at most δ. The algorithm uses O(αβ(log(M β) + log N ) log N log(log N αβ/δ) log(αβ/δ)) bits in the worst
case.
The high level idea of this algorithm is to hash the original set of points to a universe of smaller size. For cells with less
points, the collision rate is much smaller than cells with more points. To recover one bit of a point, we update that bit and
the cell ID and also its hash tag to the K-Set-data structure. If there are no other points with hash values colliding with
this point, the count of that point is simply 1. If this is the case, we immediately recover the bit. By repeating the above
procedure once for each bit, we can successfully recover the set of points with no colliding hash tags. For those points with
colliding hash tags, we simply ignore them. Each point has a constant probability to collide with another point, thus not be
in the output. By running the whole procedure O(log(αβ/)) times in parallel, we reduce the probability to roughly  for
each point in cells with less than β points. To formally prove Theorem C.11, we first prove the following lemma, which is
the guarantee of Algorithm C.4.1.
Lemma C.12. Given input a set of dynamically updating streaming points P ⊂ [N ], a set of mutually disjoint cells
C ⊂ [M ], whose union covers the region of P . Algorithm C.4.1 outputs a set of points in cells with less than β points

Clustering High Dimensional Dynamic Data Streams

or output Fail. If with the promise that at most α cells from C contain a point of P , then the algorithm outputs Fail
with probability at most δ. Conditioning on the event that the algorithm does not output Fail, each point p from cell
with less than β points is in the output with marginal probability at least 0.9. The algorithm uses O(αβ(log(M β) +
log N ) log N log(log N αβ/δ)) bits in the worst case.
Proof. We prove this lemma by showing that (a) if a point p ∈ P contained in cell C, with |C ∩ P | ≤ β, then with
probability at least 0.99, there are no other points p0 ∈ C ∩ P with H(p) = H(p0 ), (b) conditioning on the event that the
algorithm does not output Fail, then for any cell C ∈ C, if a point p ∈ C such that no other points in C ∩ P has the same
hash value H(p), then p is in the output and (c) the algorithm outputs Fail with probability at most δ. The correctness of
the algorithm follows by (a), (b) and (c).
To show (a), consider any cell C ∈ C with |C| ≤ β, let p ∈ C with hash value H(p). Since H is 2-wise independent, the
expected number of other points hashed to the same hash value H(p) is at most β/U = 1/100. By Markov’s inequality,
with probability at least 0.99, no other point in C is hashed to H(p).
To show (b), notice that if the algorithm does not output Fail, then for a given cell C, let c be its ID, and pj be the j-th
bit of point p. Then (c, h, pj ) has 1 count and (c, h, 1 − pj ) has 0 count for each j ∈ [t], where t = dlog N e. Thus we can
uniquely recover each bit of point p, hence the point p.
For (c), since there are at most α cells, there are at most 2αU = O(αβ) many different updates for each KS structure.
Therefore, with probability at most δt , a single KS instance outputs Fail. By the union bound, with probability at least
1 − δ, no KS instance outputs Fail.
Finally, the space usage is dominated by the KS data structures. Since the input data to KS is from universe [M ] ×
[U ] × {0, 1}, each KS structure uses space O(αβ(log(M β) + log N ) log(tαβ/δ)) bits of memory, the total space is
O(αβ(log(M β) + log N ) log N log(tαβ/δ)).
Proof of Theorem C.11. Each instance of SparseCellsSingle fails with probability at most δ/(4A), where A is the
number independent SparseCellsSingle instances. By the union bound, with probability at least 1 − δ/4, none of
them output Fail. Conditioning on this event, the random bits of the hash functions of each SparseCellsSingle instance are independent, thus by Lemma C.12 a fixed point p ∈ C with |C| ≤ β is in the output with probability at least
4αβ
10− log δ ≤ δ/(4αβ). Since there are at most αβ points in cells with less than β points, by the union bound we conclude
that with probability at least 1 − δ/4, every point in cells with less than β points is in the output set S. In sum, with
probability at least 1 − δ/2, S contains all the desired points.
The other KS instance outputs Fail with probability at most δ/2. Thus if T is not Fail, then T contains the exact number
of points of each cell. If any desired point is not in S, then |C| > |C ∩ S|, we output Fail. This happens with probability
at most δ under the gaurantee of the KSstructure.
Since each SparseCellsSingle instance uses O(αβ(log(M β) + log N ) log N log(tAαβ/δ)) bits of space, the final
space of the algorithm is O(αβ(log(M β) + log N ) log N log(tαβ/δ) log(αβ/δ)).

Algorithm 4 SparseCells(N, M, α, β, δ): input the point sets P ⊂ [N ] and set of cells C ⊂ [M ] such that at most α
cells containing a point, output the set of points in cells with less than β points.
Let A ← log 4αβ
δ ;
Let R1 , R2 , . . . , RA be the results of independent instances of SparseCellsSingle(N, M, α, β, δ/(4A)) running in
parallel;
Let T be the results of another parallel KS structure with space parameter α and error δ/2 and with input as the cell IDs of
points in P ; /*T returns the exact counts of each cell*/
if any of the data structures returns Fail:
return Fail;
Let S ← R1 ∪ R2 ∪ . . . RA ;
if ∃ set C ∈ T with |C| ≤ β and |C| 6= |C ∩ S|:
return Fail;
return S;
C.4.2 T HE A LGORITHM
With the construction of algorithm SparseCells, we now have all the tools for the streaming coreset construction. The
streaming algorithm is composed by O(L) levels of HEAVY-HITTER instances, which serve as a heavy cell identifier and

Clustering High Dimensional Dynamic Data Streams

Algorithm 5 SparseCellsSingle(N, M, α, β, δ): input the point sets P ⊂ [N ] and set of cells C ⊂ [M ] such that at
most α cells containing a point, output the set of points in cells with less than β points.
Initization:
U ← 100β .
t ← dlog N e;
H : [N ] → [U ], 2-wise independent;
K-Set structures KS1 , KS2 , . . . KSt with space parameter 2αU and probability δt ;
Update(p, op): /*op ∈ {Insert, Delete}*/
c ← cell ID of p;
for i ∈ [t]:
/*A point p is represented as (p1 , p2 , . . . , pt )*/;
pi ← the i-th bit of point p;
KSi .update((c, H(p), pi ), op);
Query:
if for any i ∈ [t], KSi returns Fail:
return Fail;
S ← ∅;
for each (c, h, p1 ) in the output of KS1 :
if (c, h, pj ) ∈
/ KSj for some j ∈ [t]:
/*A checking step, may not happen at all*/;
return Fail;
Let s(c, h, pj ) be the counts of (c, h, pj ) in KSj ;
if s(c, h, pj ) = 1 and s(c, h, 1 − pj ) = 0 for each j ∈ [t]:
p ← (p1 , p2 , . . . , pt );
S ← S ∪ {p};
return S

by O(L) levels of SparseCells instances, which sample the points from their ending levels. The full algorithm is stated
in Algorithm 6. The guarantee of the algorithm is stated in the following theorem.
Theorem C.13. Fix , ρ ∈ (0, 1/2), positive integers k and ∆, Algorithm 6 makes a single pass over the streaming
point set P ⊂ [∆]d , outputs a weighted set S with non-negative
weights for
h

i each point, such that with probability at
3

4

least 0.99, S is an -coreset for k-median of size O d L2 k d + ρ1 log kL
, where L = log ∆. The algorithm uses
ρ
h 7 7 

i
2 dkL
dkL
O d L2 k ρdL + ρ1 log2 dkL
bits in the worst case. For each update of the input, the algoρ (log log ρ + L) log
ρ

rithm needs poly (d, 1/, L, log k) time to process and outputs the coreset in time poly(d, k, L, 1/, 1/ρ, log k) after one
pass of the stream.
Proof. W.l.o.g. assume ρ ≥ ∆−d , since otherwise we can store the entire set of points. In the sequel, we will prove the
theorem with parameter O(ρ) and O(). It translates to ρ and  directly by scaling and with losing at most a constant factor
in space and time bounds. By Lemma 2.2, with probability at least 1 − ρ, for every level i ∈ [0, L], at most ekL/ρ cells C
in Gi satisfy d(C, Z ∗ ) ≤ ∆/(2i+1 d). We condition on this event for the following proof.
We first show that the HEAVY-HITTER instances faithfully implement a heavy cell identification scheme. First note that
with probability at least 1 − ρ, all HEAVY-HITTER instances
q succeed. Conditioning on this event for the following proof.
As shown in the proof of Lemma B.1, by setting 0 =  λ7 kdρ3 L3 and k 0 = λ8 kL/ρ, for appropriate positive universal
i


dOPT
constants λ7 , λ8 , then the additive error to each cell is at most λ9 d3/2
· 2 kL∆
for some universal constant λ9 , which
L
matches the requirement of Proposition C.10. For each cell C with at least 2i ρdOPT/(k(L + 1)∆) points, by Lemma C.4
it must be in the top (e + 2)k(L + 1)/ρ cells. For each cell C 0 with at least 2i−1 ρdOPT/(k(L + 1)∆) points, it must be in
2i dOPT
2i dOPT
the top (e + 4)k(L + 1)/ρ cells. Since the additive error is λ7 d3/2(L+1) · k(L+1)∆
 21 k(L+1)∆
. Thus C is in the output
1 2i dOPT

2i dOPT
c
of the HEAVY-HITTER instances, since otherwise |C| ≤
+
·
contradicts the error bound
2 k(L+1)∆

λ7 d3/2 (L+1)

k(L+1)∆

Clustering High Dimensional Dynamic Data Streams

(by choosing sufficiently large λ7 ). Thus the algorithm faithfully implements a heavy cell identification scheme.
Now we show that if there exists an o ≤ OPT such that no instance of SparseCells outputs Fail, then the result is a
desired O()-coreset.
This follows by
h
iProposition C.9 and Proposition C.10. Then we note that the coreset size is upper

bounded by O

d3 L4 k
2

d+

1
ρ

log

kL
ρ

as desired.

Next we show that there exists an OPT/2 ≤ o∗ ≤ OPT that with probability at least 1 − ρ, no SparseCells instance SCo∗ ,i outputs Fail. By Chernoff
at
h 3bound,
 with probability

i least 1 − O(ρ), as also shown in the proof
d L4 k
1
kL OPT
of Proposition C.9, per level at most O
d + ρ log ρ
cells is occupied by a point. And at most
2
o
h 3 2
i
ρ
d L
kL
L
O 2 ρd + log ρ + kL log ρ
points is sampled per light cell. Conditioned on this fact and that each instances
fails with probability at most O(ρ/(dL)), with probability at least 1 − O(ρ), no nstance SCo∗ ,i fails.
Lastly,
usage and update/query time. For the HEAVY-HITTER instances, the total space used
 we boundthe4 space
1 d L5 k
is O dL + log ρ
bits, analogous to the proof of Theorem 3.6. Each SparseCells instance uses space
ρ2
i
i
h 5 4 2 
h 6 6 2 
2
r 2 (log r+L)
d L r k
bits.
O d L2r k ρdL + r (logρr+L) , where r = log dkL
.
The
total
space
bound
is
O
ρdL
+
2
ρ

ρ
As a same argument in the proof
of
Theorem
3.6,
the
cost
of
de-randomization
introduce
an
additional
dL
factor.
Thus,
h 7 7 2 
i
2
the final space bound is O d L2r k ρdL + r (logρr+L)
bits. The query time and update time is similar to that of



Theorem 3.6 thus poly d, L, 1 , ρ1 , k and poly d, L, 1 , log k .

D

Synthetic Dataset

Figure 2. 65536 points are drawn from a Gaussian Mixture distribution. The contours illustrate the PDF function.

Clustering High Dimensional Dynamic Data Streams

Algorithm 6 PositiveCoreSet(S, k, ρ, ): construct a -coreset for dynamic stream S.
Initization:
Initialize a grid structure;

 2 2

√
2
3
dk
∆
+ λ42di kL
O ← {1, 2, 4, . . . , d∆d+1 }; L ← dlog ∆e; πi (o) ← min λ32dl ∆L
log 2L∆
log
2 ρo
2o
ρ
i
h 3 2
i
h 3 4 
q
ρ
L
, β ← O d L
, 0 ←  λ7 kdρ3 L3 ; m ← 0;
α ← O d L2 k d + ρ1 log kL
ρd + log kL
2
ρ
ρ + kL log ρ



30kL2
ρ2 , 1

;

For each o ∈ O and i ∈ [0, L], construct fully independent hash function ho,i : [∆]d → {0, 1} with
P rho,i (ho,i [q] = 1) = πi (o); initialize SparseCells(∆d , (1 + 2i )d , α, β, O(ρ/(dL))) instances SCo,i ;
Initialize HEAVY-HITTER(∆d , 10Lk/ρ, 0 , ρ/L) instances, HH0 , HH1 , . . . , HHL−1 , one for a level;
Update (S):
for each update (op, q) ∈ S:
/*op ∈ {Insert, Delete}*/
m ← m ± 1; /*Insert: +1, Delete:−1*/
for each i ∈ [0, L]:
ciq ← the center of the cell contains q at level i;
HHi .update(op, ciq );
for each o ∈ [O]:
if ho,i (q) == 1:
SCo,i .update(op, ciq );
Query:
Let o∗ be the smallest o such that no instance of SCo,0 , SCo,1 , . . . , SCo,L returns Fail;
S ← {};
[
C−1 ← the cell of the entire space [∆]d ; |C
−1 | ← m;
for i ∈ [0, L − 1]:
Ci ← HHi .query().top((e + 4)(L + 1)k/ρ);
Remove cells C from Ci if C P (C) 6∈ Ci−1 , where C P (C) is the parent cell of C in level i − 1;
Bi ← SCo∗ ,i .query();
Si ← {p ∈ Bi : C(p, i − 1) ∈ Ci−1 AND C(p, i) 6∈ Ci };
Each point in Si receives weight 1/πi (o∗ );
PS ← S ∪ Si ;
k 0 ← i∈[0,L] |Ci |;
Let n
{C1 , C2 , . . . Ck0 } = o
∪i∈[0,L] Ci ∪ {C−1 } be the set of heavy cells;
d
d
d
Let |C1 |, |C2 |, . . . |Ck0 | be the estimated frequency of each cell;
d
d
d
R ← RectifyWeights(|C
1 |, |C2 |, . . . |Ck0 |, S);
return R.

