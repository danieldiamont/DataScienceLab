Active Learning for Cost-Sensitive Classification

A. Experimental Details
A.1. Finding Cost Ranges with Online Approximation
Consider the maximum and minimum costs for a fixed label y at round i, both of which may be suppressed. First, define
b
g w , arg min R(g)
+ w(g(x)

0)2

g2G

b
g w , arg min R(g)
+ w(g(x)

1)2

g2G

b
and recall the definition of gi,y , argming2G R(g)
given in Algorithm 1. Owing to the monotonicity property of
R̂(g, w, c; y) (Lemma 1), an alternative to M IN C OST and M AX C OST is to find
b
w := max{w | R(g
)
w
b w)
w := max{w | R(g

b i,y ) 
R(g
b i,y ) 
R(g

(9)

i}

(10)

i}

and return g w (x) and g w (x) as the minimum and maximum costs. We use two steps of approximation here. Using the
definition of g w and g w we have:

b w)
R(g

b
R(g
)
w

b i,y )  w · gi,y (x)2
R(g

b i,y )  w · (gi,y (x)
R(g

1)2

w · g w (x)2

w · (g w (x)

1)2 .

b w ) R(g
b i,y ) in Eqs. (9) and (10). Second, we replace gi,y , g , and g w with
We use this upper bound in place of R(g
w
o
approximations obtained by online updates. More specifically, we replace gi,y with gi,y
, the current regressor produced by
all online updates so far, and approximate the others by
o
g w (x) ⇡ gi,y
(x)

o
w · s(x, 0, gi,y
)

o
o
g w (x) ⇡ gi,y
(x) + w · s(x, 1, gi,y
),
o
where s(x, y, gi,y
)
0 is a sensitivity value that approximates the change in prediction on x resulting from an online
o
update to gi,y
with features x and label y. The computation of this sensitivity value is governed by the actual online update
where we compute the derivative of the change in the prediction as a function of the importance weight w for a hypothetical
example with cost 0 or cost 1 and the same features. This is possible for essentially all online update rules on importance
weighted examples and it corresponds to taking the limit as w ! 0 of the change in prediction due to an update divided
by w. By inspection this requires only O(d) time per example, where d is the average number of non-zero features. With
these two steps, we obtain approximate minimum and maximum costs using
o
gi,y
(x)

o
wo · s(x, 0, gi,y
)

o
o
gi,y
(x) + wo · s(x, 1, gi,y
),

where

w

o

o
wo , max{w | w gi,y,
(x)2

, max{w | w

o
(gi,y,
(x)

1)

2

o
(gi,y
(x)

o
(gi,y
(x)

+w·

o
w · s(x, 0, gi,y
))2 

o
s(x, 1, gi,y
)

1)

2



i}

i }.

o
The online update guarantees that gi,y
(x) 2 [0, 1]. Since the minimum cost is lower bounded by 0, we have wo 2
⇣
i
o
gi,y (x)
o
o
o
0, s(x,0,g
. Finally, because the objective w(gi,y
(x))2 w(gi,y
(x) w · s(x, 0, gi,y
))2 is increasing in w within this
o
i,y )
range (which can be seen by inspecting the derivative), we can find wo with binary search. Using the same techniques, we
also obtain an approximate maximum cost.

It is worth noting that the approximate cost ranges (without the sensitivity trick) are contained in the exact cost ranges
because we approximate the difference in squared error by an upper bound. Hence, the query rule in this online algorithm
should be more aggressive than the query rule in Algorithm 1.

Active Learning for Cost-Sensitive Classification

passive
active (10
active (10
active (10

)
)
3
)
1
2

ImageNet 20
1
0.05
0.05
1

Table 2. Best learning rates

ImageNet 40
1
0.1
0.5
10

RCV1-v2
0.5
0.5
0.5
0.5

POS
1.0
1.0
1.0
10

NER
0.5
0.1
0.5
0.5

NER-wiki
0.5
0.5
0.5
0.5

Figure 3. Additional figures for simulated active learning experiments. The plots show the test cost as a function of the number of
examples where even a single query was issued.

A.2. Choosing the Learning Rate
For all experiments, we show the results obtained by the best learning rate for each mellowness on each dataset. We choose
the best learning rate as follows. For each dataset let perf(m, l, q, t) denote the test performance of the algorithm using
mellowness m and learning rate l on the tth permutation of the training data under a query budget of 2(q 1) · 10 · K, q 1.
Let query(m, l, q, t) denote the number of queries actually made. Note that query(m, l, q, t) < 2(q 1) · 10 · K if the
algorithm runs out of the training data before reaching the q th query budget8 . To evaluate the trade-off between test
performance and number of queries, we define the following performance measure:
AUC(m, l, t) =

◆
qmax
⌘ ✓
1 X⇣
query(m, l, q + 1, t)
perf(m, l, q + 1, t) + perf(m, l, q, t) · log2
,
2 q=1
query(m, l, q, t)

(11)

where qmax is the minimum q such that 2(q 1) · 10 is larger than the size of the training data. This performance measure
is the area under the curve of test performance against numbers of queries in log2 scale. A large value means the test
performance quickly improves with the number of queries. The best learning rate for mellowness m is then chosen as
l? (m) , arg max median1t100
l

AUC(m, l, t).

The best learning rates for different datasets and mellowness settings are in Table 2.
A.3. Additional Figures for Simulated Active Learning
In Figure 3, we plot the test error as a function of the number of examples for which at least one query was requested, for
each dataset and mellowness parameter. This experimentally corresponds to the L1 term in our label complexity analysis.
In comparison with Figure 2 involving the total number of queries, the improvements offered by active learning are slightly
less dramatic here. This suggests that our algorithm queries just a few labels for each example, but does end up issuing
at least one query on most of the examples. Nevertheless, one can still achieve test cost competitive with passive learning
using a factor of 2-16 less labeling effort, as measured by L1 .
In Figure 4, we compare COAL with the two active learning baselines, A LL O R N ONE and N O D OM described in Section 6,
along with passive learning, on the RCV1-v2 dataset. As in the ImageNet 40 results, here COAL substantially outperforms both baselines and passive learning. However, here A LL O R N ONE offers marginal improvement over passive, while
8
In fact, we check the test performance only in between examples, so query(m, l, q, t) may be larger than 2(q
additive factor of K, which is negligibly small.

1)

· 10 · K by an

Active Learning for Cost-Sensitive Classification

Figure 4. Test cost versus number of queries for COAL, in comparison with active and passive baselines on the RCV1-v2 dataset.
Passive learning and N O D OM are nearly identical.

N O D OM improves over passive on ImageNet 40. Thus, depending on the task, the baselines can improve performance, but
COAL is reliably better.

B. Running time analysis
Throughout this section, fix an x, y pair, an iteration i, as well a radius and an accuracy ✏. We focus on approximating
c+ (x, G( ; y)) (See Eqs. (2) and (7)), approximating the minimum cost is very similar. To simplify notation, we drop
bi (g; y) (Eq. (5)), except we drop the dependence on both y and i
dependence on x and y. We recall our earlier notation R
which are fixed in this section. We also recall some other important notation which is accordingly simplified for brevity:
b
R(g)
= Ê[(g(x)
gmin

e w, c) = R(g)
b
R(g,
+ w(g(x) c)2
b
b
G( ) = {g 2 G : R(g)
min R(g)


c(y))2 1 (y queried on x)],
b
= argming2G R(g),

c+ (↵ ) = maxg2G(↵

)

g

g(x),

}

c? = c+ ( ).

b
b
R(g)
is the empirical square loss used to define the set of good regressors G( ) in the algorithm. The precise form of R(g)
e
does not matter in this section. R(g, w, c) is the empirical square loss with one additional example, with features x, target
c, and weight w. gmin is the empirical square loss minimizer, which is the center of the ball G( ). This functional is used
to define new square loss problems in our algorithm. Our goal is to find a number ĉ such that,
p
c?  ĉ  c+ (4 ) + 3✏.
b ? ) R(g
b min )  . In other words g? realizes the maximum
Finally, let g? be any function such that g? (x) = c? and R(g
cost on example x. Note that g? is not the same regressor that satisfies the realizability condition.
We start the running time analysis with several lemmas characterizing the behavior of various components of the algorithm.
An important structure to the square loss problem is a monotonicity property of both the risk functional and the predictions.
e w, c) and g 0 = argming R(g,
e w0 , c). Then
Lemma 1. For any c and for w0 w 0, let g = argming R(g,
Proof. By the definitions,

b 0)
R(g

b 0 ) + w0 (g 0 (x)
R(g
Rearranging shows that
(w0

b
R(g)
and (g 0 (x)

c)2  (g(x)

c)2 .

e 0 , w0 , c)  R(g)
b
c)2 = R(g
+ w0 (g(x) c)2
b
= R(g)
+ w(g(x) c)2 + (w0 w)(g(x)
b 0 ) + w(g 0 (x)
 R(g

w)(g 0 (x)

c)2  (w0

c)2 + (w0

w)(g(x)

w)(g(x)

c)2 .

c)2
c)2 .

Active Learning for Cost-Sensitive Classification

Since w0

w, we have (g 0 (x)

c)2  (g(x)

Rearranging this inequality gives,

c)2 , which is the second claim. For the first claim, the definition of g gives

b
R(g)
+ w(g(x)

b 0)
R(g

b
R(g)

b 0 ) + w(g 0 (x)
c)2  R(g

w((g(x)

c)2

(g 0 (x)

c)2

c)2 )

0.

The next critical lemma shows that the termination condition in Line 6 of M AX C OST meets the accuracy guarantee.
e w, c) then g(x) c? ✏. Further, if g 2 G( ), then g(x)  c? .
Lemma 2. If c c? , w
/✏2 and g = argming R(g,
Proof. The second claim is straightforward by the definition of c? .

For the first claim, we work to establish a contradiction. Suppose that g(x) < c?
e w, c), gmin is the minimizer of R(g),
b
of R(g,
and c c? , we have
w(c

(c?

✏))2 < w(c

e w, c)
g(x))2  R(g,

We may further lower bound (again using c
(c
Rearranging proves that w <
the desired claim.

2

c? ),

c? + ✏) = (c

✏. By the facts that g is the minimizer

b min )  R(g
e ? , w, c)
R(g

c? )2 + 2(c

c? )✏ + ✏2

/✏2 . The contrapositive is that if w

(c

b min ) 
R(g

+ w(c

c? ) 2 .

c? ) 2 + ✏2 .

/✏2 , then we must have g(x)

c?

✏, which is

The next lemma is the main result for the B INARY S EARCH subroutine.
Lemma 3. Suppose we invoke the subroutine B INARY S EARCH with parameters ✏ and . Then it terminates in polynomial
time with O(log2 (1/(✏2 ))) oracle calls. The algorithm outputs two regressors (g` , gh ) and if c c? is passed as input then
c? 2 [g` (x), gh (x)]. If additionally, gh 2
/ G(4 ) then c?  (g` (x) + gh (x))/2.
Proof. The logarithmic running time is fairly straightforward since in each iteration the algorithm halves the interval, has
initial interval of size /✏2 and terminates when the interval is smaller than 2 . Thus for T
log2 (1/(2✏2 )) the interval
has size at most
2

T

( /✏2 )  2

log2 (1/(2✏2 ))

( /✏2 ) = 2

Hence the number of iterations is upper bounded by dlog2 (1/(2✏2 ))e.
For the first termination claim, the invariant that we maintain is that for all t
e wt,` , c) satisfies gt,` (x)  c? .
gt,h (x) c? while gt,` = argming R(g,

e wt,h , c) satisfies
1, gt,h = argming R(g,

For gt,h , we first establish the base case. Observe that g1,h = gc (computed in M AX C OST just before the invocation of
b c)
b min ) + by the termination check in Line 6. By construction, in this iteration and
B INARY S EARCH) and R(g
R(g
in all others, we have that gt,h 2
/ G( ), since this is the requirement for updating wt,h . But since gt,h minimizes the risk
e wt,h , c) we get,
function R(g,
Since c

b min ) +
R(g

+ wt,h (gt,h (x)

c? , this implies that gt,h (x)

e t,h , wt,h , c)  R(g
e ? , wt,h , c)  R(g
b min ) +
c)2  R(g

+ wt,h (c?

c)2 .

c? .

The proof for gt,` is simpler, since we only shrink the interval up if we find something in G( ). By definition of c? this
check guarantees that g` (x)  c? .
For the second termination claim we must use the fact that |wt,h wt,` |  2 by the termination condition and gh 2
/
e wt,` , c) and analogously for gh . Assume for the sake of
G(4 ). Let t be the terminal iteration, so g` = argming R(g,
contradiction that c? (gh (x) + g` (x))/2. Since c c? , this implies that
b min ) + 4
R(g

+ wt,h (gh (x)

e h , wt,h , c)  R(g
e ? , wt,h , c)  R(g
b min ) + + wt,h (c
c)2  R(g
✓
◆2
gh (x) + g` (x)
b
 R(gmin ) + + wt,h c
.
2

c? )2

Active Learning for Cost-Sensitive Classification

Similarly we have
b min ) + wt,` (g` (x)
R(g

e ` , wt,` , c)  R(g
e ? , wt,` , c)  R(g
b min ) +
c)2  R(g

✓
+ wt,` c

gh (x) + g` (x)
2

◆2

.

Adding the two equations gives
2
)2
)2

◆2
gh (x) + g` (x)
2
✓
◆2
⇥
⇤
gh (x) + g` (x)
2
2
+ wt,h (c g` (x)) + (c gh (x))  2wt,h c
+ (wt,h wt,` )(c g` (x))2
2
✓
◆2
⇥
⇤
gh (x) + g` (x)
+ wt,h (c g` (x))2 + (c gh (x))2  2wt,h c
+2
since c, g` (x) 2 [0, 1]
2
✓
◆2
1
1
gh (x) + g` (x)
2
2
) (c g` (x)) + (c gh (x))  c
.
2
2
2

+ wt,` (c

g` (x))2 + wt,h (c

✓
gh (x))2  (wt,h + wt,` ) c

The last line is a contradiction since E[f (Z)]
Unif({g` (x), gh (x)}) and f (y) = (c y)2 .

f (E[Z]) for convex f , which can be applied by taking Z =

The last lemma ensures sufficient progress in the case when gh 2
/ G(4 ), which is crucial for the oracle complexity bound.
p
Lemma 4. Suppose c c? and that there exists g 2 G( ) such that c g(x) = with 2 [ 3✏, 1]. Then if the output
(g` , gh ) of B INARY S EARCH satisfies gh 2
/ G(4 ), then gh (x)  c +
✏2 .
Proof. We never use a weight larger than /✏2 by the initialization of w1,` , w1,h . Now suppose that we output gh such
b h ) R(g
b min ) > 4 , which by construction is the minimizer of R(·,
e w, c) for some w  /✏2 . Then
that R(g
b min ) + 4
R(g

+ w(gh (x)

e h , w, c)  R(g,
e w, c)  R(g
b min ) +
c)2  R(g

Rearranging, using the fact that

+ w(g(x)

w✏2 , and dividing through by w > 0 gives
(gh (x)

c)2 

2

c|  |

+ w 2.

3✏2 .

The condition on ensures that the right hand side is non-negative. It is easy to see that
expanding the square. Hence we get that
|gh (x)

b min ) +
c)2 = R(g

✏2 / | 

2

3✏2  (

✏2 / )2 simply by

✏2 .

p
We can safely remove the absolute value on the right hand side since we have the condition that
3✏, which ensures
that
✏2 / is non-negative. The absolute value on the left hand side can also be removed, since if gh (x)  c we have
already proved what
we must have ✏ 2 (0, 1) for the preconditions of the lemma to be
p is required. Specifically, since
satisfied and
3✏, it follows that c  c +
✏2 . Since gh is the result of an oracle call with weight w  /✏2 , either
b h ) R(g
b min )  4 , or it must have gh (x)  c +
it has R(g
✏2 .
We are now ready to prove Theorem 1.

Proof of Theorem 1. The first step of the proof is to inductively verify that c c? , h c? , `  c? at all steps in the algorithm execution. These invariants are clearly maintained at the onset of the algorithm. Now suppose they are maintained at
b c )  R(g
b min ) + , then by Lemma 2 we are done. Otherwise, we obtain two
the onset of some iteration. If gc satisfies R(g
regressors (g` , gh ) from B INARY S EARCH. For the lower bound, we always have g` (x)  c? by Lemma 3, which verifies
b h ) R(g
b min )  4 then by Lemma 3, we know that c?  gh (x),
the inductive step for `. For the upper bound to c? , if R(g
b h ) > 4 , but here we
but we also know that gh (x)  c+ (4 ) by the definition, so we are done. The last case is when R(g
may apply the second statement of Lemma 3, which asserts that c?  (gh (x) + gl (x))/2. The settings of `, h, c now verify
the inductive claim, since ` gl implies that (h + l)/2 (gh (x) + gl (x))/2 c? .

Active Learning for Cost-Sensitive Classification

This immediately proves the correctness of the algorithm, since the loop stopping condition, along with the invariant,
guarantees that c c? ` which means that
ĉ

c?  ĉ

`=

h

`
2



p

3✏.

For the iteration complexity, we must apply Lemma 4. In particular, we use the width of the interval [`, h] which contains
c? as a potential function and show that it decreases with every step. Let t denote h `, which is the width of the interval
before the tth iteration (so 1 = 1). Every non-terminal iteration satisfies c c? . Moreover, for any t > 1, we use as the
regressor g, the one that achieved the value ` used to define c. This ensures that g(x) = `. Furthermore, in application of
Lemma 4, we set , c g(x)
gives 2 = t = h `. Recall that we entered the loop at tth
p = c `, which conveniently
p
iteration, meaning that t 2 3✏ and hence 2 [ 3✏, 1]. Lemma 4 states that either we terminate successfully, or we are
guaranteed that gh (x)  c +
✏2 . This means that
t+1

✏2

max{`, g` (x)}  c +

= gh (x)

`= +

✏2 =

t

✏2 ,

where the first equality used c ` = which is true by definition. Since we terminate at the first T such that
we require at most O(1/✏2 ) iterations. By Lemma 3, each iteration takes O(log(1/✏)) oracle calls.

T

p
 2 3✏,

C. Generalization analysis
To bound the generalization error of Algorithm 1, we start by defining the central random variable in the analysis. At round
i, recall our notation Qi (y) = 1 (query y on example xi ) which indicates the query rule. The central random variable is,
Mi (g; y)

, (g(xi )

ci (y))2

(f ? (xi ; y)

ci (y))2 Qi (y).

(12)

Here (xi , ci ) is the ith example and cost presented to the algorithm. For simplicity, we write Mi when the dependence on
g and y is clear from context. For a vector regressor f , we write
Mi (f ; y) , Mi (f (·; y); y).
We also recall some of the constants and notation defined in Algorithm 1 which are heavily used throughout this appendix.
✓ 2
◆
⇣n⌘
✏i 1
2n |G|K
, ✏i =
log
,  = 80.
i =
i 1
i
bi (g; y) =
R

1

i

1

i 1
X
⇥

(g(xj )

j=1

bi (g; y),
gi,y = argmin R
g2G

⇤
cj (y))2 Qj (y) .

and fi = {gi,y }K
y=1 .

bi (g; y)  R
bi (gi,y ; y) + i },
Gi (y) = {g 2 G | 8y, R
bi (f (·; y); y)  R
bi (gi,y ; y) +
Fi = {f 2 G K | 8y, R

i }.

For 1 we use the convention that 1/0 = 1 so the initial radius is infinite. Let Ei [·] and Vari [·] denote the expectation
and variance conditioned on all randomness up to and including round i 1. With these definitions, we turn to several
supporting claims.
C.1. Supporting Lemmata
Theorem 7 (Freedman-type inequality (Beygelzimer et al., 2011; Agarwal et al., 2014)). Let X1 , . . . , XT be a sequence
of real-valued random variables. Assume for all t 2 {1, . . . , T } that |Xt |  R and E[Xt |X1 , . . . , Xt 1 ] = 0. Define
PT
PT
S = t=1 Xt and V = t=1 E[Xt2 |X1 , . . . , Xt 1 ]. For any 2 (0, 1) and 2 [0, 1/R], with probability at least 1
,
S  (e

2) V +

ln(1/ )

.

Active Learning for Cost-Sensitive Classification

Lemma 5 (Concentration of squared loss). For any
g 2 G, y 2 Y, i 2 [n], t 2 [n]:
i+t
X1

i+t
X1

Ej [Mj ]

j=i

where ⌫n , log
Note that ✏i =

⇣

n
i

2n2 |G|K

⌘

2 (0, 1), with probability at least 1

Mj

j=i

, the following holds for all

v
ui+t 1
uX
 2t
Var[Mj ]⌫n + 2⌫n ,
j=i

j

.

⌫n , is a scaled version of the confidence bound here, where the scaling shrinks polynomially with i.

Proof. First observe that by the rescaling of the failure parameter, we can apply Freedman’s inequality for each i, t, y, g
and for each tail and a union bound proves the result.
We now apply the Freedman-type inequality in Theorem 7. For a fixed g 2 G, y 2 Y , the random variable Mi is measurable
with respect to the -field ({(xj , {(c(xj ; y), Qj (y))}y2Y )}ij=1 ), so Mi Ei [Mi ] forms a martingale difference sequence
adapted to this filtration. Moreover Mi Ei [Mi ] and Ei [Mi ] Mi are both conditionally centered and clearly at most 2.
Thus Freedman’s inequality gives,
i+t
X1

v
ui+t 1
uX
Ej [Mj ]  2t
Var[Mj ]⌫n + 2⌫n ,

Mj

j=i

except with probability
the constraint

2n2 |G|K .

j=i

j

This follows by observing that (e

 1/R. Otherwise we set

2)  1 and setting =
p
= 1/R and use the fact that 1/R  ⌫n /V .

p
⌫n /V , provided it meets

The bound on the right hand side also holds for the lower tail, again except with same probability. Thus a union bound
over both tails, all g 2 G, y 2 Y and pairs i, t gives the result.
Lemma 6 (Bounding variance of regression regret). We have for all (g, y) 2 G ⇥ Y ,
⇥
Ei [Mi ] = Ei Qi (y)(g(xi )

Var[Mi ]  4Ei [Mi ].
i

⇤
f ? (xi ; y))2 ,

Proof. We take expectation of Mi over the cost conditioned on a fixed example xi = x and a fixed query outcome Qi (y):
E[Mi | xi = x, Qi (y)] = Qi (y) ⇥ Ec [g(x)2

f ? (x; y)2

= Qi (y) g(x)2

f ? (x; y)2

= Qi (y)(g(x)

f ? (x; y))2 .

2c(y)(g(x)

2f ? (x; y)(g(x)

f ? (x; y)) | xi = x]

f ? (x; y))

The second equality is by Assumption 1, which implies E[c(y) | xi = x] = f ? (x; y). Taking expectation over xi and
Qi (y), we have

For the variance:

⇥
Ei [Mi ] = Ei Qi (y)(g(xi )

⇤
f ? (xi ; y))2 .

Var[Mi ]  Ei [Mi2 ]
i
⇥
= Ei Qi (y)(g(xi ) f ? (xi ; y))2 (g(xi ) + f ? (xi ; y)
⇥
⇤
 4 · Ei Qi (y)(g(xi ) f ? (xi ; y))2
= 4Ei [Mi ].

2c(y))2

⇤

Active Learning for Cost-Sensitive Classification

Lemma 7 (Sharp cost-sensitive bound). For all i > 0, if f ? 2 Fi , then for all f 2 Fi
(
)
4⌘i2
6X
Ex,c [c(hf (x)) c(hf ? (x))]  min ⇣P⇣ + 1 (⇣  2⌘i ) 2⌘i +
+
Ei [Mi (f ; y)] ,
⇣>0
⇣
⇣ y
where P⇣ = Prx⇠D [miny6=hf ? (x) f ? (x, y)  f ? (x, hf ? (x)) + ⇣] is the probability that the expected cost of the second best
and best label are within ⇣ of each other.
Proof. Fix some f 2 Fi , and let y(x) = hf (x) and y ? (x) = hf ? (x) for shorthand. Define S⇣ (x) =
1 (f ? (x, y(x))  f ? (x, y ? (x)) + ⇣) and S⇣0 (x) = 1 miny6=y? (x) f ? (x, y)  f ? (x, y ? (x)) + ⇣ . Observe that for fixed
⇣, S⇣ (x)1 (y(x) 6= y ? (x))  S⇣0 (x) for all x. We can also majorize the complementary indicator to obtain the inequality
S⇣C (x) 

(f ? (x, y(x))

f ? (x, y ? (x)))
⇣

.

We begin with the definition of realizability, which gives
Ex,c [c(hf (x))

c(hf ? (x)] = Ex [(f ? (x, y(x)) f ? (x, y ? (x))) 1 (y(x) 6= y ? (x))]
⇥
⇤
= Ex S⇣ (x) + S⇣C (x) (f ? (x, y(x)) f ? (x, y ? (x))) 1 (y(x) 6= y ? (x))
⇥
⇤
 ⇣Ex S⇣0 (x) + Ei S⇣C (x)1 (y(x) 6= y ? (x)) (f ? (x, y(x)) f ? (x, y ? (x))) .

The first term here is exactly the ⇣P⇣ term in the bound. We now focus on the second term, which depends on our query
rule. For this we must consider three cases.
Case 1. If both y(x) and y ? (x) are not queried, then it must be the case that both have small cost ranges. This follows since
f 2 Fi and hf (x) = y(x) so y ? (x) does not dominate y(x). Moreover, since the cost ranges are small on both y(x) and
y ? (x), since we know that f ? is well separated under event S⇣C (x), the relationship between ⇣ and ⌘i governs whether we
make a mistake or not. Specifically, we get that S⇣C (x)1 (y(x) 6= y ? (x)) 1 (no query)  1 (⇣  2⌘i ) at round i. In other
words, if we do not query and the separation is big but we make a mistake, then it must mean that the cost range threshold
⌘i is also big.
Using this argument, we can bound the second term as,
⇥
⇤
Ei S⇣C (x)1 (y(x) 6= y ? (x)) 1 (y(x), y ? (x) not queried) (f ? (x, y(x)) f ? (x, y ? (x)))
⇥
 Ei S⇣C (x)1 (y(x) 6= y ? (x)) 1 (y(x), y ? (x) not queried) (f ? (x, y(x)) f (x, y(x)) + f (x, y ? (x))
⇥
⇤
 Ei S⇣C (x)1 (y(x) 6= y ? (x)) 1 (y(x), y ? (x) not queried) 2⌘i

f ? (x, y ? (x)))

⇤

 Ei [1 (⇣  2⌘i ) 2⌘i ] = 1 (⇣  2⌘i ) 2⌘i .

Case 2. If both y(x) and y ? (x) are queried, we can easily relate the second term to the square loss,
⇥
⇤
Ei S⇣C (x)1 (y(x), y ? (x) both queried) (f ? (x, y(x)) f ? (x, y ? (x)))
i
1 h
2
 Ei 1 (y(x), y ? (x) both queried) (f ? (x, y(x)) f ? (x, y ? (x)))
⇣
i
1 h
2
 Ei 1 (y(x), y ? (x) both queried) (f ? (x, y(x)) f (x, y(x)) + f (x, y ? (x)) f ? (x, y ? (x)))
⇣
⇤
2 ⇥
 Ei 1 (y(x) queried) (f ? (x, y(x)) f (x, y(x)))2 + 1 (y ? (x) queried) (f (x, y ? (x)) f ? (x, y ? (x)))2
⇣
⇤ 2X
2X ⇥

Ei Qi (y)(f ? (x, y) f (x, y))2 =
Ei [Mi (f ; y)] .
⇣ y
⇣ y
Passing from the second to third line here is justified by the fact that f ? (x, y(x))
f ? (x, y ? (x)) and f (x, y(x)) 
?
f (x, y (x)) so we added two non-negative quantities together. The last step uses Lemma 6. While not written, we also use
the event 1 (y(x) 6= y ? (x)) to avoid losing a factor of 2.

Active Learning for Cost-Sensitive Classification

Case 3. The last case is if one label is queried and the other is not. Both cases here are analogous, so we do the derivation
for when y(x) is queried but y ? (x) is not. Since in this case, y ? (x) is not dominated (hf (x) is never dominated provided
f 2 Fi ), we know that the cost range for y ? (x) must be small. Using this fact, and essentially the same argument as in
case 2, we get
⇥
⇤
Ei S⇣C (x)1 (y(x) queried, y ? (x) not) (f ? (x, y(x)) f ? (x, y ? (x)))
i
1 h
2
Ei 1 (y(x) queried, y ? (x) not) (f ? (x, y(x)) f ? (x, y ? (x)))
⇣
2 h
2
 Ei 1 (y(x) queried, y ? (x) not) (f ? (x, y(x)) f (x, y(x))) + (f (x, y ? (x))
⇣
i
2⌘ 2
2 h
2
 i + Ei 1 (y(x) queried) (f ? (x, y(x)) f (x, y(x)))
⇣
⇣
2⌘i2
2X

+
Ei [Mi (f ; y)] .
⇣
⇣ y

f ? (x, y ? (x)))

2

i

We also obtain this term for the other case where y ? (x) is queried by y(x) is not.
To summarize, adding up the contributions from these cases (which is an over-estimate since at most one case can occur
and all are non-negative), we get

Ex,c [c(hf (x))

c(hf ? (x)]  ⇣P⇣ + 1 (⇣  2⌘i ) 2⌘i +

4⌘i2
6X
+
Ei [Mi (f ; y)] .
⇣
⇣ y

This bound holds for any ⇣, so it holds for the minimum.
C.2. Proof of Theorem 3
Conditioning on the high-probability event in Lemma 5, we prove the theorem by induction. Define

0
i

⌫n
= min{1,
},
i 1

⌫n = log

✓

2n2 |G|K

◆

.

We will make use of the following simple fact, which applies since i  n, so the premultiplier on ✏i is at least 1.
Fact 1. For all i 2 [n], we have ⌫n  ✏i .
Concretely we consider the inductive hypothesis:

8y, 8i

1,

bi (f ? (·; y); y)  min R
bi (g; y) + c0 ⌫n
R
g2G
i 1

and E[c(hfi (x))

⇢

c(hf ? (x))]  min ⇣P⇣ +
⇣>0

2K
⇣

0
i

(13)

where c0 = 10. The first claim in particular implies that f ? (·; y) 2 Fi since we chose i+1 = ✏i /i and using Fact 1. For
the base case i = 1, observe that the right hand side of the first inequality is infinity but the empirical squared loss is 0 for
all regressors. Hence the first claim is trivially satisfied. Moreover, because the excess cost-sensitive classification risk is
2K 0
always upper-bounded by 1, it is trivially bounded by ⇣ 1 for any ⇣ 2 [0, 1]. For ⇣ > 1, we have ⇣P⇣ = ⇣ so again the
bound is trivial.
Now assume the inductive hypothesis holds for the first i rounds, i
1. We want to analyze the set Fi+1 , which is
computed at the end of the ith iteration of Algorithm 1 based on i examples (technically the beginning of the (i + 1)st

Active Learning for Cost-Sensitive Classification

iteration). Invoking Lemma 5, with parameters 1 and i, and Lemma 6, we have for all (g, y) 2 G ⇥ Y ,
i
X

Ej [Mj (g; y)]

j=1

i
X
j=1

v
u
i
X
u
Mj (g; y)  2t4⌫n
Ej [Mj (g; y)] + 2⌫n
j=1

0

1
i
X
1
 2 @4⌫n +
Ej [Mj (g; y)]A + 2⌫n
4 j=1
i

= 10⌫n +

1X
Ej [Mj ].
2 j=1

This bound implies that
i
X
j=1

(since Ej [Mj (g; y)]

Mj  10⌫n ,

and therefore

0 by Lemma 6)

bi+1 (f ? (·; y); y)  R
bi+1 (g; y) + c0 ⌫n .
R
i

(14)

Since this bound applies for all g 2 G, it proves the first part of the inductive claim.

Next we prove that the empirical squared loss minimizer fi+1 after iteration i has small excess risk. Fix some label y. To
simplify notation, we drop the dependence on y and define for any j:
gj , fj (·; y),

g ? , f ? (·; y),

Gj , Gj (y),

bj (g) , R
bj (g; y).
R

Let Mj be defined for gi+1 and y according to Eq. (12). We first prove that since gi+1 is the empirical loss minimizer at
round i for label y, it must have been in the version space Gj (y) for all j 2 {1, . . . , i + 1}.
Because gi+1 is the loss minimizer for label y after round i, we have
i
X

Mj =

j=1

i
X
j=1

Mj (gi+1 ; y)  0.

Now suppose gi+1 2
/ Gt+1 for some t 2 {0, . . . , i}. We have
t
X

Mj

=

j=1

=

⇣
bt+1 (gi+1 )
t R

bt+1 (g ? )
R

⇣
bt+1 (gi+1 )
t R
✏t

⌘

bt+1 (gt+1 ) + R
bt+1 (gt+1 )
R

c0 ⌫ n .

bt+1 (g ? )
R

⌘

(15)

bt+1 (gi+1 ) R
bt+1 (gt+1 ) ✏t /t by the elimination
The last inequality here follows since gi+1 2
/ Gt+1 so it must have R
rule.
Simultaneously,
we
use
Eq.
(14)
which
lower
bounds
the
second
term.
Combining
this inequality with the fact that
Pi
M

0
gives
j
j=1
i
X

j=t+1

Applying Lemmas 5 and 6 along with the inequality
i
X

j=t+1

Ej [Mj ]

i
X

j=t+1

Mj



Mj  c 0 ⌫ n

p

✏t .

(16)

4ab  a/↵ + ↵b for all ↵ > 0, gives

v
u
i
i
X
u
1 X
2t4⌫n
Ej [Mj ] + 2⌫n 
Ej [Mj ] + 10⌫n .
2 j=t+1
j=t+1

(17)

Active Learning for Cost-Sensitive Classification

Combining the last inequality and Eq. (16), we get
i
X

j=t+1

Ej [Mj ]  20⌫n + 2c0 ⌫n

2✏t  (40

2)✏t < 0.

The strict inequality here is based on Fact 1 and the parameter setting  = 80. This is a contradiction since Ej [Mj ] is a
quadratic form and hence non-negative by Lemma 6. The same analysis applies to every y. Therefore, we know that the
empirical square loss vector regressor fi+1 is in Fj for all j 2 {1, . . . , i + 1}, and hence we can apply Lemma 7 for all of
these rounds, to obtain
i Ex,c [c(hfi+1 (x)) c(hf ? (x))]
8
!9
i
<X
=
4⌘j2
6X
 min
⇣P⇣ + 1 (⇣  2⌘j ) 2⌘j +
+
Ej [Mj (fi+1 ; y)]
;
⇣>0 :
⇣
⇣ y
j=1
8
!9
i
<
=
X
4⌘j2
6X
 min i⇣P⇣ +
1 (⇣  2⌘j ) 2⌘j +
+
Ej [Mj (fi+1 ; y)]
.
;
⇣>0 :
⇣
⇣ y
j=1

We study the four terms separately. Thepfirst one is straightforward and contributes ⇣P⇣ to the instantaneous cost sensitive
regret. Using our definition of ⌘j = 1/ j the second term can be bounded as
i
X

1 (⇣ < 2⌘j ) 2⌘j =

j=1

The inequality above,

Pn

i=1

1
p
i

d4/⇣ 2 e

X
j=1

p
2
12
p  4 d4/⇣ 2 e 
.
⇣
j

p
 2 n, is well known. For the third term, using our definition of ⌘j gives
i
X
4⌘j2
j=1

⇣

i

=

4X1
4
 (1 + log(i)).
⇣ j=1 j
⇣

Finally, the fourth term can be bounded using Lemma 5 (Eq. (17) with t = 0), which reveals
i
X
j=1

Since for each y,
we get

Pi

j=1

Ej [Mj ]  2

i
X

Mj + 20⌫n

j=1

Mj (fi ; y)  0 for the empirical square loss minimizer (which is what we are considering now),
i

6 XX
120
Ej [Mj (fi+1 ; y)] 
K⌫n .
⇣ y j=1
⇣
And hence, we obtain
Ex,c [c(x; hfi+1 (x))

⇢

1
c(x; h (x))]  min ⇣P⇣ + (4 log(i) + 16 + 120K⌫n )
⇣>0
⇣i
⇢
⇢
140K⌫n
2K⌫n
 min ⇣P⇣ +
 min ⇣P⇣ +
⇣>0
⇣>0
⇣i
⇣i
f?

To obtain this last bound, we observe that 1  log(i)  ⌫n under our assumption that < 1/e so the coefficient in the
numerator is at most 140. The inductive claim follows by the definition of 0i+1 . Or more precisely, if 0i+1 = 1 then the
inductive claim is trivial and otherwise we have proved what is required.

Active Learning for Cost-Sensitive Classification

D. Label complexity analysis
D.1. Supporting Lemmata
Our label complexity analysis builds on the following lemma, which uses the sets Gi? and Gi :
bi (g; y)
Gi ( ; y) , {g | R

n
Gi? ( ; y) , g

1

i

1

bi (g 0 ; y) 
min R

i 1
X

Qj (y)(g(xj )

j=1

(18)

},

g 0 2G

f ? (xj ; y))2 

o

(19)

.

Throughout we use the definitions.
i

, ✏i

1 /(i

p

1),  , 80, c0 , 10, c1 , 25/3, c2 , 1/3, ⌘i , 1/ i, ⌫n , log

✓

2n2 |G|K

◆

These are the constants defined in Algorithm 1 with some additional numerical constants that we use in the analysis. We
also require a new definition:
I (i) = max{t 2 N|(t
Note that I (i) is well defined for i
We first study the I functional.

1)  (c2 /c1 )1/ (i

(20)

1)}.

1 since the right hand side is non-negative. However I (i) could be as small as 1.

Fact 2. Define i , 2(c1 /c2 )1/ + 1. Then for i
I (i)

i , we have
max{(c2 /c1 )1/ (i

1

1)/2, 2}.

Proof. The proof is by direct calculation.
I (i)

1 = b(c2 /c1 )1/ (i

I (i)

1

(c2 /c1 )1/ (i

b(c2 /c1 )1/ (i

1)c
1)

1)c = 2

1 = (c2 /c1 )1/ (i

1)

(c2 /c1 )1/ (i
2

1)

(c2 /c1 )1/ (i
2

1)

.

We now turn to the more intricate lemmas.
Lemma 8. For any

2 (0, 1), with probability at least 1
Gi? (c2

i ; y)

⇢ Gi (

i ; y)

⇢ Gi (4

, for all i
i ; y)

⇢ Gi? (c1

1 and all y,
i ; y)

⇢ GI?

(i) (c2

I (i) ; y),

where I (i) is in Eq. (20).
Proof. The second containment is trivial.
Recall our earlier definition that for a fixed g 2 G and y 2 Y ,
Mj , (g(xj )

cj (y))2

(f ? (xj ; y)

cj (y))2 Qj (y).

Let Ec [Mj ] and Varc [Mj ] denote the expectation and variance taken with respect to the cost c at round j, conditioned on
all randomness up to round j 1 and on xj . Following the same proof for Lemma 6, we have that
Ec [Mj ] = Qj (y)(g(xj )

f ? (xj ; y))2 ,

and

Var[Mj ]  4Ec [Mj (g; y)].
c

It is also easy to prove a concentration result similar to Lemma 5 where Ej [Mj ] and Varj [Mj ] are replaced by Ec [Mj ]
and Varc [Mj ], respectively. Thus we have for any 2 (0, 1), with probability at least 1
, the following holds for all
(g, y) 2 G ⇥ Y and all i, t 2 [n]:
v
u
i+t
i+t
i+t
X1
X1
X1
u
Ec [Mj ]
Mj  2t4⌫n
Ec [Mj ] + 2⌫n ,
(21)
j=i

j=i

j=i

Active Learning for Cost-Sensitive Classification

where ⌫n = log

⇣

2n2 |G|K

⌘

as in Lemma 5. This bound, via the inequality
i+t
X1
j=i

Ec [Mj ]  2

i+t
X1
j=i

Mj 

i+t
X1

p

4ab  ↵a + b/↵ implies
(22)

Mj + 20⌫n

j=i

i+t 1
3 X
Ec [Mj ] + 10⌫n
2 j=i

(23)

We start with proving the first containment. Fix some round i, some label y, and some g 2 Gi? (c2
the above high-probability event and starting with Eq. (23), we have
0
1
i 1
i 1
X
3 @X
3
Mj  ·
Ec [Mj ]A + 10⌫n  · (i 1) · c2 i + 10⌫n
2
2
j=1
j=1
⇣
⌘
3

= c2 ✏i 1 + 10⌫n 
+ c0 ✏i 1 .
2
2
Above, the second inequality is by
i 1
X

Ec [Mj ] =

j=1

i 1
X

f ? (xj ; y))2  c2

Qj (y)(g(xj )

j=1

since g 2 Gi? (c2 i ; y), and the final inequality uses ⌫n  ✏i
bi (g; y), we have
bound and with gi = argming2G R
(i

⇣
bi (g; y)
1) · R

1

i

⇥ (i

i ; y).

Conditioning on

1)

(Fact 1) and our choices of , c0 and c2 . Using the above

i 1
⌘ X
bi (gi ; y) =
R
Mj + (i
j=1

⇣
bi (f ? ; y)
1) R

bi (gi ; y)
R

⌘

 (/2 + c0 )✏i 1 + c0 ⌫n  ✏i 1 ,
Pi 1
where the first inequality is by the above upper bound on j=1 Mj and Eq. (14), which upper bounds the excess empirical
square loss of f ? . Thus, g 2 Gi ( i ; y) ⇢ Gi (4 i ; y).
To prove the third containment, we fix some i, y, and g 2 Gi (4
i 1
X
j=1

Ec [Mj ]  2

i 1
X

i ; y).

Starting from (22) we have

Mj + 20⌫n

j=1

= 2(i
 2(i

 8✏i

 c1 ✏i

bi (g; y)
1) · (R
bi (g; y)
1) · (R

1

+ 20⌫n

1,

bi (f ? ; y)) + 20⌫n
R
bi (gi ; y)) + 20⌫n
R

where the second inequality is by the fact that gi is the square loss minimizer at round i for label y, the third inequality is
by g 2 Gi (4 i ; y), and the last inequality is by ⌫n  ✏i 1 (Fact 1) and our choices of c1 and . Thus, g 2 Gi? (c1 i ; y).
For the final containment, observe that
(i

1)c1

i

= c1 ✏i

1

= c1 

✓

n
i

Using the definition of I (i) in Eq. (20), we get that (i
I (i) 1  i 1 since c2  c1 . Hence,
I (i) 1

X
j=1

Ej Qj (y)(g(xj )

f ? (xj ; y))2 

i 1
X
j=1

1

◆

⌫n

1)c1

Ej Qj (y)(g(xj )

!
i

0"

= c2  @

✓

 (I (i)

c1
c2

◆1/

1)c2

f ? (xj ; y))2  (i

n
i

1

I (i) .

1)c1

#

1

⌫n A .

Of course we always have

i

 (I (i)

1)c2

I (i) .

Active Learning for Cost-Sensitive Classification

Thus we get that Gi? (c1

i)

⇢ GI?

(i) (c2

I (i) ).

Before bounding the label complexity, we first prove the following regret bound:
Lemma
Q ? 9. For any
y Gi (c2 i ; y),

 1/e, with probability at least 1
Ex,c [c(x, hf (x))

, for all i

1 and for all vector regressors f 2 Fi? (c2
⇢

c(x, hf ? (x))]  min ⇣P⇣ +
⇣>0

14K
⇣

i

i)

,

.

Note that this cost-sensitive regret bound is polynomially worse than the one in Theorem 3 that we prove just for the
empirical risk minimizer fi . This is because we set the confidence radius i using a polynomial function of n/i, which
will be important for our label complexity analysis.
Proof. The proof follows a similar argument to that of Lemma 7 in that we must argue that each g 2 Gi? (c2 i ; y) is
involved in driving the query rule for a large fraction of the rounds. First observe that f ? 2 Fi? (c2 i ) for i
1 by the
definition of Fi? .
?
Next, fix a label y and a function g 2 Gi+1
(c2 i+1 ; y) for i 0. We prove that g 2 Gt+1 ( t ) for all t 2 {0, . . . , i}. In
?
search of a contradiction, suppose that g 2
/ Gt+1 ( t+1 ) for some t 2 {0, . . . , i}. First, since g 2 Gi+1
(c2 i+1 ; y), using
the Freedman-style deviation bound in Eq. (23), we have
i
X
j=1

Here we also use the definition of

i+1

At the same time, since g 2
/ Gt+1 (

t+1 ; y),

t+1

✓

◆
3
c2  + c0 ✏i .
2

R̂t+1 (gt+1 )  R̂t+1 (g)

R̂t+1 (g ? ) +

i

3X
Ec [Mj ] + 10⌫n 
2 j=1

Mj 

= ✏i /i, c0 = 10, and Fact 1.
we know that

< R̂t+1 (g)

c0 ⌫n
.
t

The last inequality uses Eq. (14). Together with the above, this implies that
i
X

j=t+1

Now, since i

t and
i
X

j=t+1

Mj 

✓

◆
3
c2  + c0 ✏i
2

✏t + c0 ⌫n .

2 (0, 1), we get that ✏i < ✏t . Using Eq. (22) as before, we get

Ec [Mj ]  2

i
X

j=t+1

Mj + 20⌫n  2

✓

◆
3
c2  + c0 ✏i
2

2✏t + 4c0 ⌫n  (  + 6c0 )✏t < 0.

The last non-strict inequality follows from the fact that ✏t
✏i
⌫n since i t, and then the strict inequality is by our
choices for the constants. This is a contradiction since the left hand side is a quadratic form and so, g 2 Gt+1 ( t+1 ) for
all t 2 {0, . . . , i}.
This argument applies for all y, and hence, we may apply Lemma 7, so that for all regressors f 2 Fi? (c2 i+1 ),
8
!9
i
<
=
X
4⌘j2
6X
i · (Ex,c [c(x, hf (x)) c(x; hf ? (x))])  min i⇣P⇣ +
1 (⇣  2⌘j ) 2⌘j +
+
Ej [Mj (f ; y)]
;
⇣>0 :
⇣
⇣ y
j=1
8
9
i
<
=
16 + 4 log(i) 6 X X
 min i⇣P⇣ +
+
Ej [Mj (f ; y)] .
;
⇣>0 :
⇣
⇣ y
j=1

The last inequality here uses identical bounds as the proof of Theorem 3.

Active Learning for Cost-Sensitive Classification

In a similar way to (17), we use Lemma 5 to obtain
i
X
j=1

Ej [Mj (f ; y)]  2

i
X
j=1

⇣
bi+1 (f ; y)
Mj (f ; y) + 20⌫n = 2i · R

⇣
bi+1 (f ; y)
 2i · R
 (2 + 20)✏i .

⌘
bi+1 (fi+1 ; y) + 20⌫n
R

⌘
bi+1 (f ? ; y) + 20⌫n
R

?
The last bound uses the definition of i+1 and Fact 1, along with the fact that Gi+1
(c2 i+1 ; y) ⇢ Gi+1 ( i+1 ; y) so we
know the empirical risk to fi+1 is controlled. Finally, we collect the latter three terms and the constant 6(2 + 20) + 20
(which requires < 1/e). This gives,
⇢
14K✏i
Ex,c [c(x, hf (x)) c(x, hf ? (x))]  min ⇣P⇣ +
.
⇣>0
i⇣
?
This proves the statement since we are considering f 2 Fi+1
(c2

i+1 )

and ✏i /i =

i+1 .

For the rest of the analysis, it will be convenient to introduce the shorthand b(xi , y) = b
c+ (xi , y) b
c (xi , y), where
b
c+ (xi , y) and b
c (xi , y) are the approximate maximum and minimum costs computed in Algorithm 1 at round i. Moreover,
let Yi be the set of non-dominated labels at round i of the algorithm, which in the pseudocode we call Y 0 . Formally,
Yi = {y | b
c (xi , y)  miny0 b
c+ (xi , y 0 )}.
Lemma 10 (Cost Range Translation). Fix i and suppose that the conclusions of Lemmas 8 and 9 hold. Then for any x, y
pair, we have
n

where ri = min⇣>0 ⇣P⇣ +
Proof. We have

14K
⇣

i

o

b(xi , y)  (xi , y, Fcsr (rI

(i) ))

+ ⌘i /2,

and I (i) is in Eq. (20).

⌘i
2
⌘i
 (xi , y, Fcsr (rI (i) )) +
2

b(xi , y)  (xi , y, Gi? (c1

i ; y))

(By Theorem 1, setting of ✏ in Algorithm 1 and Lemma 8)

+

(By Lemmas 8 and 9)

Lemma 11. Fix i and suppose that the conclusions of Lemmas 8 and 9 hold. Define yi? = argminy f ? (xi ; y), ȳi =
argminy cc
c (xi , Gi (y)). Then for y 6= yi? , we have
+ (xi , Gi (y)), ỹi = argminy6=yi? c
y 2 Yi ) f ? (xi ; y)

f ? (xi ; yi? )

⌘i
 ( (xi , y) + (xi , yi? )) ,
2

and for yi? :
|Yi | > 1 ^ yi? 2 Yi ) f ? (xi ; ỹi )

f ? (xi ; yi? )

In both bounds, all the cost ranges are computed using Fcsr (rI

⌘i
 ( (xi , ỹi ) + (xi , yi? )) .
2

(i) ).

Proof. Suppose y 6= yi?
y 2 Yi ) cc (xi , Gi (y))  cc
+ (xi , Gi (ȳi ))

?
) cc (xi , Gi (y))  cc
+ (xi , Gi (yi ))

) c (xi , Gi? (c1
) f ? (xi ; y)

i ; y))

 c+ (xi , Gi? (c1

(xi , Gi? (c1

) f ? (xi ; y)

f ? (xi ; yi? )

) f ? (xi ; y)

f ? (xi ; yi? )

i ; y))

?
i ; yi ))

+

⌘i
2

 f ? (xi ; yi? ) + (xi , Gi? (c1

?
i ; yi ))

+

⌘i
2

⌘i
 (xi , Gi? (c1 i ; y)) + (xi , Gi? (c1 i ; yi? ))
2
⌘i
 (xi , y, Fcsr (rI (i) )) + (xi , yi? , Fcsr (rI (i) )) .
2

Active Learning for Cost-Sensitive Classification

For yi? we need to consider two cases. First assume yi? = ȳi . Then
?
|Yi | > 1 ^ yi? 2 Yi ^ yi? = ȳi ) cc (xi , Gi (ỹi ))  cc
+ (xi , Gi (yi ))
⌘i
) f ? (xi , ỹi ) f ? (xi , yi? )
 (xi , ỹi ) + (xi , yi? ).
2

This is true since if |Yi | > 1 then it must be the case that ỹi is confused, since it has the minimal lower cost estimate. On
the other hand if yi? 6= ȳi then
?
|Yi | > 1 ^ yi? 2 Yi ^ yi? 6= ȳi ) cc
c
+ (xi , Gi (ȳi ))  c
+ (xi , Gi (yi ))

?
) cc (xi , Gi (ỹi ))  cc
+ (xi , Gi (yi ))
⌘i
) f ? (xi , ỹi ) f ? (xi , yi? )
 (xi , ỹi ) + (xi , yi? ).
2

The second step here is because the search for ỹi includes ȳi , since the latter is not yi? . Thus we obtain
?
|Yi | > 1 ^ yi? 2 Yi ) cc (xi , Gi (ỹi ))  cc
+ (xi , Gi (yi ))
⌘i
) f ? (xi ; ỹi ) f ? (xi ; yi? )
 ( (xi , ỹi ) + (xi , yi? )) ,
2

as desired.
D.2. Low Noise (Massart) Case (Theorem 6)
Fix some round i. Let Fi be the set of vector regressors used at round i of COAL and let Gi (y) be the corresponding
?
?
regressors for label y. Let ȳi , argminy cc
c (xi , Gi (y)).
+ (xi , Gi (y)), yi = argminy f (xi ; y), and ỹi , argminy6=yi? c
Assume Lemmas 8 and 9 hold. The label complexity L2 for round i is
X
X
X
Qi (y) =
1 (|Yi | > 1 ^ y 2 Yi ) 1 (b(xi , y, Fi ) > ⌘i ) =
1 (|Yi | > 1 ^ y 2 Yi ) Qi (y).
y

y

y

We need to do two things with Qi (y), so we have duplicated it here. First, observe that y 2 Yi implies that there exists
a vector regressor f 2 Fi such that hf (xi ) = y. This follows since the domination condition means that there exists
g 2 Gi (y) such that g(xi )  miny0 6=y maxg0 2Gi (y0 ) g 0 (xi ). Since we are using a factored representation, we can take f to
use g on the y th coordinate and use the maximizers for all the other coordinates. Moreover, |Yi | > 1 implies there exists a
regressor that does not predict y. Of course, through Lemmas 8 and 9, we know that Fi ⇢ Fcsr (rI (i) ), and so we get the
bound:
1 (|Yi | > 1 ^ y 2 Yi )  1 9f, f 0 2 Fcsr (rI

(i) )

| hf (xi ) = y ^ hf 0 (xi ) 6= y .

For y 6= yi? , we take f 0 to be f ? which is always in the cost-sensitive regret ball. For yi? , we take f 0 to be any regressor such
that hf 0 (xi ) = ỹi , which must exist in the ball if |Yi | > 1. We will use these as an upper bound on Qi (y) momentarily.
Secondly, we apply Lemma 11 along with the Massart noise assumption. For y 6= yi?
⇣
⌘
⌘i
 (xi , y) + (xi , yi? )
1 (|Yi | > 1 ^ y 2 Yi )  1 f ? (xi ; y) f ? (xi ; yi? )
2 ⌘
⇣
⌘i
1 ⌧
 (xi , y) + (xi , yi? ) .
2

Recall that we use the convention that all quantities without an explicit regressor ball use Fcsr (rI (i) ). For yi? we obtain the
same inequality but using ỹi via Lemma 11. Together this gives the bound:
X
L2 
1 (⌧ ⌘i /2  (xi , y) + (xi , yi? )) ⇥ Qi (y) + 1 (⌧ ⌘i /2  (xi , ỹi ) + (xi , yi? )) ⇥ Qi (yi? ).
y6=yi?

Let us focus on just one of these terms (say where y 6= yi? ) and consider any round i where ⌧
1 (⌧

⌘i /2  (xi , y) + (xi , yi? )) Qi (y)  1 (⌧ /2  (xi , y) + (xi , yi? )) Qi (y)

2⌘i .

 1 (⌧ /4  (xi , y)) Qi (y) + 1 (⌧ /4  (xi , yi? )) Qi (y).

Active Learning for Cost-Sensitive Classification

Using the upper bound on Qi (y), the first term here is clearly bounded by
1 (⌧ /4  (xi , y)) 1 9f, f 0 2 Fcsr (rI

(i) )

| hf (xi ) = y ^ hf 0 (xi ) 6= y , Di (y).

Fortunately, the second term is bounded in the same way, since we know that hf ? 2 Fcsr (rI
hf (xi ) = y 6= yi? exists implies that the second term is at most Di (yi? ).

(i) ),

the fact that some f with

The last term, which involves Qi (yi? ) is bounded in essentially the same way, since we know that when |Yi | > 1 (which is
all we are considering), there exists two functions f, f 0 2 Fi such that hf (xi ) = ỹi and hf 0 (xi ) = yi? . Thus we can bound
the label complexity at round i by
Di (ỹi ) + Di (yi? ) +

X

y6=yi?

(Di (y) + Di (yi? ))  KDi (yi? ) + 2

X

Di (y).

y

For the rounds i where ⌧ < 2⌘i we simply upper bound the label complexity by K.
The last step in the proof is to apply Freedman’s inequality to the sequence of indicators. The conditional mean of each
term is at most (for rounds i where ⌧ > 2⌘i ),
Ei

"

KDi (yi? )

+2

X
y

#

Di (y) 

4rI (i)
[K✓1 + 2✓2 ] .
⌧

The part involving ✓2 is straightforward and the premultiplier follows since we are measuring the probability of querying
with a cost range parameter of ⌧ /4 and over a cost-sensitive regret ball of radius rI (i) in Di (y). To obtain ✓1 we use the
fact that if Di (yi? ) = 1, then certainly there exists some confused label, namely yi? , and hence the indicator in ✓1 is also 1.
The range is 3K since Di (y) 2 {0, 1} and since the terms are non-negative, the variance is at most the range times the
mean. In such cases, Freedman’s inequality gives
X  EX + 2

p

REX log(1/ ) + 2R log(1/ )  2EX + 3R log(1/ ),

with probability at leastp1
where X is the non-negative random variable with range R and expectation EX. The last
step is by the fact that 2 ab  a + b.
In our case, we get that with probability at least 1
n
X

KDi (yi? ) + 2

i=i?

X
y

,

Di (y) 

n
X
8rI (i)
[K✓1 + 2✓2 ] + 9K log(1/ ).
⌧
i=i?

Here we only consider rounds i i? where i? is the smallest index such that ⌧ < 2⌘i? and i? i (Recall Fact 2). For the
first i? rounds, we will upper bound the per-round label complexity by K, so that the overall label complexity is at most
Ki? +
K

n
X
8rI (i)
[K✓1 + 2✓2 ] + 9K log(1/ )
⌧
i=i?

n
X
i=1

1 (⌧  2⌘i ) + Ki + +

n
X
8rI (i)
[K✓1 + 2✓2 ] + 9K log(1/ )
⌧
i=i

p
Using our choice of ⌘i = 1/ i, the first term is at most Kd4/⌧ 2 e. The second term is bounded by Fact 2. The last step is
to use the definition of rI (i) to simplify the sum. Since we are in the Massart noise case, we will set ⇣ = ⌧ in the definition
of ri in Lemma 10. Since P⌧ = 0 by the definition of the noise condition, this yields ri = 14K i /⌧ . Substituting this

Active Learning for Cost-Sensitive Classification

choice, along with our definition of
n
X

rI

(i)

i=i

i

yields

n
14n K⌫n X
(I (i) 1) 1
⌧
i=i
0
✓ ◆ 1+ X
n
14n K⌫n @ (1+ )
c1

⇥ 2
⇥
(i 1)
⌧
c2
i=i
"✓ ◆ 1 n
#
X
56(c1 /c2 )n K⌫n
c1
1

(i 1)
⌧
c2
i=2
✓ ◆1
56(c1 /c2 )n K⌫n c1

(2 ⇥ log(n)) .
⌧
c2

=

1

1
A

Including the extra O(K) term, the overall bound is
✓

◆
✓ ◆1
4
8 ⇥ 56 ⇥ 25 ⇥ 2n K⌫n c1
1/
K d 2 e + 2(c1 /c2 )
+1 +
log(n)[K✓1 + 2✓2 ] + 9K log(1/ )
⌧
⌧2
c2
✓
◆
n K log(n)⌫n
K log(1/ )
 a0 251/
[K✓
+
2✓
]
+
,
1
2
⌧2
⌧2
where a0 is a universal constant.
For L1 we can use a very similar argument. First,
L1 =

X
i

1 (|Yi | > 1 ^ 9y 2 Yi , b(xi , y, Fi ) > ⌘i ) 

X
i

1 |Yi | > 1 ^ 9y 2 Yi , (xi , y, Fcsr (rI

(i) ))

> ⌘i /2 .

This inequality is an application of Lemma 10. Now as above, we know that,
|Yi | > 1 ^ y 2 Yi ) 9f, f 0 2 Fcsr (rI

(i) ), hf (xi )

= y ^ hf 0 (xi ) 6= y,

since if y 2 Yi then some classifier must select it, and since |Yi | > 1, something else must also be selected. We also know
that we can always take f 0 to be f ? when y 6= yi? . For yi? we can always take the classifier to be the one that predicts ỹi .
Moreover we also have that when ⌧

2⌘i ,

|Yi | > 1 ^ y 2 Yi ) f ? (xi ; y)

f ? (xi ; yi? )

⌘i /2  (xi , y) + (xi , yi? )

) ⌧ /4  (xi , y) _ ⌧ /4  (xi , yi? )

Thus, putting things together, and considering only rounds where ⌧
L1 

n
X

1 (⌧ < 2⌘i ) +

i=1

n
X
i=1

1 9y | 9f, f 0 2 Fcsr (rI

2⌘i we get

(i) ), hf (xi )

= y ^ hf 0 (xi ) 6= y ^ (x, y)

⌧ /4 .

Here we dropped the (x; yi? )
⌧ /4 term from consideration since the term gets included in the existential quantifier
when the chosen label y = yi? . Now we may apply Freedman’s inequality to upper bound L1 by
✓
◆
n
X
4rI (i)
n K log(n)⌫n
log(1/ )
1/
L1  i + d4/⌧ e + 2
✓1 + 2 log(1/ )  a0 25
✓1 +
,
⌧
⌧2
⌧2
i=i
2

where a0 is a universal constant.

Active Learning for Cost-Sensitive Classification

D.3. High noise case (Theorem 5)
Fix some round i. Let Fi be the set of vector regressors used at round i of COAL and let Gi (y) be the corresponding
?
?
regressors for label y. Let ȳi , argminy cc
c (xi , Gi (y)).
+ (xi , Gi (y)), yi = argminy f (xi ; y), and ỹi , argminy6=yi? c
Assume Lemmas 8 and 9 hold. The label complexity L2 for round i is
X
X
Qi (y) =
1 (|Yi | > 1 ^ y 2 Yi ) 1 (b(xi , y, Fi ) > ⌘i ) .
y

y

First we apply Lemma 10 on the latter indicator to get
X
1 (|Yi | > 1 ^ y 2 Yi ) 1
y

(xi , y, Fcsr (rI

(i) ))

⌘i /2 .

For the former indicator, observe that y 2 Yi implies that there exists a vector regressor f 2 Fi such that hf (xi ) = y. This
follows since the domination condition means that there exists g 2 Gi (y) such that g(xi )  miny0 maxg0 2Gi (y0 ) g 0 (xi ).
Since we are using a factored representation, we can take f to use g on the y th coordinate and use the maximizers for all
the other coordinates.
Since y 2 Yi implies there exists f 2 Fi such that hf (xi ) = y, and by Lemmas 8 and 9, we get that f 2 Fcsr (rI
Similarly there exists f 0 2 Fi such that hf 0 (xi ) 6= y. Thus we can bound the the label complexity for round i as,
X
1 9f, f 0 2 Fcsr (rI (i) ) | hf (xi ) = y 6= hf 0 (xi ) 1 (xi , y, Fcsr (rI (i) )) ⌘i /2
y

=

X
y

1 x 2 DIS(rI

(i) , y)

^ (xi , y, Fcsr (rI

(i) ))

⌘i /2 .

Now we can apply Freedman’s inequality on the sequence here to find that with probability at least 1
L2  Ki +

(i) ).

,

n
X
4rI (i)
✓2 + 3K log(1/ )
⌘i
i=i

Again i = 2(c1 /c2 )1/ + 1 is from Fact 2. We just need to upper bound the sequence:
s
n
n
X
X
p
rI (i)
14Kn ⌫n
=2
i
⌘
(I
(i) 1)1+
i
i=i
i=i
s
n
p
X
21+ i
 2 14Kn ⌫n ⇥
1+
(c2 /c1 )
(i 1)1+
i=i
s
n
p
X
22+
 2 14Kn ⌫n ⇥
1+
(c2 /c1 )
(i 1)
i=i
q
n
1
X
1+
 448(c1 /c2 )
Kn ⌫n ⇥
i /2
i=1

q
1+
 2 448(c1 /c2 )
Kn ⌫n ⇥ n1
q
1+
 2 448(c1 /c2 )
K⌫n ⇥ n.

/2

The first line follows by the definition of ⌘i and by optimizing the bound in Lemma 9 using the definition of
second line uses Fact 2. The remaining steps are simple calculations using 2 (0, 1) and an integral bound.
Thus in total we get a label complexity of

L2  a0 (25)1/

⇣

n✓2

p

⌘
K⌫n + K log(1/ ) .

i.

The

Active Learning for Cost-Sensitive Classification

Similarly for L1 we can derive the bound
X
L1 
1 9y | (xi , y, Fcsr (rI
i

(i) ))

⌘i /2 ^ x 2 DIS(rI

(i) , y)

.

and then apply Freedman’s inequality to this sequence to obtain that with probability at least 1
L1  i + 2

n
⌘
⇣
X
p
2rI
✓1 + 3 log(1/ )  a0 (25)1/ n✓1 K⌫n + log(1/ ) .
⌘i
i=i

