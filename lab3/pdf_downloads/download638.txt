Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

In the following appendices, we provide detailed proofs of theorems stated in the main paper. In Section A we first prove
a basic inequality which is useful throughout the rest of the convergence analysis. Section B contains general analysis of
the batch primal-dual algorithm that are common for proving both Theorem 1 and Theorem 3. Sections C, D, E and F give
proofs for Theorem 1, Theorem 3, Theorem 2 and Theorem 4, respectively.

A. A basic lemma
Lemma 2. Let h be a strictly convex function and Dh be its Bregman divergence. Suppose ψ is ν-strongly convex with
respect to Dh and 1/δ-smooth (with respect to the Euclidean norm), and
ŷ = arg min
y∈C



	
ψ(y) + ηDh (y, ȳ) ,

where C is a compact convex set that lies within the relative interior of the domains of h and ψ (i.e., both h and ψ are
differentiable over C). Then for any y ∈ C and ρ ∈ [0, 1], we have

ρδ
2
ψ(y) + ηDh (y, ȳ) ≥ ψ(ŷ) + ηDh (ŷ, ȳ) + η + (1 − ρ)ν Dh (y, ŷ) +
k∇ψ(y) − ∇ψ(ŷ)k .
2
Proof. The minimizer ŷ satisfies the following first-order optimality condition:
h∇ψ(ŷ) + η∇Dh (ŷ, ȳ), y − ŷi ≥ 0,

∀ y ∈ C.

Here ∇D denotes partial gradient of the Bregman divergence with respect to its first argument, i.e., ∇D(ŷ, ȳ) = ∇h(ŷ) −
∇h(ȳ). So the above optimality condition is the same as




∇ψ(ŷ) + η(∇h(ŷ) − ∇h(ȳ)), y − ŷ ≥ 0,

∀ y ∈ C.

(17)

Since ψ is ν-strongly convex with respect to Dh and 1/δ-smooth, we have

ψ(y) ≥ ψ(ŷ) + h∇ψ(ŷ), y − ŷi + νDh (y, ŷ),
2
δ
ψ(y) ≥ ψ(ŷ) + h∇ψ(ŷ), y − ŷi + ∇ψ(y) − ∇ψ(ŷ) .
2

For the second inequality, see, e.g., Theorem 2.1.5 in Nesterov (2004). Multiplying the two inequalities above by (1 − ρ)
and ρ respectively and adding them together, we have
ψ(y) ≥ ψ(ŷ) + h∇ψ(ŷ), y − ŷi + (1 − ρ)νDh (y, ŷ) +
The Bregman divergence Dh satisfies the following equality:


ρδ 
∇ψ(y) − ∇ψ(ŷ)2 .
2




Dh (y, ȳ) = Dh (y, ŷ) + Dh (ŷ, ȳ) + ∇h(ŷ) − ∇h(ȳ), y − ŷ .

We multiply this equality by η and add it to the last inequality to obtain
ψ(y) + ηDh (y, ȳ)

≥

2

ρδ 
ψ(ŷ) + ηDh (y, ŷ) + η + (1 − ρ)ν Dh (ŷ, ȳ) + ∇ψ(y) − ∇ψ(ŷ)
2



+ ∇ψ(ŷ) + η(∇h(ŷ) − ∇h(ȳ)), y − ŷ .

Using the optimality condition in (17), the last term of inner product is nonnegative and thus can be dropped, which gives
the desired inequality.

B. Common Analysis of Batch Primal-Dual Algorithms
We consider the general primal-dual update rule as:

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Iteration: (x̂, ŷ) = PDτ,σ (x̄, ȳ, x̃, ỹ)
x̂ =
ŷ

=



1
arg min g(x) + ỹ T Ax +
kx − x̄k2 ,
2τ
x∈Rd


1
∗
T
arg min f (y) − y Ax̃ + D(y, ȳ) .
σ
y∈Rn

(18)
(19)

Each iteration of Algorithm 1 is equivalent to the following specification of PDτ,σ :
x̂ = x(t+1) ,
ŷ = y (t+1) ,

x̄ = x(t) ,
ȳ = y (t) ,

x̃ = x(t) + θ(x(t) − x(t−1) ),
ỹ = y (t+1) .

(20)

Besides Assumption 2, we also assume that f ∗ is ν-strongly convex with respect to a kernel function h, i.e.,
f ∗ (y ′ ) − f ∗ (y) − h∇f ∗ (y), y ′ − yi ≥ νDh (y ′ , y),
where Dh is the Bregman divergence defined as
Dh (y ′ , y) = h(y ′ ) − h(y) − h∇h(y), y ′ − yi.
We assume that h is γ ′ -strongly convex and 1/δ ′ -smooth. Depending on the kernel function h, this assumption on f ∗ may
impose additional restrictions on f . In this paper, we are mostly interested in two special cases: h(y) = (1/2)kyk2 and
h(y) = f ∗ (y) (for the latter we always have ν = 1). From now on, we will omit the subscript h and use D denote the
Bregman divergence.
Under the above assumptions, any solution (x⋆ , y ⋆ ) to the saddle-point problem (6) satisfies the optimality condition:
−AT y ⋆
Ax⋆

∈
=

∂g(x⋆ ),
∇f ∗ (y ⋆ ).

(21)
(22)

The optimality conditions for the updates described in equations (18) and (19) are
1
(x̄ − x̂)
τ

1
Ax̃ −
∇h(ŷ) − ∇h(ȳ)
σ
−AT ỹ +

∈

∂g(x̂),

(23)

=

∇f ∗ (ŷ).

(24)

Applying Lemma 2 to the dual minimization step in (19) with ψ(y) = f ∗ (y) − y T Ax̃, η = 1/σ, y = y ⋆ and ρ = 1/2, we
obtain
f ∗ (y ⋆ ) − y ⋆ T Ax̃ +

1
D(y ⋆ , ȳ)
σ

≥

1
f ∗ (ŷ) − ŷ T Ax̃ + D(ŷ, ȳ)
σ
1
2
δ
ν
+
D(y ⋆ , ŷ) + ∇f ∗ (y ⋆ ) − ∇f ∗ (ŷ) .
+
σ
2
4

(25)

Similarly, for the primal minimization step in (18), we have (setting ρ = 0)
g(x⋆ ) + ỹ T Ax⋆ +


1
11
1 ⋆
kx − x̄k2 ≥ g(x̂) + ỹ T Ax̂ +
kx̂ − x̄k2 +
+ λ kx⋆ − x̂k2 .
2τ
2τ
2 τ

Combining the two inequalities above with the definition L(x, y) = g(x) + y T Ax − f ∗ (y), we get
L(x̂, y ⋆ ) − L(x⋆ , ŷ)

=
≤

g(x̂) + y ⋆ T Ax̂ − f ∗ (y ⋆ ) − g(x⋆ ) − ŷ T Ax⋆ + f ∗ (ŷ)

1
1 ⋆
1
11
ν
D(y ⋆ , ŷ)
kx − x̄k2 + D(y ⋆ , ȳ) −
+ λ kx⋆ − x̂k2 −
+
2τ
σ
2 τ
σ
2
2
1
δ
1
− kx̂ − x̄k2 − D(ŷ, ȳ) − ∇f ∗ (y ⋆ ) − ∇f ∗ (ŷ)
2τ
σ
4
+ y ⋆ T Ax̂ − ŷ T Ax⋆ + ỹ T Ax⋆ − ỹ T Ax̂ − y ⋆ T Ax̃ + ŷ T Ax̃.

(26)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

We can simplify the inner product terms as
y ⋆ T Ax̂ − ŷ T Ax⋆ + ỹ T Ax⋆ − ỹ T Ax̂ − y ⋆ T Ax̃ + ŷ T Ax̃ = (ŷ − ỹ)T A(x̂ − x⋆ ) − (ŷ − y ⋆ )T A(x̂ − x̃).
Rearranging terms on the two sides of the inequality, we have
1
1 ⋆
kx − x̄k2 + D(y ⋆ , ȳ)
2τ
σ

≥

L(x̂, y ⋆ ) − L(x⋆ , ŷ)

1
11
ν
+
D(y ⋆ , ŷ)
+ λ kx⋆ − x̂k2 +
+
2 τ
σ
2
2
1
δ
1
+ kx̂ − x̄k2 + D(ŷ, ȳ) + ∇f ∗ (y ⋆ ) − ∇f ∗ (ŷ)
2τ
σ
4
+ (ŷ − y ⋆ )T A(x̂ − x̃) − (ŷ − ỹ)T A(x̂ − x⋆ ).

Applying the substitutions in (20) yields
1
1 ⋆
kx − x(t) k2 + D(y ⋆ , y (t) )
2τ
σ

≥

L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )

1
11
ν
+
D(y ⋆ , y (t+1) )
+ λ kx⋆ − x(t+1) k2 +
+
2 τ
σ
2
2
1
δ
1
+ kx(t+1) − x(t) k2 + D(y (t+1) , y (t) ) + ∇f ∗ (y ⋆ ) − ∇f ∗ (y (t+1) )
2τ
σ
4

+ (y (t+1) − y ⋆ )T A x(t+1) − (x(t) + θ(x(t) − x(t−1) ) .
(27)

We can rearrange the inner product term in (27) as

=

(y (t+1) − y ⋆ )T A x(t+1) − (x(t) + θ(x(t) − x(t−1) )



(y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) − θ(y (t) − y ⋆ )T A(x(t) − x(t−1) ) − θ(y (t+1) − y (t) )T A(x(t) − x(t−1) ).

Using the optimality conditions in (22) and (24), we can also bound k∇f ∗ (y ⋆ ) − ∇f ∗ (y (t+1) )k2 :

=
≥

 ∗ ⋆

∇f (y ) − ∇f ∗ (y (t+1) )2


 1
 ⋆
2
∇h(y (t+1) ) − ∇h(y (t) ) 
Ax − A x(t) + θ(x(t) − x(t−1) ) +
σ



 2

1 
2

A(x⋆ − x(t) ) − (α − 1)θA(x(t) − x(t−1) ) − 1 ∇h(y (t+1) ) − ∇h(y (t) ) 
1−
 ,
α
σ

where α > 1. With the definition µ =
with the inequality (27) leads to

p

λmin (AT A), we also have kA(x⋆ − x(t) )k2 ≥ µ2 kx⋆ − x(t) k2 . Combining them

1 ⋆
1
kx − x(t) k2 + D(y ⋆ , y (t) ) + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
2τ
σ
≥ L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )

1
11
ν
+
+ λ kx⋆ − x(t+1) k2 +
+
D(y ⋆ , y (t+1) ) + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) )
2 τ
σ
2
1
1
+ kx(t+1) − x(t) k2 + D(y (t+1) , y (t) ) − θ(y (t+1) − y (t) )T A(x(t) − x(t−1) )
2τ
σ

 1

δ
1  δµ2 ⋆

2
kx − x(t) k2 − (α − 1) θA(x(t) − x(t−1) ) −
+ 1−
∇h(y (t+1) ) − ∇h(y (t) )  .
α 4
4
σ

(28)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

C. Proof of Theorem 1
Let the kernel function be h(y) = (1/2)kyk2 . In this case, we have D(y ′ , y) = (1/2)ky ′ − yk2 and ∇h(y) = y. Moreover,
γ ′ = δ ′ = 1 and ν = γ. Therefore, the inequality (28) becomes


1 1 
1 ⋆
1  δµ2
kx⋆ − x(t) k2 +
− 1−
ky − y (t) k2 + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
2 τ
α 2
2σ
≥

L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )

11
1 1
γ ⋆
+
ky − y (t+1) k2 + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) )
+ λ kx⋆ − x(t+1) k2 +
+
2 τ
2 σ
2
1 (t+1)
1
ky
− y (t) k2 − θ(y (t+1) − y (t) )T A(x(t) − x(t−1) )
+ kx(t+1) − x(t) k2 +
2τ
2σ
2
 1
δ


− (α − 1) θA(x(t) − x(t−1) ) − (y (t+1) − y (t) ) .
4
σ

(29)

Next we derive another form of the underlined items above:

=
=
≥

1 (t+1)
ky
− y (t) k2 − θ(y (t+1) − y (t) )T A(x(t) − x(t−1) )
2σ

σ 1 (t+1)
θ (t+1)
(t) 2
(t) T
(t)
(t−1)
ky
− y k − (y
− y ) A(x − x
)
2 σ2
σ


2
σ 
1 (t+1)
(t) 
(t)
(t−1)
2
(t)
(t−1) 2
− y ) − θ kA(x − x
) − (y
)k
θA(x − x
2
σ
2 σθ2 L2
1
σ


kx(t) − x(t−1) k2 ,
θA(x(t) − x(t−1) ) − (y (t+1) − y (t) ) −
2
σ
2

where in the last inequality we used kAk ≤ L and hence kA(x(t) − x(t−1) )k2 ≤ L2 kx(t) − x(t−1) k2 . Combining with
inequality (29), we have


1 1 
1 (t)
1  δµ2
σθ2 L2 (t)
kx(t) − x⋆ k2 +
− 1−
ky − y ⋆ k2 + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) ) +
kx − x(t−1) k2
2 τ
α 2
2σ
2
≥

L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )




1 1
1 (t+1)
1 1
γ
+
ky (t+1) − y ⋆ k2 + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) +
+ λ kx(t+1) − x⋆ k2 +
+
kx
− x(t) k2
2 τ
2 σ
2
2τ


2
δ 
σ
1


− (α − 1)
+
(30)
θA(x(t) − x(t−1) ) − (y (t+1) − y (t) ) .
2
4
σ

We can remove the last term in the above inequality as long as its coefficient is nonnegative, i.e.,
δ
σ
− (α − 1) ≥ 0.
2
4

In order to maximize 1 − 1/α, we take the equality and solve for the largest value of α allowed, which results in
α=1+

2σ
,
δ

1−

1
2σ
=
.
α
2σ + δ

Applying these values in (30) gives


1 1
1 (t)
σδµ2
σθ2 L2 (t)
kx(t) − x⋆ k2 +
−
ky − y ⋆ k2 + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) ) +
kx − x(t−1) k2
2 τ
2σ + δ
2σ
2
≥

L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )




1 (t+1)
1 1
γ
1 1
ky (t+1) − y ⋆ k2 + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) +
+ λ kx(t+1) − x⋆ k2 +
+
kx
− x(t) k2 .
+
2 τ
2 σ
2
2τ
(31)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

We use ∆(t+1) to denote the last row in (31). Equivalently, we define
∆(t)

=
=


11
+ λ kx⋆ − x(t) k2 +
2 τ

11
+ λ kx⋆ − x(t) k2 +
2 τ

1 (t)
1 1
γ ⋆
ky − y (t) k2 + (y (t) − y ⋆ )T A(x(t) − x(t−1) ) +
+
kx − x(t−1) k2
2 σ
2
2τ

  (t)
T  1

γ ⋆
1 x(t) − x(t−1)
x − x(t−1)
I −AT
(t) 2
τ
.
ky − y k +
1
−A
y ⋆ − y (t)
y ⋆ − y (t)
4
2
σ

The quadratic form in the last term is nonnegative if the matrix
M=



1
τI

−A

−AT
1
σ



is positive semidefinite, for which a sufficient condition is τ σ ≤ 1/L2 . Under this condition,
∆(t) ≥
If we can to choose τ and σ so that


γ
11
+ λ kx⋆ − x(t) k2 + ky ⋆ − y (t) k2 ≥ 0.
2 τ
4
1
γ
1
,
≤θ
+
σ
σ
2

1

1
σδµ2
−
≤θ
+λ ,
τ
2σ + δ
τ

then, according to (31), we have

σθ2 L2
1
≤θ ,
2
2τ

(32)

(33)

∆(t+1) + L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) ) ≤ θ∆(t) .
Because ∆(t) ≥ 0 and L(x(t) , y ⋆ ) − L(x⋆ , y (t) ) ≥ 0 for any t ≥ 0, we have
∆(t+1) ≤ θ∆(t) ,
which implies
∆(t) ≤ θt ∆(0)
and
L(x(t) , y ⋆ ) − L(x⋆ , y (t) ) ≤ θt ∆(0) .
Let θx and θy be two contraction factors determined by the first two inequalities in (33), i.e.,
θx

=

θy

=





1
1
1
σδµ2
τ σδµ2
−
+λ = 1−
,
τ
2σ + δ
τ
2σ + δ 1 + τ λ

1
1
1
γ
=
+
.
σ
σ
2
1 + σγ/2



Then we can let θ = max{θx , θy }. We note that any θ < 1 would satisfy the last condition in (33) provided that
τσ =

1
,
L2

which also makes the matrix M positive semidefinite and thus ensures the inequality (32).
Among all possible pairs τ, σ that satisfy τ σ = 1/L2 , we choose
1
τ=
L
which give the desired results of Theorem 1.

r

γ
,
λ + δµ2

1
σ=
L

s

λ + δµ2
,
γ

(34)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

D. Proof of Theorem 3
If we choose h = f ∗ , then
• h is γ-strongly convex and 1/δ-smooth, i.e., γ ′ = γ and δ ′ = δ;
• f ∗ is 1-strongly convex with respect to h, i.e., ν = 1.
For convenience, we repeat inequality (28) here:
1 ⋆
1
kx − x(t) k2 + D(y ⋆ , y (t) ) + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
2τ
σ
≥ L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )

1
11
ν
D(y ⋆ , y (t+1) ) + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) )
+
+ λ kx⋆ − x(t+1) k2 +
+
2 τ
σ
2
1
1
+ kx(t+1) − x(t) k2 + D(y (t+1) , y (t) ) − θ(y (t+1) − y (t) )T A(x(t) − x(t−1) )
2τ
σ


 1
δ
1  δµ2 ⋆

2
kx − x(t) k2 − (α − 1) θA(x(t) − x(t−1) ) −
∇h(y (t+1) ) − ∇h(y (t) )  .
+ 1−
α 4
4
σ

(35)

We first bound the Bregman divergence D(y (t+1) , y (t) ) using the assumption that the kernel h is γ-strongly convex and
1/δ-smooth. Using similar arguments as in the proof of Lemma 2, we have for any ρ ∈ [0, 1],
D(y (t+1) , y (t) )

=
≥

h(y (t+1) ) − h(y (t) ) − h∇h(y (t) ), y (t+1) − y (t) i
2
γ
δ
(1 − ρ) ky (t+1) − y (t) k2 + ρ ∇h(y (t+1) ) − ∇h(y (t) ) .
2
2

(36)

For any β > 0, we can lower bound the inner product term

θ2 L2 (t)
β
kx − x(t−1) k2 .
−θ(y (t+1) − y (t) )T A(x(t) − x(t−1) ) ≥ − ky (t+1) − y (t) k2 −
2
2β
In addition, we have

2

 1
2
2

∇h(y (t+1) ) − ∇h(y (t) )  ≤ 2θ2 L2 kx(t) − x(t−1) k2 + 2 ∇h(y (t+1) ) − ∇h(y (t) ) .
θA(x(t) − x(t−1) ) −
σ
σ

Combining these bounds with (35) and (36) with ρ = 1/2, we arrive at

≥



1 1 
1
1  δµ2
kx⋆ − x(t) k2 + D(y ⋆ , y (t) ) + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
− 1−
2 τ
α 2
σ
 θ 2 L2
δθ2 L2  (t)
kx − x(t−1) k2
+ (α − 1)
+
2β
2

L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )

1
11
1
+
D(y ⋆ , y (t+1) ) + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) )
+ λ kx⋆ − x(t+1) k2 +
+
2 τ
σ 2
γ
 δ

β  (t+1)
(α − 1)δ 
∇h(y (t+1) ) − ∇h(y (t) )2
+
−
−
ky
− y (t) k2 +
2
4σ
2
4σ
2σ
1 (t+1)
(t) 2
−x k .
+ kx
2τ

We choose α and β in (37) to zero out the coefficients of ky (t+1) − y (t) k2 and k∇h(y (t+1) ) − ∇h(y (t) )k2 :
α=1+

σ
,
2

β=

γ
.
2σ

(37)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Then the inequality (37) becomes


1
1 1
σδµ2
kx⋆ − x(t) k2 + D(y ⋆ , y (t) ) + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
−
2 τ
4 + 2σ
σ

 2 2
δσθ2 L2
σθ L
kx(t) − x(t−1) k2
+
+
γ
4
≥

L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )



1
1
11
⋆
(t+1) 2
D(y ⋆ , y (t+1) ) + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) )
+ λ kx − x
k +
+
+
2 τ
σ 2
1
+ kx(t+1) − x(t) k2 .
2τ

The coefficient of kx(t) − x(t−1) k2 can be bounded as

1
σθ2 L2
4 + γδ 2 2
δσθ2 L2
δ 2 2
2σθ2 L2
σθ L =
+
=
+
σθ L <
,
γ
4
γ
4
4γ
γ

where in the inequality we used γδ ≤ 1. Therefore we have


1
σδµ2
2σθ2 L2 (t)
1 1
kx⋆ − x(t) k2 + D(y ⋆ , y (t) ) + θ(y (t) − y ⋆ )T A(x(t) − x(t−1) ) +
−
kx − x(t−1) k2
2 τ
4 + 2σ
σ
γ
≥

L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) )



11
1 (t+1)
1
1
⋆
(t+1) 2
+
D(y ⋆ , y (t+1) ) + (y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) +
+ λ kx − x
k +
+
kx
− x(t) k2 .
2 τ
σ 2
2τ

We use ∆(t+1) to denote the last row of the above inequality. Equivalently, we define



11
1 (t)
1
1
(t)
⋆
(t) 2
∆ =
D(y ⋆ , y (t) ) + (y (t) − y ⋆ )T A(x(t) − x(t−1) ) +
+ λ kx − x k +
+
kx − x(t−1) k2 .
2 τ
σ 2
2τ
Since h is γ-strongly convex, we have D(y ⋆ , y (t) ) ≥ γ2 ky ⋆ − y (t) k2 , and thus
∆(t)

≥
=


11
+ λ kx⋆ − x(t) k2 +
2 τ

11
+ λ kx⋆ − x(t) k2 +
2 τ

1
D(y ⋆ , y (t) ) +
2
1
D(y ⋆ , y (t) ) +
2

γ (t)
1 (t)
ky − y ⋆ k2 + (y (t) − y ⋆ )T A(x(t) − x(t−1) ) +
kx − x(t−1) k2
2σ
2τ

T  1
  (t)

1 x(t) − x(t−1)
x − x(t−1)
−AT
τI
.
γ
−A
y ⋆ − y (t)
y ⋆ − y (t)
2
σ

The quadratic form in the last term is nonnegative if τ σ ≤ γ/L2 . Under this condition,
∆(t) ≥
If we can to choose τ and σ so that

then we have


11
1
+ λ kx⋆ − x(t) k2 + D(y ⋆ , y (t) ) ≥ 0.
2 τ
2

1

1
σδµ2
−
≤θ
+λ ,
τ
4 + 2σ
τ

1
1
1
≤θ
+
,
σ
σ 2

∆(t+1) + L(x(t+1) , y ⋆ ) − L(x⋆ , y (t+1) ) ≤ θ∆(t) .

Because ∆(t) ≥ 0 and L(x(t) , y ⋆ ) − L(x⋆ , y (t) ) ≥ 0 for any t ≥ 0, we have
∆(t+1) ≤ θ∆(t) ,
which implies

1
2σθ2 L2
≤θ ,
γ
2τ

∆(t) ≤ θt ∆(0)

(38)

(39)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

and
L(x(t) , y ⋆ ) − L(x⋆ , y (t) ) ≤ θt ∆(0) .
To satisfy the last condition in (39) and also ensure the inequality (38), it suffices to have
τσ ≤
We choose
τ=

1
2L

r

γ
,
λ + δµ2

γ
.
4L2

σ=

With the above choice and assuming γ(λ + δµ2 ) ≪ L2 , we have
1
σ

θy =

1
σ

+

1
2

1 p
γ(λ + δµ2 ).
2L

1
1
p
≈1−
=
=
1 + σ/2
1 + γ(λ + δµ2 )/(4L)

p

γ(λ + δµ2 )
.
4L

For the contraction factor over the primal variables, we have
θx =

1
τ

−

1
τ

σδµ2
4+2σ

+λ

=

1−

τ σδµ2
4+2σ

1 + τλ

=

1−

γδµ2
4(4+2σ)L2

λ
γδµ2
−
≈1−
16L2
2L

1 + τλ

r

γ
.
λ + δµ2

This finishes the proof of Theorem 3.

E. Proof of Theorem 2
We consider the SPDC algorithm in the Euclidean case with h(x) = (1/2)kxk2 . The corresponding batch case analysis is
given in Section C. For each i = 1, . . . , n, let ỹi be
)
(
(t)
(y − yi )2
(t)
∗
− yhai , x̃ i .
ỹi = arg min φi (y) +
y
2σ
Based on the first-order optimality condition, we have
(t)

hai , x̃(t) i −

′
(ỹi − yi )
∈ φ∗i (ỹi ).
σ

Also, since yi⋆ minimizes φ∗i (y) − yhai , x⋆ i, we have
′

hai , x⋆ i ∈ φ∗i (yi⋆ ).
By Lemma 2 with ρ = 1/2, we have
(t)

−yi⋆ hai , x̃(t) i

+

φ∗i (yi⋆ )

(y − yi⋆ )2
≥
+ i
2σ


+

γ
1
+
σ
2



(ỹi − yi⋆ )2
+ φ∗i (ỹi ) − ỹi hai , x̃(t) i
2

(t)

′
′
(ỹi − yi )2
δ
+ (φ∗i (ỹi ) − φ∗i (yi⋆ ))2 ,
2σ
4

and re-arranging terms, we get
(t)

(yi − yi⋆ )2
≥
2σ


(t)
γ (ỹi − yi⋆ )2
(ỹi − yi )2
1
+
+
− (ỹi − yi⋆ )hai , x̃(t) i + (φ∗i (ỹi ) − φ∗i (yi⋆ ))
σ
2
2
2σ
′
′
δ
+ (φ∗i (ỹi ) − φ∗i (yi⋆ ))2 .
4



(40)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Notice that
n − 1 (t)
1
· ỹi +
· yi ,
n
n
(t)
(ỹi − yi⋆ )2
(n − 1)(yi − yi⋆ )2
(t+1)
E[(yi
− yi⋆ )2 ] =
+
,
n
n
(t)
(ỹi − yi )2
(t+1)
(t)
,
E[(yi
− yi )2 ] =
n
1
n − 1 ∗ (t)
(t+1)
E[φ∗i (yi
)] = · φ∗i (ỹi ) +
· φi (yi ).
n
n
(t+1)

E[yi

]=

Plug the above relations into (40) and divide both sides by n, we have




1
1
1
(n − 1)γ
γ
(t)
(t+1)
(t+1)
(t)
⋆ 2
(yi − yi ) ≥
E[(yi
− yi⋆ )2 ] +
+
+
E[(yi
− yi )2 ]
2σ
4n
2σ
4
2σ


1 (t)
(t+1)
(t)
⋆
− E[(yi
− yi )] + (yi − yi ) hai , x̃(t) i
n
1
(t+1)
(t)
(t)
+ E[φ∗i (yi
)] − φ∗i (yi ) + (φ∗i (yi ) − φ∗i (yi⋆ ))
n
!2
(t)
(ỹ
−
y
)
δ
i
i
hai , x̃(t) − x⋆ i −
,
+
4n
σ
and summing over i = 1, . . . , n, we get




E[ky (t+1) − y (t) k2 ]
1
(n − 1)γ
γ
1
(t)
⋆ 2
ky − y k ≥
E[ky (t+1) − y ⋆ k2 ] +
+
+
2σ
4n
2σ
4
2σ
n
1 X ∗ (t)
(t+1)
(t)
(φ (y ) − φ∗i (yi⋆ ))
+ φ∗k (yk
) − φ∗k (yk ) +
n i=1 i i
E
D
− n(u(t+1) − u(t) ) + (u(t) − u∗ ), x̃(t)


(t) 2
δ 
A(x∗ − x̃(t) ) + (ỹ − y )  ,
+

4n 
σ

where

n

u(t) =

1 X (t)
y ai ,
n i=1 i

On the other hand, since x(t+1) minimizes the

n

u(t+1) =
1
τ

1 X (t+1)
y
ai ,
n i=1 i

n

and

u⋆ =

1X ⋆
y ai .
n i=1 i

+ λ-strongly convex objective

D
E kx − x(t) k2
,
g(x) + u(t) + n(u(t+1) − u(t) ), x +
2τ

we can apply Lemma 2 with ρ = 0 to obtain

kx(t) − x⋆ k2
2τ


kx(t+1) − x(t) k2
λ
1
(t+1)
(t)
(t+1)
(t)
(t+1)
≥ g(x
) + hu + n(u
− u ), x
i+
kx(t+1) − x⋆ k2 ,
+
+
2τ
2τ
2

g(x⋆ ) + hu(t) + n(u(t+1) − u(t) ), x⋆ i +

and re-arranging terms we get


E[kx(t+1) − x(t) k2 ]
λ
1
kx(t) − x⋆ k2
E[kx(t+1) − x⋆ k2 ] +
≥
+
+ E[g(x(t+1) ) − g(x⋆ )]
2τ
2τ
2
2τ
+ E[hu(t) + n(u(t+1) − u(t) ), x(t+1) − x⋆ i].

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Also notice that
L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) )) − (n − 1)(L(x⋆ , y ⋆ ) − L(x⋆ , y (t) ))
n
1 X ∗ (t)
(t+1)
(t)
(φ (y ) − φ∗i (y ⋆ )) + (φ∗k (yk
) − φ∗k (yk )) + g(x(t+1) ) − g(x⋆ )
=
n i=1 i i
+ hu⋆ , x(t+1) i − hu(t) , x⋆ i + nhu(t) − u(t+1) , x⋆ i.

Combining everything together, we have


kx(t) − x⋆ k2
(n − 1)γ
1
ky (t) − y ⋆ k2 + (n − 1)(L(x⋆ , y ⋆ ) − L(x⋆ , y (t) ))
+
+
2τ
2σ
4n




E[kx(t+1) − x(t) k2 ] E[ky (t+1) − y (t) k2 ]
1
1
λ
γ
≥
E[kx(t+1) − x⋆ k2 ] +
E[ky (t+1) − y ⋆ k2 ] +
+
+
+
2τ
2
2σ
4
2τ
2σ
+ E[L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) ))]

2
(ỹ − y (t) ) 
δ 
⋆
(t)
(t)
⋆
(t+1)
(t)
(t+1)
t

 .
A(x − x̃ ) +
+ E[hu − u + n(u
− u ), x
− x̄ i] +

4n 
σ

Next we notice that

2


(t+1)
(t) 2

δ 
] − y (t) ) 
A(x⋆ − x̃(t) ) + n(E[y
 = δ A(x⋆ − x(t) ) − θA(x(t) − x(t−1) ) + (ỹ − y ) 


4n 
σ
4n 
σ




1
δ 
2
≥ 1−
A(x⋆ − x(t) )
α 4n

2
(ỹ − y (t) ) 
δ 
(t)
(t−1)

 ,
θA(x − x
)+
− (α − 1)

4n 
σ
for some α > 1 and
and


2


A(x⋆ − x(t) ) ≥ µ2 kx⋆ − x(t) k2 ,



(t) 2

θA(x(t) − x(t−1) ) + (ỹ − y )  ≥ − 2θ2 kA(x(t) − x(t−1) )k2 − 2 kỹ − y (t) k2


σ
σ2
2n
≥ − 2θ2 L2 kx(t) − x(t−1) k2 − 2 E[ky (t+1) − y (t) k2 ].
σ
We follow the same reasoning as in the standard SPDC analysis,
hu(t) − u⋆ + n(u(t+1) − u(t) ), x(t+1) − x̃(t) i =

(y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
−
n
n

(n − 1) (t+1)
(y
− y (t) )T A(x(t+1) − x(t) ) − θ(y (t+1) − y (t) )T A(x(t) − x(t−1) ),
n
and using Cauchy-Schwartz inequality, we have
+

|(y (t+1) − y (t) )T A(x(t) − x(t−1) )| ≤
≤

kx(t) − x(t−1) k2
k(y (t+1) − y (t) )T Ak2
+
1/(2τ )
8τ
ky (t+1) − y (t) k2
,
1/(2τ R2 )

and
|(y (t+1) − y (t) )T A(x(t+1) − x(t) )| ≤
≤

k(y (t+1) − y (t) )T Ak2
kx(t+1) − x(t) k2
+
1/(2τ )
8τ
ky (t+1) − y (t) k2
.
1/(2τ R2 )

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Thus we get
(y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
−
n
n
ky (t+1) − y (t) k2
kx(t+1) − x(t) k2
θkx(t) − x(t−1) k2
−
−
−
.
1/(4τ R2 )
8τ
8τ

hu(t) − u⋆ + n(u(t+1) − u(t) ), x(t+1) − x̃(t) i ≥

Putting everything together, we have




1
(1 − 1/α)δµ2
(n − 1)γ
1
(t)
⋆ 2
kx − x k +
ky (t) − y ⋆ k2 + θ(L(x(t) , y ⋆ ) − L(x⋆ , y ⋆ ))
−
+
2τ
4n
2σ
4n


θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
1
(α − 1)θδL2
+ (n − 1)(L(x⋆ , y ⋆ ) − L(x⋆ , y (t) )) + θ
kx(t) − xt−1 k2 +
+
8τ
2n
n




(t+1)
⋆ T
(t+1)
E[(y
− y ) A(x
− x(t) )]
1
1
λ
γ
E[kx(t+1) − x⋆ k2 ] +
E[ky (t+1) − y ⋆ k2 ] +
≥
+
+
2τ
2
2σ
4
n
+ E[L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) ))]


1
1
+
E[kx(t+1) − x(t) k2 ]
−
2τ
8τ


1
(α − 1)δ
2
+
E[ky (t+1) − y (t) k2 ].
− 4R τ −
2σ
2σ 2

If we choose the parameters as
α=
then we know
and

σ
+ 1,
4δ

στ =

1
,
16R2

1
1
(α − 1)δ
1
1
=
− 4R2 τ −
−
−
> 0,
2σ
2σ 2
2σ 4σ 8σ
σL2
σR2
1
(α − 1)θδL2
≤
≤
≤
,
2
2n
8n
8
256τ

thus

1
(α − 1)θδL2
3
+
≤
.
8τ
2n
8τ

In addition, we have
1−

1
σ
=
.
α
σ + 4δ

Finally we obtain




1
σδµ2
(n − 1)γ
1
(t)
⋆ 2
kx − x k +
ky (t) − y ⋆ k2 + θ(L(x(t) , y ⋆ ) − L(x⋆ , y ⋆ ))
−
+
2τ
4n(σ + 4δ)
2σ
4n

3 (t)
θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
+ (n − 1)(L(x⋆ , y ⋆ ) − L(x⋆ , y (t) )) + θ ·
kx − x(t−1) k2 +
8τ
n




1
E[(y (t+1) − y ⋆ )T A(x(t+1) − x(t) )]
1
λ
γ
(t+1)
⋆ 2
(t+1)
⋆ 2
≥
E[kx
−x k ]+
E[ky
−y k ]+
+
+
2τ
2
2σ
4
n
3
E[kx(t+1) − x(t) k2 ].
+ E[L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) ))] +
8τ

Now we can define θx and θy as the ratios between the coefficients in the x-distance and y-distance terms, and let θ =
max{θx , θy } as before. Choosing the step-size parameters as
s
r
1
1
γ
nλ + δµ2
τ=
,
σ
=
4R nλ + δµ2
4R
γ
gives the desired result.

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

F. Proof of Theorem 4
In this setting, for i-th coordinate of the dual variables y we choose h = φ∗i , let
Di (yi , yi′ ) = φ∗i (yi ) − φ∗i (yi′ ) + h(φ∗i )′ (yi′ ), yi − yi′ i,
and define
′

D(y, y ) =
For i = 1, . . . , n, let ỹi be
ỹi = arg min
y

(

φ∗i (y)

n
X
i=1

Di (yi , yi′ ).

)
(t)
Di (y, yi )
(t)
− yhai , x̃ i .
+
σ

Based on the first-order optimality condition, we have
(t)

hai , x̃(t) i −

(φ∗i )′ (ỹi ) − (φ∗i )′ (yi )
∈ (φ∗i )′ (ỹi ).
σ

Also since yi⋆ minimizes φ∗i (y) − yhai , x∗ i, we have
hai , x∗ i ∈ (φ∗i )′ (yi⋆ ).
Using Lemma 2 with ρ = 1/2, we obtain
(t)

−yi⋆ hai , x̃(t) i + φ∗i (yi⋆ ) +

Di (yi⋆ , yi )
≥
σ


+

1
1
+
σ 2



Di (yi⋆ , ỹi ) + φ∗i (ỹi ) − ỹi hai , x̃(t) i
(t)

Di (ỹi , yi ) δ
+ ((φ∗i )′ (ỹi ) − (φ∗i )′ (yi⋆ ))2 ,
σ
4

and rearranging terms, we get


(t)
(t)
Di (ỹi , yi )
1
1
Di (yi⋆ , yi )
Di (yi⋆ , ỹi ) +
≥
+
− (ỹi − yi⋆ )hai , x̃(t) i + (φ∗i (ỹi ) − φ∗i (yi⋆ ))
σ
σ 2
σ
δ
+ ((φ∗i )′ (ỹi ) − (φ∗i )′ (yi⋆ ))2 .
4
With i.i.d. random sampling at each iteration, we have the following relations:
n − 1 (t)
1
· ỹi +
· yi ,
n
n
(t)
Di (ỹi , yi⋆ ) (n − 1)Di (yi , yi⋆ )
(t+1) ⋆
+
,
E[Di (yi
, yi )] =
n
n
(t)
Di (ỹi , yi )
(t+1) (t)
E[Di (yi
, yi )] =
,
n
1
n − 1 ∗ (t)
(t+1)
E[φ∗i (yi
)] = · φ∗i (ỹi ) +
· φi (yi ).
n
n
(t+1)

E[yi

]=

Plugging the above relations into (41) and dividing both sides by n, we have




1
1
(n − 1)
1
1
(t) ⋆
(t+1) ⋆
(t+1) (t)
Di (yi , yi ) ≥
Di (yi
, yi ) + E[Di (yi
+
+
, yi )]
σ
2n
σ 2
σ


1 (t)
(t+1)
(t)
− E[(yi
− yi )] + (yi − yi⋆ ) hai , x̃(t) i
n
1
(t+1)
(t)
(t)
+ E[φ∗i (yi
)] − φ∗i (yi ) + (φ∗i (yi ) − φ∗i (yi⋆ ))
n
!2
(t)
((φ∗i )′ (ỹi ) − (φ∗i )′ (yi ))
δ
(t)
⋆
hai , x̃ − x i −
,
+
4n
σ

(41)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

and summing over i = 1, . . . , n, we get




E[D(y (t+1) , y (t) )]
1
(n − 1)
1
1
D(y (t) , y ⋆ ) ≥
E[D(y (t+1) , y ⋆ )] +
+
+
σ
2n
σ 2
σ
n
X
1
(t)
(t+1)
(t)
(φ∗ (y ) − φ∗i (yi⋆ ))
+ φ∗k (yk
) − φ∗k (yk ) +
n i=1 i i
E
D
− n(u(t+1) − u(t) ) + (u(t) − u∗ ), x̃(t)

2
′
′
δ 
(φ∗ (ỹ) − φ∗ (y (t) )) 


⋆
(t)
+
A(x − x̃ ) +
 ,

4n 
σ
′

where φ∗ (y (t) ) is a n-dimensional vector such that the i-th coordinate is
′

(t)

[φ∗ (y (t) )]i = (φ∗i )′ (yi ),
and

n

u(t) =

n

1 X (t)
y ai ,
n i=1 i

On the other hand, since x(t+1) minimizes a

u(t+1) =
1
τ

1 X (t+1)
y
ai ,
n i=1 i

n

and

u⋆ =

1X ⋆
y ai .
n i=1 i

+ λ-strongly convex objective

D
E kx − x(t) k2
g(x) + u(t) + n(u(t+1) − u(t) ), x +
,
2τ

we can apply Lemma 2 with ρ = 0 to obtain

kx(t) − x⋆ k2
2τ


kx(t+1) − x(t) k2
λ
1
(t+1)
(t)
(t+1)
(t)
(t+1)
≥ g(x
) + hu + n(u
− u ), x
i+
kx(t+1) − x⋆ k2 ,
+
+
2τ
2τ
2

g(x⋆ ) + hu(t) + n(u(t+1) − u(t) ), x⋆ i +

and rearranging terms, we get


E[kx(t+1) − x(t) k2 ]
λ
1
kx(t) − x⋆ k2
E[kx(t+1) − x⋆ k2 ] +
≥
+
+ E[g(x(t+1) ) − g(x⋆ )]
2τ
2τ
2
2τ
+ E[hu(t) + n(u(t+1) − u(t) ), x(t+1) − x⋆ i].

Notice that
L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) )) − (n − 1)(L(x⋆ , y ⋆ ) − L(x⋆ , y (t) ))
n
1 X ∗ (t)
(t+1)
(t)
=
(φ (y ) − φ∗i (y ⋆ )) + (φ∗k (yk
) − φ∗k (yk )) + g(x(t+1) ) − g(x⋆ )
n i=1 i i
+ hu⋆ , x(t+1) i − hu(t) , x⋆ i + nhu(t) − u(t+1) , x⋆ i,

so



kx(t) − x⋆ k2
(n − 1)
1
D(y (t) , y ⋆ ) + (n − 1)(L(x⋆ , y ⋆ ) − L(x⋆ , y (t) ))
+
+
2τ
σ
2n




E[kx(t+1) − x(t) k2 ] E[D(y (t+1) , y (t) )]
1
1
λ
1
(t+1)
⋆ 2
≥
E[kx
−x k ]+
E[D(y (t+1) , y ⋆ )] +
+
+
+
2τ
2
σ 2
2τ
σ
+ E[L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) ))]

2
′
′
δ 
(φ∗ (ỹ) − φ∗ (y (t) )) 


(t)
⋆
(t+1)
(t)
(t+1)
t
⋆
(t)
+ E[hu − u + n(u
− u ), x
− x̄ i] +
A(x − x̃ ) +
 .

4n 
σ

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Next, we have

2

2
′
′
′
′
δ 
(φ∗ (ỹ) − φ∗ (y (t) )) 
(φ∗ (ỹ) − φ∗ (y (t) )) 
δ 




⋆
(t)
⋆
(t)
(t)
(t−1)
)+
A(x − x̃ ) +
 =
A(x − x ) − θA(x − x



4n 
σ
4n 
σ




2
1
δ 

≥ 1−
A(x⋆ − x(t) )
α 4n

2
′
′
(φ∗ (ỹ) − φ∗ (y (t) )) 
δ 


(t)
(t−1)
)+
− (α − 1)
θA(x − x
 ,

4n 
σ
for any α > 1 and
and


2


A(x⋆ − x(t) ) ≥ µ2 kx⋆ − x(t) k2 ,


2
′
′

′
′
(φ∗ (ỹ) − φ∗ (y (t) )) 
2


(t)
(t−1)
)+
θA(x − x
 ≥ − 2θ2 kA(x(t) − x(t−1) )k2 − 2 kφ∗ (ỹ) − φ∗ (y (t) )k2 ]


σ
σ
≥ − 2θ2 L2 kx(t) − x(t−1) k2 −

′
′
2n
E[kφ∗ (y (t+1) ) − φ∗ (y (t) )k2 ].
2
σ

Following the same reasoning as in the standard SPDC analysis, we have
hu(t) − u⋆ + n(u(t+1) − u(t) ), x(t+1) − x̃(t) i =

(y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
−
n
n

(n − 1) (t+1)
(y
− y (t) )T A(x(t+1) − x(t) ) − θ(y (t+1) − y (t) )T A(x(t) − x(t−1) ),
n
and using Cauchy-Schwartz inequality, we have
+

|(y (t+1) − y (t) )T A(x(t) − x(t−1) )| ≤
≤

k(y (t+1) − y (t) )T Ak2
kx(t) − x(t−1) k2
+
1/(2τ )
8τ
ky (t+1) − y (t) k2
,
1/(2τ R2 )

and
|(y (t+1) − y (t) )T A(x(t+1) − x(t) )| ≤
≤

kx(t+1) − x(t) k2
k(y (t+1) − y (t) )T Ak2
+
1/(2τ )
8τ
ky (t+1) − y (t) k2
.
1/(2τ R2 )

Thus we get
(y (t+1) − y ⋆ )T A(x(t+1) − x(t) ) θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
−
n
n
(t+1)
(t) 2
(t+1)
(t) 2
(t)
(t−1) 2
ky
−y k
kx
−x k
θkx − x
k
−
−
−
.
1/(4τ R2 )
8τ
8τ

hu(t) − u∗ + n(u(t+1) − u(t) ), x(t+1) − x̃(t) i ≥

Also we can lower bound the term D(y (t+1) , y (t) ) using Lemma 2 with ρ = 1/2:
D(y (t+1) , y (t) ) =
≥

n 
X

(t+1)

φ∗i (yi

i=1

n 
X
γ
i=1

2

(t+1)
(yi

(t)

(t)

(t+1)

) − φ∗i (yi ) − h(φ∗i )′ (yi ), yi
−

(t)
yi )2

(t)

− yi i



δ
(t+1)
(t)
+ ((φ∗i )′ (yi
) − (φ∗i )′ (yi ))2
2

′
′
γ
δ
= ky (t+1) − y (t) k2 + kφ∗ (y (t+1) ) − φ∗ (y (t) )k2 .
2
2



Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Combining everything above together, we have




1
(1 − 1/α)δµ2
(n − 1)
1
(t)
⋆ 2
kx − x k +
D(y (t) , y ⋆ ) + θ(L(x(t) , y ⋆ ) − L(x⋆ , y ⋆ ))
−
+
2τ
4n
σ
2n


θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
1
(α − 1)θδL2
⋆ ⋆
⋆ (t)
+ (n − 1)(L(x , y ) − L(x , y )) + θ
kx(t) − xt−1 k2 +
+
8τ
2n
n




(t+1)
⋆ T
(t+1)
E[(y
− y ) A(x
− x(t) )]
1
1
λ
1
≥
E[kx(t+1) − x⋆ k2 ] +
E[D(y (t+1) , y ⋆ )] +
+
+
2τ
2
σ 2
n
+ E[L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) ))]


γ

1
1
+
E[kx(t+1) − x(t) k2 ] +
−
− 4R2 τ E[ky (t+1) − y (t) k2 ]
2τ
8τ
2σ


′
′
δ
(α − 1)δ
E[kφ∗ (y (t+1) ) − φ∗ (y (t) )k2 ].
+
−
2σ
2σ 2

If we choose the parameters as
α=
then we know
and

σ
+ 1,
4

στ =

γ
,
16R2

γ
γ
γ
− 4R2 τ =
−
> 0,
2σ
2σ 4σ
δ
(α − 1)δ
δ
δ
=
−
−
>0
2
2σ
2σ
2σ 8σ

and

(α − 1)θδL2
σδL2
δσR2
δγ
1
≤
≤
≤
≤
,
2
2n
8n
8
256τ
256τ

thus

(α − 1)θδL2
3
1
+
≤
.
8τ
2n
8τ

In addition, we have
1−

1
σ
=
.
α
σ+4

Finally we obtain




1
σδµ2
(n − 1)
1
(t)
⋆ 2
kx − x k +
D(y (t) , y ⋆ ) + θ(L(x(t) , y ⋆ ) − L(x⋆ , y ⋆ ))
−
+
2τ
4n(σ + 4)
σ
2n

3 (t)
θ(y (t) − y ⋆ )T A(x(t) − x(t−1) )
+ (n − 1)(L(x⋆ , y ⋆ ) − L(x⋆ , y (t) )) + θ ·
kx − x(t−1) k2 +
8τ
n




1
E[(y (t+1) − y ⋆ )T A(x(t+1) − x(t) )]
1
λ
1
(t+1)
⋆ 2
(t+1)
⋆ 2
≥
E[kx
−x k ]+
E[ky
−y k ]+
+
+
2τ
2
σ 2
n
3
+ E[L(x(t+1) , y ⋆ ) − L(x⋆ , y ⋆ ) + n(L(x⋆ , y ⋆ ) − L(x⋆ , y (t+1) ))] +
E[kx(t+1) − x(t) k2 ].
8τ

As before, we can define θx and θy as the ratios between the coefficients in the x-distance and y-distance terms, and let
θ = max{θx , θy }. Then choosing the step-size parameters as
r
1 p
1
γ
, σ=
τ=
γ(nλ + δµ2 )
2
4R nλ + δµ
4R
gives the desired result.

