Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

In the following appendices, we provide detailed proofs of theorems stated in the main paper. In Section A we first prove
a basic inequality which is useful throughout the rest of the convergence analysis. Section B contains general analysis of
the batch primal-dual algorithm that are common for proving both Theorem 1 and Theorem 3. Sections C, D, E and F give
proofs for Theorem 1, Theorem 3, Theorem 2 and Theorem 4, respectively.

A. A basic lemma
Lemma 2. Let h be a strictly convex function and Dh be its Bregman divergence. Suppose Ïˆ is Î½-strongly convex with
respect to Dh and 1/Î´-smooth (with respect to the Euclidean norm), and
yÌ‚ = arg min
yâˆˆC



	
Ïˆ(y) + Î·Dh (y, yÌ„) ,

where C is a compact convex set that lies within the relative interior of the domains of h and Ïˆ (i.e., both h and Ïˆ are
differentiable over C). Then for any y âˆˆ C and Ï âˆˆ [0, 1], we have

ÏÎ´
2
Ïˆ(y) + Î·Dh (y, yÌ„) â‰¥ Ïˆ(yÌ‚) + Î·Dh (yÌ‚, yÌ„) + Î· + (1 âˆ’ Ï)Î½ Dh (y, yÌ‚) +
kâˆ‡Ïˆ(y) âˆ’ âˆ‡Ïˆ(yÌ‚)k .
2
Proof. The minimizer yÌ‚ satisfies the following first-order optimality condition:
hâˆ‡Ïˆ(yÌ‚) + Î·âˆ‡Dh (yÌ‚, yÌ„), y âˆ’ yÌ‚i â‰¥ 0,

âˆ€ y âˆˆ C.

Here âˆ‡D denotes partial gradient of the Bregman divergence with respect to its first argument, i.e., âˆ‡D(yÌ‚, yÌ„) = âˆ‡h(yÌ‚) âˆ’
âˆ‡h(yÌ„). So the above optimality condition is the same as




âˆ‡Ïˆ(yÌ‚) + Î·(âˆ‡h(yÌ‚) âˆ’ âˆ‡h(yÌ„)), y âˆ’ yÌ‚ â‰¥ 0,

âˆ€ y âˆˆ C.

(17)

Since Ïˆ is Î½-strongly convex with respect to Dh and 1/Î´-smooth, we have

Ïˆ(y) â‰¥ Ïˆ(yÌ‚) + hâˆ‡Ïˆ(yÌ‚), y âˆ’ yÌ‚i + Î½Dh (y, yÌ‚),
2
Î´
Ïˆ(y) â‰¥ Ïˆ(yÌ‚) + hâˆ‡Ïˆ(yÌ‚), y âˆ’ yÌ‚i + âˆ‡Ïˆ(y) âˆ’ âˆ‡Ïˆ(yÌ‚) .
2

For the second inequality, see, e.g., Theorem 2.1.5 in Nesterov (2004). Multiplying the two inequalities above by (1 âˆ’ Ï)
and Ï respectively and adding them together, we have
Ïˆ(y) â‰¥ Ïˆ(yÌ‚) + hâˆ‡Ïˆ(yÌ‚), y âˆ’ yÌ‚i + (1 âˆ’ Ï)Î½Dh (y, yÌ‚) +
The Bregman divergence Dh satisfies the following equality:


ÏÎ´ 
âˆ‡Ïˆ(y) âˆ’ âˆ‡Ïˆ(yÌ‚)2 .
2




Dh (y, yÌ„) = Dh (y, yÌ‚) + Dh (yÌ‚, yÌ„) + âˆ‡h(yÌ‚) âˆ’ âˆ‡h(yÌ„), y âˆ’ yÌ‚ .

We multiply this equality by Î· and add it to the last inequality to obtain
Ïˆ(y) + Î·Dh (y, yÌ„)

â‰¥

2

ÏÎ´ 
Ïˆ(yÌ‚) + Î·Dh (y, yÌ‚) + Î· + (1 âˆ’ Ï)Î½ Dh (yÌ‚, yÌ„) + âˆ‡Ïˆ(y) âˆ’ âˆ‡Ïˆ(yÌ‚)
2



+ âˆ‡Ïˆ(yÌ‚) + Î·(âˆ‡h(yÌ‚) âˆ’ âˆ‡h(yÌ„)), y âˆ’ yÌ‚ .

Using the optimality condition in (17), the last term of inner product is nonnegative and thus can be dropped, which gives
the desired inequality.

B. Common Analysis of Batch Primal-Dual Algorithms
We consider the general primal-dual update rule as:

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Iteration: (xÌ‚, yÌ‚) = PDÏ„,Ïƒ (xÌ„, yÌ„, xÌƒ, yÌƒ)
xÌ‚ =
yÌ‚

=



1
arg min g(x) + yÌƒ T Ax +
kx âˆ’ xÌ„k2 ,
2Ï„
xâˆˆRd


1
âˆ—
T
arg min f (y) âˆ’ y AxÌƒ + D(y, yÌ„) .
Ïƒ
yâˆˆRn

(18)
(19)

Each iteration of Algorithm 1 is equivalent to the following specification of PDÏ„,Ïƒ :
xÌ‚ = x(t+1) ,
yÌ‚ = y (t+1) ,

xÌ„ = x(t) ,
yÌ„ = y (t) ,

xÌƒ = x(t) + Î¸(x(t) âˆ’ x(tâˆ’1) ),
yÌƒ = y (t+1) .

(20)

Besides Assumption 2, we also assume that f âˆ— is Î½-strongly convex with respect to a kernel function h, i.e.,
f âˆ— (y â€² ) âˆ’ f âˆ— (y) âˆ’ hâˆ‡f âˆ— (y), y â€² âˆ’ yi â‰¥ Î½Dh (y â€² , y),
where Dh is the Bregman divergence defined as
Dh (y â€² , y) = h(y â€² ) âˆ’ h(y) âˆ’ hâˆ‡h(y), y â€² âˆ’ yi.
We assume that h is Î³ â€² -strongly convex and 1/Î´ â€² -smooth. Depending on the kernel function h, this assumption on f âˆ— may
impose additional restrictions on f . In this paper, we are mostly interested in two special cases: h(y) = (1/2)kyk2 and
h(y) = f âˆ— (y) (for the latter we always have Î½ = 1). From now on, we will omit the subscript h and use D denote the
Bregman divergence.
Under the above assumptions, any solution (xâ‹† , y â‹† ) to the saddle-point problem (6) satisfies the optimality condition:
âˆ’AT y â‹†
Axâ‹†

âˆˆ
=

âˆ‚g(xâ‹† ),
âˆ‡f âˆ— (y â‹† ).

(21)
(22)

The optimality conditions for the updates described in equations (18) and (19) are
1
(xÌ„ âˆ’ xÌ‚)
Ï„

1
AxÌƒ âˆ’
âˆ‡h(yÌ‚) âˆ’ âˆ‡h(yÌ„)
Ïƒ
âˆ’AT yÌƒ +

âˆˆ

âˆ‚g(xÌ‚),

(23)

=

âˆ‡f âˆ— (yÌ‚).

(24)

Applying Lemma 2 to the dual minimization step in (19) with Ïˆ(y) = f âˆ— (y) âˆ’ y T AxÌƒ, Î· = 1/Ïƒ, y = y â‹† and Ï = 1/2, we
obtain
f âˆ— (y â‹† ) âˆ’ y â‹† T AxÌƒ +

1
D(y â‹† , yÌ„)
Ïƒ

â‰¥

1
f âˆ— (yÌ‚) âˆ’ yÌ‚ T AxÌƒ + D(yÌ‚, yÌ„)
Ïƒ
1
2
Î´
Î½
+
D(y â‹† , yÌ‚) + âˆ‡f âˆ— (y â‹† ) âˆ’ âˆ‡f âˆ— (yÌ‚) .
+
Ïƒ
2
4

(25)

Similarly, for the primal minimization step in (18), we have (setting Ï = 0)
g(xâ‹† ) + yÌƒ T Axâ‹† +


1
11
1 â‹†
kx âˆ’ xÌ„k2 â‰¥ g(xÌ‚) + yÌƒ T AxÌ‚ +
kxÌ‚ âˆ’ xÌ„k2 +
+ Î» kxâ‹† âˆ’ xÌ‚k2 .
2Ï„
2Ï„
2 Ï„

Combining the two inequalities above with the definition L(x, y) = g(x) + y T Ax âˆ’ f âˆ— (y), we get
L(xÌ‚, y â‹† ) âˆ’ L(xâ‹† , yÌ‚)

=
â‰¤

g(xÌ‚) + y â‹† T AxÌ‚ âˆ’ f âˆ— (y â‹† ) âˆ’ g(xâ‹† ) âˆ’ yÌ‚ T Axâ‹† + f âˆ— (yÌ‚)

1
1 â‹†
1
11
Î½
D(y â‹† , yÌ‚)
kx âˆ’ xÌ„k2 + D(y â‹† , yÌ„) âˆ’
+ Î» kxâ‹† âˆ’ xÌ‚k2 âˆ’
+
2Ï„
Ïƒ
2 Ï„
Ïƒ
2
2
1
Î´
1
âˆ’ kxÌ‚ âˆ’ xÌ„k2 âˆ’ D(yÌ‚, yÌ„) âˆ’ âˆ‡f âˆ— (y â‹† ) âˆ’ âˆ‡f âˆ— (yÌ‚)
2Ï„
Ïƒ
4
+ y â‹† T AxÌ‚ âˆ’ yÌ‚ T Axâ‹† + yÌƒ T Axâ‹† âˆ’ yÌƒ T AxÌ‚ âˆ’ y â‹† T AxÌƒ + yÌ‚ T AxÌƒ.

(26)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

We can simplify the inner product terms as
y â‹† T AxÌ‚ âˆ’ yÌ‚ T Axâ‹† + yÌƒ T Axâ‹† âˆ’ yÌƒ T AxÌ‚ âˆ’ y â‹† T AxÌƒ + yÌ‚ T AxÌƒ = (yÌ‚ âˆ’ yÌƒ)T A(xÌ‚ âˆ’ xâ‹† ) âˆ’ (yÌ‚ âˆ’ y â‹† )T A(xÌ‚ âˆ’ xÌƒ).
Rearranging terms on the two sides of the inequality, we have
1
1 â‹†
kx âˆ’ xÌ„k2 + D(y â‹† , yÌ„)
2Ï„
Ïƒ

â‰¥

L(xÌ‚, y â‹† ) âˆ’ L(xâ‹† , yÌ‚)

1
11
Î½
+
D(y â‹† , yÌ‚)
+ Î» kxâ‹† âˆ’ xÌ‚k2 +
+
2 Ï„
Ïƒ
2
2
1
Î´
1
+ kxÌ‚ âˆ’ xÌ„k2 + D(yÌ‚, yÌ„) + âˆ‡f âˆ— (y â‹† ) âˆ’ âˆ‡f âˆ— (yÌ‚)
2Ï„
Ïƒ
4
+ (yÌ‚ âˆ’ y â‹† )T A(xÌ‚ âˆ’ xÌƒ) âˆ’ (yÌ‚ âˆ’ yÌƒ)T A(xÌ‚ âˆ’ xâ‹† ).

Applying the substitutions in (20) yields
1
1 â‹†
kx âˆ’ x(t) k2 + D(y â‹† , y (t) )
2Ï„
Ïƒ

â‰¥

L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )

1
11
Î½
+
D(y â‹† , y (t+1) )
+ Î» kxâ‹† âˆ’ x(t+1) k2 +
+
2 Ï„
Ïƒ
2
2
1
Î´
1
+ kx(t+1) âˆ’ x(t) k2 + D(y (t+1) , y (t) ) + âˆ‡f âˆ— (y â‹† ) âˆ’ âˆ‡f âˆ— (y (t+1) )
2Ï„
Ïƒ
4

+ (y (t+1) âˆ’ y â‹† )T A x(t+1) âˆ’ (x(t) + Î¸(x(t) âˆ’ x(tâˆ’1) ) .
(27)

We can rearrange the inner product term in (27) as

=

(y (t+1) âˆ’ y â‹† )T A x(t+1) âˆ’ (x(t) + Î¸(x(t) âˆ’ x(tâˆ’1) )



(y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) âˆ’ Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) ) âˆ’ Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) ).

Using the optimality conditions in (22) and (24), we can also bound kâˆ‡f âˆ— (y â‹† ) âˆ’ âˆ‡f âˆ— (y (t+1) )k2 :

=
â‰¥

 âˆ— â‹†

âˆ‡f (y ) âˆ’ âˆ‡f âˆ— (y (t+1) )2


 1
 â‹†
2
âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) ) 
Ax âˆ’ A x(t) + Î¸(x(t) âˆ’ x(tâˆ’1) ) +
Ïƒ



 2

1 
2

A(xâ‹† âˆ’ x(t) ) âˆ’ (Î± âˆ’ 1)Î¸A(x(t) âˆ’ x(tâˆ’1) ) âˆ’ 1 âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) ) 
1âˆ’
 ,
Î±
Ïƒ

where Î± > 1. With the definition Âµ =
with the inequality (27) leads to

p

Î»min (AT A), we also have kA(xâ‹† âˆ’ x(t) )k2 â‰¥ Âµ2 kxâ‹† âˆ’ x(t) k2 . Combining them

1 â‹†
1
kx âˆ’ x(t) k2 + D(y â‹† , y (t) ) + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
2Ï„
Ïƒ
â‰¥ L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )

1
11
Î½
+
+ Î» kxâ‹† âˆ’ x(t+1) k2 +
+
D(y â‹† , y (t+1) ) + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) )
2 Ï„
Ïƒ
2
1
1
+ kx(t+1) âˆ’ x(t) k2 + D(y (t+1) , y (t) ) âˆ’ Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) )
2Ï„
Ïƒ

 1

Î´
1  Î´Âµ2 â‹†

2
kx âˆ’ x(t) k2 âˆ’ (Î± âˆ’ 1) Î¸A(x(t) âˆ’ x(tâˆ’1) ) âˆ’
+ 1âˆ’
âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) )  .
Î± 4
4
Ïƒ

(28)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

C. Proof of Theorem 1
Let the kernel function be h(y) = (1/2)kyk2 . In this case, we have D(y â€² , y) = (1/2)ky â€² âˆ’ yk2 and âˆ‡h(y) = y. Moreover,
Î³ â€² = Î´ â€² = 1 and Î½ = Î³. Therefore, the inequality (28) becomes


1 1 
1 â‹†
1  Î´Âµ2
kxâ‹† âˆ’ x(t) k2 +
âˆ’ 1âˆ’
ky âˆ’ y (t) k2 + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
2 Ï„
Î± 2
2Ïƒ
â‰¥

L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )

11
1 1
Î³ â‹†
+
ky âˆ’ y (t+1) k2 + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) )
+ Î» kxâ‹† âˆ’ x(t+1) k2 +
+
2 Ï„
2 Ïƒ
2
1 (t+1)
1
ky
âˆ’ y (t) k2 âˆ’ Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) )
+ kx(t+1) âˆ’ x(t) k2 +
2Ï„
2Ïƒ
2
 1
Î´


âˆ’ (Î± âˆ’ 1) Î¸A(x(t) âˆ’ x(tâˆ’1) ) âˆ’ (y (t+1) âˆ’ y (t) ) .
4
Ïƒ

(29)

Next we derive another form of the underlined items above:

=
=
â‰¥

1 (t+1)
ky
âˆ’ y (t) k2 âˆ’ Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) )
2Ïƒ

Ïƒ 1 (t+1)
Î¸ (t+1)
(t) 2
(t) T
(t)
(tâˆ’1)
ky
âˆ’ y k âˆ’ (y
âˆ’ y ) A(x âˆ’ x
)
2 Ïƒ2
Ïƒ


2
Ïƒ 
1 (t+1)
(t) 
(t)
(tâˆ’1)
2
(t)
(tâˆ’1) 2
âˆ’ y ) âˆ’ Î¸ kA(x âˆ’ x
) âˆ’ (y
)k
Î¸A(x âˆ’ x
2
Ïƒ
2 ÏƒÎ¸2 L2
1
Ïƒ


kx(t) âˆ’ x(tâˆ’1) k2 ,
Î¸A(x(t) âˆ’ x(tâˆ’1) ) âˆ’ (y (t+1) âˆ’ y (t) ) âˆ’
2
Ïƒ
2

where in the last inequality we used kAk â‰¤ L and hence kA(x(t) âˆ’ x(tâˆ’1) )k2 â‰¤ L2 kx(t) âˆ’ x(tâˆ’1) k2 . Combining with
inequality (29), we have


1 1 
1 (t)
1  Î´Âµ2
ÏƒÎ¸2 L2 (t)
kx(t) âˆ’ xâ‹† k2 +
âˆ’ 1âˆ’
ky âˆ’ y â‹† k2 + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) ) +
kx âˆ’ x(tâˆ’1) k2
2 Ï„
Î± 2
2Ïƒ
2
â‰¥

L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )




1 1
1 (t+1)
1 1
Î³
+
ky (t+1) âˆ’ y â‹† k2 + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) +
+ Î» kx(t+1) âˆ’ xâ‹† k2 +
+
kx
âˆ’ x(t) k2
2 Ï„
2 Ïƒ
2
2Ï„


2
Î´ 
Ïƒ
1


âˆ’ (Î± âˆ’ 1)
+
(30)
Î¸A(x(t) âˆ’ x(tâˆ’1) ) âˆ’ (y (t+1) âˆ’ y (t) ) .
2
4
Ïƒ

We can remove the last term in the above inequality as long as its coefficient is nonnegative, i.e.,
Î´
Ïƒ
âˆ’ (Î± âˆ’ 1) â‰¥ 0.
2
4

In order to maximize 1 âˆ’ 1/Î±, we take the equality and solve for the largest value of Î± allowed, which results in
Î±=1+

2Ïƒ
,
Î´

1âˆ’

1
2Ïƒ
=
.
Î±
2Ïƒ + Î´

Applying these values in (30) gives


1 1
1 (t)
ÏƒÎ´Âµ2
ÏƒÎ¸2 L2 (t)
kx(t) âˆ’ xâ‹† k2 +
âˆ’
ky âˆ’ y â‹† k2 + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) ) +
kx âˆ’ x(tâˆ’1) k2
2 Ï„
2Ïƒ + Î´
2Ïƒ
2
â‰¥

L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )




1 (t+1)
1 1
Î³
1 1
ky (t+1) âˆ’ y â‹† k2 + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) +
+ Î» kx(t+1) âˆ’ xâ‹† k2 +
+
kx
âˆ’ x(t) k2 .
+
2 Ï„
2 Ïƒ
2
2Ï„
(31)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

We use âˆ†(t+1) to denote the last row in (31). Equivalently, we define
âˆ†(t)

=
=


11
+ Î» kxâ‹† âˆ’ x(t) k2 +
2 Ï„

11
+ Î» kxâ‹† âˆ’ x(t) k2 +
2 Ï„

1 (t)
1 1
Î³ â‹†
ky âˆ’ y (t) k2 + (y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) ) +
+
kx âˆ’ x(tâˆ’1) k2
2 Ïƒ
2
2Ï„

  (t)
T  1

Î³ â‹†
1 x(t) âˆ’ x(tâˆ’1)
x âˆ’ x(tâˆ’1)
I âˆ’AT
(t) 2
Ï„
.
ky âˆ’ y k +
1
âˆ’A
y â‹† âˆ’ y (t)
y â‹† âˆ’ y (t)
4
2
Ïƒ

The quadratic form in the last term is nonnegative if the matrix
M=



1
Ï„I

âˆ’A

âˆ’AT
1
Ïƒ



is positive semidefinite, for which a sufficient condition is Ï„ Ïƒ â‰¤ 1/L2 . Under this condition,
âˆ†(t) â‰¥
If we can to choose Ï„ and Ïƒ so that


Î³
11
+ Î» kxâ‹† âˆ’ x(t) k2 + ky â‹† âˆ’ y (t) k2 â‰¥ 0.
2 Ï„
4
1
Î³
1
,
â‰¤Î¸
+
Ïƒ
Ïƒ
2

1

1
ÏƒÎ´Âµ2
âˆ’
â‰¤Î¸
+Î» ,
Ï„
2Ïƒ + Î´
Ï„

then, according to (31), we have

ÏƒÎ¸2 L2
1
â‰¤Î¸ ,
2
2Ï„

(32)

(33)

âˆ†(t+1) + L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ) â‰¤ Î¸âˆ†(t) .
Because âˆ†(t) â‰¥ 0 and L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y (t) ) â‰¥ 0 for any t â‰¥ 0, we have
âˆ†(t+1) â‰¤ Î¸âˆ†(t) ,
which implies
âˆ†(t) â‰¤ Î¸t âˆ†(0)
and
L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y (t) ) â‰¤ Î¸t âˆ†(0) .
Let Î¸x and Î¸y be two contraction factors determined by the first two inequalities in (33), i.e.,
Î¸x

=

Î¸y

=





1
1
1
ÏƒÎ´Âµ2
Ï„ ÏƒÎ´Âµ2
âˆ’
+Î» = 1âˆ’
,
Ï„
2Ïƒ + Î´
Ï„
2Ïƒ + Î´ 1 + Ï„ Î»

1
1
1
Î³
=
+
.
Ïƒ
Ïƒ
2
1 + ÏƒÎ³/2



Then we can let Î¸ = max{Î¸x , Î¸y }. We note that any Î¸ < 1 would satisfy the last condition in (33) provided that
Ï„Ïƒ =

1
,
L2

which also makes the matrix M positive semidefinite and thus ensures the inequality (32).
Among all possible pairs Ï„, Ïƒ that satisfy Ï„ Ïƒ = 1/L2 , we choose
1
Ï„=
L
which give the desired results of Theorem 1.

r

Î³
,
Î» + Î´Âµ2

1
Ïƒ=
L

s

Î» + Î´Âµ2
,
Î³

(34)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

D. Proof of Theorem 3
If we choose h = f âˆ— , then
â€¢ h is Î³-strongly convex and 1/Î´-smooth, i.e., Î³ â€² = Î³ and Î´ â€² = Î´;
â€¢ f âˆ— is 1-strongly convex with respect to h, i.e., Î½ = 1.
For convenience, we repeat inequality (28) here:
1 â‹†
1
kx âˆ’ x(t) k2 + D(y â‹† , y (t) ) + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
2Ï„
Ïƒ
â‰¥ L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )

1
11
Î½
D(y â‹† , y (t+1) ) + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) )
+
+ Î» kxâ‹† âˆ’ x(t+1) k2 +
+
2 Ï„
Ïƒ
2
1
1
+ kx(t+1) âˆ’ x(t) k2 + D(y (t+1) , y (t) ) âˆ’ Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) )
2Ï„
Ïƒ


 1
Î´
1  Î´Âµ2 â‹†

2
kx âˆ’ x(t) k2 âˆ’ (Î± âˆ’ 1) Î¸A(x(t) âˆ’ x(tâˆ’1) ) âˆ’
âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) )  .
+ 1âˆ’
Î± 4
4
Ïƒ

(35)

We first bound the Bregman divergence D(y (t+1) , y (t) ) using the assumption that the kernel h is Î³-strongly convex and
1/Î´-smooth. Using similar arguments as in the proof of Lemma 2, we have for any Ï âˆˆ [0, 1],
D(y (t+1) , y (t) )

=
â‰¥

h(y (t+1) ) âˆ’ h(y (t) ) âˆ’ hâˆ‡h(y (t) ), y (t+1) âˆ’ y (t) i
2
Î³
Î´
(1 âˆ’ Ï) ky (t+1) âˆ’ y (t) k2 + Ï âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) ) .
2
2

(36)

For any Î² > 0, we can lower bound the inner product term

Î¸2 L2 (t)
Î²
kx âˆ’ x(tâˆ’1) k2 .
âˆ’Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) ) â‰¥ âˆ’ ky (t+1) âˆ’ y (t) k2 âˆ’
2
2Î²
In addition, we have

2

 1
2
2

âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) )  â‰¤ 2Î¸2 L2 kx(t) âˆ’ x(tâˆ’1) k2 + 2 âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) ) .
Î¸A(x(t) âˆ’ x(tâˆ’1) ) âˆ’
Ïƒ
Ïƒ

Combining these bounds with (35) and (36) with Ï = 1/2, we arrive at

â‰¥



1 1 
1
1  Î´Âµ2
kxâ‹† âˆ’ x(t) k2 + D(y â‹† , y (t) ) + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
âˆ’ 1âˆ’
2 Ï„
Î± 2
Ïƒ
 Î¸ 2 L2
Î´Î¸2 L2  (t)
kx âˆ’ x(tâˆ’1) k2
+ (Î± âˆ’ 1)
+
2Î²
2

L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )

1
11
1
+
D(y â‹† , y (t+1) ) + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) )
+ Î» kxâ‹† âˆ’ x(t+1) k2 +
+
2 Ï„
Ïƒ 2
Î³
 Î´

Î²  (t+1)
(Î± âˆ’ 1)Î´ 
âˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) )2
+
âˆ’
âˆ’
ky
âˆ’ y (t) k2 +
2
4Ïƒ
2
4Ïƒ
2Ïƒ
1 (t+1)
(t) 2
âˆ’x k .
+ kx
2Ï„

We choose Î± and Î² in (37) to zero out the coefficients of ky (t+1) âˆ’ y (t) k2 and kâˆ‡h(y (t+1) ) âˆ’ âˆ‡h(y (t) )k2 :
Î±=1+

Ïƒ
,
2

Î²=

Î³
.
2Ïƒ

(37)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Then the inequality (37) becomes


1
1 1
ÏƒÎ´Âµ2
kxâ‹† âˆ’ x(t) k2 + D(y â‹† , y (t) ) + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
âˆ’
2 Ï„
4 + 2Ïƒ
Ïƒ

 2 2
Î´ÏƒÎ¸2 L2
ÏƒÎ¸ L
kx(t) âˆ’ x(tâˆ’1) k2
+
+
Î³
4
â‰¥

L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )



1
1
11
â‹†
(t+1) 2
D(y â‹† , y (t+1) ) + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) )
+ Î» kx âˆ’ x
k +
+
+
2 Ï„
Ïƒ 2
1
+ kx(t+1) âˆ’ x(t) k2 .
2Ï„

The coefficient of kx(t) âˆ’ x(tâˆ’1) k2 can be bounded as

1
ÏƒÎ¸2 L2
4 + Î³Î´ 2 2
Î´ÏƒÎ¸2 L2
Î´ 2 2
2ÏƒÎ¸2 L2
ÏƒÎ¸ L =
+
=
+
ÏƒÎ¸ L <
,
Î³
4
Î³
4
4Î³
Î³

where in the inequality we used Î³Î´ â‰¤ 1. Therefore we have


1
ÏƒÎ´Âµ2
2ÏƒÎ¸2 L2 (t)
1 1
kxâ‹† âˆ’ x(t) k2 + D(y â‹† , y (t) ) + Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) ) +
âˆ’
kx âˆ’ x(tâˆ’1) k2
2 Ï„
4 + 2Ïƒ
Ïƒ
Î³
â‰¥

L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )



11
1 (t+1)
1
1
â‹†
(t+1) 2
+
D(y â‹† , y (t+1) ) + (y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) +
+ Î» kx âˆ’ x
k +
+
kx
âˆ’ x(t) k2 .
2 Ï„
Ïƒ 2
2Ï„

We use âˆ†(t+1) to denote the last row of the above inequality. Equivalently, we define



11
1 (t)
1
1
(t)
â‹†
(t) 2
âˆ† =
D(y â‹† , y (t) ) + (y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) ) +
+ Î» kx âˆ’ x k +
+
kx âˆ’ x(tâˆ’1) k2 .
2 Ï„
Ïƒ 2
2Ï„
Since h is Î³-strongly convex, we have D(y â‹† , y (t) ) â‰¥ Î³2 ky â‹† âˆ’ y (t) k2 , and thus
âˆ†(t)

â‰¥
=


11
+ Î» kxâ‹† âˆ’ x(t) k2 +
2 Ï„

11
+ Î» kxâ‹† âˆ’ x(t) k2 +
2 Ï„

1
D(y â‹† , y (t) ) +
2
1
D(y â‹† , y (t) ) +
2

Î³ (t)
1 (t)
ky âˆ’ y â‹† k2 + (y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) ) +
kx âˆ’ x(tâˆ’1) k2
2Ïƒ
2Ï„

T  1
  (t)

1 x(t) âˆ’ x(tâˆ’1)
x âˆ’ x(tâˆ’1)
âˆ’AT
Ï„I
.
Î³
âˆ’A
y â‹† âˆ’ y (t)
y â‹† âˆ’ y (t)
2
Ïƒ

The quadratic form in the last term is nonnegative if Ï„ Ïƒ â‰¤ Î³/L2 . Under this condition,
âˆ†(t) â‰¥
If we can to choose Ï„ and Ïƒ so that

then we have


11
1
+ Î» kxâ‹† âˆ’ x(t) k2 + D(y â‹† , y (t) ) â‰¥ 0.
2 Ï„
2

1

1
ÏƒÎ´Âµ2
âˆ’
â‰¤Î¸
+Î» ,
Ï„
4 + 2Ïƒ
Ï„

1
1
1
â‰¤Î¸
+
,
Ïƒ
Ïƒ 2

âˆ†(t+1) + L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ) â‰¤ Î¸âˆ†(t) .

Because âˆ†(t) â‰¥ 0 and L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y (t) ) â‰¥ 0 for any t â‰¥ 0, we have
âˆ†(t+1) â‰¤ Î¸âˆ†(t) ,
which implies

1
2ÏƒÎ¸2 L2
â‰¤Î¸ ,
Î³
2Ï„

âˆ†(t) â‰¤ Î¸t âˆ†(0)

(38)

(39)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

and
L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y (t) ) â‰¤ Î¸t âˆ†(0) .
To satisfy the last condition in (39) and also ensure the inequality (38), it suffices to have
Ï„Ïƒ â‰¤
We choose
Ï„=

1
2L

r

Î³
,
Î» + Î´Âµ2

Î³
.
4L2

Ïƒ=

With the above choice and assuming Î³(Î» + Î´Âµ2 ) â‰ª L2 , we have
1
Ïƒ

Î¸y =

1
Ïƒ

+

1
2

1 p
Î³(Î» + Î´Âµ2 ).
2L

1
1
p
â‰ˆ1âˆ’
=
=
1 + Ïƒ/2
1 + Î³(Î» + Î´Âµ2 )/(4L)

p

Î³(Î» + Î´Âµ2 )
.
4L

For the contraction factor over the primal variables, we have
Î¸x =

1
Ï„

âˆ’

1
Ï„

ÏƒÎ´Âµ2
4+2Ïƒ

+Î»

=

1âˆ’

Ï„ ÏƒÎ´Âµ2
4+2Ïƒ

1 + Ï„Î»

=

1âˆ’

Î³Î´Âµ2
4(4+2Ïƒ)L2

Î»
Î³Î´Âµ2
âˆ’
â‰ˆ1âˆ’
16L2
2L

1 + Ï„Î»

r

Î³
.
Î» + Î´Âµ2

This finishes the proof of Theorem 3.

E. Proof of Theorem 2
We consider the SPDC algorithm in the Euclidean case with h(x) = (1/2)kxk2 . The corresponding batch case analysis is
given in Section C. For each i = 1, . . . , n, let yÌƒi be
)
(
(t)
(y âˆ’ yi )2
(t)
âˆ—
âˆ’ yhai , xÌƒ i .
yÌƒi = arg min Ï†i (y) +
y
2Ïƒ
Based on the first-order optimality condition, we have
(t)

hai , xÌƒ(t) i âˆ’

â€²
(yÌƒi âˆ’ yi )
âˆˆ Ï†âˆ—i (yÌƒi ).
Ïƒ

Also, since yiâ‹† minimizes Ï†âˆ—i (y) âˆ’ yhai , xâ‹† i, we have
â€²

hai , xâ‹† i âˆˆ Ï†âˆ—i (yiâ‹† ).
By Lemma 2 with Ï = 1/2, we have
(t)

âˆ’yiâ‹† hai , xÌƒ(t) i

+

Ï†âˆ—i (yiâ‹† )

(y âˆ’ yiâ‹† )2
â‰¥
+ i
2Ïƒ


+

Î³
1
+
Ïƒ
2



(yÌƒi âˆ’ yiâ‹† )2
+ Ï†âˆ—i (yÌƒi ) âˆ’ yÌƒi hai , xÌƒ(t) i
2

(t)

â€²
â€²
(yÌƒi âˆ’ yi )2
Î´
+ (Ï†âˆ—i (yÌƒi ) âˆ’ Ï†âˆ—i (yiâ‹† ))2 ,
2Ïƒ
4

and re-arranging terms, we get
(t)

(yi âˆ’ yiâ‹† )2
â‰¥
2Ïƒ


(t)
Î³ (yÌƒi âˆ’ yiâ‹† )2
(yÌƒi âˆ’ yi )2
1
+
+
âˆ’ (yÌƒi âˆ’ yiâ‹† )hai , xÌƒ(t) i + (Ï†âˆ—i (yÌƒi ) âˆ’ Ï†âˆ—i (yiâ‹† ))
Ïƒ
2
2
2Ïƒ
â€²
â€²
Î´
+ (Ï†âˆ—i (yÌƒi ) âˆ’ Ï†âˆ—i (yiâ‹† ))2 .
4



(40)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Notice that
n âˆ’ 1 (t)
1
Â· yÌƒi +
Â· yi ,
n
n
(t)
(yÌƒi âˆ’ yiâ‹† )2
(n âˆ’ 1)(yi âˆ’ yiâ‹† )2
(t+1)
E[(yi
âˆ’ yiâ‹† )2 ] =
+
,
n
n
(t)
(yÌƒi âˆ’ yi )2
(t+1)
(t)
,
E[(yi
âˆ’ yi )2 ] =
n
1
n âˆ’ 1 âˆ— (t)
(t+1)
E[Ï†âˆ—i (yi
)] = Â· Ï†âˆ—i (yÌƒi ) +
Â· Ï†i (yi ).
n
n
(t+1)

E[yi

]=

Plug the above relations into (40) and divide both sides by n, we have




1
1
1
(n âˆ’ 1)Î³
Î³
(t)
(t+1)
(t+1)
(t)
â‹† 2
(yi âˆ’ yi ) â‰¥
E[(yi
âˆ’ yiâ‹† )2 ] +
+
+
E[(yi
âˆ’ yi )2 ]
2Ïƒ
4n
2Ïƒ
4
2Ïƒ


1 (t)
(t+1)
(t)
â‹†
âˆ’ E[(yi
âˆ’ yi )] + (yi âˆ’ yi ) hai , xÌƒ(t) i
n
1
(t+1)
(t)
(t)
+ E[Ï†âˆ—i (yi
)] âˆ’ Ï†âˆ—i (yi ) + (Ï†âˆ—i (yi ) âˆ’ Ï†âˆ—i (yiâ‹† ))
n
!2
(t)
(yÌƒ
âˆ’
y
)
Î´
i
i
hai , xÌƒ(t) âˆ’ xâ‹† i âˆ’
,
+
4n
Ïƒ
and summing over i = 1, . . . , n, we get




E[ky (t+1) âˆ’ y (t) k2 ]
1
(n âˆ’ 1)Î³
Î³
1
(t)
â‹† 2
ky âˆ’ y k â‰¥
E[ky (t+1) âˆ’ y â‹† k2 ] +
+
+
2Ïƒ
4n
2Ïƒ
4
2Ïƒ
n
1 X âˆ— (t)
(t+1)
(t)
(Ï† (y ) âˆ’ Ï†âˆ—i (yiâ‹† ))
+ Ï†âˆ—k (yk
) âˆ’ Ï†âˆ—k (yk ) +
n i=1 i i
E
D
âˆ’ n(u(t+1) âˆ’ u(t) ) + (u(t) âˆ’ uâˆ— ), xÌƒ(t)


(t) 2
Î´ 
A(xâˆ— âˆ’ xÌƒ(t) ) + (yÌƒ âˆ’ y )  ,
+

4n 
Ïƒ

where

n

u(t) =

1 X (t)
y ai ,
n i=1 i

On the other hand, since x(t+1) minimizes the

n

u(t+1) =
1
Ï„

1 X (t+1)
y
ai ,
n i=1 i

n

and

uâ‹† =

1X â‹†
y ai .
n i=1 i

+ Î»-strongly convex objective

D
E kx âˆ’ x(t) k2
,
g(x) + u(t) + n(u(t+1) âˆ’ u(t) ), x +
2Ï„

we can apply Lemma 2 with Ï = 0 to obtain

kx(t) âˆ’ xâ‹† k2
2Ï„


kx(t+1) âˆ’ x(t) k2
Î»
1
(t+1)
(t)
(t+1)
(t)
(t+1)
â‰¥ g(x
) + hu + n(u
âˆ’ u ), x
i+
kx(t+1) âˆ’ xâ‹† k2 ,
+
+
2Ï„
2Ï„
2

g(xâ‹† ) + hu(t) + n(u(t+1) âˆ’ u(t) ), xâ‹† i +

and re-arranging terms we get


E[kx(t+1) âˆ’ x(t) k2 ]
Î»
1
kx(t) âˆ’ xâ‹† k2
E[kx(t+1) âˆ’ xâ‹† k2 ] +
â‰¥
+
+ E[g(x(t+1) ) âˆ’ g(xâ‹† )]
2Ï„
2Ï„
2
2Ï„
+ E[hu(t) + n(u(t+1) âˆ’ u(t) ), x(t+1) âˆ’ xâ‹† i].

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Also notice that
L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )) âˆ’ (n âˆ’ 1)(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t) ))
n
1 X âˆ— (t)
(t+1)
(t)
(Ï† (y ) âˆ’ Ï†âˆ—i (y â‹† )) + (Ï†âˆ—k (yk
) âˆ’ Ï†âˆ—k (yk )) + g(x(t+1) ) âˆ’ g(xâ‹† )
=
n i=1 i i
+ huâ‹† , x(t+1) i âˆ’ hu(t) , xâ‹† i + nhu(t) âˆ’ u(t+1) , xâ‹† i.

Combining everything together, we have


kx(t) âˆ’ xâ‹† k2
(n âˆ’ 1)Î³
1
ky (t) âˆ’ y â‹† k2 + (n âˆ’ 1)(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t) ))
+
+
2Ï„
2Ïƒ
4n




E[kx(t+1) âˆ’ x(t) k2 ] E[ky (t+1) âˆ’ y (t) k2 ]
1
1
Î»
Î³
â‰¥
E[kx(t+1) âˆ’ xâ‹† k2 ] +
E[ky (t+1) âˆ’ y â‹† k2 ] +
+
+
+
2Ï„
2
2Ïƒ
4
2Ï„
2Ïƒ
+ E[L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ))]

2
(yÌƒ âˆ’ y (t) ) 
Î´ 
â‹†
(t)
(t)
â‹†
(t+1)
(t)
(t+1)
t

 .
A(x âˆ’ xÌƒ ) +
+ E[hu âˆ’ u + n(u
âˆ’ u ), x
âˆ’ xÌ„ i] +

4n 
Ïƒ

Next we notice that

2


(t+1)
(t) 2

Î´ 
] âˆ’ y (t) ) 
A(xâ‹† âˆ’ xÌƒ(t) ) + n(E[y
 = Î´ A(xâ‹† âˆ’ x(t) ) âˆ’ Î¸A(x(t) âˆ’ x(tâˆ’1) ) + (yÌƒ âˆ’ y ) 


4n 
Ïƒ
4n 
Ïƒ




1
Î´ 
2
â‰¥ 1âˆ’
A(xâ‹† âˆ’ x(t) )
Î± 4n

2
(yÌƒ âˆ’ y (t) ) 
Î´ 
(t)
(tâˆ’1)

 ,
Î¸A(x âˆ’ x
)+
âˆ’ (Î± âˆ’ 1)

4n 
Ïƒ
for some Î± > 1 and
and


2


A(xâ‹† âˆ’ x(t) ) â‰¥ Âµ2 kxâ‹† âˆ’ x(t) k2 ,



(t) 2

Î¸A(x(t) âˆ’ x(tâˆ’1) ) + (yÌƒ âˆ’ y )  â‰¥ âˆ’ 2Î¸2 kA(x(t) âˆ’ x(tâˆ’1) )k2 âˆ’ 2 kyÌƒ âˆ’ y (t) k2


Ïƒ
Ïƒ2
2n
â‰¥ âˆ’ 2Î¸2 L2 kx(t) âˆ’ x(tâˆ’1) k2 âˆ’ 2 E[ky (t+1) âˆ’ y (t) k2 ].
Ïƒ
We follow the same reasoning as in the standard SPDC analysis,
hu(t) âˆ’ uâ‹† + n(u(t+1) âˆ’ u(t) ), x(t+1) âˆ’ xÌƒ(t) i =

(y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
âˆ’
n
n

(n âˆ’ 1) (t+1)
(y
âˆ’ y (t) )T A(x(t+1) âˆ’ x(t) ) âˆ’ Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) ),
n
and using Cauchy-Schwartz inequality, we have
+

|(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) )| â‰¤
â‰¤

kx(t) âˆ’ x(tâˆ’1) k2
k(y (t+1) âˆ’ y (t) )T Ak2
+
1/(2Ï„ )
8Ï„
ky (t+1) âˆ’ y (t) k2
,
1/(2Ï„ R2 )

and
|(y (t+1) âˆ’ y (t) )T A(x(t+1) âˆ’ x(t) )| â‰¤
â‰¤

k(y (t+1) âˆ’ y (t) )T Ak2
kx(t+1) âˆ’ x(t) k2
+
1/(2Ï„ )
8Ï„
ky (t+1) âˆ’ y (t) k2
.
1/(2Ï„ R2 )

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Thus we get
(y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
âˆ’
n
n
ky (t+1) âˆ’ y (t) k2
kx(t+1) âˆ’ x(t) k2
Î¸kx(t) âˆ’ x(tâˆ’1) k2
âˆ’
âˆ’
âˆ’
.
1/(4Ï„ R2 )
8Ï„
8Ï„

hu(t) âˆ’ uâ‹† + n(u(t+1) âˆ’ u(t) ), x(t+1) âˆ’ xÌƒ(t) i â‰¥

Putting everything together, we have




1
(1 âˆ’ 1/Î±)Î´Âµ2
(n âˆ’ 1)Î³
1
(t)
â‹† 2
kx âˆ’ x k +
ky (t) âˆ’ y â‹† k2 + Î¸(L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ))
âˆ’
+
2Ï„
4n
2Ïƒ
4n


Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
1
(Î± âˆ’ 1)Î¸Î´L2
+ (n âˆ’ 1)(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t) )) + Î¸
kx(t) âˆ’ xtâˆ’1 k2 +
+
8Ï„
2n
n




(t+1)
â‹† T
(t+1)
E[(y
âˆ’ y ) A(x
âˆ’ x(t) )]
1
1
Î»
Î³
E[kx(t+1) âˆ’ xâ‹† k2 ] +
E[ky (t+1) âˆ’ y â‹† k2 ] +
â‰¥
+
+
2Ï„
2
2Ïƒ
4
n
+ E[L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ))]


1
1
+
E[kx(t+1) âˆ’ x(t) k2 ]
âˆ’
2Ï„
8Ï„


1
(Î± âˆ’ 1)Î´
2
+
E[ky (t+1) âˆ’ y (t) k2 ].
âˆ’ 4R Ï„ âˆ’
2Ïƒ
2Ïƒ 2

If we choose the parameters as
Î±=
then we know
and

Ïƒ
+ 1,
4Î´

ÏƒÏ„ =

1
,
16R2

1
1
(Î± âˆ’ 1)Î´
1
1
=
âˆ’ 4R2 Ï„ âˆ’
âˆ’
âˆ’
> 0,
2Ïƒ
2Ïƒ 2
2Ïƒ 4Ïƒ 8Ïƒ
ÏƒL2
ÏƒR2
1
(Î± âˆ’ 1)Î¸Î´L2
â‰¤
â‰¤
â‰¤
,
2
2n
8n
8
256Ï„

thus

1
(Î± âˆ’ 1)Î¸Î´L2
3
+
â‰¤
.
8Ï„
2n
8Ï„

In addition, we have
1âˆ’

1
Ïƒ
=
.
Î±
Ïƒ + 4Î´

Finally we obtain




1
ÏƒÎ´Âµ2
(n âˆ’ 1)Î³
1
(t)
â‹† 2
kx âˆ’ x k +
ky (t) âˆ’ y â‹† k2 + Î¸(L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ))
âˆ’
+
2Ï„
4n(Ïƒ + 4Î´)
2Ïƒ
4n

3 (t)
Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
+ (n âˆ’ 1)(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t) )) + Î¸ Â·
kx âˆ’ x(tâˆ’1) k2 +
8Ï„
n




1
E[(y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) )]
1
Î»
Î³
(t+1)
â‹† 2
(t+1)
â‹† 2
â‰¥
E[kx
âˆ’x k ]+
E[ky
âˆ’y k ]+
+
+
2Ï„
2
2Ïƒ
4
n
3
E[kx(t+1) âˆ’ x(t) k2 ].
+ E[L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ))] +
8Ï„

Now we can define Î¸x and Î¸y as the ratios between the coefficients in the x-distance and y-distance terms, and let Î¸ =
max{Î¸x , Î¸y } as before. Choosing the step-size parameters as
s
r
1
1
Î³
nÎ» + Î´Âµ2
Ï„=
,
Ïƒ
=
4R nÎ» + Î´Âµ2
4R
Î³
gives the desired result.

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

F. Proof of Theorem 4
In this setting, for i-th coordinate of the dual variables y we choose h = Ï†âˆ—i , let
Di (yi , yiâ€² ) = Ï†âˆ—i (yi ) âˆ’ Ï†âˆ—i (yiâ€² ) + h(Ï†âˆ—i )â€² (yiâ€² ), yi âˆ’ yiâ€² i,
and define
â€²

D(y, y ) =
For i = 1, . . . , n, let yÌƒi be
yÌƒi = arg min
y

(

Ï†âˆ—i (y)

n
X
i=1

Di (yi , yiâ€² ).

)
(t)
Di (y, yi )
(t)
âˆ’ yhai , xÌƒ i .
+
Ïƒ

Based on the first-order optimality condition, we have
(t)

hai , xÌƒ(t) i âˆ’

(Ï†âˆ—i )â€² (yÌƒi ) âˆ’ (Ï†âˆ—i )â€² (yi )
âˆˆ (Ï†âˆ—i )â€² (yÌƒi ).
Ïƒ

Also since yiâ‹† minimizes Ï†âˆ—i (y) âˆ’ yhai , xâˆ— i, we have
hai , xâˆ— i âˆˆ (Ï†âˆ—i )â€² (yiâ‹† ).
Using Lemma 2 with Ï = 1/2, we obtain
(t)

âˆ’yiâ‹† hai , xÌƒ(t) i + Ï†âˆ—i (yiâ‹† ) +

Di (yiâ‹† , yi )
â‰¥
Ïƒ


+

1
1
+
Ïƒ 2



Di (yiâ‹† , yÌƒi ) + Ï†âˆ—i (yÌƒi ) âˆ’ yÌƒi hai , xÌƒ(t) i
(t)

Di (yÌƒi , yi ) Î´
+ ((Ï†âˆ—i )â€² (yÌƒi ) âˆ’ (Ï†âˆ—i )â€² (yiâ‹† ))2 ,
Ïƒ
4

and rearranging terms, we get


(t)
(t)
Di (yÌƒi , yi )
1
1
Di (yiâ‹† , yi )
Di (yiâ‹† , yÌƒi ) +
â‰¥
+
âˆ’ (yÌƒi âˆ’ yiâ‹† )hai , xÌƒ(t) i + (Ï†âˆ—i (yÌƒi ) âˆ’ Ï†âˆ—i (yiâ‹† ))
Ïƒ
Ïƒ 2
Ïƒ
Î´
+ ((Ï†âˆ—i )â€² (yÌƒi ) âˆ’ (Ï†âˆ—i )â€² (yiâ‹† ))2 .
4
With i.i.d. random sampling at each iteration, we have the following relations:
n âˆ’ 1 (t)
1
Â· yÌƒi +
Â· yi ,
n
n
(t)
Di (yÌƒi , yiâ‹† ) (n âˆ’ 1)Di (yi , yiâ‹† )
(t+1) â‹†
+
,
E[Di (yi
, yi )] =
n
n
(t)
Di (yÌƒi , yi )
(t+1) (t)
E[Di (yi
, yi )] =
,
n
1
n âˆ’ 1 âˆ— (t)
(t+1)
E[Ï†âˆ—i (yi
)] = Â· Ï†âˆ—i (yÌƒi ) +
Â· Ï†i (yi ).
n
n
(t+1)

E[yi

]=

Plugging the above relations into (41) and dividing both sides by n, we have




1
1
(n âˆ’ 1)
1
1
(t) â‹†
(t+1) â‹†
(t+1) (t)
Di (yi , yi ) â‰¥
Di (yi
, yi ) + E[Di (yi
+
+
, yi )]
Ïƒ
2n
Ïƒ 2
Ïƒ


1 (t)
(t+1)
(t)
âˆ’ E[(yi
âˆ’ yi )] + (yi âˆ’ yiâ‹† ) hai , xÌƒ(t) i
n
1
(t+1)
(t)
(t)
+ E[Ï†âˆ—i (yi
)] âˆ’ Ï†âˆ—i (yi ) + (Ï†âˆ—i (yi ) âˆ’ Ï†âˆ—i (yiâ‹† ))
n
!2
(t)
((Ï†âˆ—i )â€² (yÌƒi ) âˆ’ (Ï†âˆ—i )â€² (yi ))
Î´
(t)
â‹†
hai , xÌƒ âˆ’ x i âˆ’
,
+
4n
Ïƒ

(41)

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

and summing over i = 1, . . . , n, we get




E[D(y (t+1) , y (t) )]
1
(n âˆ’ 1)
1
1
D(y (t) , y â‹† ) â‰¥
E[D(y (t+1) , y â‹† )] +
+
+
Ïƒ
2n
Ïƒ 2
Ïƒ
n
X
1
(t)
(t+1)
(t)
(Ï†âˆ— (y ) âˆ’ Ï†âˆ—i (yiâ‹† ))
+ Ï†âˆ—k (yk
) âˆ’ Ï†âˆ—k (yk ) +
n i=1 i i
E
D
âˆ’ n(u(t+1) âˆ’ u(t) ) + (u(t) âˆ’ uâˆ— ), xÌƒ(t)

2
â€²
â€²
Î´ 
(Ï†âˆ— (yÌƒ) âˆ’ Ï†âˆ— (y (t) )) 


â‹†
(t)
+
A(x âˆ’ xÌƒ ) +
 ,

4n 
Ïƒ
â€²

where Ï†âˆ— (y (t) ) is a n-dimensional vector such that the i-th coordinate is
â€²

(t)

[Ï†âˆ— (y (t) )]i = (Ï†âˆ—i )â€² (yi ),
and

n

u(t) =

n

1 X (t)
y ai ,
n i=1 i

On the other hand, since x(t+1) minimizes a

u(t+1) =
1
Ï„

1 X (t+1)
y
ai ,
n i=1 i

n

and

uâ‹† =

1X â‹†
y ai .
n i=1 i

+ Î»-strongly convex objective

D
E kx âˆ’ x(t) k2
g(x) + u(t) + n(u(t+1) âˆ’ u(t) ), x +
,
2Ï„

we can apply Lemma 2 with Ï = 0 to obtain

kx(t) âˆ’ xâ‹† k2
2Ï„


kx(t+1) âˆ’ x(t) k2
Î»
1
(t+1)
(t)
(t+1)
(t)
(t+1)
â‰¥ g(x
) + hu + n(u
âˆ’ u ), x
i+
kx(t+1) âˆ’ xâ‹† k2 ,
+
+
2Ï„
2Ï„
2

g(xâ‹† ) + hu(t) + n(u(t+1) âˆ’ u(t) ), xâ‹† i +

and rearranging terms, we get


E[kx(t+1) âˆ’ x(t) k2 ]
Î»
1
kx(t) âˆ’ xâ‹† k2
E[kx(t+1) âˆ’ xâ‹† k2 ] +
â‰¥
+
+ E[g(x(t+1) ) âˆ’ g(xâ‹† )]
2Ï„
2Ï„
2
2Ï„
+ E[hu(t) + n(u(t+1) âˆ’ u(t) ), x(t+1) âˆ’ xâ‹† i].

Notice that
L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) )) âˆ’ (n âˆ’ 1)(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t) ))
n
1 X âˆ— (t)
(t+1)
(t)
=
(Ï† (y ) âˆ’ Ï†âˆ—i (y â‹† )) + (Ï†âˆ—k (yk
) âˆ’ Ï†âˆ—k (yk )) + g(x(t+1) ) âˆ’ g(xâ‹† )
n i=1 i i
+ huâ‹† , x(t+1) i âˆ’ hu(t) , xâ‹† i + nhu(t) âˆ’ u(t+1) , xâ‹† i,

so



kx(t) âˆ’ xâ‹† k2
(n âˆ’ 1)
1
D(y (t) , y â‹† ) + (n âˆ’ 1)(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t) ))
+
+
2Ï„
Ïƒ
2n




E[kx(t+1) âˆ’ x(t) k2 ] E[D(y (t+1) , y (t) )]
1
1
Î»
1
(t+1)
â‹† 2
â‰¥
E[kx
âˆ’x k ]+
E[D(y (t+1) , y â‹† )] +
+
+
+
2Ï„
2
Ïƒ 2
2Ï„
Ïƒ
+ E[L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ))]

2
â€²
â€²
Î´ 
(Ï†âˆ— (yÌƒ) âˆ’ Ï†âˆ— (y (t) )) 


(t)
â‹†
(t+1)
(t)
(t+1)
t
â‹†
(t)
+ E[hu âˆ’ u + n(u
âˆ’ u ), x
âˆ’ xÌ„ i] +
A(x âˆ’ xÌƒ ) +
 .

4n 
Ïƒ

Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Next, we have

2

2
â€²
â€²
â€²
â€²
Î´ 
(Ï†âˆ— (yÌƒ) âˆ’ Ï†âˆ— (y (t) )) 
(Ï†âˆ— (yÌƒ) âˆ’ Ï†âˆ— (y (t) )) 
Î´ 




â‹†
(t)
â‹†
(t)
(t)
(tâˆ’1)
)+
A(x âˆ’ xÌƒ ) +
 =
A(x âˆ’ x ) âˆ’ Î¸A(x âˆ’ x



4n 
Ïƒ
4n 
Ïƒ




2
1
Î´ 

â‰¥ 1âˆ’
A(xâ‹† âˆ’ x(t) )
Î± 4n

2
â€²
â€²
(Ï†âˆ— (yÌƒ) âˆ’ Ï†âˆ— (y (t) )) 
Î´ 


(t)
(tâˆ’1)
)+
âˆ’ (Î± âˆ’ 1)
Î¸A(x âˆ’ x
 ,

4n 
Ïƒ
for any Î± > 1 and
and


2


A(xâ‹† âˆ’ x(t) ) â‰¥ Âµ2 kxâ‹† âˆ’ x(t) k2 ,


2
â€²
â€²

â€²
â€²
(Ï†âˆ— (yÌƒ) âˆ’ Ï†âˆ— (y (t) )) 
2


(t)
(tâˆ’1)
)+
Î¸A(x âˆ’ x
 â‰¥ âˆ’ 2Î¸2 kA(x(t) âˆ’ x(tâˆ’1) )k2 âˆ’ 2 kÏ†âˆ— (yÌƒ) âˆ’ Ï†âˆ— (y (t) )k2 ]


Ïƒ
Ïƒ
â‰¥ âˆ’ 2Î¸2 L2 kx(t) âˆ’ x(tâˆ’1) k2 âˆ’

â€²
â€²
2n
E[kÏ†âˆ— (y (t+1) ) âˆ’ Ï†âˆ— (y (t) )k2 ].
2
Ïƒ

Following the same reasoning as in the standard SPDC analysis, we have
hu(t) âˆ’ uâ‹† + n(u(t+1) âˆ’ u(t) ), x(t+1) âˆ’ xÌƒ(t) i =

(y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
âˆ’
n
n

(n âˆ’ 1) (t+1)
(y
âˆ’ y (t) )T A(x(t+1) âˆ’ x(t) ) âˆ’ Î¸(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) ),
n
and using Cauchy-Schwartz inequality, we have
+

|(y (t+1) âˆ’ y (t) )T A(x(t) âˆ’ x(tâˆ’1) )| â‰¤
â‰¤

k(y (t+1) âˆ’ y (t) )T Ak2
kx(t) âˆ’ x(tâˆ’1) k2
+
1/(2Ï„ )
8Ï„
ky (t+1) âˆ’ y (t) k2
,
1/(2Ï„ R2 )

and
|(y (t+1) âˆ’ y (t) )T A(x(t+1) âˆ’ x(t) )| â‰¤
â‰¤

kx(t+1) âˆ’ x(t) k2
k(y (t+1) âˆ’ y (t) )T Ak2
+
1/(2Ï„ )
8Ï„
ky (t+1) âˆ’ y (t) k2
.
1/(2Ï„ R2 )

Thus we get
(y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) ) Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
âˆ’
n
n
(t+1)
(t) 2
(t+1)
(t) 2
(t)
(tâˆ’1) 2
ky
âˆ’y k
kx
âˆ’x k
Î¸kx âˆ’ x
k
âˆ’
âˆ’
âˆ’
.
1/(4Ï„ R2 )
8Ï„
8Ï„

hu(t) âˆ’ uâˆ— + n(u(t+1) âˆ’ u(t) ), x(t+1) âˆ’ xÌƒ(t) i â‰¥

Also we can lower bound the term D(y (t+1) , y (t) ) using Lemma 2 with Ï = 1/2:
D(y (t+1) , y (t) ) =
â‰¥

n 
X

(t+1)

Ï†âˆ—i (yi

i=1

n 
X
Î³
i=1

2

(t+1)
(yi

(t)

(t)

(t+1)

) âˆ’ Ï†âˆ—i (yi ) âˆ’ h(Ï†âˆ—i )â€² (yi ), yi
âˆ’

(t)
yi )2

(t)

âˆ’ yi i



Î´
(t+1)
(t)
+ ((Ï†âˆ—i )â€² (yi
) âˆ’ (Ï†âˆ—i )â€² (yi ))2
2

â€²
â€²
Î³
Î´
= ky (t+1) âˆ’ y (t) k2 + kÏ†âˆ— (y (t+1) ) âˆ’ Ï†âˆ— (y (t) )k2 .
2
2



Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms

Combining everything above together, we have




1
(1 âˆ’ 1/Î±)Î´Âµ2
(n âˆ’ 1)
1
(t)
â‹† 2
kx âˆ’ x k +
D(y (t) , y â‹† ) + Î¸(L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ))
âˆ’
+
2Ï„
4n
Ïƒ
2n


Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
1
(Î± âˆ’ 1)Î¸Î´L2
â‹† â‹†
â‹† (t)
+ (n âˆ’ 1)(L(x , y ) âˆ’ L(x , y )) + Î¸
kx(t) âˆ’ xtâˆ’1 k2 +
+
8Ï„
2n
n




(t+1)
â‹† T
(t+1)
E[(y
âˆ’ y ) A(x
âˆ’ x(t) )]
1
1
Î»
1
â‰¥
E[kx(t+1) âˆ’ xâ‹† k2 ] +
E[D(y (t+1) , y â‹† )] +
+
+
2Ï„
2
Ïƒ 2
n
+ E[L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ))]


Î³

1
1
+
E[kx(t+1) âˆ’ x(t) k2 ] +
âˆ’
âˆ’ 4R2 Ï„ E[ky (t+1) âˆ’ y (t) k2 ]
2Ï„
8Ï„
2Ïƒ


â€²
â€²
Î´
(Î± âˆ’ 1)Î´
E[kÏ†âˆ— (y (t+1) ) âˆ’ Ï†âˆ— (y (t) )k2 ].
+
âˆ’
2Ïƒ
2Ïƒ 2

If we choose the parameters as
Î±=
then we know
and

Ïƒ
+ 1,
4

ÏƒÏ„ =

Î³
,
16R2

Î³
Î³
Î³
âˆ’ 4R2 Ï„ =
âˆ’
> 0,
2Ïƒ
2Ïƒ 4Ïƒ
Î´
(Î± âˆ’ 1)Î´
Î´
Î´
=
âˆ’
âˆ’
>0
2
2Ïƒ
2Ïƒ
2Ïƒ 8Ïƒ

and

(Î± âˆ’ 1)Î¸Î´L2
ÏƒÎ´L2
Î´ÏƒR2
Î´Î³
1
â‰¤
â‰¤
â‰¤
â‰¤
,
2
2n
8n
8
256Ï„
256Ï„

thus

(Î± âˆ’ 1)Î¸Î´L2
3
1
+
â‰¤
.
8Ï„
2n
8Ï„

In addition, we have
1âˆ’

1
Ïƒ
=
.
Î±
Ïƒ+4

Finally we obtain




1
ÏƒÎ´Âµ2
(n âˆ’ 1)
1
(t)
â‹† 2
kx âˆ’ x k +
D(y (t) , y â‹† ) + Î¸(L(x(t) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ))
âˆ’
+
2Ï„
4n(Ïƒ + 4)
Ïƒ
2n

3 (t)
Î¸(y (t) âˆ’ y â‹† )T A(x(t) âˆ’ x(tâˆ’1) )
+ (n âˆ’ 1)(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t) )) + Î¸ Â·
kx âˆ’ x(tâˆ’1) k2 +
8Ï„
n




1
E[(y (t+1) âˆ’ y â‹† )T A(x(t+1) âˆ’ x(t) )]
1
Î»
1
(t+1)
â‹† 2
(t+1)
â‹† 2
â‰¥
E[kx
âˆ’x k ]+
E[ky
âˆ’y k ]+
+
+
2Ï„
2
Ïƒ 2
n
3
+ E[L(x(t+1) , y â‹† ) âˆ’ L(xâ‹† , y â‹† ) + n(L(xâ‹† , y â‹† ) âˆ’ L(xâ‹† , y (t+1) ))] +
E[kx(t+1) âˆ’ x(t) k2 ].
8Ï„

As before, we can define Î¸x and Î¸y as the ratios between the coefficients in the x-distance and y-distance terms, and let
Î¸ = max{Î¸x , Î¸y }. Then choosing the step-size parameters as
r
1 p
1
Î³
, Ïƒ=
Ï„=
Î³(nÎ» + Î´Âµ2 )
2
4R nÎ» + Î´Âµ
4R
gives the desired result.

