Appendix for ‘Toward Efficient and Accurate Covariance Matrix
Estimation on Compressed Data’

This appendix is organized as follows. In Section 1, we state all theoretical results, including our proposed Lemma 1 and
Lemma 2 whose details are not presented in the main text of the paper. In Section 2, we provide detailed proofs for all of
the results. In Section 3, we reformulate and discuss the current theoretical results of the counterparts: Gauss-Inverse and
UniSample-HD. In Section 4, we give a detailed analysis of the computational complexity. Finally, in Section 5, we study
the impact of different α on the estimation accuracy.
Before proceeding, we first show the notations used in this appendix.
Notation. Let [k] denote a set of integers {1, 2, . . . , k}. Given a matrix X ∈ Rd×n , for j ∈ [d], i ∈ [n], we let xi ∈ Rd
denote the i-th column of X, and xji denote the (j, i)-th element of X or j-th element of xi . Let {Xt }kt=1 denote the set of
matrices {X1 , X2 , . . . , Xk }, and xji,t denote the (j, i)-th element of Xt . Let XT denote the transpose of X, and Tr(X)
denote its trace. Let |x| denote the absolute value of x. Let kXk2 and kXkF denote the spectral norm and Frobenius norm
Pd
of X, respectively. Let kxkq = ( j=1 |xj |q )1/q for q ≥ 1 be the `q -norm of x ∈ Rd . Let D(x) or D({xj }) be a square
diagonal matrix with the elements of vector x on the main diagonal, and D(X) also be a square diagonal matrix whose
main diagonal has only the main diagonal elements of X. Finally, X  Y means that Y − X is positive semidefinite.

1. Provable Results
For convenience, we first restate the theorems and their corollaries in the following.
Theorem 1. Assume X ∈ Rd×n and the sampling size 2 ≤ m < d. Sample m entries from each xi ∈ Rd with replacement
by running Algorithm 1. Let {pki }dk=1 and Si ∈ Rd×m denote the sampling
Pn probabilities and sampling matrix, respectively.
Then, the unbiased estimator for the target covariance matrix C = n1 i=1 xi xTi = n1 XXT can be recovered as
b1 − C
b 2,
Ce = C
b1 =
where C
E [Ce ] = C.

m
nm−n

Pn

i=1

b2 =
Si STi xi xTi Si STi , C

m
nm−n

Pn

i=1

D(Si STi xi xTi Si STi )D(bi ) with bki =

(1)
1
1+(m−1)pki ,

and

Theorem 2. Given X ∈ Rd×n and the sampling size 2 ≤ m < d, let C and Ce be defined as in Theorem 1. If the sampling
x2
|xki |
+ (1 − α) kxki
probabilities satisfy pki = α kx
2 with 0 < α < 1 for all k ∈ [d] and i ∈ [n], then with probability at least
i k1
i k2
1 − η − δ,
r
2d 2R
2d
kCe − Ck2 ≤ log( )
+ 2σ 2 log( ),
(2)
δ 3
δ
h
i
Pn h 8kxi k42
14kxi k21
4kxi k21 kxi k22
7kxi k22
2
where we define that R = maxi∈[n]
+ log2 ( 2nd
i=1 n2 m2 (1−α)2 + n2 m3 α2 (1−α)
n
η ) nmα2 , and σ =
i
Pn kx k2 x x2
9kxi k42
2kx k22 kxi k21
+ n2 mi2 α(1−α)
+ k i=1 ni 21mαi i k2 .
+ n2 m(1−α)
Corollary 1. Given X ∈√Rd×n and sampling size 2 ≤ m < d, let C and Ce be constructed by Algorithm 1. Define
kxi k1
d, and kxi k2 ≤ τ for all i ∈ [n]. Then, with probability at least 1 − η − δ we have
kxi k2 ≤ ϕ with 1 ≤ ϕ ≤
r
r
r
r
 
2
τ
ϕ
1
1
τ
ϕ
dkCk
dkCk2 
2
2
e f+
e f+
kCe − Ck2 ≤ min{O
+τ
,O
+τ
},
m
n
nm
m
n
nm
q
2 2
ϕ
2
e
+ τnm
+ τ ϕ kCk
nm , and O(·) hides the logarithmic factors on η, δ, m, n, d, and α.


where f =

τ2
n

(3)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

Corollary 2. Given X ∈ Rd×n (2 ≤ d) and an unknown population covariance matrix Cp ∈ Rd×d with each column
vector xi ∈ Rd i.i.d. generated from the Gaussian distribution N (0, Cp ). Let Ce be constructed by Algorithm 1 with
sampling size 2 ≤ m < d. Then, with probability at least 1 − η − δ − ζ,
r
 d2
d d
kCe − Cp k2
e
;
(4)
≤O
+
kCp k2
nm m n
Additionally, assuming rank(Cp )≤ r, with probability at least 1 − η − δ − ζ we have
r
r
 rd
k[Ce ]r − Cp k2
r
d
rd 
e
,
≤O
+
+
kCp k2
nm m n
nm

(5)

e hides the logarithmic factors on η, δ, ζ, m, n, d, and α.
where [Ce ]r is the solution to minrank(A)≤r kA − Ce k2 , and O(·)
Q
Pk
Q
Pk
Corollary 3. Given X, d, m, Cp and Ce as in Corollary 2. Let
=
ui uT and b =
ûi ûT with {ui }k
k

i=1

i

k

i=1

i

i=1

and {ûi }ki=1 being the leading k eigenvectors of Cp and Ce , respectively. Denote by λk the k-th largest eigenvalue of Cp .
Then, with probability at least 1 − η − δ − ζ,
r
Q
Q
 d2
k b k − k k2
1
d d
e
O
+
,
(6)
≤
kCp k2
λk − λk+1
nm m n
e hides the logarithmic factors on η, δ, ζ, m, n, d, and α.
where the eigengap λk − λk+1 > 0 and O(·)
Next, we present two lemmas: Lemma 1 and Lemma 2, which are used to prove the foregoing theorems. The detailed
statements of the two lemmas are omitted in the main text of the paper owing to limited space, and now they are
described below.
Lemma 1. Given any vector x ∈ Rd , and m < d, sample m entries from x with replacement by running Algorithm 1 with
the inputs x and m. Let {pk }dk=1 denote the corresponding sampling probabilities, S ∈ Rd×m denote the corresponding
rescaled sampling matrix, and {ek }dk=1 denote the standard basis vectors for Rd . Then, we have
d

 X
x2k
m−1 T
E SST xxT SST =
xx ;
ek eTk +
mpk
m

(7)

k=1

d

 X
1
m−1 2
E D(SST xxT SST ) =
(
+
)xk ek eTk ;
mpk
m
k=1
d 

 X
7(m − 1) 6(m2 − 3m + 2)
1
T
T
T 2
+
+
E (D(SS xx SS )) =
m3 p3k
m3 p2k
m3 pk
k=1

m3 − 6m2 + 11m − 6 4
+
xk ek eTk ;
m3




E SST xxT SST D(SST xxT SST ) = (E D(SST xxT SST )SST xxT SST )T

d 
X
1
6(m − 1) 3(m2 − 3m + 2) 4
m−1 T
x2k
T
=
+
+
x
e
e
+
xx
D({
})
k
k
k
m3 p3k
m3 p2k
m3 pk
m3
p2k
k=1


3(m2 − 3m + 2) T
x2k
m−3
2
+
xx D({ }) +
D({xk }) ;
m3
pk
3

d 

 X
4(m − 1)
1
T
T
T 2
E (SS xx SS ) =
+ 3 3 x4k ek eTk
m3 p2k
m pk
k=1
"
#
d
d
X
kxk22 (m2 − 3m + 2) m − 1 X x2k x2k
+
+
ek eTk
m3
m3
pk pk
k=1
k=1
"
#
d
kxk22 (m3 − 6m2 + 11m − 6) m2 − 3m + 2 X x2k
+
+
xxT
m3
m3
pk
k=1

(8)

(9)

(10)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’


x2k
x2k
2(m2 − 3m + 2)
m−1
+ xx
D({ }) +
D({ 2 })
m3
pk
m3
pk


2(m2 − 3m + 2)
x2k
x2k
m−1
+
D({
D({
})
xxT ,
})
+
m3
pk
m3
p2k
T



(11)

where the expectation is w.r.t. S, and D({x2k }) denotes a square diagonal matrix with {x2k }dk=1 on its diagonal that can be
extended to other similar notations.
Pd
Lemma 2. Given the definitions in Lemma 1. Then, with probability at least 1 − k=1 ηk , we have
X
kSST xxSST k2 ≤
f 2 (xk , ηk , m),
(12)
k∈Γ

where Γhis a set containing
at most m different elements
i of [d] with its cardinality |Γ| ≤ m, and f (xk , ηk , m) = |xk | +
q
log( η2k )

|xk |
3mpk

+ |xk |

1
9m2 p2k

2
1
log(2/ηk ) ( mpk

+

−

1
m)

.

Remark 1. For the expressions in Lemma 1 and Lemma 2, the sampling probability pk appears in the denominator, which
indicates that the derived bound may be sensitive to a highly small pk 6= 0. However, in terms of any pk = 0, we can define
|xk |a
= 0 for a, b > 0, because we follow the rule that pk = 0 only when xk = 0 and xk = 0 can never be sampled. Thus,
pbk
the aforementioned two lemmas and other derived results are applicable to the case where there exists pk = 0.

2. Analysis
2.1. Technical Theorems
Below, we first show the Matrix Bernstein inequality employed for characterizing the sums of independent random variables/matrices, and then present a matrix perturbation result for eigenvalues.
Theorem 3 (Tropp 2015, p. 76). Let {Ai }L
∈ Rd×n be independent random matrices with E [Ai ] = 0 and kAi k2 ≤ R.



PL i=1
PL
PL
2
Define the variance σ = max{k i=1 E Ai ATi k2 , k i=1 E ATi Ai k2 }. Then, P(k i=1 Ai k2 ≥ ) ≤ (d +
2

/2
n) exp( σ2−
+R/3 ) for all  ≥ 0.

Theorem 4 (Golub & Van Loan 1996, p. 396). If A ∈ Rd×d and A + E ∈ Rd×d are symmetric matrices, then
λk (A) + λd (E) ≤ λk (A + E) ≤ λk (A) + λ1 (E)

(13)

for k ∈ [d], where λk (A + E) and λk (A) designate the k-th largest eigenvalues.
2.2. Proof of Lemma 1
Proof. According to Algorithm 1 in the main text of the paper, each column vector in the rescaled sampling matrix S ∈
1
Rd×m is sampled with replacement from {rk = √mp
ek }dk=1 with corresponding probabilities {pk }dk=1 , where {ek }dk=1
k
are the standard basis vectors for Rd .
Firstly, we prove Eq. (7). By the definition, we expand
m
m
X
X
SST xxT SST =
stj sTtj x
xT stj sTtj
j=1

=

m
X

stj sTtj xxT stj sTtj +

j=1

(14)

j=1

X

sti sTti xxT stj sTtj ,

(15)

i6=j∈[m]

where the random variable tj is in [d].
Passing the expectation over S through the sum in Eq. (15), we have
m
m X
d
X
X
E
stj sTtj xxT stj sTtj =
P(tj = k)rk rTk xxT rk rTk
j=1

=

m X
d
X
j=1 k=1

j=1 k=1
d

pk

X x2
1
T
T
T
k
e
e
xx
e
e
=
ek eTk ,
k
k
k
k
m2 p2k
mpk
k=1

(16)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

and similarly
X

E

P(ti = k)P(tj = q)rk rTk xxT rq rTq

(17)

i6=j∈[m] k=1 q=1

i6=j∈[m]

=

d X
d
X X

sti sTti xxT stj sTtj =

d X
d
X

xk xq

k=1 q=1

m−1 T
m−1
ek eTq =
xx .
m
m

(18)

Now, combing Eq. (16) with Eq. (18) immediately proves Eq. (7).
Then, Eq. (8) can be proved based on Eq. (7) by

d
X




m−1 2
1
E D(SST xxT SST ) = D(E SST xxT SST ) =
+
)xk ek eTk .
(
mpk
m

(19)

k=1

Alternatively, D(SST xxT SST ) can be explicitly expanded by
D(SST xxT SST ) =

m
X

d
X

stj sTtj

j=1

x2k ek eTk

m
X

stj sTtj .

(20)

j=1

k=1

Thus, the whole target expectations in Eq. (9), Eq. (10) and Eq. (11) can be explicitly expanded, and we can use similar
ways of proving Eq. (7) to prove the remainder of the lemma.
To prove Eq. (9), we expand


m
d
m
X
X
X


E (D(SST xxT SST ))2 = E (
stj sTtj
x2k ek eTk
stj sTtj )2 
j=1



m
d
m
m
d
m
X
X
X
X
X
X
= E
stj sTtj
x2k ek eTk
stj sTtj
stj sTtj
x2k ek eTk
stj sTtj 
j=1

=E

m
X

stj sTtj

j=1

+E

m
X

d
X

stj sTtj

d
X

X

sti sTti

X

m
X

stj sTtj

d
X

d
X

d
X

sti sTti

m
X

stj sTtj

j=1

x2k ek eTk stj sTtj

k=1

x2k ek eTk stj sTtj

d
X

x2k ek eTk stj sTtj

(24)

d
X

x2k ek eTk stj sTtj

(25)

k=1

X

sti sTti

i6=j∈[m]

d
X
k=1

where the four terms in the last equations are calculated as:
(23) = E

m
X

stj sTtj

j=1

=E

m
X
j=1

stj sTtj

d
X
k=1

d
X
k=1

x2k ek eTk stj sTtj

m
X

stj sTtj

j=1

x2k ek eTk stj sTtj stj sTtj

d
X
k=1

(23)

k=1

i6=j∈[m]

x2k ek eTk stj sTtj

(22)

j=1

k=1

k=1

X

x2k ek eTk stj sTtj

k=1

sti sTti

i6=j∈[m]

j=1

j=1

k=1

i6=j∈[m]

+E

x2k ek eTk stj sTtj

k=1

j=1

+E

j=1

k=1

(21)

j=1

k=1

d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stj sTtj

x2k ek eTk stj sTtj ,

(26)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

X

+E

sti sTti

=

m
X

pk1

k1 =1 j=1

d
X

pk1 pq

i6=j∈[m] k1 =1 q=1

=

d
X

(

k=1

m
X

stj sTtj

stg sTtg

d
X

X

d
X

stg sTtg

g=i6=j∈[m]

k=1

X

d
X

stg sTtg

g=j6=i∈[m]

d
X

x2k ek eTk ek1 eTk1

x2k ek eTk ek1 eTk1 eq eTq

k=1

d
X

x2k ek eTk eq eTq

k=1

d
X
k1 ,k3 =1
d
X
k1 ,k2 =1
d
X
k1 =1

X

k=1

x2k ek eTk stg sTtg sti sTti

d
X

x2k ek eTk stj sTtj

k=1
d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stg sTtg sti sTti

d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stg sTtg sti sTti

d
X

x2k ek eTk stj sTtj

k=1

k=1

d

X
X
m(m − 1)(m − 2)
T
2
T
T
T
e
e
x
e
e
e
e
e
e
x2k ek eTk ek3 eTk3
k
k
k
k
k
k
k
k
k
1
1
2
1
1
2
m4 pk1
k=1

m(m − 1)
ek1 eTk1
m4 p2k1
m(m − 1)
ek1 eTk1
m4 p2k1

d
X

k=1

x2k ek eTk ek1 eTk1 ek1 eTk1

k=1
d
X

d
X

x2k ek eTk ek3 eTk3

k=1

x2k ek eTk ek1 eTk1 ek2 eTk2

d
X

x2k ek eTk ek1 eTk1

k=1

k=1
d
X
k1 =1

d
X
m(m − 1) 4
m(m − 1) 4
T
x
xk1 ek1 eTk1
e
e
+
k
k1
1 k1
2
4
m pk1
m4 p2k1
k1 =1


4

m(m − 1)(m − 2) 4 2m(m − 1)xk
xk +
ek eTk ;
m4 pk
m4 p2k

(25) = E

X

sti sTti

d
X

x2k ek eTk stj sTtj

m
X

stj sTtj

j=1

k=1

i6=j∈[m]
d 
X

sti sTti

i6=j∈[m]

m(m − 1)(m − 2) 4
xk1 ek1 eTk1 +
m4 pk1

d 
X

k=1

(27)

d

d
X
k1 ,k2 ,k3 =1

=

d
X
k=1

1
ek eT
m4 p2k1 p2q 1 k1

x2k ek eTk stj sTtj

k=1

+E

=

d
X

g6=i6=j∈[m]

+E

=

k=1

k=1

X

=E

+

x2k ek eTk ek1 eTk1 ek1 eTk1

1
m−1
+ 3 2 )x4k ek eTk ;
m3 p3k
m pk

j=1

+

d
X

k=1

(24) = E

=

x2k ek eTk stj sTtj

d
d
X
X
x4k
(m2 − m)x4k
T
e
e
+
ek eTk
k
k
3
m3 pk
m4 p2k

k=1

=

d
X
k=1

1
ek eT
m4 p4k1 1 k1

d
X

X

+E

x2k ek eTk sti sTti stj sTtj

k=1

i6=j∈[m]
d
X

d
X

d
X

(28)

x2k ek eTk stj sTtj

k=1


4

m(m − 1)(m − 2) 4 2m(m − 1)xk
xk +
ek eTk ;
m4 pk
m4 p2k

(26) = E

X
i6=j∈[m]

sti sTti

d
X
k=1

x2k ek eTk stj sTtj

X
i6=j∈[m]

sti sTti

d
X
k=1

(29)
x2k ek eTk stj sTtj

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

X

=E

sti sTti

d
X
k=1

i6=j6=g6=h∈[m]

X

+E

x2k ek eTk stj sTtj stg sTtg

sti sTti

k=1

X

d
X

sti sTti

i6=j,i=h,j6=g,g6=h∈[m]

k=1

X

d
X

+E

sti sTti

i6=j,i6=g,j=h,g6=h∈[m]

k=1

X

d
X

+E

sti sTti

i6=j,i6=h,j=g,g6=h∈[m]

k=1

X

d
X

+E

sti sTti

i6=j,i=g,j=h,g6=h∈[m]

k=1

X

d
X

+E

sti sTti

=

k=1

d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1

k=1

i6=j,i=h,j=g,g6=h∈[m]
d 
X

x2k ek eTk sth sTth

k=1
d
X

i6=j,i=g,j6=h,g6=h∈[m]

+E

d
X


m(m − 1)(m − 2)(m − 3) 4 4m(m − 1)(m − 2) 4 2m(m − 1) 4
T
x
+
x
x
+
k
k ek ek .
k
m4
m4 pk
m4 p2k

(30)
(31)

Combing the above terms with simplification and reformulation completes the proof of Eq. (9).
Now, we continue to prove Eq. (10).


E SST xxT SST D(SST xxT SST )


d
m
m
m
m
X
X
X
X
X
x2k ek eTk
stj sTtj 
stj sTtj
xT stj sTtj
stj sTtj x
= E

=E

m
X

j=1

j=1

j=1

stj sTtj xxT stj sTtj

m
X

stj sTtj xxT stj sTtj

j=1

+E

d
X

sti sTti

sti sTti xxT stj sTtj

X

m
X

stj sTtj

j=1

sti sTti xxT stj sTtj

i6=j∈[m]

(32)

d
X

x2k ek eTk stj sTtj

(33)

x2k ek eTk stj sTtj

(34)

k=1

i6=j∈[m]

X

x2k ek eTk stj sTtj

k=1

m
X

i6=j∈[m]

+E

stj sTtj

j=1

j=1

+E

m
X

j=1

k=1

d
X
k=1

X

sti sTti

i6=j∈[m]

d
X

x2k ek eTk stj sTtj ,

(35)

k=1

where we calculate the four terms in the last equation as shown in below:
(32) = E

m
X
j=1

=E

m
X
j=1

stj sTtj xxT stj sTtj

m
X

stj sTtj

j=1

stj sTtj xxT stj sTtj stj sTtj

d
X
k=1

d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stj sTtj + E

X
i6=j∈[m]

sti sTti xxT sti sTti stj sTtj

d
X
k=1

x2k ek eTk stj sTtj

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

=

d X
m
X

d

pk1

k1 =1 j=1

k=1

d
X

X

+E

X
1
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk ek1 eTk1
k
k
k
k
k
k
1
1
1
1
1
1
m4 p4k1
d
X

pk1 pq

i6=j∈[m] k1 =1 q=1

=

d
X
k=1

=

d
X

x4k
ek eTk +
m3 p3k
1

(

m3 p3k
k=1

(33) = E

m
X

+

d
X
1
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk eq eTq
k
k
q
q
1 k1
m4 p2k1 p2q 1 k1
k=1

d
X
k=1

(m2 − m)x4k
ek eTk
m4 p2k

m−1 4
)x ek eTk ;
m3 p2k k

stj sTtj xxT stj sTtj

j=1

X

stg sTtg xxT stg sTtg sti sTti

X

stg sTtg xxT stg sTtg sti sTti

X

stg sTtg xxT stg sTtg sti sTti

=

+

k1 ,k3 =1
d
X

+

x2k ek eTk stj sTtj

d
X

x2k ek eTk stj sTtj

d
X

x2k ek eTk stj sTtj
d

k1 ,k2 ,k3 =1
d
X

k=1
d
X

k=1

g=j6=i∈[m]
d
X

x2k ek eTk stj sTtj

k=1

g=i6=j∈[m]

+E

d
X

k=1

g6=i6=j∈[m]

+E

sti sTti

i6=j∈[m]

X

=E

(36)

X
m(m − 1)(m − 2)
ek1 eTk1 xxT ek1 eTk1 ek2 eTk2
x2k ek eTk ek3 eTk3
4
m pk1
k=1

m(m − 1)
ek1 eTk1 xxT ek1 eTk1 ek1 eTk1
m4 p2k1

d
X

x2k ek eTk ek3 eTk3

k=1
d

k1 ,k2 =1

X
m(m − 1)
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk ek1 eTk1
k
k
k
k
k
k
1
1
2
2
1
1
2
m4 pk1
k=1

d
d
d
X
X
X
m(m − 1) 4
m(m − 1) 4
m(m − 1)(m − 2) 4
T
T
x
e
e
+
x
e
e
+
xk1 ek1 eTk1
=
k
k
k
k
k
k
1
1
1
1
1
1
m4 pk1
m4 p2k1
m4 p2k1
k1 =1

k1 =1

=

d 
X
m(m − 1)(m − 2)

m4 pk

k=1

(34) = E

X

sti sTti xxT stj sTtj

i6=j∈[m]

=E

X

x4k +

2m(m − 1) 4
xk ek eTk ;
m4 p2k
m
X

X

sti sTti xxT stj sTtj stg sTtg

X
i=g6=j∈[m]

d
X

x2k ek eTk stj sTtj

k=1
d
X

x2k ek eTk stg sTtg

k=1

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk stg sTtg

k=1

i6=j=g∈[m]

+E

stj sTtj

j=1

i6=j6=g∈[m]

+E

k1 =1



sti sTti xxT stj sTtj stg sTtg

d
X
k=1

x2k ek eTk stg sTtg

(37)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’
d

d
X

=

X
m(m − 1)(m − 2)
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk ek3 eTk3
k
k
k
k
k
k
1
2
3
1
2
3
m4 pk3
k=1

k1 ,k2 ,k3 =1
d
X

+

k1 ,k3 =1
d
X

+

d
X
k1 ,k3 =1

=

k=1

d
X
m(m − 1)(m − 2)
3
T
x
x
e
e
+
k1 k3 k1 k3
m4 pk3

k1 ,k3 =1

X

sti sTti xxT stj sTtj

X

=E

sti sTti xxT stj sTtj stg sTtg

X

+E

X

sti sTti
d
X

X

+E

X

sti sTti xxT stj sTtj stg sTtg

X

sti sTti xxT stj sTtj stg sTtg

X

sti sTti xxT stj sTtj stg sTtg

X
i6=j,i=h,j=g,g6=h∈[m]

k1 ,k2 ,k3 ,k4 =1

k1 ,k2 ,k4 =1
d
X
k1 ,k2 ,k3 =1
d
X
k1 ,k2 ,k3 =1

x2k ek eTk sth sTth

d
X

x2k ek eTk sth sTth

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk sth sTth

k=1

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk sth sTth

k=1

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk sth sTth

k=1
d

d
X

d
X

d
X

k=1

i6=j,i=g,j=h,g6=h∈[m]

+E

x2k ek eTk sth sTth

k=1

i6=j,i6=h,j=g,g6=h∈[m]

+E

d
X
k=1

i6=j,i6=g,j=h,g6=h∈[m]

+E

x2k ek eTk stj sTtj

k=1

i6=j,i=h,j6=g,g6=h∈[m]

+E

d
X

k=1

x4k
ek eTk ;
p2k

x2k ek eTk sth sTth

i6=j,i=g,j6=h,g6=h∈[m]

+

k3 =1

d
X

k=1

i6=j∈[m]

i6=j6=g6=h∈[m]

+

d
X
m(m − 1) 4
m(m − 1)
3
T
x
x
e
e
+
xk3 ek3 eTk3
k1 k3 k1 k3
m4 p2k3
m4 p2k3

x2
x2
m(m − 1)(m − 2) T
m(m − 1)
m(m − 1) T
xx D({ k }) +
xx D({ 2k }) +
4
4
m
pk
m
pk
m4

i6=j∈[m]

+

k=1

X
m(m − 1)
ek3 eTk3 xxT ek2 eTk2 ek3 eTk3
x2k ek eTk ek3 eTk3
2
4
m pk3

(35) = E

=

x2k ek eTk ek3 eTk3

d

k2 ,k3 =1

=

m(m − 1)
ek1 eTk1 xxT ek3 eTk3 ek3 eTk3
m4 p2k3

d
X

X
m(m − 1)(m − 2)(m − 3)
xk1 xk2 ek1 eTk2 ek3 eTk3
x2k ek eTk ek4 eTk4
4
m

m(m − 1)(m − 2)
xk1 xk2 ek1 eTk2 ek1 eTk1
m4 pk1

k=1

d
X

x2k ek eTk ek4 eTk4

k=1
d

X
m(m − 1)(m − 2)
xk1 xk2 ek1 eTk2 ek3 eTk3
x2k ek eTk ek1 eTk1
4
m pk1
k=1
d

X
m(m − 1)(m − 2)
xk1 xk2 ek1 eTk2 ek3 eTk3
x2k ek eTk ek2 eTk2
4
m pk2
k=1

(38)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’
d

d
X

+

k1 ,k2 ,k4 =1

+

d
X
k1 ,k2

+

d
X

k1 ,k2 =1

+

k1 =1

+

+

x2k ek eTk ek2 eTk2

k=1

X
m(m − 1)
xk1 xk2 ek1 eTk2 ek2 eTk2
x2k ek eTk ek1 eTk1
4
m pk1 pk2
k=1

d
X
m(m − 1)(m − 2) 4
m(m − 1)(m − 2)(m − 3)
3
T
x
x
e
e
+
xk1 ek1 eTk1
k1 k2 k1 k2
m4
m4 pk1
k1 =1

m(m − 1)(m − 2) 4
xk1 ek1 eTk1 +
m4 pk1

d
X
k1 ,k2 =1

=

d
X

d

d
X

d
X

k=1

m(m − 1)
xk xk ek eT ek eT
m4 pk1 pk2 1 2 1 k2 1 k1

k1 ,k2 =1

=

X
m(m − 1)(m − 2)
T
T
x
x
e
e
e
e
x2k ek eTk ek4 eTk4
k
k
k
k
k
k
1
2
1
2
2
2
m4 pk2

d
X
k1 ,k2 =1

m(m − 1)(m − 2)
xk1 x3k2 ek1 eTk2
m4 pk2

d
d
X
X
m(m − 1) 4
m(m − 1) 4
m(m − 1)(m − 2)
3
T
T
xk1 xk2 ek1 ek2 +
xk1 ek1 ek1 +
xk1 ek1 eTk1
m4 pk2
m4 p2k1
m4 p2k1
k1 =1

m(m − 1)(m − 2)(m − 3) T
xx D({x2k }) +
m4
d
m(m − 1)(m − 2) X x4

k

m4

k=1

pk

ek eTk +

k1 =1

d
m(m − 1)(m − 2) X x4

k

m4

k=1

pk

ek eTk

m(m − 1)(m − 2) T
x2k
xx
D({
})
m4
pk
d

+

m(m − 1)(m − 2) T
x2
2m(m − 1) X x4k
xx D({ k }) +
ek eTk .
4
m
pk
m4
p2k

(39)

k=1

Combing the above terms with simplification and reformulation completes the proof of Eq. (10).
Finally, we have to prove Eq. (11).


m
m
X
X


E (SST xxT SST )2 = E (
stj sTtj x
xT stj sTtj )2 
j=1

= E(

m
X

stj sTtj xxT stj sTtj +

j=1

= E(

m
X

j=1

X

sti sTti xxT stj sTtj )2

i6=j∈[m]

stj sTtj xxT stj sTtj )2

(40)

j=1

+ E(

X

sti sTti xxT stj sTtj )2

(41)

i6=j∈[m]

+E

m
X
j=1

+E

X

stj sTtj xxT stj sTtj

X

sti sTti xxT stj sTtj

(42)

stj sTtj xxT stj sTtj ,

(43)

i6=j∈[m]

sti sTti xxT stj sTtj

m
X
j=1

i6=j∈[m]

where we calculate the four terms in the last equation as shown in below:
(40) = E

m
X
j=1

stj sTtj xxT stj sTtj stj sTtj xxT stj sTtj + E

X
i6=j∈[m]

sti sTti xxT sti sTti stj sTtj xxT stj sTtj

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

=

d X
m
X

1

pk

m4 p4k

k=1 j=1

ek eTk xxT ek eTk ek eTk xxT ek eTk

d X
d
X X

+E

pk pq

i6=j∈[m] k=1 q=1

=

d
X

d

k=1

=

d
X

1
ek eTk xxT ek eTk eq eTq xxT eq eTq
4
m p2k p2q

X (m2 − m)x4
x4k
k
ek eTk +
ek eTk
3
3
m pk
m4 p2k
k=1

1

(

+

m3 p3k
k=1

m−1 4
)x ek eTk ;
m3 p2k k

(44)



X

(41) = E 

sti sTti xxT stj sTtj

i6=j∈[m]

X

=E

X

sti sTti xxT stj sTtj 

i6=j∈[m]

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j6=g6=h∈[m]

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

X

X

sti sTti xxT stj sTtj stg sTtg xxT sth sTth + E

X

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i6=g,j=h,g6=h∈[m]

i6=j,i6=h,j=g,g6=h∈[m]

+E

X

+E

i6=j,i=h,j6=g,g6=h∈[m]

+E

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i=g,j6=h,g6=h∈[m]

X

+E

X

+E

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i=g,j=h,g6=h∈[m]

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i=h,j=g,g6=h∈[m]
d
X

=

k1 ,k2 ,k3 ,k4 =1

+

d
X
k1 ,k2 ,k4 =1

+

d
X
k1 ,k2 ,k3 =1

+

d
X
k1 ,k2

=

d
X

x2k2

d
X
k1 ,k2 =1

+

d
X
k1 =1

=

m(m − 1)(m − 2)
xk1 x2k2 xk3 ek1 eTk2 ek3 eTk2 +
m4 pk2

d
X
k1 ,k4 =1

d
X
k1 ,k4 =1

+

m(m − 1)(m − 2) 2
xk1 xk2 xk4 ek1 eTk2 ek1 eTk4 +
m4 pk1

m(m − 1) 2 2
x x ek eT ek eT +
m4 pk1 pk2 k1 k2 1 k2 1 k2

k2 =1

+

m(m − 1)(m − 2)(m − 3)
xk1 xk2 xk3 xk4 ek1 eTk2 ek3 eTk4
m4

d
X
k1 ,k2 =1

d
X
k1 ,k2 ,k3 =1
d
X
k1 ,k2 ,k4 =1

m(m − 1)(m − 2) 2
xk1 xk2 xk3 ek1 eTk2 ek3 eTk1
m4 pk1
m(m − 1)(m − 2)
xk1 x2k2 xk4 ek1 eTk2 ek2 eTk4
m4 pk2

m(m − 1) 2 2
x x ek eT ek eT
m4 pk1 pk2 k1 k2 1 k2 2 k1

m(m − 1)(m − 2)(m − 3)
xk1 xk4 ek1 eTk4
m4

d
d
X
X
m(m − 1)(m − 2) 2
m(m − 1)(m − 2) 3
T
2
x
x
e
e
+
x
xk1 ek1 eTk1
k1 k4 k1 k4
k2
m4 pk1
m4 pk1
k2 =1

m(m − 1)(m − 2)
xk1 x3k2 ek1 eTk2 +
m4 pk2

m(m − 1) 4
xk1 ek1 eTk1 +
m4 p2k1

kxk22 m(m

d
X
k2 =1

x2k2
pk2

d
X
k1 =1

d
X
k2 =1

k1 =1

x2k2
pk2

d
X
k1 ,k4 =1

m(m − 1)(m − 2)
xk1 xk4 ek1 eTk4
m4

m(m − 1) 2
xk1 ek1 eTk1
m4 pk1

x2k
− 1)(m − 2)(m − 3) T m(m − 1)(m − 2)
xx
+
D({
})xxT
m4
m4
pk

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’
d

+

kxk22 m(m − 1)(m − 2) X x2k
m(m − 1)(m − 2) T
x2k
T
xx
e
e
+
D({
})
k
k
m4
pk
m4
pk
k=1

+

d
m(m − 1)(m − 2) X x2

k

m4

(42) = E

m
X

k=1

stj sTtj xxT stj sTtj

j=1

d
d
d
m(m − 1) X x4k
m(m − 1) X x2k X x2k
T
e
e
+
ek eTk ;
k
k
m4
p2k
m4
pk
pk
k=1

X

(45)

k=1

sti sTti xxT stj sTtj

stg sTtg xxT stg sTtg sti sTti xxT stj sTtj

X

X

+E

g6=i6=j∈[m]

+E

k=1

i6=j∈[m]

X

=E

pk

xxT +

stg sTtg xxT stg sTtg sti sTti xxT stj sTtj

g=i6=j∈[m]

stg sTtg xxT stg sTtg sti sTti xxT stj sTtj

g=j6=i∈[m]

=

d
X
k1 ,k2 ,k3 =1

+

d
X
k1 ,k3 =1

=

d
X
k1 ,k3 =1

=

m(m − 1)(m − 2)
ek1 eTk1 xxT ek1 eTk1 ek2 eTk2 xxT ek3 eTk3
m4 pk1

d
X
m(m − 1)
T
T
T
T
T
T
e
e
xx
e
e
e
e
xx
e
e
+
k1 k1
k1 k1 k1 k1
k3 k3
m4 p2k1

k1 ,k2 =1

m(m − 1)(m − 2) 3
xk1 xk3 ek1 eTk3 +
m4 pk1

d
X
k1 ,k3 =1

m(m − 1)
ek1 eTk1 xxT ek1 eTk1 ek2 eTk2 xxT ek1 eTk1
m4 p2k1

d
X
m(m − 1) 3
m(m − 1) 4
T
x
xk1 ek1 eTk1
x
e
e
+
k1 k3 k1 k3
m4 p2k1
m4 p2k1
k1 =1

x2k
m(m − 1)
x2k
m(m − 1)
m(m − 1)(m − 2)
T
D({
})xx
+
D({
})xxT +
m4
pk
m4
p2k
m4

d
X
k=1

x4k
ek eTk ;
p2k

(46)

d

(43) =

m(m − 1)(m − 2) T
x2
x2
m(m − 1) X x4k
m(m − 1) T
xx D({ k }) +
xx D({ 2k }) +
ek eTk .
4
4
m
pk
m
pk
m4
p2k

(47)

k=1

Combing the above terms with simplification and reformulation completes the proof of Eq. (11). To this end, we complete
the whole proof.
2.3. Proof of Lemma 2
Proof. According to the setting, we have that
(a)

kSST xxT SST k2 = kSST xk22 = k

m
X

stj sTtj xk22 = k

j=1

=k

m X
d
X
j=1 k=1

δ tj k
xk ek k22 =
mpk

d
X

m
X

(

k=1 j=1

m
X
j=1

1
xt et k2
mptj j j 2

m
δtj k xk 2 (b) X X δtj k xk 2
) =
(
) ,
mpk
mpk
j=1

(48)

k∈Γ

|Γ|

where we let Γ = {γt }t=1 be a set containing at most m different elements of [d] with its cardinality |Γ| ≤ m.
In Eq. (48), (a) follows because SST xxT SST is a positive semidefinite matrix of rank 1, δtj k returns 1 only when tj = k
and 0 otherwise, and P(δtj k = 1) = P(tj = k) = pk . (b) holds due to that we perform random sampling with replacement
m times on the d entries of x ∈ Rd , and consequently at most m certain different entries from x are sampled.
Pm δtj γ1 xγ1
δtj γ1 xγ1
xγ1
Let k = γ1 with γ1 ∈ Γ, and we first bound | j=1 mp
for all
|. Define a random variable aj = mp
− m
γ1
γ1
m
j ∈ [m]. We can easily check that {aj }j=1 are independent with E [aj ] = 0, so that we can leverage Theorem 3 to
continue our following analysis. We see that
max |aj | = max{

j∈[m]

|xγ1 | 1
|xγ1 |
|xγ1 |
(
− 1),
}≤
,
m p γ1
m
mpγ1

(49)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

and
m
X
 
x2γ
x2γ1
− 1.
E a2j =
mpγ1
m
j=1

Thus, applying Theorem 3 with R =

P(|

m
X

|xγ1 |
mpγ1

and σ 2 =

aj | ≥ ) ≤ 2 exp(

j=1

x2γ
1
mpγ1

−

x2γ
1
m

(50)

obtains that

−2 /2
),
x2γ1 /(mpγ1 ) − x2γ1 /m + |xγ1 |/(3mpγ1 )

Pm
Pm
whose RHS is denoted by ηγ1 . Then, with probability at least 1 − ηγ1 we have | j=1 aj | ≤ , i.e., | j=1
|xγ1 | + . We then replace  by other variables to obtain that
"
#
s
1
2
2
|xγ1 |
1
1
+
|xγ1 | +  = |xγ1 | + log(
)
+ |xγ1 |
(
− ) ,
ηγ1 3mpγ1
9m2 p2γ1
log(2/ηγ1 ) mpγ1
m
which is denoted by f (xγ1 , ηγ1 , m).
Pm
In a similar way, we can bound | j=1
over cases for all k ∈ [d].

δtj k xk
mpk |

(51)
δtj γ1 xγ1
mp1

|≤

(52)

for any other k ∈ [d]. The lemma then follows by using the union bound

2.4. Proof of Theorem 1
b1 − C
b 2,
Proof. We have to prove
that the unbiased estimator for original
covariance matrix C is Eq. (1), i.e., Ce = C
1
b 1 = m Pn Si ST xi xT Si ST , and C
b 2 = m Pn D(Si ST xi xT Si ST )D(bi ) with bki =
where C
i
i
i
i
i
i
i=1
i=1
mn−n
mn−n
1+(m−1)pki .
n
Note that each Si is created by running Algorithm 1, and {Si }i=1 are independent matrices. Thus, taking all summands


b 1,
E Si STi xi xTi Si STi together and leveraging Eq. (7) in Lemma 1 achieves the expectation of C
b 1] =
E[C

" d
#
n
n
X
m X X x2ki
m
m−1
T
T
T
T
T
Si Si xi xi Si Si =
E
xi xi
ek ek +
nm − n i=1
nm − n i=1
mpki
m
k=1

=

1
nm − n

d
n X
X
i=1 k=1

x2ki
pki

ek eTk +

1
XXT .
n

(53)

b 1 is a biased estimator for the original covariance matrix C = 1 XXT = 1 Pn xi xT . We still
Eq. (53) indicates that C
i
i=1
n
n
b 1 to get an unbiased estimator. By Eq. (8) in Lemma 1, it can be shown that
need to apply a debiasing procedure to C
b 2] =
E[C

n
n X
d
X

m X 
1
x2ki
E D(Si STi xi xTi Si STi ) D(bi ) =
ek eTk .
nm − n i=1
nm − n i=1
pki

(54)

k=1

b1 − C
b 2 is unbiased for C.
Combing Eq. (53) with Eq. (54), we immediately see that Ce = C
2.5. Proof of Theorem 2
Proof. Here, we have to bound the error kCe −Ck2 . To make the representation compact, we define Ai = Ai1 −Ai2 −Ai3
T
T
T
T
Pn
mSi ST
mD(Si ST
x xT
i xi xi Si Si
i xi xi Si Si )D(bi )
with Ai1 =
, A i2 =
, Ai3 = in i . Then, i=1 Ai = Ce − C holds. It
nm−n
nm−n
is straightforward to see that {Ai }ni=1 are independent zero-mean random matrices, which are exactly the setting of the
Matrix Bernstein inequality, as shown in Theorem 3. To bound kCe −Ck2 via Theorem 3, we need to calculate the relevant
parameters R and σ 2 that characterize the range and variance of Ai respectively.
We first derive R by bounding kAi k2 so that kAi k2 ≤ R for all i ∈ [n]. Expanding kAi k2 gets that
kAi k2 = kAi1 − Ai2 − Ai3 k2 ≤ kAi1 − Ai2 k2 + kAi3 k2

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

≤kAi1 k2 + kAi3 k2 .

(55)

The last inequality in Eq. (55) results from
kAi1 − Ai2 k2 = max |λk (Ai1 − Ai2 )|
k∈[d]

(a)

≤ max{|λd (Ai1 ) − λ1 (Ai2 )|, |λ1 (Ai1 ) − λd (Ai2 )|}

(56)

(b)

= max{λ1 (Ai2 ), |λ1 (Ai1 ) − λd (Ai2 )|}

(57)

(c)

= max{λ1 (Ai2 ), λ1 (Ai1 ) − λd (Ai2 )}

(58)

(d)

≤ λ1 (Ai1 )

(59)

(e)

= kAi1 k2 ,

(60)

where λk (·) is the k-th largest eigenvalue.
(a) follows from that λk (Ai1 ) − λ1 (Ai2 ) ≤ λk (Ai1 − Ai2 ) ≤ λk (Ai1 ) − λd (Ai2 ) for any k ∈ [d], which can be proved
by combining Theorem 4 with the fact that λd (−Ai2 ) = −λ1 (Ai2 ) and λ1 (−Ai2 ) = −λd (Ai2 ) for Ai2 ∈ Rd×d .
(b) holds because of that λk≥2 (Ai1 ) = 0 since Ai1 is a positive semidefinite matrix of rank 1, and λk∈[d] (Ai2 ) ≥ 0 since
Ai2 is positive semidefinite.
Pd
(c) follows owing to that λ1 (Ai1 ) = Tr(Ai1 ) ≥ Tr(Ai2 ) = k=1 λk (Ai2 ) ≥ λd (Ai2 ) ≥ 0, where the first equality
holds because λk≥2 (Ai1 ) = 0, the first inequality results from the fact that the diagonal matrix Ai2 is constructed by the
diagonal elements of Ai1 multiplied by positive scalars not bigger than 1, and the second inequality is the consequence of
λk∈[d] (Ai2 ) ≥ 0.
(d) results from that λk∈[d] (Ai2 ) ≥ 0.
(e) follows owing to that Ai1 is positive semidefinite.
Now, we only need to bound kAi1 k2 and kAi3 k2 . We have that
kxi k22
xi xTi
k2 =
.
n
n
Pd
Then, Lemma 2 reveals that with probability at least 1 − k=1 ηki ,
kAi3 k2 = k

kAi1 k2 ≤

(61)

X
m
f 2 (xki , ηki , m),
nm − n

(62)

k∈Γi

|Γ |

i
where Γi = {γti }t=1
is a set occupying
at most
its cardinality |Γi | ≤ m, and
h
i
q m different elements of [d] with
|xki |
2
1
2
1
1
f (xki , ηki , m) = |xki | + log( ηki ) 3mpki + |xki | 9m2 p2 + log(2/ηki ) ( mpki − m ) .
ki

{xi }ni=1 .

Then, by union bound, with probability at least 1 −
"
#
X
m
kxi k22
2
R = max
f (xki , ηki , m) +
.
n
i∈[n] nm − n

We derive the similar results for all

Pn

i=1

Pd

k=1

ηki , we have
(63)

k∈Γi

Pn
Pn
Applying the well known inequality ( t=1 at )2 ≤ n t=1 a2t , we have
f 2 (xki , ηki , m) ≤ 3x2ki + 3 log2 (
≤ 3x2ki + log2 (

2
x2
2
x2
2
x2
x2
) 2ki 2 + 3 log2 (
) 2ki 2 + 6 log(
)( ki − ki )
ηki 9m pki
ηki 9m pki
ηki mpki
m

2
2x2
2 6x2ki
) 2ki2 + log(
)
.
ηki 3m pki
ηki mpki

(64)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’
x2

|xki |
+ (1 − α) kxki
Before continuing characterizing R in Eq. (63), we set the sampling probabilities as pki = α kx
2 . It is
i k1
i k2
Pd
|xki |
easy to check that k=1 pki = 1. For 0 < α < 1, we also have pki ≥ α kxi k1 , then plugging it in the second and third
term of Eq. (64) respectively getting that

f 2 (xki , ηki , m) ≤ 3x2ki + log2 (

2 2kxi k21
2 6|xki |kxi k1
+ log(
)
)
.
ηki 3m2 α2
ηki
mα

(65)

η
for all i ∈ [n] and k ∈ [d], we bound R with probability at least 1 −
Equipped with Eq. (63) and setting ηki = nd
Pn Pd
i=1
k=1 ηki = 1 − η by
"
#
2
X
2nd 6|xki |kxi k1  kxi k22
m
2 2nd 2kxi k1
2
3xki + log (
+ log(
+
R ≤ max
)
)
η 3m2 α2
η
mα
n
i∈[n] nm − n
k∈Γi
 


2
2nd 2kxi k21
2nd 6kxi k21
kxi k22
≤ max
3kxi k22 + log2 (
+ log(
+
)
)
2
η 3mα
η
mα
n
i∈[n] n


2
2
7kxi k2
2nd 14kxi k1
,
(66)
≤ max
+ log2 (
)
n
η
nmα2
i∈[n]
m
where the second inequality follows from that m−1
≤ 2 for m ≥ 2 and |Γi | ≤ m, and the last inequality results from that
2nd
α ≤ 1 and log( η ) ≥ 1 for n ≥ 1, d ≥ 2, and η ≤ 1.
Pn
At this stage, we have to derive σ 2 by only bounding for k i=1 E [Ai Ai ] k2 since Ai is symmetric. Expanding E [Ai Ai ]
obtains that

0  E [Ai Ai ] = E [Ai1 Ai1 + Ai2 Ai2 + Ai3 Ai3 − Ai1 Ai2 − Ai2 Ai1
−Ai1 Ai3 − Ai3 Ai1 + Ai2 Ai3 + Ai3 Ai2 ] ,
in RHS of which, we bound the expectation of each term. Specifically, invoking Lemma 1, we have that
d 
X


1
4
+
x4 ek eTk
n E [Ai Ai ] =
m(m − 1)p2ki
(m − 1)2 mp3ki ki
k=1
{z
}
|
1

#
#
"
"
d
d
d
2
2
2
X
X
X
1
x2ki x2ki
m
−
2
x
kxi k22 (m − 2)
kx
k
(m
−
5m
+
6)
i
2
ki
+
ek eTk +
+
xi xTi
+
m(m − 1)
m(m − 1)
pki pki
m(m − 1)
m(m − 1)
pki
k=1
k=1
k=1
|
{z
} |
{z
}
2
3


2

x2
1
x2
x2
1
x2
2(m − 2)
2(m − 2)
xi xTi D({ ki }) +
xi xTi D({ 2ki }) +
D({ ki })xi xTi +
D({ 2ki })xi xTi
m(m − 1)
pki
m(m − 1)
pki
m(m − 1)
pki
m(m − 1)
p
|
{z
} |
{z
} |
{z
} |
{z ki
}
4
6


7
5



d 
X
1
7
6(m − 2)
(m − 2)(m − 3) 4
+
xki ek eTk
+ D(bi )D(bi )
+
+
m(m − 1)2 p3ki
m(m − 1)p2ki
m(m − 1)pki
m(m − 1)
k=1
|
{z
}
8

+

d
d
X
X
1
1
+ kxi k22 xi xTi +
+ 1)x2ki ek eTk D(bi )xi xTi + xi xTi
+ 1)x2ki ek eTk D(bi )
(
(
| {z }
(m − 1)pki
(m − 1)pki
k=1
k=1
9

|
{z
} |
{z
}
10
11



d 
X
1
6
3(m − 2)
3(m − 2)
x2ki
4
T
T
−2
+
+
x
e
e
D(b
)
−
x
x
D({
})D(bi )
k
i
i
k
i
m(m − 1)2 p3ki
m(m − 1)p2ki
m(m − 1)pki ki
m(m − 1)
pki
k=1
|
{z
}
|
{z
}
13

12


Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

−

−

−

(m − 2)(m − 3)
3(m − 2)
(m − 2)(m − 3)
x2
xi xTi D({x2ki })D(bi ) −
D(bi )D({ ki })xi xTi −
D(bi )D({x2ki })xi xTi
m(m − 1)
m(m − 1)
pki
m(m − 1)
{z
} |
{z
} |
{z
}
|
14
15
16



d
X
1
x2ki
x2ki
x2
ek eTk xi xTi − kxi k22 xi xTi −
xi xTi ek eTk − kxi k22 xi xTi −
xi xTi D({ 2ki })D(bi )
| {z }
| {z } m(m − 1)
(m − 1)pki
(m − 1)pki
pki
k=1
k=1
|
{z
}
18
20


{z
}
{z
}
|
|
21

17
19


d
X

x2
1
D(bi )D({ 2ki })xi xTi .
m(m − 1)
pki
|
{z
}
22


(67)

x2

x2

ki d
ki
}) is to denote a square diagonal matrix in Rd×d with { pki
}k=1 on its diagonal,
Because of the limited space, D({ pki
which is also extended to other similar notations.

In Eq. (67), it can be checked that for m ≥ 2, we have
10 − 
17 = 0;

11 − 
19 = 0;


xi xTi
((m − 1)/pki )x2ki
(m − 2)(m + 1 − 1/pki )x2ki
D({
+
});
m(m − 1)
1 + (m − 1)pki
1 + (m − 1)pki
(m − 2)(m + 1 − 1/pki )x2ki
xi xTi
((m − 1)/pki )x2ki
6 −
15 + 
7 −
16 − 
22 = D({
+
})
;

1 + (m − 1)pki
1 + (m − 1)pki
m(m − 1)
"
#
d
(6 − 4m)kxi k22
m − 2 X x2ki
3 +
9 −
18 − 
20 =

+
xi xTi
m(m − 1)
m(m − 1)
pki

4 −
13 + 
5 −
14 − 
21 =


k=1

d
1 X x2ki

xi xTi ;
m
pki
k=1

8 −
12  0;



d 
X
4x4ki
8x4ki
1 
+ 3 3 ek eTk ;

m2 p2ki
m pki
k=1
"
#
d
d
X
kxi k22 x2ki
2x2ki X x2ki
2 

+ 2
ek eTk .
mpki
m pki
pki
k=1

(68)

k=1

Then, applying Eq. (67) and Eq. (68) obtains that
"
#
d
d
4x4ki
kxi k22 x2ki
2x2ki X x2ki
1 X 8x4ki
+ 3 3 +
+ 2
ek eTk
0  E [Ai Ai ]  2
n
m2 p2ki
m pki
mpki
m pki
pki
k=1

xi xTi

1)/pki )x2ki

k=1
1/pki )x2ki

((m −
(m − 2)(m + 1 −
D({
+
})
n2 m(m − 1)
1 + (m − 1)pki
1 + (m − 1)pki
((m − 1)/pki )x2ki
(m − 2)(m + 1 − 1/pki )x2ki
xi xTi
+ D({
+
}) 2
1 + (m − 1)pki
1 + (m − 1)pki
n m(m − 1)
+

d

+

1 X x2ki
xi xTi .
2
n m
pki

(69)

k=1

With Eq. (69) in hand, we can formulate σ 2 as
"
#
n
n
d
X
X
1
8x4ki
4x4ki
kxi k22 x2ki
2x2ki X x2ki
2
σ =k
E [Ai Ai ] k2 ≤
max 2
+ 3 3 +
+ 2
m2 p2ki
m pki
mpki
m pki
pki
k∈[d] n
i=1
i=1
k=1

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’
n
X



n X
d
X
2kxi k22 ((m − 1)/pki )x2ki
(m − 2)(m + 1 + 1/pki )x2ki
1
1
x2ki
+
(
+
) + 2 k
xi xTi k2
max 2
m(m
−
1)
1
+
(m
−
1)p
1
+
(m
−
1)p
n
m
p
k∈[d] n
ki
ki
ki
i=1
i=1 k=1
"
#
n
d
4
2 2
4
2 X
2
X
4x
kxi k2 xki
8xki
2x
1
xki
+ 3 ki3 +
≤
+ 2 ki
max 2
2 p2
m
m
p
mp
m
p
pki
k∈[d] n
ki
ki
ki
ki
i=1
k=1


n X
n
d
X
X
1
1 8kxi k22 x2ki
x2ki
+ 2 k
xi xTi k2 .
(70)
+
max 2
mp
n
m
p
k∈[d] n
ki
ki
i=1
i=1
k=1

x2

|xki |
Again, we have to consider the sampling distributions pki = α kx
+ (1 − α) kxki
2 with 0 < α < 1. Plugging pki ≥
i k1
ik
2

x2

|xki |
and pki ≥ (1 − α) kxki
α kx
2 in Eq. (70), we have
i k1
ik
2

"

d
4kxi k21 kxi k22
8kxi k42
kxi k42
2kxi k22 X |xki |kxi k1
+
+
+
m2 (1 − α)2
m3 α2 (1 − α) m(1 − α) m2 (1 − α)
α
k=1


n
n X
d
X
X
1
8kxi k42
1
|xki |kxi k1
+
max 2
xi xTi k2
+ 2 k
m(1
−
α)
n
m
α
k∈[d] n
i=1
i=1 k=1

n 
4
2
X
8kxi k2
4kxi k1 kxi k22
9kxi k42
2kxi k22 kxi k21
=
+ 2 3 2
+
+
n2 m2 (1 − α)2
n m α (1 − α) n2 m(1 − α) n2 m2 α(1 − α)
i=1
n
X

1
σ ≤
max 2
k∈[d] n
i=1
2

+k

n
X
kxi k2 xi x2
1

i=1

i

n2 mα

#

k2 .

(71)
4x4

4/3

Note that employing pki = Ω( Pd|xki|x|

) for the term m3 pki3 in Eq. (70) can produce a result tighter than that in
ki
k=1
Pd
Eq. (71), which is because of the fact that ( k=1 |xki |4/3 )3 ≤ kxi k21 kxi k22 always holds owing to the Holder’s in4/3
ki |

4/3

equality. However, it is not necessary to apply pki = Ω( Pd|xki|x|
4kxi k21 kxi k22
n2 m3 α2 (1−α)

to the term
2kxi k22 kxi k21
n2 m2 α(1−α)

kx k2 kx k2
O( in21m3i 2 )

=

4x4ki
m3 p3ki

k=1

4/3
ki |

) to the term

in Eq. (71) obtained by applying pki =

|xki |
α kx
i k1

4x4ki
m3 p3ki

in Eq. (70), because the term
x2

|xki |
+ (1 − α) kxki
+
2 = Ω( kx k
i 1
ik
2

x2ki
)
kxi k22

in Eq. (70) has already been small enough, which can be smaller than other terms in Eq. (71) like
kx k2 kx k2

q

= O( in21m2i 2 ). Similarly, applying other sampling probabilities pki = Ω( Pd|xki|x| |q ) with q 6= 1, 34 , 2
ki
k=1
to Eq. (70) will produce a result larger than Eq. (71), which may not be bounded. This is also why we only use
x2
|xki |
|xki |
pki = α kx
+ (1 − α) kxki
2 = Ω( kx k ) to tighten R in Eq. (66). This derivation justifies our selection of q = 1, 2 in
i k1
i 1
ik
2

q

pki = Ω( Pd|xki|x|
k=1

ki

x2

|xki |
) used for constructing the sampling probability pki = α kx
+ (1 − α) kxki
2.
|q
i k1
ik
2

We then invoke Theorem 3 to obtain that for  ≥ 0,
P(kCe − Ck2 ≥ ) ≤ 2d exp(

−2 /2
).
σ 2 + R/3

(72)

2

/2
Denote the RHS of Eq. (72) by δ = 2d exp( σ2−
+R/3 ) and consider the failure probability η in Eq. (66), then by union
2

/2
bound we have kCe − Ck2 ≤  holds with probability at least 1 − η − δ. Furthermore, δ = 2d exp( σ2−
+R/3 ) yields the
following quadratic equation in 

2
R
−
− σ 2 = 0.
2 log(2d/δ)
3
Solving Eq. (73) gets only one positive root
s
"
#
R 2
2d R
2σ 2
 = log( )
+ ( ) +
δ
3
3
log(2d/δ)

(73)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

2d 2R
≤ log( )
+
δ 3
2R
Thus, immediately we have kCe − Ck2 ≤ log( 2d
δ ) 3 +
completes the whole proof.

r
2σ 2 log(

2d
).
δ

(74)

q
2σ 2 log( 2d
δ ) holds with probability at least 1 − η − δ, which

2.6. Proof of Corollary 1
√
i k1
d, and m < d into
Proof. According to the setting, substituting that kxi k2 ≤ τ for all i ∈ [n], kx
kxi k2 ≤ ϕ with 1 ≤ ϕ ≤
Theorem 2 establishes that
r
τ2
τ4
τ 4 ϕ2
τ4
kCk2 τ 2 ϕ2 
τ 2 ϕ2
τ 4 ϕ2
e
+
+
+
kCe − Ck2 ≤ O
+
+
+
n
nm
nm2
nm3
nm
nm2
nm
r
r
r
τ2

2 2
2
1
kCk2
τ ϕ
τ ϕ 1
e
≤O
,
(75)
+
+
+ τ2
+ τϕ
n
nm
m
n
nm
nm
Pn
Pn x xT
where the first inequality invokes i=n kxi k42 ≤ nτ 4 , and C = i=1 in i is the original covariance matrix.
Pn
Pn
Pn
Pn
Also, we can adopt i=1 kxi k42 ≤ ndτ 2 kCk2 , which holds because i=1 kxi k42 ≤ τ 2 i=1 kxi k22 and i=1 kxi k22 =
nTr(C) ≤ ndkCk2 .
Hence, we have
 2
e τ +
kCe − Ck2 ≤ O
n
τ2
e
+
≤O
n

r
p
τ 2 ϕ2
d
dϕ2
d
dϕ2
ϕ2 
+ τ kCk2
+
+
+
+
nm
nm2
nm3
nm nm2
nm
r
r
r
2 2
τ ϕ
τ ϕ dkCk2
dkCk2
kCk2 
+
+τ
+ τϕ
.
nm
m
n
nm
nm

(76)

Finally, assigning kCe − Ck2 by the smaller one of Eq. (75) and Eq. (76) completes the proof.
2.7. Proof of Corollaries 2 and 3
Proof. The proof
Pn follows (AzizyanPetn al., 2015, Corollaries 4-6), where the key component kCe − Cp k2 is upper bounded
by kCe − n1 i=1 xi xTi k2 + k n1 i=1 xi xTi − Cp k2 . Then, the derivation results from Theorem 2 in our paper and the
Gaussian tail bounds in (Azizyan et al., 2015, Proposition 14).
(Azizyan et al., 2015, Proposition 14) shows that with probability at least 1 − ζ for d ≥ 2,
q
max kxi k2 ≤ 2Tr(Cp ) log(nd/ζ);
i∈[n]

k

1
n

n
X

xi xTi − Cp k2 ≤ O kCp k2

p

log(2/ζ)/n .

(77)

i=1

Then, applying them and Corollary 1 along with the fact that kxi k1 ≤
n

√

dkxi k2 and Tr(Cp ) ≤ dkCp k2 establishes

n

1X
1X
xi xTi k2 + k
xi xTi − Cp k2
n i=1
n i=1
s
r
r
r 
Pn
τ2
2 2
2
k n1 i=1 xi xTi k2  e 
τ
ϕ
τ
ϕ
1
1
1
2
e
≤O
+
+
+τ
+ τϕ
+ O kCp k2
n
nm
m
n
nm
nm
n
r
r
r
r 

τ2

2 2
2
τ
ϕ
τ
ϕ
1
1
kC
k
p
2
2
e
e kCp k2 1
+
+
+τ
+ τϕ
≤O
+O
n
nm
m
n
nm
nm
n
r
r
r
r 
 d2 kC k
dkCp k2 d
1
1
1
p 2
e
+
+ dkCp k2
+ dkCp k2
+ kCp k2
≤O
nm
m
n
nm
nm
n

kCe − Cp k2 ≤ kCe −

(78)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

r 
 d2 kC k
d
dkC
k
p
2
p
2
e
≤O
+
nm
m
n

(79)

with probability at least 1 − η − δ − ζ, where Eq. (78) results from that we invoke Eq. (77) to get k n1
Pn
e
k n1 i=1 xi xTi − Cp k2 + kCp k2 ≤ O(kC
p k2 ).

Pn

i=1

xi xTi k2 ≤

The proof for the low-rank case where rank(Cp )≤ r additionally adopts
k[Ce ]r − Cp k2 ≤ k[Ce ]r − Ce k2 + kCe − Cp k2
≤ k[Cp ]r − Ce k2 + kCe − Cp k2
≤ k[Cp ]r − Cp k2 + kCp − Ce k2 + kCe − Cp k2
= 2kCe − Cp k2 ,

(80)

where the last equality holds because rank(Cp ) ≤ r. Then, armed with Tr(Cp ) ≤ rank(Cp )kCp k2 ≤ rkCp k2 , we have
n

n

1X
1X
xi xTi k2 + k
xi xTi − Cp k2 )
n i=1
n i=1
r
r
r 
1
rd
1
+ rkCp k2
+ kCp k2
+ kCp k2
nm
nm
n
r
rd 
+ kCp k2
nm

k[Ce ]r − Cp k2 ≤ O(kCe − Cp k2 ) ≤ O(kCe −
r
 rdkC k
rkC
k
d
p
2
p
2
e
+
≤O
nm
m
n
r
 rdkC k
rkC
k
d
p
2
p
2
e
+
≤O
nm
m
n

(81)

with probability at least 1 − η − δ − ζ.
The given definitions also implicitly indicate that Cp and Ce are symmetric. Then, following (Azizyan et al., 2015), the
desired bound in Corollary 3 immediately results from Corollary 2 combined with the Davis-Kahan Theorem (Davis &
Q
Q
1
kCe − Cp k2 .
Kahan, 1970) that shows k b k − k k2 ≤ λk −λ
k+1

3. Discussion for Counterparts
3.1. Theorems for Gauss-Inverse and UniSample-HD
We first use our notations to rephrase current theoretical results provided in (Azizyan et al., 2015, Theorem 3) and (Anaraki
& Becker, 2017, Theorem 6), which correspond to Gauss-Inverse and UniSample-HD, respectively.
Theorem 5 (Azizyan et al. 2015, Theorem 3). Let d ≥ 2 and define,
n

n

1X
1X
S1 = k
kxi k22 xi xTi k2 , S2 =
kxi k42 .
n i=1
n i=1
There exists universal constants κ1 , κ2 > 0 such that for any 0 < δ < 1, with probability at least 1 − δ,
r
r
r d
 log(d/δ)
d maxi∈[n] kxi k22
d
kCe − Ck2 ≤ κ1
S1 +
S
+
κ
log(d/δ).
2
2
m
m2
n
nm

(82)

Theorem 6 (Anaraki & Becker 2017, Theorem 6). Let each column of Si ∈ Rd×m be chosen uniformly at random from
the set of all canonical basis vectors without replacement. Let ρ > 0 be a bound such that kSi STi xi k22 ≤ ρkxi k22 for all
i ∈ [n]. Then, with probability at least 1 − δ
kCe − Ck2 ≤ ,
(83)


h

i
2
/2
d(d−1)
d(d−m)
1
2
2
2
where δ = d exp σ2−
=
+R/3 , R = n
m(m−1) ρ + 1 maxi∈[n] kxi k2 + m(m−1) maxk∈[d],i∈[n] xki , and σ
h
d(d−1)
m(m−1)
d−m
2
2
nm(m−1) (ρ − d(d−1) ) maxi∈[n] kxi k2 kCk2 + m−1 ρ maxi∈[n] kxi k2 kD(C)k2
i
P
4
(d−m)2 maxk∈[d] n
2(d−m)kXk2
i=1 xki
+ n(m−1) F maxk∈[d],i∈[n] x2ki +
.
n(d−1)(m−1)

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

3.2. Discussion
In this subsection, we will simplify the foregoing two theorems by making Eq. (82) and Eq. (83) explicitly dependent on
n, m and d. Our derivations are natural and straightforward, and we will not deliberately loose Eq. (82) and Eq. (83) in
order to demonstrate the superiority of the theoretical results gained by our weighted sampling method. We define that
maxi∈n kxi k2 ≤ τ .
1
kXk2F ≤ kCk2 ≤
In terms of Eq. (82) in Theorem 5, S1 ≤ maxi∈[n] kxi k22 kCk2 and S2 ≤ maxi∈[n] kxi k42 . Note that nd
2
maxi∈[n] kxi k2 . Then, Eq. (82) can be simplified and reformulated as
s
s
 dkCk max
2
kx
k
d maxi∈[n] kxi k42
d maxi∈[n] kxi k22 
2
i 2
i∈[n]
e
+
kCe − Ck2 ≤ O
+
2
nm
nm
nm
r
r

2
2 
τ d
d
e τ dkCk2 + τ
.
(84)
≤O
+
nm
m n nm

If applying S2 ≤ d maxi∈[n] kxi k22 kCk2 in the original paper (Azizyan et al., 2015), we will get that
s
s
 dkCk max
2
kx
k
d2 maxi∈[n] kxi k22 kCk2
d maxi∈[n] kxi k22 
2
i
i∈[n]
2
e
kCe − Ck2 ≤ O
+
+
nm
nm2
nm
r

2 
e τ d kCk2 + τ d .
≤O
m
n
nm

(85)

In summary,

e τ
kCe − Ck2 ≤ min{O

r

dkCk2
τ2
+
nm
m

r

d
τ 2 d  e τ d
+
,O
n nm
m

r

kCk2
τ 2d 
+
}.
n
nm

(86)

For Eq. (83) in Theorem 6, we first simplify its R and σ 2 . According to (Anaraki & Becker, 2017), to obtain a more accurate
estimation, each xi is required to be multiplied by HD to flatten its large entries before being sampled uniformly without
replacement, where H is a Hadamard matrix with its dimension being 2l (l is a certain positive integer), and D is a diagonal
matrix with its diagonal elements being i.i.d. Rademacher random variables. Note that HDDT HT = DT HT HD is an
identity matrix.
Suppose that we do not have to pad X with zeros until its dimension d = 2l holds. Hence, assuming that d = 2l for
X ∈ Rd×n without loss of generality, we define Y = HDX ∈ Rd×n below.
Corollary 2 of (Anaraki & Becker, 2017) indicates that with probability at least 1 − β, we have
r s
1
2nd
max |yki | ≤
2 log(
) max kxi k2
d
β i∈[n]
k∈[d],i∈[n]

(87)

and
s
max kyi k2 ≤
i∈[n]

2 log(

2nd
) max kxi k2 .
β i∈[n]

Corollary 3 of (Anaraki & Becker, 2017) indicates that with probability at least 1 − β, we have
r s
m
2nd
T
kSi Si yi k2 ≤
2 log(
)kxi k2 .
d
β
To make a compact representation, we define θ =

q

(88)

(89)

2 log( 2nd
β ). Obviously, θ > 1.

Then, in Theorem 6, we can replace the input data X by Y. Combing Eq. (89) with the fact that kyi k2 = kHDxi k2 =
p
mθ 2
2
kxi k2 getting ρ = (( m
d θ) ) = d for the setting of Theorem 6. Along with θ > 1 and m ≤ d, we have

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

r

1  d2 mθ2
1 2
d(d − m)
2
2
(
+ 1)θ max kxi k2 +
θ) max kxi k22
R= O ( 2
2
n
m d
m
d
i∈[n]
i∈[n]

 dθ2
)θ2 max kxi k22
=O (
nm
i∈[n]
 d

e
=O
max kxi k22
nm i∈[n]
 τ 2d 
e
,
=O
nm

(90)

and
σ2 ≤

T T
T
d2  mθ2
m(m − 1) 2
2 HDXX D H
O
(
−
)θ
max
kx
k
k2
k
i
2
nm2
d
d(d − 1)
n
i∈[n]

(91)

(d − m) mθ2 2
HDXXT DT HT
θ max kxi k22 kD(
)k2
m
d
n
i∈[n]

(d − m)2 θ4
d − m θ2
max kxi k22 kHDXk2F +
n 2 max kxi k42
+
nm d i∈[n]
ndm
d i∈[n]
p

2
2
2
m 2 m−1 2
(d − m)θ4
d
2
2 n( 1/dθ) maxi∈[n] kxi k2
(θ
−
)θ
max
kx
k
max
kx
k
O
kCk
+
=
i
i
2
2
2
nm2
d
d−1
d
n
i∈[n]
i∈[n]

2
2
2 4
(d − m)θ
θ
(d − m) θ
+
max kxi k22 nd max kxi k22 +
max kxi k42
nmd
d i∈[n]
d3 m
i∈[n]
i∈[n]
 d
d−m
e
=O
max kxi k22 kCk2 +
max kxi k42
nm i∈[n]
nm2 i∈[n]

d(d − m)
(d − m)2
4
4
+
max
kx
k
+
max
kx
k
i
i
2
2
nm3 i∈[n]
nm3 d i∈[n]

 d
d(d − m)
(d − m)2
4
4
e
max kxi k22 kCk2 +
max
kx
k
+
max
kx
k
=O
i
i
2
2
nm i∈[n]
nm3 i∈[n]
nm3 d i∈[n]
 τ 2 dkCk
4
4
2
τ d(d − m) τ (d − m)
2
e
+
=O
+
.
(92)
nm
nm3
nm3 d
Note that Eq. (92) for simplifying σ 2 in Eq. (83) is tighter than the simplification result in the original paper (Anaraki &
d2
2
Becker, 2017) that scales with nm
2 . Recalling Eq. (83), and replacing its  by R and σ to get that with probability at least
1 − δ − β, we have
r
r
r

τ 2 d(d − m) τ 2 (d − m)
τ 2d 
dkCk2
1
e
+
+
+
.
(93)
kCe − Ck2 ≤ O τ
nm
m
nm
m
nmd nm
If m = d, then
r

dkCk2
τ 2d 
e
kCe − Ck2 ≤ O τ
+
.
(94)
nm
nm
Although pure sampling without replacement makes no estimation error when m = d, processing the data by a Hadamard
matrix before sampling can result in the error as shown in Eq. (94).
+

If m < d with m being close to d, then d − m = O(1), and thus we have
r
r

dkCk2
τ2
d
τ 2d 
e
kCe − Ck2 ≤ O τ
+
+
.
nm
m nm nm

(95)

If m  d or there exists a certain constant κ < 1 with m < κd, then O(d − m) = O(d). In addition to considering that
1
2
2
2
nd kXkF ≤ kCk2 ≤ maxi∈[n] kxi k2 = τ , then we have
r
r

dkCk2
τ 2d
1
τ 2d 
e
kCe − Ck2 ≤ O τ
+
+
.
(96)
nm
m
nm nm

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

4. Computational Complexity
Recall that we have n data samples in the d-dimensional space, and let m be the target compressed dimension. The
computational comparisons between our proposed method and the other approaches are presented in Table 1, in which
Standard method means computing C directly without data compression. We should explain some terms in the table
before proceeding.
Storage: storing data and random projection matrices (if any) in the remote sites and the fusion center, and storing the
covariance matrix in the fusion center.
Communication: shipping the data and random projection matrices (if any) from remote sites to the fusion center (high
communication cost requires tremendous bandwidth and power consumption).
Time (FLOPS): compressing the data in the remote sites, and calculating the covariance matrix in the fusion center (a low
time complexity means a low power cost and high efficiency for the data processing).
Note that, instead of only using the fusion center, data have to be first collected from many remote sites like a network of
g  n sensors. Then, they are transmitted to the fusion center to estimate the covariance matrix. This procedure shows
why communication cost is required. In the table, except for the communication, the two other compared terms have
contained the total costs in both the remote sites and fusion center.
Pn
For a covariance
matrix defined as C = n1 XXT − x̄x̄T , we can exactly calculate x̄ = n1 i=1 xi in the fusion center by
P
g
x̄ = n1 j=1 uj , where {xi }ni=1 are distributed in g  n remote sites, and uj ∈ Rd is the summation of all data vectors in
the j-th remote site before being compressed. Hence, about O(gd) storage, O(gd) communication cost, and O(nd) time
have to be added to the last four methods in Table 1, with g  n.
Table 1. Computational costs in terms of storage, communication, and time.

Method
Standard
Gauss-Inverse
Sparse
UniSample-HD
Our method

Storage
O(nd + d2 )
O(nm + d2 )
O(nm + d2 )
O(nm + d2 )
O(nm + d2 )

Communication
O(nd)
O(nm)
O(nm)
O(nm)
O(nm)

From now on, we can focus on the covariance matrix defined as C =

Time
O(nd2 )
O(nmd + nm2 d + nd2 ) + TG
O(d + nm2 ) + TS
O(nd log d + nm2 )
O(nd + nm log d + nm2 )
1
T
n XX .

First, we derive the computational costs in our propose algorithm. Computing {pki }k∈[d],i∈[n] takes O(nd) time. Then,
sampling nm entries from all data vectors to get Y ∈ Rm×n takes time that is scaled on nm log d up to a certain small
constant. In Eq. (1), each Si , STi xi , Si STi xi , and Si STi (squared diagonal), has at most m non-zero entries. Hence,
recovering {Si }ni=1 via the sampled nm entries in Y and the sampling indices in T ∈ Rm×n incurs O(nm) time. With
Y and T in hand, {Si STi xi }ni=1 can be accurately computed in O(nm) time. Equipped with {Si STi xi }ni=1 , computing
b 1 = m Pn Si ST xi xT Si ST additionally takes only O(nm2 ) time, this is due to that each Si ST xi ∈ Rd and
C
i
i
i
i
i=1
nm−n
b 1 , computing
Si STi xi xTi Si STi ∈ Rd×d has at most m and m2 non-zero entries respectively. Based on the obtained C
Pn
m
T
T
T
b
the square diagonal matrix C2 =
D(Si S xi x Si S )D(bi ) takes O(nm) time since each Si ST xi xT Si ST
nm−n

i=1

i

i

i

i

i

i

b1 − C
b 2 incurs O(d) extra time. The total
has at most m non-zero entries in its diagonal. Finally, obtaining C = C
2
running time is about O(nd + nm log d + nm + nm + nm + nm + nm + d) = O(nd + nm log d + nm2 ). In the
remote sites, data are compressed into m dimensional
space. Computing bki only corresponding to the sampled entries
b 2 = m Pn D(Si ST xi xT Si ST )D(bi ) in Eq. (1), so that at most nm entries
is enough to exactly calculate the C
i
i
i
i=1
nm−n
1
from {pki }k∈[d],i∈[n] have to be retained to obtain {bki }, since bki = 1+(m−1)p
. Thus, in the remote sites, Y ∈ Rm×n
ki
and T ∈ Rm×n dominate the storage cost, taking about O(nm) space in total. In the fusion center, O(d2 ) storage is
additionally used to store the estimated covariance Ce ∈ Rd×d . Similarly, about O(nm) communication cost is required
because of transmitting Y ∈ Rm×n , T ∈ Rm×n , v ∈ Rn , w ∈ Rn and α.
Then, for Standard in Table 1 that means directly calculating covariance matrix through the observed data samples without
compression, it is straightforward to check its computational complexity. X ∈ Rd×n and C ∈ Rd×d takes about O(nd+d2 )
storage in total, and X ∈ Rd×n leads to about O(nd) communication burden. Calculating the covariance matrix C =

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’
1
T
n XX

costs O(nd2 ) time.
Pn
For Gauss-Inverse, i=1 Si (STi Si )−1 STi xi xTi Si (STi Si )−1 STi , which is the main part of its unbiased estimator, dominates
the computational cost. Generating n different Gaussian matrices {Si ∈ Rd×m }ni=1 by the pseudorandom number generator like Mersenne twister (Matsumoto & Nishimura, 1998), which is by far the most widely used, takes considerably large
amount of time in practice. The time cost can be denoted by TG . As Si is dense, computing {STi xi }ni=1 takes O(nmd)
time. Calculating {(STi Si )−1 }ni=1 requires O(nm2 d + nm3 ), which involves matrix multiplications and inversions. Subsequently, we repeat the matrix-vector multiplications in {Si (STi Si )−1 STi xi ∈ Rd }ni=1 from the left to right, based on
which we get the target covariance matrix. Finally, it takes at least O(nmd + nm2 d + nm3 + nm2 + ndm + nd2 ) + TG =
O(nmd+nm2 d+nd2 )+TG time for Gauss-Inverse. In the remote sites, we compress data by STi xi ∈ Rm before sending
them to the fusion center. Along with O(d2 ) storage for the derived covariance matrix, about O(nm + d2 ) storage space is
required in total. Also, sending {STi xi ∈ Rm }ni=1 requires about a O(nm) computational burden.
Note that we have not listed the synchronization cost of Gauss-Inverse in Table 1. In practice, a pseudo-random number
generator is applied to the program in both the remote sites and the fusion center to generate/reconstruct n Gaussian random
matrices {Si ∈ Rd×m }ni=1 , and only n seeds are required to be transmitted from remote sites to the fusion center to recover
the Gaussian random matrices. Therefore, only about O(n) storage and communication cost have to be added in Table 1.
Also, calculating each (STi Si )−1 has to load each STi Si ∈ Rm×m into memory, hence at least O(m2 ) memory is required.
Pn
For Sparse, calculating i=1 Si STi xi xTi Si STi and subtracting its rescaled diagonal entries dominate the computational
cost (Anaraki, 2016). Generating sparse projection matrices {Si ∈ Rd×q }ni=1 is also expensive (Anaraki & Becker, 2017),
1
1
, 1 − 1s , 2s
}.
whose time cost is denoted by TS . The entries of each Si are distributed on {−1, 0, 1} with probabilities { 2s
d
Then, each column of Si has s non-zero entries in expectation. Empirically, we can fix that q/d = 0.2 or 0.4 according
to (Anaraki & Hughes, 2014; Anaraki, 2016). The number of non-zero entries of Si STi xi ∈ Rd is at least d(1 − (1 − 1s )q )
q
dq
1 q
in expectation, which ranges from dq
s (1 − 2s ) to s . Define d(1 − (1 − s ) ) = m < d, thus we can solve s with
d2
T
q/d = 0.2 or 0.4 fixed to obtain that s = O( m ). Then computing {Si xi ∈ Rq }ni=1 takes O( ndq
s ) = O(nm) time
ndq
T
d n
in expectation. Based on it, computing {Si Si xi ∈ R }i=1 additionally costs O( s ) = O(nm)
Pn time in expectation.
Since each Si STi xi ∈ Rd contains only m non-zeros entries in expectation, thus obtaining i=1 Si STi xi xTi Si STi and
subtracting its rescaled diagonal entries requires O(nm + nm + nm2 + d) + TS = O(nm2 + d) + TS time in total. Storing
{Si STi xi ∈ Rd }ni=1 and the estimated covariance matrix requires O(nm + d2 ) storage in expectation, where a O(nm) cost
results from O(nm) non-zero entries in {Si STi xi ∈ Rd }ni=1 along with O(nm) corresponding indices. Similarly, sending
{Si STi xi ∈ Rd }ni=1 from remote sites to the fusion center takes at most O(nm) communication cost in expectation.
For UniSample-HD, processing data by a Hadamard matrix by HDX ∈ Rd×n requires O(nd log d) time, where H ∈ Rd×d
can be a Hadamard matrix, D ∈ Rd×d is a diagonal matrix with diagonal elements being i.i.d. Rademacher random
variables, and we suppose that d = 2l holds (l is a certain positive integer). Then, sampling m entries uniformly without
replacement
on each data vector by {STi HDxi ∈ Rd }ni=1 takes O(nm) time. Hence, it is straightforward to check that
Pn
T T
T
T T
T
T
d×d
requires O(nd log d+nm+nm2 +d2 log d) = O(nd log d+nm2 )
i=1 HDSi Si D H xi xi D H Si Si HD ∈ R
d×d
time in total. HD ∈ R
can be generated on the fly when we process the data. About O(nm + d2 ) storage has to be
used for the compressed data and estimated covariance matrix. Obviously, about O(nm) communication cost is required.

5. Impact of the Parameter α
5.1. Discussion
To determine if the k-th entry of the data vector xi ∈ Rd should be retained or not, the sampling probability applied in our
method is
pki = α

x2
|xki |
+ (1 − α) ki 2 .
kxi k1
kxi k2

(97)

Achieving our theoretical bound of Theorem 2 requires 0 < α < 1. However, The case α = 1 and α = 0 can also
obtain weaker error bounds, which can be straightforwardly derived from Eqs. (64)(65) and Eqs. (70)(71). The following
illustration reveals the connection between α and error bounds on data owning different properties.
1. Only using α = 0, i.e., `2 -norm based sampling pki =

x2ki
kxi k22

can yield a very weak bound if there exist some very

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

small entries |xki | in xi ∈ Rd . E.g., substituting pki =
in

kx k4
maxk∈[d] x2i 2
ki

x2ki
kxi k22

into the term maxk∈[d]

x2ki
p2ki

of Eq. (64) or Eq. (70) results

in the final error bound, which becomes infinite if the positive entry |xki | gets close to 0;

|xki |
kxi k1 yields a slightly weak bound if there exist some very
x4
|xki |
large entries |xki | in xi ∈ Rd . E.g., substituting pki = kx
into the term maxk∈[d] p2ki of Eq. (70) results in
i k1
ki
maxk∈[d] x2ki kxi k21 in the final error bound, which is always greater than or equal to maxk∈[d] kxi k42 = kxi k42 derived
x2
x4ki
4
by employing pki = kxki
2 to bound maxk∈[d] p2 . Specifically, assume kxi k2 = 1 without loss of generality, then
i k2
ki
q√
√
d+1
√
and xki,k6=j =
it is possible that maxxi ⊂Rd ,kxi k42 =1 maxk∈[d] x2ki kxi k21 = d+2 4 d+1  1 if when xji =
2 d
q

2. Only using α = 1, i.e., `1 -norm based sampling pki =

q

1√
2d+2 d

for all k ∈ [d] with k 6= j. Also, minxi ⊂Rd ,kxi k42 =1 maxk∈[d] x2ki kxi k21 = 1 if we have xki =

1
d

for all

d

k ∈ [d] or we have xji = 1 and xki,k6=j = 0 for all k ∈ [d] with k 6= j. Note xi ⊂ R in the above optimizations
means that xi is a vector variable in the d-dimensional space, and j is an arbitrary integer in the set [d].
3. Therefore, α balances the performance by `1 -norm based sampling and `2 -norm based sampling. `2 sampling penalizes small entries more than `1 sampling, hence `2 sampling is more likely to select larger entries to decrease error
(e.g., case 2). However, different from `1 sampling, `2 sampling is unstable and sensitive to small entries, and it can
make estimation error incredibly high if extremely small entries are picked (e.g., case 1). Then 0 < α < 1 is applied
x2
to achieve the desired tight bound with pki ≥ (1 − α) kxki
2 to tackle the extreme situation in the case 2 that cannot
ik
2

|xki |
be well handled purely by pki ≥ α kx
. When α turns from 1 to 0, the estimation error is likely to first decrease and
i k1
then increase.

5.2. Experiments

1.4
1.2
1
0.8
0.6
0.05

0.1

m/d

0.15

0.2

1.6

A2, d=1000 n=10000

1.4
1.2
1
0.8
0.6
0.05

0.1

m/d

0.15

0.2

1.6

A3, d=1000 n=10000
Rescaled Error

A1, d=1000 n=10000

Rescaled Error

1.6

Rescaled Error

Alpha-1
Alpha-0.9
Alpha-0.8
Alpha-0.7
Alpha-0.6
Alpha-0.5
Alpha-0.4
Alpha-0.3
Alpha-0.2
Alpha-0.1
Alpha-0

Rescaled Error

Accordingly, we create four different synthetic datasets: {Ai }4i=1 ∈ R1000×10000
q (i.e., d = 1000 and nq= 10000). All
1√
1√
entries in A1 and A2 are i.i.d. generated from the Gaussian distributions N ( 2d+2
, 1 ) and N ( 2d+2
, 1 ),
d 1000
d 100
q√
√ , 1 ), and the other entries follow
respectively. For A3 , the entries of its one row are i.i.d. generated from N ( 2d+1
d 100
q
1
1√
( 2d+2 d , 100 ). For A4 , its generation follows the way of X1 in the main text of the paper.
n1X8, d=1024 N
n=10000

1.4
1.2
1
0.8
0.6
0.05

0.1

m/d

0.15

0.2

4

A4, d=1000 n=10000

2

0
0.05

0.1

0.15

0.2

m/d

Figure 1. Accuracy comparison by decreasing α from 1 to 0 with a step size of 0.1. The error at each α is normalized by that at α = 1
on y-axis, and m/d varies from 0.005 to 0.2 with a step size of 0.005 on x-axis. Roughly, α = 0.9 is a good choice, and the smaller
parameter like α = 0 usually leads to a poorer accuracy and higher variance compared with the other α values.
0.1
m/d

0.15

0.2

In Figure 1, the y-axis reports the errors that are normalized by the error incurred at α = 1. For A1 , the magnitudes of
the data entries tend to be highly uniformly distributed. Thus, nearly the same results are returned over all α. For A2 ,
its entries are slightly uniformly distributed with some entries having extremely small magnitudes. Hence, α = 0 has a
poorer performance compared with the others, which is consistent with the case 1 in Section 5.1. A3 contains some entries
larger than the others, and neither α = 0 nor α = 1 achieves the best performance obtained roughly at α = 0.9. Also,
the estimation error first decreases and then increases when α turns from 1 to 0. All such simulation results conform to
the case 2 and case 3 in Section 5.1. Considering A4 that is not likely to contain the extreme situation as mentioned in the
case 2 of Section 5.1, we see that best performance is roughly achieved when α gets close to 1.

Appendix for ‘Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data’

References
Anaraki, F. Estimation of the sample covariance matrix from compressive measurements. IET Signal Processing, 2016.
Anaraki, F. and Becker, S. Preconditioned data sparsification for big data with applications to pca and k-means. IEEE Transactions on
Information Theory, 2017.
Anaraki, F. and Hughes, S. Memory and computation efficient pca via very sparse random projections. In Proceedings of the 31st
International Conference on Machine Learning (ICML-14), pp. 1341–1349, 2014.
Azizyan, M., Krishnamurthy, A., and Singh, A.
arXiv:1506.00898, 2015.

Extreme compressive sampling for covariance estimation.

arXiv preprint

Davis, C. and Kahan, W. M. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1–46, 1970.
Golub, G. H. and Van Loan, C. F. Matrix computations. 1996.
Matsumoto, M. and Nishimura, T. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator.
ACM Transactions on Modeling and Computer Simulation, 1998.
Tropp, J. A. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8(1-2):1–230, 2015.

