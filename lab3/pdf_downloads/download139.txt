Appendix for â€˜Toward Efficient and Accurate Covariance Matrix
Estimation on Compressed Dataâ€™

This appendix is organized as follows. In Section 1, we state all theoretical results, including our proposed Lemma 1 and
Lemma 2 whose details are not presented in the main text of the paper. In Section 2, we provide detailed proofs for all of
the results. In Section 3, we reformulate and discuss the current theoretical results of the counterparts: Gauss-Inverse and
UniSample-HD. In Section 4, we give a detailed analysis of the computational complexity. Finally, in Section 5, we study
the impact of different Î± on the estimation accuracy.
Before proceeding, we first show the notations used in this appendix.
Notation. Let [k] denote a set of integers {1, 2, . . . , k}. Given a matrix X âˆˆ RdÃ—n , for j âˆˆ [d], i âˆˆ [n], we let xi âˆˆ Rd
denote the i-th column of X, and xji denote the (j, i)-th element of X or j-th element of xi . Let {Xt }kt=1 denote the set of
matrices {X1 , X2 , . . . , Xk }, and xji,t denote the (j, i)-th element of Xt . Let XT denote the transpose of X, and Tr(X)
denote its trace. Let |x| denote the absolute value of x. Let kXk2 and kXkF denote the spectral norm and Frobenius norm
Pd
of X, respectively. Let kxkq = ( j=1 |xj |q )1/q for q â‰¥ 1 be the `q -norm of x âˆˆ Rd . Let D(x) or D({xj }) be a square
diagonal matrix with the elements of vector x on the main diagonal, and D(X) also be a square diagonal matrix whose
main diagonal has only the main diagonal elements of X. Finally, X  Y means that Y âˆ’ X is positive semidefinite.

1. Provable Results
For convenience, we first restate the theorems and their corollaries in the following.
Theorem 1. Assume X âˆˆ RdÃ—n and the sampling size 2 â‰¤ m < d. Sample m entries from each xi âˆˆ Rd with replacement
by running Algorithm 1. Let {pki }dk=1 and Si âˆˆ RdÃ—m denote the sampling
Pn probabilities and sampling matrix, respectively.
Then, the unbiased estimator for the target covariance matrix C = n1 i=1 xi xTi = n1 XXT can be recovered as
b1 âˆ’ C
b 2,
Ce = C
b1 =
where C
E [Ce ] = C.

m
nmâˆ’n

Pn

i=1

b2 =
Si STi xi xTi Si STi , C

m
nmâˆ’n

Pn

i=1

D(Si STi xi xTi Si STi )D(bi ) with bki =

(1)
1
1+(mâˆ’1)pki ,

and

Theorem 2. Given X âˆˆ RdÃ—n and the sampling size 2 â‰¤ m < d, let C and Ce be defined as in Theorem 1. If the sampling
x2
|xki |
+ (1 âˆ’ Î±) kxki
probabilities satisfy pki = Î± kx
2 with 0 < Î± < 1 for all k âˆˆ [d] and i âˆˆ [n], then with probability at least
i k1
i k2
1 âˆ’ Î· âˆ’ Î´,
r
2d 2R
2d
kCe âˆ’ Ck2 â‰¤ log( )
+ 2Ïƒ 2 log( ),
(2)
Î´ 3
Î´
h
i
Pn h 8kxi k42
14kxi k21
4kxi k21 kxi k22
7kxi k22
2
where we define that R = maxiâˆˆ[n]
+ log2 ( 2nd
i=1 n2 m2 (1âˆ’Î±)2 + n2 m3 Î±2 (1âˆ’Î±)
n
Î· ) nmÎ±2 , and Ïƒ =
i
Pn kx k2 x x2
9kxi k42
2kx k22 kxi k21
+ n2 mi2 Î±(1âˆ’Î±)
+ k i=1 ni 21mÎ±i i k2 .
+ n2 m(1âˆ’Î±)
Corollary 1. Given X âˆˆâˆšRdÃ—n and sampling size 2 â‰¤ m < d, let C and Ce be constructed by Algorithm 1. Define
kxi k1
d, and kxi k2 â‰¤ Ï„ for all i âˆˆ [n]. Then, with probability at least 1 âˆ’ Î· âˆ’ Î´ we have
kxi k2 â‰¤ Ï• with 1 â‰¤ Ï• â‰¤
r
r
r
r
 
2
Ï„
Ï•
1
1
Ï„
Ï•
dkCk
dkCk2 
2
2
e f+
e f+
kCe âˆ’ Ck2 â‰¤ min{O
+Ï„
,O
+Ï„
},
m
n
nm
m
n
nm
q
2 2
Ï•
2
e
+ Ï„nm
+ Ï„ Ï• kCk
nm , and O(Â·) hides the logarithmic factors on Î·, Î´, m, n, d, and Î±.


where f =

Ï„2
n

(3)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

Corollary 2. Given X âˆˆ RdÃ—n (2 â‰¤ d) and an unknown population covariance matrix Cp âˆˆ RdÃ—d with each column
vector xi âˆˆ Rd i.i.d. generated from the Gaussian distribution N (0, Cp ). Let Ce be constructed by Algorithm 1 with
sampling size 2 â‰¤ m < d. Then, with probability at least 1 âˆ’ Î· âˆ’ Î´ âˆ’ Î¶,
r
 d2
d d
kCe âˆ’ Cp k2
e
;
(4)
â‰¤O
+
kCp k2
nm m n
Additionally, assuming rank(Cp )â‰¤ r, with probability at least 1 âˆ’ Î· âˆ’ Î´ âˆ’ Î¶ we have
r
r
 rd
k[Ce ]r âˆ’ Cp k2
r
d
rd 
e
,
â‰¤O
+
+
kCp k2
nm m n
nm

(5)

e hides the logarithmic factors on Î·, Î´, Î¶, m, n, d, and Î±.
where [Ce ]r is the solution to minrank(A)â‰¤r kA âˆ’ Ce k2 , and O(Â·)
Q
Pk
Q
Pk
Corollary 3. Given X, d, m, Cp and Ce as in Corollary 2. Let
=
ui uT and b =
uÌ‚i uÌ‚T with {ui }k
k

i=1

i

k

i=1

i

i=1

and {uÌ‚i }ki=1 being the leading k eigenvectors of Cp and Ce , respectively. Denote by Î»k the k-th largest eigenvalue of Cp .
Then, with probability at least 1 âˆ’ Î· âˆ’ Î´ âˆ’ Î¶,
r
Q
Q
 d2
k b k âˆ’ k k2
1
d d
e
O
+
,
(6)
â‰¤
kCp k2
Î»k âˆ’ Î»k+1
nm m n
e hides the logarithmic factors on Î·, Î´, Î¶, m, n, d, and Î±.
where the eigengap Î»k âˆ’ Î»k+1 > 0 and O(Â·)
Next, we present two lemmas: Lemma 1 and Lemma 2, which are used to prove the foregoing theorems. The detailed
statements of the two lemmas are omitted in the main text of the paper owing to limited space, and now they are
described below.
Lemma 1. Given any vector x âˆˆ Rd , and m < d, sample m entries from x with replacement by running Algorithm 1 with
the inputs x and m. Let {pk }dk=1 denote the corresponding sampling probabilities, S âˆˆ RdÃ—m denote the corresponding
rescaled sampling matrix, and {ek }dk=1 denote the standard basis vectors for Rd . Then, we have
d

 X
x2k
mâˆ’1 T
E SST xxT SST =
xx ;
ek eTk +
mpk
m

(7)

k=1

d

 X
1
mâˆ’1 2
E D(SST xxT SST ) =
(
+
)xk ek eTk ;
mpk
m
k=1
d 

 X
7(m âˆ’ 1) 6(m2 âˆ’ 3m + 2)
1
T
T
T 2
+
+
E (D(SS xx SS )) =
m3 p3k
m3 p2k
m3 pk
k=1

m3 âˆ’ 6m2 + 11m âˆ’ 6 4
+
xk ek eTk ;
m3




E SST xxT SST D(SST xxT SST ) = (E D(SST xxT SST )SST xxT SST )T

d 
X
1
6(m âˆ’ 1) 3(m2 âˆ’ 3m + 2) 4
mâˆ’1 T
x2k
T
=
+
+
x
e
e
+
xx
D({
})
k
k
k
m3 p3k
m3 p2k
m3 pk
m3
p2k
k=1


3(m2 âˆ’ 3m + 2) T
x2k
mâˆ’3
2
+
xx D({ }) +
D({xk }) ;
m3
pk
3

d 

 X
4(m âˆ’ 1)
1
T
T
T 2
E (SS xx SS ) =
+ 3 3 x4k ek eTk
m3 p2k
m pk
k=1
"
#
d
d
X
kxk22 (m2 âˆ’ 3m + 2) m âˆ’ 1 X x2k x2k
+
+
ek eTk
m3
m3
pk pk
k=1
k=1
"
#
d
kxk22 (m3 âˆ’ 6m2 + 11m âˆ’ 6) m2 âˆ’ 3m + 2 X x2k
+
+
xxT
m3
m3
pk
k=1

(8)

(9)

(10)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™


x2k
x2k
2(m2 âˆ’ 3m + 2)
mâˆ’1
+ xx
D({ }) +
D({ 2 })
m3
pk
m3
pk


2(m2 âˆ’ 3m + 2)
x2k
x2k
mâˆ’1
+
D({
D({
})
xxT ,
})
+
m3
pk
m3
p2k
T



(11)

where the expectation is w.r.t. S, and D({x2k }) denotes a square diagonal matrix with {x2k }dk=1 on its diagonal that can be
extended to other similar notations.
Pd
Lemma 2. Given the definitions in Lemma 1. Then, with probability at least 1 âˆ’ k=1 Î·k , we have
X
kSST xxSST k2 â‰¤
f 2 (xk , Î·k , m),
(12)
kâˆˆÎ“

where Î“his a set containing
at most m different elements
i of [d] with its cardinality |Î“| â‰¤ m, and f (xk , Î·k , m) = |xk | +
q
log( Î·2k )

|xk |
3mpk

+ |xk |

1
9m2 p2k

2
1
log(2/Î·k ) ( mpk

+

âˆ’

1
m)

.

Remark 1. For the expressions in Lemma 1 and Lemma 2, the sampling probability pk appears in the denominator, which
indicates that the derived bound may be sensitive to a highly small pk 6= 0. However, in terms of any pk = 0, we can define
|xk |a
= 0 for a, b > 0, because we follow the rule that pk = 0 only when xk = 0 and xk = 0 can never be sampled. Thus,
pbk
the aforementioned two lemmas and other derived results are applicable to the case where there exists pk = 0.

2. Analysis
2.1. Technical Theorems
Below, we first show the Matrix Bernstein inequality employed for characterizing the sums of independent random variables/matrices, and then present a matrix perturbation result for eigenvalues.
Theorem 3 (Tropp 2015, p. 76). Let {Ai }L
âˆˆ RdÃ—n be independent random matrices with E [Ai ] = 0 and kAi k2 â‰¤ R.



PL i=1
PL
PL
2
Define the variance Ïƒ = max{k i=1 E Ai ATi k2 , k i=1 E ATi Ai k2 }. Then, P(k i=1 Ai k2 â‰¥ ) â‰¤ (d +
2

/2
n) exp( Ïƒ2âˆ’
+R/3 ) for all  â‰¥ 0.

Theorem 4 (Golub & Van Loan 1996, p. 396). If A âˆˆ RdÃ—d and A + E âˆˆ RdÃ—d are symmetric matrices, then
Î»k (A) + Î»d (E) â‰¤ Î»k (A + E) â‰¤ Î»k (A) + Î»1 (E)

(13)

for k âˆˆ [d], where Î»k (A + E) and Î»k (A) designate the k-th largest eigenvalues.
2.2. Proof of Lemma 1
Proof. According to Algorithm 1 in the main text of the paper, each column vector in the rescaled sampling matrix S âˆˆ
1
RdÃ—m is sampled with replacement from {rk = âˆšmp
ek }dk=1 with corresponding probabilities {pk }dk=1 , where {ek }dk=1
k
are the standard basis vectors for Rd .
Firstly, we prove Eq. (7). By the definition, we expand
m
m
X
X
SST xxT SST =
stj sTtj x
xT stj sTtj
j=1

=

m
X

stj sTtj xxT stj sTtj +

j=1

(14)

j=1

X

sti sTti xxT stj sTtj ,

(15)

i6=jâˆˆ[m]

where the random variable tj is in [d].
Passing the expectation over S through the sum in Eq. (15), we have
m
m X
d
X
X
E
stj sTtj xxT stj sTtj =
P(tj = k)rk rTk xxT rk rTk
j=1

=

m X
d
X
j=1 k=1

j=1 k=1
d

pk

X x2
1
T
T
T
k
e
e
xx
e
e
=
ek eTk ,
k
k
k
k
m2 p2k
mpk
k=1

(16)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

and similarly
X

E

P(ti = k)P(tj = q)rk rTk xxT rq rTq

(17)

i6=jâˆˆ[m] k=1 q=1

i6=jâˆˆ[m]

=

d X
d
X X

sti sTti xxT stj sTtj =

d X
d
X

xk xq

k=1 q=1

mâˆ’1 T
mâˆ’1
ek eTq =
xx .
m
m

(18)

Now, combing Eq. (16) with Eq. (18) immediately proves Eq. (7).
Then, Eq. (8) can be proved based on Eq. (7) by

d
X




mâˆ’1 2
1
E D(SST xxT SST ) = D(E SST xxT SST ) =
+
)xk ek eTk .
(
mpk
m

(19)

k=1

Alternatively, D(SST xxT SST ) can be explicitly expanded by
D(SST xxT SST ) =

m
X

d
X

stj sTtj

j=1

x2k ek eTk

m
X

stj sTtj .

(20)

j=1

k=1

Thus, the whole target expectations in Eq. (9), Eq. (10) and Eq. (11) can be explicitly expanded, and we can use similar
ways of proving Eq. (7) to prove the remainder of the lemma.
To prove Eq. (9), we expand
ï£®
ï£¹
m
d
m
X
X
X


E (D(SST xxT SST ))2 = E ï£°(
stj sTtj
x2k ek eTk
stj sTtj )2 ï£»
j=1

ï£®
ï£¹
m
d
m
m
d
m
X
X
X
X
X
X
= Eï£°
stj sTtj
x2k ek eTk
stj sTtj
stj sTtj
x2k ek eTk
stj sTtj ï£»
j=1

=E

m
X

stj sTtj

j=1

+E

m
X

d
X

stj sTtj

d
X

X

sti sTti

X

m
X

stj sTtj

d
X

d
X

d
X

sti sTti

m
X

stj sTtj

j=1

x2k ek eTk stj sTtj

k=1

x2k ek eTk stj sTtj

d
X

x2k ek eTk stj sTtj

(24)

d
X

x2k ek eTk stj sTtj

(25)

k=1

X

sti sTti

i6=jâˆˆ[m]

d
X
k=1

where the four terms in the last equations are calculated as:
(23) = E

m
X

stj sTtj

j=1

=E

m
X
j=1

stj sTtj

d
X
k=1

d
X
k=1

x2k ek eTk stj sTtj

m
X

stj sTtj

j=1

x2k ek eTk stj sTtj stj sTtj

d
X
k=1

(23)

k=1

i6=jâˆˆ[m]

x2k ek eTk stj sTtj

(22)

j=1

k=1

k=1

X

x2k ek eTk stj sTtj

k=1

sti sTti

i6=jâˆˆ[m]

j=1

j=1

k=1

i6=jâˆˆ[m]

+E

x2k ek eTk stj sTtj

k=1

j=1

+E

j=1

k=1

(21)

j=1

k=1

d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stj sTtj

x2k ek eTk stj sTtj ,

(26)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

X

+E

sti sTti

=

m
X

pk1

k1 =1 j=1

d
X

pk1 pq

i6=jâˆˆ[m] k1 =1 q=1

=

d
X

(

k=1

m
X

stj sTtj

stg sTtg

d
X

X

d
X

stg sTtg

g=i6=jâˆˆ[m]

k=1

X

d
X

stg sTtg

g=j6=iâˆˆ[m]

d
X

x2k ek eTk ek1 eTk1

x2k ek eTk ek1 eTk1 eq eTq

k=1

d
X

x2k ek eTk eq eTq

k=1

d
X
k1 ,k3 =1
d
X
k1 ,k2 =1
d
X
k1 =1

X

k=1

x2k ek eTk stg sTtg sti sTti

d
X

x2k ek eTk stj sTtj

k=1
d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stg sTtg sti sTti

d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stg sTtg sti sTti

d
X

x2k ek eTk stj sTtj

k=1

k=1

d

X
X
m(m âˆ’ 1)(m âˆ’ 2)
T
2
T
T
T
e
e
x
e
e
e
e
e
e
x2k ek eTk ek3 eTk3
k
k
k
k
k
k
k
k
k
1
1
2
1
1
2
m4 pk1
k=1

m(m âˆ’ 1)
ek1 eTk1
m4 p2k1
m(m âˆ’ 1)
ek1 eTk1
m4 p2k1

d
X

k=1

x2k ek eTk ek1 eTk1 ek1 eTk1

k=1
d
X

d
X

x2k ek eTk ek3 eTk3

k=1

x2k ek eTk ek1 eTk1 ek2 eTk2

d
X

x2k ek eTk ek1 eTk1

k=1

k=1
d
X
k1 =1

d
X
m(m âˆ’ 1) 4
m(m âˆ’ 1) 4
T
x
xk1 ek1 eTk1
e
e
+
k
k1
1 k1
2
4
m pk1
m4 p2k1
k1 =1


4

m(m âˆ’ 1)(m âˆ’ 2) 4 2m(m âˆ’ 1)xk
xk +
ek eTk ;
m4 pk
m4 p2k

(25) = E

X

sti sTti

d
X

x2k ek eTk stj sTtj

m
X

stj sTtj

j=1

k=1

i6=jâˆˆ[m]
d 
X

sti sTti

i6=jâˆˆ[m]

m(m âˆ’ 1)(m âˆ’ 2) 4
xk1 ek1 eTk1 +
m4 pk1

d 
X

k=1

(27)

d

d
X
k1 ,k2 ,k3 =1

=

d
X
k=1

1
ek eT
m4 p2k1 p2q 1 k1

x2k ek eTk stj sTtj

k=1

+E

=

d
X

g6=i6=jâˆˆ[m]

+E

=

k=1

k=1

X

=E

+

x2k ek eTk ek1 eTk1 ek1 eTk1

1
mâˆ’1
+ 3 2 )x4k ek eTk ;
m3 p3k
m pk

j=1

+

d
X

k=1

(24) = E

=

x2k ek eTk stj sTtj

d
d
X
X
x4k
(m2 âˆ’ m)x4k
T
e
e
+
ek eTk
k
k
3
m3 pk
m4 p2k

k=1

=

d
X
k=1

1
ek eT
m4 p4k1 1 k1

d
X

X

+E

x2k ek eTk sti sTti stj sTtj

k=1

i6=jâˆˆ[m]
d
X

d
X

d
X

(28)

x2k ek eTk stj sTtj

k=1


4

m(m âˆ’ 1)(m âˆ’ 2) 4 2m(m âˆ’ 1)xk
xk +
ek eTk ;
m4 pk
m4 p2k

(26) = E

X
i6=jâˆˆ[m]

sti sTti

d
X
k=1

x2k ek eTk stj sTtj

X
i6=jâˆˆ[m]

sti sTti

d
X
k=1

(29)
x2k ek eTk stj sTtj

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

X

=E

sti sTti

d
X
k=1

i6=j6=g6=hâˆˆ[m]

X

+E

x2k ek eTk stj sTtj stg sTtg

sti sTti

k=1

X

d
X

sti sTti

i6=j,i=h,j6=g,g6=hâˆˆ[m]

k=1

X

d
X

+E

sti sTti

i6=j,i6=g,j=h,g6=hâˆˆ[m]

k=1

X

d
X

+E

sti sTti

i6=j,i6=h,j=g,g6=hâˆˆ[m]

k=1

X

d
X

+E

sti sTti

i6=j,i=g,j=h,g6=hâˆˆ[m]

k=1

X

d
X

+E

sti sTti

=

k=1

d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1
d
X

x2k ek eTk stj sTtj stg sTtg

x2k ek eTk sth sTth

k=1

k=1

i6=j,i=h,j=g,g6=hâˆˆ[m]
d 
X

x2k ek eTk sth sTth

k=1
d
X

i6=j,i=g,j6=h,g6=hâˆˆ[m]

+E

d
X


m(m âˆ’ 1)(m âˆ’ 2)(m âˆ’ 3) 4 4m(m âˆ’ 1)(m âˆ’ 2) 4 2m(m âˆ’ 1) 4
T
x
+
x
x
+
k
k ek ek .
k
m4
m4 pk
m4 p2k

(30)
(31)

Combing the above terms with simplification and reformulation completes the proof of Eq. (9).
Now, we continue to prove Eq. (10).


E SST xxT SST D(SST xxT SST )
ï£¹
ï£®
d
m
m
m
m
X
X
X
X
X
x2k ek eTk
stj sTtj ï£»
stj sTtj
xT stj sTtj
stj sTtj x
= Eï£°

=E

m
X

j=1

j=1

j=1

stj sTtj xxT stj sTtj

m
X

stj sTtj xxT stj sTtj

j=1

+E

d
X

sti sTti

sti sTti xxT stj sTtj

X

m
X

stj sTtj

j=1

sti sTti xxT stj sTtj

i6=jâˆˆ[m]

(32)

d
X

x2k ek eTk stj sTtj

(33)

x2k ek eTk stj sTtj

(34)

k=1

i6=jâˆˆ[m]

X

x2k ek eTk stj sTtj

k=1

m
X

i6=jâˆˆ[m]

+E

stj sTtj

j=1

j=1

+E

m
X

j=1

k=1

d
X
k=1

X

sti sTti

i6=jâˆˆ[m]

d
X

x2k ek eTk stj sTtj ,

(35)

k=1

where we calculate the four terms in the last equation as shown in below:
(32) = E

m
X
j=1

=E

m
X
j=1

stj sTtj xxT stj sTtj

m
X

stj sTtj

j=1

stj sTtj xxT stj sTtj stj sTtj

d
X
k=1

d
X

x2k ek eTk stj sTtj

k=1

x2k ek eTk stj sTtj + E

X
i6=jâˆˆ[m]

sti sTti xxT sti sTti stj sTtj

d
X
k=1

x2k ek eTk stj sTtj

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

=

d X
m
X

d

pk1

k1 =1 j=1

k=1

d
X

X

+E

X
1
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk ek1 eTk1
k
k
k
k
k
k
1
1
1
1
1
1
m4 p4k1
d
X

pk1 pq

i6=jâˆˆ[m] k1 =1 q=1

=

d
X
k=1

=

d
X

x4k
ek eTk +
m3 p3k
1

(

m3 p3k
k=1

(33) = E

m
X

+

d
X
1
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk eq eTq
k
k
q
q
1 k1
m4 p2k1 p2q 1 k1
k=1

d
X
k=1

(m2 âˆ’ m)x4k
ek eTk
m4 p2k

mâˆ’1 4
)x ek eTk ;
m3 p2k k

stj sTtj xxT stj sTtj

j=1

X

stg sTtg xxT stg sTtg sti sTti

X

stg sTtg xxT stg sTtg sti sTti

X

stg sTtg xxT stg sTtg sti sTti

=

+

k1 ,k3 =1
d
X

+

x2k ek eTk stj sTtj

d
X

x2k ek eTk stj sTtj

d
X

x2k ek eTk stj sTtj
d

k1 ,k2 ,k3 =1
d
X

k=1
d
X

k=1

g=j6=iâˆˆ[m]
d
X

x2k ek eTk stj sTtj

k=1

g=i6=jâˆˆ[m]

+E

d
X

k=1

g6=i6=jâˆˆ[m]

+E

sti sTti

i6=jâˆˆ[m]

X

=E

(36)

X
m(m âˆ’ 1)(m âˆ’ 2)
ek1 eTk1 xxT ek1 eTk1 ek2 eTk2
x2k ek eTk ek3 eTk3
4
m pk1
k=1

m(m âˆ’ 1)
ek1 eTk1 xxT ek1 eTk1 ek1 eTk1
m4 p2k1

d
X

x2k ek eTk ek3 eTk3

k=1
d

k1 ,k2 =1

X
m(m âˆ’ 1)
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk ek1 eTk1
k
k
k
k
k
k
1
1
2
2
1
1
2
m4 pk1
k=1

d
d
d
X
X
X
m(m âˆ’ 1) 4
m(m âˆ’ 1) 4
m(m âˆ’ 1)(m âˆ’ 2) 4
T
T
x
e
e
+
x
e
e
+
xk1 ek1 eTk1
=
k
k
k
k
k
k
1
1
1
1
1
1
m4 pk1
m4 p2k1
m4 p2k1
k1 =1

k1 =1

=

d 
X
m(m âˆ’ 1)(m âˆ’ 2)

m4 pk

k=1

(34) = E

X

sti sTti xxT stj sTtj

i6=jâˆˆ[m]

=E

X

x4k +

2m(m âˆ’ 1) 4
xk ek eTk ;
m4 p2k
m
X

X

sti sTti xxT stj sTtj stg sTtg

X
i=g6=jâˆˆ[m]

d
X

x2k ek eTk stj sTtj

k=1
d
X

x2k ek eTk stg sTtg

k=1

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk stg sTtg

k=1

i6=j=gâˆˆ[m]

+E

stj sTtj

j=1

i6=j6=gâˆˆ[m]

+E

k1 =1



sti sTti xxT stj sTtj stg sTtg

d
X
k=1

x2k ek eTk stg sTtg

(37)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™
d

d
X

=

X
m(m âˆ’ 1)(m âˆ’ 2)
T
T
T
T
e
e
xx
e
e
e
e
x2k ek eTk ek3 eTk3
k
k
k
k
k
k
1
2
3
1
2
3
m4 pk3
k=1

k1 ,k2 ,k3 =1
d
X

+

k1 ,k3 =1
d
X

+

d
X
k1 ,k3 =1

=

k=1

d
X
m(m âˆ’ 1)(m âˆ’ 2)
3
T
x
x
e
e
+
k1 k3 k1 k3
m4 pk3

k1 ,k3 =1

X

sti sTti xxT stj sTtj

X

=E

sti sTti xxT stj sTtj stg sTtg

X

+E

X

sti sTti
d
X

X

+E

X

sti sTti xxT stj sTtj stg sTtg

X

sti sTti xxT stj sTtj stg sTtg

X

sti sTti xxT stj sTtj stg sTtg

X
i6=j,i=h,j=g,g6=hâˆˆ[m]

k1 ,k2 ,k3 ,k4 =1

k1 ,k2 ,k4 =1
d
X
k1 ,k2 ,k3 =1
d
X
k1 ,k2 ,k3 =1

x2k ek eTk sth sTth

d
X

x2k ek eTk sth sTth

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk sth sTth

k=1

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk sth sTth

k=1

sti sTti xxT stj sTtj stg sTtg

d
X

x2k ek eTk sth sTth

k=1
d

d
X

d
X

d
X

k=1

i6=j,i=g,j=h,g6=hâˆˆ[m]

+E

x2k ek eTk sth sTth

k=1

i6=j,i6=h,j=g,g6=hâˆˆ[m]

+E

d
X
k=1

i6=j,i6=g,j=h,g6=hâˆˆ[m]

+E

x2k ek eTk stj sTtj

k=1

i6=j,i=h,j6=g,g6=hâˆˆ[m]

+E

d
X

k=1

x4k
ek eTk ;
p2k

x2k ek eTk sth sTth

i6=j,i=g,j6=h,g6=hâˆˆ[m]

+

k3 =1

d
X

k=1

i6=jâˆˆ[m]

i6=j6=g6=hâˆˆ[m]

+

d
X
m(m âˆ’ 1) 4
m(m âˆ’ 1)
3
T
x
x
e
e
+
xk3 ek3 eTk3
k1 k3 k1 k3
m4 p2k3
m4 p2k3

x2
x2
m(m âˆ’ 1)(m âˆ’ 2) T
m(m âˆ’ 1)
m(m âˆ’ 1) T
xx D({ k }) +
xx D({ 2k }) +
4
4
m
pk
m
pk
m4

i6=jâˆˆ[m]

+

k=1

X
m(m âˆ’ 1)
ek3 eTk3 xxT ek2 eTk2 ek3 eTk3
x2k ek eTk ek3 eTk3
2
4
m pk3

(35) = E

=

x2k ek eTk ek3 eTk3

d

k2 ,k3 =1

=

m(m âˆ’ 1)
ek1 eTk1 xxT ek3 eTk3 ek3 eTk3
m4 p2k3

d
X

X
m(m âˆ’ 1)(m âˆ’ 2)(m âˆ’ 3)
xk1 xk2 ek1 eTk2 ek3 eTk3
x2k ek eTk ek4 eTk4
4
m

m(m âˆ’ 1)(m âˆ’ 2)
xk1 xk2 ek1 eTk2 ek1 eTk1
m4 pk1

k=1

d
X

x2k ek eTk ek4 eTk4

k=1
d

X
m(m âˆ’ 1)(m âˆ’ 2)
xk1 xk2 ek1 eTk2 ek3 eTk3
x2k ek eTk ek1 eTk1
4
m pk1
k=1
d

X
m(m âˆ’ 1)(m âˆ’ 2)
xk1 xk2 ek1 eTk2 ek3 eTk3
x2k ek eTk ek2 eTk2
4
m pk2
k=1

(38)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™
d

d
X

+

k1 ,k2 ,k4 =1

+

d
X
k1 ,k2

+

d
X

k1 ,k2 =1

+

k1 =1

+

+

x2k ek eTk ek2 eTk2

k=1

X
m(m âˆ’ 1)
xk1 xk2 ek1 eTk2 ek2 eTk2
x2k ek eTk ek1 eTk1
4
m pk1 pk2
k=1

d
X
m(m âˆ’ 1)(m âˆ’ 2) 4
m(m âˆ’ 1)(m âˆ’ 2)(m âˆ’ 3)
3
T
x
x
e
e
+
xk1 ek1 eTk1
k1 k2 k1 k2
m4
m4 pk1
k1 =1

m(m âˆ’ 1)(m âˆ’ 2) 4
xk1 ek1 eTk1 +
m4 pk1

d
X
k1 ,k2 =1

=

d
X

d

d
X

d
X

k=1

m(m âˆ’ 1)
xk xk ek eT ek eT
m4 pk1 pk2 1 2 1 k2 1 k1

k1 ,k2 =1

=

X
m(m âˆ’ 1)(m âˆ’ 2)
T
T
x
x
e
e
e
e
x2k ek eTk ek4 eTk4
k
k
k
k
k
k
1
2
1
2
2
2
m4 pk2

d
X
k1 ,k2 =1

m(m âˆ’ 1)(m âˆ’ 2)
xk1 x3k2 ek1 eTk2
m4 pk2

d
d
X
X
m(m âˆ’ 1) 4
m(m âˆ’ 1) 4
m(m âˆ’ 1)(m âˆ’ 2)
3
T
T
xk1 xk2 ek1 ek2 +
xk1 ek1 ek1 +
xk1 ek1 eTk1
m4 pk2
m4 p2k1
m4 p2k1
k1 =1

m(m âˆ’ 1)(m âˆ’ 2)(m âˆ’ 3) T
xx D({x2k }) +
m4
d
m(m âˆ’ 1)(m âˆ’ 2) X x4

k

m4

k=1

pk

ek eTk +

k1 =1

d
m(m âˆ’ 1)(m âˆ’ 2) X x4

k

m4

k=1

pk

ek eTk

m(m âˆ’ 1)(m âˆ’ 2) T
x2k
xx
D({
})
m4
pk
d

+

m(m âˆ’ 1)(m âˆ’ 2) T
x2
2m(m âˆ’ 1) X x4k
xx D({ k }) +
ek eTk .
4
m
pk
m4
p2k

(39)

k=1

Combing the above terms with simplification and reformulation completes the proof of Eq. (10).
Finally, we have to prove Eq. (11).
ï£®
ï£¹
m
m
X
X


E (SST xxT SST )2 = E ï£°(
stj sTtj x
xT stj sTtj )2 ï£»
j=1

= E(

m
X

stj sTtj xxT stj sTtj +

j=1

= E(

m
X

j=1

X

sti sTti xxT stj sTtj )2

i6=jâˆˆ[m]

stj sTtj xxT stj sTtj )2

(40)

j=1

+ E(

X

sti sTti xxT stj sTtj )2

(41)

i6=jâˆˆ[m]

+E

m
X
j=1

+E

X

stj sTtj xxT stj sTtj

X

sti sTti xxT stj sTtj

(42)

stj sTtj xxT stj sTtj ,

(43)

i6=jâˆˆ[m]

sti sTti xxT stj sTtj

m
X
j=1

i6=jâˆˆ[m]

where we calculate the four terms in the last equation as shown in below:
(40) = E

m
X
j=1

stj sTtj xxT stj sTtj stj sTtj xxT stj sTtj + E

X
i6=jâˆˆ[m]

sti sTti xxT sti sTti stj sTtj xxT stj sTtj

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

=

d X
m
X

1

pk

m4 p4k

k=1 j=1

ek eTk xxT ek eTk ek eTk xxT ek eTk

d X
d
X X

+E

pk pq

i6=jâˆˆ[m] k=1 q=1

=

d
X

d

k=1

=

d
X

1
ek eTk xxT ek eTk eq eTq xxT eq eTq
4
m p2k p2q

X (m2 âˆ’ m)x4
x4k
k
ek eTk +
ek eTk
3
3
m pk
m4 p2k
k=1

1

(

+

m3 p3k
k=1

mâˆ’1 4
)x ek eTk ;
m3 p2k k

(44)
ï£¹

ï£®
X

(41) = E ï£°

sti sTti xxT stj sTtj

i6=jâˆˆ[m]

X

=E

X

sti sTti xxT stj sTtj ï£»

i6=jâˆˆ[m]

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j6=g6=hâˆˆ[m]

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

X

X

sti sTti xxT stj sTtj stg sTtg xxT sth sTth + E

X

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i6=g,j=h,g6=hâˆˆ[m]

i6=j,i6=h,j=g,g6=hâˆˆ[m]

+E

X

+E

i6=j,i=h,j6=g,g6=hâˆˆ[m]

+E

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i=g,j6=h,g6=hâˆˆ[m]

X

+E

X

+E

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i=g,j=h,g6=hâˆˆ[m]

sti sTti xxT stj sTtj stg sTtg xxT sth sTth

i6=j,i=h,j=g,g6=hâˆˆ[m]
d
X

=

k1 ,k2 ,k3 ,k4 =1

+

d
X
k1 ,k2 ,k4 =1

+

d
X
k1 ,k2 ,k3 =1

+

d
X
k1 ,k2

=

d
X

x2k2

d
X
k1 ,k2 =1

+

d
X
k1 =1

=

m(m âˆ’ 1)(m âˆ’ 2)
xk1 x2k2 xk3 ek1 eTk2 ek3 eTk2 +
m4 pk2

d
X
k1 ,k4 =1

d
X
k1 ,k4 =1

+

m(m âˆ’ 1)(m âˆ’ 2) 2
xk1 xk2 xk4 ek1 eTk2 ek1 eTk4 +
m4 pk1

m(m âˆ’ 1) 2 2
x x ek eT ek eT +
m4 pk1 pk2 k1 k2 1 k2 1 k2

k2 =1

+

m(m âˆ’ 1)(m âˆ’ 2)(m âˆ’ 3)
xk1 xk2 xk3 xk4 ek1 eTk2 ek3 eTk4
m4

d
X
k1 ,k2 =1

d
X
k1 ,k2 ,k3 =1
d
X
k1 ,k2 ,k4 =1

m(m âˆ’ 1)(m âˆ’ 2) 2
xk1 xk2 xk3 ek1 eTk2 ek3 eTk1
m4 pk1
m(m âˆ’ 1)(m âˆ’ 2)
xk1 x2k2 xk4 ek1 eTk2 ek2 eTk4
m4 pk2

m(m âˆ’ 1) 2 2
x x ek eT ek eT
m4 pk1 pk2 k1 k2 1 k2 2 k1

m(m âˆ’ 1)(m âˆ’ 2)(m âˆ’ 3)
xk1 xk4 ek1 eTk4
m4

d
d
X
X
m(m âˆ’ 1)(m âˆ’ 2) 2
m(m âˆ’ 1)(m âˆ’ 2) 3
T
2
x
x
e
e
+
x
xk1 ek1 eTk1
k1 k4 k1 k4
k2
m4 pk1
m4 pk1
k2 =1

m(m âˆ’ 1)(m âˆ’ 2)
xk1 x3k2 ek1 eTk2 +
m4 pk2

m(m âˆ’ 1) 4
xk1 ek1 eTk1 +
m4 p2k1

kxk22 m(m

d
X
k2 =1

x2k2
pk2

d
X
k1 =1

d
X
k2 =1

k1 =1

x2k2
pk2

d
X
k1 ,k4 =1

m(m âˆ’ 1)(m âˆ’ 2)
xk1 xk4 ek1 eTk4
m4

m(m âˆ’ 1) 2
xk1 ek1 eTk1
m4 pk1

x2k
âˆ’ 1)(m âˆ’ 2)(m âˆ’ 3) T m(m âˆ’ 1)(m âˆ’ 2)
xx
+
D({
})xxT
m4
m4
pk

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™
d

+

kxk22 m(m âˆ’ 1)(m âˆ’ 2) X x2k
m(m âˆ’ 1)(m âˆ’ 2) T
x2k
T
xx
e
e
+
D({
})
k
k
m4
pk
m4
pk
k=1

+

d
m(m âˆ’ 1)(m âˆ’ 2) X x2

k

m4

(42) = E

m
X

k=1

stj sTtj xxT stj sTtj

j=1

d
d
d
m(m âˆ’ 1) X x4k
m(m âˆ’ 1) X x2k X x2k
T
e
e
+
ek eTk ;
k
k
m4
p2k
m4
pk
pk
k=1

X

(45)

k=1

sti sTti xxT stj sTtj

stg sTtg xxT stg sTtg sti sTti xxT stj sTtj

X

X

+E

g6=i6=jâˆˆ[m]

+E

k=1

i6=jâˆˆ[m]

X

=E

pk

xxT +

stg sTtg xxT stg sTtg sti sTti xxT stj sTtj

g=i6=jâˆˆ[m]

stg sTtg xxT stg sTtg sti sTti xxT stj sTtj

g=j6=iâˆˆ[m]

=

d
X
k1 ,k2 ,k3 =1

+

d
X
k1 ,k3 =1

=

d
X
k1 ,k3 =1

=

m(m âˆ’ 1)(m âˆ’ 2)
ek1 eTk1 xxT ek1 eTk1 ek2 eTk2 xxT ek3 eTk3
m4 pk1

d
X
m(m âˆ’ 1)
T
T
T
T
T
T
e
e
xx
e
e
e
e
xx
e
e
+
k1 k1
k1 k1 k1 k1
k3 k3
m4 p2k1

k1 ,k2 =1

m(m âˆ’ 1)(m âˆ’ 2) 3
xk1 xk3 ek1 eTk3 +
m4 pk1

d
X
k1 ,k3 =1

m(m âˆ’ 1)
ek1 eTk1 xxT ek1 eTk1 ek2 eTk2 xxT ek1 eTk1
m4 p2k1

d
X
m(m âˆ’ 1) 3
m(m âˆ’ 1) 4
T
x
xk1 ek1 eTk1
x
e
e
+
k1 k3 k1 k3
m4 p2k1
m4 p2k1
k1 =1

x2k
m(m âˆ’ 1)
x2k
m(m âˆ’ 1)
m(m âˆ’ 1)(m âˆ’ 2)
T
D({
})xx
+
D({
})xxT +
m4
pk
m4
p2k
m4

d
X
k=1

x4k
ek eTk ;
p2k

(46)

d

(43) =

m(m âˆ’ 1)(m âˆ’ 2) T
x2
x2
m(m âˆ’ 1) X x4k
m(m âˆ’ 1) T
xx D({ k }) +
xx D({ 2k }) +
ek eTk .
4
4
m
pk
m
pk
m4
p2k

(47)

k=1

Combing the above terms with simplification and reformulation completes the proof of Eq. (11). To this end, we complete
the whole proof.
2.3. Proof of Lemma 2
Proof. According to the setting, we have that
(a)

kSST xxT SST k2 = kSST xk22 = k

m
X

stj sTtj xk22 = k

j=1

=k

m X
d
X
j=1 k=1

Î´ tj k
xk ek k22 =
mpk

d
X

m
X

(

k=1 j=1

m
X
j=1

1
xt et k2
mptj j j 2

m
Î´tj k xk 2 (b) X X Î´tj k xk 2
) =
(
) ,
mpk
mpk
j=1

(48)

kâˆˆÎ“

|Î“|

where we let Î“ = {Î³t }t=1 be a set containing at most m different elements of [d] with its cardinality |Î“| â‰¤ m.
In Eq. (48), (a) follows because SST xxT SST is a positive semidefinite matrix of rank 1, Î´tj k returns 1 only when tj = k
and 0 otherwise, and P(Î´tj k = 1) = P(tj = k) = pk . (b) holds due to that we perform random sampling with replacement
m times on the d entries of x âˆˆ Rd , and consequently at most m certain different entries from x are sampled.
Pm Î´tj Î³1 xÎ³1
Î´tj Î³1 xÎ³1
xÎ³1
Let k = Î³1 with Î³1 âˆˆ Î“, and we first bound | j=1 mp
for all
|. Define a random variable aj = mp
âˆ’ m
Î³1
Î³1
m
j âˆˆ [m]. We can easily check that {aj }j=1 are independent with E [aj ] = 0, so that we can leverage Theorem 3 to
continue our following analysis. We see that
max |aj | = max{

jâˆˆ[m]

|xÎ³1 | 1
|xÎ³1 |
|xÎ³1 |
(
âˆ’ 1),
}â‰¤
,
m p Î³1
m
mpÎ³1

(49)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

and
m
X
 
x2Î³
x2Î³1
âˆ’ 1.
E a2j =
mpÎ³1
m
j=1

Thus, applying Theorem 3 with R =

P(|

m
X

|xÎ³1 |
mpÎ³1

and Ïƒ 2 =

aj | â‰¥ ) â‰¤ 2 exp(

j=1

x2Î³
1
mpÎ³1

âˆ’

x2Î³
1
m

(50)

obtains that

âˆ’2 /2
),
x2Î³1 /(mpÎ³1 ) âˆ’ x2Î³1 /m + |xÎ³1 |/(3mpÎ³1 )

Pm
Pm
whose RHS is denoted by Î·Î³1 . Then, with probability at least 1 âˆ’ Î·Î³1 we have | j=1 aj | â‰¤ , i.e., | j=1
|xÎ³1 | + . We then replace  by other variables to obtain that
"
#
s
1
2
2
|xÎ³1 |
1
1
+
|xÎ³1 | +  = |xÎ³1 | + log(
)
+ |xÎ³1 |
(
âˆ’ ) ,
Î·Î³1 3mpÎ³1
9m2 p2Î³1
log(2/Î·Î³1 ) mpÎ³1
m
which is denoted by f (xÎ³1 , Î·Î³1 , m).
Pm
In a similar way, we can bound | j=1
over cases for all k âˆˆ [d].

Î´tj k xk
mpk |

(51)
Î´tj Î³1 xÎ³1
mp1

|â‰¤

(52)

for any other k âˆˆ [d]. The lemma then follows by using the union bound

2.4. Proof of Theorem 1
b1 âˆ’ C
b 2,
Proof. We have to prove
that the unbiased estimator for original
covariance matrix C is Eq. (1), i.e., Ce = C
1
b 1 = m Pn Si ST xi xT Si ST , and C
b 2 = m Pn D(Si ST xi xT Si ST )D(bi ) with bki =
where C
i
i
i
i
i
i
i=1
i=1
mnâˆ’n
mnâˆ’n
1+(mâˆ’1)pki .
n
Note that each Si is created by running Algorithm 1, and {Si }i=1 are independent matrices. Thus, taking all summands


b 1,
E Si STi xi xTi Si STi together and leveraging Eq. (7) in Lemma 1 achieves the expectation of C
b 1] =
E[C

" d
#
n
n
X
m X X x2ki
m
mâˆ’1
T
T
T
T
T
Si Si xi xi Si Si =
E
xi xi
ek ek +
nm âˆ’ n i=1
nm âˆ’ n i=1
mpki
m
k=1

=

1
nm âˆ’ n

d
n X
X
i=1 k=1

x2ki
pki

ek eTk +

1
XXT .
n

(53)

b 1 is a biased estimator for the original covariance matrix C = 1 XXT = 1 Pn xi xT . We still
Eq. (53) indicates that C
i
i=1
n
n
b 1 to get an unbiased estimator. By Eq. (8) in Lemma 1, it can be shown that
need to apply a debiasing procedure to C
b 2] =
E[C

n
n X
d
X

m X 
1
x2ki
E D(Si STi xi xTi Si STi ) D(bi ) =
ek eTk .
nm âˆ’ n i=1
nm âˆ’ n i=1
pki

(54)

k=1

b1 âˆ’ C
b 2 is unbiased for C.
Combing Eq. (53) with Eq. (54), we immediately see that Ce = C
2.5. Proof of Theorem 2
Proof. Here, we have to bound the error kCe âˆ’Ck2 . To make the representation compact, we define Ai = Ai1 âˆ’Ai2 âˆ’Ai3
T
T
T
T
Pn
mSi ST
mD(Si ST
x xT
i xi xi Si Si
i xi xi Si Si )D(bi )
with Ai1 =
, A i2 =
, Ai3 = in i . Then, i=1 Ai = Ce âˆ’ C holds. It
nmâˆ’n
nmâˆ’n
is straightforward to see that {Ai }ni=1 are independent zero-mean random matrices, which are exactly the setting of the
Matrix Bernstein inequality, as shown in Theorem 3. To bound kCe âˆ’Ck2 via Theorem 3, we need to calculate the relevant
parameters R and Ïƒ 2 that characterize the range and variance of Ai respectively.
We first derive R by bounding kAi k2 so that kAi k2 â‰¤ R for all i âˆˆ [n]. Expanding kAi k2 gets that
kAi k2 = kAi1 âˆ’ Ai2 âˆ’ Ai3 k2 â‰¤ kAi1 âˆ’ Ai2 k2 + kAi3 k2

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

â‰¤kAi1 k2 + kAi3 k2 .

(55)

The last inequality in Eq. (55) results from
kAi1 âˆ’ Ai2 k2 = max |Î»k (Ai1 âˆ’ Ai2 )|
kâˆˆ[d]

(a)

â‰¤ max{|Î»d (Ai1 ) âˆ’ Î»1 (Ai2 )|, |Î»1 (Ai1 ) âˆ’ Î»d (Ai2 )|}

(56)

(b)

= max{Î»1 (Ai2 ), |Î»1 (Ai1 ) âˆ’ Î»d (Ai2 )|}

(57)

(c)

= max{Î»1 (Ai2 ), Î»1 (Ai1 ) âˆ’ Î»d (Ai2 )}

(58)

(d)

â‰¤ Î»1 (Ai1 )

(59)

(e)

= kAi1 k2 ,

(60)

where Î»k (Â·) is the k-th largest eigenvalue.
(a) follows from that Î»k (Ai1 ) âˆ’ Î»1 (Ai2 ) â‰¤ Î»k (Ai1 âˆ’ Ai2 ) â‰¤ Î»k (Ai1 ) âˆ’ Î»d (Ai2 ) for any k âˆˆ [d], which can be proved
by combining Theorem 4 with the fact that Î»d (âˆ’Ai2 ) = âˆ’Î»1 (Ai2 ) and Î»1 (âˆ’Ai2 ) = âˆ’Î»d (Ai2 ) for Ai2 âˆˆ RdÃ—d .
(b) holds because of that Î»kâ‰¥2 (Ai1 ) = 0 since Ai1 is a positive semidefinite matrix of rank 1, and Î»kâˆˆ[d] (Ai2 ) â‰¥ 0 since
Ai2 is positive semidefinite.
Pd
(c) follows owing to that Î»1 (Ai1 ) = Tr(Ai1 ) â‰¥ Tr(Ai2 ) = k=1 Î»k (Ai2 ) â‰¥ Î»d (Ai2 ) â‰¥ 0, where the first equality
holds because Î»kâ‰¥2 (Ai1 ) = 0, the first inequality results from the fact that the diagonal matrix Ai2 is constructed by the
diagonal elements of Ai1 multiplied by positive scalars not bigger than 1, and the second inequality is the consequence of
Î»kâˆˆ[d] (Ai2 ) â‰¥ 0.
(d) results from that Î»kâˆˆ[d] (Ai2 ) â‰¥ 0.
(e) follows owing to that Ai1 is positive semidefinite.
Now, we only need to bound kAi1 k2 and kAi3 k2 . We have that
kxi k22
xi xTi
k2 =
.
n
n
Pd
Then, Lemma 2 reveals that with probability at least 1 âˆ’ k=1 Î·ki ,
kAi3 k2 = k

kAi1 k2 â‰¤

(61)

X
m
f 2 (xki , Î·ki , m),
nm âˆ’ n

(62)

kâˆˆÎ“i

|Î“ |

i
where Î“i = {Î³ti }t=1
is a set occupying
at most
its cardinality |Î“i | â‰¤ m, and
h
i
q m different elements of [d] with
|xki |
2
1
2
1
1
f (xki , Î·ki , m) = |xki | + log( Î·ki ) 3mpki + |xki | 9m2 p2 + log(2/Î·ki ) ( mpki âˆ’ m ) .
ki

{xi }ni=1 .

Then, by union bound, with probability at least 1 âˆ’
"
#
X
m
kxi k22
2
R = max
f (xki , Î·ki , m) +
.
n
iâˆˆ[n] nm âˆ’ n

We derive the similar results for all

Pn

i=1

Pd

k=1

Î·ki , we have
(63)

kâˆˆÎ“i

Pn
Pn
Applying the well known inequality ( t=1 at )2 â‰¤ n t=1 a2t , we have
f 2 (xki , Î·ki , m) â‰¤ 3x2ki + 3 log2 (
â‰¤ 3x2ki + log2 (

2
x2
2
x2
2
x2
x2
) 2ki 2 + 3 log2 (
) 2ki 2 + 6 log(
)( ki âˆ’ ki )
Î·ki 9m pki
Î·ki 9m pki
Î·ki mpki
m

2
2x2
2 6x2ki
) 2ki2 + log(
)
.
Î·ki 3m pki
Î·ki mpki

(64)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™
x2

|xki |
+ (1 âˆ’ Î±) kxki
Before continuing characterizing R in Eq. (63), we set the sampling probabilities as pki = Î± kx
2 . It is
i k1
i k2
Pd
|xki |
easy to check that k=1 pki = 1. For 0 < Î± < 1, we also have pki â‰¥ Î± kxi k1 , then plugging it in the second and third
term of Eq. (64) respectively getting that

f 2 (xki , Î·ki , m) â‰¤ 3x2ki + log2 (

2 2kxi k21
2 6|xki |kxi k1
+ log(
)
)
.
Î·ki 3m2 Î±2
Î·ki
mÎ±

(65)

Î·
for all i âˆˆ [n] and k âˆˆ [d], we bound R with probability at least 1 âˆ’
Equipped with Eq. (63) and setting Î·ki = nd
Pn Pd
i=1
k=1 Î·ki = 1 âˆ’ Î· by
"
#
2
X
2nd 6|xki |kxi k1  kxi k22
m
2 2nd 2kxi k1
2
3xki + log (
+ log(
+
R â‰¤ max
)
)
Î· 3m2 Î±2
Î·
mÎ±
n
iâˆˆ[n] nm âˆ’ n
kâˆˆÎ“i
 


2
2nd 2kxi k21
2nd 6kxi k21
kxi k22
â‰¤ max
3kxi k22 + log2 (
+ log(
+
)
)
2
Î· 3mÎ±
Î·
mÎ±
n
iâˆˆ[n] n


2
2
7kxi k2
2nd 14kxi k1
,
(66)
â‰¤ max
+ log2 (
)
n
Î·
nmÎ±2
iâˆˆ[n]
m
where the second inequality follows from that mâˆ’1
â‰¤ 2 for m â‰¥ 2 and |Î“i | â‰¤ m, and the last inequality results from that
2nd
Î± â‰¤ 1 and log( Î· ) â‰¥ 1 for n â‰¥ 1, d â‰¥ 2, and Î· â‰¤ 1.
Pn
At this stage, we have to derive Ïƒ 2 by only bounding for k i=1 E [Ai Ai ] k2 since Ai is symmetric. Expanding E [Ai Ai ]
obtains that

0  E [Ai Ai ] = E [Ai1 Ai1 + Ai2 Ai2 + Ai3 Ai3 âˆ’ Ai1 Ai2 âˆ’ Ai2 Ai1
âˆ’Ai1 Ai3 âˆ’ Ai3 Ai1 + Ai2 Ai3 + Ai3 Ai2 ] ,
in RHS of which, we bound the expectation of each term. Specifically, invoking Lemma 1, we have that
d 
X


1
4
+
x4 ek eTk
n E [Ai Ai ] =
m(m âˆ’ 1)p2ki
(m âˆ’ 1)2 mp3ki ki
k=1
{z
}
|
1

#
#
"
"
d
d
d
2
2
2
X
X
X
1
x2ki x2ki
m
âˆ’
2
x
kxi k22 (m âˆ’ 2)
kx
k
(m
âˆ’
5m
+
6)
i
2
ki
+
ek eTk +
+
xi xTi
+
m(m âˆ’ 1)
m(m âˆ’ 1)
pki pki
m(m âˆ’ 1)
m(m âˆ’ 1)
pki
k=1
k=1
k=1
|
{z
} |
{z
}
2
3


2

x2
1
x2
x2
1
x2
2(m âˆ’ 2)
2(m âˆ’ 2)
xi xTi D({ ki }) +
xi xTi D({ 2ki }) +
D({ ki })xi xTi +
D({ 2ki })xi xTi
m(m âˆ’ 1)
pki
m(m âˆ’ 1)
pki
m(m âˆ’ 1)
pki
m(m âˆ’ 1)
p
|
{z
} |
{z
} |
{z
} |
{z ki
}
4
6


7
5



d 
X
1
7
6(m âˆ’ 2)
(m âˆ’ 2)(m âˆ’ 3) 4
+
xki ek eTk
+ D(bi )D(bi )
+
+
m(m âˆ’ 1)2 p3ki
m(m âˆ’ 1)p2ki
m(m âˆ’ 1)pki
m(m âˆ’ 1)
k=1
|
{z
}
8

+

d
d
X
X
1
1
+ kxi k22 xi xTi +
+ 1)x2ki ek eTk D(bi )xi xTi + xi xTi
+ 1)x2ki ek eTk D(bi )
(
(
| {z }
(m âˆ’ 1)pki
(m âˆ’ 1)pki
k=1
k=1
9

|
{z
} |
{z
}
10
11



d 
X
1
6
3(m âˆ’ 2)
3(m âˆ’ 2)
x2ki
4
T
T
âˆ’2
+
+
x
e
e
D(b
)
âˆ’
x
x
D({
})D(bi )
k
i
i
k
i
m(m âˆ’ 1)2 p3ki
m(m âˆ’ 1)p2ki
m(m âˆ’ 1)pki ki
m(m âˆ’ 1)
pki
k=1
|
{z
}
|
{z
}
13

12


Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

âˆ’

âˆ’

âˆ’

(m âˆ’ 2)(m âˆ’ 3)
3(m âˆ’ 2)
(m âˆ’ 2)(m âˆ’ 3)
x2
xi xTi D({x2ki })D(bi ) âˆ’
D(bi )D({ ki })xi xTi âˆ’
D(bi )D({x2ki })xi xTi
m(m âˆ’ 1)
m(m âˆ’ 1)
pki
m(m âˆ’ 1)
{z
} |
{z
} |
{z
}
|
14
15
16



d
X
1
x2ki
x2ki
x2
ek eTk xi xTi âˆ’ kxi k22 xi xTi âˆ’
xi xTi ek eTk âˆ’ kxi k22 xi xTi âˆ’
xi xTi D({ 2ki })D(bi )
| {z }
| {z } m(m âˆ’ 1)
(m âˆ’ 1)pki
(m âˆ’ 1)pki
pki
k=1
k=1
|
{z
}
18
20


{z
}
{z
}
|
|
21

17
19


d
X

x2
1
D(bi )D({ 2ki })xi xTi .
m(m âˆ’ 1)
pki
|
{z
}
22


(67)

x2

x2

ki d
ki
}) is to denote a square diagonal matrix in RdÃ—d with { pki
}k=1 on its diagonal,
Because of the limited space, D({ pki
which is also extended to other similar notations.

In Eq. (67), it can be checked that for m â‰¥ 2, we have
10 âˆ’ 
17 = 0;

11 âˆ’ 
19 = 0;


xi xTi
((m âˆ’ 1)/pki )x2ki
(m âˆ’ 2)(m + 1 âˆ’ 1/pki )x2ki
D({
+
});
m(m âˆ’ 1)
1 + (m âˆ’ 1)pki
1 + (m âˆ’ 1)pki
(m âˆ’ 2)(m + 1 âˆ’ 1/pki )x2ki
xi xTi
((m âˆ’ 1)/pki )x2ki
6 âˆ’
15 + 
7 âˆ’
16 âˆ’ 
22 = D({
+
})
;

1 + (m âˆ’ 1)pki
1 + (m âˆ’ 1)pki
m(m âˆ’ 1)
"
#
d
(6 âˆ’ 4m)kxi k22
m âˆ’ 2 X x2ki
3 +
9 âˆ’
18 âˆ’ 
20 =

+
xi xTi
m(m âˆ’ 1)
m(m âˆ’ 1)
pki

4 âˆ’
13 + 
5 âˆ’
14 âˆ’ 
21 =


k=1

d
1 X x2ki

xi xTi ;
m
pki
k=1

8 âˆ’
12  0;



d 
X
4x4ki
8x4ki
1 
+ 3 3 ek eTk ;

m2 p2ki
m pki
k=1
"
#
d
d
X
kxi k22 x2ki
2x2ki X x2ki
2 

+ 2
ek eTk .
mpki
m pki
pki
k=1

(68)

k=1

Then, applying Eq. (67) and Eq. (68) obtains that
"
#
d
d
4x4ki
kxi k22 x2ki
2x2ki X x2ki
1 X 8x4ki
+ 3 3 +
+ 2
ek eTk
0  E [Ai Ai ]  2
n
m2 p2ki
m pki
mpki
m pki
pki
k=1

xi xTi

1)/pki )x2ki

k=1
1/pki )x2ki

((m âˆ’
(m âˆ’ 2)(m + 1 âˆ’
D({
+
})
n2 m(m âˆ’ 1)
1 + (m âˆ’ 1)pki
1 + (m âˆ’ 1)pki
((m âˆ’ 1)/pki )x2ki
(m âˆ’ 2)(m + 1 âˆ’ 1/pki )x2ki
xi xTi
+ D({
+
}) 2
1 + (m âˆ’ 1)pki
1 + (m âˆ’ 1)pki
n m(m âˆ’ 1)
+

d

+

1 X x2ki
xi xTi .
2
n m
pki

(69)

k=1

With Eq. (69) in hand, we can formulate Ïƒ 2 as
"
#
n
n
d
X
X
1
8x4ki
4x4ki
kxi k22 x2ki
2x2ki X x2ki
2
Ïƒ =k
E [Ai Ai ] k2 â‰¤
max 2
+ 3 3 +
+ 2
m2 p2ki
m pki
mpki
m pki
pki
kâˆˆ[d] n
i=1
i=1
k=1

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™
n
X



n X
d
X
2kxi k22 ((m âˆ’ 1)/pki )x2ki
(m âˆ’ 2)(m + 1 + 1/pki )x2ki
1
1
x2ki
+
(
+
) + 2 k
xi xTi k2
max 2
m(m
âˆ’
1)
1
+
(m
âˆ’
1)p
1
+
(m
âˆ’
1)p
n
m
p
kâˆˆ[d] n
ki
ki
ki
i=1
i=1 k=1
"
#
n
d
4
2 2
4
2 X
2
X
4x
kxi k2 xki
8xki
2x
1
xki
+ 3 ki3 +
â‰¤
+ 2 ki
max 2
2 p2
m
m
p
mp
m
p
pki
kâˆˆ[d] n
ki
ki
ki
ki
i=1
k=1


n X
n
d
X
X
1
1 8kxi k22 x2ki
x2ki
+ 2 k
xi xTi k2 .
(70)
+
max 2
mp
n
m
p
kâˆˆ[d] n
ki
ki
i=1
i=1
k=1

x2

|xki |
Again, we have to consider the sampling distributions pki = Î± kx
+ (1 âˆ’ Î±) kxki
2 with 0 < Î± < 1. Plugging pki â‰¥
i k1
ik
2

x2

|xki |
and pki â‰¥ (1 âˆ’ Î±) kxki
Î± kx
2 in Eq. (70), we have
i k1
ik
2

"

d
4kxi k21 kxi k22
8kxi k42
kxi k42
2kxi k22 X |xki |kxi k1
+
+
+
m2 (1 âˆ’ Î±)2
m3 Î±2 (1 âˆ’ Î±) m(1 âˆ’ Î±) m2 (1 âˆ’ Î±)
Î±
k=1


n
n X
d
X
X
1
8kxi k42
1
|xki |kxi k1
+
max 2
xi xTi k2
+ 2 k
m(1
âˆ’
Î±)
n
m
Î±
kâˆˆ[d] n
i=1
i=1 k=1

n 
4
2
X
8kxi k2
4kxi k1 kxi k22
9kxi k42
2kxi k22 kxi k21
=
+ 2 3 2
+
+
n2 m2 (1 âˆ’ Î±)2
n m Î± (1 âˆ’ Î±) n2 m(1 âˆ’ Î±) n2 m2 Î±(1 âˆ’ Î±)
i=1
n
X

1
Ïƒ â‰¤
max 2
kâˆˆ[d] n
i=1
2

+k

n
X
kxi k2 xi x2
1

i=1

i

n2 mÎ±

#

k2 .

(71)
4x4

4/3

Note that employing pki = â„¦( Pd|xki|x|

) for the term m3 pki3 in Eq. (70) can produce a result tighter than that in
ki
k=1
Pd
Eq. (71), which is because of the fact that ( k=1 |xki |4/3 )3 â‰¤ kxi k21 kxi k22 always holds owing to the Holderâ€™s in4/3
ki |

4/3

equality. However, it is not necessary to apply pki = â„¦( Pd|xki|x|
4kxi k21 kxi k22
n2 m3 Î±2 (1âˆ’Î±)

to the term
2kxi k22 kxi k21
n2 m2 Î±(1âˆ’Î±)

kx k2 kx k2
O( in21m3i 2 )

=

4x4ki
m3 p3ki

k=1

4/3
ki |

) to the term

in Eq. (71) obtained by applying pki =

|xki |
Î± kx
i k1

4x4ki
m3 p3ki

in Eq. (70), because the term
x2

|xki |
+ (1 âˆ’ Î±) kxki
+
2 = â„¦( kx k
i 1
ik
2

x2ki
)
kxi k22

in Eq. (70) has already been small enough, which can be smaller than other terms in Eq. (71) like
kx k2 kx k2

q

= O( in21m2i 2 ). Similarly, applying other sampling probabilities pki = â„¦( Pd|xki|x| |q ) with q 6= 1, 34 , 2
ki
k=1
to Eq. (70) will produce a result larger than Eq. (71), which may not be bounded. This is also why we only use
x2
|xki |
|xki |
pki = Î± kx
+ (1 âˆ’ Î±) kxki
2 = â„¦( kx k ) to tighten R in Eq. (66). This derivation justifies our selection of q = 1, 2 in
i k1
i 1
ik
2

q

pki = â„¦( Pd|xki|x|
k=1

ki

x2

|xki |
) used for constructing the sampling probability pki = Î± kx
+ (1 âˆ’ Î±) kxki
2.
|q
i k1
ik
2

We then invoke Theorem 3 to obtain that for  â‰¥ 0,
P(kCe âˆ’ Ck2 â‰¥ ) â‰¤ 2d exp(

âˆ’2 /2
).
Ïƒ 2 + R/3

(72)

2

/2
Denote the RHS of Eq. (72) by Î´ = 2d exp( Ïƒ2âˆ’
+R/3 ) and consider the failure probability Î· in Eq. (66), then by union
2

/2
bound we have kCe âˆ’ Ck2 â‰¤  holds with probability at least 1 âˆ’ Î· âˆ’ Î´. Furthermore, Î´ = 2d exp( Ïƒ2âˆ’
+R/3 ) yields the
following quadratic equation in 

2
R
âˆ’
âˆ’ Ïƒ 2 = 0.
2 log(2d/Î´)
3
Solving Eq. (73) gets only one positive root
s
"
#
R 2
2d R
2Ïƒ 2
 = log( )
+ ( ) +
Î´
3
3
log(2d/Î´)

(73)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

2d 2R
â‰¤ log( )
+
Î´ 3
2R
Thus, immediately we have kCe âˆ’ Ck2 â‰¤ log( 2d
Î´ ) 3 +
completes the whole proof.

r
2Ïƒ 2 log(

2d
).
Î´

(74)

q
2Ïƒ 2 log( 2d
Î´ ) holds with probability at least 1 âˆ’ Î· âˆ’ Î´, which

2.6. Proof of Corollary 1
âˆš
i k1
d, and m < d into
Proof. According to the setting, substituting that kxi k2 â‰¤ Ï„ for all i âˆˆ [n], kx
kxi k2 â‰¤ Ï• with 1 â‰¤ Ï• â‰¤
Theorem 2 establishes that
r
Ï„2
Ï„4
Ï„ 4 Ï•2
Ï„4
kCk2 Ï„ 2 Ï•2 
Ï„ 2 Ï•2
Ï„ 4 Ï•2
e
+
+
+
kCe âˆ’ Ck2 â‰¤ O
+
+
+
n
nm
nm2
nm3
nm
nm2
nm
r
r
r
Ï„2

2 2
2
1
kCk2
Ï„ Ï•
Ï„ Ï• 1
e
â‰¤O
,
(75)
+
+
+ Ï„2
+ Ï„Ï•
n
nm
m
n
nm
nm
Pn
Pn x xT
where the first inequality invokes i=n kxi k42 â‰¤ nÏ„ 4 , and C = i=1 in i is the original covariance matrix.
Pn
Pn
Pn
Pn
Also, we can adopt i=1 kxi k42 â‰¤ ndÏ„ 2 kCk2 , which holds because i=1 kxi k42 â‰¤ Ï„ 2 i=1 kxi k22 and i=1 kxi k22 =
nTr(C) â‰¤ ndkCk2 .
Hence, we have
 2
e Ï„ +
kCe âˆ’ Ck2 â‰¤ O
n
Ï„2
e
+
â‰¤O
n

r
p
Ï„ 2 Ï•2
d
dÏ•2
d
dÏ•2
Ï•2 
+ Ï„ kCk2
+
+
+
+
nm
nm2
nm3
nm nm2
nm
r
r
r
2 2
Ï„ Ï•
Ï„ Ï• dkCk2
dkCk2
kCk2 
+
+Ï„
+ Ï„Ï•
.
nm
m
n
nm
nm

(76)

Finally, assigning kCe âˆ’ Ck2 by the smaller one of Eq. (75) and Eq. (76) completes the proof.
2.7. Proof of Corollaries 2 and 3
Proof. The proof
Pn follows (AzizyanPetn al., 2015, Corollaries 4-6), where the key component kCe âˆ’ Cp k2 is upper bounded
by kCe âˆ’ n1 i=1 xi xTi k2 + k n1 i=1 xi xTi âˆ’ Cp k2 . Then, the derivation results from Theorem 2 in our paper and the
Gaussian tail bounds in (Azizyan et al., 2015, Proposition 14).
(Azizyan et al., 2015, Proposition 14) shows that with probability at least 1 âˆ’ Î¶ for d â‰¥ 2,
q
max kxi k2 â‰¤ 2Tr(Cp ) log(nd/Î¶);
iâˆˆ[n]

k

1
n

n
X

xi xTi âˆ’ Cp k2 â‰¤ O kCp k2

p

log(2/Î¶)/n .

(77)

i=1

Then, applying them and Corollary 1 along with the fact that kxi k1 â‰¤
n

âˆš

dkxi k2 and Tr(Cp ) â‰¤ dkCp k2 establishes

n

1X
1X
xi xTi k2 + k
xi xTi âˆ’ Cp k2
n i=1
n i=1
s
r
r
r 
Pn
Ï„2
2 2
2
k n1 i=1 xi xTi k2  e 
Ï„
Ï•
Ï„
Ï•
1
1
1
2
e
â‰¤O
+
+
+Ï„
+ Ï„Ï•
+ O kCp k2
n
nm
m
n
nm
nm
n
r
r
r
r 

Ï„2

2 2
2
Ï„
Ï•
Ï„
Ï•
1
1
kC
k
p
2
2
e
e kCp k2 1
+
+
+Ï„
+ Ï„Ï•
â‰¤O
+O
n
nm
m
n
nm
nm
n
r
r
r
r 
 d2 kC k
dkCp k2 d
1
1
1
p 2
e
+
+ dkCp k2
+ dkCp k2
+ kCp k2
â‰¤O
nm
m
n
nm
nm
n

kCe âˆ’ Cp k2 â‰¤ kCe âˆ’

(78)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

r 
 d2 kC k
d
dkC
k
p
2
p
2
e
â‰¤O
+
nm
m
n

(79)

with probability at least 1 âˆ’ Î· âˆ’ Î´ âˆ’ Î¶, where Eq. (78) results from that we invoke Eq. (77) to get k n1
Pn
e
k n1 i=1 xi xTi âˆ’ Cp k2 + kCp k2 â‰¤ O(kC
p k2 ).

Pn

i=1

xi xTi k2 â‰¤

The proof for the low-rank case where rank(Cp )â‰¤ r additionally adopts
k[Ce ]r âˆ’ Cp k2 â‰¤ k[Ce ]r âˆ’ Ce k2 + kCe âˆ’ Cp k2
â‰¤ k[Cp ]r âˆ’ Ce k2 + kCe âˆ’ Cp k2
â‰¤ k[Cp ]r âˆ’ Cp k2 + kCp âˆ’ Ce k2 + kCe âˆ’ Cp k2
= 2kCe âˆ’ Cp k2 ,

(80)

where the last equality holds because rank(Cp ) â‰¤ r. Then, armed with Tr(Cp ) â‰¤ rank(Cp )kCp k2 â‰¤ rkCp k2 , we have
n

n

1X
1X
xi xTi k2 + k
xi xTi âˆ’ Cp k2 )
n i=1
n i=1
r
r
r 
1
rd
1
+ rkCp k2
+ kCp k2
+ kCp k2
nm
nm
n
r
rd 
+ kCp k2
nm

k[Ce ]r âˆ’ Cp k2 â‰¤ O(kCe âˆ’ Cp k2 ) â‰¤ O(kCe âˆ’
r
 rdkC k
rkC
k
d
p
2
p
2
e
+
â‰¤O
nm
m
n
r
 rdkC k
rkC
k
d
p
2
p
2
e
+
â‰¤O
nm
m
n

(81)

with probability at least 1 âˆ’ Î· âˆ’ Î´ âˆ’ Î¶.
The given definitions also implicitly indicate that Cp and Ce are symmetric. Then, following (Azizyan et al., 2015), the
desired bound in Corollary 3 immediately results from Corollary 2 combined with the Davis-Kahan Theorem (Davis &
Q
Q
1
kCe âˆ’ Cp k2 .
Kahan, 1970) that shows k b k âˆ’ k k2 â‰¤ Î»k âˆ’Î»
k+1

3. Discussion for Counterparts
3.1. Theorems for Gauss-Inverse and UniSample-HD
We first use our notations to rephrase current theoretical results provided in (Azizyan et al., 2015, Theorem 3) and (Anaraki
& Becker, 2017, Theorem 6), which correspond to Gauss-Inverse and UniSample-HD, respectively.
Theorem 5 (Azizyan et al. 2015, Theorem 3). Let d â‰¥ 2 and define,
n

n

1X
1X
S1 = k
kxi k22 xi xTi k2 , S2 =
kxi k42 .
n i=1
n i=1
There exists universal constants Îº1 , Îº2 > 0 such that for any 0 < Î´ < 1, with probability at least 1 âˆ’ Î´,
r
r
r d
 log(d/Î´)
d maxiâˆˆ[n] kxi k22
d
kCe âˆ’ Ck2 â‰¤ Îº1
S1 +
S
+
Îº
log(d/Î´).
2
2
m
m2
n
nm

(82)

Theorem 6 (Anaraki & Becker 2017, Theorem 6). Let each column of Si âˆˆ RdÃ—m be chosen uniformly at random from
the set of all canonical basis vectors without replacement. Let Ï > 0 be a bound such that kSi STi xi k22 â‰¤ Ïkxi k22 for all
i âˆˆ [n]. Then, with probability at least 1 âˆ’ Î´
kCe âˆ’ Ck2 â‰¤ ,
(83)


h

i
2
/2
d(dâˆ’1)
d(dâˆ’m)
1
2
2
2
where Î´ = d exp Ïƒ2âˆ’
=
+R/3 , R = n
m(mâˆ’1) Ï + 1 maxiâˆˆ[n] kxi k2 + m(mâˆ’1) maxkâˆˆ[d],iâˆˆ[n] xki , and Ïƒ
h
d(dâˆ’1)
m(mâˆ’1)
dâˆ’m
2
2
nm(mâˆ’1) (Ï âˆ’ d(dâˆ’1) ) maxiâˆˆ[n] kxi k2 kCk2 + mâˆ’1 Ï maxiâˆˆ[n] kxi k2 kD(C)k2
i
P
4
(dâˆ’m)2 maxkâˆˆ[d] n
2(dâˆ’m)kXk2
i=1 xki
+ n(mâˆ’1) F maxkâˆˆ[d],iâˆˆ[n] x2ki +
.
n(dâˆ’1)(mâˆ’1)

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

3.2. Discussion
In this subsection, we will simplify the foregoing two theorems by making Eq. (82) and Eq. (83) explicitly dependent on
n, m and d. Our derivations are natural and straightforward, and we will not deliberately loose Eq. (82) and Eq. (83) in
order to demonstrate the superiority of the theoretical results gained by our weighted sampling method. We define that
maxiâˆˆn kxi k2 â‰¤ Ï„ .
1
kXk2F â‰¤ kCk2 â‰¤
In terms of Eq. (82) in Theorem 5, S1 â‰¤ maxiâˆˆ[n] kxi k22 kCk2 and S2 â‰¤ maxiâˆˆ[n] kxi k42 . Note that nd
2
maxiâˆˆ[n] kxi k2 . Then, Eq. (82) can be simplified and reformulated as
s
s
 dkCk max
2
kx
k
d maxiâˆˆ[n] kxi k42
d maxiâˆˆ[n] kxi k22 
2
i 2
iâˆˆ[n]
e
+
kCe âˆ’ Ck2 â‰¤ O
+
2
nm
nm
nm
r
r

2
2 
Ï„ d
d
e Ï„ dkCk2 + Ï„
.
(84)
â‰¤O
+
nm
m n nm

If applying S2 â‰¤ d maxiâˆˆ[n] kxi k22 kCk2 in the original paper (Azizyan et al., 2015), we will get that
s
s
 dkCk max
2
kx
k
d2 maxiâˆˆ[n] kxi k22 kCk2
d maxiâˆˆ[n] kxi k22 
2
i
iâˆˆ[n]
2
e
kCe âˆ’ Ck2 â‰¤ O
+
+
nm
nm2
nm
r

2 
e Ï„ d kCk2 + Ï„ d .
â‰¤O
m
n
nm

(85)

In summary,

e Ï„
kCe âˆ’ Ck2 â‰¤ min{O

r

dkCk2
Ï„2
+
nm
m

r

d
Ï„ 2 d  e Ï„ d
+
,O
n nm
m

r

kCk2
Ï„ 2d 
+
}.
n
nm

(86)

For Eq. (83) in Theorem 6, we first simplify its R and Ïƒ 2 . According to (Anaraki & Becker, 2017), to obtain a more accurate
estimation, each xi is required to be multiplied by HD to flatten its large entries before being sampled uniformly without
replacement, where H is a Hadamard matrix with its dimension being 2l (l is a certain positive integer), and D is a diagonal
matrix with its diagonal elements being i.i.d. Rademacher random variables. Note that HDDT HT = DT HT HD is an
identity matrix.
Suppose that we do not have to pad X with zeros until its dimension d = 2l holds. Hence, assuming that d = 2l for
X âˆˆ RdÃ—n without loss of generality, we define Y = HDX âˆˆ RdÃ—n below.
Corollary 2 of (Anaraki & Becker, 2017) indicates that with probability at least 1 âˆ’ Î², we have
r s
1
2nd
max |yki | â‰¤
2 log(
) max kxi k2
d
Î² iâˆˆ[n]
kâˆˆ[d],iâˆˆ[n]

(87)

and
s
max kyi k2 â‰¤
iâˆˆ[n]

2 log(

2nd
) max kxi k2 .
Î² iâˆˆ[n]

Corollary 3 of (Anaraki & Becker, 2017) indicates that with probability at least 1 âˆ’ Î², we have
r s
m
2nd
T
kSi Si yi k2 â‰¤
2 log(
)kxi k2 .
d
Î²
To make a compact representation, we define Î¸ =

q

(88)

(89)

2 log( 2nd
Î² ). Obviously, Î¸ > 1.

Then, in Theorem 6, we can replace the input data X by Y. Combing Eq. (89) with the fact that kyi k2 = kHDxi k2 =
p
mÎ¸ 2
2
kxi k2 getting Ï = (( m
d Î¸) ) = d for the setting of Theorem 6. Along with Î¸ > 1 and m â‰¤ d, we have

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

r

1  d2 mÎ¸2
1 2
d(d âˆ’ m)
2
2
(
+ 1)Î¸ max kxi k2 +
Î¸) max kxi k22
R= O ( 2
2
n
m d
m
d
iâˆˆ[n]
iâˆˆ[n]

 dÎ¸2
)Î¸2 max kxi k22
=O (
nm
iâˆˆ[n]
 d

e
=O
max kxi k22
nm iâˆˆ[n]
 Ï„ 2d 
e
,
=O
nm

(90)

and
Ïƒ2 â‰¤

T T
T
d2  mÎ¸2
m(m âˆ’ 1) 2
2 HDXX D H
O
(
âˆ’
)Î¸
max
kx
k
k2
k
i
2
nm2
d
d(d âˆ’ 1)
n
iâˆˆ[n]

(91)

(d âˆ’ m) mÎ¸2 2
HDXXT DT HT
Î¸ max kxi k22 kD(
)k2
m
d
n
iâˆˆ[n]

(d âˆ’ m)2 Î¸4
d âˆ’ m Î¸2
max kxi k22 kHDXk2F +
n 2 max kxi k42
+
nm d iâˆˆ[n]
ndm
d iâˆˆ[n]
p

2
2
2
m 2 mâˆ’1 2
(d âˆ’ m)Î¸4
d
2
2 n( 1/dÎ¸) maxiâˆˆ[n] kxi k2
(Î¸
âˆ’
)Î¸
max
kx
k
max
kx
k
O
kCk
+
=
i
i
2
2
2
nm2
d
dâˆ’1
d
n
iâˆˆ[n]
iâˆˆ[n]

2
2
2 4
(d âˆ’ m)Î¸
Î¸
(d âˆ’ m) Î¸
+
max kxi k22 nd max kxi k22 +
max kxi k42
nmd
d iâˆˆ[n]
d3 m
iâˆˆ[n]
iâˆˆ[n]
 d
dâˆ’m
e
=O
max kxi k22 kCk2 +
max kxi k42
nm iâˆˆ[n]
nm2 iâˆˆ[n]

d(d âˆ’ m)
(d âˆ’ m)2
4
4
+
max
kx
k
+
max
kx
k
i
i
2
2
nm3 iâˆˆ[n]
nm3 d iâˆˆ[n]

 d
d(d âˆ’ m)
(d âˆ’ m)2
4
4
e
max kxi k22 kCk2 +
max
kx
k
+
max
kx
k
=O
i
i
2
2
nm iâˆˆ[n]
nm3 iâˆˆ[n]
nm3 d iâˆˆ[n]
 Ï„ 2 dkCk
4
4
2
Ï„ d(d âˆ’ m) Ï„ (d âˆ’ m)
2
e
+
=O
+
.
(92)
nm
nm3
nm3 d
Note that Eq. (92) for simplifying Ïƒ 2 in Eq. (83) is tighter than the simplification result in the original paper (Anaraki &
d2
2
Becker, 2017) that scales with nm
2 . Recalling Eq. (83), and replacing its  by R and Ïƒ to get that with probability at least
1 âˆ’ Î´ âˆ’ Î², we have
r
r
r

Ï„ 2 d(d âˆ’ m) Ï„ 2 (d âˆ’ m)
Ï„ 2d 
dkCk2
1
e
+
+
+
.
(93)
kCe âˆ’ Ck2 â‰¤ O Ï„
nm
m
nm
m
nmd nm
If m = d, then
r

dkCk2
Ï„ 2d 
e
kCe âˆ’ Ck2 â‰¤ O Ï„
+
.
(94)
nm
nm
Although pure sampling without replacement makes no estimation error when m = d, processing the data by a Hadamard
matrix before sampling can result in the error as shown in Eq. (94).
+

If m < d with m being close to d, then d âˆ’ m = O(1), and thus we have
r
r

dkCk2
Ï„2
d
Ï„ 2d 
e
kCe âˆ’ Ck2 â‰¤ O Ï„
+
+
.
nm
m nm nm

(95)

If m  d or there exists a certain constant Îº < 1 with m < Îºd, then O(d âˆ’ m) = O(d). In addition to considering that
1
2
2
2
nd kXkF â‰¤ kCk2 â‰¤ maxiâˆˆ[n] kxi k2 = Ï„ , then we have
r
r

dkCk2
Ï„ 2d
1
Ï„ 2d 
e
kCe âˆ’ Ck2 â‰¤ O Ï„
+
+
.
(96)
nm
m
nm nm

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

4. Computational Complexity
Recall that we have n data samples in the d-dimensional space, and let m be the target compressed dimension. The
computational comparisons between our proposed method and the other approaches are presented in Table 1, in which
Standard method means computing C directly without data compression. We should explain some terms in the table
before proceeding.
Storage: storing data and random projection matrices (if any) in the remote sites and the fusion center, and storing the
covariance matrix in the fusion center.
Communication: shipping the data and random projection matrices (if any) from remote sites to the fusion center (high
communication cost requires tremendous bandwidth and power consumption).
Time (FLOPS): compressing the data in the remote sites, and calculating the covariance matrix in the fusion center (a low
time complexity means a low power cost and high efficiency for the data processing).
Note that, instead of only using the fusion center, data have to be first collected from many remote sites like a network of
g  n sensors. Then, they are transmitted to the fusion center to estimate the covariance matrix. This procedure shows
why communication cost is required. In the table, except for the communication, the two other compared terms have
contained the total costs in both the remote sites and fusion center.
Pn
For a covariance
matrix defined as C = n1 XXT âˆ’ xÌ„xÌ„T , we can exactly calculate xÌ„ = n1 i=1 xi in the fusion center by
P
g
xÌ„ = n1 j=1 uj , where {xi }ni=1 are distributed in g  n remote sites, and uj âˆˆ Rd is the summation of all data vectors in
the j-th remote site before being compressed. Hence, about O(gd) storage, O(gd) communication cost, and O(nd) time
have to be added to the last four methods in Table 1, with g  n.
Table 1. Computational costs in terms of storage, communication, and time.

Method
Standard
Gauss-Inverse
Sparse
UniSample-HD
Our method

Storage
O(nd + d2 )
O(nm + d2 )
O(nm + d2 )
O(nm + d2 )
O(nm + d2 )

Communication
O(nd)
O(nm)
O(nm)
O(nm)
O(nm)

From now on, we can focus on the covariance matrix defined as C =

Time
O(nd2 )
O(nmd + nm2 d + nd2 ) + TG
O(d + nm2 ) + TS
O(nd log d + nm2 )
O(nd + nm log d + nm2 )
1
T
n XX .

First, we derive the computational costs in our propose algorithm. Computing {pki }kâˆˆ[d],iâˆˆ[n] takes O(nd) time. Then,
sampling nm entries from all data vectors to get Y âˆˆ RmÃ—n takes time that is scaled on nm log d up to a certain small
constant. In Eq. (1), each Si , STi xi , Si STi xi , and Si STi (squared diagonal), has at most m non-zero entries. Hence,
recovering {Si }ni=1 via the sampled nm entries in Y and the sampling indices in T âˆˆ RmÃ—n incurs O(nm) time. With
Y and T in hand, {Si STi xi }ni=1 can be accurately computed in O(nm) time. Equipped with {Si STi xi }ni=1 , computing
b 1 = m Pn Si ST xi xT Si ST additionally takes only O(nm2 ) time, this is due to that each Si ST xi âˆˆ Rd and
C
i
i
i
i
i=1
nmâˆ’n
b 1 , computing
Si STi xi xTi Si STi âˆˆ RdÃ—d has at most m and m2 non-zero entries respectively. Based on the obtained C
Pn
m
T
T
T
b
the square diagonal matrix C2 =
D(Si S xi x Si S )D(bi ) takes O(nm) time since each Si ST xi xT Si ST
nmâˆ’n

i=1

i

i

i

i

i

i

b1 âˆ’ C
b 2 incurs O(d) extra time. The total
has at most m non-zero entries in its diagonal. Finally, obtaining C = C
2
running time is about O(nd + nm log d + nm + nm + nm + nm + nm + d) = O(nd + nm log d + nm2 ). In the
remote sites, data are compressed into m dimensional
space. Computing bki only corresponding to the sampled entries
b 2 = m Pn D(Si ST xi xT Si ST )D(bi ) in Eq. (1), so that at most nm entries
is enough to exactly calculate the C
i
i
i
i=1
nmâˆ’n
1
from {pki }kâˆˆ[d],iâˆˆ[n] have to be retained to obtain {bki }, since bki = 1+(mâˆ’1)p
. Thus, in the remote sites, Y âˆˆ RmÃ—n
ki
and T âˆˆ RmÃ—n dominate the storage cost, taking about O(nm) space in total. In the fusion center, O(d2 ) storage is
additionally used to store the estimated covariance Ce âˆˆ RdÃ—d . Similarly, about O(nm) communication cost is required
because of transmitting Y âˆˆ RmÃ—n , T âˆˆ RmÃ—n , v âˆˆ Rn , w âˆˆ Rn and Î±.
Then, for Standard in Table 1 that means directly calculating covariance matrix through the observed data samples without
compression, it is straightforward to check its computational complexity. X âˆˆ RdÃ—n and C âˆˆ RdÃ—d takes about O(nd+d2 )
storage in total, and X âˆˆ RdÃ—n leads to about O(nd) communication burden. Calculating the covariance matrix C =

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™
1
T
n XX

costs O(nd2 ) time.
Pn
For Gauss-Inverse, i=1 Si (STi Si )âˆ’1 STi xi xTi Si (STi Si )âˆ’1 STi , which is the main part of its unbiased estimator, dominates
the computational cost. Generating n different Gaussian matrices {Si âˆˆ RdÃ—m }ni=1 by the pseudorandom number generator like Mersenne twister (Matsumoto & Nishimura, 1998), which is by far the most widely used, takes considerably large
amount of time in practice. The time cost can be denoted by TG . As Si is dense, computing {STi xi }ni=1 takes O(nmd)
time. Calculating {(STi Si )âˆ’1 }ni=1 requires O(nm2 d + nm3 ), which involves matrix multiplications and inversions. Subsequently, we repeat the matrix-vector multiplications in {Si (STi Si )âˆ’1 STi xi âˆˆ Rd }ni=1 from the left to right, based on
which we get the target covariance matrix. Finally, it takes at least O(nmd + nm2 d + nm3 + nm2 + ndm + nd2 ) + TG =
O(nmd+nm2 d+nd2 )+TG time for Gauss-Inverse. In the remote sites, we compress data by STi xi âˆˆ Rm before sending
them to the fusion center. Along with O(d2 ) storage for the derived covariance matrix, about O(nm + d2 ) storage space is
required in total. Also, sending {STi xi âˆˆ Rm }ni=1 requires about a O(nm) computational burden.
Note that we have not listed the synchronization cost of Gauss-Inverse in Table 1. In practice, a pseudo-random number
generator is applied to the program in both the remote sites and the fusion center to generate/reconstruct n Gaussian random
matrices {Si âˆˆ RdÃ—m }ni=1 , and only n seeds are required to be transmitted from remote sites to the fusion center to recover
the Gaussian random matrices. Therefore, only about O(n) storage and communication cost have to be added in Table 1.
Also, calculating each (STi Si )âˆ’1 has to load each STi Si âˆˆ RmÃ—m into memory, hence at least O(m2 ) memory is required.
Pn
For Sparse, calculating i=1 Si STi xi xTi Si STi and subtracting its rescaled diagonal entries dominate the computational
cost (Anaraki, 2016). Generating sparse projection matrices {Si âˆˆ RdÃ—q }ni=1 is also expensive (Anaraki & Becker, 2017),
1
1
, 1 âˆ’ 1s , 2s
}.
whose time cost is denoted by TS . The entries of each Si are distributed on {âˆ’1, 0, 1} with probabilities { 2s
d
Then, each column of Si has s non-zero entries in expectation. Empirically, we can fix that q/d = 0.2 or 0.4 according
to (Anaraki & Hughes, 2014; Anaraki, 2016). The number of non-zero entries of Si STi xi âˆˆ Rd is at least d(1 âˆ’ (1 âˆ’ 1s )q )
q
dq
1 q
in expectation, which ranges from dq
s (1 âˆ’ 2s ) to s . Define d(1 âˆ’ (1 âˆ’ s ) ) = m < d, thus we can solve s with
d2
T
q/d = 0.2 or 0.4 fixed to obtain that s = O( m ). Then computing {Si xi âˆˆ Rq }ni=1 takes O( ndq
s ) = O(nm) time
ndq
T
d n
in expectation. Based on it, computing {Si Si xi âˆˆ R }i=1 additionally costs O( s ) = O(nm)
Pn time in expectation.
Since each Si STi xi âˆˆ Rd contains only m non-zeros entries in expectation, thus obtaining i=1 Si STi xi xTi Si STi and
subtracting its rescaled diagonal entries requires O(nm + nm + nm2 + d) + TS = O(nm2 + d) + TS time in total. Storing
{Si STi xi âˆˆ Rd }ni=1 and the estimated covariance matrix requires O(nm + d2 ) storage in expectation, where a O(nm) cost
results from O(nm) non-zero entries in {Si STi xi âˆˆ Rd }ni=1 along with O(nm) corresponding indices. Similarly, sending
{Si STi xi âˆˆ Rd }ni=1 from remote sites to the fusion center takes at most O(nm) communication cost in expectation.
For UniSample-HD, processing data by a Hadamard matrix by HDX âˆˆ RdÃ—n requires O(nd log d) time, where H âˆˆ RdÃ—d
can be a Hadamard matrix, D âˆˆ RdÃ—d is a diagonal matrix with diagonal elements being i.i.d. Rademacher random
variables, and we suppose that d = 2l holds (l is a certain positive integer). Then, sampling m entries uniformly without
replacement
on each data vector by {STi HDxi âˆˆ Rd }ni=1 takes O(nm) time. Hence, it is straightforward to check that
Pn
T T
T
T T
T
T
dÃ—d
requires O(nd log d+nm+nm2 +d2 log d) = O(nd log d+nm2 )
i=1 HDSi Si D H xi xi D H Si Si HD âˆˆ R
dÃ—d
time in total. HD âˆˆ R
can be generated on the fly when we process the data. About O(nm + d2 ) storage has to be
used for the compressed data and estimated covariance matrix. Obviously, about O(nm) communication cost is required.

5. Impact of the Parameter Î±
5.1. Discussion
To determine if the k-th entry of the data vector xi âˆˆ Rd should be retained or not, the sampling probability applied in our
method is
pki = Î±

x2
|xki |
+ (1 âˆ’ Î±) ki 2 .
kxi k1
kxi k2

(97)

Achieving our theoretical bound of Theorem 2 requires 0 < Î± < 1. However, The case Î± = 1 and Î± = 0 can also
obtain weaker error bounds, which can be straightforwardly derived from Eqs. (64)(65) and Eqs. (70)(71). The following
illustration reveals the connection between Î± and error bounds on data owning different properties.
1. Only using Î± = 0, i.e., `2 -norm based sampling pki =

x2ki
kxi k22

can yield a very weak bound if there exist some very

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

small entries |xki | in xi âˆˆ Rd . E.g., substituting pki =
in

kx k4
maxkâˆˆ[d] x2i 2
ki

x2ki
kxi k22

into the term maxkâˆˆ[d]

x2ki
p2ki

of Eq. (64) or Eq. (70) results

in the final error bound, which becomes infinite if the positive entry |xki | gets close to 0;

|xki |
kxi k1 yields a slightly weak bound if there exist some very
x4
|xki |
large entries |xki | in xi âˆˆ Rd . E.g., substituting pki = kx
into the term maxkâˆˆ[d] p2ki of Eq. (70) results in
i k1
ki
maxkâˆˆ[d] x2ki kxi k21 in the final error bound, which is always greater than or equal to maxkâˆˆ[d] kxi k42 = kxi k42 derived
x2
x4ki
4
by employing pki = kxki
2 to bound maxkâˆˆ[d] p2 . Specifically, assume kxi k2 = 1 without loss of generality, then
i k2
ki
qâˆš
âˆš
d+1
âˆš
and xki,k6=j =
it is possible that maxxi âŠ‚Rd ,kxi k42 =1 maxkâˆˆ[d] x2ki kxi k21 = d+2 4 d+1  1 if when xji =
2 d
q

2. Only using Î± = 1, i.e., `1 -norm based sampling pki =

q

1âˆš
2d+2 d

for all k âˆˆ [d] with k 6= j. Also, minxi âŠ‚Rd ,kxi k42 =1 maxkâˆˆ[d] x2ki kxi k21 = 1 if we have xki =

1
d

for all

d

k âˆˆ [d] or we have xji = 1 and xki,k6=j = 0 for all k âˆˆ [d] with k 6= j. Note xi âŠ‚ R in the above optimizations
means that xi is a vector variable in the d-dimensional space, and j is an arbitrary integer in the set [d].
3. Therefore, Î± balances the performance by `1 -norm based sampling and `2 -norm based sampling. `2 sampling penalizes small entries more than `1 sampling, hence `2 sampling is more likely to select larger entries to decrease error
(e.g., case 2). However, different from `1 sampling, `2 sampling is unstable and sensitive to small entries, and it can
make estimation error incredibly high if extremely small entries are picked (e.g., case 1). Then 0 < Î± < 1 is applied
x2
to achieve the desired tight bound with pki â‰¥ (1 âˆ’ Î±) kxki
2 to tackle the extreme situation in the case 2 that cannot
ik
2

|xki |
be well handled purely by pki â‰¥ Î± kx
. When Î± turns from 1 to 0, the estimation error is likely to first decrease and
i k1
then increase.

5.2. Experiments

1.4
1.2
1
0.8
0.6
0.05

0.1

m/d

0.15

0.2

1.6

A2, d=1000 n=10000

1.4
1.2
1
0.8
0.6
0.05

0.1

m/d

0.15

0.2

1.6

A3, d=1000 n=10000
Rescaled Error

A1, d=1000 n=10000

Rescaled Error

1.6

Rescaled Error

Alpha-1
Alpha-0.9
Alpha-0.8
Alpha-0.7
Alpha-0.6
Alpha-0.5
Alpha-0.4
Alpha-0.3
Alpha-0.2
Alpha-0.1
Alpha-0

Rescaled Error

Accordingly, we create four different synthetic datasets: {Ai }4i=1 âˆˆ R1000Ã—10000
q (i.e., d = 1000 and nq= 10000). All
1âˆš
1âˆš
entries in A1 and A2 are i.i.d. generated from the Gaussian distributions N ( 2d+2
, 1 ) and N ( 2d+2
, 1 ),
d 1000
d 100
qâˆš
âˆš , 1 ), and the other entries follow
respectively. For A3 , the entries of its one row are i.i.d. generated from N ( 2d+1
d 100
q
1
1âˆš
( 2d+2 d , 100 ). For A4 , its generation follows the way of X1 in the main text of the paper.
n1X8, d=1024 N
n=10000

1.4
1.2
1
0.8
0.6
0.05

0.1

m/d

0.15

0.2

4

A4, d=1000 n=10000

2

0
0.05

0.1

0.15

0.2

m/d

Figure 1. Accuracy comparison by decreasing Î± from 1 to 0 with a step size of 0.1. The error at each Î± is normalized by that at Î± = 1
on y-axis, and m/d varies from 0.005 to 0.2 with a step size of 0.005 on x-axis. Roughly, Î± = 0.9 is a good choice, and the smaller
parameter like Î± = 0 usually leads to a poorer accuracy and higher variance compared with the other Î± values.
0.1
m/d

0.15

0.2

In Figure 1, the y-axis reports the errors that are normalized by the error incurred at Î± = 1. For A1 , the magnitudes of
the data entries tend to be highly uniformly distributed. Thus, nearly the same results are returned over all Î±. For A2 ,
its entries are slightly uniformly distributed with some entries having extremely small magnitudes. Hence, Î± = 0 has a
poorer performance compared with the others, which is consistent with the case 1 in Section 5.1. A3 contains some entries
larger than the others, and neither Î± = 0 nor Î± = 1 achieves the best performance obtained roughly at Î± = 0.9. Also,
the estimation error first decreases and then increases when Î± turns from 1 to 0. All such simulation results conform to
the case 2 and case 3 in Section 5.1. Considering A4 that is not likely to contain the extreme situation as mentioned in the
case 2 of Section 5.1, we see that best performance is roughly achieved when Î± gets close to 1.

Appendix for â€˜Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Dataâ€™

References
Anaraki, F. Estimation of the sample covariance matrix from compressive measurements. IET Signal Processing, 2016.
Anaraki, F. and Becker, S. Preconditioned data sparsification for big data with applications to pca and k-means. IEEE Transactions on
Information Theory, 2017.
Anaraki, F. and Hughes, S. Memory and computation efficient pca via very sparse random projections. In Proceedings of the 31st
International Conference on Machine Learning (ICML-14), pp. 1341â€“1349, 2014.
Azizyan, M., Krishnamurthy, A., and Singh, A.
arXiv:1506.00898, 2015.

Extreme compressive sampling for covariance estimation.

arXiv preprint

Davis, C. and Kahan, W. M. The rotation of eigenvectors by a perturbation. iii. SIAM Journal on Numerical Analysis, 7(1):1â€“46, 1970.
Golub, G. H. and Van Loan, C. F. Matrix computations. 1996.
Matsumoto, M. and Nishimura, T. Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator.
ACM Transactions on Modeling and Computer Simulation, 1998.
Tropp, J. A. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8(1-2):1â€“230, 2015.

