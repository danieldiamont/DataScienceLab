000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053
054

Supplementary Materials:
Dueling Bandits with Weak Regret
June 12, 2017
Abstract

in this iteration by Lemma 0.1 is

E
1−pi,j
−1
pi,j
E+1
E
−

E+1
1 − 2pi,j
1 − 2pi,j 1−pi,j
−1
pi,j

This note contains supplementary materials
for Dueling Bandits with Weak Regret.

0. Gambler’s Ruin Lemma

≤

In our analysis of WS-W, we will use results from a
special case of the Gambler’s ruin problem (Karlin,
1968), stated as follows: suppose a gambler has m
dollars initially. In each of a sequence of rounds, he
loses 1 dollar with probability q 6= 12 and wins 1 dollar
with probability 1 − q. He stops playing when he has
either m + 1 dollars or has no money left. We have the
following result, with a proof available on Page 73 of
Karlin (1968).

The proof of second statement is similar. Using the
same notation but now supposing pi,j ≥ p > 12 , we
have that the expected length of time we spend in this
iteration is
E

1−pi,j
−1
pi,j
E
E+1
−

E+1
1 − 2pi,j
1 − 2pi,j 1−pi,j
−1
pi,j

Lemma 0.1 (Gambler’s Ruin Lemma). In the gambler’s ruin problem: (1) the probability that the gambler reaches m + 1 dollars before reaching 0 dolm
( 1−q ) −1
lars is qm = 1−qq m+1 ; (2) the expected number
−1
( q )
m
of steps before the gambler stops playing is 1−2q
−
1−q m
−1
)
(
q
m+1
1−2q ( 1−q )m+1 −1 .
q

=
≤

1
2pi,j − 1

−

E + 1 pi,j (1 − pi,j )E − (1 − pi,j )E+1
1 − 2pi,j
(1 − pi,j )n+1 − pE+1
i,j

1
.
2p − 1

2. Proof of Lemma 2

Observe that the conditional distribution of T`,k and
the winner of iteration k round `, given the two arms
being pulled, is given by the result above for the Gambler’s ruin problem. We leverage this in our proof.

1. Proof of Lemma 1
Proof. Suppose we are comparing arm i versus arm j
in this iteration with i > j and arm i is the incumbent.
Then we know C(t`,k − 1, i) = (N − 1)(` − 1) + k − 1
and C(t`,k − 1, j) = −` + 1. We will keep playing these
two arms until C(t`,k + T`,k − 1, i) = (N − 1)(` − 1) + k
or C(t`,k + T`,k − 1, j) = (N − 1)(` − 1) + k. Further,
since the winning probability of arm i over arm j is
pi,j over this period, we know the dynamics of this
iteration are the same as those of the Gambler’s Ruin
problem. Denote E = C(t`,k −1, i)−C(t`,k −1, j)+1 =
N l+k−N . Then the expected length of time we spend

E
E
≤
.
1 − 2pi,j
2p − 1

1

In this section, we prove Lemma 2 from the main
paper. This section is structured as follows: In section 2.1, we provide two bounds for the incumbent’s
losing and winning probability; In section 2.2, we consider a version of the problem in which better and
worse incumbents have constant (but different) winning probabilities and provide a upper bound for the
number of worse incumbents in a round before a better
incumbent loses ; In section 2.3, we use the results from
the previous subsection to bound the expected number
of iterations with a worse incumbent in a single round
before a better incumbent loses, starting from within a
round; In section 2.4, we prove a similar bound on the
expected number of iterations with a worse incumbent
in this and future rounds before a better incumbent
loses, starting from the beginning of a round; In section 2.5, we complete the proof of Lemma 2.

055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107
108
109

Throughout this section, we use a one to one correspondence between n and (`, k) defined by n =
(` − 1)(N − 1) + k, 0 ≤ k ≤ N − 1 and ` = dn/(N − 1)e.
We also denote p∗ = 2p−1
p .

N (` − 1) + k. We have


1−pi,j
pi,j

E

−1
E+1
1−pi,j
−1
pi,j
E

1−pi,j
[1 − 1−p
pi,j
p ]
=
E+1

1−p
1 − pi,ji,j

E 
E
1 − pi,j
1−p
≤
.
≤
pi,j
p

1 − qE = 1 − 

2.1. Bounds on Win and Loss Probabilities
We first prove the following two lemmas, which give
• a lower bound for the probability that a worse
incumbent loses an iteration;
• an upper bound for the probability that a better
incumbent loses an iteration.
Lemma 2.1. In iteration k of round ` conditioned on
the identities of the incumbent and the challenger, if
the incumbent is worse than the challenger, then the
incumbent loses the iteration with conditional probability at least p∗ = 2p−1
p .
Proof. Let i be the incumbent and j be the challenger,
with i > j. C(i, t`,k ) ≥ 0 and C(j, t`,k ) ≤ 0. Let
E = C(i, t`,k ) + |C(j, t`,k )| + 1. The probability that
arm i loses this iterations is the same as 1 − qE in the
Gambler’s Ruin Lemma, Lemma 0.1, with q = pi,j <
0.5. This probability is:

1 − qE = 1 − 

≥

≥

1−pj,i
pi,j

1−pi,j
pi,j

1−pi,j
pi,j

E



g(b, m)
=

b−1
m−1
X
X 1
1 ∗ 0
b
+
p g(b , m − 1) +
g(b, m − 1)
m
m
m
0
0
b =0



=

−1
1−p

b−1
X
b0 =0

−1

− pi,ji,j
E+1

1−pi,j
pi,j

In this section, we define a function g(b, m) as follows.
First, we define g(0, m) = 0 for any m. We define
g(b, m) for other integers b, m satisfying m > 0 and
0 ≤ b ≤ m recursively, as follows:

+

E+1

E+1

2.2. Definition and Upper Bound for g(b, m)

b =b

1
(1 − p∗ )g(b − 1, m − 1)
m

b−1
X
b
1 ∗ 0
m−b
+
p g(b , m − 1) +
g(b, m − 1)
m
m
m
0
b =0

E
=

1 − 2pi,j
1 − pi,j

2p − 1
.
p

Lemma 2.2. In iteration k of round ` conditioned on
the identities of the incumbent and the challenger, if
the incumbent is better than the challenger, then the
incumbent loses the iteration with conditional proba
E
bility at most 1−p
, where E = N (` − 1) + k.
p
Proof. This proof is similar to the previous one. Suppose we are pulling arm i and j with i < j and i is the
incumbent. Then we know C(t`,k − 1, i) = (N − 1)(` −
1) + k − 1 and C(t`,k − 1, j) = −` + 1. The probability
that arm i loses is equal to 1 − qE from the gambler’s
ruin problem, where E = (N −1)(`−1)+k −1+`−1 =

b
+ (1 − p∗ )g(b − 1, m − 1)
m

(1)

Intuitively, g(b, m) is the expected number of future
iterations in which the incumbent is worse than the
challenger, starting with m arms that have not dueled
yet b of which are better than the incumbent, when
we stop counting when we reach the end of the round
or when an incumbent loses to a worse challenger, in
a simplified problem in which worse incumbents beat
better challengers with probability p∗ . In our problem,
this probability is not p∗ , but is bounded below by this
quantity, and in the next section we will show that
g(b, m) is an upper bound on an analogous quantity in
our problem.
We prove the following result about g.
Lemma 2.3. For 0 ≤ b ≤ m ≤ N − 1, we have
g(b, m) = g(b, b) ≤

log(b) + 1
.
p∗

110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164

Proof. Given the boundary conditions g(0, m) = 0 for
all m, we know Equation 1 has a unique solution. In
this proof,

Therefore,

• Finally, we show g(b, m) ≤

F (k)

k=1

• We first assume g(b, m) = g(b, b) for all b ≤ m
and solve for g(b, m);
• Then we show that this g(b, m) is indeed the solution for Equation 1, verifying that g(b, m) is as
claimed;

b
X

g(b, b) =

b 
X
1

=

k=1


1 − p∗
(1 − p∗ )k−1
+
+ ···
.
k
k
k

Thus, if g(b, m) = g(b, b) for all b ≤ m, we know

log(b)+1
.
p∗

g(b, m) =

b 
X
1
k=1

First, we solve for g(b, m) with the assumption that
g(b, m) = g(b, b) for b ≤ m. Setting m = b in Equation 1 provides
g(b, b) = 1 +

0
b−1 ∗
X
p g(b , b)
+ (1 − p∗ )g(b − 1, b − 1).
b
0

k


1 − p∗
(1 − p∗ )k−1
+ ···
.
k
k

Now we verify that this is the correct solution. We
prove this by induction on b. For b = 1, Equation 1
becomes
g(1, m) =

b =0

(2)

1
m−1
+
g(1, m − 1).
m
m

Since g(1, 1) = 1, it is easy to check g(1, 2) = g(1, 3) =
· · · = g(1, N − 1) = 1.

Thus, we know
b−1
X

+

0

p∗ g(b , b + 1)

Suppose this g(b, m) = g(b, b) are true for all b ≤ m,
b ≤ k. For b = k + 1, Equation 1 becomes

b0 =1

=

b−1
X

g(k + 1, m)
0

p∗ g(b , b)

0

b =1

=b [g(b, b) − 1 − (1 − p∗ )g(b − 1, b − 1)] .
Therefore, Equation 2 becomes
g(b + 1, b + 1)
b
=1 +
[g(b, b) − 1 − (1 − p∗ )g(b − 1, b − 1)]
b+1
p∗ g(b, b)
+
+ (1 − p∗ g(b, b).
b+1
Re-organizing the terms, we have
g(b + 1, b + 1) − g(b, b)
1
b
=
+
(1 − p∗ )[g(b, b) − g(b − 1, b − 1)].
b+1 b+1
Denote F (b) = g(b, b)−g(b−1, b−1). We know F (1) =
1. Thus, we have
1 b−1
F (b) = +
(1 − p∗ )F (b − 1)
b
b
1 1 − p∗
b−2
= +
+
(1 − p∗ )2 F (b − 2)
b
b
b
1 1 − p∗
(1 − p∗ )b−1
= +
+ ···
.
b
b
b

=

k
0
k + 1 X p∗
m−k−1
+
g(b , m − 1) +
g(k + 1, m − 1)
m
m
m
0
b =0

k+1
(1 − p∗ )g(k, m − 1)
+
m
k
0
0
k + 1 X p∗
m−k−1
=
+
g(b , b ) +
g(k + 1, m − 1)
m
m
m
0
b =0

k+1
+
(1 − p∗ )g(k, k).
m
To show g(k + 1, m) does not depend on m, we need to
prove the following equation is true for m = k + 2, k +
3, · · · , N − 1.
k
0
0
k+1
k + 1 X p∗
+
g(b , b ) +
(1 − p∗ )g(k, k)
m
m
m
0
b =0

k+1
=
g(k + 1, m − 1)
m
k
X
0
0
⇐⇒ k + 1 +
p∗ g(b , b ) + (k + 1)(1 − p∗ )g(k, k)
b0 =0

=(k + 1)g(k + 1, m − 1)

(3)

We first check Equation 3 when m = k + 2. Starting

165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219

from the left hand side, we have

k+1+

k
X

0

the round. Formally, we define this quantity as:
h(i, n, A) = E

0

g(b , b ) + (k + 1)(1 − p∗ )g(k, k)

#
0

B(n )|A, in = i ,

n0 =n

0

b =0

=k + 1 + (k + 1)[g(k + 1, k + 1) − 1 − (1 − p∗ )g(k, k)]
(4)
+ (k + 1)(1 − p∗ )g(k, k)
=(k + 1)g(k + 1, k + 1),
which equals to the right hand side. Equation (4) follows from Equation (2) (Equation (2) holds because
g(b, m) = g(b, b) for all b ≤ k).
Again, by induction, we know (3) is true for all m =
k + 2, · · · , N − 1 and thus we concludes our induction.
We have shown that g(b, m) = g(b, b) for all b ≤ m.
Finally, we prove g(b, b) = g(b, m) ≤
because

" σ−1
X

log(b)+1
.
p∗

This is

g(b, m) =g(b, b)

b 
X
1 1 − p∗
(1 − p∗ )k−1
=
+
+ ···
k
k
k
k=1

b 
X
1 1 − p∗
(1 − p∗ )b−1
≤
+
+ ···
k
k
k
k=1

b
X

1
=
1 + (1 − p∗ ) + · · · + (1 − p∗ )b−1
k
k=1

log(b) + 1
≤
,
p∗
which concludes our proof.
2.3. Bound on the Number of Iterations in
One Round with a Worse Incumbent,
Starting from Within the Round

where
• Conditioning on A is understood to mean that
we are conditoning on C(n − 1, j) = −` + 1 ∀ j ∈
/
A ∪ {in }, and C(n − 1, j) = −` ∀ j ∈ A, where
` = dn/(N − 1)e is the round in which iteration n
resides. In other words, it is understood to mean
that A contains the set of arms that have not yet
dueled in this round.
• σ = min {n0 > n : J(n0 ) = 1, n0 = N dn/(N − 1)e}
where J(n) is an indicator that equals 1 when a
better incumbent loses at iteration n, i.e., σ is
the first time that either a better incumbent loses
or the round ends.
Lemma 2.4. For any i, `, k and A, we have
h(i, n, A) ≤ g(b, m) ≤

log(N ) + 1
,
p∗

where m = N − k and b = |{u ∈ A : u < i}|.
Proof. Denote qi,j (n) as the probability that incumbent arm i will beat challenger j at time n. We first
write a recursive expression for h(i, n, A) that applies
when n is not divisible by N :
X 
qi,j (n)
h(i, n, A) =
1+
h(i, n + 1, A ∪ {j})
N −k
{j∈A:i>j}

1 − qi,j (n)
+
h(j, n + 1, A ∪ {i})
N −k
X
qi,j (n)
h(i, n + 1, A ∪ j). (5)
+
N −k
{j∈A:i<j}

Let B(n) denote an indicator function that equals 1
if we have a better incumbent at the nth iteration.
The definition of B(n) is very similar to B(`, k) except B(`, k) tracks both round and iteration number.
Similarly, we use B̄(n) = 1−B(n) to denote an indicator function that equals 1 if we have a worse incumbent
at the nth iteration.
Let h(i, n, A) be the expected number of iterations
with an incumbent that is worse than the challenger,
between iteration n and the first time that a better incumbent loses to a challenger or the round ends, given
that the incumbent arm at iteration n is i and A is
the set of arms that have not yet previously dueled in

When n is divisible by N − 1, the only allowed value
of A is ∅ and h(i, n, ∅) = 0.
We then prove the desired result via induction on the
number of iterations in the round, i.e., on n (mod N −
1). When n (mod N − 1) = 0, we have h(i, n, ∅) = 0,
b = 0, and g(b, m) = 0. Thus the result holds in this
case.
Then suppose the result holds for all n with a particular value of n (mod N − 1) and we show it holds for
n − 1.
Applying the induction hypothesis to the right-hand

220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274

side of (5), we have
X 
qi,j (n)
h(i, n, A) ≤
1+
g(bi,j , m − 1)
m
{j∈A:i>j}

1 − qi,j (n)
+
g(bj,j , m − 1)
m
X
qi,j (n)
+
g(bi,j , m − 1), (6)
m

stopping as soon as a better incumbent loses, giving
that we have arm i as the incumbent at the start of
round `.
Lemma 2.5. For any i and `, we have
f (i, `) ≤

log(N ) + 1
.
(p∗ )2

{j∈A:i<j}

0

0

where bu,j = #{u ∈ A \ {j} : u < u}.
Consider the summand in the first sum in (6), drop1
,
ping the constants 1 and m
qi,j (n)g(bi,j , m − 1) + (1 − qi,j (n))g(bj,j , m − 1). (7)

Proof. Let U (i, `) denote the expected number of iterations in round ` with a worse incumbent before a
better incumbent loses. We use V (`) to denote an indicator which equals to 1 if a better incumbent does
not lose in the round `. Then for i > 1,
f (i, `) = U (i, `) + E[f (Z(`), ` + 1)V (`)|Z(` − 1) = i].

This is increasing in qi,j (n) when i > j since bi,j >
bj,j , and since g(b, m) is increasing in b. Since i is an
incumbent that is worse than the challenger when i >
in
j, Lemma 2.1 shows that qi,j (n) ≤ 1−p∗ = 1− 2p−1
p
this situation. Thus, this summand is bounded above
by (1 − p∗ )g(bi,j , m − 1) + p∗ g(bj,j , m − 1).

for all i and `.

Substituting this into (6), along with the inequality
qi,j (n) ≤ 1 in the last term, we have

For the second term, since f (Z(`), ` + 1) = 0 when
Z(`) = 1, we know the second term is bounded by

h(i, n, A)

X 
1 − p∗
p∗
≤
1+
g(bi,j , m−1) + g(bj,j , m−1)
m
m

The first term is bounded by Lemma 2.4 by
U (i, `) ≤

E[f (Z(`), ` + 1)V (`)|Z(` − 1) = i]
≤E[f (Z(`), ` + 1)|Z(`) 6= 1, V (`) = 1, Z(` − 1) = i]
× P (Z(`) 6= 1, V (`)|Z(` − 1) = i).

{j∈A:i>j}

+

X
{j∈A:i<j}

=

1
g(bi,j , m − 1)
m

b−1 ∗
X
0
b
b
p
+ (1 − p∗ )g(b − 1, m − 1) +
g(b , m − 1)
m m
m
0

Let sj = P (Z(`) = j|Z(`) 6= 1, V (`), Z(` − 1) = i) to
be the probability distribution over the integers from
2 through N . Then we know
E[f (Z(`), ` + 1)|Z(`) 6= 1, V (`) = 1, Z(` − 1) = i]

b =0

m−b
+
g(b, m − 1)
m
=g(b, m)
In the second to last line we have used that {bi,j :
j ∈ A, i > j} = {0, . . . , b − 1} and bi,j = b − 1 when
i > j; bi,j = b when i < j; and that the cardinality
of {j ∈ A : i > j} and {j ∈ A : i < j} are b and
m − b respectively. In the last line we have used the
recursive definition of g(b, m) in terms of g(·, m − 1).
This shows the first inequality in the statement of the
lemma. The second inequality follows directly from
Lemma 2.3.

=

N
X

sj f (j, ` + 1)

j=2

≤ max f (j, ` + 1).
j=2,,N

Further, since if arm 1 wins its first duel as a challenger (which happens with probability at least p∗ ),
then either Z(`) = 1 (it wins all subsequent duel in
the round) or V (`) = 0 (it loses a subsequent duel),
we have P (Z(`) 6= 1, V (`)|Z(` − 1) = i) ≤ 1 − p∗.
Thus, we know
f (i, `) ≤

2.4. Bound on the Number of Iterations with
a Worse Incumbent, Starting from a
Round Beginning
Denote f (i, `) to be the expected number of iterations
with a worse incumbent in this and future rounds,

log(N ) + 1
,
p∗

log(N ) + 1
+ (1 − p∗ ) max f (j, ` + 1).
j=2,··· ,N
p∗

Let f (`) = maxj=2,··· ,N f (j, `). Then,
f (`) ≤

log(N ) + 1
+ (1 − p∗ )f (` + 1).
p∗

275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329

Let `k = dτk /(N − 1)e. Then,
" ∞
#
X
E
1{n < τk+1 }B̄(n)|Hτk , τk < ∞

Thus,
log(N ) + 1
+ (1 − p∗ )f (2)
p∗
log(N ) + 1
≤
(1 + (1 − p∗ ) + (1 − p∗ )2 + · · · )
p∗
log(N ) + 1
=
.
(p∗ )2

f (1) ≤

n=τk

=E

"` N
k
X

#
1{n < τk+1 }B̄(n)|Hτk , τk < ∞

n=τk

"
+E

∞
X

#
1{n < τk+1 }B̄(n)|Hτk , τk < ∞

n=`k N +1

log(N ) + 1 log(N ) + 1
+
p∗
(p∗ )2
2(log(N ) + 1)
≤
(p∗ )2
≤

2.5. Completing the Proof of Lemma 3
With the lemmas in the preceding subsections established, we now complete the proof of Lemma 2.

Proof. Let τ0 = 0 and τk = {n > τk−1 : J(n) =
1}. The expected number of iterations with a worse
incumbent is

"
E

∞
X

where the second
hP to last inequality relies on Lemmai2.4
`k N
to show E
n=τk 1{n < τk+1 }B̄(n)|Hτk , τk < ∞ is
)+1
and Lemma 2.5 to
bounded above by log(N
p∗
P∞

show E
1{n
<
τ
}
B̄(n)|H
k+1
τk , τk < ∞ is
n=`k N +1

bounded above by
Thus,
"

#
B̄(n)

E

n=0

=E

∞
X

n=0

1{τk < ∞}

=

∞
X

1{n < τk+1 }B̄(n)

"
P (τk < ∞)E

∞
X

#

B̄(n)

≤

2(log(N ) + 1) X
P (τk < ∞).
(p∗ )2
k=0

able with success rate less than

1{n < τk+1 }B̄(n)|τk < ∞

n=τk

k=0

∞

#

Now we bound P (τk < ∞) for a fixed k. Based on
Lemma 2.2, we know J(n) is a Bernoulli

 random vari-

n=τk

k=0
∞
X

∞
X

log(N )+1
(p∗ )2 .

where we have used Tonelli’s theorem to exchange the
expectation of an infinite sum of non-negative terms
with an infinite sum of expectations of the same terms.

E

#
1{n < τk+1 }B̄(n)|τk < ∞

=E E

∞
X

∞
X

!
Qi ≥ k .

i=1

n=τk

" "

(this is be-

i=1

≤P
∞
X

n

cause of Lemma 2.2 and n = (N −1)(`−1)+k < E), independent across n. Let Qndenote
n a Bernoulli random
1−p
variable with success rate
. Then we know:
p
!
∞
X
P (τk < ∞) ≤ P
J(i) ≥ k

Conditioning on the history available at time τk , we
have that the inner expectation can be written as,
"

1−p
p

#

#

1{n < τk+1 }B̄(n)|Hτk , τk < ∞ |τk < ∞ ,

n=τk

where Hn is the sigma algebra generated by (C(i, s) :
s < t`,k0 , i = 1, . . . , N ), where ` = n (mod N − 1),
k 0 = dn/(N − 1)e, and Hτk is the filtration (Hn : n)
stopped at τk .
We
break
this
inner
term
P∞ further

E
1{n
<
τ
}
B̄(n)|H
,
τ
<
∞
into
two
k+1
τk k
n=τk
parts: the part that occurs during the round in which
τk resides, and the part that occurs in future rounds.

Pm
Let Wm = i=1 Qi , which follows a Poisson Bernoulli
distribution, and let W = limm→∞ Wm . W follows a
i
P∞ 
=
Poisson distribution with parameter i=1 1−p
p
1−p
2p−1

(Theorem 4, Wang (1993)). Thus,
"∞
#
∞
X
2(log(N ) + 1) X
E
B̄(n) ≤
P (W ≥ k)
(p∗ )2
n=0
k=0

2p2 (1 − p)
(log(N ) + 1)
=
(2p − 1)3
2p2
≤
(log(N ) + 1)
(2p − 1)3

330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384

3. Proof of Lemma 3
Proof. It is easy to see that at the last iteration which
has a worse incumbent, the better arm is always arm
1. Thus, we only consider C(t, 1) in this proof. At the
end of the `th round, if C(t`+1 − 1, 1) < 0, we know
C(t`+1 − 1, 1) = −`.
Let us consider a simple random walk W(t) such that
W (t + 1) = W (t) + 1 with probability p > 12 and
W (t + 1) = W (t) − 1 with probability 1 − p for t ≥ 1.
If we denote p∗` = P (∃t∗ , W (t∗ ) = −`) for ` > 0, then
`

.
it is easy to calculate that p∗` = 1−p
p
Now let us consider C(t, 1). If we pull arm 1 with some
other arm i at time t, then C(t, 1) = C(t − 1, 1) + 1
happens with probability p1,i > p and C(t, 1) = C(t −
1, 1) − 1 with probability 1 − p1,i < 1 − p. If we do not
pull arm 1 at time t, then C(t, 1) = C(t − 1, 1) with
probability 1.
Define τ1 = 1 and τk = mint {t > τk−1 , C(t, 1) 6=
C(τk−1 , 1)}, for k = 1, 2, · · · ,. Because τk is a nondecreasing right continuous stopping time, we know it
is a valid random change of time (Barndorff-Nielsen &
Shiryaev, 2015). Define R(k) a new stochastic process
where R(k) = C(τk , 1). Then we know at every time
k, R(k) = R(k − 1) + 1 with probability greater or
equal to p and R(k) = R(k − 1) − 1 with probability
less than 1-p. Define p` = P (∃t∗ , R(t∗ ) = −`), then
`

using first step
it is easy to prove p` ≤ p∗` = 1−p
p
analysis and induction (we leave the proof as an exercise for the reader), which means P (∃t∗ , C(t∗ , 1) =

`
−`) ≤ 1−p
.
p

Thus, we have that (8) is bounded above by

`−1
1
1−p
1
,
P (D̄(`) = 1) ≤
2p − 1
2p − 1
p
where the final inequality follows from Lemma 3 and
the fact that D̄(`) = 1 implies L ≥ ` − 1.
To show the second claimed equation, we use the same
proof technique used for the first and get:
E[B(`, k)T`,k V (`, k)] ≤

1
P (V (`, k) = 1).
2p − 1

Now we just need to compute P (V (`, k) = 1). Given
C(t` −1, 1) = (N −1)(`−1) at the beginning of round `,
it loses only if there exists a t0 ≥ t` and C(1, t0 ) = −`.
Using the results from Lemma 3, we know P (V (`, k) =
`

. This completes the proof of the second
1) ≤ 1−p
p
claimed equation.

5. Proof of Lemma 5
Proof. For the first inequality, we know
"N −1
#
X
E
B̄(`, k)T`,k D̄(`)
k=1

=

N
−1
X



E E[B̄(`, k)T`,k |D(`) = 0]D̄(`) .

(9)

k=1

Moreover,
E[B̄(`, k)T`,k |D(`) = 0]

4. Proof of Lemma 4
Proof. To show the first claimed equation, we have:
E[B(`, k)T`,k D̄(`)]
=E[B(`, k)T`,k |D̄(`) = 1]P (D̄(`) = 1).

bounded above by 1/(2p − 1), and we have that
E[B(`, k)T`,k |D̄(`) = 1] ≤ 1/(2p − 1).

(8)

The first term E[B(`, k)T`,k |D̄(`) = 1] can be
bounded by writing it as E[B(`, k)T`,k |D̄(`) = 1] =
E[E[B(`, k)T`,k |D̄(`) = 1, A(`, k)]|D̄(`) = 1], where
A(`, k) denotes the pair of arms being pulled in iteration k round `.
We focus on the inner term E[B(`, k)T`,k |D̄(`) =
1, A(`, k)]. B(`, k) is observable given A(`, k). If
B(`, k) = 0 then this inner term is 0. If B(`, k) =
1 then this inner term is E[T`,k |A(`, k)] (where we
note that T`,k is conditionally independent of D̄(`)
given A(`, k)) and is bounded above by 1/(2p − 1)
by Lemma 1. In both cases, the inner term is

=E[T`,k |B(`, k) = 0, D(`) = 0]P (B(`, k) = 0|D(`) = 0)
N`
≤
P (B(`, k) = 0|D(`) = 0),
2p − 1
where the last equation follows from applying
Lemma 1 and iterated conditional expectation. Thus,
we know

N
−1
X

N`
P (B(`, k) = 0|D(`) = 0)E[D̄(`)]
2p − 1
k=1

`−1
N
−1
X
N`
1−p
≤
P (B(`, k) = 0|D(`) = 0)
2p − 1
p

(9) =

k=1

(10)

≤

1−p
p

`−1

2

2N `p
(log(N ) + 1),
(2p − 1)4

385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439

where equation (10) is because Lemma 2.
The proof of the second inequality follows very similarly, and is omitted.

In the sushi experiment, the user’s preference matrix
is given by Figure 1.

6. Proof of Theorem 2
In this section, we prove the cumulative expected weak
regret of WS-W is bounded by O(N 2 ) in the Condorcet
winner setting. First, we want to give an example to illustrate why our algorithm will not have O(N log(N ))
regret under the Condorcet winner setting.
In the Condorcet winner setting, Lemma 2 is no longer
true. Here is a counter example to illustrate why
Lemma 2 does not hold true anymore. Suppose we
have N = 3k + 1 arms in total, which includes a
Condorcet winner arm and three types of other arms:
k type-A arms, k type-B arms and k type-C arms.
Among these arms, we assume the user prefers typeA arms than type-B arms, type-B arms than type-C
arms and type-C arms than type-A arms. Among each
type of arms, there is a total order. In this setting, the
expected number of iterations with a worse incumbent
is O(N ) instead of O(log(N )), which means Lemma 2
is no longer true.
Now we start our proof for Theorem 2.
Proof. In the Condorcent winner setting, Lemmas 3
and 4 hold, but as explained earlier, Lemma 2 does
not. Because the proof of Lemma 5 utilizes Lemma 2,
Lemma 5 also no longer holds.
On the other hand, since we can have at most N − 1
iterations in a round, we know the following statement is true: the conditional expected number of iterations with a worse incumbent is bounded by N in
each round. Thus, we know Lemma 5 now becomes:

"N −1
X

#

`−1
N 2`
1−p
,
E
B̄(`, k)T`,k D̄(`) ≤
p
2p − 1
k=1
"N −1
# 
`
X
1−p
N 2`
E
B̄(`, k)T`,k V (`, k) ≤
.
p
2p − 1


k=1

Thus, following the same reasoning as in the proof of
Theorem 1, we know the expected weak regret in the
Condorcet winner setting is bounded by
NR
pN 2
+
,
2
(2p − 1)
(2p − 1)3
which concludes our proof.

7. Preference Matrices

In the MSLR experiment, the ranker’s preference matrix is given by:









0.535 0.613 0.757 0.765
0.5 0.580 0.727 0.738 

0.420 0.5 0.659 0.669 

0.276 0.341 0.5 0.510 
0.262 0.331 0.490 0.5

0.5
0.465
0.387
0.243
0.235

8. Condorcet Winner Experiment
In the main paper, we considered numerical examples
in which the arms have a total order. This is common
in the dueling bandits literature, where even work that
considers more general settings theoretically test their
methods on problems that satisfy the total order assumption (Komiyama et al., 2016; Urvoy et al., 2013).
In this section, we consider an additional example that
has a Condorcet winner but does not have a total order
among arms. The example has a cyclic struture, and
is similar to the cyclic example in Komiyama et al.
(2015).
The preference matrix is:


0.5 0.6 0.6 0.6
 0.4 0.5 0.6 0.4 


 0.4 0.4 0.5 0.6 
0.4 0.6 0.4 0.5


In the above example, arm 1 is the Condorcet winner.
Arm 2 beats arm 3, arm 3 beats arm 4 and arm 4 beats
arm 2.
Again, we consider both binary strong regret and the
utility-based strong regret. The utility-based strong
regret is defined the same as the other two experiments. The result is summarized in Figure 2. WS-S
outperforms all benchmarks considered in all time periods on binary regret, and outperforms them all in all
time periods except T = 102 on utility-based regret.

9. Sensitivity Analysis
In this section, we conduct a sensitivity analysis of β
in WS-S using the MSLR dataset. In this analysis,

440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494




























0.5
0.488
0.378
0.345
0.302
0.274
0.289
0.292
0.251
0.2
0.259
0.217
0.153
0.183
0.146
0.132

0.512
0.5
0.398
0.317
0.348
0.224
0.337
0.317
0.262
0.291
0.214
0.198
0.17
0.15
0.129
0.127

0.622
0.602
0.5
0.472
0.446
0.467
0.466
0.409
0.427
0.407
0.339
0.295
0.266
0.328
0.213
0.178

0.655
0.683
0.528
0.5
0.447
0.381
0.434
0.359
0.325
0.313
0.335
0.304
0.197
0.177
0.204
0.156

0.698
0.652
0.554
0.553
0.5
0.487
0.476
0.482
0.392
0.462
0.357
0.39
0.305
0.328
0.319
0.225

0.726
0.776
0.533
0.619
0.513
0.5
0.487
0.441
0.425
0.379
0.409
0.299
0.298
0.213
0.171
0.189

0.711
0.663
0.534
0.566
0.524
0.513
0.5
0.441
0.447
0.387
0.436
0.393
0.297
0.265
0.264
0.199

0.708
0.683
0.591
0.641
0.518
0.559
0.559
0.5
0.444
0.473
0.438
0.42
0.332
0.195
0.223
0.233

0.749
0.738
0.573
0.675
0.608
0.575
0.553
0.556
0.5
0.488
0.452
0.458
0.388
0.214
0.29
0.315

0.8
0.709
0.593
0.687
0.538
0.621
0.613
0.527
0.512
0.5
0.457
0.421
0.387
0.282
0.315
0.253

0.741
0.786
0.661
0.665
0.643
0.591
0.564
0.562
0.548
0.543
0.5
0.436
0.375
0.382
0.298
0.316

0.783
0.802
0.705
0.696
0.61
0.701
0.607
0.58
0.542
0.579
0.564
0.5
0.458
0.356
0.3
0.267

0.847
0.83
0.734
0.803
0.695
0.702
0.703
0.668
0.612
0.613
0.625
0.542
0.5
0.423
0.393
0.404

0.817
0.85
0.672
0.823
0.672
0.787
0.735
0.805
0.786
0.718
0.618
0.644
0.577
0.5
0.422
0.363

0.854
0.871
0.787
0.796
0.681
0.829
0.736
0.777
0.71
0.685
0.702
0.7
0.607
0.578
0.5
0.414

0.868
0.873
0.822
0.844
0.775
0.811
0.801
0.767
0.685
0.747
0.684
0.733
0.596
0.637
0.586
0.5




























Figure 1: User’s preference matrix for the Sushi experiment

(a) Cyclic dataset with utility-based strong regret

(b) Cyclic dataset with binary strong regret

Figure 2: Comparison of the strong regret between WS-S and 7 benchmarks on the cyclic dataset. WS-S
outperforms all benchmarks in all settings studied.
we choose β = 1.01, 1.05, 1.1, 1.2, 1.5 respectively and
compare them with RMED and RUCB. The result is
summarized in Figure 3.

ever, as long as β is within a reasonable range, WS-S
can outperform existing state-of-art algorithms.

References
Barndorff-Nielsen, Ole E and Shiryaev, Albert.
Change of time and change of measure, volume 21.
World Scientific Publishing Co Inc, 2015.
Karlin, Samuel. A First Course In Stochastic Processes. Academic Press, 1968.
Komiyama, Junpei, Honda, Junya, Kashima, Hisashi,
and Nakagawa, Hiroshi. Regret lower bound and
optimal algorithm in dueling bandit problem. In
COLT, pp. 1141–1154, 2015.
Figure 3: Sensitivity Analysis
Based on Figure 3, WS-S with β = 1.05, 1.1, 1.2 outperforms RMED and RUCB. When β = 1.01, we
spend too much time on the exploration period and
do not exploit enough. Similarly, WS-S with β = 1.5
over exploits and does not explore enough. In both
cases, WS-S underperforms RMED and RUCB. How-

Komiyama, Junpei, Honda, Junya, and Nakagawa,
Hiroshi. Copeland dueling bandit problem: Regret lower bound, optimal algorithm, and computationally efficient algorithm. arXiv preprint
arXiv:1605.01677, 2016.
Urvoy, Tanguy, Clerot, Fabrice, Féraud, Raphael, and
Naamane, Sami. Generic exploration and k-armed
voting bandits. In ICML (2), pp. 91–99, 2013.

495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549

Wang, Y.H. On the number of success in independent
trials. Statistica Sinica, 1993.

