Active Heteroscedastic Regression

A. Technical Lemmas

Finally, by applying Markov inequality, for any t > 0:

Lemma 6. Let z 2 Rd be a fixed vector. Let Uz? denote the subspace orthogonal to z. Assume n0 Cd log d.
Let x1 , x2 , . . . , xn0 denote random Gaussian vectors from
N (0, Id ) such that Uz? xi are mutually independent. Then
with probability at least 1
exp( Cn0 ), the following
holds for all v 2 Uz? such that kvk2 = 1:
◆
✓
◆
n0 ✓
X
1 0
zzT
zzT
T
T
n v
Id
x i x i Id
v  2n0 .
2
2
2
kzk
kzk
i=1
Proof. First, note that x̃i :=

✓

I

zzT
kzk2

◆

xi are iid Gaus-

sian random variables drawn from N (0, Uz? ). We can apply Lemma 14 of (Jain and Tewari, 2015) to get the statement of the lemma.
Lemma 7. Let R(1)  R(2)  · · ·  R(n) be the order
statistics of absolute values of a standard Gaussian sample
R1 , R2 , . . . , Rn . Then, with probability at least 1 1/n10 ,
R(k)  CU
for some positive constant CU .
R(k)
(k/n) .

Let µ = E[R̃(k) ]. For a fixed p
log n, and for any 1 
k  n, consider the moment:
 X
✓ ◆
p
p
l p
l
E[|R̃(k) µ| ] = E
( 1)
R̃(k)
µp l
l
l=1
p ✓ ◆
X
p
l

E[R̃(k)
]µp l
l
l=1
p ✓ ◆
X
p
l
l
=
(E[R̃(k)
])1/l µp l (6)
l
l=1

From Theorem 7 of (Gordon et al., 2006), we have:
p
p
l
(E[R̃(k)
])1/l  4 ⇡ l + ln(k + 1)  4 ⇡(p + ln n).
We also know from (Gordon et al., 2006) that:
µ = E[R̃(k) ]  C ln k  C ln n,

t) 

E[|R̃(k)
t

µ|p ]

or

E[|R̃(k) µ|p ]
.
t̃p
p
Choosing p = 10 ln n and t̃ = e 8 ⇡ + C)p, we get:
P (|R̃(k)

P (|R̃(k)

µ|

t̃) 

µ|

p
e 80 ⇡ + C) ln n)  e

10 ln n

=

1
n10

p
Note that P R̃(k)
10e 8 ⇡ + C) ln n  P |R̃(k)
p
µ| 10e 8 ⇡ + C) ln n . Finally, observing that R̃(k)
p
p
10e 8 ⇡ + C) ln n () R(k) 10e 8 ⇡ + C) ln n nk ,
p
we get the statement of the lemma with CU = 10e 8 ⇡ +
C).

B. Proofs
B.1. Proof of Theorem 1
(I) Consider the weighted least squares estimate:

k
ln n,
n

Proof. Define the scaled random variable R̃(k) =

µ|p

P (|R̃(k)

for some positive constant C. Substituting these upper
bounds in (6), we get:
p ✓ ◆
X
p
p
l
p
E[|R̃(k) µ| ] 
4 ⇡(p + ln n) (C ln n)p l
l
l=1
✓
◆p
p
= 4 ⇡(p + ln n) + C ln n
✓
◆p
p
 8 ⇡ + C)p

bGLS

=
=

(X T W X)

1

XT W y
n
X
(X T W X) 1
wi (h

⇤

i=1

, xi i + gi hxi , f ⇤ i)xi ,

where W is the diagonal matrix with wi = 1/hf ⇤ , xi i2
along the diagonal, gi are i.i.d. N (0, 1) random variables.
So we have:
bGLS

k bGLS

⇤

⇤ 2
k2

=
=

(X T W X)
✓

1

n
X
i=1

T

tr (X W X)

2

gi
⇤
hf , x
T

X W

ii

xi

0.5

T

gg W

0.5

X ,

Note that because E[ggT ] = In (where the expectation is
wrt. to the randomness in the labels given by the oracle O)
and tr is linear operator, we have:
✓
◆
⇤ 2
T
1
b
Ek GLS
k2 = tr (X W X)
.

Consider X T W X. We can apply Lemma 1 to lowerbound the (d
1) smallest eigenvalues of this matrix
by O(n2 /(kf ⇤ k2 d ln n)) and the largest eigenvalue by
O(n/kf ⇤ k2 ), with probability at least (1 ndc ). This im1
plies an upper-bound for the eigenvalues of (X T W X)
,
✓
and in turn its trace can be bounded by C 0 kf ⇤ k22 n1 +
◆
(d 1)d ln n
, for some constant C 0 > 0. The proof is comn2
plete.

◆

Active Heteroscedastic Regression

B.2. Proof of Lemma 1

corresponding to Uf ? is given by

1. By definition, the smallest singular value,

vector v 2 Uf ? , we have:

d (X

T

W X) =

inf

v2Rd ,kvk=1
T

f

kf k
=

n
X
i=1

n
X
i=1

vT X T W Xv

vT X T W Xv =

1
f
xi xTi
kf k
(xTi f )2

1
n
=
kf k2
kf k2

Let v⇤ denote the smallest eigenvector of X T W X. Write
v⇤ as:
q
v⇤ = 1 ↵d2 v? + ↵d f /kf k
where v? denotes the component along the subspace orthogonal to f , and ↵d = v⇤T f /kf k. Now:
v⇤T X T W Xv⇤ = ↵d2 . n/kf k2
n
T
X
(v?
xi )2
+ (1 ↵d2 )
T
(f xi )2
i=1
n
q
T
X
v?
xi
2
+ 2↵d 1 ↵d
Tx
f
i
i=1
 n/kf k2

where the inequality is due to the upper bound in (7). The
second term in the above equation can be lower bounded
with probability at least 1 1/n by (1 ↵d2 )n2 d/kf k2 .
To lower bound the
summation in
the third term as
qP
qP
T
P n v?
xi
n
n
1
T
2


T
T
2
i=1 f xi
i=1 (f xi )
i=1 (v? xi )
q
p
Pn
1
2n
i=1 (f T xi )2 , with probability at least 1
exp( 2n) (Using Lemma 6).q We conjecture that with
Pn
1
2
probability at least 1 1/n,
i=1 (f T xi )2  n/kf k
(note that it holds in expectation, shown by Gordon et al.
(2006)). So we have, with probability at least 1 2/n:
q
p
↵d2 n + (1 ↵d2 )n2 d 2↵d 1 ↵d2 n 2n
v

⇤T

T

⇤

X W Xv  n

For the above inequality to hold, it must be the case that
↵d2 1 16 nd1 2 .
2. Consider the variational characterization of the second
smallest singular value d 1 given by:
d 1 (X

T

W X) =

max

min

U :dim(U )=d 1 v2U,kvk=1

T

T

v X W Xv.

Consider the particular d 1 dimensional subspace Uf ? =
{v 2 Rd | vT f = 0}. Note that the projection matrix

ffT
kf k2

Id

◆

. For any

n
X
v T xi xT v
i

(8)

(xTi f )2

i=1

(7)

✓

Note that gi = vT xi , i = 1, 2, . . . , n and hi = xTi f ,
i = 1, 2, . . . , n are iid Gaussian random variables; in particular, as v is in the orthogonal subspace of f , gi and hi
are independent of each other. We will now lower bound
⌃ n ⌥
Pn (vT xi )2
2 , by dividing xi , x2 , . . . , xn into
i=1 (xT
2d ln n
f
)
i
batches of size s = 2d ln n. Let x(1) , x(2) , . . . , x(n) denote the new ordering of instances, such that
|xT(1) f |  |xT(2) f |  · · ·  |xT(n) f |.
Let B1 denote the first s instances according the new ordering. Using Lemma 7, we have, with probability at least
n
(1 2dnln
10 ):
s
X
v T xi xT v

s
1 X
n2
(vT xi xTi v)
2
2 k 2 ln2 n
CU
kf
k
k=1

i

k=1

(xT(k) f )2

✓

ffT
kf k2

We can replace v by Id

◆

v in the RHS of the above

inequality, which is true by definition. Now, we can apply
Lemma 6 to control the resulting quantity: with probability
at least 1 n14d , over all v 2 Uf ⇤? , we have:
s
X
v T xi xT v

1
n2
(d ln n)
CU2 4kf k2 d2 ln2 n

i

k=1

(xT(k) f )2

=

1
n2
CU2 4kf k2 d ln n

Plugging this lower-bound in (8), we get with probability
2
n
1
at least (1 2dnln
), d 1 (X T X) C 0 kf k2nd ln n .
10
n4d
B.3. Proof of Lemma 2
1. By definition, the smallest singular value,
d (X

T

X) =

inf

v2Rd ,kvk=1
n



vT X T Xv
n

X (xT f )2
f X
f
i
xi xTi
=
kf k i=1
kf k
kf
k2
i=1

 n⌧ 2

(9)

Let v⇤ denote the smallest singular vector of X T X. Write
v⇤ as:
q
v⇤ = 1 ↵d2 v? + ↵d f /kf k

Active Heteroscedastic Regression

where v? denotes the component along the subspace orthogonal to f , and ↵d = v⇤T f /kf k. Now:
v⇤T X T Xv⇤ = ↵d2

n
X
(f T xi )2

kf k2

i=1

↵d2 )

+ (1

n
X
i=1

+ 2↵d

q

T
(v?
xi )2

↵d2

1

n
X
(vT xi )(f T xi )
?

i=1

 n⌧ 2

kf k

The second term in the above equation can be lower
bounded with probability at least 1 exp( 2n) by (1
↵d2 ) n2 . We can upper bound the summation in the third term
qP
qP
Pn v T x f T x
n
n
(f T xi )2
T
2
as i=1 ? kfi k i 
i=1 kf k2
i=1 (v? xi ) 
p
p
⌧ n 2n, with probability at least 1 exp( 2n). The first
term is a positive quantity. So we have, with probability at
least 1 2 exp( 2n):
q
p
p
n
(1 ↵d2 )
2 2↵d 1 ↵d2 n n⌧
2
 v⇤T X T Xv⇤  n⌧ 2
This implies,
(1

↵d2 )

p
4 2↵d

q

Solving the above, we get
↵d2 1 50⌧ 2 .

p
↵d2 n n⌧

1

p

1

2⌧ 2  0

p
↵d2  5 2⌧ , and in turn,

2. Consider the variational characterization of the second
smallest singular value d 1 given by:
d 1 (X

T

X) =

max

min

U :dim(U )=d 1 v2U,kvk=1

vT X T Xv.

Consider the particular d 1 dimensional subspace Uf ? =
{v 2 Rd | vT f = 0}. Note that
✓ the projection
◆ matrix
corresponding to Uf ? is given by

Id

Uf ? , we have:

vT X T Xv =
=

n
X
i=1

✓

v T Id

◆

✓

n
X
i=1

ffT
xi xTi Id
kf k2

ffT
kf k2

. For v 2

vT xi xTi v
◆

ffT
v
kf k2

where the above equality follows by definition. ✓ Even
though xi ’s are not independent, observe that Id

ffT
kf k2

◆

xi are iid random variables from the distribution
T

ff
N (0, (Id kf
k2 )) and therefore we can invoke Lemma 6 to
bound the above quantity uniformly over all v 2 Uf ? , with
probability at least 1 exp( 2n), by n/2. Thus we have
a lower-bound for minv2Uf ? vT X T Xv, which immediately implies a lower bound for d 1 . We conclude that
1
T
d 1 (X X)
2 n with probability at least 1 exp( 2n).

B.4. Proof of Lemma 3
Note that gi are iid draws from N (0, kzk2 ). First note that
for any i,
⌧2
⌧2
2⌧
1
p e 2kzk2 > ⌧ e 2kzk2 .
2
2⇡

P (|gi |  kzk⌧ )
We divide U into
we have:

m⌧
2

batches of size ⌧2 . For each batch Bj ,

P (min |gi | > ⌧ ) = 1
i2Bj

✓
 1
1

P (|gi |  ⌧ )
1 ⌧2
⌧ e 2kzk2
2
⌧2

2/⌧

◆2/⌧

e 2kzk2
⌧2

So, P (mini2Bj |gi | ✓⇢ ⌧ ) > e 2kzk2 .

As the batches
◆

are independent, P
i : |gi |  ⌧
✓
◆
3
2
P 8j, mini2Bj |gi |  ⌧ > e m⌧ /(4kzk ) .

m⌧
2

B.5. Proof of Lemma 4
Recall that yi = h ⇤ , xi i + ⌘i where ⌘i = hxi , f ⇤ igi where
gi ⇠ N (0, 1). Consider the following RV:
(f ⇤ )T Sf ⇤ =

m1
1 X
(xT (
m1 i=1 i

⇤

0

+ gi f ⇤ ))2 (xTi f ⇤ )2 .

(10)
As f ⇤ , ⇤ , 0 , gi are all fixed w.r.t. xi . Hence, xTi ( ⇤
⇤
⇤
⇤ 2
T ⇤
⇠
0 + gi f ) ⇠ N (0, k
0 + gi f k ) and xi f
⇤ 2
N (0, kf k ). Hence, for all i, w.p.
1 3 exp( m1 ):
⇤ 2 T ⇤ 2
⇤ 2
⇤
we have (xTi ( ⇤
0 + gi f )) (xi f )  2kf k (k
2
2
⇤ 2
0 k + log m1 kf k ) log m1 . Using standard Hoeffding
bound, we have w.p. 1 m110 :
1

1
m1

m1
X

(xTi (

⇤

0

+ gi f ⇤ ))2 (xTi f ⇤ )2

i=1

3kf ⇤ k4

k

⇤

log3 m1
 p
· 2kf ⇤ k2 (k
m1

2
⇤ 2
0 k kf k |
⇤

2
0k

+ log m1 kf ⇤ k2 ).

Active Heteroscedastic Regression

That is,
m

1
1 X
⇤ 2 T ⇤ 2
(xT ( ⇤
0 + gi f )) (xi f )
m1 i=1 i
✓
◆
10 log3 m1
1
(3kf ⇤ k4 + k ⇤
p
m1

2
⇤ 2
0 k kf k ).

(11)

T ⇤
Similarly, let f? be a unit vector s.t. f?
f = 0. Now,
consider the following RV:
m1
1 X
(f? ) Sf? =
(xT (
m1 i=1 i
T

⇤

0

+ gi f ⇤ ))2 (xTi f? )2 .

Using similar argument as above, we have w.p.
3 exp( m1 )
:
(f? )T Sf?  (kf ⇤ k2 + k

⇤

0k

(12)
1

Hence, using the fact that m1 = ⌦(d log d) along with
standard ✏-net argument, we have:
f,kf k=1,f ?f ⇤

2

0 k ).

(14)

Lemma 3 ensures that, with probability at least
n3
exp( 4m2 kf
⇤ k2 ), there will be least n = |L| (by the assumption on n in the statement of the theorem) samples
at the end of Step 1 of the Algorithm. Now, consider the
weighted least squares estimate computed in Step 2 of Algorithm 2:

=

T

(X W X)

1
1

XT W y
n
X
1
i=1

hxi , f ⇤ i2

(h

⇤

⇤

, xi i + gi hxi , f i)xi ,

where gi are i.i.d. N (0, 1) random variables. So we have:
b

⇤

(X T W X)

=

1

n
X
i=1

1
gi hxi , f ⇤ ixi ,
hxi , f ⇤ i2

and
kb

⇤ 2
k2

=

tr (X T W X)

2

X T W 0.5 ggT W 0.5 X ,

T

m2
.
d log2 d

W X)

Now, again using same argument as Lemma 1 along with
(f ⇤ )T X T W Xf ⇤ = n and the above bound, we can show
n
that d
2.
✓
◆
T
1
Theorem now follows by using tr (X W X)

d (X

B.6. Proof of Theorem 2

(X T W X)

d 1 (X

1

⇤

Lemma now follows using (11) and (14).

b =

In particular, d  (f ⇤ )T X T W X(f ⇤ ) = n. Now, we
wish to bound smallest eigenvalue of X T W X in space orthogonal to f ⇤ . Note that our algorithm selects xi s.t. i is
amongst n smallest |xTi f ⇤ |. Also, n 4d. Let i1 , . . . , i2d
be s.t. ik 2 L and |xTi1 f ⇤ |  · · ·  |xTi2d log d f ⇤ |. Note that
d
using Lemma 7, w.h.p. |xTi2d log d f ⇤ | = O( d log
m ).

(13)

3

min
f T Sf 
0
1
s
3
10d log d A
1.1 @1 +
(kf ⇤ k2 + k
m1

We now lower bound each eigenvalue of (X T W X) 1 to
obtain the required bound. Note that this claim is similar to
Lemma 1.

Hence, using argument similar to Lemma 1, we have:

2

)
!
p
3
10 log m1 log(1/ )
1+
.
p
m1

Note that because E[ggT ] = In (where the expectation is
wrt. to the randomness in the labels given by the oracle O)
and tr is linear operator, we have:
✓
◆
⇤ 2
T
1
b
Ek GLS
k2 = tr (X W X)
.

T W X)

+

d

d

1 (X

T W X)

.

B.7. Proof of Theorem 3
c denote the diagonal matrix with estimated weights
Let W
cii := w
W
bi = 1/(hfb, xi i2 + 2 ). Consider the weighted
least squares estimate:
bGLS

=
=

c X)
(X T W

cy
XT W
n
X
c X) 1
(X T W
w
bi (h
1

i=1

⇤

, xi i + gi hxi , f ⇤ i)xi ,

where gi are i.i.d. N (0, 1) random variables. Rearranging,
we get:
bGLS

k bGLS

⇤

=

⇤ 2
k2

=

n
X
gi hf ⇤ , xi i
xi
2
2
b
i=1 hf , xi i +
✓
◆
f ggT W
fX ,
tr (X T W X) 2 X T W

c X)
(X T W

1

f is the n ⇥ n diagonal matrix with W
fii =
where W
hf ⇤ ,xi i
T
. Note that because E[gg ] = In (where the
hfb,xi i2 + 2
expectation is wrt. to the randomness in the labels given by
the oracle O) and tr is linear operator, we have:
✓
◆
⇤ 2
c X) 2 X T W
f2X
Ek bGLS
k2 = tr (X T W

Active Heteroscedastic Regression

Write f ⇤ = fb + f , where k f k2  . Let W denote
h ,x i
f as:
the matrix with Wii = b f 2 i 2 . We can bound W

where gi are i.i.d. N (0, 1) random variables. So we have:

hf ,xi i +

f 2  2(W
c+
W

So, we have:
Ek bGLS

kb

✓

⇤ 2
k2

✓

W 2)

c X)
= 2tr (X T W

Tc

+2tr (X W X)

2

X

T

✓

2

1

W X

◆
◆

Tc

(15)
◆

1.
Consider the first term tr (X W X)
=
Pn
1
T
It can be bounded readily by
i=1 hfb,xi i2 + 2 xi xi .
maxi (hfb, xi i2 + 2 )tr (X T X) 1 .
We can bound
tr (X T X) 1 by nd , using standard arguments. Applying Lemma 7, we can bound maxi (hfb, xi i2 + 2 ) by
CU2 ln2 n + 2 , with
at least 1 1/n10 . To✓ probability ◆
c X)
gether, we have tr (X T W
2. ✓

Now

Tc

tr (X W X)

2

to

X

T

1

1

 CU2 nd ln2 n.

bound
the
second
term
◆
W X , first consider the mah

c X)
bound tr (X T W

,x i2

similar to the case above.
✓
◆
c X) 2 X T W 2 X
Together, we have, tr (X T W

✓
◆
⇤ 2
4
c X) 2 ) kX T Xk2  d ln24 n (n) = kf k d ln n .
tr (X T W
n
n
1

Plugging the above two bounds in (15), the proof is
complete.
B.8. Proof of Lemma 5
Denote |L| by n⌧ . Let X 2 Rn⌧ ⇥d denote the design matrix with instances in L as rows. Consider the ordinary least
squares estimate:

=

(X T X)

⇤

⇤ 2
k2

=
=

(X T X)

1

✓

n⌧
X
i=1

tr (X T X)

2

gi hxi , f ⇤ ixi
XT W

0.5

ggT W

0.5

◆
X ,

where W 0.5 is the diagonal matrix with ith diagonal entry
hf ⇤ , xi i. Note that because E[ggT ] = In⌧ and tr is linear
operator, we have:
✓
◆
⇤ 2
Ek b
k2 = tr (X T X) 2 X T W 1 X ,
Now, write f ⇤ = fb + f , where k f k 
(as given in
⇤
the statement of the Theorem). So, hf , xi i = hfb, xi i +
h f , xi i, and hf ⇤ , xi i2  2kf ⇤ k2 ( 2 + ⌧ 2 ).
✓
◆
⇤ 2
T
2 T
1
b
Ek
k2 = tr X(X X) X W
,
✓
◆
2
2
T
2 T
= 2(⌧ +
)tr X(X X) X
=

2(⌧ 2 +

2

)tr (X T X)

1

2

trix W 2 . The ith entry of this matrix is b f 2 i 2 2 .
(hf ,xi i + )
Without loss of generality, assume f is orthogonal to
k k2
fb. In expectation, the diagonal entry is at most f4 .
From Lemma 4, and by the choice of in the statement
of the theorem, the quantity is at most kf ⇤ k2 . Thus
in expectation
W 2 can◆ be bounded by In . We can
✓

b =

b

XT y
n⌧
X
(X T X) 1
(h

by O

1
n⌧ ⌧ 2

+ dn⌧1 . Using Lemma 3 we can lower bound

n⌧ by m⌧ with probability at least exp( m⌧ 3 ). This completes the proof.
B.9. Proof of Theorem 4

From Lemma 3, we know that about n⌧ = m⌧
2 instances
out of m unlabeled instances satisfy the tolerance condition
in Step 4 of the algorithm with high probability. So, we
want to choose ⌧ as a function of = kfb f ⇤ k, m, and
d so that the RHS of the bound in Lemma 5 is minimized.
Solving the resulting quadratic problem, we see that ⌧ =
⇤
is optimal choice, up to constant
p and kf k factors. From
Lemma 4, we have
= O( d/m1 ). Choosing m1 =
n/2, we then have with probability at least
p exp( 1/n), at
b
least n examples satisfying |hxi , f i|  2 d/n in Step 4 of
the algorithm. We can now apply Lemma 5 to recover the
statement of the theorem.

C. Iterative Estimation Algorithm of (Carroll
et al., 1988)

1

i=1

We can use identical arguments as in the proof of Lemma
2, we✓can upper bound
◆ the trace quantity in the above RHS

⇤

, xi i + gi hxi , f ⇤ i)xi ,

We now apply the analysis of (Carroll et al., 1988) to bound
the estimation error of weighted least squares estimator

Active Heteroscedastic Regression

with estimated weights (Algorithm 3). In fact, Carroll et al.
(1988) develop an iterative algorithm where the estimates fb
and b are iteratively improved. So we will mimic the setup,
and derive bounds for the iterative version of Algorithm 3.
In the following, bt and fbt denote the estimators at the end
of round t. We use the same 0 as in Algorithm 3. Define
the following quantities:

(t)
rbi

1. rbi :=
= yi
when t is implicit.

Define the quantities:
Rd⇥d 3 B0

=

XT W X

=

XT W

R 3 ⌘i

=

R 3 l0

=

Rd 3 l 1

=

Rd⇥1 3 v0
d

hxi , bt i; sometimes we write rbi

1

B0 v 0
n
X
B0 1
g0T rf wi xi ⌘i
i=1

d

R 3 l2

=

i

= yi

⌧i , where ⌧i = hxi ,

⇤

i

:=

n
X

I)f .

( i , f ) = ( i2 xi xTi

R

Let:
n

Rd⇥d 3 Af

=

Rd⇥d 3 A

=

Rd⇥d 3 A1

=

Rd⇥d 3 H1

=

2

⇥d

3W

=

Rd⇥1 3 g0

=

1X
r
n i=1

1
p
2 n

i=1

i

i

I ⌦ x i A1 1 r ⌧ ⌧

n
X
1
p A1 1
n
i=1

i=1

n
X

0.5

2

⇥d

=

g0T r2f wi g0 xi ⌘i

1

B0

X
n
i=1

3Q

i

. g0 xTi

◆

⇤

n
X

=

xi ⌘i rf wiT H1

B0 1 xi ⌦ (rf wiT ⌦ I)W ⌘i

i=1

in terms of bt

1
1
l0 + p l 1 + l 2
n
n
✓
1
p C + [I ⌦ ( bt
n

=
+
+

Op (n

3/2

).

◆

) ]Q ( bt

⇤ T

)

⇤

Corollary 1 (Case f is known). When f is known, we
have: l1 = l2 = C = Q = 0. So for all t > 0, we have:
⇤

. xTi

bt

⇤

= l0 = (X T W X)

Note that the initial b0 satisfies:

i

I)T rf wi xi ⌘i

(g0T rf wi )(g0T rf wj )(xTi B0 xj )xi ⌘j

3C

bt+1

E[Af ] = E[ 12 x1 xT1 ]
✓
n
p
1X
A1 1
nA +
r⌧ f
n i=1
n
X

n
p X
n
g0T (Af 1 A1

Lemma 9 (Bounding bt+1

i

n

Rd

d⇥d

Rd
1X
rf
n i=1



i=1

i.

i,j=1

3.

1

B0

+
2.

xTi B0 v0

i

( b0

⇤

) = (X T X)

1

1

XT W .

X T := ⇠0 .

Corollary 2 (Case f ⇤ is estimated). We have:
Lemma 8 (Bounding fbt f in terms of bt
). As n !
1, the error in the estimate fb has the expansion:
fbt

f

⇤

=
+
+

1
p Af 1 A1 g 0
n
✓
1
p H1 + [I ⌦ ( bt
n
Op (n

3/2

),

1. b1

=
+
+

◆

) ]W ( bt

⇤ T

⇤

)

where Op (n 3/2 ) captures lower-order error quantities
that converge (in probability) to 0 at or faster than the rate
1
O np
.
n

and for t
2. bt

1
l0 + p l 1
n
1
l2 + C⇠0 + (I ⌦ ⇠0T )Q⇠0
n
Op (n 3/2 ),

(16)

1
l 0 + p l1
n
1
l2 + Cl0 + (I ⌦ l0T )Ql0
n
Op (n 3/2 ) .

(17)

2,
⇤

=
+
+

)

Active Heteroscedastic Regression

The bounds obtained offer little insight, and importantly,
the dependence on factors n and d are not clear. Even for
the case when f ⇤ is known, the analysis gives no convergence rates.

D. Active Regression
Algorithm 5 considers a slightly more powerful oracle
model, where the same instance can be queried multiple
times, and each time the response is generated independent
of the other trials. Theorem 5 shows that the learning rate
in this setting is O(1/n), as in Theorem 2.
Theorem 5 (Active Regression with Noise Oracle). Assume n d. Consider the output estimator b of Algorithm
5. We have, with probability at least 1 1/nc :
✓ ◆
⇤ 2
0
⇤ 2 1
b
k
k2  C kf k2
,
n
for some positive constants c, C 0 .

⇤

⇤T

Proof. First, note that the matrix N? = Id fkff⇤ k2 cor2
responds to (d 1) directions orthogal to f ⇤ , and thus we
have N ? f ⇤ = 0. Let N = kf 1⇤ k2 f ⇤ 1Tn d as in the Step
2 of the algorithm. Clearly, when n = d + 1, the matrix
X = [N? N ]T has full rank, with all the d singular values equal to 1. For a general n > d, the largest singular
value of X is proportional to n, while the other singular
values are 1. In this case, notice that the direction of the
largest singular vector of X is f ⇤ . Let xi denote the rows
(instances) of this X.
Now consider the ordinary least squares estimate:
b =
=

(X T X)

1

XT y
n
X
(X T X) 1
(h
i=1

=

⇤

+ (X T X)

1

⇤

, xi i + gi hxi , f ⇤ i)xi ,

✓X
d

0+

i=1

n
X

i=d+1

◆
gi f ⇤ ,

where gi are i.i.d. N (0, 1) random variables, and the last
equality is true by construction of X. So we have:
kb

⇤

k

=

(X T X)

n
X

1

gi f ⇤

i=d+1



(X T X)

1 ⇤

f

n
X

gi

i=d+1

Notice that f ⇤ is the smallest singular vector of (X T X) 1 ,
and therefore (X T X) 1 f ⇤ is proportional to the smallest singular value of (X T X) 1 , which is 1/k(X T X)k =

1/n. So:
kb

⇤

k  kf ⇤ k

n
1 X
gi .
n
i=d+1

The sum in the above term can be controlled with high
probability using Chernoff bounds,
p with
Pn which yields,
probability at least 1 1/nc , | i=d+1 gi |  C 0 n d,
for c, C 0 > 0. The proof is complete.
Using essentially identical arguments, we can also prove a
lower bound, so that effectively we have:
✓
◆
⇤ 2
⇤ 21
b
k
k2 = O kf k2
.
n

Active Heteroscedastic Regression

Algorithm 5 Active Regression With Noise Oracle
Input: Labeling oracle O, noise model f ⇤ , label budget n > d.
f ⇤f ⇤T
1. Form the matrix N? = Id
, and query O for (exact) labels of each column of the matrix (call them
kf ⇤ k22
y1 , y2 , . . . , yd .
2. Make n d queries to O and obtain (noisy) labels along the direction f ⇤ . Call these labels yd+1 , yd+2 , . . . , yn . Let
N = kf 1⇤ k2 f ⇤ 1Tn d , where 1Tn d denotes the vector of all ones, in n d dimensions.
2. Estimate b by solving y ⇡ X b (ordinary least squares) where X = [N? N ]T 2 Rn⇥d and y 2 Rn .
Output: b.

