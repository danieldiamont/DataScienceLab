Lost Relatives of the Gumbel Trick

APPENDIX: Lost Relatives of the Gumbel Trick
Here we provide proofs for the results stated in the main text, together with additional supporting lemmas required for
these proofs.

A. Comparison of Gumbel and Exponential tricks
In Section 2.3.1 we analyzed the asymptotic efficiency of different estimators of Z by measuring their asymptotic variance.
(As all our estimators in the full-rank perturbation setting are consistent, their bias is 0 in the limit of infinite data, and so
this asymptotic variance equals the asymptotic MSE.) In the non-asymptotic regime, where an estimator Ẑ is constructed
from a finite set of M samples, we can analyze both the variance var(Ẑ) and the bias (E[Ẑ] Z) of the estimator. While
in most cases these cannot be obtained analytically and there we can resort to an empirical evaluation, for the estimators
stemming from the Gumbel and Exponential tricks analytical treatment turns out to be possible using standard methods.
A.1. Estimating Z
Gumbel trick The Gumbel trick yields an unbiased estimator for ln Z, and we can turn it into a consistent estimator of
Z by exponentiating it:
M
1 X
Xm
M m=1

Ẑ := exp

!

iid

where

X1 , . . . , XM ⇠ Gumbel( c + ln Z).

Recalling that the moment generating function of a Gumbel(µ) distribution is G(t) = (1
independence of the samples:

E[Ẑ] =

M
Y

E[eXm /M ] =

m=1

E[Ẑ 2 ] =

M
Y

⇣

E[e2Xm /M ] =

m=1

1/M )e(ln Z

(1

⇣

c)/M

2/M )e2(ln Z

(1

⌘M

c)/M

1/M )M e

= (1

⌘M

t)eµt , we can obtain by using

= (1

c

Z,

2/M )M e

2c

Z 2.

Therefore the squared bias, variance and MSE of the estimator Ẑ are, respectively:
bias(Ẑ)2 = (E[Ẑ]
2

var(Ẑ) = E[Ẑ ]

Z)2 = Z 2
2

E[Ẑ] = Z

1/M )M e

(1
2

(1

2

MSE(Ẑ) = bias(Ẑ) + var(Ẑ) = Z

2

M

c

2/M ) e
(1

1 ,
2c
M

2/M ) e

(1
2c

1/M )2M e
2 (1

2c
M

1/M ) e

,
c

+1 .

These formulas hold for M > 2 where the moment generating functions are defined. For M = 1 the estimator has infinite
bias (and infinite variance), and for M = 2 it has infinite variance. Figure 1 (left) shows the functional dependence of
MSE(Ẑ) on the number of samples M 3, in units of Z 2 .
Exponential trick The Exponential trick yields an unbiased estimator of 1/Z, and we can turn it into a consistent
estimator of Z by inverting it:

Ẑ :=

M
1 X
Xm
M m=1

!

1

where

iid

X1 , . . . , XM ⇠ Exp(Z).

As X1 , . . . , XM are independent and exponentially distributed with identical rates Z, their sum follows the Gamma distribution with shape M and rate Z. Therefore the estimator Ẑ can be written as Ẑ = M Y , where Y ⇠ InvGamma(M, Z).

Lost Relatives of the Gumbel Trick

Recalling the mean and variance of the Inverse-Gamma distribution, we obtain:
✓
◆
M
1
bias(Ẑ)2 = (E[Ẑ] Z)2 = Z 2
1 = Z2
,
M 1
M 1
1
var(Ẑ) = Z 2 M 2
,
(M 1)2 (M 2)
M 2 + M2
M +2
MSE(Ẑ) = bias(Ẑ)2 + var(Ẑ) = Z 2
= Z2
2
(M 1) (M 2)
(M 1)(M

2)

.

Again these formulas hold for M > 2 where the relevant expectations are defined: for M = 1 the estimator has infinite
bias, and for M 2 {1, 2} it has infinite variance. Figure 1 (left) shows the functional dependence of MSE(Ẑ) on the
number of samples M
3, in units of Z 2 . By inspecting the curves we observe that the Gumbel trick estimator requires
roughly 45% more samples to yield the same MSE as the Exponential trick estimator.
A.2. Estimating ln Z
A similar analysis can be performed for estimating ln Z rather than Z. In that case the Gumbel trick estimator of ln Z is
1 ⇡2
unbiased and has variance (and thus MSE) equal to M
6 . On the other hand, the Exponential trick estimator is
!
M
1 X
iid
d
ln Z = ln
Xm
where
X1 , . . . , XM ⇠ Exp(Z).
M m=1
Again

PM

m=1

Xm ⇠ Gamma(M, Z) and by reference to properties of the Gamma distribution,
d
bias(ln
Z)2 = (E[Ẑ]
d
var(ln
Z) =

Z)2 = (ln M

( (M )

ln Z)

2

ln Z) = (ln M

2

(M )) ,

1 (M ),

d
d
d
MSE(ln
Z) = bias(ln
Z)2 + var(ln
Z) = (ln M

2

(M )) +

1 (M ),

where (·) is the digamma function and 1 (·) is the trigamma function. Note that the estimator can be debiased by
subtracting its bias (ln M
(M )). Figure 1 (right) compares the MSE of the Gumbel and Exponential trick estimators
of ln Z. We observe that the Gumbel trick estimator performs better only for M = 1, and even in that case the Exponential
trick estimator is better when debiased.

B. Sum-unary perturbations
Recall that sum-unary perturbations refer to the setting where each variable’s unary potentials are perturbed with Gumbel
noise, and the perturbed potential of a configuration sums the perturbations from all variables (see Definition 3 in the
main text). Using sum-unary perturbations we can derive a family U (↵) of upper bounds on the log partition function
(Proposition 4) and construct sequential samplers for the Gibbs distribution (Algorithm 1). Here we provide proofs for the
related results stated in Sections 3.1 and 3.2.
Notation

We will write pow x for x , where x,

2 R, when we find this increases clarity of our exposition.

Lemma 13 (Weibull and Fréchet tricks). For any finite set Y and any function h, we have

⇢
X
W (y)
i.i.d.
pow
pow h(y) = EW min h(y)
where {W (y)}y2Y ⇠ Weibull(1, ↵ 1 )
y
(1 + ↵)
↵
1/↵
y2Y

⇢
X
F (y)
i.i.d.
pow
pow h(y) = EF max h(y)
where {F (y)}y2Y ⇠ Fréchet(1, ↵ 1 )
y
(1 + ↵)
↵
1/↵
y2Y

for ↵ 2 (0, 1),
for ↵ 2 ( 1, 0).

Proof. This follows from setting up competing exponential clocks with rates y = h(y) 1/↵ and then applying the function g(x) = x↵ as in Example 1 for the case of the Weibull trick. The case of the Fréchet trick is similar, except that g is
strictly decreasing for ↵ 2 ( 1, 0), hence the maximization in place of the minimization.

Lost Relatives of the Gumbel Trick

B.1. Upper bounds on the partition function
Proposition 4.

For any ↵ 2 ( 1, 0) [ (0, 1), the upper bound ln Z  U(↵) holds with
U (↵) := n

⇥
1
ln E e
↵

ln (1 + ↵)
+ nc
↵

↵U

⇤

.

Proof. We show the result for ↵ 2 (0, 1) using the Weibull trick; the case of ↵ 2 ( 1, 0) can be proved similarly using
the Fréchet trick. The idea is to prove by induction on n that Z ↵
e ↵U (↵) , so that the claimed result follows by
applying the monotonically decreasing function x 7! ln(x)/↵.
The base case n = 1 is the Clamping Lemma 7 below with j = n = 1. Now assume the claim for n 1
1 and for
xn 2 Xn define
"
(
)!#
n
X
ln (1 + ↵)
1
Un 1 (↵, x1 ) := (n 1)
+ (n 1)c
ln E exp
↵ max
(x) +
.
i (xi )
x2 ,...,xn
↵
↵
i=2
With this definition, the Clamping Lemma with j = 1 states that
Z

↵

pow
↵

X

pow e

=e

1 (↵,x1 )

x1

pow

1/↵

e

↵Un

1 (↵,x1 )

 pow

1/↵

e

↵U (↵)

, so:

[inductive hypothesis]

1/↵

x1 2X1

pow pow e
↵

↵Un

P

↵U (↵)

[Clamping Lemma]

1/↵

↵U (↵)

,

as required to complete the inductive step.
Proposition 5.

The limit of U (↵) as ↵ ! 0 exists and equals U (0) := E[U ], i.e. the Gumbel trick upper bound.

⇥ ↵U ⇤
1
Proof. Recall that U (↵) = n ln (1+↵)
+ nc
. The first term tends to n (1) = cn as
↵
↵ ln E e
⇥ ↵ !
⇤ 0 by
L’Hôpital’s rule, where is the digamma function. The second term is constant in ↵. In the last term, E e ↵U is the
moment generating function of U evaluated at ↵, and as such its derivative at ↵ = 0 exists and equals the negative of the
mean of U . Hence by L’Hôpital’s rule,
lim

↵!0

⇥
1
ln E e
↵

↵U

⇤

=

lim

↵!0

E[U ]
= E[U ] = U (0).
E [e ↵U ]

The claimed result then follows by the Algebra of Limits, as the contributions of the first two terms cancel.
Proposition 6.

The function U (↵) is differentiable at ↵ = 0 and the derivative equals
d
U (↵)
d↵

=n
↵=0

⇡2
12

var(U )
.
2

Proof. First we show that U (↵) is differentiable on ( 1, 0) [ (0, 1), and that the limit of the derivative as ↵ ! 0 exists
and equals n⇡ 2 /12 var(U )/2.
The first term of U (↵) is n ln
derivative equals

where

(1+↵)
,
↵

which is differentiable for ↵ 2 ( 1, 0) [ (0, 1) by the Quotient Rule, and its

d ln (1 + ↵)
(1 + ↵)↵ ln (1 + ↵)
n
=n
,
d↵
↵
↵2
is the digamma function (logarithmic derivative of the gamma function). Applying L’Hôpital’s rule we note that
lim

↵!0

d ln (1 + ↵)
(1 + ↵) + ↵
n
= n lim
↵!0
d↵
↵

(1)

(1 + ↵)
2↵

(1 + ↵)

(1)

=n

(1)
⇣(2)
⇡2
=n
=n ,
2
2
12

Lost Relatives of the Gumbel Trick

where (1) is the trigamma function (derivative of the digamma function), whose value at 1 is known to be ⇣(2) = ⇡ 2 /6,
the Riemann zeta function evaluated at 2.
The second term of U (↵) is constant in ↵. The last term can be written as K( ↵)/( ↵), where K is the cumulant
generating function (logarithm of the moment generating function) of the random variable U . The cumulant generating
function is differentiable, and by the Quotient rule
d K( ↵)
=
d↵
↵

↵K 0 ( ↵) K( ↵)
.
↵2

Applying L’Hôpital’s rule we note that
lim

↵!0

d K( ↵)
K 0 ( ↵) + ↵K 00 ( ↵)
= lim
↵!0
d↵
↵
2↵

K 0 ( ↵)

=

K 00 (0)
var(U )
=
,
2
2

where we have used that the second derivative of the cumulant generating function is the variance.
As U (↵) is continuous at 0 by construction, the above implies that it has left and right derivatives at 0. As the values of
these derivatives coincide, the function is differentiable at 0 and the derivative has the stated value.
Recall that for a variable index j 2 {1, . . . , n} we also defined partial sum-unary perturbations
8
9
n
<
=
X
Uj (x1 , . . . , xj 1 ) := max
(x) +
,
i (xi )
xj ,...,xn :
;
i=j

which fix the variables x1 , . . . , xj

1

and perturb the remaining ones.

Lemma 7 (Clamping Lemma). For any j 2 {1, . . . , n} and any fixed partial variable assignment (x1 , . . . , xj
X1 ⇥ · · · ⇥ Xj 1 , the following inequality holds with any trick parameter ↵ 2 ( 1, 0) [ (0, 1):
X

xj 2Xj

E

E
h

e

h

e

(n j) ln (1+↵) ↵(n j)c)

e

↵Uj+1 (x1 ,...,xj )

(n (j 1)) ln (1+↵) ↵(n (j 1))c)

e

i

↵Uj (x1 ,...,xj

1)

2

1/↵

1)

i

1/↵

.

Proof. For ↵ > 0, from the Weibull trick (Lemma 13), using independence of the perturbations and Jensen’s inequality,
2
3
n
X
Y
W (xi ) 5
pow
pow EW 4 min p̃(x) ↵
xj+1 ,...,xn
(1 + ↵)
↵
1/↵
i=j+1
xj 2Xj
8
93
2
2
3
n
<
Y
W (xi ) 5 W (xj ) =5
= EW 4 min EW 4 min p̃(x) ↵
xj+1 ,...,xn
xj 2Xj :
(1 + ↵)
(1 + ↵) ;
i=j+1
2
3
n
Y
W (xi ) 5
 EW 4 min p̃(x) ↵
xj ,...,xn
(1 + ↵)
i=j
Representing the Weibull random variables in terms of Gumbel random variables using the transformation W = e
where ⇠ Gumbel( c), and manipulating the obtained expressions yields the claimed result.

( +c)↵

,

Lost Relatives of the Gumbel Trick

B.2. Sequential samplers for the Gibbs distribution
The family of sequential samplers for the Gibbs distribution presented in the main text as Algorithm 1 has the same overall
structure as the sequential sampler derived by Hazan et al. (2013) from the Gumbel trick upper bound U (0), and hence
correctness can be argued similarly. Conditioned on accepting the sample, the probability that x = (x1 , . . . , xn ) is returned
is
⇥
⇤ 1/↵
1/↵
n
n
Y
Y
E e ↵Ui+1 (x1 ,...,xi )
e ↵ (x1 ,...,xn )
e c
e nc
pi (xi ) =
=
/ p(x),
(1 + ↵)1/↵ E ⇥e ↵Ui (x1 ,...,xi 1 ) ⇤ 1/↵
(1 + ↵)n/↵
E[e ↵U ] 1/↵
i=1
i=1

as required to show that the produced samples follow the Gibbs distribution p. Note, however, that in practice one introduces an approximation by replacing expectations with sample averages.
B.3. Relationship between errors of sum-unary Gumbel perturbations
We write x⇤ for the (random) MAP configuration after sum-unary perturbation of the potential function, i.e.,
(
)
n
X
⇤
x := argmax
(x) +
i (xi ) .
x2X

i=1

Let qsum (x) := P[x = x⇤ ] be the probability mass function of x⇤ .
The following results links together the errors acquired when using summed unary perturbations to upper bound the log
partition function ln Z  U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample
from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum
using the bound due to Maji et al. (2014).
Proposition 11. Writing p for the Gibbs distribution, we have
(U (0) ln Z) + KL(qsum k p) = E i [ i (x⇤i )] H(qsum ) .
|
{z
} |
{z
} |
{z
}
sampling error

error in ln Z bound

error in entropy estimation

Proof. By conditioning on the maximizing configuration x⇤ , we can rewrite the Gumbel trick upper bound U (0) as follows:
"
(
)#
n
X
U (0) = E max ✓(x) +
i (xi )
x2X

=

X

i=1

qsum (x) ✓(x) + E

x2X

=

X

qsum (x)✓(x) +

n
X

"

n
X

i (xi )

i=1

|x=x

⇤

#!

E i [ i (x⇤i )] .

i=1

x2X

At the same time, the KL divergence between qsum and the Gibbs distribution p generally expands as
KL(qsum k p) =

H(qsum )

=

H(qsum )

X

x2X

X

qsum (x) ln P

exp (✓(x))
x̃2X exp (✓(x̃))

qsum (x)✓(x) + ln Z.

x2X

Adding the two equations together and rearranging yields the claimed result.

Lost Relatives of the Gumbel Trick

C. Averaged unary perturbations
C.1. Lower bounds on the partition function
In the main text we stated the following two lower bounds on the log partition function ln Z.
Proposition 9.

Let ↵ 2 ( 1, 0) [ (0, 1). For any subset S ✓ {1, . . . , n} of the variables x1 , . . . , xn we have ln Z
h
i
ln (1 + ↵)
1
c+
ln E e ↵ maxx { (x)+ S (xS )} ,
↵
↵

where xS := {xi : i 2 S} and

S (xS )

⇠ Gumbel( c) independently for each setting of xS .

Proof. Let S̄ := {1, . . . , n} \ S. First we handle the case ↵ > 0. We have trivially that
XX
X
pow ↵ Z = pow ↵
e (xS ,xS̄ )  pow ↵
max e (xS ,xS̄ ) .
xS

xS̄

xS

xS̄

P
iid
h(y)
The Weibull trick tells us that pow ↵ y pow 1/↵ h(y) = EW [miny (1+↵)
W (y)] where {W (y)}y ⇠ Weibull(1, ↵
Applying this to the summation over xS on the right-hand side of the above inequality, we obtain
"
#
pow ↵ maxxS̄ e (xS ,xS̄ )
pow ↵ Z  EW min
W (xS ) .
xS
(1 + ↵)
Expressing the Weibull random variable W (xS ) as e
be simplified as follows:
pow

↵

Z
=

Taking the logarithm and dividing by
similarly, obtaining that
pow

↵

↵(

S (xS )+c)

with

S (xS )

1

).

⇠ Gumbel( c), the right-hand side can


1
E pow ↵ max max e (xS ,xS̄ ) e S (xS )+c
xS
xS̄
(1 + ↵)
h
⇣
⌘i
↵c
e
E exp
↵ max { (x) + S (xS )} .
x
(1 + ↵)

↵ < 0 yields the claimed result for positive ↵. For ↵ 2 ( 1, 0) we proceed
Z

pow
"

↵

X
xS

= EF min

max e

(xS ,xS̄ )

xS̄

pow

xS

↵

#
maxxS̄ e (xS ,xS̄ )
F (xS ) ,
(1 + ↵)

where F (x(S)) ⇠ Fréchet(1, ↵ 1 ). Representing these random variables as e ↵( S (xS )+c) with S (xS ) ⇠
Gumbel( c), simplifying as in the previous case and finally dividing the inequality by ↵ > 0 yields the claimed result for ↵ 2 ( 1, 0).
Corollary 10.

For any ↵ 2 ( 1, 0) [ (0, 1), we have the lower bound ln Z
L(↵) := c +

ln (1 + ↵)
↵

L(↵), where

1
ln E [exp ( n↵L)] ,
n↵

Proof. Applying Proposition 9 n times with all singleton sets S = {i} and averaging the obtained lower bounds yields
ln Z

c+

ln (1 + ↵)
↵

=c+

ln (1 + ↵)
↵

=c+

ln (1 + ↵)
↵

n
h
⇣
⌘i
1X1
ln E exp
↵ max{ (x) + i (xi )}
x
n i=1 ↵
"
!#
n
X
1
ln E exp
↵ max{ (x) + i (xi )}
x
n↵
i=1
"
!#
n
1
1X
ln E exp
n↵
max{ (x) + i (xi )}
,
n↵
n i=1 x

Lost Relatives of the Gumbel Trick

where the first equality used the fact that the perturbations i (xi ) are mutually independent for different indices i to replace
the product of expectations with the expectation of the product. The claimed result follows by applying Jensen’s inequality
to swap the summation and the convex maxx function, noting that the inequality works out the right way for both positive
and negative ↵.
Jensen’s inequality can be used to relate the general lower bound L(↵) to the Gumbel trick lower bound L(0), showing
that the former cannot be arbitrarily worse than the latter:
Proposition 14. For all ↵ 2 ( 1, 0), the lower bound L(↵) on ln Z satisfies
L(↵)

L(0) +

ln (1 + ↵)
+c
↵

Proof. Apply Jensen’s inequality with the convex function x 7! e
the inequality works out the stated way for ↵ < 0.

n↵

to the last term in the definition of L(↵), noting that

Note that ln (1+↵)
+ c  0 for ↵ 2 ( 1, 0) so this result does not imply that the Fréchet lower bounds are tighter than the
↵
Gumbel lower bound L(0); it merely says that they cannot be arbitrarily worse than L(0).
C.2. Relationship between errors of averaged-unary Gumbel perturbations
In this section we write x⇤ for the (random) MAP configuration after average-unary perturbation of the potential function,
i.e.,
(
)
n
X
1
x⇤ := argmax
(x) +
i (xi ) .
n i=1
x2X
i.i.d.

where { i (xi ) | xi 2 Xi , 1  i  n} ⇠ Gumbel( c). Let qavg (x) := P[x = x⇤ ] be the probability mass function of x⇤ .
The Gumbel trick lower bound on the log partition function ln Z due to Hazan et al. (2013) is:
"
(
)#
n
1X
ln Z L(0) = L (0) := E min
(x) +
.
(3)
i (xi )
x2X
n i=1
We show that the gap of this Gumbel trick lower bound on ln Z upper bounds the KL divergence between the approximate
distribution qavg and the Gibbs distribution p. To this end, we first need an entropy bound for qavg analogous to Theorem 1
of (Maji et al., 2014).
Theorem 15. The entropy of qavg can be lower bounded using expected values of max-perturbations as follows:
n

H(qavg )

1X
E [ i (x⇤i )]
n i=1 i

Remark. Theorem 1 of (Maji et al., 2014) and this Theorem 15 differ in three aspects: (1) the former is an upper bound and
the latter is a lower bound, (2) the former sums the expectations while the latter averages them, and (3) the distributions
qsum and qavg of x⇤ in the two theorems are different.
Proof. By the duality relation between negative entropy and the log partition function (Wainwright & Jordan, 2008), the
entropy H(qavg ) of the unary-avg perturb-max distribution qavg can be expressed as
(
)
X
H(qavg ) = inf ln Z'
qavg (x)'(x) ,
'

x2X

P
where the variable ' ranges over all potential functions on X , and Z' = x2X exp '(x). Applying the Gumbel trick
lower bound on the log partition function gives
(
)
X
H(qavg ) inf L' (0)
qavg (x)'(x) ,
'

x2X

Lost Relatives of the Gumbel Trick

P
Proposition 16 in Appendix D shows that L' (0) is a convex function of '. The expression
x2X
P q(x)'(x) is a linear
function of ', so also convex, and thus as a sum of two convex functions, the quantity L' (0)
x2X q(x)'(x) within
the infimum is a convex function of '. Moreover, Proposition 17 in Appendix D tells us that the partial derivatives can be
computed as
!
X
@
L' (0)
qavg (x)'(x) = q' (x) qavg (x)
@'(x)
x2X

where q' (x) is the unary-avg perturb-max distribution associated with the potential function '. Proposition 18 in Appendix DP
confirms that these partial derivatives are continuous, so we observe that as a function of ', the expression
L' (0)
x2X qavg (x)'(x) is a convex function with continuous partial derivatives, so it is a differentiable convex function. This is sufficient to establish that the point ' = is a global minimum of this function (Wright & Nocedal, 1999).
Hence
)
(
X
H(qavg ) inf L' (0)
qavg (x)'(x)
'

= L (0)
=
=

X

x2X

qavg (x)E

x2X
n
X

1
n

X

x2X

qavg (x) (x)
"

n

1X
(x) +
n i=1

i (xi )

| x = x⇤

#

X

qavg (x) (x)

x2X

E i [ i (x⇤i )]

i=1

where we conditioned on the maximizing configuration x⇤ when expanding L (0).
Remark. This proof proceeded in the same way as the proof of Maji et al. (2014) for the upper bound, except that establishing the minimizing configuration of the infimum is a non-trivial stepPthat is actually required in this case. The
second revision
of (Hazan et al., 2016) computes the derivative of U' (0)
x2X qsum (x)'(x), which is similar to our
P
L' (0)
x2X qavg (x)'(x), by differentiating under the expectation.
Equipped with Theorem 15, we can now show a link between the approximation “errors” of the averaged-unary perturbation MAP configuration distribution qavg (to the Gibbs distribution p) and estimate L(0) (to ln Z).
Proposition 12. Let p be the Gibbs distribution on X . Then
ln Z L(0)
|
{z
}

error in ln Z bound

KL(qavg k p)
|
{z
}

0

sampling error

Remark. While we knew from Hazan et al. (2013) that ln Z L(0)
0 (i.e. that L(0) is a lower bound on ln Z), this
is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the average-unary
perturbation MAP distribution qavg and the Gibbs distribution p.
Proof. The Kullback-Leibler divergence in question expands as
KL(qavg k p) =

H(qavg )

X

x2X

qavg (x) ln P

From the proof of Theorem 15 we know that H(qavg )
KL(qavg k p) 

L(0) +

X

x2X

exp (x)
=
x̃2X exp (x̃)

L(0)

qavg (x) (x)

P

x2X

X

x2X

H(qavg )

X

qavg (x) (x) + ln Z.

x2X

qavg (x) (x), so

qavg (x) (x) + ln Z = ln Z

L(0).

Lost Relatives of the Gumbel Trick

D. Technical results
In this section we write L( ) instead of L (0) for the Gumbel trick lower bound on ln Z associated with the potential
function , see equation (3).
Proposition 16. The Gumbel trick lower bound L( ), viewed as a function of the potentials , is convex.
Proof. Convexity can be proved directly from definition. Let
product space X , and let 2 [0, 1]. Then
L(

1

+ (1
)
"
(

1

and

2

be two arbitrary potential functions on a discrete

2)

)#
n
1X
= E max
) 2 (x) +
1 (x) + (1
i (xi )
x2X
n i=1
"
(
!
!)#
n
n
1X
1X
)
= E max
1 (x) +
i (xi ) + (1
2 (x) +
i (xi )
x2X
n i=1
n i=1
"
(
)
(
)#
n
n
1X
1X
E
max
) max
1 (x) +
i (xi ) + (1
2 (x) +
i (xi )
x2X
x2X
n i=1
n i=1
= L(

1)

+ (1

)L(

2 ),

where we have used convexity of the max function to obtain the inequality, and linearity of expectation to arrive at the final
equality.
Remark. This convexity proof goes through for other (low-dimensional) perturbations as well, e.g. it also works for U (0).
Proposition 17. The Gumbel trick lower bound L( ), viewed as a function of the potentials , has partial derivatives
@
L( ) = q (x̃)
@ (x̃)
where q is the probability mass function of the average-unary perturbation MAP configuration’s distribution associated
with the potential function .
Proof. Let x̃ 2 X , so that (x̃) is a general component of , and let ex̃ be the indicator vector of x̃. For any 2 R, the
change in the lower bound L due to replacing (x̃) with (x̃) + is
"
(
)#
"
(
)#
n
n
1X
1X
L( + ex̃ ) L( ) = E max
(x) + {x = x̃} +
E max
(x) +
i (xi )
i (xi )
x2X
x2X
n i=1
n i=1
"
(
)
(
)#
n
n
1X
1X
= E max
(x) + {x = x̃} +
max
(x) +
i (xi )
i (xi )
x2X
x2X
n i=1
n i=1
= E [ ( , , x̃, )]

by linearity of expectation, where we have denoted by ( , , x̃, ) the change in maximum due to replacing the potential
(x̃) with (x̃) + . Let’s condition on the argmax before modifying :
X
L( + ex̃ ) L( ) = E [ ( , , x̃, )] =
q (x)E [ ( , , x̃, ) | x is the original argmax]
x2X

Now let’s condition on the size of the gap G between the maximum and the runner-up:
E [ ( , , x̃, ) | x is the original argmax] = P(G  | |)E [ ( , , x̃, ) | x is the original argmax, G  | |]
+ P(G > | |)E [ ( , , x̃, ) | x is the original argmax, G > | |]

Let’s examine all four terms on the right-hand side one by one:

Lost Relatives of the Gumbel Trick

1.
2.
3.
4.

P(G  | |) ! P(G = 0) = 0 as ! 0 by monotonicity of measure.
E [ ( , , x̃, ) | x is the original argmax, G  | |]  since | ( , , x̃, )|  | | always holds.
P(G > | |) ! P(G 0) = 1 as ! 0 by monotonicity of measure.
E [ ( , , x̃, ) | x is the original argmax, G > | |] =
{x = x̃} since in this case both maximizations in the
definition of ( , , x̃, ) are maximized at x.

Therefore, as

! 0,
E [ ( , , x̃, ) | x is the original argmax] = o(1)o( ) + (1 + o(1))

{x = x̃}

Putting things together, we have
lim

!0

L( + ex̃ )

L( )

=

X

q (x) lim

!0

x2X

=

X

x2X

1

E [ ( , , x̃, ) | x is the original argmax]

q (x) {x = x̃}

= q (x̃),
which proves the stated claim directly from definition of a partial derivative.
Proposition 18. The probability mass function q of the average-unary perturbation MAP configuration’s distribution
associated with a potential function is continuous in .
Proof. For any x⇤ 2 X we have from definition
"
(
⇤

⇤

q (x ) = P x = argmax
"

x2X

n

1X
(x) +
n i=1

i (xi )

)#

(
)#
n
n
1X
1X
⇤
= P (x ) +
max
(x) +
i (xi ) >
i (xi )
n i=1
n i=1
x2X \{x⇤ }
" (
(
))#
n
n
X
X
1
1
⇤
=E
(x⇤ ) +
max
(x) +
i (xi ) >
i (xi )
n i=1
n i=1
x2X \{x⇤ }

which is continuous in

⇤

by continuity of max, of {· > ·} (as a function of ) and by the Bounded Convergence Theorem.

Remark. The results above show that the Gumbel trick lower bound L( ), viewed as a function of the potentials , is
convex and has continuous partial derivatives.

