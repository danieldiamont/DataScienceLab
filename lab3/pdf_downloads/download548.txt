Bottleneck Conditional Density Estimation

A. Factored Inference
When training the BJDE in the semi-supervised regime,
we introduce a factored inference procedure that reduce the
number of parameters in the recognition model.
In the semi-supervised regime, the 1-layer BJDE recognition model requires approximating three posteriors:
p(z|x, y) / p(z)p(x, y|z), p(z|x) / p(z)p(x|z), and
p(z|y) / p(z)p(y|z). The standard approach would be to
assign one recognition network for each approximate posterior. This approach, however, does not take advantage
of the fact that these posteriors share the same likelihood
functions, i.e., p(x, y|z) = p(x|z)p(y|z).
Rather than learning the three approximate posteriors independently, we propose to learn the approximate likeliˆ x) ⇡ p(x|z), `(z;
ˆ y) ⇡ p(y|z) and
hood functions `(z;
ˆ
ˆ
ˆ
let `(z; x, y) = `(z; x)`(z; y). Consequently, this factorization of the recognition model enables parameter sharing
within the joint recognition model (which is beneficial for
semi-supervised learning) and eliminates the need for constructing a neural network that takes both x and y as inputs. The latter property is especially useful when learning
a joint model over multiple, heterogeneous data types (e.g.
image, text, and audio).

Since our model includes unpaired y, we modify Eq. (15)
to include
˜ ✓) = p(✓,
˜ ✓)
p(Xl , Yl , Xu , Yu , ✓,
˜
˜
˜
p(Xu |✓)p(Y
u |✓)p(Xl |✓)p(Yl |Xl , ✓).

To account for the variational parameters, we include them
in the joint density as well,
˜ ˜, ✓, ) = p(✓,
˜ ˜, ✓, )
p(Xl , Yl , Xu , Yu , ✓,
˜ ˜)p(Yu |✓,
˜ ˜)
p(Xu |✓,
˜ ˜)p(Yl |Xl , ✓, )
p(Xl |✓,

where ˜ parameterizes the recognition networks. To ensure proper normalization in Eq. (14), it is sufficient for `ˆ
to be bounded. If the prior p(z) belongs to an exponential
family with sufficient
statistics⌘T (z), we can parameterize
⇣
ˆ
` ˜(z; y) = exp hT (z), ⌘ ˜(y)i , where ⌘ ˜(y) is a network
such that ⌘ ˜(y) 2 {⌘|{hT (z), ⌘i 8z} is upper bounded}.
Then the approximate posterior can be obtained by simple
addition in the natural parameter space of the corresponding exponential family. When the prior and approximate
likelihood are both Gaussians, this is exactly precisionweighted merging of the means and variances (Sønderby
et al., 2016).

B. Derivation of the Hybrid Objective
We first provide the derivation of Eq. (11). We begin with
the factorization proposed in Eq. (7), which we repeat here
for self-containedness,
˜ ✓) = p(✓,
˜ ✓)
p(Xl , Yl , Xu , ✓,
˜
˜
p(Xu |✓)p(X
l |✓)p(Yl |Xl , ✓).

(15)

(17)

By taking the log and replacing the necessary densities with
their variational lower bound,
˜ ˜, ✓, ) ln p(✓,
˜ ˜, ✓, ) +
ln p(Xl , Yl , Xu , Yu , ✓,
˜ ˜; Xu ) + Jy (✓,
˜ ˜; Yu ) +
Jx (✓,
˜ ˜; Xl ) + C(✓, ; Xl , Yl ),
Jx (✓,

(18)

we arrive at Eq. (11). We note, however, that a more general hybrid objective Eq. (13) is achievable. To derive the
general objective, we consider an alternative factorization
of the joint density in Eq. (17),

In practice, we directly learn recognition networks for
ˆ y) and perform factored inference as folq(z|x) and `(z;
lows
q(z|x, y) / q ˜(z|x)`ˆ˜(z; y), q(z|y) / p(z)`ˆ˜(z; y),
(14)

(16)

˜ ˜, ✓, ) = p(✓,
˜ ˜, ✓, , )
p(Xl , Yl , Xu , Yu , ✓,
˜ ˜, ✓, ).
p(Xl , Yl , Xu , Yu |✓,
(19)
We factorize the likelihood term such that Xu and Yu are
˜ ˜,
always explained by the joint parameters ✓,
˜ ˜, ✓, ) = p(Xu |✓,
˜ ˜)p(Yu |✓,
˜ ˜)
p(Xl , Yl , Xu , Yu |✓,
˜ ˜, ✓, ).
p(Xl , Yl |✓,
(20)
We then introduce an auxiliary variable s = {0, 1},
˜ ˜, ✓, )
p(Xl , Yl |✓,
X
˜ ˜, ✓, , s),
=
p(s)p(Xl , Yl |✓,

(21)

s

where
˜ ˜, ✓, , s0 ) = p(Xl , Yl |✓,
˜ ˜)
p(Xl , Yl |✓,
(22)
1
˜ ˜, ✓, , s ) = p(Xl |✓,
˜ ˜)p(Yl |Xl , ✓, ).
p(Xl , Yl |✓,
(23)
Using Jensen’s inequality,
˜ ˜, ✓, ) with
ln p(Xl , Yl |✓,

we

can

lower

bound

˜ ˜) + p(s1 ) ln p(Xl |✓,
˜ ˜)p(Yl |Xl , ✓, ).
p(s0 ) ln p(Xl , Yl |✓,
(24)
By taking the log of Eq. (19), replacing all remaining
densities with their variational lower bound, and setting

Bottleneck Conditional Density Estimation

p(s0 ) = ↵,
˜ ˜, ✓, )
ln p(Xl , Yl , Xu , Yu , ✓,
˜ ˜, ✓, ; Xl , Yl , Xu , Yu )
H(✓,
˜ ˜, ✓, ) +
= ln p(✓,

(25)

˜ ˜; Xu ) + Jy (✓,
˜ ˜; Yu ) +
Jx (✓,
˜ ˜; Xl , Yl ) +
↵ · Jxy (✓,
h
i
˜ ˜; Xl ) + C(✓, ; Xl , Yl ) , (26)
(1 ↵) · Jx (✓,

we arrive at the general hybrid objective. Note that when
↵ = 0, Eq. (26) reduces to Eq. (18).

C. Visualizations for CelebA and SVHN

(a) Conditional Rec.

(b) Conditional Pred.

(c) Pre-train Rec.

(d) Pre-train Pred.

(e) Hybrid Rec.

(f) Hybrid Pred.

We show visualizations of the hybrid BCDE predictions for
CelebA and SVHN on the top-down prediction task in the
nl = 10000 semi-supervised regime. For each data set, we
visualize both the images sampled during reconstruction as
well as prediction using an approximation of the MAP estimate by greedily sampling the mode of each conditional
distribution in the generative path.

(a) Hybrid Rec.

(b) Hybrid Pred.

Figure 6. Visualization of the reconstructed and predicted bottom
half of SVHN test set images when conditioned on the top half.

(g) Hybrid Factored Rec. (h) Hybrid Factored Pred.
Figure 7. Visualization of the reconstructed and predicted bottom
half of CelebA test set images when conditioned on the top half.

