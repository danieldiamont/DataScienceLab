Uniform Deviation Bounds for k-Means Clustering

Olivier Bachem 1 Mario Lucic 1 S. Hamed Hassani 1 Andreas Krause 1

Abstract
Uniform deviation bounds limit the difference between a modelâ€™s expected loss and its loss on a
random sample uniformly for all models in a learning problem. In this paper, we provide a novel
framework to obtain uniform deviation bounds
for unbounded loss functions. As a result, we
obtain competitive uniform deviation bounds for
k-Means clustering under weak assumptions on
the underlying distribution. If the fourth
moment


âˆ’ 12
is bounded, we prove a rate of O m
com

1
pared to the previously known O mâˆ’ 4 rate.
We further show that this rate also depends on the
kurtosis â€” the normalized fourth moment which
measures the â€œtailednessâ€ of the distribution. We
also provide improved rates under progressively
stronger assumptions, namely, bounded higher
moments, subgaussianity and bounded support of
the underlying distribution.

1. Introduction
Empirical risk minimization â€” i.e. the training of models on
a finite sample drawn i.i.d from an underlying distribution
â€” is a central paradigm in machine learning. The hope is
that models trained on the finite sample perform provably
well even on previously unseen samples from the underlying distribution. But how many samples m are required
to guarantee a low approximation error ? Uniform deviation bounds provide the answer. Informally, they are the
worst-case difference across all possible models between
the empirical loss of a model and its expected loss. As such,
they determine how many samples are required to achieve a
fixed error in terms of the loss function. In this paper, we
consider the popular k-Means clustering problem and provide uniform deviation bounds based on weak assumptions
on the underlying data generating distribution.
1
Department of Computer Science, ETH Zurich. Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

Related work. Traditional Vapnik-Chervonenkis theory
provides tools to obtain uniform deviation bounds for binary concept classes such as classification using halfspaces
(Vapnik & Chervonenkis, 1971). While these results have
been extended to provide uniform deviation bounds for sets
of continuous functions bounded in [0, 1] (Haussler, 1992;
Li et al., 2001), these results are not readily applied to kMeans clustering as the underlying loss function in k-Means
clustering is continuous and unbounded.
In their seminal work, Pollard (1981) shows that k-Means
clustering is strongly consistent, i.e., that the optimal cluster
centers on a random sample converge almost surely to the
optimal centers of the distribution under a weak assumption. This has sparked a long line of research on cluster
stability (Ben-David et al., 2006; Rakhlin & Caponnetto,
2007; Shamir & Tishby, 2007; 2008) which investigates the
convergence of optimal parameters both asymptotically and
for finite samples. The vector quantization literature offers
insights into the convergence of empirically optimal quantizers for k-Means
 in terms of the objective: A minimax
1
rate of O mâˆ’ 2 is known if the underlying distribution
has bounded support (Linder etal., 1994; Bartlett et al.,
1998). A better rate of O mâˆ’1 may be achieved for finite support (Antos et al., 2005) or under both bounded
support and regularity assumptions (Levrard et al., 2013).
Ben-David (2007) provides a uniform convergence result for
center based clustering under a bounded support assumption
and Telgarsky & Dasgupta (2013) prove uniform deviation
bounds for k-Means clustering if the underlying distribution
satisfies moment assumptions (see Section 4).
Empirical risk minimization with fat-tailed losses has been
studied in Mendelson et al. (2014), Mendelson (2014),
GruÌˆnwald & Mehta (2016) and Dinh et al. (2016). Dinh et al.
(2016) provide fast learning rates for k-Means clustering
but under stronger assumptions than the ones considered in
this paper. Guarantees similar to uniform deviation bounds
can be obtained using importance sampling in the context
of coreset construction (Bachem et al., 2015; Lucic et al.,
2016; 2017).
Our contributions. We provide a new framework to obtain
uniform deviation bounds for unbounded loss functions. We
apply it to k-Means clustering and provide
uniform devi
1
ation bounds with a rate of O mâˆ’ 2 for finite samples

Uniform Deviation Bounds for k-Means Clustering

under weak assumptions. In contrast to prior work, our
bounds are all scale-invariant and hold for any set of k cluster centers (not only for a restricted solution set). We show
that convergence depends on the kurtosis of the underlying distribution, which is the normalized fourth moment
and measures the â€œtailednessâ€ of a distribution. If bounded
higher moments are available, we provide improved bounds
that depend upon the normalized higher moments and we
sharpen them even further under the stronger assumptions
of subgaussianity and bounded support.

2. Problem statement for k-Means
We first focus on uniform deviation bounds for k-Means
clustering, and defer the (more technical) framework for
unbounded loss functions to Section 5. We consider the
d-dimensional Euclidean space and define
d(x, Q)2 = minkx âˆ’ qk22
qâˆˆQ

for any x âˆˆ Rd and any finite set Q âŠ‚ Rd . Furthermore,
slightly abusing the notation, for any x, y âˆˆ Rd , we set
d(x, y)2 = d(x, {y})2 = kx âˆ’ yk22 .
d
Statistical k-Means. Let P
 be any2 distribution on R with
2
Âµ = EP [x] and Ïƒ = EP d(x, Âµ) âˆˆ (0, âˆ). For any set
Q âŠ‚ Rd of k âˆˆ N cluster
 centers,
 the expected quantization
error is given by EP d(x, Q)2 . The goal of the statistical
k-Means problem is to find a set of k cluster centers such
that the expected quantization error is minimized.

Empirical k-Means. Let X denote a finite set of points in
Rd . The goal of the empirical k-Means problem is to find
a set Q of k cluster centers in Rd such that the empirical
quantization error Ï†X (Q) is minimized, where
1 X
Ï†X (Q) =
d(x, Q)2 .
|X |
xâˆˆX

Empirical risk minimization. In practice, the underlying
data distribution P in statistical learning problems is often
unknown. Instead, one is only able to observe independent
samples from P . The empirical risk minimization principle
advocates that finding a good solution on a random sample Xm also provides provably good solutions to certain
statistical learning problems if enough samples m are used.
Uniform deviation bounds. For k-Means, such a result
may be shown by bounding the deviation between the expected loss and the empirical error, i.e.,



Ï†Xm (Q) âˆ’ EP d(x, Q)2  ,
uniformly for all possible clusterings Q âˆˆ RdÃ—k . If this
difference is sufficiently small for a given m, one may then
solve the empirical k-Means problem on Xm and obtain
provable guarantees on the expected quantization error.

3. Uniform deviation bounds for k-Means
A simple approach would be to bound the deviation by an
absolute error , i.e., to require that



Ï†Xm (Q) âˆ’ EP d(x, Q)2  â‰¤ 
(1)
uniformly for a set of possible solutions (Telgarsky & Dasgupta, 2013). However, in this paper, we provide uniform
deviation bounds of a more general form: For any distribution P and a sample of m = f (, Î´, k, d, P ) points, we
require that with probability at least 1 âˆ’ Î´





Ï†X (Q) âˆ’ EP d(x, Q)2  â‰¤  Ïƒ 2 +  EP d(x, Q)2
m
2
2
(2)
uniformly for all Q âˆˆ RdÃ—k . The terms on the right-hand
side may be interpreted as follows: The first term based on
the variance Ïƒ 2 corresponds to a scale-invariant, additive
approximation error. The second term is a multiplicative approximation error that allows the guarantee to hold even for
solutions Q with a large expected quantization error. Similar
additive error terms were recently explored by Bachem et al.
(2016; 2017) in the context of seeding for k-Means.
There are three key reasons why we choose (2) over (1):
First, (1) is not scale-invariant and may thus not hold for
classes of distributions that are equal up to scaling. Second, (1) may not hold for an unbounded solution space, e.g.
RdÃ—k . Third, we can always rescale P to unit variance
 and
restrict ourselves to solutions Q with EP d(x, Q)2 â‰¤ Ïƒ 2 .
Then, (2) implies (1) for a suitable transformation of P .
Importance of scale-invariance. If we scale all the points
in a data set X and all possible sets of solutions Q by some
Î» > 0, then the empirical quantization error is scaled by
Î»2 . Similarly, if we consider the random variable Î»x where
x âˆ¼ P , then the expected quantization error is scaled by
Î»2 . At the same time, the k-Means problem remains the
same: an optimal solution of the scaled problem is simply a
scaled optimal solution of the original problem. Crucially,
however, it is impossible to achieve the guarantee in (1) for
distributions that are equal up to scaling: Suppose that (1)
holds for some error tolerance , and sample size m with
probability at least 1 âˆ’ Î´. Consider a distribution P and a
solution Q âˆˆ RdÃ—k such that with probability at least Î´



a < Ï†Xm (Q) âˆ’ EP d(x, Q)2  .
for some a > 0.1 For Î» >

âˆš1 ,
a

let PÌƒ be the distribution of

the random variable Î»x where x âˆ¼ P and let XÌƒm consist of
m samples from PÌƒ . Defining QÌƒ = {Î»q | q âˆˆ Q}, we have
with probability at least Î´

 
h 
 i


Ï†XÌƒm QÌƒ âˆ’ EPÌƒ d x, QÌƒ 2  > aÎ»2 > 
1
For example, let P be a nondegenerate multivariate normal
distribution and Q consist of k copies of the origin.

Uniform Deviation Bounds for k-Means Clustering

which contradicts (1) for the distribution PÌƒ and the solution QÌƒ. Hence, (1) cannot hold for both P and its scaled
transformation PÌƒ .
Unrestricted solution space One way to guarantee scaleinvariance would be require that



Ï†Xm (Q) âˆ’ EP d(x, Q)2  â‰¤ Ïƒ 2

(3)

for all Q âˆˆ RdÃ—k . However, while (3) is scale-invariant, it is
also impossible to achieve for all solutions Q as the following example shows. For simplicity, consider the 1-Means
problem in 1-dimensional space and let P be a distribution
with zero mean. Let Xm denote m independent samples
from P and denote by ÂµÌ‚ the mean of Xm . For any finite m,
suppose that ÂµÌ‚ 6= 0 with high probability2 and consider a
solution Q consisting of a single point q âˆˆ R. We then have



Ï†X ({q}) âˆ’ EP d(x, {q})2 
m


= Ï†Xm ({ÂµÌ‚}) + d(ÂµÌ‚, q)2 âˆ’ Ïƒ 2 âˆ’ d(0, q)2 


= Ï†Xm ({ÂµÌ‚}) âˆ’ Ïƒ 2 + q 2 âˆ’ 2q ÂµÌ‚ + ÂµÌ‚2 âˆ’ q 2 


= Ï†X ({ÂµÌ‚}) âˆ’ Ïƒ 2 + ÂµÌ‚2 âˆ’ 2q ÂµÌ‚

(4)

m

Since ÂµÌ‚ 6= 0 with high probability, clearly this expression
diverges as q â†’ âˆ and thus (3) cannot hold for arbitrary
solutions Q âˆˆ RdÃ—k . Intuitively, the key issue is that both
the empirical and the statistical error become unbounded as
q â†’ âˆ. Previous approaches such as Telgarsky & Dasgupta
(2013) solve this issue by restricting the solution space from
RdÃ—k to solutions that are no worse than some threshold. In
contrast, we allow the deviation between the empirical
and

the expected quantization error to scale with EP d(x, Q)2 .
Arbitrary distributions. Finally, we show that we either
need to impose assumptions on P or equivalently make the
relationship between m,  and Î´ in (2) depend on the underlying distribution P . Suppose that there exists a sample size
m âˆˆ N, an error tolerance  âˆˆ (0, 1) and a maximal failure
probability Î´ âˆˆ (0, 1) such that (2) holds for any distribution
P . Let P be the Bernoulli distribution on {0, 1} âŠ‚ R with
1
m
P [x = 1] = p for p âˆˆ (Î´
 , 1). 2By
 design, we have Âµ = p,
2
Ïƒ = p(1 âˆ’ p) and EP d(x, 1) = (1 âˆ’ p). Furthermore,
with probability at least Î´, the set Xm of m independent
samples from P consists of m copies of a point at one.
Hence, (2) implies that with probability at least 1 âˆ’ Î´





Ï†Xm (1) âˆ’ EP d(x, 1)2  â‰¤ EP d(x, 1)2


since Ïƒ 2 â‰¤ EP d(x, 1)2 . However, with probability at
least Î´, we have Ï†Xm (1) = 0 which would imply 1 â‰¤  and
thus lead to a contradiction with  âˆˆ (0, 1).
2

For example, if P is the standard normal distribution.

4. Key results for k-Means
In this section, we present our main results for k-Means and
defer the analysis and proofs to Sections 6.
4.1. Kurtosis bound
Similar to Telgarsky & Dasgupta (2013), the weakest assumption that we require is that the fourth moment of
d(x, Âµ) for x âˆˆ P is bounded.3 Our results are based on the
kurtosis of P which we define as


EP d(x, Âµ)4
.
MÌ‚4 =
Ïƒ4
The kurtosis is the normalized fourth moment and is a scaleinvariant measure of the â€œtailednessâ€ of a distribution. For
example, the normal distribution has a kurtosis of 3, while
more heavy tailed distributions such as the t-Student distribution or the Pareto distribution have a potentially unbounded kurtosis. A natural interpretation of the kurtosis is
provided by Moors (1986). For simplicity, consider a data
set with unit variance. Then, the kurtosis may be restated as
the shifted variance of d(x, Âµ)2 , i.e.,

MÌ‚4 = Var d(x, Âµ)2 + 1.
This provides a valuable insight into why the kurtosis is relevant for our setting: For simplicity, suppose we would like
to estimate the expected quantization error EP d(x, Âµ)2
by the empirical quantization error Ï†Xm ({Âµ}) on a finite
sample Xm .4 Then, the kurtosis measures
 the dispersion
of d(x, Âµ)2 around its mean EP d(x, Âµ)2 and provides a
bound on how many samples are required to achieve an error
of . While this simple example provides the key insight
for the trivial solution Q = {Âµ}, it requires a non-trivial
effort to extend the guarantee in (2) to hold uniformly for
all solutions Q âˆˆ RdÃ—k .
With the use of a novel framework to learn unbounded loss
functions (presented in Section 5), we are able to provide
the following guarantee for k-Means.
Theorem 1 (Kurtosis). Let , Î´ âˆˆ (0, 1) and k âˆˆ N. Let P
be any distribution on Rd with kurtosis MÌ‚4 < âˆ. For



12800 8 + MÌ‚4 
1
mâ‰¥
3
+
30k(d
+
4)
log
6k
+
log
2 Î´
Î´
let X = {x1 , x2 , . . . , xm } be m independent samples from
P . Then, with probability at least 1 âˆ’ Î´, for all Q âˆˆ RdÃ—k





Ï†X (Q) âˆ’ EP d(x, Q)2  â‰¤  Ïƒ 2 +  EP d(x, Q)2 .
2
2
3
While our random variables x âˆˆ P are potentially multivariate, it suffices to consider the behavior of the univariate random
variable d(x, Âµ) for the assumptions in thissection. 
4
This is a hypothetical exercise as EP d(x, Âµ)2 = 1 by design. However, it provides an insight to the importance of the
kurtosis.

Uniform Deviation Bounds for k-Means Clustering

The proof is provided in Section 6.1. The number of sufficient samples

!
MÌ‚4
1
mâˆˆO 2
dk log k + log
 Î´
Î´
is linear in the kurtosis MÌ‚4 and the dimensionality d, nearlinear in the number of clusters k and 1Î´ , and quadratic in
1
the bound may be interpreted as follows:
 . Intuitively,

â„¦

MÌ‚4
2 Î´

samples are required such that the guarantee holds

for a single solution Q âˆˆ RdÃ—k . Informally, a generalization of the Vapnik Chervonenkis dimension for k-Means
clustering may be bounded by O(dk log k) and measures
the â€œcomplexityâ€ of the learning problem. The multiplicative dk log k + log 1Î´ term intuitively extends the guarantee
uniformly to all possible Q âˆˆ RdÃ—k .
Comparison to Telgarsky & Dasgupta (2013). While we
require a bound on the normalized fourth moment, i.e., the
kurtosis, Telgarsky & Dasgupta (2013) consider the case
where all unnormalized moments up to the fourth are uniformly bounded by some M , i.e.,


EP d(x, Âµ)l â‰¤ M, 1 â‰¤ l â‰¤ 4.
They provide uniform deviation bounds
for all

 solutions Q
such that either Ï†X (Q) â‰¤ c or EP d(x, Q)2 â‰¤ c for some
c > 0. To compare our bounds, we consider a data set with
unit variance and restrict ourselves to solutions Q âˆˆ RdÃ—k
with an expected
quantization
error of at most the variance,


i.e., EP d(x, Q)2 â‰¤ Ïƒ 2 = 1. Consider the deviation



Ï†X (Q) âˆ’ EP d(x, Q)2  .
âˆ†=
sup
QâˆˆRdÃ—k :EP [d(x,Q)2 ]â‰¤1

Telgarsky & Dasgupta (2013) bound this deviation by
s
!

 r
1
M2
1
âˆš
âˆ†âˆˆO
dk log(M dm) + log
.
+
Î´
mÎ´ 2
m
In contrast, our bound in Theorem 1 implies
ï£«s
ï£¶


MÌ‚
1
4
ï£¸.
âˆ† âˆˆ Oï£­
dk log k + log
mÎ´
Î´
The key difference lies in how âˆ† scales with the sample
size m.WhileTelgarsky & Dasgupta (2013)
 show
 a rate of
1
1
âˆ† âˆˆ O mâˆ’ 4 , we improve it to âˆ† âˆˆ O mâˆ’ 2 .
4.2. Bounded higher moments
The tail behavior of d(x, Âµ) may be characterized by the
moments of P . For p âˆˆ N, consider the standardized p-th
moment of P , i.e.,
MÌ‚p =

EP [d(x, Âµ)p ]
.
Ïƒp

Theorem 2 provides an improved uniform deviation bound
if P has bounded higher moments.
Theorem 2 (Moment bound). Let  âˆˆ (0, 1), Î´ âˆˆ (0, 1)
and k âˆˆ N. Let P be any distribution on Rd with finite p-th
order moment
 bound MÌ‚p8 <
 âˆ for p âˆˆ {4, 8, . . . , âˆ}. For
3200m1
8 p
m â‰¥ max
, Î´
with
2



4
1
m1 = p 4 + MÌ‚p p
3 + 30k(d + 4) log 6k + log
Î´
let X = {x1 , x2 , . . . , xm } be m independent samples from
P . Then, with probability at least 1 âˆ’ Î´, for all Q âˆˆ RdÃ—k





Ï†X (Q) âˆ’ EP d(x, Q)2  â‰¤  Ïƒ 2 +  EP d(x, Q)2 .
2
2
The proof is provided in Section 6.2. Compared to the
previous bound based on the kurtosis, Theorem 2 requires
ï£«
ï£¶
4 
   p8
p
p
MÌ‚
1
1
p
ï£¸
m âˆˆ â„¦ï£­ 2
+
dk log k + log

Î´
Î´
samples. With higher order moment bounds, it is easier to
achieve high probability results since the dependence on 1Î´
 8 
is only of O 1Î´ p compared to near linear for a kurtosis
4

bound. The quantity MÌ‚p p may be interpreted as a bound on
the kurtosis MÌ‚4 based on the higher order moment MÌ‚p since
4

Hoelderâ€™s inequality implies that MÌ‚4 â‰¤ MÌ‚p p . While the
result only holds for p âˆˆ {8, 12, 16, . . . , âˆ}, it is trivially
j k
extended to p0 â‰¥ 8: Consider Theorem 2 with p = 4
4

p0
4

4

0
and note that by Hoelderâ€™s inequality MÌ‚p p â‰¤ MË†p0 p .

Comparison to Telgarsky & Dasgupta (2013). Again,
we consider distributions P that have unit variance and
we restrict ourselves to solutions Q âˆˆ RdÃ—k with an
expected
quantization
error of at most the variance, i.e.,


EP d(x, Q)2 â‰¤ Ïƒ 2 = 1. Telgarsky & Dasgupta (2013)
require that there exists a bound M


EP d(x, Âµ)l â‰¤ M, 1 â‰¤ l â‰¤ p.
Then, for m sufficiently large, âˆ† is of
ï£¶
ï£«s


  p4
8
p
p
4
4
M
1
2
1
ï£¸.
Oï£­
dk ln(M p dm) + ln
+ 3âˆ’2
4
1âˆ’ p
Î´
Î´
4
p
m
m
In contrast, we obtain that, for m sufficiently large,
ï£«v
ï£¶
u
4 

u
p
1 ï£·
ï£¬t pMÌ‚p
âˆ† âˆˆ Oï£­
dk log k + log
ï£¸.
m
Î´
While
& Dasgupta (2013) only
 Telgarsky

 show
 a rate of
âˆ’ 12
âˆ’ 12
O m
as p â†’ âˆ, we obtain a âˆˆ O m
rate for all
higher moment bounds.

Uniform Deviation Bounds for k-Means Clustering

4.3. Subgaussianity
If the distribution P is subgaussian, then all its moments
MÌ‚p are bounded. By optimizing p in Theorem 2, we are
able to show the following bound.
Theorem 3 (Subgaussian bound). Let  âˆˆ (0, 1), Î´ âˆˆ (0, 1)
and k âˆˆ N. Let P be any distribution on Rd with Âµ =
EP [x] and
 2
t
âˆ€t > 0 : P [d(x, Âµ) > tÏƒ] â‰¤ a exp âˆ’ âˆš
b
1
for some a > 1, b > 0. Let m â‰¥ 3200m
with
2




abp2
1
m1 = p 4 +
3 + 30k(d + 4) log 6k + log
.
4
Î´

and p = 9 + 3 log 1Î´ . Let X = {x1 , x2 , . . . , xm } be m
independent samples from P . Then, with probability at least
1 âˆ’ Î´, for all Q âˆˆ RdÃ—k





Ï†X (Q) âˆ’ EP d(x, Q)2  â‰¤  Ïƒ 2 +  EP d(x, Q)2 .
2
2
The proof is provided in Section 6.3. In O(Â·) notation,

!
ab log3 1Î´
1
mâˆˆO
dk log k + log
Ïƒ2
Î´
samples are hence sufficient. This result features a polylogarithmic dependence on 1Î´ compared to the polynomial
dependence for the bounds based on bounded higher moments. The sufficient sample size further scales linearly
with the (scale-invariant) subgaussianity parameters a and b.
For example, if P is a one-dimensional normal distribution
of any scale, we would have a = 2 and b = 1.
4.4. Bounded support
The strongest assumption that we consider is if the support
of P is bounded by a hypersphere in Rd with diameter
R > 0. This ensures that almost surely d(x, Âµ) â‰¤ R and
4
hence MÌ‚4 â‰¤ R
Ïƒ 4 . This allows us to obtain Theorem 4.
Theorem 4 (Bounded support). Let  âˆˆ (0, 1), Î´ âˆˆ (0, 1)
and k âˆˆ N. Let P be any distribution
on Rd , with Âµ =

2
2
EP [x] and Ïƒ = EP d(x, Âµ) âˆˆ (0, âˆ), whose support
is contained in a d-dimensional hypersphere of diameter
R > 0. For


4


12800 8 + R
4
Ïƒ
1
mâ‰¥
3
+
30k(d
+
4)
log
6k
+
log
2
Î´
let X = {x1 , x2 , . . . , xm } be m independent samples from
P . Then, with probability at least 1 âˆ’ Î´, for all Q âˆˆ RdÃ—k





Ï†X (Q) âˆ’ EP d(x, Q)2  â‰¤  Ïƒ 2 +  EP d(x, Q)2 .
2
2

The proof is provided in Section 6.4. Again, the sufficient
4
sample size scales linearly with the kurtosis bound R
Ïƒ 4 . However, the bound is only logarithmic in 1Î´ .

5. Framework for unbounded loss functions
To obtain the results presented in Section 4, we propose a
novel framework to uniformly approximate the expected
values of a set of unbounded functions based on a random
sample. We consider a function family F mapping from
an arbitrary input space X to Râ‰¥0 and a distribution P
on X . We further require a generalization of the VapnikChervonenkis dimension to continuous, unbounded functions5 â€” the pseudo-dimension.
Definition 1 (Haussler (1992); Li et al. (2001)). The pseudodimension of a set F of functions from X to Râ‰¥0 , denoted by Pdim(F), is the largest d0 such there is a sequence x1 , . . . , xd0 of domain elements from X and a sequence r1 , . . . , rd0 of reals such that for each b1 , . . . , bd0 âˆˆ
{above, below}, there is an f âˆˆ F such that for all
i = 1, . . . , d0 , we have f (xi ) â‰¥ ri â‡â‡’ bi = above.
Similar to the VC dimension, the pseudo-dimension measures the cardinality of the largest subset of X that can be
shattered by the function family F. Informally, the pseudodimension measures the richness of F and plays a critical
role in providing a uniform approximation guarantee across
all f âˆˆ F. With this notion, we are able to state the main
result in our framework.
Theorem 5. Let  âˆˆ (0, 1), Î´ âˆˆ (0, 1) and t > 0. Let F
be a family of functions from X to Râ‰¥0 with Pdim(F ) =
d < âˆ. Let s : X â†’ Râ‰¥0 be a function such that s(x) â‰¥
supf âˆˆF f (x) for all x âˆˆ X . Let P be any distribution on
X and for


200t
1
,
mâ‰¥ 2
3 + 5d + log

Î´
let x1 , x2 , . . . , x2m be 2m independent samples from P .
Then, if
"

#
2m
1 X
Î´
2
P
s(xi ) > t â‰¤ ,
2m i=1
4
(5)
it holds with probability at least 1 âˆ’ Î´ that


m
1 X



f (xi ) âˆ’ EP [f (x)] â‰¤ , âˆ€f âˆˆ F.
(6)

m



EP s(x)2 â‰¤ t and

i=1

Applying Theorem 5 to a function family F requires three
steps: First, one needs to bound the pseudo-dimension of
5

The pseudo-dimension was originally defined for sets of functions mapping to [0, 1] (Haussler, 1992; Li et al., 2001). However,
it is trivially extended to unbounded functions mapping to Râ‰¥0 .

Uniform Deviation Bounds for k-Means Clustering

F. Second, it is necessary to find a function s : X â†’ Râ‰¥0
such that
f (x) â‰¤ s(x),

âˆ€x âˆˆ X and âˆ€f âˆˆ F.

Ideally, such a bound should be as tight as possible. Third,
one needs to find some t > 0 and a sample size


1
200t
3 + 5d + log
mâ‰¥ 2

Î´
such that
h

2

EP s(x)

"

i

â‰¤ t and

#
2m
1 X
Î´
2
P
s(xi ) > t â‰¤ .
2m i=1
4

Finding such a bound usually entails examining the tail
2
behavior of s(x) under P . Furthermore,
h it isi evident that

bounded using Hoeffdingâ€™s inequality by
2 exp âˆ’

!

2m2
1
m

Pm

max (s(xi ), s(xi+m ))
!
m2
â‰¤ 2 exp âˆ’ 1 P2m
.
2
i=1 s(xi )
2m

2

i=1

By (5), with probability at least 1 âˆ’ 4Î´ , we
 have

P2m
t log Î´1
2
1
s(x
)
â‰¤
t
and
we
hence
require
m
âˆˆ
â„¦
2
i
i=1
2m

samples to guarantee that (7) does not hold for a single
f âˆˆ F with probability at least 1 âˆ’ 2Î´ .
To bound the probability that there exists any f âˆˆ F such
that (7) holds, we show in Lemma 5 (see Section B of the
Supplementary Materials) that, given independent samples
x1 , x2 , . . . , x2m ,

2

a bound t may only be found if EP s(x) is bounded
and that assumptions on the distribution P are required. In
Section 6,
h we will
i see that for k-Means a function s(x)
2

with EP s(x)
bounded.

< âˆ may be found if the kurtosis of P is

We defer the proof of Theorem 5 to Section B of the Supplementary Materials and provide a short proof sketch that captures the main insight. The proof is based on symmetrization,
the bounding of covering numbers and chaining â€” common techniques in the empirical process literature (Pollard,
1984; Li et al., 2001; Boucheron et al., 2013; Koltchinskii,
2011; van der Vaart & Wellner, 1996). The novelty lies in
considering loss functions f (Â·) and cover functions s(Â·) in
Theorem 5 that are potentially unbounded.
Proof sketch. Our proof is based on a double sampling approach. Let xm+1 , xm+2 , . . . , x2m be an additional m independent samples from P and let Ïƒ1 , Ïƒ2 , . . . , Ïƒm be independent random variables uniformly
h
isampled from {âˆ’1, 1}.
2

Then, we show that, if EP s(x) â‰¤ t, the probability
of (6) not holding may be bounded by the probability that
there exists a f âˆˆ F such that


m
1 X



Ïƒi (f (xi ) âˆ’ f (xi+m )) > .
(7)

m

i=1

We first provide the intuition for a single function f âˆˆ F and
then show how we extend it to all f âˆˆ F. While the function
f (x) is not bounded, for a given sample x1 , x2 , . . . , x2m ,
each f (xi ) is contained within [0, s(xi )]. Given the sample
x1 , x2 , . . . , x2m , the random variable Ïƒi (f (xi ) âˆ’ f (xi+m )
is bounded in 0 Â± max (s(xi ), s(xi+m )) and has zero mean.
Hence, given independent samples x1 , x2 , . . . , x2m , the
probability of (7) occurring for a single f âˆˆ F can be



#
m

 X

1
Ïƒi (f (xi ) âˆ’ f (xi+m )) > 
P âˆƒf âˆˆ F : 

m
"

i=1

2

 m
Pdim(F ) âˆ’ 200 1 P
2m s(x )2
i
i=1
2m
â‰¤ 4 16e2
e
.

The key difficulty in proving Lemma 5 is that the functions
f âˆˆ F are not bounded uniformly in [0, 1]. To this end,
we provide in Lemma 4 a novel result that bounds the size
of -packings of F if the functions f âˆˆ F are bounded in
expectation. Based on Lemma 5, we then prove the main
claim of Theorem 5.

6. Analysis for k-Means
In order to apply Theorem 5 to k-Means clustering, we require a hsuitablei family F, an upper bound s(x) and a bound
2

on EP s(x) . We provide this in Lemma 1 and defer
P2m
2
1
bounding 2m
i=1 s(xi ) to the proofs of Theorems 2-4.

Lemma 1 (k-Means). Let k âˆˆ N. Let P be any
 distribution
on Rd with Âµ = EP [x], Ïƒ 2 = EP d(x, Âµ)2 âˆˆ (0, âˆ) and
bounded kurtosis MÌ‚4 . For any x âˆˆ Rd and any Q âˆˆ RdÃ—k ,
define
d(x, Q)2
fQ (x) = 1 2 1
(8)
2
2 Ïƒ + 2 EP [d(x, Q) ]

	
as well as the function family F = fQ (Â·) | Q âˆˆ RdÃ—k .
Let
4 d(x, Âµ)2
s(x) =
+ 8.
Ïƒ2
We then have
Pdim(F) â‰¤ 6k(d + 4) log 6k,

(9)

fQ (x) â‰¤ s(x)

(10)

Uniform Deviation Bounds for k-Means Clustering

for any x âˆˆ Rd and Q âˆˆ RdÃ—k and
h
i
2
EP s(x) = 128 + 16MÌ‚4 .

(11)

The proof of Lemma 1 is provided in Section C of the
Supplementary Materials. The definition of fQ (x) in (8) is
motivated as follows: If we use Theorem 5 to guarantee
m

X



f (xi ) âˆ’ EP [f (x)] â‰¤  âˆ€f âˆˆ F.
(12)



i=1

then this implies





Ï†X (Q) âˆ’ EP d(x, Q)2  â‰¤  Ïƒ 2 +  EP d(x, Q)2
2
2
(13)
as is required by Theorems 2-4. Lemma 1 further shows
2
that E [s(x)] is bounded if and only if the kurtosis of P is
bounded. This is the reason why a bounded kurtosis is the
weakest assumption on P that we require in Section 4.
We now proceed to prove Theorems 2-4 by applying
P2m Theo2
1
rem 5 and examining the tail behavior of 2m
i=1 s(xi ) .
6.1. Proof of Theorem 1 (kurtosis bound)

Proof. Hoelderâ€™s inequality implies


4
4
EP d(x, Âµ)4
EP [d(x, Âµ)p ] p
p
MÌ‚4 =
â‰¤
â‰¤
MÌ‚
p
Ïƒ4
Ïƒ4


Hence, by Lemma 1 we have that EP s(x)2 â‰¤ 128 +
4

16MÌ‚p p Since s(x)2 â‰¥ 0 for all x âˆˆ Rd , we have





s(x)2 âˆ’ EP s(x)2  â‰¤ max s(x)2 , EP s(x)2


4
p
2
â‰¤ max s(x) , 128 + 16MÌ‚p
â‰¤ 128


4
d(x, Âµ)4
+ 16 max MÌ‚p p , 2
.
Ïƒ4
(14)
This implies that
h

 p i
EP s(x)2 âˆ’ EP s(x)2  4


p
p EP [d(x, Âµ) ]
p
p
4
4
4
â‰¤ 256 + 32 max MÌ‚p , 2
Ïƒp


p
p
p
â‰¤ 256 4 + 32 4 max MÌ‚p , 2 4 MÌ‚p
p

p

The bound based on the kurtosis follows easily from
Markovâ€™s inequality.




Proof. We consider the choice t = 4 128 + 16MÌ‚4 /Î´.
By Markovâ€™s inequality and linearity of expectation, we
then have by Lemma 1 that
#
"


2m
E s(x)2
Î´
1 X
2
s(xi ) > t â‰¤
= .
P
2m i=1
t
4
h
i
2
Furthermore, EP s(x) â‰¤ t. Hence, we may apply Theorem 5 to obtain that for



12800 8 + MÌ‚4 
1
3 + 30k(d + 4) log 6k + log
,
mâ‰¥
2 Î´
Î´
it holds with probability at least 1 âˆ’ Î´ that


m

1 X


f (xi ) âˆ’ E [f (x)] â‰¤  âˆ€f âˆˆ F.

m

i=1

This implies the main claim and thus concludes the proof.

6.2. Proof of Theorem 2 (higher order moment bound)
We prove
by bounding the higher moments
P2mthe result
2
1
of 2m
s(x
)
using
the Marcinkiewicz-Zygmund ini
i=1
equality and subsequently applying Markovâ€™s inequality.

(15)

â‰¤ 256 4 + 64 4 MÌ‚p .
We apply a variant of the Marcinkiewicz-Zygmund inequality (Ren & Liang,
2001)
to the zero-mean random variable


s(x)2 âˆ’ EP s(x)2 to obtain
ï£®
 p4 ï£¹
2m
 1 X





EP ï£°
s(xi )2 âˆ’ EP s(x)2  ï£»
 2m

i=1



pâˆ’4
âˆš
4 2m

 p4



pâˆ’4
âˆš
4 2m

 p4 

â‰¤
â‰¤

h

 p i
EP s(x)2 âˆ’ EP s(x)2  4
p

p

256 4 + 64 4 MÌ‚p

(16)



For u > 0, the Markov inequality implies

#
"
2m
 1 X



2
2 
P 
s(xi ) âˆ’ EP s(x)  > u

 2m
i=1

 p4 

p
p
pâˆ’4
âˆš
â‰¤
256 4 + 64 4 MÌ‚p
4u 2m

 p4


p
p
pâˆ’4
âˆš
â‰¤
2 max 256 4 , 64 4 MÌ‚p
4u 2m


 p4
4
pâˆ’4
p
âˆš
â‰¤2
max 256, 64MÌ‚p
4u 2m


 p4
4
pâˆ’4
p
âˆš
â‰¤2
64 + 16MÌ‚p
.
u 2m

(17)

Uniform Deviation Bounds for k-Means Clustering
p



4
p
For u = (p âˆ’ 4) 64 + 16MÌ‚p , we thus have

#
"
2m
 1 X


p

2
2 
P 
s(xi ) âˆ’ EP s(x)  > u â‰¤ 2mâˆ’ 8
 2m

i=1

(18)
Since m â‰¥

8
Î´

 p8

, this implies


#
"
2m
 1 X



Î´


P 
s(xi )2 âˆ’ EP s(x)2  > u â‰¤
 2m

4
i=1

(19)

p

p

By the definition of the gamma function and since p is even,
we have
Z âˆ

 p  p2 âˆ’1
p p
p
eâˆ’t t 2 âˆ’1 dt = Î“
=
âˆ’ 1 !â‰¤
2
2
2
0
Hence, for p âˆˆ {4, 8, . . . , âˆ}, we have
4

MÌ‚p p â‰¤

It holds that


4
4


p
2
u + EP s(x) = (p âˆ’ 4) 64 + 16MÌ‚p
+ 128 + 16MÌ‚p p


4
p
â‰¤ p 64 + 16MÌ‚p
(20)


4
We set t = p 64 + 16MÌ‚p p and thus have
"
P

#
2m
Î´
1 X
2
s(xi ) > t â‰¤
2m i=1
4
h

2

In combination with EP s(x)

i

(21)

p

Let u(t) = b 4 t 2 which implies du/dt = b 4 p2 t 2 âˆ’1 . Hence,
p
Z
ab 4 p âˆ âˆ’t p âˆ’1
MÌ‚p â‰¤
e t 2 dt.
2
0

Let pâˆ— = 4

5
4

+

3
4

log

1
Î´

1 p4 2
1
a bp â‰¤ abp2 .
4
4


which implies

pâˆ— â‰¥ 5 + 3 log
8
Î´

1
8
8
â‰¥
log
Î´
log 48
Î´

 p8

â‰¤ 48. We instantiate Theorem 2 with
8
the pâˆ— th-order bound MË†pâˆ— of P . Since 8Î´ pâˆ— â‰¤ 48, the
minimum sample size is thus



3200pâˆ—
1
abpâˆ— 2
3 + 30k(d + 4) log 6k + log
.
4+
2
4
Î´
and thus

âˆ—

The main claim finally holds since pâˆ— â‰¤ p = 9 + 3 log 1Î´ .

â‰¤ t by Lemma 1, we may

1
thus apply Theorem 5. Since m â‰¥ 3200m
with
2




4
1
m1 = p 4 + MÌ‚p p
3 + 30k(d + 4) log 6k + log
Î´

it holds with probability at least 1 âˆ’ Î´ that


m
1 X



f (xi ) âˆ’ E [f (x)] â‰¤  âˆ€f âˆˆ F.

m


6.4. Proof of Theorem 4 (bounded support)
Proof. Let t = 128 + 64R4 /Ïƒ 4 . Since the support of P is
bounded,h we have
x âˆˆ Rd . This implies
i s(x) â‰¤ t for all
P
2
2m
2
1
that EP s(x) â‰¤ t and that 2m
i=1 s(xi ) â‰¤ t almost
surely. The result then follows from Theorem 5.

i=1

This implies the main claim and thus concludes the proof.

6.3. Proof of Theorem 3 (subgaussianity)
Under subgaussianity, all moments of d(x, Âµ) are bounded.
We show the result by optimizing over p in Theorem 2.
Proof. For p âˆˆ {4, 8, . . . , âˆ}, we have
 

 d(x, Âµ) p

MÌ‚p = EP 
Ïƒ 


Z âˆ
1
d(x, Âµ)
p
=
P
> u du
Ïƒ
0
!
2
Z âˆ
up
â‰¤
a exp âˆ’ âˆš
du.
b
0

(22)

7. Conclusion
We have presented a framework to uniformly approximate
the expected value of unbounded functions on a sample.
With this framework we are able to provide theoretical guarantees for empirical risk minimization in k-Means clustering
if the kurtosis of the underlying distribution is bounded. In
particular, we obtain state-of-the art bounds on the sufficient
number of samples to achieve a given uniform approximation error. If the underlying distribution fulfills stronger
assumptions, such as bounded higher moments, subgaussianity or bounded support, our analysis yields progressively
better bounds. We conjecture that Theorem 5 can be applied
to other related problems such as hard and soft Bregman
clustering, likelihood estimation of Gaussian mixture models, as well as nonparametric clustering problems. However,
such results do not follow immediately and require additional arguments beyond the scope of this paper.

Uniform Deviation Bounds for k-Means Clustering

Acknowledgements
This research was partially supported by SNSF NRP 75,
ERC StG 307036, a Google Ph.D. Fellowship and an IBM
Ph.D. Fellowship. This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing.

References
Antos, AndraÌs, Gyorfi, L, and Gyorgy, Andras. Individual convergence rates in empirical vector quantizer design. IEEE Transactions on Information Theory, 51(11):4013â€“4022, 2005.
Bachem, Olivier, Lucic, Mario, and Krause, Andreas. Coresets for
nonparametric estimation - the case of DP-means. In International Conference on Machine Learning (ICML), 2015.
Bachem, Olivier, Lucic, Mario, Hassani, S. Hamed, and Krause,
Andreas. Fast and provably good seedings for k-means. In
Advances in Neural Information Processing Systems (NIPS), pp.
55â€“63, 2016.
Bachem, Olivier, Lucic, Mario, and Krause, Andreas. Distributed
and provably good seedings for k-means in constant rounds. In
To appear in International Conference on Machine Learning
(ICML), pp. 209â€“217, 2017.
Bartlett, Peter L, Linder, TamaÌs, and Lugosi, GaÌbor. The minimax
distortion redundancy in empirical quantizer design. IEEE
Transactions on Information Theory, 44(5):1802â€“1813, 1998.
Ben-David, Shai. A framework for statistical clustering with
constant time approximation algorithms for k-median and kmeans clustering. Machine Learning, 66(2-3):243â€“257, 2007.
Ben-David, Shai, Von Luxburg, Ulrike, and PaÌl, DaÌvid. A sober
look at clustering stability. In International Conference on
Computational Learning Theory (COLT), pp. 5â€“19. Springer,
2006.
Boucheron, SteÌphane, Lugosi, GaÌbor, and Massart, Pascal. Concentration inequalities: A nonasymptotic theory of independence.
Oxford University Press, 2013.
Dinh, Vu C, Ho, Lam S, Nguyen, Binh, and Nguyen, Duy. Fast
learning rates with heavy-tailed losses. In Advances in Neural
Information Processing Systems (NIPS), pp. 505â€“513, 2016.
GruÌˆnwald, Peter D and Mehta, Nishant A. Fast rates with unbounded losses. arXiv preprint arXiv:1605.00252, 2016.
Har-Peled, Sariel. Geometric approximation algorithms, volume
173. American Mathematical Society Boston, 2011.
Haussler, David. Decision theoretic generalizations of the pac
model for neural net and other learning applications. Information and Computation, 100(1):78â€“150, 1992.
Hoeffding, Wassily. Probability inequalities for sums of bounded
random variables. Journal of the American Statistical Association, 58(301):13â€“30, 1963.
Koltchinskii, Vladimir. Oracle Inequalities in Empirical Risk
Minimization and Sparse Recovery Problems. Lecture Notes in
Mathematics. Springer, 2011.

Levrard, CleÌment et al. Fast rates for empirical vector quantization.
Electronic Journal of Statistics, 7:1716â€“1746, 2013.
Li, Yi, Long, Philip M, and Srinivasan, Aravind. Improved bounds
on the sample complexity of learning. Journal of Computer and
System Sciences, 62(3):516â€“527, 2001.
Linder, TamaÌs, Lugosi, GaÌbor, and Zeger, Kenneth. Rates of convergence in the source coding theorem, in empirical quantizer
design, and in universal lossy source coding. IEEE Transactions
on Information Theory, 40(6):1728â€“1740, 1994.
Lucic, Mario, Bachem, Olivier, and Krause, Andreas. Strong
coresets for hard and soft bregman clustering with applications
to exponential family mixtures. In International Conference on
Artificial Intelligence and Statistics (AISTATS), May 2016.
Lucic, Mario, Faulkner, Matthew, Krause, Andreas, and Feldman,
Dan. Training mixture models at scale via coresets. To appear
in Journal of Machine Learning Research (JMLR), 2017.
Mendelson, Shahar. Learning without concentration for general
loss functions. arXiv preprint arXiv:1410.3192, 2014.
Mendelson, Shahar et al. Learning without concentration. In
International Conference on Computational Learning Theory
(COLT), pp. 25â€“39, 2014.
Moors, Johannes J A. The meaning of kurtosis: Darlington reexamined. The American Statistician, 40(4):283â€“284, 1986.
Pollard, David. Strong consistency of k-means clustering. The
Annals of Statistics, 9(1):135â€“140, 1981.
Pollard, David. Convergence of stochastic processes. Springer
Series in Statistics. Springer, 1984.
Rakhlin, Alexander and Caponnetto, Andrea. Stability of k-means
clustering. Advances in Neural Information Processing Systems
(NIPS), 19:1121, 2007.
Ren, Yao-Feng and Liang, Han-Ying. On the best constant in
Marcinkiewicz-Zygmund inequality. Statistics & Probability
Letters, 53(3):227â€“233, 2001.
Sauer, Norbert. On the density of families of sets. Journal of
Combinatorial Theory, Series A, 13(1):145â€“147, 1972.
Shamir, Ohad and Tishby, Naftali. Cluster stability for finite
samples. In Advances in Neural Information Processing Systems
(NIPS), pp. 1297â€“1304, 2007.
Shamir, Ohad and Tishby, Naftali. Model selection and stability
in k-means clustering. In International Conference on Computational Learning Theory (COLT), pp. 367â€“378. Citeseer,
2008.
Telgarsky, Matus J and Dasgupta, Sanjoy. Moment-based uniform
deviation bounds for k-means and friends. In Advances in
Neural Information Processing Systems (NIPS), pp. 2940â€“2948,
2013.
van der Vaart, Aad W and Wellner, Jon A. Weak convergence and
empirical processes: with applications to statistics. Springer
Series in Statistics. Springer New York, 1996.
Vapnik, Vladimir N and Chervonenkis, Alexey Ya. On the uniform
convergence of relative frequencies of events to their probabilities. Theory of Probability & Its Applications, 16(2):264â€“280,
1971.

Uniform Deviation Bounds for k-Means Clustering

A. Auxiliary lemmas

For j â‰¥ 2, we have

For the following proofs we require two auxiliary lemmas.
Lemma 2. Let x > 0 and a > 0. If
x â‰¤ a log x

(23)

x â‰¤ 2a log 2a.

(24)

then it holds that

âˆš
âˆš
Proof. Since x > 0, we have x > 0 and thus log x â‰¤
âˆš
x. Together with (23), this implies
x â‰¤ a log x = 2a log

âˆš

âˆš
x â‰¤ 2a x,

and thus
x â‰¤ 4a2 .
We show the result by contradiction. Suppose that
x > 2a log 2a.
Together with (23), this implies

âˆš

jâˆ’

âˆš

jâˆ’1â‰¤

âˆš

2 âˆ’ 1 and hence

r
r !
n r j
X
1
1 âˆš
1
Sn â‰¤
+
2âˆ’1
1âˆ’
2
2
2
j=2
r
r
âˆš
jâˆ’2
n
1
2âˆ’1X 1
=
+
2
2
2
j=2
r
r
âˆš
j
n
1
2âˆ’1X 1
+
=
.
2
2
2
j=0
| {z }
(âˆ—)

The term (âˆ—) is a geometric series and hence
q âˆ’1
Pn q j 
. This implies
limnâ†’âˆ j=0 12 = 1 âˆ’ 12
ï£®
ï£¹
r
r !âˆ’1
âˆš
2âˆ’1 ï£º
1
ï£¯ 1
q ï£» 1 âˆ’
lim Sn â‰¤ ï£°
+ 
n â†’âˆ
2 2 1âˆ’ 1
2
2
q 
âˆš 
1
2
1
âˆ’
2
1
q + 
=âˆš 
q 2
2 1 âˆ’ 12
2 1 âˆ’ 12

2a log 2a < a log x

2
2âˆ’1
âˆš
2
2+1
âˆš
=âˆš
2âˆ’1 2+1
âˆš
=2+2 2
â‰¤5
=âˆš

which in turn leads to the contradiction
x > 4a2 .
This concludes the proof since (24) must hold.
Lemma 3. For n âˆˆ N, define
Sn =

n
X
j=1

r

as desired.
j
2j

Then,
lim Sn â‰¤ 5.

nâ†’âˆ

Proof. Subtracting
r
n r
n r
X
X
1
j
jâˆ’1
Sn =
=
j+1
2
2
2j
j=1
j=2
from Sn yields
r !
âˆš
n âˆš
X
1
jâˆ’ jâˆ’1
1âˆ’
Sn =
2
2j/2
j=1
r
âˆš
n âˆš
1 X jâˆ’ jâˆ’1
=
+
.
2 j=2
2j/2

B. Proof of Theorem 5
We first show two results, Lemma 4 and 5 and then use them
to prove Theorem 5.
Definition 2. Let F be a family of functions from X to Râ‰¥0
and Q an arbitrary measure on X . For any f, g âˆˆ F, we
define the distance function
Z
dL1 (Q) (f, g) =
|f (x) âˆ’ g(x)| dQ(x).
X

For any f âˆˆ F and A âŠ† F, we further define
dL1 (Q) (f, A) = min dL1 (Q) (f, g).
gâˆˆA

Definition 3. For  > 0, a set A âŠ† B is an -packing of B
with respect to some metric d if for any two distinct x, y âˆˆ A,
d(x, y) > . The cardinality of the largest -packing of B
with respect to d is denoted by M (, B, d).

Uniform Deviation Bounds for k-Means Clustering

Lemma 4 (-packing). Let F be a family of functions from
X to Râ‰¥0 with Pdim(F ) = d. For all x âˆˆ X , let s(x) =
supf âˆˆF f (x). Let Q be an arbitrary measure on X with
0 < EQ [s(x)] < âˆ. Then, for all 0 <  â‰¤ EQ [s(x)],

Consider the set X0 = {x âˆˆ X : s(x) = 0} and define
X>0 = X \ X0 . By definition of QÌƒ, X0 is zero set of
QÌƒ
R and, since f (x), g(x) âˆˆ [0, s(x)] for all x âˆˆ X , we have
|f (x) âˆ’ g(x)| dQ(x) = 0.
X0


2d

2eEQ [s(x)]
M , F, dL1 (Q) â‰¤ 8
.


For any two distinct f, g âˆˆ G, we thus have for all i =
1, . . . , m
P [sign(f (xi ) âˆ’ ri ) 6= sign(g(xi ) âˆ’ ri )]
Z Z |f (xi )âˆ’g(xi )|
1
=
drdQÌƒ(x)
s(x)
X 0
Z
Z |f (xi )âˆ’g(xi )|
1
drdQÌƒ(x)
=
s(x)
X>0 0
Z
|f (xi ) âˆ’ g(xi )|
=
dQÌƒ(x)
s(x)
X>0
Z
|f (xi ) âˆ’ g(xi )|
dQ(x)
=
EQ [s(x)]
X>0
Z
|f (xi ) âˆ’ g(xi )|
=
dQ(x)
EQ [s(x)]
X

>
EQ [s(x)]

Proof. Our proof is similar to the proof of Theorem 6
in Haussler (1992). The difference is that we consider a
function family F that is not uniformly bounded but only
bounded in expectation. The key idea is to construct a random sample and to use the expected number of dichotomies
on that set to bound the size of an -packing by the pseudodimension.
Noting that by definition s(x) â‰¥ 0 and EQ [s(x)] < âˆ, we
define the probability measure QÌƒ on X using the RadonNikodym derivative
dQÌƒ(x)
s(x)
=
,
dQ(x)
EQ [s(x)]

âˆ€x âˆˆ X .

Let ~x = (x1 , x2 , . . . , xm ) be a random vector in X m , where
each xi is drawn independently at random from QÌƒ. Given
~x, let ~r = (r1 , r2 , . . . , rm ) be a random vector, where each
ri is drawn independently at random from a uniform distribution on [0, s(xi )].
For any f
âˆˆ
F, we denote the restriction
(f (x1 ), . . . , f (xm )) by f~x and set F~x = {f~x | f âˆˆ F}.
For any vector ~z âˆˆ Rm , we define

This allows us to bound the probability that two distinct
f, g âˆˆ G produce the same dichotomy on all m samples, i.e.
P [sign(f~x âˆ’ ~r) = sign(g~x âˆ’ ~r)]
=

m
Y

(1 âˆ’ P [ sign(f (xi ) âˆ’ ri ) 6= sign(g(xi ) âˆ’ ri ) ])

i=1

sign(~z) = (sign(z1 ), . . . , sign(zm )) .


â‰¤

The set of dichotomies induced by ~r on F~x is given by
sign(F~x âˆ’ ~r) = {sign(f~x âˆ’ ~r) | f âˆˆ F} .


1âˆ’
EQ [s(x)]


â‰¤ exp âˆ’

m
EQ [s(x)]


.

Given ~x âˆˆ X m and ~r âˆˆ Rm , let H be the subset of G with
unique dichotomies, i.e., H âŠ† G such that for any f âˆˆ H,

For m â‰¥ d, Sauerâ€™s Lemma (Sauer, 1972; Vapnik & Chervonenkis, 1971) bounds the size of this set by
d

|sign(F~x âˆ’ ~r)| â‰¤ (em/d) ,
m

m

sign(f~x âˆ’ ~r) 6= sign(g~x âˆ’ ~r),
for all g âˆˆ G \ {f }. We then have

m

for all ~x âˆˆ X and ~r âˆˆ R . Hence, the expected number
of dichotomies is also bounded, i.e.
d

E [|sign(F~x âˆ’ ~r)|] â‰¤ (em/d) .

(25)

P [f âˆˆ
/ H]
= P [ âˆƒg âˆˆ G \ {f } : sign(f~x âˆ’ ~r) = sign(g~x âˆ’ ~r) ]
â‰¤ |G| max P [ sign(f~x âˆ’ ~r) = sign(g~x âˆ’ ~r) ]

1
Let G be a -separated subset
 of F with respect to dL (Q)
with |G|= M , F, dL1 (Q) . By definition, for any two
distinct f, g âˆˆ G, we have
Z
|f (x) âˆ’ g(x)| dQ(x) > .

X

gâˆˆG\{f }


â‰¤ |G|Â· exp âˆ’

m
EQ [s(x)]


.

This allows us to bound the expected number of dichotomies

Uniform Deviation Bounds for k-Means Clustering


Together with (27) and |G|= M , F, dL1 (Q) , we have

from below, i.e.,
E [|sign(F~x âˆ’ ~r)|] â‰¥ E [|sign(G~x âˆ’ ~r)|]
â‰¥ E [|sign(H~x âˆ’ ~r)|]
â‰¥ E [|H|]
X
=
(1 âˆ’ P [ f âˆˆ
/ H ])





M , F, dL1 (Q) = |G|â‰¤ 8

2eEQ [s(x)]


2d

as required which concludes the proof.

f âˆˆG



â‰¥ |G| 1 âˆ’ |G|Â· exp âˆ’

m
EQ [s(x)]

Together with (25), we thus have for m â‰¥ d



 em d
m
â‰¥ |G| 1 âˆ’ |G|Â· exp âˆ’
.
d
EQ [s(x)]


.

(26)

Lemma 5 (Chaining). Let F be a family of functions from
X to Râ‰¥0 with Pdim(F ) = d < âˆ. For all x âˆˆ X ,
let s(x) = supf âˆˆF f (x). For m â‰¥ 200K(2d + 1)/2 , let
P2m
1
2
x1 , . . . , x2m be a subset of X with K = 2m
i=1 s(xi ) <
âˆ. For i = 1, 2, . . . , m, let Ïƒi be drawn from {âˆ’1, 1}
uniformly at random. Then, for all 0 <  â‰¤ 1,


#
m
 X

1

P âˆƒf âˆˆ F : 
Ïƒi (f (xi ) âˆ’ f (xi+m )) > 
m

"

Consider the case

i=1

EQ [s(x)]
ln(2|G|) < d.


2 d

â‰¤ 4 16e


Since  â‰¤ EQ [s(x)] and |G|= M , F, dL1 (Q) , we then
have

1
1
M , F, dL1 (Q) = |G|â‰¤ ed/EQ [s(x)] â‰¤ ed
2
2

m
X

Ïƒi (f (xi ) âˆ’ f (xi+m ) = 0

i=1

for all f âˆˆ F. For the remainderP
of the proof, we hence
2m
1
only need to consider the case 2m
i=1 s(xi ) > 0.
We define the discrete measure Q by placing an atom at
each xi with weight proportional to s(xi ). More formally,

Together with (26) and m â‰¥ d, it follows that

|G|â‰¤ 2

eEQ [s(x)]
ln(2|G|)
d

PXâˆ¼Q [X = x] =

d

|G|

Since
p

d
âˆš
2d dd 2|G|
2eEQ [s(x)]
â‰¤
2
2
.
(ln 2|G|)d


2m
X
i=1

and hence
p

e

P2m
1
Proof. Consider the case 2m
i=1 s(xi ) â‰¤ 0. By definition, we have s(xi ) â‰¥ f (xi ) â‰¥ 0 for all f âˆˆ F and
i = 1, . . . , 2m. Thus, f (xi ) = 0 for all i = 1, . . . , 2m.
The claim then follows directly since

as required to show the result. We hence assume
EQ [s(x)] ln(2|G|)/ â‰¥ d for the remainder of the proof.
Let m â‰¥ EQ [s(x)] ln(2|G|)/ which implies


m
1
1 âˆ’ |G|Â· exp âˆ’
â‰¥ .
EQ [s(x)]
2

2

 m
âˆ’ 200K

(27)

P2m

i=1

s(xi )
1{xi =x} ,
P2m
k=1 s(xk )

s(xi )2 < âˆ and

P2m

P2m

EQ [s(x)] = Pi=1
2m

i=1

Since ln x â‰¤ x, we have for x = (2|G|)1/2d that
ln(2|G|)1/2d â‰¤ (2|G|)1/2d
which implies
ln(2|G|) â‰¤ 2d(2|G|)1/2d
and hence

p
2d dd 2|G|
1â‰¤
.
(ln 2|G|)d

s(xi ) > 0, we have

s(xi )2

k=1

âˆ€x âˆˆ X .

s(xk )

< âˆ.

(28)

For j âˆˆ N, let Î³j = EQ [s(x)] /2j . We define a sequence
G1 , G2 , . . . , Gâˆ of Î³j -packings of F as follows: Let the set
G0 consist of an arbitrary element f âˆˆ F. For any j âˆˆ N,
we initialize Gj to Gjâˆ’1 . Then, we select a single element
f âˆˆ F with dL1 (Q) (f, Gj ) > Î³j and add it to Gj . We repeat
this until no such element f âˆˆ F with dL1 (Q) (f, Gj ) > Î³j
is left. By definition, Gj is an Î³j -packing of F with respect
to dL1 (Q) . Hence, for any f âˆˆ F, we have
dL1 (Q) (f, Gj ) â‰¤ Î³j = EQ [s(x)] /2j .

(29)

Uniform Deviation Bounds for k-Means Clustering

By Lemma 4, the size of Gj is bounded by
|Gj |â‰¤ 2(2e2j )2d = 22d(j+1)+1 e2d .

(30)

for some Îº > 0. By (29) and j sufficiently large, there
exists a g âˆˆ G such that
dL1 (Q) (f, g) <

For each f âˆˆ F and j âˆˆ N, we define the closest element
in Gj by
Ï†j (f ) = arg min dL1 (Q) (f, g).
gâˆˆGj

By (30), Gj is finite for each j âˆˆ N and the minimum is
well-defined.

(32)

The key idea is that intuitively any f âˆˆ F can be additively decomposed into functions from the sequence
H1 , H2 , . . . , Hâˆ . By definition, for any j âˆˆ N, any function g âˆˆ Gj can be rewritten as
hg,k

k=1

where (hg,1 , hg,2 , . . . , hg,j ) are functions in H1 Ã— H2 Ã—
. . . Ã— Hj . Let j â†’ âˆ and define
G=

âˆ
[

(35)

i=1

i=1

2m

+

1 X
|f (xi ) âˆ’ g(xi )| .
m i=1

(31)

Furthermore, by (30), we have for all j âˆˆ N

j
X

Îº2 .



m
1 X



â‰¤
Ïƒi (g(xi ) âˆ’ g(xi+m ))
m


For all j âˆˆ N and h âˆˆ Hj , there is hence a gh âˆˆ Gj such
that h = gh (x) âˆ’ Ï†jâˆ’1 (gh ). By (29), we thus have for all
j âˆˆ N and h âˆˆ Hj

g=

k=1 s(xk )



m
1 X



+Îº=
Ïƒi (f (xi ) âˆ’ f (xi+m ))
m


Hj = {g âˆ’ Ï†jâˆ’1 (g) : g âˆˆ Gj } .

|Hj |â‰¤ |Gj |â‰¤ 22d(j+1)+1 e2d .

4
P2m

Using the triangle inequality, we have

We construct the following sequence H1 , H2 , . . . , Hâˆ : Let
H1 be equal to G1 . For each j âˆˆ {2, 3, . . . , âˆ}, we define

EQ [|h(x)|] = dL1 (Q) (gh , Gjâˆ’1 ) â‰¤ Î³jâˆ’1 .

1
2m

Gj .

j=1

Clearly, G is dense in F with respect to dL1 (Q) . We claim
that, as a consequence,


m
1 X



âˆƒf âˆˆ F : 
Ïƒi (f (xi ) âˆ’ f (xi+m )) > 
(33)
m

i=1

Using the Cauchy-Schwarz inequality, the fact that
f (x), g(x) âˆˆ [0, s(x)] for all x âˆˆ X , as well as the definition of dL1 (Q) and (35), we may bound
2m
1 X
|f (xi ) âˆ’ g(xi )|
m i =1

v
u
2m
u 1 X
2
â‰¤t 2
|f (xi ) âˆ’ g(xi )|
m i=1
v
u
2m
u 1 X
|f (xi ) âˆ’ g(xi )| s(xi )
â‰¤t 2
m i=1
v
u P2m
2m
u
s(xk ) X
s(xi )
= t k=1 2
|f (xi ) âˆ’ g(xi )| P2m
m
k=1 s(xk )
i=1
s
=

4

P2m

s(xk )
dL1 (Q) (f, g)
2m

k=1

< Îº.

if and only if


m
1 X



âˆƒg âˆˆ G : 
Ïƒi (g(xi ) âˆ’ g(xi+m )) > .
m


Together with (B), we hence have
(34)

i=1

Since G âŠ† F, we have (34) =â‡’ (33). To show the
converse, assume âˆƒf âˆˆ F such that


m
1 X



Ïƒi (f (xi ) âˆ’ f (xi+m )) =  + Îº,

m

i=1



m
1 X



Ïƒi (g(xi ) âˆ’ g(xi+m )) > .

m

i=1

which implies (33) =â‡’ (34) as claimed.
As a consequence, it is sufficient to only consider G instead

Uniform Deviation Bounds for k-Means Clustering

of F. More formally,

Since all Xi are independent, we may apply Hoeffdingâ€™s
inequality. By Theorem 2 of Hoeffding (1963), we have



#
m
 X

1

P âˆƒf âˆˆ F : 
Ïƒi (f (xi ) âˆ’ f (xi+m )) > 
m

i=1


#
"
m

 X

1
Ïƒi (g(xi ) âˆ’ g(xi+m )) > 
= P âˆƒg âˆˆ G : 

m
 i=1

"
#
m X
âˆ
 X

1

â‰¤ P âˆƒg âˆˆ G : 
Ïƒi (hg,j (xi ) âˆ’ hg,j (xi+m )) > 
m

i=1 j=1


#
"
m
âˆ 

X

1 X
Ïƒi (hg,j (xi ) âˆ’ hg,j (xi+m )) > 
â‰¤ P âˆƒg âˆˆ G :


m
"

j=1

i=1


"
#
22 m
m
1 X

âˆ’ 1 Pm j


2
P 
Xi âˆ’ E [Xi ] > j â‰¤ e m i=1 (ai âˆ’bi )
m

i=1

Using (37), we have
(ai âˆ’bi )2 = 4(h(xi )âˆ’h(xi+m ))2 â‰¤ 4h(xi )2 +4h(xi+m )2
which implies that
m

q
For i âˆˆ N, let j = 5 2jj . In Lemma 3, we show that
Pâˆ
j=1 j â‰¤ . Suppose it holds that


m
1 X



Ïƒi (hg,j (xi ) âˆ’ hg,j (xi+m )) â‰¤ j

m

i=1
for all g âˆˆ G and j âˆˆ N. Then, we have that


âˆ
m
âˆ 
 X
X

1 X
j â‰¤ 
Ïƒi (hg,j (xi ) âˆ’ hg,j (xi+m )) â‰¤


m
j=1
i=1
j=1
for all g âˆˆ G. Hence, using the union bound, we have


#
m
 X

1

P âˆƒf âˆˆ F : 
Ïƒi (f (xi ) âˆ’ f (xi+m )) > 
m

i=1


"
#
âˆ
m


X

1 X
â‰¤
P âˆƒg âˆˆ G : 
Ïƒi (hg,j (xi ) âˆ’ hg,j (xi+m )) > j
m

j=1
i=1


"
#
âˆ
m


X
1 X

P âˆƒh âˆˆ Hj : 
=
Ïƒi (h(xi ) âˆ’ h(xi+m )) > j
m

j=1
i=1


"
#
âˆ
m
 X

X
1

|Hj | max P 
Ïƒi (h(xi ) âˆ’ h(xi+m )) > j
â‰¤
hâˆˆHj
m

"

j=1

Using h(xi ) âˆˆ [0, s(xi )] and the definition of EQ [Â·], we
have
m
2m
1 X
1 X
(ai âˆ’ bi )2 â‰¤ 8
|h(xi )|s(xi )
m i =1
2m i=1
!
2m
1 X
=8
s(xi ) EQ [|h(x)|]
2m i=1

By (31) and (28), we thus have
!
2m
1 X
s(xi ) Î³jâˆ’1
2m i=1
!
2m
1 X
4âˆ’j
s(xi ) EQ [s(x)]
=2
2m i=1
!P
2m
2m
s(xi )2
1 X
4âˆ’j
=2
s(xi ) Pi=1
2m
2m i=1
k=1 s(xk )
!
2m
1 X
s(xi )2
= 24âˆ’j
2m i=1

m
1 X
(ai âˆ’ bi )2 â‰¤ 8
m i =1

i=1

(36)

We now use Hoeffdingâ€™s inequality to bound the probability
that


m
1 X



Ïƒi (h(xi ) âˆ’ h(xi+m )) > j ,

m

i=1

= 24âˆ’j K
Since i =

for a single j âˆˆ N and h âˆˆ Hj .


5

q

For i âˆˆ {1, 2, . . . , m}, consider the random variables
âˆ’
Xi = Ïƒi (h(xi ) âˆ’ h(xi+m )).
Since Ïƒi are uniformly drawn at random from {âˆ’1, 1}, we
have E [Xi ] = 0 for all i âˆˆ {1, 2, . . . , m}. Furthermore,
each Xi is bounded in
[ai , bi ] = [0 Â± (h(xi ) âˆ’ h(xi+m ))].

(37)

2m

1 X
1 X
(ai âˆ’ bi )2 â‰¤ 8
h(xi )2 .
m i=1
2m i=1

j
2j

this implies

22j m
2 mj
P
â‰¤
âˆ’
m
1
2
200K
i=1 (ai âˆ’ bi )
m

Hence, for any j âˆˆ N and any h âˆˆ Hj

"
#
m
1 X

2 mj


P 
Ïƒi (h(xi ) âˆ’ h(xi+m )) > j â‰¤ eâˆ’ 200K .
m

i=1

Uniform Deviation Bounds for k-Means Clustering

Together with (32), this allows us to bound (36), i.e.,


#
m
1 X



P âˆƒf âˆˆ F : 
Ïƒi (f (xi ) âˆ’ f (xi+m )) > 
m

"

i=1

â‰¤
â‰¤

âˆ
X

and assume that the event A holds, i.e., there exists a f 0 âˆˆ F
such that


m
1 X



0
0
f (xi ) âˆ’ E [f (x)] > .

m

i=1

|Hj |e

j=1
âˆ
X

2 mj
âˆ’ 200K

22d(jâˆ’1)+4d+1 e2d e

For any f âˆˆ F, Markovâ€™s inequality in combination with
Jensenâ€™s inequality implies

2 mj
âˆ’ 200K

j=1

= 24d+1 e2d e

2 m
âˆ’ 200K

âˆ 
X

4d e

2 m
âˆ’ 200K


"
#
2m
1 X




P 
f (xi ) âˆ’ E [f (x)] >
m
 2
i=m+1

2 
 1 P2m

4 Â· E m
f
(x
)
âˆ’
E
[f
(x)]

i
i=m+1
â‰¤
2

2 
 1 P2m

4 Â· E m
f
(x
)
i 
i=m+1
â‰¤
2
h P
i
2m
2
1
4Â·E m
|f
(x
)|
i
i=m+1
â‰¤
m2


4 Â· E s(x)2
=
.
m2

jâˆ’1

j=1

= 24d+1 e2d e

2 m
âˆ’ 200K

âˆ 
X

2 m

4d eâˆ’ 200K

j

.

j=0

By assumption in the main claim, we have m â‰¥ 200K(2d +
1)/2 and hence
2 m

0 â‰¤ 4d eâˆ’ 200K â‰¤

1
.
2

This implies
âˆ 
X

2 m

4d eâˆ’ 100K

j

j=0

â‰¤

âˆ
X
1
=2
2j
j=0

Together with

and hence

mâ‰¥



#
m

1 X


Ïƒi (f (xi ) âˆ’ f (xi+m )) > 
P âˆƒf âˆˆ F : 

m
i=1
d
2 m
â‰¤ 4 16e2 eâˆ’ 200K

200t
2



1
8t
3 + 5d + log
â‰¥ 2
Î´


"

which concludes the proof.
With these results we are able to prove Theorem 5.

this implies that

#
"
2m
1 X

1


0
0
P 
f (xi ) âˆ’ E [f (x)] â‰¤ /2 â‰¥
m

2
i=m+1


since EP s(x)2 â‰¤ t by (5). Thus, given A, the event B
holds with probability at least 1/2, i.e.,
P [B | A] â‰¤ 1/2.

Proof of Theorem 5. Our goal is to upper bound the probability of the event


(
)
m
1 X



A = âˆƒf âˆˆ F : 
f (xi ) âˆ’ E [f (x)] > 
m


Since

by Î´, i.e., to prove P [A] â‰¤ Î´. Consider the event

we thus have

i=1

P [B] = P [A âˆ© B] = P [B | A] P [A] ,

P [A] â‰¤ 2 Â· P [B] .
(
B=



m
1 X



âˆƒf âˆˆ F : 
f (xi ) âˆ’ E [f (x)] > 
m

i=1


)
2m
1 X



âˆ©
f (xi ) âˆ’ E [f (x)] â‰¤ /2 .
m

i=m+1

(38)

We consider the event


(
)
m
1 X



C = âˆƒf âˆˆ F : 
(f (xi ) âˆ’ f (xi+m )) > /2
m

i=1

Uniform Deviation Bounds for k-Means Clustering

and note that, if B holds, then there exists a f 0 âˆˆ F such
that


m
1 X



< 
f 0 (xi ) âˆ’ E [f 0 (x)]
m

 i=1

m
2m
1 X

1 X 0


0
â‰¤
f (xi ) âˆ’
f (xi )
m

m
i=1
i=m+1


2m
1 X



+
f 0 (xi ) âˆ’ E [f 0 (x)]
m

i=m+1
|
{z
}

Consider any fixed vector ~x = (x1 , x2 , . . . , x2m ): If
E does not hold, then P~Ïƒ [D|E] 1E = 0. Otherwise,
P2m
1
2
â‰¤ t and consequently Lemma 5 with
i=1 s(xi )
2m
K = t implies that for m â‰¥ 200t(2d + 1)/2
P~Ïƒ [D|E] â‰¤ 4 16e2

h
d 2 m i
Î´
+ E~x 4 16e2 eâˆ’ 200t 1E
4
d 2 m
Î´
â‰¤ + 4 16e2 eâˆ’ 200t .
4

P [D] â‰¤

In combination with (39) and (40), this implies that for
m â‰¥ 200t(2d + 1)/2

Hence, B âŠ† C which in combination with (38) implies that
P [A] â‰¤ 2 Â· P [B] â‰¤ 2 Â· P [C] .

2 m

eâˆ’ 200t .

As a result, we have for m â‰¥ 200t(2d + 1)/2

â‰¤ 2

which implies that there exists a f 0 âˆˆ F with


m
1 X



0
0
(f (xi ) âˆ’ f (xi+m )) > /2 .

m

i=1

d

(39)

Let ~Ïƒ = (Ïƒ1 , Ïƒ2 , . . . , Ïƒm ) be a random vector where each
Ïƒi is sampled independently at random from a uniform
distribution on {âˆ’1, 1}. We define the event


(
)
m
1 X



D= f âˆˆF : 
Ïƒi (f (xi ) âˆ’ f (xi+m )) > /2 .
m


P [A] â‰¤

d 2 m
Î´
+ 8 16e2 eâˆ’ 200t .
2

By the main claim, we always have m â‰¥ 200t(2d + 1)/2
and hence we only need to show that
P [A] â‰¤

d 2 m
Î´
+ 8 16e2 eâˆ’ 200t â‰¤ Î´.
2

This is equivalent to

i=1

In essence, Ïƒi randomly permutes xi and xm+i for any
i âˆˆ {1, . . . , m}. Hence, since all xi are identically and
independently distributed and hence exchangeable, we have
P [C] = P [D] .

(40)

d

2 m

eâˆ’ 200t â‰¤ Î´/2

and
log 16 + d(log 16 + 2) âˆ’

2 m
â‰¤ ln Î´.
200t

This is the case if we have

Consider the event
(
E=

8 16e2

2m

1 X
s(xi )2 â‰¤ t
2m i=1

)

and let E denote its complement. By (5), we have that
  Î´
P E â‰¤ .
4
Let E~x [Â·] denote the expectation with regards to the random
vector ~x = (x1 , x2 , . . . , x2m ) and P~Ïƒ [Â·] the probability with
regards to the random vector ~Ïƒ . By construction, ~Ïƒ and ~x
are independent and the event E only depends on ~x but not
on ~Ïƒ . We thus have


P [D] = P D âˆ© E + P [D âˆ© E]
 
â‰¤ P E + E~x [P~Ïƒ [D âˆ© E]]
Î´
â‰¤ + E~x [P~Ïƒ [D] 1E ]
4
Î´
= + E~x [P~Ïƒ [D|E] 1E ] .
4

log 16 + d(log 16 + 2) + log

1
2 m
â‰¤
Î´
200t

or equivalently
mâ‰¥

200t
2


log 16 + d(log 16 + 2) + log

The main claim thus holds since


200t
1
mâ‰¥ 2
3 + 5d + log

Î´
which concludes the proof.

1
Î´


.

Uniform Deviation Bounds for k-Means Clustering

C. Proof of Lemma 1
Proof. We first show (9) with the same notation as in the
proof of Lemma 4. For any f âˆˆ F, ~x âˆˆ X m and ~r âˆˆ Rm ,
we denote the restriction (f (x1 ), . . . , f (xm )) by f~x and set
F~x = {f~x | f âˆˆ F}. For any vector ~z âˆˆ Rm , we define
sign(~z) = (sign(z1 ), . . . , sign(zm )) .

By (42), both F and G induce the same dichotomies on
(xÌ„, rÌ„) and (xÌƒ, ~0) respectively and thus
|sign(FxÌ„ âˆ’ rÌ„)| = |sign(GxÌƒ )| .

(43)

We define the function family


H = min hÂ·, qÌƒi | Q âˆˆ R(d+3)Ã—k
qÌƒâˆˆQ

The set of dichotomies induced by ~r on F~x is given by
and note that by construction G âŠ† H. This implies
sign(F~x âˆ’ ~r) = {sign(f~x âˆ’ ~r) | f âˆˆ F} .

|sign(GxÌƒ )| â‰¤ |sign(HxÌƒ )| .

Let mÌ„ be equal to the pseudo-dimension of F. This implies
that there exist two vectors xÌ„ âˆˆ X mÌ„ and rÌ„ âˆˆ RmÌ„ that are
shattered by F, i.e.,
|sign(FxÌ„ âˆ’ rÌ„)| = 2mÌ„ .

(41)

Consider any x âˆˆ ~x, its corresponding

r âˆˆ ~r and any
2
fQ (Â·) âˆˆ F. Defining ÏƒQ
= EP d(x, Q)2 , we have that
sign(fQ (x) âˆ’ rx )


d(x, Q)2
= sign 1 2 1
âˆ’r
2
2 Ïƒ + 2 EP [d(x, Q) ]


rx 2
2
= sign d(x, Q)2 âˆ’
Ïƒ + ÏƒQ
2



r 2
2
2
= sign min d(x, q) âˆ’
Ïƒ + ÏƒQ
qâˆˆQ
2



 T
 r 2
2
T
T
Ïƒ + ÏƒQ
= sign min x x âˆ’ 2x q + q q âˆ’
qâˆˆQ
2
!
= sign

min hxÌƒ(x, r), qÌƒi ,
qÌƒâˆˆQÌƒ(Q)

(42)
where we have used the mappings
ï£«
ï£¶
âˆ’2x
ï£¬âˆ’r/2ï£·
ï£·
xÌƒ(x, r) = ï£¬
ï£­ 1 ï£¸
xT x

qÌƒâˆˆQ

qÌƒâˆˆQ

Since |Q|= k, this implies that there exists an injective
mapping from H to the k-fold Cartesian product of I that
generates the same dichotomies. In turn, this implies
k

|sign(HxÌƒ )| â‰¤ |sign(IxÌƒ )| .

(45)

The dichotomies induced by I are generated by halfspaces
in Rd+3 . The Vapnik-Chervonenkis dimension of halfspaces
in Rd+3 is bounded by d + 4 (Har-Peled, 2011) and thus
Pdim(I) â‰¤ d + 4. Together with Sauerâ€™s Lemma (Sauer,
1972; Vapnik & Chervonenkis, 1971), this implies

d+4
emÌ„
|sign(IxÌƒ )| â‰¤
.
(46)
d+4

2

Consider the vector xÌƒ = (xÌƒ(xÌ„1 , rÌ„1 ), . . . , xÌƒ(xÌ„mÌ„ , rÌ„mÌ„ )) and
the function family
(
)
qÌƒâˆˆQÌƒ(Q)

For any Q âˆˆ R(d+3)Ã—k , it holds that


[
x âˆˆ xÌƒ | min hx, qÌƒi â‰¤ 0 =
{x âˆˆ xÌƒ | hx, qÌƒi â‰¤ 0} .

mÌ„

ï£±ï£«
ï£¼
ï£¶
q
ï£´
ï£´
ï£´
ï£´
ï£²ï£¬ 2
ï£½
2ï£·
Ïƒ
+
Ïƒ
Qï£·
ï£¬
QÌƒ(Q) = ï£­ T
|
q
âˆˆ
Q
.
q q ï£¸
ï£´
ï£´
ï£´
ï£´
ï£³
ï£¾
1

min hÂ·, qÌƒi | Q âˆˆ R

Consider the function family

	
I = hÂ·, qÌƒi | qÌƒ âˆˆ Rd+3 .

Combining (41), (43), (44), (45) and (46) yields

and

G=

(44)

dÃ—k

.


â‰¤

emÌ„
d+4

(d+4)k
.

(47)

For Pdim(F) = mÌ„ < d + 4, the main claim holds trivially.
On the other hand, for mÌ„ â‰¥ d + 4, (47) implies that


k
mÌ„
2k
mÌ„
mÌ„
â‰¤
1 + log
â‰¤
log
d+4
log 2
d+4
log 2
d+4
Since

mÌ„
d+4

> 0 and

2k
log 2

> 0, Lemma 2 implies that

mÌ„
4k
4k
â‰¤
log
d+4
log 2
log 2
Since

4
log 2

â‰ˆ 5.77 < 6, this proves the claim in (9), i.e.,
Pdim(F) = mÌ„ â‰¤ 6k(d + 4) log 6k.

Uniform Deviation Bounds for k-Means Clustering

Next, we prove (10). For any x âˆˆ Rd and Q âˆˆ RdÃ—k , we
have by the triangle inequality
d(x, Q)2 â‰¤ (d(x, Âµ) + d(Âµ, Q))

2

= d(x, Âµ)2 + d(Âµ, Q)2 + 2 d(x, Âµ) d(Âµ, Q)
For any 0 â‰¤ a â‰¤ b, it holds that
2ab = ab + a(b âˆ’ a) + a2 â‰¤ ab + b(b âˆ’ a) + a2 = b2 + a2 .
Since either 0 â‰¤ d(x, Âµ) â‰¤ d(Âµ, Q) or 0 â‰¤ d(Âµ, Q) <
d(x, Âµ), we thus have for any x âˆˆ Rd and Q âˆˆ RdÃ—k
d(x, Q)2 â‰¤ 2 d(x, Âµ)2 + 2 d(Âµ, Q)2 .

(48)

By the same argument it also holds that for any x âˆˆ Rd and
Q âˆˆ RdÃ—k
d(Âµ, Q)2 â‰¤ 2 d(x, Âµ)2 + 2 d(x, Q)2 .
By takingthe expectation
with regards to P and noting that

Ïƒ 2 = EP d(x, Âµ)2 < âˆ, we obtain for any Q âˆˆ RdÃ—k


d(Âµ, Q)2 â‰¤ 2Ïƒ 2 + 2EP d(x, Q)2 .

(49)

Combining (48) and (49) implies that for any x âˆˆ Rd and
Q âˆˆ RdÃ—k


d(x, Q)2 â‰¤ 2 d(x, Âµ)2 + 4Ïƒ 2 + 4EP d(x, Q)2




2 d(x, Âµ)2
â‰¤ 4+
Ïƒ 2 + 4EP d(x, Q)2
2
Ïƒ




2 d(x, Âµ)2
Ïƒ 2 + EP d(x, Q)2
â‰¤ 4+
2
Ïƒ




4 d(x, Âµ)2 1 2
Ïƒ + EP d(x, Q)2 .
â‰¤ 8+
2
Ïƒ
2
By the definition of fQ (x), this proves (10). Finally we
have
"
2 #
h
i
4 d(x, Âµ)2
2
EP s(x) = EP
+8
Ïƒ2


16 d(x, Âµ)4
64 d(x, Âµ)2
= EP
+
+ 64
Ïƒ4
Ïƒ2


EP d(x, Âµ)4
= 128 + 16
Ïƒ4
= 128 + 16MÌ‚4 .
which shows (11) and concludes the proof.

