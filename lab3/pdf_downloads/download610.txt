Model-Independent Online Learning for Influence Maximization

Appendices
A. Proof of Theorem 1
Proof. Theorem 1 can be proved based on the definitions of monotonicity and submodularity. Note that from Assumption 1,
for any seed set S 2 C, any seed node u 2 S, and any target node v 2 V, we have F ({u}, v)  F (S, v), which implies
that
f (S, v, p⇤ ) = max F ({u}, v)  F (S, v),
u2S

hence

f (S, p⇤ ) =

X

v2V

This proves the first part of Theorem 1.

f (S, v, p⇤ ) 

X

F (S, v) = F (S).

v2V

We now prove the second part of the theorem. First, note that from the first part, we have
e p⇤ )  F (S)
e  F (S ⇤ ),
f (S,

where the first inequality follows from the first part of this theorem, and the second inequality follows from the definition
of S ⇤ . Thus, we have ⇢  1. To prove that ⇢
1/K, we assume that S = {u1 , u2 , . . . , uK }, and define Sk =
{u1 , u2 , . . . , uk } for k = 1, 2, . . . , K. Thus, for any S ✓ V with |S| = K, we have
F (S) = F (S1 ) +

K
X1

[F (Sk+1 )

F (Sk )]

k=1




K
X

k=1

X

v2V

F ({uk }) =

K X
X

k=1 v2V

F ({uk }, v)

K max F ({u}, v) = K
u2S

X

f (S, v, p⇤ ) = Kf (S, p⇤ ),

v2V

where the first inequality follows from the submodularity of F (·). Thus we have
e p⇤ ),
F (S ⇤ )  Kf (S ⇤ , p⇤ )  Kf (S,

e This implies that ⇢
where the second inequality follows from the definition of S.

1/K.

B. Proof of Theorem 2

We start by defining some useful notations. We use Ht to denote the “history” by the end of time t. For any node pair
(u, v) 2 V ⇥ V and any time t, we define the upper confidence bound (UCB) Ut (u, v) and the lower confidence bound
(LCB) Lt (u, v) respectively as
✓
◆
q
1
T
b
Ut (u, v) = Proj[0,1] h✓ u,t 1 , xv i + c xv ⌃u,t 1 xv
✓
◆
q
1
T
b
Lt (u, v) = Proj[0,1] h✓ u,t 1 , xv i c xv ⌃u,t 1 xv
(8)
Notice that Ut is the same as the UCB estimate p defined in Algorithm 1. Moreover, we define the “good event” F as
⇢
q
bu,t 1 ✓ ⇤ )|  c xT ⌃ 1 xv , 8u, v 2 V, 8t  T ,
F = |xTv (✓
(9)
u
v u,t 1
and the “bad event” F as the complement of F.

Model-Independent Online Learning for Influence Maximization

B.1. Regret Decomposition
Recall that the realized scaled regret at time t is Rt⇢↵ = F (S ⇤ )
Rt⇢↵ = F (S ⇤ )

1
(a) 1
e p⇤ )
F (St ) = f (S,
⇢↵
⇢

thus we have

1
⇢↵ F (St ),

(b) 1
1
e p⇤ )
F (St )  f (S,
⇢↵
⇢

1
f (St , p⇤ ),
⇢↵

(10)

e p⇤ )/F (S ⇤ )), and inequality (b) follows
where equality (a) follows from the definition of ⇢ (i.e. ⇢ is defined as ⇢ = f (S,
⇤
from f (St , p )  F (St ) (see Theorem 1). Thus, we have
" T
#
X ⇢↵
R⇢↵ (T ) = E
Rt
t=1

( T
)
i
Xh
1
⇤
⇤
e p ) f (St , p )/↵
 E
f (S,
⇢
t=1
( T
)
( T
i
Xh
Xh
P (F)
P (F)
⇤
⇤
e
e p⇤ )
=
E
f (S, p ) f (St , p )/↵ F +
E
f (S,
⇢
⇢
t=1
t=1
( T
)
h
i
X
1
e p⇤ ) f (St , p⇤ )/↵ F + P (F) nT,
 E
f (S,
⇢
⇢
t=1

e p⇤ )
where the last inequality follows from the naive bounds P (F)  1 and f (S,
“good” event F, we have
Lt (u, v)  p⇤uv = xTv ✓u⇤  Ut (u, v)

⇤

i

f (St , p )/↵ F

)
(11)

f (St , p⇤ )/↵  n. Notice that under
(12)

for all node pair (u, v) and for all time t  T . Thus, we have f (S, Lt )  f (S, p⇤ )  f (S, Ut ) for all S and t  T under
event F. So under event F, we have
(a)

(b)

(c)

(d)

e p⇤ )  f (S,
e Ut )  max f (S, Ut ) 
f (St , Lt )  f (St , p⇤ )  f (S,
S2C

1
f (St , Ut )
↵

for all t  T , where inequalities (a) and (c) follow from (12), inequality (b) follows from Se 2 arg maxS2C f (S, p⇤ ), and
inequality (d) follows from the fact that ORACLE is an ↵-approximation algorithm. Specifically, the fact that ORACLE is an
↵-approximation algorithm implies that f (St , Ut ) ↵ maxS2C f (S, Ut ).
Consequently, under event F, we have
e p⇤ )
f (S,

1
1
1
f (St , p⇤ )  f (St , Ut )
f (St , Lt )
↵
↵
↵

X
1
=
max Ut (u, v) max Lt (u, v)
u2St
u2St
↵
v2V
1X X

[Ut (u, v) Lt (u, v)]
↵
v2V u2St
X X 2c q

xTv ⌃u,t1 1 xv .
↵

(13)

v2V u2St

So we have
( T
X X Xq
2c
R (T ) 
E
xTv ⌃u,t1
⇢↵
t=1
⇢↵

1 xv

u2St v2V

In the remainder of this section, we will provide a worst-case bound on

F

)

PT

+

t=1

P (F)
nT.
⇢

P

u2St

pendix B.2) and a bound on the probability of “bad event” P (F) (see Appendix B.3).

P

q
xTv ⌃u,t1
v2V

(14)

1 xv

(see Ap-

Model-Independent Online Learning for Influence Maximization

PT

B.2. Worst-Case Bound on
Notice that

t=1

P

u2St

T X Xq
X

P

q
xTv ⌃u,t1
v2V

xTv ⌃u,t1

1 xv =

t=1 u2St v2V

For each u 2 V, we define Ku =
have the following lemma:

PT

t=1

1 xv

T
XX

u2V t=1

1 [u 2 St ]

t=1

xTv ⌃u,t1

1 xv

v2V

1 [u 2 St ] as the number of times at which u is chosen as a source node, then we

Lemma 1. For all u 2 V, we have
T
X

Xq

1 [u 2 St ]

Xq

xTv ⌃u,t1



1 xv

v2V

p

nKu

s

dn log 1 +
log 1 +

nKu
d 2
1

.

2

Moreover, when X = I, we have
T
X
t=1

Xq
1 [u 2 St ]
xTv ⌃u,t1

p
nKu
1 xv 

v2V

s

n log 1 +
log 1 +

Ku

2

1

.

2

q
Proof. To simplify the exposition, we use ⌃t to denote ⌃u,t , and define zt,v = xTv ⌃u,t1 1 xv for all t  T and all v 2 V.
Recall that
1 [u 2 St ]
1 [u 2 St ] X
⌃t = ⌃t 1 +
XX T = ⌃t 1 +
xv xTv .
2
2
v2V

Note that if u 2
/ St , ⌃t = ⌃t

1.

If u 2 St , then for any v 2 V, we have

1
det [⌃t ] det ⌃t 1 + 2 xv xTv

✓
◆
1
1
1
1
1
T
2
2
2
= det ⌃t 1 I + 2 ⌃t 1 xv xv ⌃t 1 ⌃t2

1
1
1
= det [⌃t 1 ] det I + 2 ⌃t 21 xv xTv ⌃t 21
= det [⌃t

1]

✓

1+

1

xT ⌃ 1 x
2 v t 1 v

Hence, we have
det [⌃t ]

n

det [⌃t

1]

n

◆

Y

1

= det [⌃t

1+

zt2

1]

1,v
2

v2V

!

1+

zt2

1,v
2

!

.

(15)

.

Note that the above inequality holds for any X. However, if X = I, then all ⌃t ’s are diagonal and we have
!
Y
zt2 1,v
det [⌃t ] = det [⌃t 1 ]
1+
.
2
v2V

As we will show later, this leads to a tighter regret bound in the tabular (X = I) case.
Let’s continue our analysis for general X. The above results imply that
n log (det [⌃t ])

n log (det [⌃t

1 ])

+ 1 (u 2 St )

X

v2V

log 1 +

zt2

1,v
2

!

(16)

Model-Independent Online Learning for Influence Maximization

and hence
n log (det [⌃T ])

n log (det [⌃0 ]) +

T
X
t=1

T
X

= nd log( ) +

t=1

1 (u 2 St )

1 (u 2 St )

X

X

zt2

log 1 +

v2V

log 1 +

zt2

1,v
2

v2V

1,v
2

!

!
(17)

.

On the other hand, we have that
"

Tr [⌃T ] = Tr ⌃0 +

T
X
1 [u 2 St ] X
2

t=1

= Tr [⌃0 ] +

v2V

T
X
1 [u 2 St ] X
2

t=1

= d+

v2V

T
X
1 [u 2 St ] X
2

t=1

v2V

xv xTv

#

⇥
⇤
Tr xv xTv

kxv k2  d +

nKu
2

(18)

,

where the last inequality follows from the assumption that kxv k  1 and the definition of Ku . From the trace-determinant
1
inequality, we have d1 Tr [⌃T ] det [⌃T ] d . Thus, we have
!
✓
◆
✓
◆
T
X
X
zt2 1,v
nKu
1
dn log
+
dn log
Tr [⌃T ]
n log (det [⌃T ]) dn log( ) +
1 (u 2 St )
log 1 +
.
2
d 2
d
t=1
v2V

That is

T
X
t=1

1 (u 2 St )

X

log 1 +

zt2

1,v
2

v2V

!

✓

nKu
 dn log 1 +
d 2

◆

2

Notice that zt2 1,v = xTv ⌃t 11 xv  xTv ⌃0 1 xv = kxv k  1 . Moreover, for all y 2 [0, 1/ ], we have log 1 +
log 1 + 1 2 y based on the concavity of log(·). Thus, we have
✓

log 1 +

1
2

◆X
T
t=1

1 (u 2 St )

X

zt2 1,v

v2V

✓

nKu
 dn log 1 +
d 2

◆

y
2

.

Finally, from Cauchy-Schwarz inequality, we have that
T
X
t=1

1 (u 2 St )

X

zt

1,v

v2V



Combining the above results, we have
T
X
t=1

1 (u 2 St )

X

v2V

zt

1,v

p

v
u T
X
uX
nKu t
1 (u 2 St )
z2

t 1,v .

t=1

p
 nKu

s

v2V

dn log 1 +
log 1 +

nKu
d 2
1

.

(19)

2

This concludes the proof for general X. Based on (16), the analysis for the tabular (X = I) case is similar, and we omit
the detailed analysis. In the tabular case, we have
s
T
X
X
p
n log 1 + Ku2
1 (u 2 St )
zt 1,v  nKu
.
(20)
log 1 + 1 2
t=1
v2V

Model-Independent Online Learning for Influence Maximization

We now develop a worst-case bound. Notice that for general X, we have
T
XX

u2V t=1

1 [u 2 St ]

Xq

xTv ⌃u,t1

v2V

Xp
nKu
1 xv 
u2V

s

s

dn log 1 +
log 1 +

nKu
d 2
1
2

d log 1 + dnT 2 X p
Ku
log 1 + 1 2 u2V
s
sX
(b)
d log 1 + dnT 2 p
n
n
Ku
log 1 + 1 2
u2V
s
dKT log 1 + dnT 2
(c) 3
=n2
,
log 1 + 1 2

(a)

n

(21)

where inequality (a) follows
P from the naive bound Ku  T , inequality (b) follows from Cauchy-Schwarz inequality, and
equality (c) follows from u2V Ku = KT . Similarly, for the special case with X = I, we have
s
s
T
Ku
XX
Xq
Xp
n
log
1
+
KT log 1 + T 2
3
2
1
1 [u 2 St ]
xTv ⌃u,t 1 xv 
nKu
 n2
.
(22)
1
log 1 + 2
log 1 + 1 2
u2V t=1
v2V
u2V
This concludes the derivation of a worst-case bound.
B.3. Bound on P F
We now derive a bound on P F based on the “Self-Normalized Bound for Matrix-Valued Martingales” developed in Theorem 3 (see Theorem 3). Before proceeding, we define Fu for all u 2 V as
⇢
q
bu,t 1 ✓ ⇤ )|  c xT ⌃ 1 xv , 8v 2 V, 8t  T ,
Fu = |xTv (✓
(23)
u
v u,t 1
and the F u as the complement of Fu . Note that by definition, F =
then we develop a bound on P F based on union bound.
Lemma 2. For all u 2 V, all ,

S

u2V

F u . Hence, we first develop a bound on P F u ,

> 0, all 2 (0, 1), and all
s
✓
◆
✓ ◆
p
1
nT
1
c
dn log 1 + 2
+ 2 log
+
k✓ ⇤u k2
d

we have P F u  .

Proof. To simplify the expositions, we omit the subscript u in this proof. For instance, we use ✓⇤ , ⌃t , yt and bt to
respectively denote ✓u⇤ , ⌃u,t , yu,t and bu,t . We also use Ht to denote the “history” by the end of time t, and hence
1
{Ht }t=0 is a filtration. Notice that Ut is Ht 1 -adaptive, and hence St and 1 [u 2 St ] are also Ht 1 -adaptive. We define
⇢
⇢
X if u 2 St
yt X T ✓⇤ if u 2 St
n
⌘t =
2<
and Xt =
2 <d⇥n
(24)
0 otherwise
0
otherwise
Note that Xt is Ht 1 -adaptive, and ⌘t is Ht -adaptive. Moreover, k⌘t k1  1 always holds, and E [⌘t |Ht 1 ] = 0.
To simplify the expositions, we further define yt = 0 for all t s.t. u 2
/ St . Note that with this definition, we have

Model-Independent Online Learning for Influence Maximization

XtT ✓⇤ for all t. We further define

⌘t = y t

V t =n

2

2

⌃t = n

I +n

t
X

Xs XsT

s=1

t
X

St =

Xs ⌘s =

s=1

bt =
Thus, we have ⌃t ✓

2

2

bt =

t
X
s=1

S t + [⌃t
bt
✓

⇥
Xs y s

⇤
XsT ✓⇤ = bt

2

I] ✓⇤

[⌃t

(25)

I] ✓ ⇤ , which implies
⇥ 2
⇤
✓ ⇤ = ⌃t 1
St
✓⇤ .

Consequently, for any v 2 V, we have
⇣
⌘
⇥ 2
b t ✓ ⇤ = xT ⌃ 1
xTv ✓
St
v t
q
h
 xTv ⌃t 1 xv k

✓⇤
2

⇤

S t k⌃

t

(26)

q
xTv ⌃t 1 xv k
i
+ k ✓ ⇤ k⌃ 1 ,

2


1

St

✓ ⇤ k⌃

1
t

(27)

t

where the first inequality follows from Cauchy-Schwarz inequality and the second inequality follows from triangular
p
1
inequality. Note that k ✓ ⇤ k⌃ 1 = k✓ ⇤ k⌃ 1  k✓ ⇤ k⌃ 1 =
k✓ ⇤ k2 . On the other hand, since ⌃t 1 = n 2 V t , we
have k

2

S t k⌃

1
t

=

p

t

n

t

kS t kV

1
t

xTv

0

. Thus, we have

⇣

bt
✓

✓

⇤

⌘

p
q
n
1
T
 x v ⌃t x v
kS t kV

1
t

+

p

k✓ ⇤ k2 .

From Theorem 3, we know with probability at least 1
kSt k2V
t
where V = n

1

 2 log

det V t

, for all t  T , we have
!
1/2
1/2
det (V )
det V T
 2 log

(28)

1/2

det (V )

1/2

!

,

I. Note that from the trace-determinant inequality, we have
⇥ ⇤
⇥ ⇤ d1
Tr V T
n 2 d + n2 T
det V T


,
d
d
⇥
⇤
⇥
⇤d
where the last inequality follows from Tr Xt XtT  n for all t. Note that det [V ] = n 2
, with a little bit algebra, we
have
s
✓
◆
✓ ◆
nT
1
kSt kV 1  d log 1 + 2
+ 2 log
8t  T
t
d
2

with probability at least 1

. Thus, if
s
✓
◆
✓ ◆
p
1
nT
1
c
dn log 1 + 2
+ 2 log
+
k✓ ⇤ k2 ,
d

then Fu holds with probability at least 1

. This concludes the proof of this lemma.

Hence, from the union bound, we have the following lemma:
Lemma 3. For all ,

> 0, all
c

2 (0, 1), and all
s
✓
◆
⇣n⌘ p
1
nT
dn log 1 + 2
+ 2 log
+
max k✓ ⇤u k2
u2V
d

(29)

Model-Independent Online Learning for Influence Maximization

we have P F  .
Proof. This lemma follows directly from the union bound. Note that for all c satisfying Equation 29, we have P F u 
S
P
for all u 2 V, which implies P F = P
u2V F u 
u2V P F u  .

n

B.4. Conclude the Proof
Note that if we choose

c
we have P F 

1
nT

1

s

✓
◆
p
nT
dn log 1 + 2
+ 2 log (n2 T ) +
max k✓ ⇤u k2 ,
u2V
d

(30)

. Hence for general X, we have
( T
)
X X Xq
2c
1
1
T
R (T ) 
E
xv ⌃u,t 1 xv F +
⇢↵
⇢
t=1 u2St v2V
s
2c 3 dKT log 1 + dnT 2
1

n2
+ .
1
⇢↵
⇢
log 1 + 2
⇢↵

(31)

q
⇣ p ⌘
p
e n2 d KT . SimiNote that with c = 1 dn log 1 + nT
+ 2 log (n2 T ) +
maxu2V k✓ ⇤u k2 , this regret bound is O
2 d
⇢↵
larly, for the special case X = I, we have
s
2c 3 KT log 1 + T 2
1
⇢↵
R (T ) 
n2
+ .
(32)
1
⇢↵
⇢
log 1 + 2
Note that with c =

n

q

log 1 + T2
✓ 5p ◆
e n 2 KT .
this regret bound is O
⇢↵

+ 2 log (n2 T ) +

p

maxu2V k✓ ⇤u k2 

n

q

log 1 +

T
2

+ 2 log (n2 T ) +

p

n,

C. Self-Normalized Bound for Matrix-Valued Martingales
In this section, we derive a “self-normalized bound” for matrix-valued Martingales. This result is a natural generalization
of Theorem 1 in Abbasi-Yadkori et al. (2011).
1

1

Theorem 3. (Self-Normalized Bound for Matrix-Valued Martingales) Let {Ht }t=0 be a filtration, and {⌘t }t=1 be a <K 1
valued Martingale difference sequence with respect to {Ht }t=0 . Specifically, for all t, ⌘t is Ht -measurable and satisfies (1)
1
E [⌘t |Ht 1 ] = 0 and (2) k⌘t k1  1 with probability 1 conditioning on Ht 1 . Let {Xt }t=1 be a <d⇥K -valued stochastic
d⇥d
process such that Xt is Ht 1 measurable. Assume that V 2 <
is a positive-definite matrix. For any t 0, define
Vt =V +K

t
X

Xs XsT

St =

s=1

Then, for any

> 0, with probability at least 1
kSt k2V
t

1

 2 log

t
X

(33)

Xs ⌘s .

s=1

, we have
det V t

1/2

det (V )

1/2

!

8t

0.

(34)

We first define some useful notations. Similarly as Abbasi-Yadkori et al. (2011), for any 2 <d and any t, we define Dt
as
✓
◆
K
T
T
2
Dt = exp
Xt ⌘t
kXt k2 ,
(35)
2
Qt
1
and Mt = s=1 Ds with convention M0 = 1. Note that both Dt and Mt are Ht -measurable, and Mt t=0 is a

Model-Independent Online Learning for Influence Maximization
1

supermartingale with respect to the filtration {Ht }t=0 . To see it, notice that conditioning on Ht 1 , we have
p
T
Xt ⌘t = (XtT )T ⌘t  kXtT k1 k⌘t k1  kXtT k1  KkXtT k2
p
with probability 1. This implies that T Xt ⌘t is conditionally KkXtT k2 -subGaussian. Thus, we have
✓
◆
✓
◆
⇥
⇤
⇥
⇤
K
K
K
T
T
2
T
2
T
2
E Dt Ht 1 = E exp
Xt ⌘t Ht 1 exp
kXt k2  exp
kXt k2
kXt k2 = 1.
2
2
2
Thus,

⇥
E Mt H t

1

1

⇤

= Mt

1E

⇥

D t Ht

1

1

⇤

 Mt

1.

So Mt t=0 is a supermartingale with respect to the filtration {Ht }t=0 . Then, following Lemma 8 of Abbasi-Yadkori
et al. (2011), we have the following lemma:
1

Lemma 4. Let ⌧ be
time with respect to the filtration {Ht }t=0 . Then for any
⇥ a stopping
⇤
well-defined and E M⌧  1.

2 <d , M⌧ is almost surely

Proof. First, we argue that M⌧ is almost surely well-defined. By Doob’s convergence theorem for nonnegative supermartingales, M1 = limt!1 ⇥Mt is
independent of ⌧ < 1
⇤ almost surely well-defined. Hence M⌧ is indeed well-defined
1
or not. Next, we show that E M⌧  1. Let Qt = Mmin{⌧,t} be a stopped version of Mt t=1 . By Fatou’s Lemma, we
⇥
⇤
⇥
⇤
⇥ ⇤
have E M⌧ = E lim inf t!1 Qt  lim inf t!1 E Qt  1.
The following results follow from Lemma 9 of Abbasi-Yadkori et al. (2011), which uses the “method of mixtures” technique. Let ⇤ be a Gaussian random vector in <d with mean 0 and covariance matrix V 1 , and independent of all the other
random variables. Let H1 be the tail -algebra
of the⇤ filtration, i.e. the -algebra generated by the union of all events
⇥
in the filtration. We further define Mt = E Mt⇤ H1 for all t = 0, 1, . . . and t = 1. Note that M1 is almost surely
well-defined since M1 is almost surely well-defined.
1

Let ⌧ be a stopping time with respect
⇥ to the
⇤ filtration {Ht }t=0 . Note that M⌧ is almost surely well-defined since M1 is
almost surely well-defined. Since E M⌧  1 from Lemma 4, we have
⇥
⇤
⇥ ⇥
⇤⇤
E [M⌧ ] = E M⌧⇤ = E E M⌧⇤ ⇤  1.
The following lemma follows directly from the proof for Lemma 9 of Abbasi-Yadkori et al. (2011), which can be derived
by algebra. The proof is omitted here.
Lemma 5. For all finite t = 0, 1, . . ., we have
Mt =

Note that Lemma 5 implies that for finite t,

✓

det(V )
det(V t )

kSt k2
Vt

Consequently, for any stopping time ⌧ , the event
(
⌧ < 1,

is equivalent to ⌧ < 1, M⌧ >

1

kS⌧ k2V 1
⌧

1

◆1/2

exp

> 2 log

> 2 log

✓

✓

1
kSt kV 1
t
2

det(V t )

det V ⌧

1/2

1/2

◆

(36)

.

det(V )

det (V )

1/2

1/2

◆

and Mt >

!)

. Finally, we prove Theorem 3:

Proof. We define the “bad event” at time t = 0, 1, . . . as:
(
Bt ( ) =

kSt k2V
t

1

> 2 log

det V t

1/2

det (V )

1/2

!)

.

1

are equivalent.

Model-Independent Online Learning for Influence Maximization

S1
We are interested in bounding the probability of the “bad event” t=1 Bt ( ). Let ⌦ denote the sample space, for any
outcome ! 2 ⌦, we define ⌧ (!)
= min{t
0 : ! 2 Bt ( )}, with the convention that min ; = +1. Thus, ⌧
S1
is a stopping time.
that t=1 B◆
t ( ) = {⌧ < 1}. Moreover, if ⌧ < 1, then by definition of ⌧ , we have
✓ Notice1/2
det(V ⌧ )
det(V ) 1/2
kS⌧ k2 1 > 2 log
, which is equivalent to M⌧ > 1 as discussed above. Thus we have
V⌧

P

1
[

t=1

Bt ( )

!

(a)

= P (⌧ < 1)

(b)

=P

kS⌧ kV2 1
⌧

det V ⌧

> 2 log

1/2

det (V )

1/2

!

,⌧ <1

!

(c)

= P (M⌧ > 1/ , ⌧ < 1)
 P (M⌧ > 1/ )

(d)

 ,

where equalities (a) and (b) follow from the definition of ⌧ , equality (c) follows from Lemma 5, and inequality (d) follows
from Markov’s inequality. This concludes the proof for Theorem 3.

We conclude this section by briefly discussing a special case. If for any t, the elements
independent
Pt of ⌘t are statistically P
t
conditioning on Ht 1 , then we can prove a variant of Theorem 3: with V t = V + s=1 Xs XsT and St = s=1 Xs ⌘s ,
Equation 34 holds with probability at least 1
. To see it, notice that in this case
"K
#
Y
⇥
⇤
T
T
E exp
Xt ⌘t Ht 1 = E
exp (Xt )(k)⌘t (k) Ht 1
k=1

(a)

=

K
Y

k=1
(b)



K
Y

⇥
E exp (XtT )(k)⌘t (k) Ht
exp

k=1

✓

(XtT )(k)2
2

◆

= exp

1

⇤

XtT
2

2

!

,

(37)

where (k) denote the k-th element of the vector. Note that the equality (a) follows from the conditional independence of the elements in ⌘t , and inequality (b) follows from |⌘t (k)|  1 for all t and k. Thus, if we redefine
Qt
Dt = exp T Xt ⌘t 12 kXtT k22 , and Mt = s=1 Ds , we can prove that {Mt }t is a supermartingale. Consequently,
using similar analysis techniques, we can prove the variant of Theorem 3 discussed in this paragraph.

D. Laplacian Regularization
As explained in section 7, enforcing Laplacian regularization leads to the following optimization problem:
t X
X
bt = arg min[
✓
(yu,j
✓
j=1 u2St

✓ u X)2 +

2

X

(u1 ,u2 )2E

||✓ u1

✓ u2 ||22 ]

Here, the first term is the data fitting term, whereas the second term is the Laplacian regularization terms which enforces
smoothness in the source node estimates. This can optimization problem can be re-written as follows:
bt = arg min
✓
✓

X
t X

j=1 u2St

(yu,j

✓ u X)2 +

2✓

T

(L ⌦ Id )✓

Model-Independent Online Learning for Influence Maximization

Here, ✓ 2 <dn is the concatenation of the n d-dimensional ✓ u vectors and A ⌦ B refers to the Kronecker product of
matrices A and B. Setting the gradient of equation 38 to zero results in solving the following linear system:
[XX T ⌦ In +

2L

b t = bt
⌦ Id ]✓

(38)

Here bt corresponds to the concatenation of the n d-dimensional vectors bu,t . This is the Sylvester equation and there exist
sophisticated methods of solving it. For simplicity, we focus on the special case when the features are derived from the
Laplacian eigenvectors (Section 7).
Let t be a diagonal matrix such that t u, u refers to the number of times node u has been selected as the source. Since the
Laplacian eigenvectors are orthogonal, when using Laplacian features, XX T ⌦ In = ⌦ Id . We thus obtain the following
system:
[( +
Note that the matrix ( + 2 L) and thus ( +
gradient (Hestenes & Stiefel, 1952).

2 L)

2 L)

b t = bt
⌦ Id ] ✓

(39)

⌦ Id is positive semi-definite and can be solved using conjugate

For conjugate gradient, the most expensive operation is the matrix-vector multiplication ( + 2 L) ⌦ Id ]v for an arbitrary
vector v. Let vec be an operation that takes a d ⇥ n matrix and stacks it column-wise converting it into a dn-dimensional
vector. Let V refer to the d ⇥ n matrix obtained by partitioning the vector v into columns of V . Given this notation, we
use the property that (B T ⌦ A)v = vec(AV B). This implies that the matrix-vector multiplication can then be rewritten
as follows:
( +

2 L)

⌦ Id v = vec(V

+

2L

T

(40)

)

Since is a diagonal matrix, V is an O(dn) operation, whereas V LT is an O(dm) operation since there are only m
bt is an order
non-zeros (corresponding to edges) in the Laplacian matrix. Hence the complexity of computing the mean ✓
O((d(m + n))) where  is the number of conjugate gradient iterations. In our experiments, similar to (Vaswani et al.,
2017), we warm-start with the solution at the previous round and find that  = 5 is enough for convergence.
Unlike independent estimation where we update the UCB estimates for only the selected nodes, when using Laplacian
regularization, the upper confidence values for each reachability probability need to be recomputed in each round. Once
we have an estimate of ✓, calculating the mean estimates for the reachabilities for all u, v requires O(dn2 ) computation.
This is the most expensive step when using Laplacian regularization.
We now describe how to compute the confidence intervals. For this, let D denote the diagonal of ( +
value zu,v,t can then be computed as:
p
zu,v,t = D u ||xv ||2

2 L)

1

. The UCB
(41)

The `2 norms for all the target nodes v can be pre-computed. If we maintain the D vector, the confidence intervals for all
pairs can be computed in O(n2 ) time.
Note that D t requires O(n) storage and can be updated across rounds in O(K) time using the Sherman Morrison formula.
Specifically, if D u,t refers to the uth element in the vector D t , then
8
< D u,t , ifu 2 St
D u,t+1 = (1 + D u,t )
:
D u,t ,
otherwise
Hence, the total complexity of implementing Laplacian regularization is O(dn2 ). We need to store the ✓ vector, the
Laplacian and the diagonal vectors and D. Hence, the total memory requirement is O(dn + m).

