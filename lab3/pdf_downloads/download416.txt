Discovering Discrete Latent Topics with Neural Variational Inference

A. Discovered Topics

B. Topic Diversity

Table 1 presents the topics by the words with highest probability (top-10 words) achieved by different neural topic
models on 20NewsGroups dataset.

An issue that exists in both probabilistic and neural topic
models is redundant topics. In neural models it straightforward regularises the distance between each of the topic
vectors in order to diversify the topics. Following Xie et al.
(2015), we apply such topic diversity regularisation while
carrying out neural variational inference. We use the cosine distance to measure the distance between two topics
|ti ·βj |
). The mean angle of all pairs
a(ti , tj ) = arccos( ||ti ||·||t
Pj ||P
of K topics is ζ = K12 i j a(ti , tj ), and the variance
P P
is ν = K12 i j (a(ti , tj ) − m)2 . We add the following
topic diversity regularisation to the variational objective:

Space
space
satellite
april
sequence
launch
president
station
radar
training
committee

Religion
god
atheism
exist
atheist
moral
existence
marriage
system
parent
murder

Encryption
Sport
Science
encryption
player
science
device
hall
theory
technology defensive scientific
protect
team
universe
americans average experiment
chip
career observation
use
league
evidence
privacy
play
exist
industry
bob
god
enforcement
year
mistake

(a) Topics learned by GSM.
Space
Religion Lawsuit
Vehicle
Science
moon
atheist
homicide
bike
theory
lunar
life
gun
motorcycle science
orbit
eternal
rate
dod
gary
spacecraft christianity handgun insurance scientific
hell
crime
bmw
sun
billion
launch
god
firearm
ride
orbit
space
christian weapon
dealer
energy
hockey
atheism
knife
oo
experiment
religion
study
car
mechanism
cost
nasa
brian
death
buy
star
(b) Topics learned by GSB.
Aerospace
instruction
spacecraft
amp
pat
wing
plane
algorithm
db
reduce
orbit

Crime Hardware Technology Science
gun
drive
technology
science
weapon
scsi
americans
hell
crime
ide
pit
scientific
firearm
scsus
encryption evidence
criminal
hd
policy
physical
use
go
industry
eternal
control controller
protect
universe
handgun
tape
privacy experiment
law
datum
product
reason
kill
isa
approach
death
(c) Topics learned by RSB.

Table 1. Topics learned by neural topic models on 20NewsGroups
dataset.

J = L + λ(ζ − ν),
where λ is a hyper-parameter for the regularisation that is
empirically set as 0.1. Though in practise diversity regularisation does not provide a significant improvement to
perplexity (2∼5 in most cases), it helps reduce topic redundancy and can be easily applied on topic vectors instead of
the simplex over the full vocabulary.

