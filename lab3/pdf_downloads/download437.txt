McGan: Mean and Covariance Feature Matching GAN

Supplementary Material for McGan: Mean and Covariance Feature Matching GAN
Youssef Mroueh * 1 2 Tom Sercu * 1 2 Vaibhava Goel 2

A. Subspace Matching Interpretation of Covariance Matching GAN
Let ∆ω = Σω (P) − Σω (Q). ∆ω is a symmetric matrix but not PSD, which has the property that its eigenvalues λj are
related to its singular values as given by: σj = |λj | and its left and right singular vectors coincides with its eigenvectors
and satisfy the following equality uj = sign(λj )vj . One can ask here if we can avoid having both U, V in the definition
of IPMΣ since at the optimum uj = ±vj . One could consider δEω (Pr , Pθ ) defined as follows:
2
max
E kU Φω (x)k
ω∈Ω,U ∈Om,k x∼Pr
Energy in the subspace
of real data

2

− E kU Φω (gθ (z))k ,
z∼pz
Energy in the subspace
of fake data

and then solve for mingθ δEω (Pr , Pθ ). Note that:
δEω (Pr , Pθ ) =

max

ω∈Ω,U ∈Om,k

= max
ω∈Ω

k
X

T race(U > (Σω (Pr ) − Σω (Pθ ))U )

λi (∆ω )

i=1

δEω is not symmetric furthermore the sum of those eigenvalues is not guaranteed to be positive and hence δEω is not
guaranteed to be non negative, and hence does not define an IPM. Noting that σi (∆ω ) = |λi (∆ω )|,we have that:
IPMΣ (Pr , Pθ ) =

k
X
i=1

σi (∆ω ) ≥

k
X

λi (∆ω ) = δEω (Pr , Pθ ).

i=1

Hence δE is not an IPM but can be optimized as a lower bound of the IPMΣ . This would have an energy interpretation
as in the energy based GAN introduced recently (Zhao et al., 2017): the discriminator defines a subspace that has higher
energy on real data than fake data, and the generator maximizes his energy in this subspace.

B. Mean and Covariance Matching Loss Combinations
We report below samples for McGan, with different IPMµ,q and IPMΣ combinations. All results are reported for the same
architecture choice for generator and discriminator, which produced qualitatively good samples with IPMΣ (Same one
reported in Section 6 in the main paper). Note that in Figure 7 with the same hyper-parameters and architecture choice,
WGAN failed to produce good sample. In other configurations training converged.

McGan: Mean and Covariance Feature Matching GAN

Figure 7. Cifar-10: Class-conditioned generated samples with IPMµ,1 (WGAN). Within each column, the random noise z is shared,
while within the rows the GAN is conditioned on the same class: from top to bottom airplane, automobile, bird, cat, deer, dog, frog,
horse, ship, truck.

McGan: Mean and Covariance Feature Matching GAN

Figure 8. Cifar-10: Class-conditioned generated samples with IPMµ,2 . Within each column, the random noise z is shared, while within
the rows the GAN is conditioned on the same class: from top to bottom airplane, automobile, bird, cat, deer, dog, frog, horse, ship,
truck.

McGan: Mean and Covariance Feature Matching GAN

Figure 9. Cifar-10: Class-conditioned generated samples with IPMΣ . Within each column, the random noise z is shared, while within
the rows the GAN is conditioned on the same class: from top to bottom airplane, automobile, bird, cat, deer, dog, frog, horse, ship,
truck.

McGan: Mean and Covariance Feature Matching GAN

Figure 10. Cifar-10: Class-conditioned generated samples with IPMµ,1 + IPMΣ . Within each column, the random noise z is shared,
while within the rows the GAN is conditioned on the same class: from top to bottom airplane, automobile, bird, cat, deer, dog, frog,
horse, ship, truck.

McGan: Mean and Covariance Feature Matching GAN

Figure 11. Cifar-10: Class-conditioned generated samples with IPMµ,2 + IPMΣ . Within each column, the random noise z is shared,
while within the rows the GAN is conditioned on the same class: from top to bottom airplane, automobile, bird, cat, deer, dog, frog,
horse, ship, truck.

