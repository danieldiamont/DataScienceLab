Follow the Moving Leader in Deep Learning

A. proofs
A.1. proof of Lemma 1
Proof. For any t and i  t,
lim wi,t

=

1 !1

1

1 !1

1
lim
1 !1 1
1
lim
!1
1
1

=
=
=

lim

=

1
.
t

t i
1) 1
t
1

(1

lim

lim

1 !1

t i
1

1
t
1

1
t 1
1

t

1 !1

1
t
1

here, the second equality holds by the limit properties. The last second equality holds by L’Hôpital’s rule.
A.2. proof of Proposition 1
Proof. from (11):
t
X

t 1
X

wi,t Qi

i=1

wi,t

1 Qi

t 1
X

=

i=1

(wi,t

wi,t

1 )Qi

+ wt,t Qt

i=1

t 1✓
X
(1

=

1

i=1

t 1✓
X

=

1

1

=

1

t 1 i◆
1) 1
Qi
t 1
1

◆
1 wi,t

1 Qi

+ wt,t Qt

+ wt,t Qt

t 1

1X
t
1 i=1

1

(1

t 1
1 )
t
1

1 (1

i=1

=

t i
1) 1
t
1

wt,t

t 1
X

wi,t

wi,t

1 Qi

1 Qi

+ wt,t Qt

+ wt,t Qt .

i=1

Rearranging, we obtain
wt,t Qt

=

t
X

wi,t Qi

i=1

=
=
Thus, Qt = diag

⇣

dt

1 dt

1

1

1

A.3. proof of Proposition 2

⌘

diag
diag

✓
✓

(1

dt
t
1

1
dt

wt,t )

wi,t

i=1

◆

(1
◆

1 dt

1

t
1

1

t 1
X

wt,t )diag

1 Qi

✓

dt
1

1
t 1
1

◆

.

.

Proof. (13) can be rewritten as (apart from a constant)
* t
✓
X
i
min
wi,t gi
✓2⇥
1
i=1

1

✓i

1

◆

,✓

+

1
+ k✓k2 ✓ d ◆ .
t
2
diag
t
1
1

(17)

Follow the Moving Leader in Deep Learning

Let zt = (1

t
1)

⇣

Pt

i=1 wi,t gi

1

✓i
1

zt

=

i

1

⌘

. By (10), we have a simple recursive update rule:

1 zt

=

✓
)
gt
1

1 + (1

1 zt 1

+ (1

t

1
t ✓t

1 )gt

1

✓t

1

1.

◆

Substituting zt into (17), we have
⌧

min
✓2⇥

zt

1
+ k✓k2 ✓ d ◆ .
t
2
diag
t
1

t,✓
1

1

1

Rearranging, we obtain
1
min k✓ + zt /dt k2 ✓ d ◆ ,
t
✓2⇥ 2
diag
t
1
1

t
1)

diag(dt /(1

with optimal solution ⇧⇥

)

( zt /dt ).

A.4. proof of Proposition 3
Proof. When

1

= 0, we have wt,t = 1 and wi,t = 0 for all i < t. Thus,
1
min hgt , ✓i + k✓
✓2⇥
2

✓t

2 ✓ ✓
q
1k
diag ⌘1
t

= dt , and (13) reduces to:

t

vt
1

t
2

+✏t 1

◆◆ ,

We can rewrite above as
0

1
min
✓
✓2⇥ 2

@✓t

q

1

⌘t
vt

t
2

1

+ ✏t 1

1

2

gt A

,
diag

✓

1
⌘t

✓

q

vt
1

+✏t 1

t
2

◆◆

with optimal solution
t
1 ))

diag(d /(1
⇧⇥ t

analogous to (10) and Lemma 1,
lim

2 !1

vt
1

t
2

0

@✓t

0

diag @ q

1

= lim

2 !1

p
p
Combining with ⌘t = ⌘/ t and ✏t = ✏/ t, we obtain
lim q
2 !1

and (18) reduces to below
diag((
⇧⇥

p

t
X
(1
i=1

⌘t
vt
1

2 +✏1)/⌘)
g1:t

t
2

✓t

+ ✏t 1

1

1

vt
1

t
2

+ ✏t 1

t i
2) 2
gi2
t
2

=p

diag

⌘t

⌘
2
g1:t

p

+ ✏1

1 1

A gt A .

(18)

t

=

1X 2
g .
t i=1 i

,

⌘
2 + ✏1
g1:t

! !
gt

,

(19)

Follow the Moving Leader in Deep Learning

A.5. proof of Proposition 4
Proof. When

1

! 1, we have
lim

t

1 !1

=

1

lim



dt

1
2
1
lim 4
1
1 !1
q
1 !1

1

=

vt

=

t
2

1

t

1 dt 1
1

q

t
1
1

1

1
vt

+ ✏t 1

t
2

1

1 (1

⌘t

1

+ ✏t 1
(t

⌘t

t 1
1 )

1)

q

vt
1

1
t 1
2

1

+ ✏t

q

vt
1

1
t 1
2

⌘t

+ ✏t

11

1

11

3
5

.

⌘t

1

Substituting this into (13), we obtain
min
✓2⇥

where mi =

t
⌘t

✓q

vt
1

t
2

+ ✏t 1

◆

t ✓
X
i=1

t 1
⌘t 1

✓q

1
hgi , ✓i + k✓
2
vt

1
t 1
2

1

+ ✏t

◆
2
k
1 diag(mi ) ,

✓i

11

◆

(20)

.

p
p
Combining with ⌘t = ⌘ t, ✏t = ✏/ t, and (19), we further obtain
q
q v
vt
t 1
t + ✏t 1
t 1 + ✏t
1
1
2
2
lim mi = lim t
(t 1)
⌘t
⌘t 1
2 !1
2 !1
q
p
2
2
g1:t
g1:t 1
=
.
⌘

11

q

p
2 + ✏1
g1:t
=
⌘

2
g1:t

1

+ ✏1

⌘

Substituting back into (20), we recover
✓ FTRL with
◆ adaptive learning rate. by using the equivalence theorem in (McMahan,
2011), we obtain ✓t

✓t

diag

1

p

⌘
2 +✏1
g1:t

gt .

A.6. proof of Theorem 1
Proof. Note that wi,t = 1
in proposition 2
* t
✓
X
wi,t gi

=

1

1

=

hzt

1

=
=

t
1

1

t
1

1
1
1

wi,t

✓i

*t 1
X

1.

1

with ⇥ = Rd , consider the first term in the objective of (17): with zt defined

◆

,✓

wi,t

+

1

i=1

✓

i

gi

1

⌧
✓
,
✓i
+
w
1
t,t gt

hdt

1 ✓t 1 , ✓i

+

1
1

t
1

h(1

1 )gt

✓t

t
1

h(1

1 )gt

dt ✓t

1
1

)

1

t 1
1 )
t
1

1 (1

1

1

i

i=1

=

t
1
t
1

(1
1

t

1
⌧

1
t
1

1( t

1

+

1

gt

✓i

✓t

1

1

◆

◆

⌧
✓
+ wt,t gt

t

1

1

✓t

1

◆

,✓

,✓

t

1

,✓

+

1

✓t

1, ✓

1 dt 1 ), ✓i

1 , ✓i ,

where the second equality follows from the definition of zt . The third equality holds since ⇥ = Rd and therefore ✓t =
zt /dt by Proposition 2. Thus, combing this expression into (17), we obtain
min

✓2Rd

1
1

t
1

h(1

1 )gt

dt ✓t

1 , ✓i

1
+ k✓k2diag( dt ) ,
t
2
1
1

Follow the Moving Leader in Deep Learning

With the definition of dt , it can be seen that solving above problem (taking gradient w.r.t. ✓ and setting it to zero) leads to
a gradient descent style update rule:
0
1
✓t

which concludes the proof.

✓t

1

B1
diag B
@1

1
t
1

✓q

⌘t

vt
(1

t
2)

C
◆C
A gt .
+ ✏t 1

A.7. proof of Proposition 5
Proof. Note that (15) can be rewritten as
* t
+
!
X
1
2
⇣
⌘
min
wi,t gi , ✓ + k✓ ✓t 1 kPt
i
✓2⇥
2
i=1 wi,t diag 1
1
i=1
0*
1
+
t
X
1
2
= min @
wi,t gi , ✓ + k✓ ✓t 1 k ✓ d ◆ A .
t
✓2⇥
2
diag
t
1
i=1
1

Thus, with the definition of dt , solving above problem, we obtain (16).

