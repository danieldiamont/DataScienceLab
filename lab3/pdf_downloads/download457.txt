Conditional Image Synthesis with Auxiliary Classifier GANs: Appendix

Augustus Odena 1 Christopher Olah 1 Jonathon Shlens 1

1. Hyperparameters
We summarize hyperparameters used for the ImageNet
model in Table 1 and for the CIFAR-10 model in Table 2.

Conditional Image Synthesis with Auxiliary Classifier GANs: Appendix

Operation
Gx (z) – 110 × 1 × 1 input
Linear
Transposed Convolution
Transposed Convolution
Transposed Convolution
Transposed Convolution
D(x) – 128 × 128 × 3 input
Convolution
Convolution
Convolution
Convolution
Convolution
Convolution
Linear
Optimizer
Batch size
Iterations
Leaky ReLU slope
Weight, bias initialization

Kernel

Strides

Feature maps

BN?

Dropout

Nonlinearity

N/A
5×5
5×5
5×5
5×5

N/A
2×2
2×2
2×2
2×2

768
384
256
192
3

×
√
√
√

0.0
0.0
0.0
0.0
0.0

ReLU
ReLU
ReLU
ReLU
Tanh

×

3×3
2×2
16
×
0.5
√
3×3
1×1
32
0.5
√
3×3
2×2
64
0.5
√
3×3
1×1
128
0.5
√
3×3
2×2
256
0.5
√
3×3
1×1
512
0.5
N/A
N/A
11
×
0.0
Adam (α = 0.0002, β1 = 0.5, β2 = 0.999)
100
50000
0.2
Isotropic gaussian (µ = 0, σ = 0.02), Constant(0)

Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Leaky ReLU
Soft-Sigmoid

Table 1. ImageNet hyperparameters. A Soft-Sigmoid refers to an operation over K +1 output units where we apply a Softmax activation
to K of the units and a Sigmoid activation to the remaining unit. We also use activation noise in the discriminator as suggested in (?).

Conditional Image Synthesis with Auxiliary Classifier GANs: Appendix

Operation
Gx (z) – 110 × 1 × 1 input
Linear
Transposed Convolution
Transposed Convolution
Transposed Convolution
D(x) – 32 × 32 × 3 input
Convolution
Convolution
Convolution
Convolution
Convolution
Convolution
Linear
Generator Optimizer
Discriminator Optimizer
Batch size
Iterations
Leaky ReLU slope
Activation noise standard deviation
Weight, bias initialization

Kernel

Strides

Feature maps

BN?

Dropout

Nonlinearity

N/A
5×5
5×5
5×5

N/A
2×2
2×2
2×2

384
192
96
3

×
√
√

0.0
0.0
0.0
0.0

ReLU
ReLU
ReLU
Tanh

×

3×3
2×2
16
×
0.5
Leaky ReLU
√
3×3
1×1
32
0.5
Leaky ReLU
√
3×3
2×2
64
0.5
Leaky ReLU
√
3×3
1×1
128
0.5
Leaky ReLU
√
3×3
2×2
256
0.5
Leaky ReLU
√
3×3
1×1
512
0.5
Leaky ReLU
N/A
N/A
11
×
0.0
Soft-Sigmoid
Adam (α = [0.0001, 0.0002, 0.0003], β1 = 0.5, β2 = 0.999)
Adam (α = [0.0001, 0.0002, 0.0003], β1 = 0.5, β2 = 0.999)
100
50000
0.2
[0, 0.1, 0.2]
Isotropic gaussian (µ = 0, σ = 0.02), Constant(0)

Table 2. CIFAR-10 hyperparameters. When a list is given for a hyperparameter it means that we performed a grid search using the
values in the list. For each set of hyperparameters, a single AC-GAN was trained on the whole CIFAR-10 dataset. For each AC-GAN
that was trained, we split up the samples into groups so that we could give some sense of the variance of the Inception Score. To the best
of our knowledge, this is identical to the analysis performed in (?).

