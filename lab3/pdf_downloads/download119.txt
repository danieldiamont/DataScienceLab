“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Supplementary material
A. Proofs from Section 2
A.1. Proof of Proposition 1
Proposition 1. Let f be L-smooth, and let y0t and xt0 be the sequence of iterates generated by AGD- UNTIL - GUILTY(f ,
y0 , L, ", ) for some " > 0 and 0 <  L. Fix w 2 Rd . If for s = 0, 1, . . . , t 1 we have
f (xs ) + rf (xs )T (u

f (u)

xs ) +

2

ku

xs k2

(5)

for both u = w and u = ys , then
f (w) 

f (yt )
where  =

L

and (w) = f (y0 )

f (w) +

2

✓

1
p


1

◆t

(6)

(w),

2

y0 k .

kw

Proof. The proof is closely based on the proof of Theorem 3.18 of (Bubeck, 2014), which itself is based on the estimate
sequence technique of Nesterov (2004). We modify the proof slightly to avoid arguments that depend on the global
minimum of f . This enables using inequalities (5) to prove the result, instead of -strong convexity of the function f .
We define -strongly convex quadratic functions

s

0 (z)

and, for s = 0, ..., t

by induction as

= f (x0 ) +

2

kz

x 0 k2 ,

1,
s+1 (z)

=

✓

1
p


1

◆

s (z)

1 ⇣
+ p f (xs ) + rf (xs )T (z


xs ) +

2

kz

Using (5) with u = w, straightforward induction shows that
✓
◆s
1
p
(w) for s = 0, 1, ..., t.
s (w)  f (w) + 1

Let

⇤
s

= minx2Rn

s (x).

⌘
xs k2 .

(20)

(21)

If
f (ys ) 

⇤
s

for s = 0, 1, ..., t

(22)

then (6) follows immediately, since
f (yt )

f (w) 

⇤
t

f (w) 

t (w)

f (w) 

✓

1

1
p


◆t

(w)

We now prove (22) by induction. Note that it is true at s = 0 since x0 = y0 is the global minimizer of

0.

We have,

(a)
1
f (ys+1 )  f (xs )
krf (xs )k2
2L
✓
◆
✓
◆
1
1
1
1
= 1 p
f (ys ) + 1 p
(f (xs ) f (ys )) + p f (xs )
krf (xs )k2
2L



✓
◆
✓
◆
(b)
1
1
1
1
⇤
p
 1 p
(f (xs ) f (ys )) + p f (xs )
krf (xs )k2
s + 1
2L



✓
◆
✓
◆
(c)
1
1
1
1
⇤
p
 1 p
rf (xs )T (xs ys ) + p f (xs )
krf (xs )k2 ,
s + 1
2L




“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

where inequality (a) follows from the definition ys+1 = xs L1 rf (xs ) and the L-smoothness of f , inequality (b) is the
induction hypothesis and inequality (c) is assumption (5) with u = ys .
Past this point the proof is identical to the proof of Theorem 3.18 of (Bubeck, 2014), but we continue for sake of completeness.
To complete the induction argument we now need to show that:
✓
◆
✓
◆
1
1
1
⇤
p
1 p
rf (xs )T (xs ys ) + p f (xs )
s + 1



Note that r2

s

1
krf (xs )k2 
2L

(23)

= In (immediate by induction) and therefore
s (x)

⇤
s

=

+

2

v s k2 ,

kx

for some vs 2 Rn . By differentiating (20) and using the above form of s we obtain
✓
◆
1
1
r s+1 (x) =
1 p
(x vs ) + p rf (xs ) + p (x



Since by definition

s+1 (vs+1 )

= 0, we have
✓
vs+1 = 1

Using (20), evaluating evaluating
s+1 (xs )

=

s+1

⇤
s+1

+

1
p


◆

1
v s + p xs


2

vs+1 k =

✓

1

1
p


which combined with (25) yields
✓
◆
1
⇤
p
=
1
s+1


2

◆2

⇤
s

xs ).

1
p rf (xs )


(24)

at xs gives,
2

✓

1
p


◆h

kxs

vs+1 k =

kxs

1
vs k + 2 krf (xs )k2


1

Substituting (24) gives
kxs

⇤
s+1 .

1
+p


2

✓

1

1
p


◆

rf (xs )T (vs

⇤
s

2

kxs

2
p

✓

+



1

i
1
vs k2 + p f (xs ).

1
p


◆

rf (xs )T (vs

(25)

xs )

1
1
xs ) + p f (xs )
krf (xs )k2
2L

✓
◆
1
+ p
1 p
kxs vs k2 .
2 


p
Examining this equation, it is seen that vs xs p
= (xs ys ) implies (23) and therefore concludes the proof of
Proposition 1. We establish the relation vs xs = (xs ys ) by induction,
✓
◆
1
1
1
p rf (xs ) xs+1
vs+1 xs+1 = 1 p
v s + p xs



p
p
p

= xs (  1)ys
rf (xs ) xs+1
L
p
p
p
= ys+1 (  1)ys xs+1 = (xs+1 ys+1 ).
where the first equality comes from (24), the second from the induction hypothesis, the third from the definition of ys+1
and the last one from the definition of xs+1 .
A.2. Proof of Lemma 1
Lemma 1. Let f : Rd ! R have L2 -Lipschitz Hessian. Let ↵ > 0 and let u and v satisfy (10). If ku
every ⌘  L↵2 , E XPLOIT-NC- PAIR(f, u, v, ⌘) finds a point z such that
f (z)  f (u)

↵⌘ 2
.
12

vk 

↵
2L2 ,

then for

(11)

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Proof. We proceed in two parts; in the first part, we show that f has negative curvature of at least ↵/2 in the direction
of u v at the point u. In the second part we show that such negative curvature guarantees that a step with magnitude ⌘
produces the required progress in function value.
For 0  ✓  ku

vk, let
(✓) = v + ✓ , where

=

Then,
Z

ku vk
0

d⌧

Z

⌧
0

⇥

T

r2 f ( (✓))

⇤

d✓ = f (u)

u
ku

v
.
vk

rf (v)T (u

f (v)

v) <

↵
ku
2

2

vk ,

where the equality follows from basic calculus, and the inequality is assumption (10).
min0✓ku vk T r2 f ( (✓)) for the integrand, we find that  < ↵.
By Lipschitz continuity of r2 f and ku
T

Substituting 

=

vk  ↵/(2L2 ) we thus have,

r2 f (u)   + L2 ku

↵ + L2 ↵/(2L2 ) 

vk <

↵
,
2

(26)

which concludes the first part of the proof.
The Lipschitz continuity of r2 f also implies that it is upper bounded by its quadratic approximation with a cubic residual
term, i.e.
f (y)  f (x) + rf (x)T (y

1
x) + (y
2

x)T r2 f (x)(y

x) +

L2
ky
6

3

xk

for any y, x 2 Rd . Applying this to u± = u ± ⌘ , we have
f (u± )  f (u) ± ⌘rf (u)T +

⌘2
2

T

r2 f (u) +

L2 3
|⌘| .
6

We note that the first order term must be negative for either u+ or u . Therefore, using (26) and ⌘  ↵/L2 , we have that
f (z) = min{f (u+ ), f (u )}  f (u)

↵⌘ 2
.
12

B. Proofs from Section 3
B.1. Proof of Lemma 2
Lemma 2. Let f : Rd ! R be L1 -smooth and have L2 -Lipschitz continuous Hessian, let ✏, ↵ > 0 and p0 2 Rd . Let
p1 , . . . , pK be the iterates G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵, L↵2 ) generates. Then for each k 2 {1, . . . , K
1},
⇢ 2
✏
↵3
f (pk )  f (pk 1 ) min
,
.
(12)
5↵ 64L22
Proof. Fix an iterate index 1  k < K; throughout the proof we let y0t , xt0 and u, v refer to outputs of AGD- UNTIL GUILTY in the kth iteration. We consider the cases u, v = NULL and u, v 6= NULL separately.

The simpler case is u, v = NULL (no convexity violation detected), in which pk = yt and krfˆ(pk )k  ✏/10 (since
AGD- UNTIL - GUILTY terminated on line 9). Moreover, k < K implies that G UARDED - NON - CONVEX -AGD does not
terminate at iteration k, and therefore krf (pk )k > ✏. Consequently,
2↵ kpk

pk

1k

= krf (pk )

rfˆ(pk )k

krf (pk )k

krfˆ(pk )k

9✏/10.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

The case u, v = NULL also implies fˆ(pk ) = fˆ(yt )  fˆ(y0 ) = f (pk
never holds, and therefore
f (pk ) = fˆ(pk )

↵ kpk

2

1k

pk

 f (pk

1)

1 ),

↵

as the condition in line 2 of C ERTIFY- PROGRESS

✓

9✏
20↵

◆2

 f (pk

1)

✏2
,
5↵

which establishes the claim in the case u, v = NULL.
Next we consider the case u, v 6= NULL (non-convexity detected). By Corollary 1,
fˆ(u) < fˆ(v) + rfˆ(v)T (u
By definition of fˆ(x) = f (x) +
f (v) + rf (v)T (u

↵
2

kx

vk

2

(27)

2

y0 k , we have that,

↵
ku
2

v)

↵
ku
2

v) +

vk

2

f (u) = fˆ(v) + rfˆ(v)T (u

v) +

↵
ku
2

for every u, v 2 Rd . Therefore, we conclude that (10) must hold.
We now set ⌧ :=
f (pk )  f (b

(1)

↵
8L2

and consider two cases. First, if f (b(1) )  f (y0 )

). Second, if f (b

(1)

)

1)

↵⌧ , by Lemma 3 we have that
2

f (y0 )

kv
Therefore, we can use Lemma 1 (with ⌘ =

↵⌧ 2 = f (pk

↵
L2 )

uk  4⌧ 

vk

↵3
64L22

2

fˆ(u) > 0

then we are done, since

↵
.
2L2

to show that

f (b(2) )  f (u)

↵3
 f (pk
12L22

where the last transition uses again f (u)  fˆ(u)  fˆ(y0 ) = f (pk
concludes the case u, v 6= NULL.

↵3
,
12L22

1)

1 ),

due to Corollary 1. This implies (12) holds and

B.2. Proof of Lemma 3
Lemma 3. Let f be L1 -smooth, and ⌧ 0. At any iteration of G UARDED - NON - CONVEX -AGD, if u, v 6= NULL and the
best iterate b(1) satisfies f (b(1) ) f (y0 ) ↵⌧ 2 then for 1  i < t,
kyi
Consequently, ku

y0 k  ⌧, and kxi

y0 k  3⌧.

vk  4⌧ .

Proof. We begin by noting that fˆ(yi )  fˆ(y0 ) = f (y0 ) for i = 1, ..., t
f (yi ) f (b(1) ) f (y0 ) ↵⌧ 2 we therefore have
↵ kyi
which implies kyi

f (yi )  f (y0 )

f (yi )  ↵⌧ 2 ,

y0 k  ⌧ . Since by Corollary 1 we also have fˆ(u)  fˆ(y0 ), we similarly obtain ku

Using xi = (1 + !)yi

!yi

1

we have, by the triangle inequality and 0  ! < 1,
kxi

for every i = 1, ..., t

2
y0 k = fˆ(yi )

1, as guaranteed by Corollary 1. Using

y0 k  (1 + !) kyi

y0 k + ! kyi

1. Finally, since v = xj for some 0  j  t
ku

vk  ku

y0 k + kxj

1

y0 k  3⌧

1, this gives
y0 k  4⌧.

y0 k  ⌧ .

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

B.3. Proof of Theorem 1
Theorem 1. Let f : Rd ! R be L1 -smooth and have L2 -Lipschitz continuous Hessian. Let p0 2 Rd ,
2/3 1/3
inf z2Rd f (z) and 0 < ✏  min{ f L2 , L21 /(64L2 )}. Set

then G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵,

p
↵ = 2 L2 ✏
↵
L2 )

(13)

finds a point pK such that krf (pK )k  ✏ with at most

1/2 1/4
f L1 L2
✏7/4

20 ·

= f (p0 )

f

log

500L1
✏2

f

(14)

gradient evaluations.
Proof. We bound two quantities: the number of calls to AGD- UNTIL - GUILTY, which we denote by K, and the maximum
number of steps AGD- UNTIL - GUILTY performs when it is called, which we denote by T . The overall number gradient
evaluations is 2KT , as we compute at most 2T gradients per iterations (at the points x0 , . . . , xt 1 and y1 , . . . , yt ).
The upper bound on K is immediate from Lemma 2, as by telescoping the progress guarantee (12) we obtain

f

f (p0 )

f (pK

1) =

K
X1

(f (pk

1)

f (pk ))

(K

k=1

1) · min

⇢

✏2 ↵ 3
,
5↵ 64L22

(K

1)

✏3/2
1/2

10L2

,

where the final inequality follows by substituting our choice (13), of ↵. We conclude that
K  1 + 10

1/2
3/2
.
f L2 ✏

(28)

To bound the number T of steps of AGD- UNTIL - GUILTY, note that for every z 2 Rd
(z) = fˆ(y0 )

↵
fˆ(z) + kz
2

2

y0 k = f (y0 )

↵
kz
2

f (z)

2

y0 k 

f.

p
Therefore, substituting " = ✏/10, L = L1 + 2↵ and L = ↵ = 2 L2 ✏ into the guarantee (7) of Corollary 1,
T 1+
We use ✏  min{
bound (28) to

s

L1
2+ p
log+
2 L2 ✏

2/3 1/3
2
f L2 , L1 /(12L2 )}

p
Applying 1  L1 /(8 L2 ✏) in the bound (29) gives
T 
2

p
200(L1 + 4 L2 ✏)
✏2

f

◆

r

1/2

1/2
3/2
f L2 ✏

simplifies the

1/2
3/2
.
f L2 ✏

3 L1
log
4 L1/4 ✏1/4
2

(29)

.

to simplify the bounds on K and T . Using 1 
K  11

where f L1 ✏
the theorem.

✓

✓

500L1
✏2

f

◆

,

8 allows us to drop the subscript from the log. Multiplying the product of the above bounds by 2 gives

C. Proofs from Section 4
C.1. Proof of Lemma 5
We begin by proving the following normalized version of Lemma 5.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Lemma 8. Let h : R ! R be thrice differentiable, h000 be L-Lipschitz continuous for some L > 0 and let
Z 1 Z ⌫
0
h(1) h( 1) 2h ( 1) =
d⌫
h00 (⇠)d⇠  A,
1

for some A

0. Then for any ⇢
min{h( 1

where ⇢0 =

p

⇢(⇢ + 2)

(30)

1

4
⇢
⇢), h(1 + ⇢0 )}  max h( 1)

A 2
⇢ , h(1)
4

A 2
⇢
6

+

L 4
⇢ ,
8

(31)

2.

Proof. Define

1
1
h̃(⇠) = h(0) + h0 (0)⇠ + h00 (0)⇠ 2 + h000 (0)⇠ 3 .
2
6

By the Lipschitz continuity of h000 , we have that |h(⇠) h̃(⇠)|  L⇠ 4 /24 for any ⇠ 2 R (see Section 1.4). Similarly,
viewing h000 as the first derivative of h00 , we have and |h00 (⇠) h̃00 (⇠)|  L⇠ 2 /2. The assumption (30) therefore implies,

Z 1 Z ⌫
Z
Z ⌫
1
1 000
L 1
1
4 h00 (0)
h (0) =
d⌫
h̃00 (⇠)d⇠  A +
d⌫
⇠ 2 d⇠ = A + L.
(32)
2
6
2
3
1
1
1
1
It is also easy to verify that
h(0) =

h̃(1) + h̃( 1)
2

1 00
h̃(1) h̃( 1)
h (0) and h0 (0) =
2
2

Substituting into the definition of h̃ and rearranging, this yields

h̃(1) h̃( 1) 0
1
0
h̃(1 + ⇢ ) = h̃(1) +
⇢ + h00 (0)
2
2
and
h̃( 1
Suppose that

⇢) = h̃( 1)

h̃(1) h̃( 1)
2

h̃(1)

Suppose now that

⇢)  h( 1)

1 000
h (0) ⇢(2 + ⇢)
6

1 000
h (0)⇢2 (2 + ⇢).
6

0. By (34), (32) and ⇢2  ⇢(2 + ⇢)  3⇢2 /2 (since ⇢

+ 16 h000 (0)⇢(2 + ⇢)

h̃(⇠)|  L⇠ 4 /24 for ⇠ =
h( 1

1 000
1
h (0) ⇢0 (2 + ⇢0 ) + h000 (0)⇢0 (2 + ⇢0 )2
6
6


h̃( 1)
1
⇢ + h00 (0)
2
2

h̃( 1
and, using |h(⇠)

1 000
h (0).
6

⇢)  h̃( 1)

1 and ⇠ =

1

(33)

(34)

4) we then have

A 2 L 2
⇢ + ⇢ ,
4
8
⇢ along with 1  ⇢/4, we get

A 2 L 2
L
⇢ + ⇢ + (1 + (1 + ⇢)4 )  h( 1)
4
8
24

A 2 L 4
⇢ + ⇢ .
4
8

(35)

h̃(1) h̃( 1)
2

h̃(1 + ⇢0 )  h̃(1)

+ 16 h000 (0)⇢(2 + ⇢) < 0 holds instead. By (33) and (32) we then have


⇥
⇤
A
L 0
1
A
⇢ (2 + ⇢0 ) + h000 (0)⇢0 (2 + ⇢0 )2 ⇢(2 + ⇢)] = h̃(1)
4
12
6
4

L 0
⇢ (2 + ⇢0 )
12

where the equality follows from the definition (2 + ⇢0 )2 = ⇢(2 + ⇢). We lower bound ⇢0 (2 + ⇢0 ) as
⇣⇢
⌘ ⇢r ⇣⇢
⌘ 2⇢2
p
⇢0 (2 + ⇢0 ) = ⇢(2 + ⇢) 2 ⇢(2 + ⇢) ⇢
+⇢
⇢
+⇢
,
2
2
2
3
p
where the first inequality follows from the fact that ⇢(⇣ + ⇢) ⇣ ⇢(⇣ + ⇢) is monotonically decreasing in ⇣ 0 and the
assumption 2  ⇢/2. Noting that ⇢0  ⇢, we have the upper bound ⇢0 (2 + ⇢0 )  ⇢(2 + ⇢)  3⇢2 /2. Combining these

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

bounds gives h̃(1 + ⇢0 )  h̃(1) A6 ⇢2 + L8 ⇢2 . Applying |h(⇠)
⇢0  ⇢ and 1  ⇢/4 once more, we obtain,
h(1 + ⇢0 )  h(1)

h̃(⇠)|  L⇠ 4 /24 at ⇠ = 1 and ⇠ = 1 + ⇢0 , and using

A 2 L 2
L
⇢ + ⇢ + (1 + (1 + ⇢)4 )  h(1)
6
8
24

A 2 L 4
⇢ + ⇢ .
6
8

(36)

The fact that either (35) or (36) must hold implies (31).

With the auxiliary Lemma 8, we prove Lemma 5.
d
Lemma
p 5. Let f : R ! R have L3 -Lipschitz third-order derivatives. Let ↵ > 0 and let u and v satisfy (10) and let
⌘  2↵/L3 . Then for every ku vk  ⌘/2, E XPLOIT-NC- PAIR 3 (f, u, v, ⌘) finds a point z such that
n
↵ 2
↵ 2o
⌘ , f (u)
⌘ .
(16)
f (z)  max f (v)
4
12

Proof. Define

h(✓) := f
We have
h(1)

h( 1)

2h0 ( 1) = f (u)

✓

◆
1+✓
1 ✓
u+
v .
2
2

f (v)

rf (v)T (u

↵
ku
2

v) <

2

vk :=

A.

4

1
Additionally, since f has L3 -Lipschitz third order derivatives, h000 is 16
L3 ku vk := L Lipschitz continuous, so we may
apply Lemmap8 at ⇢ = 2⌘/ ku vk 4. Letting = (u v)/ ku vk, we note that h(1 ⇢) = f (v ⌘ ). Similarly,
for 2 + ⇢0 = ⇢(2 + ⇢) we have h(1 + ⇢0 ) = f (u + ⌘ 0 ) with ⌘ 0 given in line 2 of E XPLOIT-NC- PAIR 3 . The result is now
immediate from (31), as
⇢
A 2
A 2
L
0
0
f (z) = min{f (v ⌘ ), f (u + ⌘ )} = min{h( 1 ⇢), h(1 + ⇢ )}  max h( 1)
⇢ , h(1)
⇢ + ⇢4
4
6
8
n
o
n
o
↵ 2
↵ 2
L3 4
↵ 2
↵ 2
= max f (v)
⌘ , f (u)
⌘ +
⌘  max f (v)
⌘ , f (u)
⌘ ,
2
3
8
4
12
q
2↵
where in the last transition we have used ⌘  L
.
3

C.2. Proof of Lemma 6

We first state and prove a normalized version of the central argument in the proof of Lemma 6
Lemma 9. Let h : R ! R be thrice differentiable and let h000 be L-Lipschitz continuous for some L > 0. If
h(0)  A, h( 1/2)
for some A, B, C, D
for any ✓ 2 [0, 1].
Proof. Define

B, h( 1)  C and h( 3)

D

(37)

0, then
h(✓)  h(0) + 7A + 12.8B + 6C + 0.2D + L

1
1
h̃(⇠) = h(0) + h0 (0)⇠ + h00 (0)⇠ 2 + h000 (0)⇠ 3 .
2
6

By the Lipschitz continuity of h000 , we have that |h(⇠) h̃(⇠)|  L⇠ 4 /24, for any ⇠ 2 R. Using the expressions for h̃(x)
at ⇠ = 3, 1, 1/2 to eliminate h0 (0), h00 (0) and h000 (0), we obtain:


1
1 2
1 3
3
7
✓ + ✓ + ✓ + h̃( 1) ✓ + ✓2 + ✓3
h̃(✓) = h(0) h̃( 3)
30
10
15
2
2


24
32 2 8 3
10
2
h̃( 1/2)
✓ + ✓ + ✓ + h(0)
✓ + 3✓2 + ✓3 .
5
5
5
3
3

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Applying (37), ✓ 2 [0, 1] and |h(⇠)

h̃(⇠)|  L⇠ 4 /24 gives the required bound:

⇤
L ⇥ 4
✓ + 0.2 · ( 3)4 + 6 · ( 1)4 + 12.8 · ( 1/2)4
24
 h(0) + 7A + 12.8B + 6C + 0.2D + L

h(✓)  h(0) + 0.2D + 6C + 12.8B + 7A +

We now prove Lemma 6 itself.
p
Lemma 6. Let f be L1 -smooth and have L3 -Lipschitz continuous third-order derivatives, and let ⌧ 
↵/(16L3 )
with ⌧, ↵, L1 , L3 > 0. Consider G UARDED - NON - CONVEX -AGD with F IND - BEST- ITERATE replaced by F IND - BESTITERATE 3 . At any iteration, if u, v 6= NULL and the best iterate b(1) satisfies f (b(1) )
f (y0 ) ↵⌧ 2 then,
f (v)  f (y0 ) + 14↵⌧ 2 .

Proof. Let 0  j < t be such that v = xj (such j always exists by Corollary 1). If j = 0 then xj = y0 and the result is
trivial, so we assume j 1. Let
h(✓) = f (yj + ✓(yj

yj

f (y0 ) for ✓ 2 R

1 ))

Note that
h( 3) = f (qj )
h( 1) = f (yj
h( 1/2) = f (cj )
h(0) = f (yj )
h(!) = f (xj )

f (b(1) )

f (y0 )

f (y0 )  0,

1)

f (b(1) )

f (y0 )

f (y0 )

↵⌧ 2 ,

f (y0 )

↵⌧ 2 ,

f (y0 )  0 and
f (y0 ),

where 0 < ! < 1 is defined in line 1 of AGD- UNTIL - GUILTY, and we have used the guarantee max{f (yj 1 ), f (yj )} 
4
f (y0 ) from Corollary 1. Moreover, by the Lipschitz continuity of the third derivatives of f , h000 is L3 kyj yj 1 k 2
Lipschitz continuous. Therefore, we can apply Lemma 9 with A = C = 0 and B = D = ↵⌧ at ✓ = ! and obtain
f (v)

f (y0 ) = f (xj )

f (y0 )  f (yj )

f (y0 ) + 13↵⌧ 2 + L3 kyj

To complete the proof, we note that Lemma 3 guarantees kyj
where we have used ⌧ 2  ↵/(16L3 ).

L3 kyj

yj

4
1k

yj

1k

yj

 kyj

4
1k

 13↵⌧ 2 + L3 kyj

y0 k + kyj

1

yj

1k

4

.

y0 k  2⌧ and therefore

 16L3 ⌧ 4  ↵⌧ 2 ,

C.3. Proof of Lemma 7
Lemma 7. Let f : Rd ! R be L1 -smooth and have L3 -Lipschitz continuous third-order derivatives, letq✏, ↵ > 0 and
2↵
p0 2 Rd . If pK
0 is the sequence of iterates produced by G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵,
L3 ), then for
every 1  k < K,
⇢ 2
✏
↵2
f (pk )  f (pk 1 ) min
,
.
(17)
5↵ 32L3
Proof. Fix an iterate index 1  k < K; throughout the proof we let y0t , xt0 and w refer to outputs of AGD- UNTIL - GUILTY
in the kth iteration. We consider only the case v, u 6= NULL, as the argument for v, u = NULL is unchanged from
Lemma 2.
q
↵
As argued in the proof of Lemma 2, when v, u 6= NULL, condition (10) holds. We set ⌧ :=
32L3 and consider
two cases. First, if f (b(1) )  f (y0 ) ↵⌧ 2 = f (pk
f (b(1) ) f (y0 ) ↵⌧ 2 , by Lemma 3 we have that
kv

1)

↵2
32L3

uk  4⌧ 

r

then we are done, since f (pk )  f (b(1) ). Second, if

↵
⌘
= ,
2L3
2

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Therefore, we can use Lemma 5 (with ⌘ as defined above) to show that
⇢
↵2
(2)
f (b )  max f (v)
, f (u)
2L3
By Corollary 1, f (u)  fˆ(u)  fˆ(y0 ) = f (pk

1 ).

↵2
6L3

Moreover, since f (b(1) )

apply Lemma 6 to obtain

f (v)  f (y0 ) + 14↵⌧ 2  f (pk

1)

+

(38)

.
↵⌧ 2 and ⌧ =

f (y0 )

7↵2
.
16L3

q

↵
32L3 ,

we may

Combining this with (38), we find that
f (pk )  f (b(2) )  f (pk

1)

min

⇢

↵2
2L3

7↵2 ↵2
,
16L3 6L3

= f (pk

1)

↵2
,
16L3

which concludes the case v, u 6= NULL under third-order smoothness.
C.4. Proof of Theorem 2
Theorem 2. Let f : Rd ! R be L1 -smooth and have L3 -Lipschitz continuous third-order derivatives. Let p0 2 Rd ,
1/2 1/6
1/3
inf z2Rd f (z) and 0 < ✏2/3  min{ f L3 , L1 /(8L3 )}. If we set
f = f (p0 )
1/3

↵ = 2L3 ✏2/3 ,
q

G UARDED - NON - CONVEX -AGD(f , p0 , L1 , ✏, ↵,
20 ·

2↵
L3 )

(18)

finds a point pK such that krf (pK )k  ✏ and requires at most

1/2 1/6
f L1 L3
✏5/3

log

✓

500L1
✏2

f

◆

(19)

gradient evaluations.
Proof. The proof proceeds exactly like the proof of Theorem 1. As argued there, the number of gradient evaluations is at
most 2KT , where K is number of iterations of G UARDED - NON - CONVEX -AGD and T is the maximum amount of steps
performed in any call to AGD- UNTIL - GUILTY.
We derive the upper bound on K directly from Lemma 7, as by telescoping (12) we obtain
f

f (p0 )

f (pK

1) =

K
X1

(f (pk

1)

f (pk ))

k=1

(K

1) · min

⇢

✏2 ↵ 2
,
5↵ 32L3

(K

1)

✏4/3
1/3

10L3

,

where the last transition follows from substituting (18), our choice of ↵. We therefore conclude that
K  1 + 10

1/3
4/3
.
f L3 ✏

(39)

To bound T , we recall that (z)  f for every z 2 Rd , as argued in the proof Theorem 1. Therefore, substituting
1/3
" = ✏/10, L = L1 + 2↵ and = ↵ = 2L3 ✏2/3 into the guarantee (7) of Corollary 1 we obtain,
!
s
1/3
L1
200(L1 + 4L3 ✏2/3 ) f
T 1+ 2+
log+
,
(40)
1/3
✏2
2L ✏2/3
3

where log+ (·) is shorthand for max{0, log(·)}.
Finally, we use ✏2/3  min{
reduces (28) to

1/2 1/6
1/3
f L3 , L1 /(8L3 )}

to simplify the bounds on K and T . Using 1 

K  11

1/3
4/3
.
f L3 ✏

1/3
4/3
f L3 ✏

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions
1/3

Applying 1  L1 /(8L3 ✏2/3 ) on (29) gives
T 
where f L1 ✏
the result.

2

r

1/2

3 L1
500L1
log
4 L1/6 ✏1/3
✏2

f

,

3

8 allows us to drop the subscript from the log. Multiplying the product of the above bounds by 2 gives

D. Adding a second-order guarantee
In this section, we sketch how to obtain simultaneous guarantees on the gradient and minimum eigenvalue of the Hessian.
e notation to hide logarithmic dependence on ✏, Lipschitz constants f , L1 , L2 , L3 and a high probability
We use the O(·)
confidence parameter 2 (0, 1), as well as lower order polynomial terms in ✏ 1 .

Using approximate eigenvector computation, we can efficiently generate a direction of negative curvature, unless the
Hessian is almostp
positive semi-definite. More explicitly, there exist methods of the form A PPROX -E IG(f , x, L1 , ↵,
e L1 /↵ log d) Hessian-vector products to produce a unit vector v such that whenever r2 f (x) ⌫ ↵I,
) that require O(
with probability at least 1
we have v T r2 f (x)v  ↵/2, e.g. the Lanczos method (see additional discussion in
(Carmon et al., 2016, §2.2)). Whenever a unit vector v satisfying v T r2 f (x)v  ↵/2 is available, we can use it to make
↵3
function progress. If r2 f is L2 -Lipschitz continuous then by Lemma 1 f (x ± L↵2 v) < f (x) 12L
2 where by f (x ± z)
2
we mean
min{f
(x
+
z),
f
(x
z)}.
If
instead
f
has
L
-Lipschitz
continuous
third-order
derivatives
then by Lemma 4,
3
q
f (x ±

2↵
L3 v)

< f (x)

↵2
4L3 .

We can combine A PPROX -E IG with Algorithm 3 that finds a point with a small gradient as follows:
ẑk

G UARDED - NON - CONVEX -AGD(f, zk , L1 , ✏, ↵, ⌘)

(41a)

vk

A PPROX -E IG(f, ẑk , L1 , ↵, 0 )

(41b)

zk+1

arg min

(41c)

f (x)

x2{ẑk +⌘vk ,ẑk ⌘vk }

p
As discussed above, under third order smoothness , ⌘ = 2↵/L3 guarantees that the step (41c) makes at least ↵2 /(4L3 )
e f L3 /↵2 ) times
function progress whenever vkT r2 f (ẑk )vk  ↵/2. Therefore the above iteration can run at most O(
T 2
T 2
0
before vk r f (ẑk )vk
↵/2 is satisfied. Whenever vk r f (ẑk )vk
↵/2, with probability 1
· k we have the
1/3 2/3
2
Hessian guarantee r f (ẑk ) ⌫ ↵I. Moreover, krf (ẑk )k  ✏ always holds. Thus, by setting ↵ = L3 ✏
we obtain
the required second order stationarity guarantee upon termination of the iterations (41).
1/3

It remains to bound the computational cost of the method, with ↵ = L3 ✏2/3 . The total number of Hessian-vector products
required by A PPROX -E IG is,
!
r
⇣
⌘
L1
1/2 1/6
2
5/3
e
e
O
L
/↵
·
log
d
=
O
L
L
✏
log
d
.
f 3
f 1
3
↵
Moreover, it is readily seen from the proof of Theorem 2 that every evaluation of (41a) requires at most
e (xk )
O((f

1/2

1/6

f (xk+1 ))L1 L3 ✏

5/3

1/2

+ L1 L3

1/6

✏

1/3

)

(42)

e f L3 /↵2 ), we guarantee
gradient and function evaluations. By telescoping the first term and multiplying the second by O(
1/3
1/2
1/6
e f L L ✏ 5/3 log d) function, gradient and Hessian-vector
krf (x)k  ✏ and r2 f (x) ⌫ L3 ✏2/3 I in at most O(
1
3
product evaluations.
The argument above is the same as the one used to prove Theorem 4.3 of (Carmon et al., 2016), but our improved guarantees
under third order smoothness allows us get a better ✏ dependence for the complexity and lower bound on the Hessian in
that regime. If instead we use the second order smoothness setting, we recover exactly the guarantees of (Carmon et al.,
1/2
e f L1/2 L1/4 ✏ 7/4 log d)
2016; Agarwal et al., 2016), namely krf (x)k  ✏ and r2 f (x) ⌫ L2 ✏1/2 I in at most O(
1
2
function, gradient and Hessian-vector product evaluations.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

Finally, we remark that the above analysis would still apply if in (41a) we replace G UARDED - NON - CONVEX -AGD with
any method with a run -time guarantee of the form (42). The resulting method will guarantee whatever the original method
does, and also r2 f (x) ⌫ ↵I. In particular, if the first method guarantees a small gradient, the combined method
guarantees convergence to second-order stationary points.

E. Experiment details
E.1. Implementation details
Semi-adaptive gradient steps Both gradient descent and AGD are based on gradients steps of the form
yt+1 = xt

1
rf (xt ).
L1

(43)

In practice L1 is often unknown and non-uniform, and therefore needs to be estimated adaptively. A common approach
is backtracking line search, which we use for conjugate gradient. However, combining line search with AGD without
invalidating its performance guarantees would involve non-trivial modification of the proposed method. Therefore, for the
rest of the methods we keep an estimate of L1 , and double it whenever the gradient steps fails to make sufficient progress.
That is, whenever
✓
◆
1
1
2
f xt
rf (xt ) > f (xt )
krf (xt )k
L1
2L1
we set L1
2L1 and try again. In all experiments we start with L1 = 1, which underestimates the actual smoothness of f
by 2-3 orders of magnitude. We call our scheme for setting L1 semi-adaptive, since we only increase L1 , and therefore do
not adapt to situations where the function becomes more smooth as optimization progresses. Thus, we avoid painstaking
tuning of L1 while preserving the ‘fixed step-size’ nature of our approach, as L1 is only doubled a small number of times.
Algorithm 3 We implement G UARDED - NON - CONVEX -AGD with the following modifications, indented to make it more
practical without substantially compromising its theoretical properties.
1. We use the semi-adaptive scheme described above to set L. Specifically, whenever the gradient steps in lines 3
and 4 of AGD- UNTIL - GUILTY and C ERTIFY- PROGRESS respectively fail, we double L until it succeeds, terminate
AGD- UNTIL - GUILTY and multiply L1 by the same factor.
2. We make the input parameters for AGD- UNTIL - GUILTY dynamic. In particular, we set ✏0 = krf (pk 1 )k /10 and
2/3
use ↵ = = C1 krf (pk 1 )k , where C1 is a hyper-parameter. We use the same value of ↵ to construct fˆ. This
makes our implementation independent on the final desired accuracy ✏.
3. In C ERTIFY- PROGRESS we also test whether
fˆ(xt ) + rfˆ(xt )T (yt

xt ) > fˆ(yt ).

Since this inequality is a clear convexity violation, we return wt = yt whenever it holds. We find that this substantially
increases our method’s capability of detecting negative curvature; most of the non-convexity detection in the first
experiment is due to this check.
4. Whenever C ERTIFY- PROGRESS produces a point wt 6= NULL (thereby proving non-convexity and stopping AGDUNTIL - GUILTY ), instead of finding a single pair (v, u) that violates strong convexity, we compute
↵v,u = 2

f (v)

f (u)
ku

rf (v)T (u
vk

v)

2

for the 2t points of the form v = xj and u = yj or u = wt , with 0  j < t, where here we use the original f rather
than fˆ given to AGD- UNTIL - GUILTY. We discard all pairs with ↵v,u < 0 (no evidence of negative curvature), and
select the 5 pairs with highest value of ↵v,u . For each selected pair v, u, we exploit negative curvature by testing all
the points of the form {z ± ⌘ } with = (u v)/ ku vk, z 2 {v, u} and ⌘ in a grid of 10 points log-uniformly
spaced between 0.01 ku vk and 100(kuk + kvk).

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

5. In F IND - BEST- ITERATE 3 we compute cj and qj for every j such that f (xj ) > f (yj ). Moreover, when v, u = NULL
(no non-convexity detected), we still set the next iterate pk to be the output of F IND - BEST- ITERATE 3 rather than just
the last AGD step.
The hyper-parameter C1 was tuned separately for each experiment by searching on a small grid. For the regression experiment the tuning was performed on different problem instances (different seeds) than the ones reported in Fig. 1. For
the neural network training problem the tuning was performed on a subsample of 10% of the data and a different random
initialization than the one reported in Fig. 2. The specific parameters used were C1 = 0.01 for regression and C1 = 0.1
for neural network training.
Algorithm 3 without negative curvature exploitation This method is identical to the one described above, except that
at every iteration pk is set to b(1) produced by F IND - BEST- ITERATE 3 (i.e. the output of negative curvature exploitation is
never used). We used the same hyper-parameters described above.
Gradient descent Gradient descent descent is simply (43), with yt+1 = xt+1 , where the semi-adaptive scheme is used
to set L1 .
Adaptive restart accelerated gradient descent We use the accelerated gradient descent scheme of Beck and Teboulle
(2009) with !t = t/(t + 3). We use the restart scheme given by O’Donoghue and Candès (2015) where if f (yt ) > f (yt 1 )
then we restart the algorithm from the point yt . For the gradient steps we use the same semi-adaptive procedure described
above and also restart the algorithm whenever the L1 estimate changes (restarts performed for this reason are not shown in
Fig. 1 and 2).
Non-linear conjugate gradient The method is given by the following recursion (Polak and Ribière, 1969),
⇢
rf (xt )T (rf (xt ) rf (xt 1 ))
=
rf
(x
)
+
max
, 0 t 1 , xt+1 = xt + ⌘t t
t
t
krf (xt 1 )k2
where 0 = 0 and ⌘t is found via backtracking line search, as follows. If
the recursion). We set ⌘t = 2⌘t 1 and then check whether
f (xt + ⌘t t )  f (xt ) +

⌘t

T

rf (xt )

0 we set

t

=

rf (xt ) (truncating

T
t rf (xt )

2

holds. If it does we keep the value of ⌘t , and if it does not we set ⌘t = ⌘t /2 and repeat. The key difference from
the semi-adaptive scheme used for the rest of the methods is the initialization ⌘t = 2⌘t 1 , that allows the step size to
grow. Performing line search is crucial for conjugate gradient to succeed, as otherwise it cannot produce approximately
conjugate directions. If instead we use the semi-adaptive step size scheme, performance becomes very similar to that of
gradient descent.
Comparison of computational cost In the figures, the x-axis is set to the number of steps performed by the methods.
We do this because it enables a one-to-one comparison between the steps of the restarted AGD and Algorithm 3. However,
Algorithm 3 requires twice the number of gradient evaluations per step of the other algorithms. Furthermore, the number
of function evaluations of Algorithm 3 increases substantially when we exploit negative curvature, due to our naive grid
search procedure. Nonetheless, we believe it is possible to derive a variation of our approach that performs only one
gradient computation per step, and yet maintains similar performance (see remark after Corollary 1, and that effective
negative curvature exploitation can be carried out with only few function evaluations, using a line search.
While the rest of the methods tested require one gradient evaluation per step, the required number of function evaluations
differs. GD requires only one function evaluation per step, while RAGD evaluates f twice per step (at xt and yt ); the
number of additional function evaluations due to the semi-adaptive scheme is negligible. NCG is expected to require
more function evaluations due to its use of a backtracking line search. In the first experiment, NCG required 2 function
evaluations per step on average, indicating that its L1 estimate was stable for long durations. Alg. 3 required 5.3 function
evaluations per step (on average over the 1,000 problem instances, with standard deviation 0.5), putting the amortized cost
of our crude negative curvature exploitation scheme at 3.3 function evaluations per step.

“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions

E.2. Non-convex regression
The problem is to

m

minimize f (x) :=

1 X
(aTi x
m i=1

bi )

where (✓) = ✓2 /(1 + ✓2 ), x 2 Rd , b 2 Rm , and ai 2 Rd . The function is a robust modification of the quadratic loss; it
is approximately quadratic for small errors, but insensitive to larger errors.
iid

To generate problem instances, we set d = 30, m = 60, and draw ai ⇠ N (0, Id ). We draw b as follows. We first define
draw a “ground truth” vector z ⇠ N (0, 4Id ). We then set b = Az + 3⌫1 + ⌫2 , where ⌫1 is standard Gaussian and the
elements of ⌫2 are i.i.d. Bernoulli(0.3). The above parameters were manually chosen to make the problem substantially
non-convex.
E.3. Neural network training
The function f is the average cross-entropy loss of 10-way prediction of class labels from input features. The prediction
if formed by applying softmax on the output of a neural network with three hidden layers of 20, 10 and 5 units and tanh
activations. To obtain data features we perform the following preprocessing, where the training examples are treated as
282 dimensional vectors. First, each example is separately normalized to zero mean and unit variance. Then, the 282 ⇥ 282
data covariance matrix is formed, and a projection to the 10 principle components is found via eigen-decomposition. The
projection is then applied to the training set, and then each of the 10 resulting features is normalized to have zero mean
and unit variance across the training set. The resulting model has d = 545 parameters and underfits the 60,000 examples
training set. We randomly initialize the weights according the well-known scaling proposed by Glorot and Bengio (2010).
We repeated the experiment for 10 different initializations of the weights, and all results were consistent with those reported
in Fig. 2.

