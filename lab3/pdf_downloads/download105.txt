On the Sampling Problem for Kernel Quadrature

A. Appendix
This appendix complements the paper “On the sampling
problem for kernel quadrature”. Section A.1 discusses the
potential lack of robustness of greedy optimization methods, which motivated the development of SMC-KQ. Sections A.2 and A.3 discuss some of the theoretical aspects
of KQ, whilst Section A.4 and A.5 presents additional numerical experiments and details for implementation. Finally, Section A.6 provides detailed pseudo-code for all algorithms used in this paper.
A.1. Lack of Robustness of Optimisation Methods
To demonstrate the non-robustness to mis-specified kernels, that is a feature of optimisation-based methods, we
considered integration against Π = N(0, 1) for functions that can be approximated by the kernel k(x, x0 ) =
exp(−(x − x0 )2 /`2 ). An initial state x1 was fixed at the
origin and then for n = 2, 3, . . . the state xn was chosen to minimise the error criterion en (w; {xj }nj=1 ) given
the location of the {xj }nj=1 . This is known as ‘sequential
Bayesian quadrature’ (SBQ; Huszar and Duvenaud, 2012;
Gunter et al., 2014; Briol et al., 2015a). The kernel length
scale was fixed at ` = 0.01 and we consider (as a thought
experiment, since it does not enter into our selection of
points) a more regular integrand, such as that shown in Fig.
5 (top). The location of the states {xj }nj=1 obtained in this
manner are shown in Fig. 5 (bottom). It is clear that SBQ is
not an efficient use of computation for integration of the integrand against N(0, 1). Of course, a bad choice of kernel
length scale parameter ` can in principle be alleviated by
kernel learning, but this will not be robust the case where n
is very small.
This example motivates sampling-based methods as an alternative to optimisation-based methods. Future work will
be required to better understand when methods such as SBQ
can be reliable in the presence of unknown kernel parameters, but this was beyond the scope of this work.

Figure 5. Sequential minimisation of the error criterion
en (w; {xj }n
j=1 ), denoted SBQ, does not lead to adequate
placement of points {xj }n
j=1 when the kernel is mis-specified.
[Here the kernel length scale was fixed to ` = 0.01. Selected
points xj are represented as red. For comparison, a collection of
draws from Π, as used in KQ, are shown as blue points.]

This space is equipped with norm
!1/2
kf kHs (Π) =

X

k(∂x1 )α1 . . . (∂xd )αd f k2L2 (Π)

.

|α|≤s

Two normed spaces (F, k · k) and (F, k · k0 ) are said to be
‘norm equivalent’ if there exists 0 < c < ∞ such that
c−1 kf k0 ≤ kf k ≤ ckf k0
for all f ∈ F.

A.2. Additional Definitions
A.3. Theoretical Results
The space L2 (Π) is defined to be the set of Π-measurable
functions f : X → R such that the Lebesgue integral
Z
f 2 dΠ
X

exists and is finite.

A.3.1. P ROOF OF T HEOREM 1
Proof. From Thm. 11.13 in Wendland (2004) we have that
there exist constants 0 < ck < ∞, h0 > 0 such that
|fˆ(x) − f (x)| ≤ ck hsn kf kH

(7)

For a multi-index α = (α1 , . . . , αd ) define |α| = α1 +
· · · + αd . The (standard) Sobolev space of order s ∈ N is
denoted

for all x ∈ X , provided hn < h0 , where

Hs (Π) = {f : X → R s.t.

Under the hypotheses, we can suppose that the deterministic states x1 , . . . , xm ensure hm < h0 . Then Eqn. 7 holds

(∂x1 )α1 . . . (∂xd )αd f ∈ L2 (Π) ∀ |α| ≤ s}.

hn = sup min kx − xi k2 .
x∈X i=1,...,n

On the Sampling Problem for Kernel Quadrature

for all n > m, where the xm+1 , . . . , xn are independent
draws from Π0 . It follows that
|Π̂(f ) − Π(f )|

≤

sup |fˆ(x) − f (x)|

≤

x∈X
ck hsn kf kH .

 n
2
X

βi


ψ(·,
x
)
−
f
sup
inf 4 

i
2

πB (xi )1/2
kf kH ≤1 kβk2 ≤ n 
i=1

Next, Lem. 1 in Oates et al. (2016) establishes that, under the present hypotheses on X and Π0 , there exists 0 <
cΠ0 , < ∞ such that
−2s/d+
E[h2s
n ] ≤ cΠ0 , m

2
≤ c2k E[h2s
n ]kf kH

≤ c2k cΠ0 , m−2s/d+ kf k2H
1/2

A.3.2. P ROOF OF T HEOREM 2
Proof. The Cauchy-Schwarz result for kernel mean embeddings (Smola et al., 2007) gives
|Π̂(f ) − Π(f )|
(8)

 n
Z

X


wi k(·, xi ) −
k(·, x)Π(dx) kf kH .



X
H

Consider the first term above. Since H is dense in L2 (Π),
it follows that Σ1/2 (the unique positive self-adjoint square
root of Σ) is an isometry from L2 (Π) to H. Now, since
k(·, x) ∈ H, there exists a unique element ψ(·, x) ∈
L2 (Π) such that Σ1/2 ψ(·, x) = k(·, x). Then we have that

 n
Z

X


wi k(·, xi ) −
k(·, x)Π(dx)



X
i=1
H


Z
n
X



1/2
1/2
= 
wi Σ ψ(·, xi ) −
Σ ψ(·, x)Π(dx)


X
i=1
H


Z
n
X



= 
wi ψ(·, xi ) −
ψ(·, x)Π(dx)
.


X
i=1

L2 (Π)

For f ∈ L2 (Π), we have f ∈ H if and only if
Z
f=
g(x)ψ(·, x)Π(dx)

L2 (Π)

i=1

as required, with ck,Π0 , = ck cΠ0 , .

i=1

with probability at least 1−δ. Fixing the function f in Eqn.
9 leads to the statement that
 n
2
Z
X

βi


ψ(·,
x
)
−
ψ(·,
x)Π(dx)
inf 4 

i
1/2
2


π
(x
)
kβk2 ≤ n
B
i
X
is at most 4λ with probability at least 1 − δ. The infimum
over kβk22 ≤ 4/n can be replaced with an unconstrained
infimum over Rn to obtain the weaker statement that
2
 n
Z

X
βi


ψ(·,
x
)
−
ψ(·,
x)Π(dx)
infn 

i

β∈R 
πB (xi )1/2
X

Combining the above results produces

≤

≤ 4λ

L2 (Π)

i=1

for all  > 0, where cΠ0 , is independent of n.

E[Π̂(f ) − Π(f )]2

Under the hypothesis on n, Prop. 1 of Bach (2015) established that when x1 , . . . , xn ∼ ΠB are independent, then

L2 (Π)

is at most 4λ with probability at least 1 − δ. Now, recall
from Sec. 2.1 that the KQ weights w are characterised
through the solution β ∗ to this optimisation problem as
wi = βi∗ πB (xi )−1/2 . It follows that

2
Z
n
X



wi ψ(·, xi ) −
ψ(·, x)Π(dx)



X
i=1

≤ 4λ

L2 (Π)

with probability at least 1 − δ. Combining this fact with
Eqn. 8 completes the proof.
A.3.3. ΠB FOR THE E XAMPLE OF F IGURE 1
In this section we consider scope to derive ΠB in closedform for the example of Fig. 1. The following will be used:
Proposition 1 (Prop. 1 in Shi et al. (2009)). Let X = R,
Π = N(µ, σ 2 ) and k(x, x0 ) = exp(−(x − x0 )2 /`2 ). Define β = 4σ 2 /`2 and denote the jth Hermite polynomial as
Hj (x). Then the eigenvalues µj and corresponding eigenfunctions ej of the integral operator Σ are
s
µj =

(1 + β +

2
√

1 + 2β)

×



j
β
√
1 + β + 1 + 2β

and
(9)

X

for some g ∈ L2 (Π), in which case kf kH is equal to the infimum of kgkL2 (Π) under all such representations g. In particular, it follows that kf kH = 1 for the particular choice
with g(x) = 1 for all x ∈ X .

 (x − µ)2 √1 + 2β − 1 
(1 + 2β)1/8
p
ej (x) =
exp −
2σ 2
2
2j j!
 1 β 1/4 x − µ 
×Hj
+
4
2
σ
for j ∈ {0, 1, 2, . . . }.

On the Sampling Problem for Kernel Quadrature

Proposition 2 (Ex. 6.8 in Temme (1996), p.167). The bilinear generating function for Hermite polynomials is
∞ j
X
t
j=0

j!

Hj (x)Hj (z)
=√



(x − 2zt)2
1
exp x2 −
.
1 − 4t2
1 − 4t2

Proposition 3. For the example in Fig. 1 we have
πB (x; λ) ∝
2

exp(−x )

∞
X
j=0


1
1
2
H
j
1 + λ2j+1 2j j!

r

3 
x .
2

Proof. For the example of Fig. 1, in the notation of Prop.
1, we have µ = 0, σ = 1, ` = 1 and β = 4. Thus
µj

=

 1 j+1

2
r 3 
√
1
2
2
=
3 exp(−x ) j Hj
x
2 j!
2

ej (x)2
and so
πB (x; λ) ∝

X
j

µj
e2 (x)
µj + λ j
2

∝ exp(−x )

∞
X
j=0


1
1
2
H
1 + λ2j+1 2j j! j

r

3 
x
2

as required.
To the best of our knowledge, the expression for ΠB in
Prop. 3 does not admit a closed form. This poses a practical challenge. However, some limited insight is available
through basic approximations:
• For large values of λ we have 1 + λ2j+1 ≈ λ2j+1 for
all j ∈ {0, 1, 2, . . . }, from which we obtain
∞
r 3 
X
1
2
2
∝
πB (x; λ) ∼ exp(−x )
H
x
4j j! j
2
j=0
∝ exp(−x2 ) exp(x2 )

=

1,

where the second step made use of Prop. 2. Thus
when large integration errors are tolerated, ΠB requires that we take the states xi to be approximately
uniform over X (of course, this limiting distribution is
improper and serves only for illustration).
• For small values of λ, the series in Prop. 3 is dominated by the first m terms such that j < m if and
only if λ2j+1 < 1. Indeed, for j ≤ m we have

Figure 6. Numerical approximation of ΠB for the running illustration. Here the regularisation parameter was λ = 10−15 .

1 + λ2j+1 ≈ 1. Thus we have a computable approximation
m
r 3 
X
1
2
2
H
x
πB (x; λ) ∝
exp(−x
)
∼
2j j! j
2
j=0

where m = d− log2 (λ)e. Empirical results (not
shown) indicate that this is not a useful approximation
from a practical standpoint, since at finite m the tails
of the approximation are explosive (due to the use of
a polynomial basis).
The approximation method in Bach (2015) was also used
to obtain the numerical approximation to ΠB shown in Fig.
6. This appears to support the intuition that it is beneficial
to over-sample from the tails of Π.
To finish, we remark that Prop. 3 implies that the integration error in this example scales as
√
µn ∼ 2−n/2
as n → ∞ when samples are drawn from ΠB . This agrees
with both intuition and empirical results that concern approximation with exponentiated quadratic kernels.
A.3.4. A DDITIONAL T HEORETICAL M ATERIAL
As mentioned in the Main Text, the worst-case error
en ({xj }nj=1 ) can be computed in closed form:
en ({xj }nj=1 )2 = Π ⊗ Π(k) − 2w> Kz + w> Kw
Here we have defined
ZZ
Π ⊗ Π(k) =

k(x, x0 ) Π ⊗ Π(dx × dx0 )

X ×X

where Π ⊗ Π is the product measure of Π with itself.
Next, we report a result which does not address KQ itself,
but considers importance sampling methods for integration

On the Sampling Problem for Kernel Quadrature

of functions in a Hilbert space. The following is due to
Plaskota et al. (2009); Hinrichs (2010) and we provide an
elementary proof of their result:
Theorem 3. The assumptions of Sec. 2.4 are taken to hold.
In addition, we assume that distributions Π, Π0 admit densities π, π 0 . Introduce importance sampling estimators of
the form
n
π(xi )
1X
f (xi ) 0
,
Π̂IS (f ) =
n i=1
π (xi )
where x1 , . . . , xn ∼ Π0 are independent, and consider the
distribution Π0 that minimises
q
sup E[Π̂IS (f ) − Π(f )]2 .

Combining Eqns. 10 and A.3.4, we have
 f π 2 

 f π 2 
0
sup
sup Π0
≤
Π
π0
π0
f ∈F
f ∈F

 π(·) 2 
= Π0 k(·, ·) 0
π (·)
As before,
p this is in fact an equality, as can be seen from
f (x) = k(x, x).
From Jensen’s inequality,

 π(·) 2 
 p
π(·) 2
k(·, ·) 0
≥
Π0
Π0 k(·, ·) 0
(11)
π (·)
π (·)
p
2
= Π k(·, ·) .

f ∈F

For F = {f } we have that Π0 is π 0 (x) ∝ |f (x)|π(x),
while for p
F = {f ∈ H : kf kH ≤ 1} we have that Π0 is
0
π (x) ∝ k(x, x)π(x).
Proof. The first result, for F = {f } is well-known; e.g.
Thm. 3.3.4 in Robert and Casella (2013).
For the second case, where F is the unit ball in H, we start
by establishing a (tight) upper bound for the supremum of
f 2 over f ∈ F:


|f (x)| = hf, k(·, x)iH 
kf kH kk(·, x)kH
p
= kf kH hk(·, x), k(·, x)iH
p
= kf kH k(x, x)
where the inequality here is Cauchy-Schwarz. Squaring
both sides and taking the supremum over f ∈ F gives
sup f (x)2 ≤ sup kf k2H k(x, x) = k(x, x).

(10)

f ∈F

This is in fact an equality,
psince for given x ∈ X we can
take f (x0 ) = k(x0 , x)/ k(x, x) which has kf kH = 1
and f (x)2 = k(x, x).
Our objective is expressed as
q
fπ

1
sup E[Π̂IS (f ) − Π(f )]2 = sup √ Std 0 ; Π0
π
n
f ∈F
f ∈F
and since
fπ
2
 f π 2 
 2
0 fπ
−
Π
Std 0 ; Π0
= Π0
π
π0
π0
we thus aim to minimise
sup Π0
f ∈F

as required.
A.4. Implementation of test(R < Rmin )

≤

f ∈F

Since the right hand side is independent of Π0 , a choice of
Π0 for which Eqn. 11 is an equality must be a minimiser of
Eqn. A.3.4. It remains just to verify this fact for π 0 (x) =
p
k(x,
p x)π(x)/C, where the normalising constant is C =
Π( k(·, ·)). For this choice

 π(·) 2 
Π0 k(·, ·) 0
= Π0 (C 2 )
π (·)
p
= (Π( k(·, ·)))2

Here we provide details for how the criterion R < Rmin
was tested. The problem with the naive approach of comparing R estimated at ti−1 directly with R estimated at ti is
that Monte Carlo error can lead to an incorrect impression
that R is increasing, when it is in fact decreasing, and cause
the algorithm to terminate when estimation is poor (see Fig.
7 and note the jaggedness of the estimated R curve as a
function of inverse temperature t). Our solution was to apply a least-squares linear smoother to the estimates for R
over 5 consecutive temperatures. This approach, denoted
test, illustrated in Fig. 7, determines whether the gradient of the linear smoother is positive or negative, and in this
way we are able to provide robustness to Monte Carlo error
in the termination criterion. To be precise, the algorithm
requires at least 5 temperature evaluations before termination is considered (Fig. 7; left) and terminates when the
gradient of the linear smoother becomes positive for the
first time (Fig. 7; right). The success of this strategy was
established in Fig. 9 later in the Appendix.
A.5. Experimental Results

 f π 2 
π0

over Π0 ∈ P(F · dΠ/dΠ0 ). (Here F · dΠ/dΠ0 denotes the
set of functions of the form f · dΠ/dΠ0 such that f ∈ F.)

A.5.1. I MPLEMENTATION OF S IMULATION S TUDY
Denote by N(x|µ, Σ) the p.d.f. of the multivariate Gaussian distribution with mean µ and covariance Σ. Furthermore, we denote by Σσ the diagonal covariance matrix

On the Sampling Problem for Kernel Quadrature

Figure 7. Implementation of test(R < Rmin ). A linear smoother (dashed line) was based on 5 consecutive (inverse) temperature
parameters ti−4 , ti−3 , ti−2 , ti−1 , ti . To begin it is required that 5 temperatures are considered (left panel). The algorithm terminates on
the first occasion when the linear smoother takes a positive gradient (right panel).

with diagonal element σ 2 . Then elementary manipulation
of Gaussian densities produces:
2
 Pd

j=1 xj − yj
k(x, y) := exp −
2
l 
√
= ( πl)d φ x|y, Σl/√2
Pd
2 j=1 (xj − yj )2
∇l k(x, y) :=
k(x, y)
l3

√ d
Π[k(·, x)] := ( πl) N x|0, Σσ + Σl/√2

√
Π ⊗ Π(k) := ( πl)d N 0|0, Σ√2σ + Σl/√2
A.5.2. D EPENDENCE ON PARAMETERS FOR THE
S IMULATION S TUDY
For the running illustration with f (x) = 1 + sin(x),
Π = N(0, 1), Π0 = N(0, σ 2 ) and k(x, x0 ) = exp(−(x −
x0 )2 /`2 ), we explored how the RMSE of KQ depends on
the choice of both σ and `. Here we go beyond the results presented in Fig. 2, which considered fixed n, to now
consider the simultaneous choice of both σ, ` for varying
n. Note that in these numerical experiments the kernel matrix inverse K−1 was replaced with the regularised inverse
(K + λI)−1 that introduces a small ‘nugget’ term λ > 0
for stabilisation. Results, shown in Fig. 8, demonstrate two
principles that guided the methodological development in
this paper:
• Length scales ` that are ‘too small’ to learn from n
samples do not permit good approximations fˆ and
lead in practice to high RMSE. At the same time, if
` is taken to be ‘too large’ then efficient approximation at size n will also be sacrificed. This is of course
well understood from a theoretical perspective and is
borne out in our empirical results. These results motivated extension of SMC-KQ to SMC-KQ-KL.
• In general the ‘sweet spot’, where σ and ` lead to minimal RMSE, is quite small. However, the problem of
optimal choice for σ and ` does not seem to become

more or less difficult as n increases. This suggests
that a method for selection of σ (and possibly also of
`) ought to be effective regardless of the number n of
states that will be used.
A.5.3. A DDITIONAL R ESULTS FOR THE S IMULATION
S TUDY
To understand whether the termination criterion of Sec. 3.5
was suitable (and, by extension, to examine the validity of
the convexity ansatz in Sec. 3.2), in Fig. 9 we presented
histograms for both estimated and actual optimal (inverse)
temperature parameter t∗ . Results supported the use of the
criterion, in the form described above for test.
In Fig. 10 reports the dependence of performance on the
choice of initial distribution Π0 . There was relatively little influence on the RMSE obtained by the method for this
wide range of initial distribution, which supports the purported robustness of the method.
We also test the method on more complex integrands in Fig.
11: f (x) = 1 + sin(4πx) and f (x) = 1 + sin(8πx). These
are more challenging for KQ compared to the illustration
in the Main Text, since they are more difficult to interpolate due to their higher periodicity. However, SMC-KQ still
manages to adapt to the complexity of the integrand and
performs as well as the best importance sampling distribution (σ = 2).
As an extension, we also study the robustness to the dimensionality to the problem. In problem, we consider the generalisation of our main test function to f : Rd → R given
Qd
by f (x) = 1 + j=1 sin(2πxj ). Notice that the integral
can still be computed analytically and equals 1. We present
results for d = 2 and d = 3 in Fig. 12. These two cases are
more challenging for both the KQ and SMC-KQ methods,
since the higher dimension implies a slower convergence
rate. Once again, we notice that SMC-KQ manages to adapt
to the complexity of the problem at hand, and provides improved performance on simpler sampling distributions.

On the Sampling Problem for Kernel Quadrature

Figure 9. Histograms for the optimal (inverse) temperature parameter t∗ . Left: Estimate of t∗ provided under the termination
criterion of Sec. 3.5. Right: Estimate of t∗ obtained by estimating R over a grid for t ∈ [0, 1] and returning the global minimum.
The similarity of these histograms is supportive of the convexity
ansatz in Sec. 3.2.

Figure 10. Comparison of the performance of SMC-KQ on the
running illustration of Figs. 1 and 2 for varying initial distribution Π0 = N(0, σ 2 ).

A.5.4. I MPLEMENTATION OF S TEIN ’ S M ETHOD
Figure 8. Example of Fig. 2, continued. Here we consider the
simultaneous choice of sampling standard deviation σ and kernel
length-scale `, reporting empirical estimates for the estimated root
mean square integration error (over M = 300 repetitions) in each
case for sample size (a) n = 25 (top), (b) n = 50 (middle) and
(c) n = 75 (bottom).

Finally, we considered replacing the independent samples
xj ∼ Π with samples drawn from a quasi-random point sequence. Fig. 13 reports results where draws from N(0, 1)
were produced based on a Halton quasi-random number
generator. In this case, the performance is improved by
up to 10 orders of magnitude in MSE when the sampling is
done with respect to a range of tempered sampling distribution (here N(0, 32 )). This suggests that a SQMC approach
(Gerber and Chopin, 2015) could provide further improvement and this suggested for future work.

Following Oates et al. (2017) we considered the Stein operator
S[f ](θ) := [∇θ + ∇ log π(θ)][f ](θ)
and denote the score function by uj (θ) = ∇θj log π(θ).
Here π is the p.d.f. for Π. Applying the Stein operator to
each argument of a base kernel kb , and adding a constant,
gives produces the new kernel:

k(θ, φ)

:=

1+

d
X
j=1

[∇θj ∇φj kb (θ, φ)
+uj (θ)∇φj kb (θ, φ)
+uj (φ)∇θj kb (θ, φ)
+uj (θ)uj (φ)kb (θ, φ)]

which we will use for our KQ estimator. Using integration
by parts, we can easily check that Π[k(·, θ)] = 1 and Π ⊗
Π(k) = 1. In this experiment, the base kernel was taken to
Pd
be Gaussian: kb (θ, φ) = exp(− j=1 (θj − φj )2 /`2j ). We

On the Sampling Problem for Kernel Quadrature

Figure 11. Performance of KQ and SMC-KQ on the integration
problem with f (x) = 1 + sin(4πx) (top) and f (x) = 1 +
sin(8πx) (bottom) integrated against N(0, 1). The SMC sampler was initiated with a N(0, 82 ) distribution. The kernel used
was Gaussian with length scales ` = 0.25 (top) and ` = 0.15
(bottom) each chosen to reflect the complexity of the functions.

obtained the derivatives:
dk(θ, φ)
dθj

= −

dk(θ, φ)
dφj

=

dk(θ, φ)
dθj dφj

=

2
(θj − φj )k(θ, φ)
`2j

2
(θj − φj )k(θ, φ)
`2j

2`2j − 4(θj − φj )2
k(θ, φ)
`4j

Furthermore, we can obtain expressions for the score function for posterior densities as follows:
uj (θ)

=

d
d
log π(θ) +
log π(y|θ).
dθj
dθj

Figure 12. Performance of Q
KQ and SMC-KQ on the integration
problem with f (x) = 1 + dj=1 sin(2πxj ) integrated against a
N(0, I) distribution for d = 2 (top), d = 3 (middle) and d = 10
(bottom). The SMC sampler was initiated with a N(0, 82 I) distribution. The kernel
P used was a (multivariate) Gaussian kernel
k(x, y) = exp(− dj=1 (xj − yj )2 /`2j ) with the length scales
`1 = · · · = `d = 0.25 were used.

A.6. Algorithms and Implementation
A.6.1. SMC S AMPLER
In Alg. 2 the standard SMC scheme is presented. Resampling occurs when the effective sample size, kwk−2
2

drops below a fraction ρ of the total number N of particles.
In this work we took ρ = 0.95 which is a common default.

On the Sampling Problem for Kernel Quadrature

Algorithm 3 Markov Iteration
function Markov(x, π, {(wj , xj )}N
j=1 )
input x (current state)
input π (density of invar. dist.)
x∗ ∼ q(x, x∗ ; {(wj , xj )}N
j=1 ) (propose)
r←

πi (x∗ )q(x∗ , x; {(wj , xj )}N
j=1 )
πi (x)q(x, x∗ ; {(wj , xj )}N
j=1 )

u ∼ Unif(0, 1)
if u < r then
x ← x∗ (accept)
end ifreturn x (next state)
Figure 13. Comparison between KQ with xj ∼ N(0, 1) independent and KQ with xj = Φ−1 (uj ) where the {uj }n
j=1 are the first
n terms in the Halton sequence and Φ is the standard Gaussian
cumulative density function.

Algorithm 2 Sequential Monte Carlo Iteration
function SMC({(wj , xj )}N
j=1 , ti , ti−1 , ρ)
input {(wj , xj )}N
(particle
approx. to Πi−1 )
j=1
input ti (next inverse-temperature)
input ti−1 (previous inverse-temperature)
input ρ (re-sample threshold)
wj0 ← wj × [π(xj )/π0 (xj )]ti −ti−1 (∀j ∈ 1 : N )
w0 ← w0 /kw0 k1 (normalise weights)
if kw0 k−2
2 < N · ρ then
a ∼ Multinom(w0 )
x0j ← xa(j) (re-sample ∀j ∈ 1 : N )
wj0 ← N −1 (reset weights ∀j ∈ 1 : N )
end if
x0j ∼ Markov(x0j ; Πi , {(wj , xj )}N
j=1 ) (Markov update
∈ 1 : N)
return {(wj0 , x0j )}N
j=1 (particle approx. to Πi )

Denote
q(x, ·; {(wj , xj )}N
j=1 )

=

µ =

N(·; µ, Σ)
N
X

wj xj

j=1

Σ

=

N
X

wj (xj − µ)(xj − µ)> .

A.6.2. C HOICE OF T EMPERATURE S CHEDULE
Following Zhou et al. (2016) we employed an adaptive temperature schedule construction. This was based on the conditional effective sample size of the SMC particle set, estimated as follows:
Algorithm 4 Conditional Effective Sample Size
function CESS({(wj , xj )}N
j=1 , t)
input {(wj , xj )}N
(particle
approx. Πi−1 )
j=1
input t (candidate next inverse-temperature)
zj ← [π(xj )/π0 (xj )]ti −ti−1 (∀j ∈ 1 : N )
P
2  P

N
N
2
E←N
j=1 wj zj
j=1 wj zj
return E (est’d. cond. ESS)
The specific construction for the temperature schedule is
detailed in Alg. 5 below and makes use of a Sequential
Least Squares Programming algorithm:
Algorithm 5 Adaptive Temperature Iteration
function temp({(wj , xj )}N
j=1 , ti−1 , ρ, ∆)
N
input {(wj , xj )}j=1 (particle approx. Πi−1 )
input ti−1 (current inverse-temperature)
input ρ (re-sample threshold)
input ∆ (max. grid size, default ∆ = 0.1)
t ← solve(CESS({(wj , xj )}N
j=1 , t) = N · ρ)
(binary search in [ti−1 , 1])
ti ← min{ti−1 + ∆, t} return ti (next inversetemperature)

j=1

The above standard adaptive independence proposal was
used within a Metropolis-Hastings Markov transition:

A.6.3. T ERMINATION C RITERION
For SMC-KQ we estimated an upper bound on the worst
case error in the unit ball of the Hilbert space H. This was
computed as follows, using a bootstrap algorithm:

On the Sampling Problem for Kernel Quadrature

Algorithm 6 Termination Criterion
function crit(Π, k, {xj }N
j=1 )
input Π (target disn.)
input k (kernel)
input {xj }N
j=1 (collection of states)
2
R ←RR
0
e0 ← X ×X k(x, x0 )Π ⊗ Π(dx × dx0 ) (in’l error)
for m = 1,. . . ,M do
N
x̃j ∼ Unif({x
j }j=1 ) (∀j ∈ 1 : n)
R
zj ← X k(·, x̃j )dΠ (k’l mean eval. ∀j ∈ 1 : n)
Kj,j 0 ← k(x̃j , x̃j 0 ) (kernel eval. ∀j, j 0 ∈ 1 : n)
w ← z T K −1 (KQ weights)
e2n ← w> Kw − 2w> z + e20
R2 ← R2 + e2n M −1
end for
return R (est’d error)

Note that this could be slightly improved using a weighted
bootstrap approach.
For SMC-KQ-KL an empirical upper bound on integration
error was estimated. This requires that the norm kf kH be
estimated, which was achieved as follows:
Algorithm 7 Termination Crit. + Kernel Learning
function crit-KL(f, Π, k, {xj }N
j=1 )
input f (integrand)
input Π (target disn.)
input k (kernel)
input {xj }N
j=1 (collection of states)
2
R ←RR
0
e0 ← X ×X k(x, x0 )Π ⊗ Π(dx × dx0 ) (in’l error)
for m = 1,. . . ,M do
x̃j ∼ Unif({xj }N
j=1 ) (∀j ∈ 1 : n)
fj ← fR(x̃j ) (function eval. ∀j ∈ 1 : n)
zj ← X k(·, x̃j )dΠ (k’l mean eval. ∀j ∈ 1 : n)
Kj,j 0 ← k(x̃j , x̃j 0 ) (kernel eval. ∀j, j 0 ∈ 1 : n)
w ← z T K −1 (KQ weights)
e2n ← w> Kw − 2w> z + e20
R2 ← R2 + e2n M −1
end for
R
zj ← X k(·, xj )dΠ (kernel mean eval. ∀j ∈ 1 : n)
Kj,j 0 ← k(xj , xj 0 ) (kernel eval. ∀j, j 0 ∈ 1 : n)
w ← z T K −1 (KQ weights)
S 2 ← R2 × w> Kw return S (est’d error bound)
In Alg. 7 the literal interpretation, that f is re-evaluated
on values of xj which have been previously examined, is
clearly inefficient. In practice such function evaluations
were cached and then do not contribute further to the total number of function evaluations that are required in the
algorithm.

A.6.4. K ERNEL L EARNING
A generic approach to select kernel parameters is the maximum marginal likelihood method:
Algorithm 8 Parameter Update
function kern-param(f , {xj }nj=1 , kθ )
input f (integrand evals.)
input {xj }nj=1 (associated states)
input kθ (parametric kernel)
θ0 ← arg minθ f > K−1
θ f + log |Kθ | (numer. opt.)
(s.t. Kθ,j,j 0 = kθ (xj , xj 0 )) return θ0 (optimal params)

A.6.5. I MPLEMENTATION OF SMC-KQ-KL
Our final algorithm to present is the full implementation for
SMC-KQ-KL:

On the Sampling Problem for Kernel Quadrature

Algorithm 9 SMC for KQ with Kernel Learning
function SMC-KQ-KL(f, Π, kθ , Π0 , ρ, n, N )
input f (integrand)
input Π (target disn.)
input kθ (parametric kernel)
input Π0 (reference disn.)
input ρ (re-sample threshold)
input n (num. func. evaluations)
input N (num. particles)
i ← 0; ti ← 0; Rmin ← ∞
x0j ∼ Π0 (initialise states ∀j ∈ 1 : N )
wj0 ← N −1 (initialise weights ∀j ∈ 1 : N )
θ0 ← kern-param(f, {x0j }nj=1 ) (kernel params)
R ← crit-KL(f, Π, kθ0 , {x0j }N
j=1 ) (est’d error)
while test(R < Rmin ) and ti < 1 do
i ← i + 1; Rmin ← R; θ ← θ0
0
0
N
{(wj , xj )}N
j=1 ← {(wj , xj )}j=1
N
ti ← temp({(wj , xj )}j=1 , ti−1 ) (next temp.)
N
{(wj0 , x0j )}N
j=1 ← SMC({(wj , xj )}j=1 , ti , ti−1 , ρ)
(next particle approx.)
θ0 ← kern-param(f, {x0j }nj=1 ) (kernel params)
R ← crit-KL(f, Π, kθ0 , {x0j }N
j=1 ) (est’d error)
end while
fj ← fR(xj ) (function eval. ∀j ∈ 1 : n)
zj ← X kθ (·, xj )dΠ (kernel mean eval. ∀j ∈ 1 : n)
Kj,j 0 ← kθ (xj , xj 0 ) (kernel eval. ∀j, j 0 ∈ 1 : n)
Π̂(f ) ← z > K−1 f (eval. KQ estimator) return Π̂(f )
(estimator)

As stated here, Alg. 9 is inefficient as function evaluations
that are produced in the kern-param and crit-KL
components are not included in the KQ estimator Π̂(f ).
Thus a trivial modification is to store all function evaluations (fj , xj ) that are produced and to include all of these
in the ultimate KQ estimator. This was the approach taken
in our experiments that involved SMC-KQ-KL. However,
since it is somewhat cumbersome to include in the pseudocode, we have not made this explicit in the notation. Our
reported results are on a per-function-evaluation basis and
so we do adjust for this detail in our reported comparisons.

