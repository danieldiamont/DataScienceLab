On the Sampling Problem for Kernel Quadrature

A. Appendix
This appendix complements the paper â€œOn the sampling
problem for kernel quadratureâ€. Section A.1 discusses the
potential lack of robustness of greedy optimization methods, which motivated the development of SMC-KQ. Sections A.2 and A.3 discuss some of the theoretical aspects
of KQ, whilst Section A.4 and A.5 presents additional numerical experiments and details for implementation. Finally, Section A.6 provides detailed pseudo-code for all algorithms used in this paper.
A.1. Lack of Robustness of Optimisation Methods
To demonstrate the non-robustness to mis-specified kernels, that is a feature of optimisation-based methods, we
considered integration against Î  = N(0, 1) for functions that can be approximated by the kernel k(x, x0 ) =
exp(âˆ’(x âˆ’ x0 )2 /`2 ). An initial state x1 was fixed at the
origin and then for n = 2, 3, . . . the state xn was chosen to minimise the error criterion en (w; {xj }nj=1 ) given
the location of the {xj }nj=1 . This is known as â€˜sequential
Bayesian quadratureâ€™ (SBQ; Huszar and Duvenaud, 2012;
Gunter et al., 2014; Briol et al., 2015a). The kernel length
scale was fixed at ` = 0.01 and we consider (as a thought
experiment, since it does not enter into our selection of
points) a more regular integrand, such as that shown in Fig.
5 (top). The location of the states {xj }nj=1 obtained in this
manner are shown in Fig. 5 (bottom). It is clear that SBQ is
not an efficient use of computation for integration of the integrand against N(0, 1). Of course, a bad choice of kernel
length scale parameter ` can in principle be alleviated by
kernel learning, but this will not be robust the case where n
is very small.
This example motivates sampling-based methods as an alternative to optimisation-based methods. Future work will
be required to better understand when methods such as SBQ
can be reliable in the presence of unknown kernel parameters, but this was beyond the scope of this work.

Figure 5. Sequential minimisation of the error criterion
en (w; {xj }n
j=1 ), denoted SBQ, does not lead to adequate
placement of points {xj }n
j=1 when the kernel is mis-specified.
[Here the kernel length scale was fixed to ` = 0.01. Selected
points xj are represented as red. For comparison, a collection of
draws from Î , as used in KQ, are shown as blue points.]

This space is equipped with norm
!1/2
kf kHs (Î ) =

X

k(âˆ‚x1 )Î±1 . . . (âˆ‚xd )Î±d f k2L2 (Î )

.

|Î±|â‰¤s

Two normed spaces (F, k Â· k) and (F, k Â· k0 ) are said to be
â€˜norm equivalentâ€™ if there exists 0 < c < âˆ such that
câˆ’1 kf k0 â‰¤ kf k â‰¤ ckf k0
for all f âˆˆ F.

A.2. Additional Definitions
A.3. Theoretical Results
The space L2 (Î ) is defined to be the set of Î -measurable
functions f : X â†’ R such that the Lebesgue integral
Z
f 2 dÎ 
X

exists and is finite.

A.3.1. P ROOF OF T HEOREM 1
Proof. From Thm. 11.13 in Wendland (2004) we have that
there exist constants 0 < ck < âˆ, h0 > 0 such that
|fË†(x) âˆ’ f (x)| â‰¤ ck hsn kf kH

(7)

For a multi-index Î± = (Î±1 , . . . , Î±d ) define |Î±| = Î±1 +
Â· Â· Â· + Î±d . The (standard) Sobolev space of order s âˆˆ N is
denoted

for all x âˆˆ X , provided hn < h0 , where

Hs (Î ) = {f : X â†’ R s.t.

Under the hypotheses, we can suppose that the deterministic states x1 , . . . , xm ensure hm < h0 . Then Eqn. 7 holds

(âˆ‚x1 )Î±1 . . . (âˆ‚xd )Î±d f âˆˆ L2 (Î ) âˆ€ |Î±| â‰¤ s}.

hn = sup min kx âˆ’ xi k2 .
xâˆˆX i=1,...,n

On the Sampling Problem for Kernel Quadrature

for all n > m, where the xm+1 , . . . , xn are independent
draws from Î 0 . It follows that
|Î Ì‚(f ) âˆ’ Î (f )|

â‰¤

sup |fË†(x) âˆ’ f (x)|

â‰¤

xâˆˆX
ck hsn kf kH .

 n
2
X

Î²i


Ïˆ(Â·,
x
)
âˆ’
f
sup
inf 4 

i
2

Ï€B (xi )1/2
kf kH â‰¤1 kÎ²k2 â‰¤ n 
i=1

Next, Lem. 1 in Oates et al. (2016) establishes that, under the present hypotheses on X and Î 0 , there exists 0 <
cÎ 0 , < âˆ such that
âˆ’2s/d+
E[h2s
n ] â‰¤ cÎ 0 , m

2
â‰¤ c2k E[h2s
n ]kf kH

â‰¤ c2k cÎ 0 , mâˆ’2s/d+ kf k2H
1/2

A.3.2. P ROOF OF T HEOREM 2
Proof. The Cauchy-Schwarz result for kernel mean embeddings (Smola et al., 2007) gives
|Î Ì‚(f ) âˆ’ Î (f )|
(8)

 n
Z

X


wi k(Â·, xi ) âˆ’
k(Â·, x)Î (dx) kf kH .



X
H

Consider the first term above. Since H is dense in L2 (Î ),
it follows that Î£1/2 (the unique positive self-adjoint square
root of Î£) is an isometry from L2 (Î ) to H. Now, since
k(Â·, x) âˆˆ H, there exists a unique element Ïˆ(Â·, x) âˆˆ
L2 (Î ) such that Î£1/2 Ïˆ(Â·, x) = k(Â·, x). Then we have that

 n
Z

X


wi k(Â·, xi ) âˆ’
k(Â·, x)Î (dx)



X
i=1
H


Z
n
X



1/2
1/2
= 
wi Î£ Ïˆ(Â·, xi ) âˆ’
Î£ Ïˆ(Â·, x)Î (dx)


X
i=1
H


Z
n
X



= 
wi Ïˆ(Â·, xi ) âˆ’
Ïˆ(Â·, x)Î (dx)
.


X
i=1

L2 (Î )

For f âˆˆ L2 (Î ), we have f âˆˆ H if and only if
Z
f=
g(x)Ïˆ(Â·, x)Î (dx)

L2 (Î )

i=1

as required, with ck,Î 0 , = ck cÎ 0 , .

i=1

with probability at least 1âˆ’Î´. Fixing the function f in Eqn.
9 leads to the statement that
 n
2
Z
X

Î²i


Ïˆ(Â·,
x
)
âˆ’
Ïˆ(Â·,
x)Î (dx)
inf 4 

i
1/2
2


Ï€
(x
)
kÎ²k2 â‰¤ n
B
i
X
is at most 4Î» with probability at least 1 âˆ’ Î´. The infimum
over kÎ²k22 â‰¤ 4/n can be replaced with an unconstrained
infimum over Rn to obtain the weaker statement that
2
 n
Z

X
Î²i


Ïˆ(Â·,
x
)
âˆ’
Ïˆ(Â·,
x)Î (dx)
infn 

i

Î²âˆˆR 
Ï€B (xi )1/2
X

Combining the above results produces

â‰¤

â‰¤ 4Î»

L2 (Î )

i=1

for all  > 0, where cÎ 0 , is independent of n.

E[Î Ì‚(f ) âˆ’ Î (f )]2

Under the hypothesis on n, Prop. 1 of Bach (2015) established that when x1 , . . . , xn âˆ¼ Î B are independent, then

L2 (Î )

is at most 4Î» with probability at least 1 âˆ’ Î´. Now, recall
from Sec. 2.1 that the KQ weights w are characterised
through the solution Î² âˆ— to this optimisation problem as
wi = Î²iâˆ— Ï€B (xi )âˆ’1/2 . It follows that

2
Z
n
X



wi Ïˆ(Â·, xi ) âˆ’
Ïˆ(Â·, x)Î (dx)



X
i=1

â‰¤ 4Î»

L2 (Î )

with probability at least 1 âˆ’ Î´. Combining this fact with
Eqn. 8 completes the proof.
A.3.3. Î B FOR THE E XAMPLE OF F IGURE 1
In this section we consider scope to derive Î B in closedform for the example of Fig. 1. The following will be used:
Proposition 1 (Prop. 1 in Shi et al. (2009)). Let X = R,
Î  = N(Âµ, Ïƒ 2 ) and k(x, x0 ) = exp(âˆ’(x âˆ’ x0 )2 /`2 ). Define Î² = 4Ïƒ 2 /`2 and denote the jth Hermite polynomial as
Hj (x). Then the eigenvalues Âµj and corresponding eigenfunctions ej of the integral operator Î£ are
s
Âµj =

(1 + Î² +

2
âˆš

1 + 2Î²)

Ã—



j
Î²
âˆš
1 + Î² + 1 + 2Î²

and
(9)

X

for some g âˆˆ L2 (Î ), in which case kf kH is equal to the infimum of kgkL2 (Î ) under all such representations g. In particular, it follows that kf kH = 1 for the particular choice
with g(x) = 1 for all x âˆˆ X .

 (x âˆ’ Âµ)2 âˆš1 + 2Î² âˆ’ 1 
(1 + 2Î²)1/8
p
ej (x) =
exp âˆ’
2Ïƒ 2
2
2j j!
 1 Î² 1/4 x âˆ’ Âµ 
Ã—Hj
+
4
2
Ïƒ
for j âˆˆ {0, 1, 2, . . . }.

On the Sampling Problem for Kernel Quadrature

Proposition 2 (Ex. 6.8 in Temme (1996), p.167). The bilinear generating function for Hermite polynomials is
âˆ j
X
t
j=0

j!

Hj (x)Hj (z)
=âˆš



(x âˆ’ 2zt)2
1
exp x2 âˆ’
.
1 âˆ’ 4t2
1 âˆ’ 4t2

Proposition 3. For the example in Fig. 1 we have
Ï€B (x; Î») âˆ
2

exp(âˆ’x )

âˆ
X
j=0


1
1
2
H
j
1 + Î»2j+1 2j j!

r

3 
x .
2

Proof. For the example of Fig. 1, in the notation of Prop.
1, we have Âµ = 0, Ïƒ = 1, ` = 1 and Î² = 4. Thus
Âµj

=

 1 j+1

2
r 3 
âˆš
1
2
2
=
3 exp(âˆ’x ) j Hj
x
2 j!
2

ej (x)2
and so
Ï€B (x; Î») âˆ

X
j

Âµj
e2 (x)
Âµj + Î» j
2

âˆ exp(âˆ’x )

âˆ
X
j=0


1
1
2
H
1 + Î»2j+1 2j j! j

r

3 
x
2

as required.
To the best of our knowledge, the expression for Î B in
Prop. 3 does not admit a closed form. This poses a practical challenge. However, some limited insight is available
through basic approximations:
â€¢ For large values of Î» we have 1 + Î»2j+1 â‰ˆ Î»2j+1 for
all j âˆˆ {0, 1, 2, . . . }, from which we obtain
âˆ
r 3 
X
1
2
2
âˆ
Ï€B (x; Î») âˆ¼ exp(âˆ’x )
H
x
4j j! j
2
j=0
âˆ exp(âˆ’x2 ) exp(x2 )

=

1,

where the second step made use of Prop. 2. Thus
when large integration errors are tolerated, Î B requires that we take the states xi to be approximately
uniform over X (of course, this limiting distribution is
improper and serves only for illustration).
â€¢ For small values of Î», the series in Prop. 3 is dominated by the first m terms such that j < m if and
only if Î»2j+1 < 1. Indeed, for j â‰¤ m we have

Figure 6. Numerical approximation of Î B for the running illustration. Here the regularisation parameter was Î» = 10âˆ’15 .

1 + Î»2j+1 â‰ˆ 1. Thus we have a computable approximation
m
r 3 
X
1
2
2
H
x
Ï€B (x; Î») âˆ
exp(âˆ’x
)
âˆ¼
2j j! j
2
j=0

where m = dâˆ’ log2 (Î»)e. Empirical results (not
shown) indicate that this is not a useful approximation
from a practical standpoint, since at finite m the tails
of the approximation are explosive (due to the use of
a polynomial basis).
The approximation method in Bach (2015) was also used
to obtain the numerical approximation to Î B shown in Fig.
6. This appears to support the intuition that it is beneficial
to over-sample from the tails of Î .
To finish, we remark that Prop. 3 implies that the integration error in this example scales as
âˆš
Âµn âˆ¼ 2âˆ’n/2
as n â†’ âˆ when samples are drawn from Î B . This agrees
with both intuition and empirical results that concern approximation with exponentiated quadratic kernels.
A.3.4. A DDITIONAL T HEORETICAL M ATERIAL
As mentioned in the Main Text, the worst-case error
en ({xj }nj=1 ) can be computed in closed form:
en ({xj }nj=1 )2 = Î  âŠ— Î (k) âˆ’ 2w> Kz + w> Kw
Here we have defined
ZZ
Î  âŠ— Î (k) =

k(x, x0 ) Î  âŠ— Î (dx Ã— dx0 )

X Ã—X

where Î  âŠ— Î  is the product measure of Î  with itself.
Next, we report a result which does not address KQ itself,
but considers importance sampling methods for integration

On the Sampling Problem for Kernel Quadrature

of functions in a Hilbert space. The following is due to
Plaskota et al. (2009); Hinrichs (2010) and we provide an
elementary proof of their result:
Theorem 3. The assumptions of Sec. 2.4 are taken to hold.
In addition, we assume that distributions Î , Î 0 admit densities Ï€, Ï€ 0 . Introduce importance sampling estimators of
the form
n
Ï€(xi )
1X
f (xi ) 0
,
Î Ì‚IS (f ) =
n i=1
Ï€ (xi )
where x1 , . . . , xn âˆ¼ Î 0 are independent, and consider the
distribution Î 0 that minimises
q
sup E[Î Ì‚IS (f ) âˆ’ Î (f )]2 .

Combining Eqns. 10 and A.3.4, we have
 f Ï€ 2 

 f Ï€ 2 
0
sup
sup Î 0
â‰¤
Î 
Ï€0
Ï€0
f âˆˆF
f âˆˆF

 Ï€(Â·) 2 
= Î 0 k(Â·, Â·) 0
Ï€ (Â·)
As before,
p this is in fact an equality, as can be seen from
f (x) = k(x, x).
From Jensenâ€™s inequality,

 Ï€(Â·) 2 
 p
Ï€(Â·) 2
k(Â·, Â·) 0
â‰¥
Î 0
Î 0 k(Â·, Â·) 0
(11)
Ï€ (Â·)
Ï€ (Â·)
p
2
= Î  k(Â·, Â·) .

f âˆˆF

For F = {f } we have that Î 0 is Ï€ 0 (x) âˆ |f (x)|Ï€(x),
while for p
F = {f âˆˆ H : kf kH â‰¤ 1} we have that Î 0 is
0
Ï€ (x) âˆ k(x, x)Ï€(x).
Proof. The first result, for F = {f } is well-known; e.g.
Thm. 3.3.4 in Robert and Casella (2013).
For the second case, where F is the unit ball in H, we start
by establishing a (tight) upper bound for the supremum of
f 2 over f âˆˆ F:


|f (x)| = hf, k(Â·, x)iH 
kf kH kk(Â·, x)kH
p
= kf kH hk(Â·, x), k(Â·, x)iH
p
= kf kH k(x, x)
where the inequality here is Cauchy-Schwarz. Squaring
both sides and taking the supremum over f âˆˆ F gives
sup f (x)2 â‰¤ sup kf k2H k(x, x) = k(x, x).

(10)

f âˆˆF

This is in fact an equality,
psince for given x âˆˆ X we can
take f (x0 ) = k(x0 , x)/ k(x, x) which has kf kH = 1
and f (x)2 = k(x, x).
Our objective is expressed as
q
fÏ€

1
sup E[Î Ì‚IS (f ) âˆ’ Î (f )]2 = sup âˆš Std 0 ; Î 0
Ï€
n
f âˆˆF
f âˆˆF
and since
fÏ€
2
 f Ï€ 2 
 2
0 fÏ€
âˆ’
Î 
Std 0 ; Î 0
= Î 0
Ï€
Ï€0
Ï€0
we thus aim to minimise
sup Î 0
f âˆˆF

as required.
A.4. Implementation of test(R < Rmin )

â‰¤

f âˆˆF

Since the right hand side is independent of Î 0 , a choice of
Î 0 for which Eqn. 11 is an equality must be a minimiser of
Eqn. A.3.4. It remains just to verify this fact for Ï€ 0 (x) =
p
k(x,
p x)Ï€(x)/C, where the normalising constant is C =
Î ( k(Â·, Â·)). For this choice

 Ï€(Â·) 2 
Î 0 k(Â·, Â·) 0
= Î 0 (C 2 )
Ï€ (Â·)
p
= (Î ( k(Â·, Â·)))2

Here we provide details for how the criterion R < Rmin
was tested. The problem with the naive approach of comparing R estimated at tiâˆ’1 directly with R estimated at ti is
that Monte Carlo error can lead to an incorrect impression
that R is increasing, when it is in fact decreasing, and cause
the algorithm to terminate when estimation is poor (see Fig.
7 and note the jaggedness of the estimated R curve as a
function of inverse temperature t). Our solution was to apply a least-squares linear smoother to the estimates for R
over 5 consecutive temperatures. This approach, denoted
test, illustrated in Fig. 7, determines whether the gradient of the linear smoother is positive or negative, and in this
way we are able to provide robustness to Monte Carlo error
in the termination criterion. To be precise, the algorithm
requires at least 5 temperature evaluations before termination is considered (Fig. 7; left) and terminates when the
gradient of the linear smoother becomes positive for the
first time (Fig. 7; right). The success of this strategy was
established in Fig. 9 later in the Appendix.
A.5. Experimental Results

 f Ï€ 2 
Ï€0

over Î 0 âˆˆ P(F Â· dÎ /dÎ 0 ). (Here F Â· dÎ /dÎ 0 denotes the
set of functions of the form f Â· dÎ /dÎ 0 such that f âˆˆ F.)

A.5.1. I MPLEMENTATION OF S IMULATION S TUDY
Denote by N(x|Âµ, Î£) the p.d.f. of the multivariate Gaussian distribution with mean Âµ and covariance Î£. Furthermore, we denote by Î£Ïƒ the diagonal covariance matrix

On the Sampling Problem for Kernel Quadrature

Figure 7. Implementation of test(R < Rmin ). A linear smoother (dashed line) was based on 5 consecutive (inverse) temperature
parameters tiâˆ’4 , tiâˆ’3 , tiâˆ’2 , tiâˆ’1 , ti . To begin it is required that 5 temperatures are considered (left panel). The algorithm terminates on
the first occasion when the linear smoother takes a positive gradient (right panel).

with diagonal element Ïƒ 2 . Then elementary manipulation
of Gaussian densities produces:
2
 Pd

j=1 xj âˆ’ yj
k(x, y) := exp âˆ’
2
l 
âˆš
= ( Ï€l)d Ï† x|y, Î£l/âˆš2
Pd
2 j=1 (xj âˆ’ yj )2
âˆ‡l k(x, y) :=
k(x, y)
l3

âˆš d
Î [k(Â·, x)] := ( Ï€l) N x|0, Î£Ïƒ + Î£l/âˆš2

âˆš
Î  âŠ— Î (k) := ( Ï€l)d N 0|0, Î£âˆš2Ïƒ + Î£l/âˆš2
A.5.2. D EPENDENCE ON PARAMETERS FOR THE
S IMULATION S TUDY
For the running illustration with f (x) = 1 + sin(x),
Î  = N(0, 1), Î 0 = N(0, Ïƒ 2 ) and k(x, x0 ) = exp(âˆ’(x âˆ’
x0 )2 /`2 ), we explored how the RMSE of KQ depends on
the choice of both Ïƒ and `. Here we go beyond the results presented in Fig. 2, which considered fixed n, to now
consider the simultaneous choice of both Ïƒ, ` for varying
n. Note that in these numerical experiments the kernel matrix inverse Kâˆ’1 was replaced with the regularised inverse
(K + Î»I)âˆ’1 that introduces a small â€˜nuggetâ€™ term Î» > 0
for stabilisation. Results, shown in Fig. 8, demonstrate two
principles that guided the methodological development in
this paper:
â€¢ Length scales ` that are â€˜too smallâ€™ to learn from n
samples do not permit good approximations fË† and
lead in practice to high RMSE. At the same time, if
` is taken to be â€˜too largeâ€™ then efficient approximation at size n will also be sacrificed. This is of course
well understood from a theoretical perspective and is
borne out in our empirical results. These results motivated extension of SMC-KQ to SMC-KQ-KL.
â€¢ In general the â€˜sweet spotâ€™, where Ïƒ and ` lead to minimal RMSE, is quite small. However, the problem of
optimal choice for Ïƒ and ` does not seem to become

more or less difficult as n increases. This suggests
that a method for selection of Ïƒ (and possibly also of
`) ought to be effective regardless of the number n of
states that will be used.
A.5.3. A DDITIONAL R ESULTS FOR THE S IMULATION
S TUDY
To understand whether the termination criterion of Sec. 3.5
was suitable (and, by extension, to examine the validity of
the convexity ansatz in Sec. 3.2), in Fig. 9 we presented
histograms for both estimated and actual optimal (inverse)
temperature parameter tâˆ— . Results supported the use of the
criterion, in the form described above for test.
In Fig. 10 reports the dependence of performance on the
choice of initial distribution Î 0 . There was relatively little influence on the RMSE obtained by the method for this
wide range of initial distribution, which supports the purported robustness of the method.
We also test the method on more complex integrands in Fig.
11: f (x) = 1 + sin(4Ï€x) and f (x) = 1 + sin(8Ï€x). These
are more challenging for KQ compared to the illustration
in the Main Text, since they are more difficult to interpolate due to their higher periodicity. However, SMC-KQ still
manages to adapt to the complexity of the integrand and
performs as well as the best importance sampling distribution (Ïƒ = 2).
As an extension, we also study the robustness to the dimensionality to the problem. In problem, we consider the generalisation of our main test function to f : Rd â†’ R given
Qd
by f (x) = 1 + j=1 sin(2Ï€xj ). Notice that the integral
can still be computed analytically and equals 1. We present
results for d = 2 and d = 3 in Fig. 12. These two cases are
more challenging for both the KQ and SMC-KQ methods,
since the higher dimension implies a slower convergence
rate. Once again, we notice that SMC-KQ manages to adapt
to the complexity of the problem at hand, and provides improved performance on simpler sampling distributions.

On the Sampling Problem for Kernel Quadrature

Figure 9. Histograms for the optimal (inverse) temperature parameter tâˆ— . Left: Estimate of tâˆ— provided under the termination
criterion of Sec. 3.5. Right: Estimate of tâˆ— obtained by estimating R over a grid for t âˆˆ [0, 1] and returning the global minimum.
The similarity of these histograms is supportive of the convexity
ansatz in Sec. 3.2.

Figure 10. Comparison of the performance of SMC-KQ on the
running illustration of Figs. 1 and 2 for varying initial distribution Î 0 = N(0, Ïƒ 2 ).

A.5.4. I MPLEMENTATION OF S TEIN â€™ S M ETHOD
Figure 8. Example of Fig. 2, continued. Here we consider the
simultaneous choice of sampling standard deviation Ïƒ and kernel
length-scale `, reporting empirical estimates for the estimated root
mean square integration error (over M = 300 repetitions) in each
case for sample size (a) n = 25 (top), (b) n = 50 (middle) and
(c) n = 75 (bottom).

Finally, we considered replacing the independent samples
xj âˆ¼ Î  with samples drawn from a quasi-random point sequence. Fig. 13 reports results where draws from N(0, 1)
were produced based on a Halton quasi-random number
generator. In this case, the performance is improved by
up to 10 orders of magnitude in MSE when the sampling is
done with respect to a range of tempered sampling distribution (here N(0, 32 )). This suggests that a SQMC approach
(Gerber and Chopin, 2015) could provide further improvement and this suggested for future work.

Following Oates et al. (2017) we considered the Stein operator
S[f ](Î¸) := [âˆ‡Î¸ + âˆ‡ log Ï€(Î¸)][f ](Î¸)
and denote the score function by uj (Î¸) = âˆ‡Î¸j log Ï€(Î¸).
Here Ï€ is the p.d.f. for Î . Applying the Stein operator to
each argument of a base kernel kb , and adding a constant,
gives produces the new kernel:

k(Î¸, Ï†)

:=

1+

d
X
j=1

[âˆ‡Î¸j âˆ‡Ï†j kb (Î¸, Ï†)
+uj (Î¸)âˆ‡Ï†j kb (Î¸, Ï†)
+uj (Ï†)âˆ‡Î¸j kb (Î¸, Ï†)
+uj (Î¸)uj (Ï†)kb (Î¸, Ï†)]

which we will use for our KQ estimator. Using integration
by parts, we can easily check that Î [k(Â·, Î¸)] = 1 and Î  âŠ—
Î (k) = 1. In this experiment, the base kernel was taken to
Pd
be Gaussian: kb (Î¸, Ï†) = exp(âˆ’ j=1 (Î¸j âˆ’ Ï†j )2 /`2j ). We

On the Sampling Problem for Kernel Quadrature

Figure 11. Performance of KQ and SMC-KQ on the integration
problem with f (x) = 1 + sin(4Ï€x) (top) and f (x) = 1 +
sin(8Ï€x) (bottom) integrated against N(0, 1). The SMC sampler was initiated with a N(0, 82 ) distribution. The kernel used
was Gaussian with length scales ` = 0.25 (top) and ` = 0.15
(bottom) each chosen to reflect the complexity of the functions.

obtained the derivatives:
dk(Î¸, Ï†)
dÎ¸j

= âˆ’

dk(Î¸, Ï†)
dÏ†j

=

dk(Î¸, Ï†)
dÎ¸j dÏ†j

=

2
(Î¸j âˆ’ Ï†j )k(Î¸, Ï†)
`2j

2
(Î¸j âˆ’ Ï†j )k(Î¸, Ï†)
`2j

2`2j âˆ’ 4(Î¸j âˆ’ Ï†j )2
k(Î¸, Ï†)
`4j

Furthermore, we can obtain expressions for the score function for posterior densities as follows:
uj (Î¸)

=

d
d
log Ï€(Î¸) +
log Ï€(y|Î¸).
dÎ¸j
dÎ¸j

Figure 12. Performance of Q
KQ and SMC-KQ on the integration
problem with f (x) = 1 + dj=1 sin(2Ï€xj ) integrated against a
N(0, I) distribution for d = 2 (top), d = 3 (middle) and d = 10
(bottom). The SMC sampler was initiated with a N(0, 82 I) distribution. The kernel
P used was a (multivariate) Gaussian kernel
k(x, y) = exp(âˆ’ dj=1 (xj âˆ’ yj )2 /`2j ) with the length scales
`1 = Â· Â· Â· = `d = 0.25 were used.

A.6. Algorithms and Implementation
A.6.1. SMC S AMPLER
In Alg. 2 the standard SMC scheme is presented. Resampling occurs when the effective sample size, kwkâˆ’2
2

drops below a fraction Ï of the total number N of particles.
In this work we took Ï = 0.95 which is a common default.

On the Sampling Problem for Kernel Quadrature

Algorithm 3 Markov Iteration
function Markov(x, Ï€, {(wj , xj )}N
j=1 )
input x (current state)
input Ï€ (density of invar. dist.)
xâˆ— âˆ¼ q(x, xâˆ— ; {(wj , xj )}N
j=1 ) (propose)
râ†

Ï€i (xâˆ— )q(xâˆ— , x; {(wj , xj )}N
j=1 )
Ï€i (x)q(x, xâˆ— ; {(wj , xj )}N
j=1 )

u âˆ¼ Unif(0, 1)
if u < r then
x â† xâˆ— (accept)
end ifreturn x (next state)
Figure 13. Comparison between KQ with xj âˆ¼ N(0, 1) independent and KQ with xj = Î¦âˆ’1 (uj ) where the {uj }n
j=1 are the first
n terms in the Halton sequence and Î¦ is the standard Gaussian
cumulative density function.

Algorithm 2 Sequential Monte Carlo Iteration
function SMC({(wj , xj )}N
j=1 , ti , tiâˆ’1 , Ï)
input {(wj , xj )}N
(particle
approx. to Î iâˆ’1 )
j=1
input ti (next inverse-temperature)
input tiâˆ’1 (previous inverse-temperature)
input Ï (re-sample threshold)
wj0 â† wj Ã— [Ï€(xj )/Ï€0 (xj )]ti âˆ’tiâˆ’1 (âˆ€j âˆˆ 1 : N )
w0 â† w0 /kw0 k1 (normalise weights)
if kw0 kâˆ’2
2 < N Â· Ï then
a âˆ¼ Multinom(w0 )
x0j â† xa(j) (re-sample âˆ€j âˆˆ 1 : N )
wj0 â† N âˆ’1 (reset weights âˆ€j âˆˆ 1 : N )
end if
x0j âˆ¼ Markov(x0j ; Î i , {(wj , xj )}N
j=1 ) (Markov update
âˆˆ 1 : N)
return {(wj0 , x0j )}N
j=1 (particle approx. to Î i )

Denote
q(x, Â·; {(wj , xj )}N
j=1 )

=

Âµ =

N(Â·; Âµ, Î£)
N
X

wj xj

j=1

Î£

=

N
X

wj (xj âˆ’ Âµ)(xj âˆ’ Âµ)> .

A.6.2. C HOICE OF T EMPERATURE S CHEDULE
Following Zhou et al. (2016) we employed an adaptive temperature schedule construction. This was based on the conditional effective sample size of the SMC particle set, estimated as follows:
Algorithm 4 Conditional Effective Sample Size
function CESS({(wj , xj )}N
j=1 , t)
input {(wj , xj )}N
(particle
approx. Î iâˆ’1 )
j=1
input t (candidate next inverse-temperature)
zj â† [Ï€(xj )/Ï€0 (xj )]ti âˆ’tiâˆ’1 (âˆ€j âˆˆ 1 : N )
P
2  P

N
N
2
Eâ†N
j=1 wj zj
j=1 wj zj
return E (estâ€™d. cond. ESS)
The specific construction for the temperature schedule is
detailed in Alg. 5 below and makes use of a Sequential
Least Squares Programming algorithm:
Algorithm 5 Adaptive Temperature Iteration
function temp({(wj , xj )}N
j=1 , tiâˆ’1 , Ï, âˆ†)
N
input {(wj , xj )}j=1 (particle approx. Î iâˆ’1 )
input tiâˆ’1 (current inverse-temperature)
input Ï (re-sample threshold)
input âˆ† (max. grid size, default âˆ† = 0.1)
t â† solve(CESS({(wj , xj )}N
j=1 , t) = N Â· Ï)
(binary search in [tiâˆ’1 , 1])
ti â† min{tiâˆ’1 + âˆ†, t} return ti (next inversetemperature)

j=1

The above standard adaptive independence proposal was
used within a Metropolis-Hastings Markov transition:

A.6.3. T ERMINATION C RITERION
For SMC-KQ we estimated an upper bound on the worst
case error in the unit ball of the Hilbert space H. This was
computed as follows, using a bootstrap algorithm:

On the Sampling Problem for Kernel Quadrature

Algorithm 6 Termination Criterion
function crit(Î , k, {xj }N
j=1 )
input Î  (target disn.)
input k (kernel)
input {xj }N
j=1 (collection of states)
2
R â†RR
0
e0 â† X Ã—X k(x, x0 )Î  âŠ— Î (dx Ã— dx0 ) (inâ€™l error)
for m = 1,. . . ,M do
N
xÌƒj âˆ¼ Unif({x
j }j=1 ) (âˆ€j âˆˆ 1 : n)
R
zj â† X k(Â·, xÌƒj )dÎ  (kâ€™l mean eval. âˆ€j âˆˆ 1 : n)
Kj,j 0 â† k(xÌƒj , xÌƒj 0 ) (kernel eval. âˆ€j, j 0 âˆˆ 1 : n)
w â† z T K âˆ’1 (KQ weights)
e2n â† w> Kw âˆ’ 2w> z + e20
R2 â† R2 + e2n M âˆ’1
end for
return R (estâ€™d error)

Note that this could be slightly improved using a weighted
bootstrap approach.
For SMC-KQ-KL an empirical upper bound on integration
error was estimated. This requires that the norm kf kH be
estimated, which was achieved as follows:
Algorithm 7 Termination Crit. + Kernel Learning
function crit-KL(f, Î , k, {xj }N
j=1 )
input f (integrand)
input Î  (target disn.)
input k (kernel)
input {xj }N
j=1 (collection of states)
2
R â†RR
0
e0 â† X Ã—X k(x, x0 )Î  âŠ— Î (dx Ã— dx0 ) (inâ€™l error)
for m = 1,. . . ,M do
xÌƒj âˆ¼ Unif({xj }N
j=1 ) (âˆ€j âˆˆ 1 : n)
fj â† fR(xÌƒj ) (function eval. âˆ€j âˆˆ 1 : n)
zj â† X k(Â·, xÌƒj )dÎ  (kâ€™l mean eval. âˆ€j âˆˆ 1 : n)
Kj,j 0 â† k(xÌƒj , xÌƒj 0 ) (kernel eval. âˆ€j, j 0 âˆˆ 1 : n)
w â† z T K âˆ’1 (KQ weights)
e2n â† w> Kw âˆ’ 2w> z + e20
R2 â† R2 + e2n M âˆ’1
end for
R
zj â† X k(Â·, xj )dÎ  (kernel mean eval. âˆ€j âˆˆ 1 : n)
Kj,j 0 â† k(xj , xj 0 ) (kernel eval. âˆ€j, j 0 âˆˆ 1 : n)
w â† z T K âˆ’1 (KQ weights)
S 2 â† R2 Ã— w> Kw return S (estâ€™d error bound)
In Alg. 7 the literal interpretation, that f is re-evaluated
on values of xj which have been previously examined, is
clearly inefficient. In practice such function evaluations
were cached and then do not contribute further to the total number of function evaluations that are required in the
algorithm.

A.6.4. K ERNEL L EARNING
A generic approach to select kernel parameters is the maximum marginal likelihood method:
Algorithm 8 Parameter Update
function kern-param(f , {xj }nj=1 , kÎ¸ )
input f (integrand evals.)
input {xj }nj=1 (associated states)
input kÎ¸ (parametric kernel)
Î¸0 â† arg minÎ¸ f > Kâˆ’1
Î¸ f + log |KÎ¸ | (numer. opt.)
(s.t. KÎ¸,j,j 0 = kÎ¸ (xj , xj 0 )) return Î¸0 (optimal params)

A.6.5. I MPLEMENTATION OF SMC-KQ-KL
Our final algorithm to present is the full implementation for
SMC-KQ-KL:

On the Sampling Problem for Kernel Quadrature

Algorithm 9 SMC for KQ with Kernel Learning
function SMC-KQ-KL(f, Î , kÎ¸ , Î 0 , Ï, n, N )
input f (integrand)
input Î  (target disn.)
input kÎ¸ (parametric kernel)
input Î 0 (reference disn.)
input Ï (re-sample threshold)
input n (num. func. evaluations)
input N (num. particles)
i â† 0; ti â† 0; Rmin â† âˆ
x0j âˆ¼ Î 0 (initialise states âˆ€j âˆˆ 1 : N )
wj0 â† N âˆ’1 (initialise weights âˆ€j âˆˆ 1 : N )
Î¸0 â† kern-param(f, {x0j }nj=1 ) (kernel params)
R â† crit-KL(f, Î , kÎ¸0 , {x0j }N
j=1 ) (estâ€™d error)
while test(R < Rmin ) and ti < 1 do
i â† i + 1; Rmin â† R; Î¸ â† Î¸0
0
0
N
{(wj , xj )}N
j=1 â† {(wj , xj )}j=1
N
ti â† temp({(wj , xj )}j=1 , tiâˆ’1 ) (next temp.)
N
{(wj0 , x0j )}N
j=1 â† SMC({(wj , xj )}j=1 , ti , tiâˆ’1 , Ï)
(next particle approx.)
Î¸0 â† kern-param(f, {x0j }nj=1 ) (kernel params)
R â† crit-KL(f, Î , kÎ¸0 , {x0j }N
j=1 ) (estâ€™d error)
end while
fj â† fR(xj ) (function eval. âˆ€j âˆˆ 1 : n)
zj â† X kÎ¸ (Â·, xj )dÎ  (kernel mean eval. âˆ€j âˆˆ 1 : n)
Kj,j 0 â† kÎ¸ (xj , xj 0 ) (kernel eval. âˆ€j, j 0 âˆˆ 1 : n)
Î Ì‚(f ) â† z > Kâˆ’1 f (eval. KQ estimator) return Î Ì‚(f )
(estimator)

As stated here, Alg. 9 is inefficient as function evaluations
that are produced in the kern-param and crit-KL
components are not included in the KQ estimator Î Ì‚(f ).
Thus a trivial modification is to store all function evaluations (fj , xj ) that are produced and to include all of these
in the ultimate KQ estimator. This was the approach taken
in our experiments that involved SMC-KQ-KL. However,
since it is somewhat cumbersome to include in the pseudocode, we have not made this explicit in the notation. Our
reported results are on a per-function-evaluation basis and
so we do adjust for this detail in our reported comparisons.

