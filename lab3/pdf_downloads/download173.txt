Consistency Analysis for Binary Classification Revisited

so that |B âˆ’ B 0 | â‰¤ 2|âˆ†p|.

A. Proofs from Section 2
A.1. Proof of Proposition 1
For the sake of readability, throughout the proof we abbreviate Î¦ = Î¦(u, v, p), Î¦0 = Î¦(u0 , v 0 , p0 ), and denote
âˆ†u = u âˆ’ u0 , âˆ†v = v âˆ’ v 0 , âˆ†p = p âˆ’ p0 . In this notation,
proving p-Lipschitzness for metric Î¦ amounts to showing
that:
0

|Î¦ âˆ’ Î¦ | â‰¤ Up |âˆ†u| + Vp |âˆ†v| + Pp |âˆ†p|,
for constants Up , Vp , Pp , which may only depend on p.
The following fact is going to be very useful in proving p-Lipschitzness. If the metric is of the rational form:
A(u,v,p)
+ C, where C is some constant,
Î¦(u, v, p) = B(u,v,p)
B(u, v, p) â‰¥ Gp for some positive constant Gp (which
may depend on p), and |Î¦(u, v, p)| â‰¤ Î¦max for some constant Î¦max , it suffices to check p-Lipschitzness of numerator and denominator separately. Indeed, using shorthand
notation A = A(u, v, p), A0 = A(u0 , v 0 , p0 ), and similarly
for B, B 0 :
0

0

u
. Follows from
c) Jaccard similarity Î¦(u, v, p) = p+vâˆ’u
the rational form of the metric, since A(u, v, p) = u,
B(u, v, p) = p + v âˆ’ u, C = 0, Î¦max = 1, Gp = p,
and the p-Lipschitzness of A(u, v, p) and B(u, v, p) is
trivial to show by the triangle inequality.

. Exploiting the
d) G-mean Î¦(u, v, p) = u(1âˆ’vâˆ’p+u)
p(1âˆ’p)
rational form of the metric, we have A(u, v, p) =
u(1 âˆ’ v âˆ’ p + u), B(u, v, p) = p(1 âˆ’ p), C = 0,
Î¦max = 1, Gp = p(1 âˆ’ p). The p-Lipschitzness of B
was shown above for AM measure. As for A:
A âˆ’ A0 = (1 âˆ’ v âˆ’ p + u)(u âˆ’ u0 )
+ u0 (u âˆ’ p âˆ’ v âˆ’ u0 âˆ’ p0 âˆ’ v 0 )
= (1 âˆ’ v âˆ’ p + u)âˆ†u + u0 (âˆ†u âˆ’ âˆ†v âˆ’ âˆ†p),
and hence the p-Lipschitzness follows by triangle inequality and the fact that |1 âˆ’ v âˆ’ p + u| â‰¤ 2 and
|u0 | â‰¤ 1.

0

A
A
A
0
A âˆ’ A0 + B
Aâˆ’ B
0B
0 B âˆ’ B0 B
=
Î¦âˆ’Î¦ =
B
B
A âˆ’ A0
A0 B âˆ’ B 0
=
+ 0
,
B
B
B
hence:
|A âˆ’ A0 | Î¦max 0
|Î¦ âˆ’ Î¦0 | â‰¤
+
|B âˆ’ B|.
Gp
Gp
0

a) Accuracy Î¦(u, v, p) = 1 âˆ’ v âˆ’ p + 2u. We have:
Î¦ âˆ’ Î¦0 â‰¤ 2âˆ†u âˆ’ âˆ†v âˆ’ âˆ†p,
so that by triangle inequality:
|Î¦ âˆ’ Î¦0 | â‰¤ 2|âˆ†u| + |âˆ†v| + |âˆ†p|.
Hence, the statement follows with Up = 2, Vp =
Pp = 1.
vpâˆ’u
b) AM Î¦(u, v, p) = 1 âˆ’ 2p(1âˆ’p)
. We can use the result on the rational metric by noting that A(u, v, p) =
u âˆ’ vp, B(u, v, p) = B(p) = 2p(1 âˆ’ p), C = 1,
Î¦max = 1, Gp = 2p(1 âˆ’ p). We can now check the
p-Lipschitzness of A and B separately:

A âˆ’ A0 = u âˆ’ vp âˆ’ u0 + v 0 p0

e) AUC (vâˆ’u)(pâˆ’u)
p(1âˆ’p) . Exploiting the rational form of the
metric, we have A(u, v, p) = (v âˆ’ u)(p âˆ’ u) and
B(u, v, p) = p(1 âˆ’ p). The p-Lipschitzness of B was
shown above for AM measure; as for A:
A âˆ’ A0 = (v âˆ’ u)(p âˆ’ u) âˆ’ (v 0 âˆ’ u0 )(p âˆ’ u)
+ (v 0 âˆ’ u0 )(p âˆ’ u) âˆ’ (v 0 âˆ’ u0 )(p0 âˆ’ u0 )
= (âˆ†v âˆ’ âˆ†u)(p âˆ’ u) + (v 0 âˆ’ u0 )(âˆ†p âˆ’ âˆ†u),
and hence the p-Lipschitzness follows by triangle inequality and the fact that |pâˆ’u| â‰¤ 1 and |v 0 âˆ’u0 | â‰¤ 1.
f) Linear-fractional metric of the form:
Î¦(u, v, p) =

a1 + a2 u + a3 v + a4 p
,
b1 + b2 u + b3 v + b4 p

as long as the denominator is bounded from below by
some positive constant Gp . This follows immediately
from the rational form of the metric, as the numerator A(u, v, p) and denominator B(u, v, p) are linear
functions of (u, v, p), so showing p-Lipschitzness of
A(u, v, p) and B(u, v, p) is straightforward.

= âˆ†u + (vp0 âˆ’ vp) + (v 0 p0 âˆ’ vp0 )
= âˆ†u âˆ’ vâˆ†p âˆ’ p0 âˆ†v,
and since |v| â‰¤ 1, |p0 | â‰¤ 1, p-Lipschitzness follows
from triangle inequality. For the denominator,
B âˆ’ B 0 = 2p(1 âˆ’ p) âˆ’ 2p0 (1 âˆ’ p0 )
= 2(p âˆ’ p0 ) + 2(p02 âˆ’ p2 )
= 2(1 âˆ’ p0 âˆ’ p)(p âˆ’ p0 ),

B. Proofs from Section 3.1
B.1. Proof of Lemma 1
We fix classifier h and use a shorthand notation u, v, u
b, vb
to denote u(h), v(h), u
b(h), vb(h). Due to the Lipschitz assumption:
|Î¦(u, v, p)âˆ’Î¦(b
u, vb, pb)| â‰¤ Up |uâˆ’b
u|+Vp |vâˆ’b
v |+Pp |pâˆ’ pb|.

Consistency Analysis for Binary Classification Revisited

Fixing x = (x1 , . . . , xn ) and taking expectation with respect to y = (y1 , . . . , yn ) conditioned on x, we have:


u, vb, pb)|
Ey|x |Î¦(u, v, p) âˆ’ Î¦(b




â‰¤ Up Ey|x |u âˆ’ u
b| + Vp |v âˆ’ vb| + Pp Ey|x |p âˆ’ pb| .
Denote:

n

pe = Ey|x [b
p] =

and similarly, with probability 1 âˆ’ Î´/4,
s


log 4Î´
|u âˆ’ u
e| â‰¤ 2Ex Rn (HÎ· ) +
,
2n

1X
Î·(xi ),
n i=1
n

u
e = Ey|x [b
u] =

Similarly, using standard Rademacher complexity arguments (see, e.g. Mohri et al., 2012), we have, uniformly
over all h âˆˆ H, with probability 1 âˆ’ Î´/4,
s


log 4Î´
,
|v âˆ’ vb| â‰¤ 2Ex Rn (H) +
2n

1X
h(xi )Î·(xi )
n i=1

We have:




Ey|x |p âˆ’ pb| = Ey|x |p âˆ’ pe + pe âˆ’ pb|


â‰¤ |p âˆ’ pe| + Ey|x |e
p âˆ’ pb|
hp
i
(e
p âˆ’ pb)2
= |p âˆ’ pe| + Ey|x
q


â‰¤ |p âˆ’ pe| + Ey|x (e
p âˆ’ pb)2
r
q
1
= |p âˆ’ pe| + Vary|x (b
p) â‰¤ |p âˆ’ pe| +
,
4n

where HÎ· = {h Â· Î· : h âˆˆ H}, and:

n

1  X

Ïƒi h(xi )
Rn (H) = EÏƒ sup 
n
hâˆˆH
i=1
is the Rademacher complexity6 of H. Furthermore, if we
i)
let zi âˆˆ {âˆ’1, 1}, i = 1, . . . , n, with Pr(zi = 1) = 1+Î·(x
,
2
so that E [zi ] = Î·(xi ), we have:
n
X
i=1

so that:




u, vb, pb)  â‰¤ Up |u âˆ’ u
e| + Vp |v âˆ’ vb|
Î¦(u, v, p) âˆ’ Ey|x Î¦(b
+ Pp |p âˆ’ pe| +

Up + V p
âˆš .
2 n

We will now show that under the class of thresholded functions H specified in the statement of the theorem to which
h belongs, all the terms on the right-hand side are well controlled. The rest of the proof follows in a straightforward
way from Hoeffdingâ€™s inequality and Vapnik-Chervonenkis
bounds, except for minor, technical details, which are included for completeness.
We first apply Hoeffdingâ€™s inequality to say that with probability at least 1 âˆ’ Î´/2,
s
log 4Î´
.
|p âˆ’ pe| â‰¤
2n

n
hX

i
Ïƒi h(xi )zi ,

i=1

so that:

where the second inequality follows from
âˆš Jensenâ€™s inequality applied to a concave function x 7â†’ x. In an analogous
way, one can show that:
r
r


u
1
b| â‰¤ |u âˆ’ u
e| +
â‰¤ |u âˆ’ u
e| +
.
Ey|x |u âˆ’ u
4n
4n
Furthermore, using the convexity of the absolute value
function, Jensenâ€™s inequality implies:


 

u, vb, pb) 
Î¦(u, v, p) âˆ’ Ey|x Î¦(b


u, vb, pb)| ,
â‰¤ Ey|x |Î¦(u, v, p) âˆ’ Î¦(b

Ïƒi h(xi )Î·(xi ) = Ez


n
i
1  h X

Rn (HÎ· ) = EÏƒ sup Ez
Ïƒi h(xi )zi 
hâˆˆH n
i=1

n


X
1

Ïƒi h(xi )zi 
â‰¤ EÏƒ,z sup 
n
hâˆˆH
i=1

n

1  X

= EÏƒ sup 
Ïƒi h(xi ) = Rn (H),
hâˆˆH n i=1
where the inequality is due to Jensenâ€™s inequality applied to
convex functions | Â· | and sup{Â·}, and the second equality is
due to the fact that Ïƒi zi and Ïƒi are distributed in the same
way.
Thus choosing Lp = max{Up , Vp , Pp }, with probability
1 âˆ’ Î´, uniformly over all h âˆˆ H,






u, vb, pb)  â‰¤ 4Lp Ex Rn (H)
Î¦(u, v, p) âˆ’ Ey|x Î¦(b
s
log 4Î´
Lp
+âˆš .
+ 3Lp
2n
n
Now, if H is the class of threshold functions on Î·, its
growth function (Mohri et al., 2012) is equal to m + 1, and
thus we have7 :
r
2 log(n + 1)
Rn (H) â‰¤
,
n
6

Variables Ïƒi , i = 1, . . . , n, are i.i.d. Rademacher variables
distributed according to P(Ïƒi = 1) = P(Ïƒi = âˆ’1) = 12 .
7
We could alternatively use the fact that VC-dimension of H
is 1, which would give a bound with log(n + 1) replaced by 1 +
log(n).

Consistency Analysis for Binary Classification Revisited

so that with probability 1âˆ’Î´, uniformly over all h âˆˆ H, we
get the bound in the statement of the theorem. The proof is
complete.
âˆš
Lower bound. The dependence OÌƒ(1/ n) on the sample size stated in Lemma 1 cannot be improved in general.
To see this, take a metric Î¦(u, v, p) = u, p-Lipschitzness
of which is trivial to show. Choose h(x)
Pn = 1 for all
1
yi . Hence,
x.
Then,
u(h)
=
p,
while
u
b
(h)
=

 n i=1






u, vb, pb)  = p âˆ’ pe , where pe =
Î¦(u, v, p) âˆ’ Ey|x Î¦(b
Pn
1
p] = p. Assume that Î·(x) follows
i=1 Î·(xi ) and Ex [e
n
a binomial distribution with P(Î·(x) = 1) = P(Î·(x) =
0) = 12 . Denote |p âˆ’ pe| by Z. By Khinchine inp
âˆš
equality, E [Z] â‰¥ 2c E [Z 2 ] = c/ n for some constant c > 0. Furthermore, by Paley-Zygmund inequality
2
2
P(Z > E [Z] /2) â‰¥ (E[Z])
4E[Z 2 ] â‰¥ c . Hence, with constant
probability,



c

u, vb, pb)  â‰¥ âˆš ,
Î¦(u, v, p) âˆ’ Ey|x Î¦(b
2 n
âˆš
for some c > 0, which shows that the rate OÌƒ(1/ n) cannot
be improved.
B.2. Proof of Theorem 1
First, note that for a given P, p-Lipschitzness implies
that Î¦(u, v, p) is continuous as a function of (u, v). Let
H = {hÎ· | hÎ· = 1Î·(x)â‰¥Î· , Î· âˆˆ [0, 1]} be the set of binary threshold functions on Î·(x). By Assumption 1, u(hÎ· )
and v(hÎ· ) are continuous in the threshold Î·, and hence the
maximizer of Î¦(u, v, p) over H exists due to compactness
of the domain of Î·. The existence of the maximizer, together with Assumption 1 and TP monotonicity implies by
(Narasimhan et al., 2014a, Lemma 11) that hâˆ—PU âˆˆ H, i.e.
the optimal PU classifier is a threshold function.8 .
For any given x = (x1 , . . . , xn ), let hâˆ—ETU (x) be the optimal ETU classifier. By TP monotonicity of Î¨, (Natarajan
et al., 2016, Theorem 1) implies that hâˆ—ETU (x) satisfies:
max {Î·(xi ) :

i=1,...,n

hâˆ—ETU (xi )

= 0}

is a threshold function on Î·(x) with threshold Ï„ âˆ— , i.e.
hâˆ—ETU âˆˆ H.
To conclude, with probability one, hâˆ—ETU (x), hâˆ—PU âˆˆ H.
q 4
q
log Î´
L
âˆšp
+
3L
Now, define /2 = 4Lp 2 log(n+1)
p
n
2n + n .
Then, with probability 1 âˆ’ Î´ (over the random choice of x),
Î¦(u(hâˆ—ETU (x)), v(hâˆ—ETU (x)), p)
â‰¤ Î¦(u(hâˆ—PU ), v(hâˆ—PU ), p)


â‰¤ Ey|x Î¦(b
u(hâˆ—PU ), vb(hâˆ—PU ), pb) + /2


â‰¤ Ey|x Î¦(b
u(hâˆ—ETU (x)), vb(hâˆ—ETU (x)), pb) + /2,
â‰¤ Î¦(u(hâˆ—ETU (x)), v(hâˆ—ETU (x)), p) + ,

where we used Lemma 1 twice in the second and fourth
inequality. Hence, with probability 1 âˆ’ Î·,


Î¦(u(hâˆ—ETU (x)), v(hâˆ—ETU (x)), p)


âˆ’ Î¦(u(hâˆ—PU ), v(hâˆ—PU ), p) â‰¤ .
Using analogous argument, one can show that with probability 1 âˆ’ Î´,




u(hâˆ—ETU (x)), vb(hâˆ—ETU (x)), pb)
Ey|x Î¦(b

 
âˆ’ Ey|x Î¦(b
u(hâˆ—PU ), vb(hâˆ—PU ), pb)  â‰¤ ,
which finishes the proof.
B.3. Finite Sample Regime: Proof of Theorem 2
The PU-optimal classifier is:
hâˆ—PU = argmax Î¦Prec (u(h), v(h), p) = argmax
h

h

u(h)
.
v(h) + Î±

Proposition 2.
(
hâˆ—PU (x)

=

1, if x âˆˆ X1 ,
0, else .

â‰¤ min {Î·(xi ) : hâˆ—ETU (xi ) = 1}.
i=1,...,n

However, by Assumption 1, Î·(xi ) 6= Î·(xj ) for all
i 6= j with probability one, so that the condition above
is satisfied with strict inequality, and hence there exists
Ï„ âˆ— , which is between max{Î·(xi ) : hâˆ—ETU (xi ) = 0} and
min{Î·(xi ) : hâˆ—ETU (xi ) = 1}. This means that hâˆ—ETU (x)
8

Lemma 11 of Narasimhan et al. (2014a) requires that the PU
maximizer within H is hÎ· for some Î· âˆˆ (0, 1). However, we do
not impose this constraint here because the lemma can easily be
extended to the case Î· âˆˆ [0, 1] under our assumption that Î·(x)
has a density over [0, 1].

Proof. Note that for the defined hâˆ—PU classifier, we have
u(hâˆ—PU ) = v(hâˆ—PU ) = P(X1 ), and
Î¦Prec (u(hâˆ—PU ), v(hâˆ—PU ), p) =

P(X1 )
.
P(X1 ) + Î±

Firstly, observe that for any candidate optimal classifier
h0 , it must hold that h0 (x) = 0 for all x âˆˆ X3 (otherwise the metric strictly decreases). Now, suppose there exists a classifier h0 6= hâˆ—PU which has strictly higher utility than hâˆ—PU . Then, it must be that h0 (x) = 1 for all

Consistency Analysis for Binary Classification Revisited

x âˆˆ X2 . We have, u(h0 ) = P(X1 ) + P(X2 )(1 âˆ’
and v(h0 ) = P(X1 ) + P(X2 ). So:
Î¦Prec (u(h0 ), v(h0 ), p) =

âˆš

Î±)

âˆš
P(X1 ) + P(X2 )(1 âˆ’ Î±)
.
P(X1 ) + P(X2 ) + Î±

But for the chosen small value of Î±, we can show the contradiction that:
Î¦Prec (u(h0 ), v(h0 ), p) < Î¦Prec (u(hâˆ—PU ), v(hâˆ—PU ), p).
Therefore,

hâˆ—PU

We know from Proposition 2 that hâˆ—PU sets the labels corresponding to indices in the set I2 to 0. Now let us examine
what happens in the case of ETU, when
âˆš labels have mild
noise (i.e. with some small probability , the label of an
instance from X2 can be 0), at optimality. Consider a candidate optimal solution s0 that behaves exactly like hâˆ—PU , i.e.
s0j = 0 for all j âˆˆ I2 , for some 1 â‰¤ k â‰¤ |I2 |.
Then, âˆ†(s0I2 , y I2 ) = 0, so:
Eyâˆ¼P(.|x) Î¦Prec (s0 , y) =

as stated is indeed optimal.

We see from the above constructed example that the PU
optimal classifier assigns negative labels to 50% of the data
which are highly likely to belong to the positive class. PU
is sensitive to label noise if the metric is less stable as implied by the high p-Lipschitz constant. Next, we show that
ETU is relatively more robust.
Given a set of instances x = {x1 , x2 , . . . , xn }, recall that
the ETU-optimal assignments can be computed as:

Eyâˆ¼P(.|x) Î¦Prec (s00 , y) =

Note that the predictions coincide with that of hâˆ—PU on these
indices.
Proof. Let Ii = {j : xj âˆˆ Xi }, for i = 1, 2, 3. Note that
the optimal value at the solution sâˆ— is given by:
P
âˆ—
âˆ—
jâˆˆI1 sj + âˆ†(sI2 , y I2 )
âˆ—
P
Eyâˆ¼P(.|x) Î¦Prec (s , y) = P
,
âˆ—
âˆ—
jâˆˆI1 âˆªI3 sj +
jâˆˆI2 sj + Î±n
(2)
where sâˆ—I2 indicates the optimal assignments corresponding
to indices in I2 and âˆ†(sâˆ—I2 , y I2 ) is a quantity that depends
only on indices in I2 , and is given by:
X

âˆ†(sâˆ—I2 , y I2 ) =
yI2

P(yI2 )hyI2 , sâˆ—I2 i

(3)

|I1 | + k(1 âˆ’ )
.
|I1 | + k + Î±n

(5)

Comparing equations (4) and (5), we have that if:

hâˆ—ETU (x) = sâˆ— := argmax Eyâˆ¼P(.|x) Î¦Prec (s, y) .
Proposition 3. On the subset of instances in x that have
deterministic labels, the ETU-optimal predictions satisfy:
(
1, if x âˆˆ X1 ,
âˆ—
âˆ—
hETU (xj ) = sj =
0, if x âˆˆ X3 .

(4)

Now, consider another candidate solution s00 that is equal
to s0 , but has a value of 1 corresponding to a subset of indices j1 , j2 , . . . , jk âˆˆ I2 . The value of this solution can be
shown to be:

<
sâˆˆ{0,1}n

|I1 |
.
|I1 | + Î±n

Î±n
,
|I1 | + Î±n

(6)

then s00 is a strictly better solution than s0 . In particular, as
(5) is mononotic in k, the optimal choice is k = |I2 |. This
immediately leads to the following corollary.
Corollary 1.

1. If |I2 | = 0, then
hâˆ—ETU (x) := sâˆ— = hâˆ—PU (x) .

2. Otherwise, if  <

Î±
1+Î± ,

then

hâˆ—ETU (x) := sâˆ— 6= hâˆ—PU (x) .
In particular, hâˆ—ETU assigns label 1 to all instances that
are overwhelmingly positive under P, corresponding
to indices I2 , whereas hâˆ—PU assigns label 0.
3. If |I1 | = 0, but |I2 | > 0 then for any 0 <  < 1,
hâˆ—ETU (x) := sâˆ— 6= hâˆ—PU (x) := 0 .
Noteâˆšthat  < Î±/(1 + Î±) does not hold for our choice of
 = Î±. However, case 3 in Corollary 1 is sufficient to establish the bound in Theorem 2, when P(X2 ) is very large.

âˆˆ{0,1}|I2 |

Fixing the optimal predictions for indices corresponding to
I2 , the value
P (2) isâˆ— maximized by maximizing the numerator
term
jâˆˆI1 sj and minimizing the denominator term
P
âˆ—
s
jâˆˆI1 âˆªI3 j . This is achieved precisely when the optimal solution satisfies the statement in the proposition. The
proof is complete.

C. Proofs for Section 4.1
Fix a binary classifier h : X â†’ {0, 1} and let the input
sample x = (x1 , . . . , xn ) be generated i.i.d. from P. For
the sake of clarity, abbreviate Î·(xi ) = Î·i and h(xi ) = hi ,
i = 1, . . . , n. In the proofs of Lemma 2 and Lemma 3 we
will use the following:

Consistency Analysis for Binary Classification Revisited

â€¢ Empirical quantities:
n

u
b(h) =

n

where we used the
of labels yi , i = 1, . . . , n.
 independence

1
Similarly, Ey|x (b
p âˆ’ pe)2 is at most 4n
, which in total
gives:

n

1X
1X
1X
hi yi , vb(h) =
hi , pb =
yi ,
n i=1
n i=1
n i=1

h
i A
e)> âˆ‡2 Î¦(e
e) â‰¤ .
Ey|x (b
zâˆ’z
z )(b
zâˆ’z
n

â€¢ Semi-empirical quantities:
n

u
e(h) =

Using a lower bound âˆ’A on the second-order derivatives
and performing a similar chain of reasoning, one also gets:

n

1X
hi Î·i ,
n i=1

and

pe =

1X
Î·i
n i=1

h
i
A
e)> âˆ‡2 Î¦(e
e) â‰¥ âˆ’ .
Ey|x (b
zâˆ’z
z )(b
zâˆ’z
n

(we do not define ve(h), as it would the same as vb(h)).

From that we have:

Note that:


u
e(h) = Ey|x u
b(h) ,

and



A
,
kEy|x Î¦(b
z ) âˆ’ Î¦(e
z )k â‰¤
2n

pe = Ey|x [b
p] .

b = (b
We will jointly denote z
u(h), pb), and similarly
e = (e
z
u(h), pe). We will also abbreviate Î¦(b
z) =
Î¦(b
u(h), vb(h), pb) and similarly for Î¦(e
z ).

which is exactly what was to be shown.

C.1. Proof of Lemma 2

Assume Î¦ is three-times differentiable, with all partial
third-order derivatives bounded by B. Taylor expanding
e up to the third order gives:
Î¦(b
z ) around point z

Assume Î¦ is two-times differentiable, with all partial
second-order derivatives bounded by A. Taylor expanding
e up to the second order gives:
Î¦(b
z ) around point z
e)
Î¦(b
z ) = Î¦(e
z ) + âˆ‡Î¦(e
z )> (b
zâˆ’z
1
e)> âˆ‡2 Î¦(z)(b
e)
zâˆ’z
zâˆ’z
+ (b
2
b and z
e. Note that Ey|x [b
e, so
for some z between z
z] = z
that:
h
i
e) = 0.
z )> (b
zâˆ’z
Ey|x âˆ‡Î¦(e
Furthermore, note that:
e)> âˆ‡2 Î¦(z)(b
e)
(b
zâˆ’z
zâˆ’z
=

âˆ‡2uu (b
uâˆ’

2

u
e) +
2

2âˆ‡2up (b
uâˆ’

u
e)(b
p âˆ’ pe) +

âˆ‡2pp (b
pâˆ’

2

2

pe)

â‰¤ A (b
uâˆ’u
e) + 2|(b
uâˆ’u
e)(b
p âˆ’ pe)| + (b
p âˆ’ pe)

2
2
â‰¤ 2A (b
uâˆ’u
e) + (b
p âˆ’ pe) ,

where we used elementary inequality ab â‰¤ a2 + b2 , and
âˆ‡2uu , âˆ‡2up , âˆ‡2pp denote the second-order derivatives evaluated at some z = (u, p). Hence:
h
i
e)> âˆ‡2 Î¦(e
e)
Ey|x (b
zâˆ’z
z )(b
zâˆ’z

h
i
h
i
â‰¤ 2A Ey|x (b
uâˆ’u
e)2 + Ey|x (b
p âˆ’ pe)2 .
Since u
b is the empirical average over
e is its
 and u
 n labels
expectation (over the labels), Ey|x (b
uâˆ’u
e)2 is the vari1
ance of u
b, which is at most 4n
, because u
b âˆˆ [0, 1]:
n
n
1 X
1X
1
var(b
u) = 2
var(hi yi ) â‰¤
hi Î·i (1 âˆ’ Î·i ) â‰¤
,
n i=1
n i=1
4n

C.2. Proof of Lemma 3

e)
Î¦(b
z ) = Î¦(e
z ) + âˆ‡Î¦(e
z )> (b
zâˆ’z
1
e)> âˆ‡2 Î¦(e
e)
+ (b
zâˆ’z
z )(b
zâˆ’z
2
2
âˆ‚ 3 Î¦(z)
1 X
+
(b
zÎ± âˆ’ zeÎ± )(b
zÎ² âˆ’ zeÎ² )(b
zÎ³ âˆ’ zeÎ³ ),
6
âˆ‚zÎ± âˆ‚zÎ² âˆ‚zÎ³
Î±,Î²,Î³=1

b and z
e. First note that Ey|x [b
e,
for some z between z
z] = z
so that:
h
i
e) = 0.
z )> (b
zâˆ’z
Ey|x âˆ‡Î¦(e
Furthermore,
h
i
e)> Î¦(e
e)
Ey|x âˆ‡2 (b
zâˆ’z
z )(b
zâˆ’z
 

2
>
e)(b
e)
= Ey|x tr âˆ‡ Î¦(e
z )(b
zâˆ’z
zâˆ’z


= tr âˆ‡2 Î¦(e
z )Î£ ,


e)(b
e)> is the covariance mawhere Î£ = Ey|x (b
zâˆ’z
zâˆ’z
bâˆ’z
e. By independence of examples,
trix of z
"
#
n
1 X
hi (yi âˆ’ Î·i )2 hi (yi âˆ’ Î·i )2
Î£= 2
Ey |x
hi (yi âˆ’ Î·i )2
(yi âˆ’ Î·i )2
n i=1 i i


n
1 X
hi hi
= 2
Î·i (1 âˆ’ Î·i )
,
hi 1
n
i=1

so that:


tr âˆ‡2 Î¦(e
z )Î£ = (âˆ‡2uu + 2âˆ‡2up )su + âˆ‡2pp sp ,

Consistency Analysis for Binary Classification Revisited

where:
sp :=

n
1 X
Î·i (1 âˆ’ Î·i ),
n2 i=1

su :=

n
1 X
hi Î·i (1 âˆ’ Î·i ),
n2 i=1

âˆ‡2uu , âˆ‡2up , âˆ‡2pp

and
denote be the second-order derivative
terms evaluated at (e
u, pe). Thus, to finish the proof, we
only need to show that the first order term is bounded by
B âˆ’3/2
. To this end, note that for any numbers ai , bijk ,
3n
such that |bijk | â‰¤ B, i, j, k = 1, . . . , 2:
X
X
bijk ai aj ak â‰¤ B
|ai ||aj ||ak | = B(|a1 | + |a2 |)3 .
ijk



uâˆ’u
e|3 , we conclude that
Using similar bound for Ey|x |b
the third-order term is bounded by B3 nâˆ’3/2 . Bounding the
third-order derivatives from below by âˆ’B, and using similar reasoning gives a lower bound of the same value. This
finishes the proof.
C.3. Proof of Theorem 3
Abbreviating Î¦(h)
Î¦a (h) = Î¦appr (h):

=



Ey|x Î¦(b
u(h), vb(h), pb) and

Î¦(hâˆ—ETU ) âˆ’ Î¦(hâˆ—a ) = Î¦(hâˆ—ETU ) âˆ’ Î¦a (hâˆ—ETU )
{z
}
|
â‰¤

B
3n3/2

2B
Î¦a (hâˆ—ETU ) âˆ’ Î¦a (hâˆ—a ) + Î¦a (hâˆ—a ) âˆ’ Î¦(hâˆ—a ) â‰¤ 3/2 ,
{z
} 3n
{z
} |
|

ijk

â‰¤0

â‰¤

By HoÌˆlderâ€™s inequality,
2
X

|ai | â‰¤

X
2

|ai |3

1/3

22/3 ,

i=1

i=1

so that:


B(|a1 | + |a2 | + |a3 |)3 â‰¤ 4B |a1 |3 + |a2 |3 + |a3 |3 .
Hence, if we bound:
3

âˆ‚ Î¦(z)
â‰¤ B,
âˆ‚zÎ± âˆ‚zÎ² âˆ‚zÎ³
P2
the third-order term 16 Î±,Î²,Î³=1 . . . is bounded by:

2B 
|b
uâˆ’u
e|3 + |b
p âˆ’ pe|3
3




p âˆ’ pe|3 . By
uâˆ’u
e|3 and Ey|x |b
We now bound Ey|x |b
Cauchy-Schwarz inequality,
h
i q

q


p âˆ’ pe|3 â‰¤ Ey|x (b
Ey|x |b
p âˆ’ pe)4 Ey|x (b
p âˆ’ pe)2 .
Before, we already showed that
h
i
1
Ey|x (b
p âˆ’ pe)2 â‰¤
.
4n
 
Denote ai = yi âˆ’ Î·i , and let Âµk = Ey|x aki . Using Âµ1 =
0, we have:
h
i
1 X
Ey|x (b
p âˆ’ pe)4 = 4
ai aj ak a`
n
i,j,k,`

1 
= 4 nÂµ4 + 3n(n âˆ’ 1)Âµ22 .
n


1
3
Since Âµ2 â‰¤ 14 and Âµ4 â‰¤ 12
, Ey|x (b
p âˆ’ pe)4 â‰¤ 16n
2 , and
thus:
h
i âˆš3
1
3
Ey|x |b
p âˆ’ pe| â‰¤
nâˆ’3/2 â‰¤ nâˆ’3/2 .
8
4

B
3n3/2

where the bounds shown in the inequalities are from
Lemma 3.
C.4. Derivation of the approximation algorithm for
FÎ² -measure
2

)u
Recall that FÎ² (u, v, p) = (1+Î²
Î² 2 p+v . The seconder order
derivatives with respect to u and p are:

âˆ‚ 2 FÎ²
âˆ‚ 2 FÎ² âˆ’Î² 2 (1 + Î² 2 ) âˆ‚ 2 FÎ² 2Î² 4 (1 + Î² 2 )u
=
0,
=
,
=
.
âˆ‚u2
âˆ‚uâˆ‚p
(Î² 2 p + v)2
âˆ‚p2
(Î² 2 p + v)3
To optimize Î¦appr (h), we first sort observations according
to Î·(xi ). Then we precompute:
n

pe =

1X
Î·(xi ),
n i=1

pevar =

n
1 X
Î·(xi )(1 âˆ’ Î·(xi )).
n2 i=1

Next, for each k = 0, 1, . . . , n, we precompute:
k
k
1X
k k
1 X
k
u
e =
Î·(xi ), vb = , u
e = 2
Î·(xi )(1âˆ’Î·(xi )).
n i=1
n var
n i=1
k

We then choose k for which the ETU approximation:
(1 + Î² 2 )e
uk
Î² 2 (1 + Î² 2 ) k
Î² 4 (1 + Î² 2 )e
uk
âˆ’
u
e
+
pevar ,
var
Î² 2 pe + nk
(Î² 2 pe + nk )2
(Î² 2 pe + nk )3
is maximized. The maximization can be done in time linear in O(n), so the most expensive operation is sorting the
instances.

D. Additional material to Section 4.2
Let x = (x1 , . . . , xn ) be the input sample (test set) of size
n generated i.i.d. from P. Given x and a function Î·b : X â†’
[0, 1], let


b
h = argmax Eyâˆ¼bÎ·(x) Î¦(b
u(h), vb(h), pb) .
{z
}
b |
hâˆˆH
b ETU (h)
=:Î¦

Consistency Analysis for Binary Classification Revisited

be the classifier returned by the ETU procedure upon receiving the input sample x. Likewise, let:


hâˆ— = argmax Eyâˆ¼Î·(x) Î¦(b
u(h), vb(h), pb) ,
{z
}
b |
hâˆˆH
=:Î¦ETU (h)

b We want to bound the
be the optimalhETU classifier in H.
i
difference Ex |Î¦ETU (b
h) âˆ’ Î¦ETU (hâˆ— )| . By the definition
of hâˆ— , Î¦ETU (b
h) â‰¤ Î¦ETU (hâˆ— ) for any x, and thus:
h
i
Ex |Î¦ETU (b
h) âˆ’ Î¦ETU (hâˆ— )|
h
i


= Ex Î¦ETU (hâˆ— ) âˆ’ Ex Î¦ETU (b
h)
h
i


b ETU (hâˆ— )
= Ex Î¦ETU (hâˆ— ) âˆ’ Ex Î¦
h
i
h
i
b ETU (hâˆ— ) âˆ’ Ex Î¦
b ETU (b
+ Ex Î¦
h)
{z
}
|



Now, the term E |b
v âˆ’ v| is well-controlled and was
q
1
shown in the proof of Lemma 1 to be at most 4n
as the
expected deviation of the empirical average of [0, 1]-valued
random variable
 0 from
 its mean.
 0 Thus it remains to bound
the terms E |b
p âˆ’ p| and E |b
u âˆ’ u| . Define:
n
 
1X
pe0 = Ey|x pb0 =
Î·b(xi ),
n i=1
n
 0
1X
u
e0 = Ey|x u
b =
h(xi )b
Î· (xi ),
n i=1
 


pÎ·b = Ex pe0 = E Î·b(x) .
 0


uÎ·b = Ex u
e = E h(x)b
Î· (x) .

â‰¤0

i
h
i
b ETU (b
+ Ex Î¦
h) âˆ’ Ex Î¦ETU (b
h)
 h
i

b ETU (h) .
â‰¤ 2 sup Ex Î¦ETU (h) âˆ’ Î¦

for some constant c. Moreover, using p-Lipschitzness of Î¦,
we have:
h
i
 0

u âˆ’ u|
E Î¦(u, v, p) âˆ’ Î¦(b
u0 , vb, pb0 ) â‰¤ Up E |b


 0

+ Vp E |b
v âˆ’ v| + Pp E |b
p âˆ’ p| .

h

(7)

We decompose:

b
hâˆˆH

|p âˆ’ pb0 | â‰¤ |p âˆ’ pÎ·b| + |pÎ·b âˆ’ pe0 | + |e
p0 âˆ’ pb0 |

Now, fix some classifier h and input sample x. We
let u
b(h), vb(h), pb denote the random variables generated
according to Î· (for fixed x), while u
b0 (h), pb0 (h) denote
random variables
Pn generated according to Î·b; for instance,
u
b0 (h) = n1 i=1 h(xi )yi , where yi âˆ¼ Î·b(xi ). Using this
notation, we have:



0
As before,
 0 we 0use
 the fact that Ex |pÎ·b âˆ’ pe | , as well
p âˆ’ pb | are both the expected deviations of the
as Ey|x |e
empirical averages of [0, 1]-valued randomqvariables from



Î¦ETU (h) = Ey|x Î¦(b
u(h), vb(h), pb) ,


b ETU (h) = Ey|x Î¦(b
Î¦
u0 (h), vb(h), pb0 )

 0

1
E |b
p âˆ’ p| â‰¤ |p âˆ’ pÎ·b| + âˆš .
n

(note that vb(h) does not depend on Î·b or Î·, we vb0 (h) =
vb(h)). We now bound the term under sup in (7):
 h
i

b ETU (h) 
Ex Î¦ETU (h) âˆ’ Î¦
h
i
â‰¤ E Î¦(b
u, vb, pb) âˆ’ Î¦(b
u0 , vb, pb0 )
h
i
â‰¤ E Î¦(b
u, vb, pb) âˆ’ Î¦(u, v, p)
h
i
+ E Î¦(u, v, p) âˆ’ Î¦(b
u0 , vb, pb0 ) ,
where the first inequality is due to Jensenâ€™s inequality applied to a convex function x 7â†’ |x|, the all expectations
except for the first line are joint with respect to (x, y),
and for the sake of clarity we drop the dependence on h
in u
b(h), vb(h), u
b0 (h). Now, it follow from Lemma 1 that:
r
h
i
log n


E Î¦(b
u, vb, pb) âˆ’ Î¦(u, v, p) â‰¤ c
,
n

their means, and therefore are bounded by

1
4n .

Hence:

Using analogous way of reasoning, one gets:
 0

1
E |b
u âˆ’ u| â‰¤ |u âˆ’ uÎ·b| + âˆš .
n
Putting it all together, we get:
 h
i

b ETU (h) 
Ex Î¦ETU (h) âˆ’ Î¦
r
log n
0
â‰¤c
+ Up |u(h) âˆ’ uÎ·b(h)| + Pp |p âˆ’ pÎ·b|,
n
for some constant c0 . Using (7), we finally get:
r
h
i
log n
âˆ—
0
Ex Î¦ETU (b
h) âˆ’ Î¦ETU (h ) â‰¤ c
+ Pp |p âˆ’ pÎ·b|
n
+ sup Up |u(h) âˆ’ uÎ·b(h)|,
b
hâˆˆH

which was to be shown.

Consistency Analysis for Binary Classification Revisited

E. Isotron Algorithm (Kalai & Sastry, 2009)
Here we include the Isotron Algorithm of (Kalai & Sastry, 2009) for completeness. The second update step is the
Pool of Adjacent Violators (PAV) routine, which solves the
isotonic regression problem:
uâˆ—1 , uâˆ—2 , . . . , uâˆ—n = arg

min

u1 â‰¤u2 â‰¤Â·Â·Â·â‰¤un

n
X
(yi âˆ’ ui )2 ,
i=1

where the instances are assumed to be sorted according to
their scores wT x (using w obtained in first update step of
the iteration). This is a convex quadratic program and can
be solved efficiently. The output link function u of the Algorithm is a piecewise linear estimate.
Algorithm 2 The Isotron algorithm (Kalai & Sastry, 2009).
Input: Training data {(xi , yi )}ni=1 , iterations T
Output: wT , uT
w0 â† 0
u0 â† z 7â†’ min(max(0, 2 Â· z + 1), 1)
for t = 1, 2, . . . , T do
Pn
wt â† wtâˆ’1 + n1 i=1 (yi âˆ’ utâˆ’1 (hwtâˆ’1 , xi i)) Â· xi
ut â† PAV({hwt , xi i, yi })
end for

