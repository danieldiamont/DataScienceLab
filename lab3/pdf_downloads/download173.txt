Consistency Analysis for Binary Classification Revisited

so that |B − B 0 | ≤ 2|∆p|.

A. Proofs from Section 2
A.1. Proof of Proposition 1
For the sake of readability, throughout the proof we abbreviate Φ = Φ(u, v, p), Φ0 = Φ(u0 , v 0 , p0 ), and denote
∆u = u − u0 , ∆v = v − v 0 , ∆p = p − p0 . In this notation,
proving p-Lipschitzness for metric Φ amounts to showing
that:
0

|Φ − Φ | ≤ Up |∆u| + Vp |∆v| + Pp |∆p|,
for constants Up , Vp , Pp , which may only depend on p.
The following fact is going to be very useful in proving p-Lipschitzness. If the metric is of the rational form:
A(u,v,p)
+ C, where C is some constant,
Φ(u, v, p) = B(u,v,p)
B(u, v, p) ≥ Gp for some positive constant Gp (which
may depend on p), and |Φ(u, v, p)| ≤ Φmax for some constant Φmax , it suffices to check p-Lipschitzness of numerator and denominator separately. Indeed, using shorthand
notation A = A(u, v, p), A0 = A(u0 , v 0 , p0 ), and similarly
for B, B 0 :
0

0

u
. Follows from
c) Jaccard similarity Φ(u, v, p) = p+v−u
the rational form of the metric, since A(u, v, p) = u,
B(u, v, p) = p + v − u, C = 0, Φmax = 1, Gp = p,
and the p-Lipschitzness of A(u, v, p) and B(u, v, p) is
trivial to show by the triangle inequality.

. Exploiting the
d) G-mean Φ(u, v, p) = u(1−v−p+u)
p(1−p)
rational form of the metric, we have A(u, v, p) =
u(1 − v − p + u), B(u, v, p) = p(1 − p), C = 0,
Φmax = 1, Gp = p(1 − p). The p-Lipschitzness of B
was shown above for AM measure. As for A:
A − A0 = (1 − v − p + u)(u − u0 )
+ u0 (u − p − v − u0 − p0 − v 0 )
= (1 − v − p + u)∆u + u0 (∆u − ∆v − ∆p),
and hence the p-Lipschitzness follows by triangle inequality and the fact that |1 − v − p + u| ≤ 2 and
|u0 | ≤ 1.

0

A
A
A
0
A − A0 + B
A− B
0B
0 B − B0 B
=
Φ−Φ =
B
B
A − A0
A0 B − B 0
=
+ 0
,
B
B
B
hence:
|A − A0 | Φmax 0
|Φ − Φ0 | ≤
+
|B − B|.
Gp
Gp
0

a) Accuracy Φ(u, v, p) = 1 − v − p + 2u. We have:
Φ − Φ0 ≤ 2∆u − ∆v − ∆p,
so that by triangle inequality:
|Φ − Φ0 | ≤ 2|∆u| + |∆v| + |∆p|.
Hence, the statement follows with Up = 2, Vp =
Pp = 1.
vp−u
b) AM Φ(u, v, p) = 1 − 2p(1−p)
. We can use the result on the rational metric by noting that A(u, v, p) =
u − vp, B(u, v, p) = B(p) = 2p(1 − p), C = 1,
Φmax = 1, Gp = 2p(1 − p). We can now check the
p-Lipschitzness of A and B separately:

A − A0 = u − vp − u0 + v 0 p0

e) AUC (v−u)(p−u)
p(1−p) . Exploiting the rational form of the
metric, we have A(u, v, p) = (v − u)(p − u) and
B(u, v, p) = p(1 − p). The p-Lipschitzness of B was
shown above for AM measure; as for A:
A − A0 = (v − u)(p − u) − (v 0 − u0 )(p − u)
+ (v 0 − u0 )(p − u) − (v 0 − u0 )(p0 − u0 )
= (∆v − ∆u)(p − u) + (v 0 − u0 )(∆p − ∆u),
and hence the p-Lipschitzness follows by triangle inequality and the fact that |p−u| ≤ 1 and |v 0 −u0 | ≤ 1.
f) Linear-fractional metric of the form:
Φ(u, v, p) =

a1 + a2 u + a3 v + a4 p
,
b1 + b2 u + b3 v + b4 p

as long as the denominator is bounded from below by
some positive constant Gp . This follows immediately
from the rational form of the metric, as the numerator A(u, v, p) and denominator B(u, v, p) are linear
functions of (u, v, p), so showing p-Lipschitzness of
A(u, v, p) and B(u, v, p) is straightforward.

= ∆u + (vp0 − vp) + (v 0 p0 − vp0 )
= ∆u − v∆p − p0 ∆v,
and since |v| ≤ 1, |p0 | ≤ 1, p-Lipschitzness follows
from triangle inequality. For the denominator,
B − B 0 = 2p(1 − p) − 2p0 (1 − p0 )
= 2(p − p0 ) + 2(p02 − p2 )
= 2(1 − p0 − p)(p − p0 ),

B. Proofs from Section 3.1
B.1. Proof of Lemma 1
We fix classifier h and use a shorthand notation u, v, u
b, vb
to denote u(h), v(h), u
b(h), vb(h). Due to the Lipschitz assumption:
|Φ(u, v, p)−Φ(b
u, vb, pb)| ≤ Up |u−b
u|+Vp |v−b
v |+Pp |p− pb|.

Consistency Analysis for Binary Classification Revisited

Fixing x = (x1 , . . . , xn ) and taking expectation with respect to y = (y1 , . . . , yn ) conditioned on x, we have:


u, vb, pb)|
Ey|x |Φ(u, v, p) − Φ(b




≤ Up Ey|x |u − u
b| + Vp |v − vb| + Pp Ey|x |p − pb| .
Denote:

n

pe = Ey|x [b
p] =

and similarly, with probability 1 − δ/4,
s


log 4δ
|u − u
e| ≤ 2Ex Rn (Hη ) +
,
2n

1X
η(xi ),
n i=1
n

u
e = Ey|x [b
u] =

Similarly, using standard Rademacher complexity arguments (see, e.g. Mohri et al., 2012), we have, uniformly
over all h ∈ H, with probability 1 − δ/4,
s


log 4δ
,
|v − vb| ≤ 2Ex Rn (H) +
2n

1X
h(xi )η(xi )
n i=1

We have:




Ey|x |p − pb| = Ey|x |p − pe + pe − pb|


≤ |p − pe| + Ey|x |e
p − pb|
hp
i
(e
p − pb)2
= |p − pe| + Ey|x
q


≤ |p − pe| + Ey|x (e
p − pb)2
r
q
1
= |p − pe| + Vary|x (b
p) ≤ |p − pe| +
,
4n

where Hη = {h · η : h ∈ H}, and:

n

1  X

σi h(xi )
Rn (H) = Eσ sup 
n
h∈H
i=1
is the Rademacher complexity6 of H. Furthermore, if we
i)
let zi ∈ {−1, 1}, i = 1, . . . , n, with Pr(zi = 1) = 1+η(x
,
2
so that E [zi ] = η(xi ), we have:
n
X
i=1

so that:




u, vb, pb)  ≤ Up |u − u
e| + Vp |v − vb|
Φ(u, v, p) − Ey|x Φ(b
+ Pp |p − pe| +

Up + V p
√ .
2 n

We will now show that under the class of thresholded functions H specified in the statement of the theorem to which
h belongs, all the terms on the right-hand side are well controlled. The rest of the proof follows in a straightforward
way from Hoeffding’s inequality and Vapnik-Chervonenkis
bounds, except for minor, technical details, which are included for completeness.
We first apply Hoeffding’s inequality to say that with probability at least 1 − δ/2,
s
log 4δ
.
|p − pe| ≤
2n

n
hX

i
σi h(xi )zi ,

i=1

so that:

where the second inequality follows from
√ Jensen’s inequality applied to a concave function x 7→ x. In an analogous
way, one can show that:
r
r


u
1
b| ≤ |u − u
e| +
≤ |u − u
e| +
.
Ey|x |u − u
4n
4n
Furthermore, using the convexity of the absolute value
function, Jensen’s inequality implies:


 

u, vb, pb) 
Φ(u, v, p) − Ey|x Φ(b


u, vb, pb)| ,
≤ Ey|x |Φ(u, v, p) − Φ(b

σi h(xi )η(xi ) = Ez


n
i
1  h X

Rn (Hη ) = Eσ sup Ez
σi h(xi )zi 
h∈H n
i=1

n


X
1

σi h(xi )zi 
≤ Eσ,z sup 
n
h∈H
i=1

n

1  X

= Eσ sup 
σi h(xi ) = Rn (H),
h∈H n i=1
where the inequality is due to Jensen’s inequality applied to
convex functions | · | and sup{·}, and the second equality is
due to the fact that σi zi and σi are distributed in the same
way.
Thus choosing Lp = max{Up , Vp , Pp }, with probability
1 − δ, uniformly over all h ∈ H,






u, vb, pb)  ≤ 4Lp Ex Rn (H)
Φ(u, v, p) − Ey|x Φ(b
s
log 4δ
Lp
+√ .
+ 3Lp
2n
n
Now, if H is the class of threshold functions on η, its
growth function (Mohri et al., 2012) is equal to m + 1, and
thus we have7 :
r
2 log(n + 1)
Rn (H) ≤
,
n
6

Variables σi , i = 1, . . . , n, are i.i.d. Rademacher variables
distributed according to P(σi = 1) = P(σi = −1) = 12 .
7
We could alternatively use the fact that VC-dimension of H
is 1, which would give a bound with log(n + 1) replaced by 1 +
log(n).

Consistency Analysis for Binary Classification Revisited

so that with probability 1−δ, uniformly over all h ∈ H, we
get the bound in the statement of the theorem. The proof is
complete.
√
Lower bound. The dependence Õ(1/ n) on the sample size stated in Lemma 1 cannot be improved in general.
To see this, take a metric Φ(u, v, p) = u, p-Lipschitzness
of which is trivial to show. Choose h(x)
Pn = 1 for all
1
yi . Hence,
x.
Then,
u(h)
=
p,
while
u
b
(h)
=

 n i=1






u, vb, pb)  = p − pe , where pe =
Φ(u, v, p) − Ey|x Φ(b
Pn
1
p] = p. Assume that η(x) follows
i=1 η(xi ) and Ex [e
n
a binomial distribution with P(η(x) = 1) = P(η(x) =
0) = 12 . Denote |p − pe| by Z. By Khinchine inp
√
equality, E [Z] ≥ 2c E [Z 2 ] = c/ n for some constant c > 0. Furthermore, by Paley-Zygmund inequality
2
2
P(Z > E [Z] /2) ≥ (E[Z])
4E[Z 2 ] ≥ c . Hence, with constant
probability,



c

u, vb, pb)  ≥ √ ,
Φ(u, v, p) − Ey|x Φ(b
2 n
√
for some c > 0, which shows that the rate Õ(1/ n) cannot
be improved.
B.2. Proof of Theorem 1
First, note that for a given P, p-Lipschitzness implies
that Φ(u, v, p) is continuous as a function of (u, v). Let
H = {hη | hη = 1η(x)≥η , η ∈ [0, 1]} be the set of binary threshold functions on η(x). By Assumption 1, u(hη )
and v(hη ) are continuous in the threshold η, and hence the
maximizer of Φ(u, v, p) over H exists due to compactness
of the domain of η. The existence of the maximizer, together with Assumption 1 and TP monotonicity implies by
(Narasimhan et al., 2014a, Lemma 11) that h∗PU ∈ H, i.e.
the optimal PU classifier is a threshold function.8 .
For any given x = (x1 , . . . , xn ), let h∗ETU (x) be the optimal ETU classifier. By TP monotonicity of Ψ, (Natarajan
et al., 2016, Theorem 1) implies that h∗ETU (x) satisfies:
max {η(xi ) :

i=1,...,n

h∗ETU (xi )

= 0}

is a threshold function on η(x) with threshold τ ∗ , i.e.
h∗ETU ∈ H.
To conclude, with probability one, h∗ETU (x), h∗PU ∈ H.
q 4
q
log δ
L
√p
+
3L
Now, define /2 = 4Lp 2 log(n+1)
p
n
2n + n .
Then, with probability 1 − δ (over the random choice of x),
Φ(u(h∗ETU (x)), v(h∗ETU (x)), p)
≤ Φ(u(h∗PU ), v(h∗PU ), p)


≤ Ey|x Φ(b
u(h∗PU ), vb(h∗PU ), pb) + /2


≤ Ey|x Φ(b
u(h∗ETU (x)), vb(h∗ETU (x)), pb) + /2,
≤ Φ(u(h∗ETU (x)), v(h∗ETU (x)), p) + ,

where we used Lemma 1 twice in the second and fourth
inequality. Hence, with probability 1 − η,


Φ(u(h∗ETU (x)), v(h∗ETU (x)), p)


− Φ(u(h∗PU ), v(h∗PU ), p) ≤ .
Using analogous argument, one can show that with probability 1 − δ,




u(h∗ETU (x)), vb(h∗ETU (x)), pb)
Ey|x Φ(b

 
− Ey|x Φ(b
u(h∗PU ), vb(h∗PU ), pb)  ≤ ,
which finishes the proof.
B.3. Finite Sample Regime: Proof of Theorem 2
The PU-optimal classifier is:
h∗PU = argmax ΦPrec (u(h), v(h), p) = argmax
h

h

u(h)
.
v(h) + α

Proposition 2.
(
h∗PU (x)

=

1, if x ∈ X1 ,
0, else .

≤ min {η(xi ) : h∗ETU (xi ) = 1}.
i=1,...,n

However, by Assumption 1, η(xi ) 6= η(xj ) for all
i 6= j with probability one, so that the condition above
is satisfied with strict inequality, and hence there exists
τ ∗ , which is between max{η(xi ) : h∗ETU (xi ) = 0} and
min{η(xi ) : h∗ETU (xi ) = 1}. This means that h∗ETU (x)
8

Lemma 11 of Narasimhan et al. (2014a) requires that the PU
maximizer within H is hη for some η ∈ (0, 1). However, we do
not impose this constraint here because the lemma can easily be
extended to the case η ∈ [0, 1] under our assumption that η(x)
has a density over [0, 1].

Proof. Note that for the defined h∗PU classifier, we have
u(h∗PU ) = v(h∗PU ) = P(X1 ), and
ΦPrec (u(h∗PU ), v(h∗PU ), p) =

P(X1 )
.
P(X1 ) + α

Firstly, observe that for any candidate optimal classifier
h0 , it must hold that h0 (x) = 0 for all x ∈ X3 (otherwise the metric strictly decreases). Now, suppose there exists a classifier h0 6= h∗PU which has strictly higher utility than h∗PU . Then, it must be that h0 (x) = 1 for all

Consistency Analysis for Binary Classification Revisited

x ∈ X2 . We have, u(h0 ) = P(X1 ) + P(X2 )(1 −
and v(h0 ) = P(X1 ) + P(X2 ). So:
ΦPrec (u(h0 ), v(h0 ), p) =

√

α)

√
P(X1 ) + P(X2 )(1 − α)
.
P(X1 ) + P(X2 ) + α

But for the chosen small value of α, we can show the contradiction that:
ΦPrec (u(h0 ), v(h0 ), p) < ΦPrec (u(h∗PU ), v(h∗PU ), p).
Therefore,

h∗PU

We know from Proposition 2 that h∗PU sets the labels corresponding to indices in the set I2 to 0. Now let us examine
what happens in the case of ETU, when
√ labels have mild
noise (i.e. with some small probability , the label of an
instance from X2 can be 0), at optimality. Consider a candidate optimal solution s0 that behaves exactly like h∗PU , i.e.
s0j = 0 for all j ∈ I2 , for some 1 ≤ k ≤ |I2 |.
Then, ∆(s0I2 , y I2 ) = 0, so:
Ey∼P(.|x) ΦPrec (s0 , y) =

as stated is indeed optimal.

We see from the above constructed example that the PU
optimal classifier assigns negative labels to 50% of the data
which are highly likely to belong to the positive class. PU
is sensitive to label noise if the metric is less stable as implied by the high p-Lipschitz constant. Next, we show that
ETU is relatively more robust.
Given a set of instances x = {x1 , x2 , . . . , xn }, recall that
the ETU-optimal assignments can be computed as:

Ey∼P(.|x) ΦPrec (s00 , y) =

Note that the predictions coincide with that of h∗PU on these
indices.
Proof. Let Ii = {j : xj ∈ Xi }, for i = 1, 2, 3. Note that
the optimal value at the solution s∗ is given by:
P
∗
∗
j∈I1 sj + ∆(sI2 , y I2 )
∗
P
Ey∼P(.|x) ΦPrec (s , y) = P
,
∗
∗
j∈I1 ∪I3 sj +
j∈I2 sj + αn
(2)
where s∗I2 indicates the optimal assignments corresponding
to indices in I2 and ∆(s∗I2 , y I2 ) is a quantity that depends
only on indices in I2 , and is given by:
X

∆(s∗I2 , y I2 ) =
yI2

P(yI2 )hyI2 , s∗I2 i

(3)

|I1 | + k(1 − )
.
|I1 | + k + αn

(5)

Comparing equations (4) and (5), we have that if:

h∗ETU (x) = s∗ := argmax Ey∼P(.|x) ΦPrec (s, y) .
Proposition 3. On the subset of instances in x that have
deterministic labels, the ETU-optimal predictions satisfy:
(
1, if x ∈ X1 ,
∗
∗
hETU (xj ) = sj =
0, if x ∈ X3 .

(4)

Now, consider another candidate solution s00 that is equal
to s0 , but has a value of 1 corresponding to a subset of indices j1 , j2 , . . . , jk ∈ I2 . The value of this solution can be
shown to be:

<
s∈{0,1}n

|I1 |
.
|I1 | + αn

αn
,
|I1 | + αn

(6)

then s00 is a strictly better solution than s0 . In particular, as
(5) is mononotic in k, the optimal choice is k = |I2 |. This
immediately leads to the following corollary.
Corollary 1.

1. If |I2 | = 0, then
h∗ETU (x) := s∗ = h∗PU (x) .

2. Otherwise, if  <

α
1+α ,

then

h∗ETU (x) := s∗ 6= h∗PU (x) .
In particular, h∗ETU assigns label 1 to all instances that
are overwhelmingly positive under P, corresponding
to indices I2 , whereas h∗PU assigns label 0.
3. If |I1 | = 0, but |I2 | > 0 then for any 0 <  < 1,
h∗ETU (x) := s∗ 6= h∗PU (x) := 0 .
Note√that  < α/(1 + α) does not hold for our choice of
 = α. However, case 3 in Corollary 1 is sufficient to establish the bound in Theorem 2, when P(X2 ) is very large.

∈{0,1}|I2 |

Fixing the optimal predictions for indices corresponding to
I2 , the value
P (2) is∗ maximized by maximizing the numerator
term
j∈I1 sj and minimizing the denominator term
P
∗
s
j∈I1 ∪I3 j . This is achieved precisely when the optimal solution satisfies the statement in the proposition. The
proof is complete.

C. Proofs for Section 4.1
Fix a binary classifier h : X → {0, 1} and let the input
sample x = (x1 , . . . , xn ) be generated i.i.d. from P. For
the sake of clarity, abbreviate η(xi ) = ηi and h(xi ) = hi ,
i = 1, . . . , n. In the proofs of Lemma 2 and Lemma 3 we
will use the following:

Consistency Analysis for Binary Classification Revisited

• Empirical quantities:
n

u
b(h) =

n

where we used the
of labels yi , i = 1, . . . , n.
 independence

1
Similarly, Ey|x (b
p − pe)2 is at most 4n
, which in total
gives:

n

1X
1X
1X
hi yi , vb(h) =
hi , pb =
yi ,
n i=1
n i=1
n i=1

h
i A
e)> ∇2 Φ(e
e) ≤ .
Ey|x (b
z−z
z )(b
z−z
n

• Semi-empirical quantities:
n

u
e(h) =

Using a lower bound −A on the second-order derivatives
and performing a similar chain of reasoning, one also gets:

n

1X
hi ηi ,
n i=1

and

pe =

1X
ηi
n i=1

h
i
A
e)> ∇2 Φ(e
e) ≥ − .
Ey|x (b
z−z
z )(b
z−z
n

(we do not define ve(h), as it would the same as vb(h)).

From that we have:

Note that:


u
e(h) = Ey|x u
b(h) ,

and



A
,
kEy|x Φ(b
z ) − Φ(e
z )k ≤
2n

pe = Ey|x [b
p] .

b = (b
We will jointly denote z
u(h), pb), and similarly
e = (e
z
u(h), pe). We will also abbreviate Φ(b
z) =
Φ(b
u(h), vb(h), pb) and similarly for Φ(e
z ).

which is exactly what was to be shown.

C.1. Proof of Lemma 2

Assume Φ is three-times differentiable, with all partial
third-order derivatives bounded by B. Taylor expanding
e up to the third order gives:
Φ(b
z ) around point z

Assume Φ is two-times differentiable, with all partial
second-order derivatives bounded by A. Taylor expanding
e up to the second order gives:
Φ(b
z ) around point z
e)
Φ(b
z ) = Φ(e
z ) + ∇Φ(e
z )> (b
z−z
1
e)> ∇2 Φ(z)(b
e)
z−z
z−z
+ (b
2
b and z
e. Note that Ey|x [b
e, so
for some z between z
z] = z
that:
h
i
e) = 0.
z )> (b
z−z
Ey|x ∇Φ(e
Furthermore, note that:
e)> ∇2 Φ(z)(b
e)
(b
z−z
z−z
=

∇2uu (b
u−

2

u
e) +
2

2∇2up (b
u−

u
e)(b
p − pe) +

∇2pp (b
p−

2

2

pe)

≤ A (b
u−u
e) + 2|(b
u−u
e)(b
p − pe)| + (b
p − pe)

2
2
≤ 2A (b
u−u
e) + (b
p − pe) ,

where we used elementary inequality ab ≤ a2 + b2 , and
∇2uu , ∇2up , ∇2pp denote the second-order derivatives evaluated at some z = (u, p). Hence:
h
i
e)> ∇2 Φ(e
e)
Ey|x (b
z−z
z )(b
z−z

h
i
h
i
≤ 2A Ey|x (b
u−u
e)2 + Ey|x (b
p − pe)2 .
Since u
b is the empirical average over
e is its
 and u
 n labels
expectation (over the labels), Ey|x (b
u−u
e)2 is the vari1
ance of u
b, which is at most 4n
, because u
b ∈ [0, 1]:
n
n
1 X
1X
1
var(b
u) = 2
var(hi yi ) ≤
hi ηi (1 − ηi ) ≤
,
n i=1
n i=1
4n

C.2. Proof of Lemma 3

e)
Φ(b
z ) = Φ(e
z ) + ∇Φ(e
z )> (b
z−z
1
e)> ∇2 Φ(e
e)
+ (b
z−z
z )(b
z−z
2
2
∂ 3 Φ(z)
1 X
+
(b
zα − zeα )(b
zβ − zeβ )(b
zγ − zeγ ),
6
∂zα ∂zβ ∂zγ
α,β,γ=1

b and z
e. First note that Ey|x [b
e,
for some z between z
z] = z
so that:
h
i
e) = 0.
z )> (b
z−z
Ey|x ∇Φ(e
Furthermore,
h
i
e)> Φ(e
e)
Ey|x ∇2 (b
z−z
z )(b
z−z
 

2
>
e)(b
e)
= Ey|x tr ∇ Φ(e
z )(b
z−z
z−z


= tr ∇2 Φ(e
z )Σ ,


e)(b
e)> is the covariance mawhere Σ = Ey|x (b
z−z
z−z
b−z
e. By independence of examples,
trix of z
"
#
n
1 X
hi (yi − ηi )2 hi (yi − ηi )2
Σ= 2
Ey |x
hi (yi − ηi )2
(yi − ηi )2
n i=1 i i


n
1 X
hi hi
= 2
ηi (1 − ηi )
,
hi 1
n
i=1

so that:


tr ∇2 Φ(e
z )Σ = (∇2uu + 2∇2up )su + ∇2pp sp ,

Consistency Analysis for Binary Classification Revisited

where:
sp :=

n
1 X
ηi (1 − ηi ),
n2 i=1

su :=

n
1 X
hi ηi (1 − ηi ),
n2 i=1

∇2uu , ∇2up , ∇2pp

and
denote be the second-order derivative
terms evaluated at (e
u, pe). Thus, to finish the proof, we
only need to show that the first order term is bounded by
B −3/2
. To this end, note that for any numbers ai , bijk ,
3n
such that |bijk | ≤ B, i, j, k = 1, . . . , 2:
X
X
bijk ai aj ak ≤ B
|ai ||aj ||ak | = B(|a1 | + |a2 |)3 .
ijk



u−u
e|3 , we conclude that
Using similar bound for Ey|x |b
the third-order term is bounded by B3 n−3/2 . Bounding the
third-order derivatives from below by −B, and using similar reasoning gives a lower bound of the same value. This
finishes the proof.
C.3. Proof of Theorem 3
Abbreviating Φ(h)
Φa (h) = Φappr (h):

=



Ey|x Φ(b
u(h), vb(h), pb) and

Φ(h∗ETU ) − Φ(h∗a ) = Φ(h∗ETU ) − Φa (h∗ETU )
{z
}
|
≤

B
3n3/2

2B
Φa (h∗ETU ) − Φa (h∗a ) + Φa (h∗a ) − Φ(h∗a ) ≤ 3/2 ,
{z
} 3n
{z
} |
|

ijk

≤0

≤

By Hölder’s inequality,
2
X

|ai | ≤

X
2

|ai |3

1/3

22/3 ,

i=1

i=1

so that:


B(|a1 | + |a2 | + |a3 |)3 ≤ 4B |a1 |3 + |a2 |3 + |a3 |3 .
Hence, if we bound:
3

∂ Φ(z)
≤ B,
∂zα ∂zβ ∂zγ
P2
the third-order term 16 α,β,γ=1 . . . is bounded by:

2B 
|b
u−u
e|3 + |b
p − pe|3
3




p − pe|3 . By
u−u
e|3 and Ey|x |b
We now bound Ey|x |b
Cauchy-Schwarz inequality,
h
i q

q


p − pe|3 ≤ Ey|x (b
Ey|x |b
p − pe)4 Ey|x (b
p − pe)2 .
Before, we already showed that
h
i
1
Ey|x (b
p − pe)2 ≤
.
4n
 
Denote ai = yi − ηi , and let µk = Ey|x aki . Using µ1 =
0, we have:
h
i
1 X
Ey|x (b
p − pe)4 = 4
ai aj ak a`
n
i,j,k,`

1 
= 4 nµ4 + 3n(n − 1)µ22 .
n


1
3
Since µ2 ≤ 14 and µ4 ≤ 12
, Ey|x (b
p − pe)4 ≤ 16n
2 , and
thus:
h
i √3
1
3
Ey|x |b
p − pe| ≤
n−3/2 ≤ n−3/2 .
8
4

B
3n3/2

where the bounds shown in the inequalities are from
Lemma 3.
C.4. Derivation of the approximation algorithm for
Fβ -measure
2

)u
Recall that Fβ (u, v, p) = (1+β
β 2 p+v . The seconder order
derivatives with respect to u and p are:

∂ 2 Fβ
∂ 2 Fβ −β 2 (1 + β 2 ) ∂ 2 Fβ 2β 4 (1 + β 2 )u
=
0,
=
,
=
.
∂u2
∂u∂p
(β 2 p + v)2
∂p2
(β 2 p + v)3
To optimize Φappr (h), we first sort observations according
to η(xi ). Then we precompute:
n

pe =

1X
η(xi ),
n i=1

pevar =

n
1 X
η(xi )(1 − η(xi )).
n2 i=1

Next, for each k = 0, 1, . . . , n, we precompute:
k
k
1X
k k
1 X
k
u
e =
η(xi ), vb = , u
e = 2
η(xi )(1−η(xi )).
n i=1
n var
n i=1
k

We then choose k for which the ETU approximation:
(1 + β 2 )e
uk
β 2 (1 + β 2 ) k
β 4 (1 + β 2 )e
uk
−
u
e
+
pevar ,
var
β 2 pe + nk
(β 2 pe + nk )2
(β 2 pe + nk )3
is maximized. The maximization can be done in time linear in O(n), so the most expensive operation is sorting the
instances.

D. Additional material to Section 4.2
Let x = (x1 , . . . , xn ) be the input sample (test set) of size
n generated i.i.d. from P. Given x and a function ηb : X →
[0, 1], let


b
h = argmax Ey∼bη(x) Φ(b
u(h), vb(h), pb) .
{z
}
b |
h∈H
b ETU (h)
=:Φ

Consistency Analysis for Binary Classification Revisited

be the classifier returned by the ETU procedure upon receiving the input sample x. Likewise, let:


h∗ = argmax Ey∼η(x) Φ(b
u(h), vb(h), pb) ,
{z
}
b |
h∈H
=:ΦETU (h)

b We want to bound the
be the optimalhETU classifier in H.
i
difference Ex |ΦETU (b
h) − ΦETU (h∗ )| . By the definition
of h∗ , ΦETU (b
h) ≤ ΦETU (h∗ ) for any x, and thus:
h
i
Ex |ΦETU (b
h) − ΦETU (h∗ )|
h
i


= Ex ΦETU (h∗ ) − Ex ΦETU (b
h)
h
i


b ETU (h∗ )
= Ex ΦETU (h∗ ) − Ex Φ
h
i
h
i
b ETU (h∗ ) − Ex Φ
b ETU (b
+ Ex Φ
h)
{z
}
|



Now, the term E |b
v − v| is well-controlled and was
q
1
shown in the proof of Lemma 1 to be at most 4n
as the
expected deviation of the empirical average of [0, 1]-valued
random variable
 0 from
 its mean.
 0 Thus it remains to bound
the terms E |b
p − p| and E |b
u − u| . Define:
n
 
1X
pe0 = Ey|x pb0 =
ηb(xi ),
n i=1
n
 0
1X
u
e0 = Ey|x u
b =
h(xi )b
η (xi ),
n i=1
 


pηb = Ex pe0 = E ηb(x) .
 0


uηb = Ex u
e = E h(x)b
η (x) .

≤0

i
h
i
b ETU (b
+ Ex Φ
h) − Ex ΦETU (b
h)
 h
i

b ETU (h) .
≤ 2 sup Ex ΦETU (h) − Φ

for some constant c. Moreover, using p-Lipschitzness of Φ,
we have:
h
i
 0

u − u|
E Φ(u, v, p) − Φ(b
u0 , vb, pb0 ) ≤ Up E |b


 0

+ Vp E |b
v − v| + Pp E |b
p − p| .

h

(7)

We decompose:

b
h∈H

|p − pb0 | ≤ |p − pηb| + |pηb − pe0 | + |e
p0 − pb0 |

Now, fix some classifier h and input sample x. We
let u
b(h), vb(h), pb denote the random variables generated
according to η (for fixed x), while u
b0 (h), pb0 (h) denote
random variables
Pn generated according to ηb; for instance,
u
b0 (h) = n1 i=1 h(xi )yi , where yi ∼ ηb(xi ). Using this
notation, we have:



0
As before,
 0 we 0use
 the fact that Ex |pηb − pe | , as well
p − pb | are both the expected deviations of the
as Ey|x |e
empirical averages of [0, 1]-valued randomqvariables from



ΦETU (h) = Ey|x Φ(b
u(h), vb(h), pb) ,


b ETU (h) = Ey|x Φ(b
Φ
u0 (h), vb(h), pb0 )

 0

1
E |b
p − p| ≤ |p − pηb| + √ .
n

(note that vb(h) does not depend on ηb or η, we vb0 (h) =
vb(h)). We now bound the term under sup in (7):
 h
i

b ETU (h) 
Ex ΦETU (h) − Φ
h
i
≤ E Φ(b
u, vb, pb) − Φ(b
u0 , vb, pb0 )
h
i
≤ E Φ(b
u, vb, pb) − Φ(u, v, p)
h
i
+ E Φ(u, v, p) − Φ(b
u0 , vb, pb0 ) ,
where the first inequality is due to Jensen’s inequality applied to a convex function x 7→ |x|, the all expectations
except for the first line are joint with respect to (x, y),
and for the sake of clarity we drop the dependence on h
in u
b(h), vb(h), u
b0 (h). Now, it follow from Lemma 1 that:
r
h
i
log n


E Φ(b
u, vb, pb) − Φ(u, v, p) ≤ c
,
n

their means, and therefore are bounded by

1
4n .

Hence:

Using analogous way of reasoning, one gets:
 0

1
E |b
u − u| ≤ |u − uηb| + √ .
n
Putting it all together, we get:
 h
i

b ETU (h) 
Ex ΦETU (h) − Φ
r
log n
0
≤c
+ Up |u(h) − uηb(h)| + Pp |p − pηb|,
n
for some constant c0 . Using (7), we finally get:
r
h
i
log n
∗
0
Ex ΦETU (b
h) − ΦETU (h ) ≤ c
+ Pp |p − pηb|
n
+ sup Up |u(h) − uηb(h)|,
b
h∈H

which was to be shown.

Consistency Analysis for Binary Classification Revisited

E. Isotron Algorithm (Kalai & Sastry, 2009)
Here we include the Isotron Algorithm of (Kalai & Sastry, 2009) for completeness. The second update step is the
Pool of Adjacent Violators (PAV) routine, which solves the
isotonic regression problem:
u∗1 , u∗2 , . . . , u∗n = arg

min

u1 ≤u2 ≤···≤un

n
X
(yi − ui )2 ,
i=1

where the instances are assumed to be sorted according to
their scores wT x (using w obtained in first update step of
the iteration). This is a convex quadratic program and can
be solved efficiently. The output link function u of the Algorithm is a piecewise linear estimate.
Algorithm 2 The Isotron algorithm (Kalai & Sastry, 2009).
Input: Training data {(xi , yi )}ni=1 , iterations T
Output: wT , uT
w0 ← 0
u0 ← z 7→ min(max(0, 2 · z + 1), 1)
for t = 1, 2, . . . , T do
Pn
wt ← wt−1 + n1 i=1 (yi − ut−1 (hwt−1 , xi i)) · xi
ut ← PAV({hwt , xi i, yi })
end for

