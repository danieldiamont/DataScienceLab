Supplement to
“A Semismooth Newton Method for Fast, Generic Convex Programming”

Alnur Ali * 1 Eric Wong * 1 J. Zico Kolter 2

This document contains proofs and supplementary details for the paper “A Semismooth Newton Method for Fast, Generic
Convex Programming”. All section, equation, and figure numbers in this supplementary document are preceded by the
letter S (all numbering without an S refers to the main paper).

S.1. Proof of Lemma 3.2
The proof relies on the proof of Lemma 3.6, below. Let z, δ ∈ R3 , and let δ → 0. Suppose z + δ converges to a point
that falls into one of the first three cases given in Section 2. Then, from the statement and proof of Lemma 3.6, an element
JPK∗exp (z + δ) of the generalized Jacobian of the projection onto the dual of the exponential cone at z + δ, is just a matrix
with fixed entries, since projections onto convex sets are continuous. If z + δ converges to a point that falls into the fourth
case, then brute force, e.g., using symbolic manipulation software, reveals that an element of the generalized Jacobian
(i.e., the inverse of the specific 4x4 matrix D given in (S.6), below) is also a constant matrix, even as z1? , z2? , ν ? → 0; for
completeness, we give D−1 in (S.26), at the end of the supplement. Thus in all the cases, the Jacobian is a constant matrix,
which is enough to establish that the limit in (15) exists.

S.2. Proof of Lemma 3.3
First, we give a useful result; its proof is elementary.
Lemma S.2.1. The affine transformation, AF +b, of a (strongly) semismooth map F : Rk → Rk , with A ∈ Rk×k , b ∈ Rk ,
is (strongly) semismooth.

Proof. First of all, we have that a map F : Rk → Rk is (strongly) semismooth if and only if its components Fi , for
i = 1, . . . , k, are (strongly) semismooth (Qi & Sun, 1993, Corollary 2.4). Additionally, we have that (strongly) semismooth
maps are closed under linear combinations (Izmailov & Solodov, 2014, Proposition 1.75). Putting the two pieces together
gives the claim.

Now, from Lemma 3.1, we have that the projections onto the nonnegative orthant, second-order cone, positive semidefinite
cone, as well as the free cone (an affine map, hence strongly semismooth (Facchinei & Pang, 2007, Proposition 7.4.7)), are
all strongly semismooth. The map F , defined in (16), is just an affine transformation of these projections; thus, by (S.2.1),
it is strongly semismooth.
When K, from (2), is the exponential cone, the analogous claim that the map F is semismooth follows, from Lemma 3.2,
in a similar way.
*

Equal contribution 1 Machine Learning Department, Carnegie Mellon University 2 Computer Science Department, Carnegie Mellon
University. Correspondence to: Alnur Ali <alnurali@cmu.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

S.3. Proof of Lemma 3.4
Proof. We have that (i) the projection onto a convex set (e.g., the nonnegative orthant, second-order cone, positive semidefinite cone, exponential cone, and free cone), naturally, yields a convex set; (ii) the affine image of a convex set is a convex
set; and (iii) retaining only some of the coordinates of a convex set is a convex set (Boyd & Vandenberghe, 2004, page 38).
Hence, the components Fi , for i = 1, . . . , 3k, of the map F : R3k → R3k , defined in (16), are convex functions. Thus,
by Clarke (1990, Proposition 1.2), the ith row of any element of the generalized Jacobian is just a subgradient of Fi . Now
observe that the element J of the generalized Jacobian, given in (17), is given by finding subgradients of the Fi .

S.4. Jacobian of the projection onto the second-order cone
In Section 3.2, we stated that, in one case, the Jacobian of the projection onto the second-order cone at some point z =
(z1 , z2 ) ∈ Rm , with z1 ∈ Rm−1 , z2 ∈ R, is a low-rank matrix D ∈ Rm×m ; the matrix D is given by


2
)1 (z1 )2
z2 (z1 )1
z2
1
1 (z1 )1
− z22 (z1kz
·
·
·
3
2 + 2kz1 k2 − 2 kz1 k32
2
kz
k
k
1 2
1 2


2
)1 (z1 )2
z2
z2 (z1 )2
1
1 (z1 )2 

− z22 (z1kz
+
−
·
·
·
3
3

2
2kz1 k2
2 kz1 k2
2 kz1 k2 
1 k2
(S.1)
D=
,
..
..
..
..


.
.
.
.


1
1 (z1 )2
1 (z1 )1
···
2 kz1 k2
2 kz1 k2
2
which can be seen as the sum of diagonal and low-rank matrices. Here, (z1 )i denotes the ith component of z1 .

S.5. Proof of Lemma 3.5
Rewrite the projection onto the positive semidefinite cone as (11) as PKpsd (Z) = Q max(Λ, 0)QT , where Z =
Q max(Λ, 0)QT is the eigenvalue decomposition of some real, symmetric matrix Z, and the max here is interpreted
diagonally. Then, using the chain rule (Magnus & Neudecker, 1995), we get that
JPKpsd (vec Z)(d vec Z) = d vec PKpsd (Z)

= vec (dQ) max(Λ, 0)QT + Q(d max(Λ, 0))QT + Q max(Λ, 0)(dQ)T ;
so, what remains is computing (each column of) dQ and d max(Λ, 0), i.e., the differential of (each column of) the matrix
of eigenvectors, and the differential of max(Λ, 0), respectively. From Magnus & Neudecker (1995, Chapter 8), we get that
dQi = (Λii I − Z)+ (dZ)Qi ,
where Z + denotes the pseudo-inverse of the matrix Z, and that
[d max(Λ, 0)]ii = I+ (Λii )QTi (dZ)Qi ,
by applying the chain rule; here, I+ (·) is the indicator function of the nonnegative orthant, i.e., it equals 1 if its argument
is nonnegative and 0 otherwise. Replacing dZ with some real, symmetric matrix Z̃ yields the claim.

S.6. Further details on the per-iteration costs of SCS, Newton-ADMM, and CVXOPT
Here, we elaborate on the costs of a single iteration of SCS, Newton-ADMM, and CVXOPT. For simplicity, we consider
the case where the cone K, in the cone program (1), is just a single cone (handling the case where K is the direct product
of multiple cones is not hard); also, we are mostly interested in the high-dimensional case, where n > m.
During a single iteration of SCS, described in (6) – (8), we must carry out the computations outlined below:
• We must update the ũ variable, which costs O(max{n2 , m2 }) (see Section 4.1 of O’Donoghue et al. (2016)).
• We must update the u variable, the cost of which is dominated by the cost of projecting an m-vector onto the dual cone
∗
K
the case of projecting onto the positive semidefinite cone, we equivalently consider a matrix with dimensions
√ ; for√
m × m. These costs are as follows:

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

–
–
–
–

For the nonnegative orthant, Kno , the cost is O(m).
For the second-order cone, Ksoc , the cost is O(m).
For the positive semidefinite cone, Kpsd , the cost is O(m3/2 ).
For the exponential cone, Kexp , the cost is roughly O(m3 ).

• We must update the v variable, which has negligible cost.
Summing up, the cost of a single iteration of SCS is O(max{n2 , m2 }) plus the cost of projecting onto the dual cone K∗ ,
as claimed in the main paper.
For Newton-ADMM, we must compute the ingredients on both sides of (19), F and J, as well as run GMRES and the backtracking line search. Computing both F and J can be seen as essentially costing the same as a single iteration of SCS, i.e.,
the cost of projecting onto the dual cone K∗ plus O(max{n2 , m2 }); the backtracking line search, then, costs the number of
backtracking iterations times the aforementioned cost. Furthermore, running GMRES costs O(max{n2 , m2 }), assuming
it returns early. Hence the cost of a single iteration of Newton-ADMM is (as claimed in the main paper) the number of
backtracking iterations times the sum of two costs: the cost of projecting onto the dual cone K∗ plus O(max{n2 , m2 }).
Finally, turning to the interior-point method CVXOPT, it can be seen that the per-iteration cost here is dominated by solving
the Newton system (1.11) in Andersen et al. (2011), essentially costing O(n3 ).
We mention that the above per-iteration costs can, of course, be improved by taking advantage of sparsity.

S.7. Proof of Lemma 3.6
First, from the Moreau decomposition given in (13), we get that
JPK∗exp (z) = I − JPKexp (−z);
so, what remains is to compute JPKexp (z), for some z ∈ Rm . Looking back at the first three cases given in Section 2, we
get that


z ∈ Kexp
I,
∗
JPKexp (z) = −I,
z ∈ Kexp


diag(1, I+ (z2 ), I+ (z3 )), z1 , z2 < 0,
where I+ (zi ), i = 2, 3, is the indicator function of the nonnegative orthant, i.e., it equals 1 if zi ≥ 0 and 0 otherwise.
For the fourth case, the projection PKexp (z) is the solution to the optimization problem given in (12). Now observe that
(i) the optimization problem (12) is, in fact, convex, since the constraint z̃2 > 0 is really just implied by the domain of
the function exp(z̃1 /z̃2 ); (ii) the optimization problem (12) is feasible, since z1? = 1, z2? = 1, z3? = exp(1) satisfies the
constraint; and (iii) we can obtain a solution to the optimization problem (12), by using a Newton method (Parikh & Boyd,
2014, Section 6.3.4).
The rest of the proof relies on the KKT conditions for the optimization problem (12), as well as differentials (see, e.g.,
Magnus & Neudecker (1995)). The Lagrangian of the optimization problem (12) is given by
(1/2)kz̃ − zk22 + ν(z̃2 exp(z̃1 /z̃2 ) − z̃3 ),
where ν ∈ R is the dual variable. Thus, we get that the KKT conditions for the optimization problem (12), at a solution
γ ? = (z1? , z2? , z3? , ν ? ), are
z1? − z1 + ν ? exp(z1? /z2? ) = 0
z2?

− z2 + ν

?

(exp(z1? /z2? )

−

(z1? /z2? ) exp(z1? /z2? ))
z3? − z3 − ν ?
?
z2 exp(z1? /z2? ) − z3?

(S.2)

=0

(S.3)

=0

(S.4)

= 0.

(S.5)

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

Now consider the differentials dz1? , dz2? , dz3? , dz4? and dz1 , dz2 , dz3 , dz4 of the KKT conditions (S.2) – (S.5); we get for
the condition (S.2) that
dz1? − dz1 + (dν ? ) exp(z1? /z2? ) + ν ? (d exp(z1? /z2? )) = 0
⇐⇒ dz1? − dz1 + (dν ? ) exp(z1? /z2? ) + ν ? exp(z1? /z2? )(d(z1? /z2? )) = 0

 ?
z1? (dz2? )
dz1
?
?
?
?
?
?
?
=0
−
⇐⇒ dz1 − dz1 + (dν ) exp(z1 /z2 ) + ν exp(z1 /z2 )
z2?
(z ? )2
 2 ? 
dz
h
i  dz1? 
ν ? exp(z1? /z2? )
ν ? exp(z1? /z2? )z1?
2 
1+
−
0 exp(z1? /z2? ) 
⇐⇒
z2?
(z2? )2
 dz3?  = dz1 .
dν ?
Repeating the above for the other conditions (S.3) – (S.5), we get that





|

ν ? exp(z1? /z2? )
z2?
ν ? exp(z1? /z2? )z1?
−
(z2? )2

−

1+

0
exp(z1? /z2? )

1+

ν ? exp(z1? /z2? )z1?
(z2? )2
ν ? exp(z1? /z2? )(z1? )2
(z2? )3

exp(z1? /z2? )

0

0
0
1
(1 − z1? /z2? ) exp(z1? /z2? ) −1
{z
D



dz1?

?
?
?
?   dz ?
2
(1 − z1 /z2 ) exp(z1 /z2 )  
 dz3?

−1
dν ?
0
|
{z
}
?
dγ





 
=
 
}

|

dz1
dz2
dz3
dν
{z
dγ



,


(S.6)

}

i.e.,
D(dγ ? ) = dγ

⇐⇒

dγ ? = D−1 (dγ);

here, D is nonsingular, since the optimization problem (12) is feasible. So, by definition, the upper left 3x3 submatrix of
D−1 is the Jacobian of the projection onto the exponential cone, for the fourth case.

S.8. Intuition behind some of the regularity conditions for Theorem 4.1, Theorem 4.2, and
Theorem 4.3
Here, we elaborate on a couple of the regularity assumptions stated in the main paper.
S.8.1. Regularity condition (A4)
Roughly speaking, the condition (A4) can be seen as requiring that the directional derivative of z̃ 7→ kF (z̃)k22 be bounded
by α1/2 kF (z̃)k22 .
We list some (useful) functions satisfying (A4):
• The function F (z) = z 2 , for z ∈ R. To show that the function F satisfies (A4), we proceed by computing the required
ingredients on both sides of (A4). Here, and for the rest of the section, we write D∆ F 2 (z) to mean the directional
derivative of the function F squared, in the direction ∆, evaluated at z.
We compute, for z > 0 and the Newton direction ∆ = −1, the left-hand side of (A4),
D∆ F 2 (z) = −4z 3 ,
and the right-hand side of (A4),
−α1/2 2z 3 .
So, satisfying (A4) means
−4z 3 ≤ −α1/2 2z 3 ⇐⇒ 2 ≥ α1/2 ,
which is certainly true. Repeating the argument for z < 0 and ∆ = 1 yields a similar result. (When z = 0, it is a
solution.) Hence, F (z) = z 2 satisfies (A4).

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

• The function F (z) = max(z + 1, cz + 1), with z ∈ R and some c > 0.
We have, for the left-hand side of (A4):
D∆ F 2 (z = 0) = −2c.
We have, for the right-hand side of (A4):
F̂ (z = 0, ∆ = −1) = J(z = 0)∆ = 1 · (−1) = −1.
So, satisfying (A4) means
−2c ≤ −α1/2 ⇐⇒ c ≥ α1/2 /2.
In words, functions that satisfy (A4) cannot have c too small.
• An argument similar the one used above for F (z) = z 2 can be used to show that the function F (z) = |z| also satisfies
(A4).
We also establish, by using the condition (A4), that the backtracking line search, used in Algorithm 1, terminates. Suppose,
for contradiction, that the backtracking line search never terminates. Then, from the backtracking line search iteration
described in Algorithm 1, we have, for all backtracking iterations k,
(kF (z) + γ (k) ∆k22 − kF (z)k22 )/γ (k) ≥ −αkF (z)k22 .
Taking the limit as k → ∞, we get
D∆ kF (z)k22 ≥ −αkF (z)k22 .

(S.7)

On the other hand, expanding the right-hand side of (A4) gives


α1/2 F (z)T F̂ (z, ∆) = α1/2 F (z)T (F̂ (z, ∆) + F (z)) − F (z)T F (z)


≤ α1/2 kF (z)k2 kF̂ (z, ∆) + F (z)k2 − kF (z)k22

≤ α1/2 kF (z)k2 εkF (z)k2 − kF (z)k22


≤ α1/2 (1 − α1/2 )kF (z)k22 − kF (z)k22
= −α1/4 kF (z)k22 .

(S.8)
(S.9)
(S.10)
(S.11)
(S.12)

Putting (A4) and (S.12) above together immediately gives
D∆ kF (z)k22 ≤ −α1/4 kF (z)k22 .
But putting (S.7) and (S.13) together gives
−αkF (z)k22 ≤ D∆ kF (z)k22 ≤ −α1/4 kF (z)k22 ,
a contradiction, since α ∈ (0, 1).
S.8.2. Regularity condition (A5)
Roughly speaking, the condition (A5) says that the Newton step on each iteration cannot be too large.

(S.13)

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

S.9. Proof of Theorem 4.1
Proof. We begin by recalling the condition under which backtracking line search continues, for a particular iteration of
Newton’s method; this happens as long as (see Algorithm 1)
kF (z (i) + t(i) ∆(i) )k22 ≥ (1 − αt(i) )kF (z (i) )k22 .

(S.14)

This means that when backtracking line search terminates, we get that
0 ≤ kF (z (i+1) )k22 < (1 − αt(i) )kF (z (i) )k22 < kF (z (i) )k22 .

(S.15)

(To be clear, in order to get the second inequality here, we used the fact that backtracking line search terminates after (S.14)
in Algorithm 1 no longer holds.) In order to get the third inequality here, we used the simple fact that 0 < 1 − αt(i) ≤ 1,
since 0 < α < 1/2 and 0 < t(i) ≤ 1. So, we have shown that the sequence (kF (z (i) )k22 )∞
i=1 is both bounded below
and decreasing. Note that this is just a sequence in R, and thus, by the monotone convergence theorem, it converges.
Furthermore, since every convergent sequence in R is Cauchy, we get that


(S.16)
lim kF (z (i) )k22 − kF (z (i+1) )k22 = 0.
i→∞

On the other hand, by rearranging the second inequality in (S.15), we get that
kF (z (i) )k22 − kF (z (i+1) )k22 > αt(i) kF (z (i) )k22 ≥ 0.

(S.17)

So, (S.16) along with taking the lim supi→∞ on both sides of (S.17) yields that limi→∞ αt(i) kF (z (i) )k22 = 0. But
assumption (A1) says that lim supi→∞ t(i) → t > 0, and since α > 0, we get that limi→∞ t̃kF (z (i) )k22 = 0, for some
t̃ > 0, and so limi→∞ kF (z (i) )k22 = 0, which implies that limi→∞ F (z (i) ) = 0, as claimed.

S.10. Proof of Theorem 4.2
Proof. First of all, by the assumption that (z (i) )∞
i=1 is convergent and assumption (A5), we must have that
0 ≤ k∆(i) k2 ≤

1
ε+1
kF̂ (z (i) , ∆(i) )k2 ≤
kF (z (i) )k2 ,
C2
C2

(S.18)

where the second inequality here follows by rearranging (A5), and the third inequality follows from (20), as well as the
triangle inequality: after computing ∆(i) on Newton iteration i, we are assured that
kF (z (i) ) + F̂ (z (i) , ∆(i) )k2 ≤ εkF (z (i) )k2
=⇒ kF̂ (z (i) , ∆(i) )k2 − kF (z (i) )k2 ≤ εkF (z (i) )k2
⇐⇒ kF̂ (z (i) , ∆(i) )k2 ≤ (ε + 1)kF (z (i) )k2 .
Hence, since
sup dist(∆(j) , ∆(`) )

≤

sup k∆(j) k2 + sup k∆(`) k2 ,
j

j,`

`

and because the right-hand side here is bounded (as per (S.18), as well as the fact that (kF (z (i) )k22 )∞
i=1 is decreasing), we
can conclude that the sequence (∆(i) )∞
is
bounded.
(We
used
the
Euclidean
distance
here.)
i=1
By the Bolzano-Weierstrass theorem (for Euclidean spaces), this sequence contains a convergent subsequence; let
(∆(i) )i∈S , for some countable set S, be this subsequence. Define γ (i) = t(i) /β, i.e., γ (i) is the last t(i) for which (S.14)
was actually true (i.e., when checked at the start of the (i + 1)th Newton iteration). Then we get
kF (z (i) + γ (i) ∆(i) )k22 − kF (z (i) )k22 ≥ −αγ (i) kF (z (i) )k22 ;
dividing through by γ (i) and taking limits gives (observe that, from assumption (A2), lim supi→∞ t(i) = 0
limi→∞ t(i) = 0)
−αkF (z)k22 ≤
≤

=⇒

lim

kF (z (i) + γ (i) ∆(j) )k22 − kF (z (i) )k22
γ (i)

(S.19)

lim

α1/2 F (z (i) )T F̂ (z (i) , ∆(j) ),

(S.20)

i,j→∞, j∈S
i,j→∞, j∈S

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

with the second line here following by assumption (A4). Expanding the right-hand side of (S.20), we get


α1/2 F (z (i) )T F̂ (z (i) , ∆(j) ) = α1/2 F (z (i) )T F̂ (z (i) , ∆(j) ) + F (z (i) ) − α1/2 F (z (i) )T F (z (i) )
≤ α1/2 kF (z (i) )k2 kF (z (i) ) + F̂ (z (i) , ∆(j) )k2 − α1/2 kF (z (i) )k22
≤ α1/2 εkF (z (i) )k22 − α1/2 kF (z (i) )k22
= −α1/2 kF (z (i) )k22 (1 − ε),
with the second line following from the Cauchy-Schwarz inequality, and the third from (20). So, we obtain for the righthand side of (S.20) that
lim

i,j→∞, j∈S

α1/2 F (z (i) )T F̂ (z (i) , ∆(j) ) ≤ −α1/2 kF (z)k22 (1 − ε).

(S.21)

Putting together (S.19) and (S.21), we get that
−αkF (z)k22 ≤ −α1/2 kF (z)k22 (1 − ε)

⇐⇒



0 ≥ α1/2 kF (z)k22 (1 − ε) − α1/2 .

Now, by assumption (A3), we require that ε < 1 − α1/2 ⇐⇒ (1 − ε) − α1/2 > 0; thus, we must have that kF (z)k22 =
0 ⇐⇒ F (z) = 0, as claimed.

S.11. Proof of Theorem 4.3
Proof. The theorem establishes that the iterates (z (i) )∞
i=1 generated by Algorithm 1 are locally quadratically convergent,
i.e., we get, for large enough i and some C > 0, that
|z (i+1) − z|
= C.
i→∞ (z (i) − z)2
lim

Let res(i) = F (z (i) ) + J(z (i) )∆(i) , for convenience. We begin by making two useful observations.
First, using the second part of assumption (A6), we get that
kF (z (i) ) − res(i)k2 = kJ(z (i) )∆(i) k2
≤ kJ(z (i) )k2 k∆(i) k2
≤ C3 k∆(i) k2 .

(S.22)

On the other hand, using the triangle inequality as well as (20), we get that
kF (z (i) ) − res(i)k2 ≥ kF (z (i) )k2 − k res(i)k2
≥ kF (z (i) )k2 − εkF (z (i) )k2
≥ (1 − ε)kF (z (i) )k2 .

(S.23)

So, putting together (S.22) and (S.23), we get that
(1 − ε)kF (z (i) )k2 ≤ C3 k∆(i) k2

=⇒

kF (z (i) )k2 ≤ C4 k∆(i) k2 ,

for some constant C4 > 0, since 1 − ε > 0. Squaring both sides, it follows that
kF (z (i) )k2 ≤ C4 k∆(i) k2
=⇒ kF (z (i) )k22 ≤ C42 k∆(i) k22
=⇒ k res(i)k2 ≤ C5 k∆(i) k22
=⇒

k res(i)k2
≤ C5 ,
k∆(i) k22

(S.24)

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

where C5 > 0 is some constant, and the third line follows because (20) and assumption (A3) tell us that k res(i)k2 ≤
CkF (z (i) )k22 for some constant C > 0. Finally, Facchinei & Kanzow (1997, Theorem 2.5) and the second part of assumption (A6) tell us that the sequence of iterates (z (i) )∞
i=1 → z converges quadratically, with F (z) = 0, as claimed.

S.12. Further details on the minimum variance portfolio optimization example
Here, we elaborate on putting the minimum variance portfolio optimization problem (22) into the cone form of (1).
First, we rewrite the minimum variance portfolio optimization problem (22) as
minimize

θ∈Rp , w∈R

subject to

w


 2Σ1/2 θ 


 1−w  ≤1+w
2
1 ≤ 1T θ ≤ 1,

where we used the simple fact (Lobo et al., 1998, Equation 8) that




2α
T


α α ≤ γδ
⇐⇒
 γ − δ  ≤ γ + δ,
2
for some vector α and nonnegative constants θ, γ (for us, α = Σ1/2 θ, γ = 1, and δ = w). Then, we rewrite the above
problem as
minimize cT x
x∈Rp+1

subject to kG1 x + hk2 ≤ q T x + z
G2 x ≤ 1, G3 x ≤ −1,
where we defined


θ
w





0
1



x=
c=



 
0
2Σ1/2 0
, h=
1
0
−1
 
0
q=
, z=1
1




G2 = 1T 0 , G3 = −1T 0 .

G1 =

Finally, we just use



−G1
 −q T 

A=
 G2  ,
G3




h
 z 

b=
 1 ,
−1

p+2
K = Ksoc
× Kno × Kno ,

p+2
denotes the (p + 1)-dimensional second-order cone.
to get the cone form of (1); here, Ksoc

S.13. Further details on the `1 -penalized logistic regression example
Here, we elaborate on putting the `1 -penalized logistic regression problem (23) into the cone form of (1). To keep the
notation light, we write
zi = yi Xi· θ.
Now, for i = 1, . . . , N , we use the simple fact (Serrano, 2015, Section 9.4.1) that
!
X
X
log
exp(αi ) ≤ −θ
⇐⇒
exp(αi + θ) ≤ 1,
i

i

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

for αi , θ ∈ R, in order to conclude that

log(exp(0) + exp(zi )) ≤ wi

⇐⇒

exp(−wi ) + exp(zi − wi ) ≤ 1,

(S.25)

where the wi ∈ R are some variables that we will introduce, later on. Next, we “split” the right-hand side of (S.25) into
the following set of constraints:


exp(−wi ) ≤ `i

⇐⇒

exp(zi − wi ) ≤ qi

⇐⇒

`i + qi ≤ 1,


−wi
 1  ∈ Kexp , i = 1, . . . , N,
`i


zi − wi

 ∈ Kexp , i = 1, . . . , N,
1
qi

i = 1, . . . , N,

where `i , qi ∈ R are more new variables. Thus, we can write the `1 -penalized logistic regression problem (23) as

minimize

θ∈Rp , w∈RN ,
t∈Rp , `∈RN ,
q∈RN

1T w + λ1T t



subject to


−wi
 1  ∈ Kexp , i = 1, . . . , N
 `i

yi Xi· θ − wi

 ∈ Kexp , i = 1, . . . , N
1
qi
`+q ≤1
−t ≤ θ ≤ t.

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

Finally, to get the cone form of (1), we use


θ
 w 



x=
 t ,
 ` 
q


0
 1 



c=
 λ1  ,
 0 
0

G1

..

.


G
N


H
1

.
A=
.

.


H
N

 0 0
0
I I

 −I 0 −I 0 0
I 0 −I 0 0

0 eTi 0
0
0
0
0
Gi =  0 0 0
0 0 0 −eTi 0


h
 .. 
 . 


 h 




 h 
0





b=
 ...  , h = 1 ,


0
 h 


 1 


 0 
0









,








,



−yi Xi·
0
Hi = 
0

eTi
0
0


0 0
0
0 0
0 ,
0 0 −eTi

i = 1, . . . , N,

N
p
p
K = Kexp × · · · × Kexp × Kexp × · · · × Kexp ×Kno
× Kno
× Kno
;
|
{z
} |
{z
}
N

N

i
here, ei , i = 1, . . . , N denotes the ith standard basis vector in RN , and Kno
denotes the i-dimensional nonnegative orthant.

S.14. Further details on the robust PCA example
Here, we elaborate on putting the robust PCA problem (24) into the cone form of (1).
First, we observe that, using duality arguments (see, e.g., Fazel et al. (2001, Section 3) or Recht et al. (2010, Proposition
2.1)), we can rewrite the robust PCA problem (24) as
minimize

W1 ∈RN ×N ,W2 ∈Rp×p ,
t∈RN p , L,S∈RN ×p

subject to

(1/2)(tr(W1 ) + tr(W2 ))
−t ≤ vec(S) ≤ t
1T t ≤ λ
L
 +S =X 
W1 L
 0.
LT W2

Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

To get the cone form of (1), we use


vec(W1 )
 vec(W2 ) 


,
t
x=


 vec(L) 
vec(S)


(1/2) vec(I)
 (1/2) vec(I) 


,
0
c=




0
0


0
0
−I
0 −I
 0
0
−I
0
I 


T
 0
0
1
0
0 
,

A=
0
0
I
I 

 0
 0
0
0 −I −I 
GW1 GW2 0 GL 0
h
i
(2,1)
(N −1,N )
(N,N )
GW1 = vec(G(1,1)
,
)
vec(G
)
·
·
·
vec(G
)
vec(G
)
W1
W1
W1
W1
(i,j)

GW2

where GW1 is 0 except with the (i, j)th entry of its upper left N × N block set to 1,
i
(2,1)
(p−1,p)
(p,p)
= vec(G(1,1)
,
)
vec(G
)
·
·
·
vec(G
)
vec(G
)
W2
W2
W2
W2
h

(i,j)

where GW2 is 0 except with the (i, j)th entry of its bottom right p × p block set to 1,
i
(2,1)
(N −1,p)
(N,p)
,
GL = vec(G(1,1)
)
vec(G
)
·
·
·
vec(G
)
vec(G
)
L
L
L
L
h

(i,j)

where GL

is 0 except with the (i, j)th entry of its upper right N × p block

and the (j, i)th entry of its lower left p × N block set to 1,


0


0




λ

b=
 vec(X)  ,


 − vec(X) 
0
N +p
Np
Np
Np
Np
K = Kno
× Kno
× Kno × Kno
× Kno
× Kpsd
.
i
Here, Kpsd
denotes the (i × i)-dimensional positive semidefinite cone. Also, observe that the last row of A, b above encodes
the constraint


W1 L
N +p
,
∈ Kpsd
LT W2

which we can write as a linear matrix inequality (Andersen et al., 2011, Equation 1.7):


X (i,j)
X (i,j)
X (i,j)
W1 L
 0 ⇐⇒
GW1 (W1 )ij +
GW2 (W2 )ij +
GL Lij  0
T
L
W2
i,j
i,j
i,j


⇐⇒ GW1 GW2 0 GL 0 x  0.

Expression for the matrix D−1 , used in the proof of Lemma 3.2:


D−1

?

?

z ? /z ? (z ? )3

ez1 /z2 ν ? (z ? )2

z ? /z ? (z ? )2

2

where k = −1 − e

2

?
2z1
ẑ2

2

−

e

?

?
z ? /z ? z1

1
− e 1 (z2?e)3 1 + 2e 1 (z?2 )e2 1 − e 1 z?2 e
 −1 −
(z2? )3
2
2

?2
2z1
?

2z1
?
?
?

?
ez1 /z2 ν ? z ?
e z2 z ?

e z2 − (z? )2 1 − z? 1
2
2
?
= (1/k) · 
2z1

?
? ν ? (z ? )2
?
?
? ? 3

z ? /z2
e z2 ν ? (z1? )2
1
e
ez1 /z2 eν (z1 )
z1? /z2?

−e
−
− e 1 (z
+
? 3
(z2? )4
(z ? )3

2)
? 2
2z1

?
?
?
?
? )2
?
?
?
?
3
z
?
?
2
z /z2 ν (z1
z /z2 ν (z1 )
?
?
e 2 ν (z1 )
e
e
−ez1 /z2 + e 1 (z
−
− e 1 (z
? )4
? )3
(z ? )3

−

?

−

e

? /z ? ν ? (z ? )2
z1
2
1

e
(z2? )3

?

?

?

? 2

ez1 /z2 eν (z1 )
(z2? )3

+

e

?
2z1
?
z2

−
−

e
(z2? )3

?
2z1
?
z2

?
2z1
?
z2
ν ? z1?
(z2? )2
?
2z1
?
e z2 ν ? z1?
(z2? )2

e

ν ? (z1? )2

?

? 2

ez1 /z2 e(z1 )
(z2? )2

−1 − e

z1? /z2? ?

ν (z1? )2
(z2? )3

?

ez1 /z2 ν ? z1?
(z2? )2

−

?

−

?

+

?

?

−
−

ez1 /z2 eν
(z2? )2

?

?

? z?
1

−e
+

? 2
e(z1 )
(z2? )2

+

e(z1 )
(z2? )2

e

(z2? )3

−

?
ez1
z2?

−

ez1
z2?

? 2

z1? /z2? (z1? )3

−

?

?

−

e

−ez1 /z2 −

ez1 /z2 ν ?
z2?

?
?
? ?
ez1 /z2 eν z1
(z2? )2

e

?

ez1 /z2 ez1
z2?

e

−e

?
2z1
?
z2

+

?
2z1
?
e z2

? )2
ν ? (z1

e
(z2? )3

−

z1? /z2?

?
?
? 3
ez1 /z2 e(z1 )
(z2? )3

?

?
3z1
?
z2

ν ? z1?
(z2? )2

1+

−

e

?
2z1
?
z2

−

e

?

?

?
2z1
?
z2

e

?
2z1
?
z2
ν ? z1?
(z2? )2
?

ν?

z?
? 2
3z1
?
z2
ν ? z1?
(z2? )2

ez1 /z2 ν ? (z1? )2
(z2? )3

−

?
2z1
?
e z2

−e

?
ν ? z1

e
(z2? )2

?

+

?

ez1 /z2 z1?
z2?

+

?

?

−

e

−ez1 /z2 −

+

?
?
? 2
2ez1 /z2 e(z1 )
(z2? )2

−

?
?
?
ez1 /z2 ez1
z2?

z1? /z2?
?

z2?

1+

ez1 /z2 ν ? (z1? )2
(z2? )3

e

(z2? )2

+

2e

z1? /z2? (z1? )2

e

(z2? )2

−

e

z1? /z2? ?

ν

z2?

−

e

z1? /z2? z1?

e

z2?

.

+


?

ez1 /z2 z1?
z2?
?

?

?

?

+

ez1 /z2 ν ?
z2?

+

ez1 /z2 ν ?
z2?

?

(S.26)
ν ? z1?

?

?

1+

?

?
2z1
?
z2
ν ? z1?
(z2? )2

ν?

ez1 /z2 ν ? (z1? )2
(z2? )3

?

ez1 /z2 ν ?
z2?

?
2z1
?
z2

e






,






Supplement to “A Semismooth Newton Method for Fast, Generic Convex Programming”

References
Andersen, Martin, Dahl, Joachim, Liu, Zhang, and Vandenberghe, Lieven. Interior point methods for large-scale cone
programming. Optimization for machine learning, pp. 55–83, 2011.
Boyd, Stephen and Vandenberghe, Lieven. Convex Optimization. Cambridge University Press, 2004.
Clarke, Frank. Optimization and Nonsmooth Analysis. SIAM, 1990.
Facchinei, Francisco and Kanzow, Christian. A nonsmooth inexact Newton method for the solution of large-scale nonlinear
complementarity problems. Mathematical Programming, 76(3):493–512, 1997.
Facchinei, Francisco and Pang, Jong-Shi. Finite-Dimensional Variational Inequalities and Complementarity Problems.
Springer, 2007.
Fazel, Maryam, Hindi, Haitham, and Boyd, Stephen. A rank minimization heuristic with application to minimum order
system approximation. In American Control Conference, 2001. Proceedings of the 2001, volume 6, pp. 4734–4739.
IEEE, 2001.
Izmailov, Alexey and Solodov, Mikhail. Newton-Type Methods for Optimization and Variational Problems. Springer, 2014.
Lobo, Miguel Sousa, Vandenberghe, Lieven, Boyd, Stephen, and Lebret, Hervé. Applications of second-order cone programming. Linear algebra and its applications, 284(1):193–228, 1998.
Magnus, Jan and Neudecker, Heinz. Matrix Differential Calculus with Applications in Statistics and Econometrics. John
Wiley & Sons, 1995.
O’Donoghue, Brendan, Chu, Eric, Parikh, Neal, and Boyd, Stephen. Conic optimization via operator splitting and homogeneous self-dual embedding. Journal of Optimization Theory and Applications, pp. 1–27, 2016.
Parikh, Neal and Boyd, Stephen. Proximal algorithms. Foundations and Trends in Optimization, 1(3):127–239, 2014.
Qi, Liqun and Sun, Jie. A nonsmooth version of Newton’s method. Mathematical Programming, 58(1-3):353–367, 1993.
Recht, Benjamin, Fazel, Maryam, and Parrilo, Pablo A. Guaranteed minimum-rank solutions of linear matrix equations
via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.
Serrano, Santiago. Algorithms for Unsymmetric Cone Optimization and an Implementation for Problems with the Exponential Cone. PhD thesis, Stanford University, 2015.

