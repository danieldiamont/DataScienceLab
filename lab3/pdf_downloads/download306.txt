An Adaptive Test of Independence with Analytic Kernel Embeddings

Supplementary Material
A. Type-I Errors
In this section, we show that all the tests have correct type-I errors (i.e., the probability of reject H0 when it is true) in real
problems. We permute the joint sample so that the dependency is broken to simulate cases in which H0 holds. The results
are shown in Figure 5.

0.02
0.01
0.00

500

QHSIC

Type-I error

NFSIC-med

Type-I error

Type-I error

NFSIC-opt

0.02
0.01
0.00

1000
1500
Sample size n

NyHSIC

FHSIC

RDC

0.02
0.01

2000 1000 2000
500
1500
Sample size n

4000
6000
2000
Sample size n

8000

(a) MSD problem (permuted sample). (b) Videos & Captions problem (permuted
sample).

Figure 5: Probability of rejecting H0 as n increases. α = 0.01.

B. Redundant Test Locations

y

2.5
0.0
−2.5

−2.5

0.0
x

2.5

2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00

λ̂n(t1, t2)

ω = 1.00

y

Here, we provide a simple illustration to show that two locations t1 = (v1 , w1 ) and t2 = (v2 , w2 ) which are too close
to each other will reduce the optimization objective. We consider the Sinusoid problem described in Section 3.1 with
ω = 1, and use J = 2 test locations. In Figure 6, t1 is fixed at the red star, while t2 is varied along the horizontal line. The
objective value λ̂n as a function of t2 is shown in the bottom figure. It can be seen that λ̂n decreases sharply when t2 is
in the neighborhood of t1 . This property implies that two locations which are too close will not maximize the objective
function (i.e., the second feature contains no additional information when it matches the first). For J > 2, the objective
sharply decreases if any two locations are in the same neighborhood.
2
0
−2

x

250
225
200
175
−2

0
t2

2

Figure 6: Plot of optimization objective values as location t2 moves along the green line. The objective sharply drops when
the two locations are in the same neighborhood.

C. Test Power vs. J
It might seem intuitive that as the number of locations J increases, the test power should also increase. Here, we empirically
show that this statement is not always true. Consider the Sinusoid toy example described in Section 3.1 with ω = 2 (also see
the left figure of Figure 7). By construction, X and Y are dependent in this problem. We run NFSIC test with a sample size
of n = 800, varying J from 1 to 600. For each value of J, the test is repeated for 500 times. In each trial, the sample is
redrawn and the J test locations are drawn from Uniform((−π, π)2 ). There is no optimization of the test locations. We use
Gaussian kernels for both X and Y , and use the median heuristic to set the Gaussian widths to 1.8. Figure 7 shows the test
power as J increases.

An Adaptive Test of Independence with Analytic Kernel Embeddings

y

2.5
0.0
−2.5

−2.5

0.0
x

2.5

2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00

1.0
Test power

ω = 2.00

0.5

10

200

400

600

J

Figure 7: The Sinusoid problem and the plot of test power vs. the number of test locations.
We observe that the test power does not monotonically increase as J increases. When J = 1, the difference of pxy and px py
cannot be adequately captured, resulting in a low power. The power increases rapidly to roughly 0.6 at J = 10, and stays at
1 until about J = 100. Then, the power starts to drop sharply when J is higher than 400 in this problem.
Unlike random Fourier features, the number of test locations in NFSIC is not the number of Monte Carlo particles used to
approximate an expectation. There is a tradeoff: if the test locations are in key regions (i.e., regions in which there is a big
difference between pxy and px py ), then they increase power; yet the statistic gains in variance (thus reducing test power) as
J increases. As can be seen in Figure 7, there are eight key regions (in blue) that can reveal the difference of pxy and px py .
Using an unnecessarily high J not only makes the covariance matrix Σ̂ harder to estimate accurately, it also increases the
computation as the complexity on J is O(J 3 ).
We note that NFSIC is not intended to be used with a large J. In practice, it should be set to be large enough so as to capture
the key regions as stated. As a practical guide, with optimization of the test locations, a good starting point is J = 5 or 10.

D. Proof of Proposition 3
Recall Proposition 3,

is characteristic and analytic). Let k(x, x0 ) = exp −(x − x0 )> A(x − x0 ) and
Proposition (A product of Gaussian kernels

l(y, y0 ) = exp −(y − y0 )> B(y − y0 ) be Gaussian kernels on Rdx ×Rdx and Rdy ×Rdy respectively, for positive definite
matrices A and B. Then, g((x, y), (x0 , y0 )) = k(x, x0 )l(y, y0 ) is characteristic and analytic on (Rdx ×Rdy )×(Rdx ×Rdy ).
Proof. Let z := (x> , y> )> and z0 := (x0> , y0> )> be vectors in Rdx +dy . 
We prove by
 reducing the product kernel to one

A 0
0
0 >
0
Gaussian kernel with g(z, z ) = exp −(z − z ) C(z − z ) where C :=
. Write g(z, z0 ) = Ψ(z − z0 ) where
0 B

Ψ(t) := exp −t> Ct . Since C is positive definite, we see that the finite measure ζ corresponding to Ψ as defined in
Lemma 12 has support everywhere in Rdx +dy . Thus, Sriperumbudur et al. (2010, Theorem 9) implies that g is characteristic.
To see that g is analytic, we observe that for each z0 ∈ Rdx +dy , z 7→ −(z − z0 )> C(z − z0 ) is a multivariate polynomial
in z, which is known to be analytic. Using the fact that t 7→ exp(t)
 is analytic on R, and that a composition of analytic
functions is analytic, we see that z 7→ exp −(z − z0 )> C(z − z0 ) is analytic on Rdx +dy for each z0 .

E. Proof of Theorem 5
Recall Theorem 5,
\ 2 is consistent). Let Σ̂ be a consistent estimate of Σ based on the joint
Theorem 5 (Independence test based on NFSIC
sample Zn , where Σ is defined in Proposition 4. Assume that VJ = {(vi , wi )}Ji=1 ∼ η where η is absolutely continuous wrt

−1
\ 2 statistic is defined as λ̂n := nû> Σ̂ + γn I
the Lebesgue measure. The NFSIC
û where γn ≥ 0 is a regularization
parameter. Assume that

1. Assumption A holds.
2. Σ is invertible η-almost surely.

An Adaptive Test of Independence with Analytic Kernel Embeddings

3. limn→∞ γn = 0.
Then, for any k, l and VJ satisfying the assumptions,
d

1. Under H0 , λ̂n → χ2 (J) as n → ∞.


\2
2. Under H1 , for any r ∈ R, limn→∞ P λ̂n ≥ r = 1 η-almost surely. That is, the independence test based on NFSIC
is consistent.

−1 p
→ Σ−1
Proof. Assume that H0 holds. The consistency of Σ̂ and the continuous mapping theorem imply that Σ̂ + γn I

which
Let a be
a random vector in RJ following N (0, Σ). By van der Vaart (2000, Theorem 2.7 (v)), it follows
 is a constant.

−1 

√
√
d 
d
that
nû, Σ̂ + γn I
→ a, Σ−1 where u = 0 almost surely by Proposition 2, and nû → N (0, Σ) by Proposition


−1 

−1
√
d
d
4. Since f (x, S) := x> Sx is continuous, f
nû, Σ̂ + γn I
→ f (a, Σ−1 ). Equivalently, nû> Σ̂ + γn I
û →
a> Σ−1 a ∼ χ2 (J) by Anderson (2003, Theorem 3.3.3). This proves the first claim.

The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al. (2015).
Assume that H1 holds. Then, u 6= 0 almost surely by Proposition 2. Since k and l are bounded, it follows that
a.s.
|ht (z, z0 )| ≤ 2Bk Bl for any z, z0 (see (8)), and we have that û → u by Serfling (2009, Section 5.4, Theorem A). Thus,

−1
d
û> Σ̂ + γn I
û − nr → u> Σ−1 u by the continuous mapping theorem, and the consistency of Σ̂. Consequently,


lim P λ̂n ≥ r
n→∞
 

−1
r
>
= 1 − lim P û Σ̂ + γn I
û − < 0
n→∞
n

(a)
(b)
= 1 − P u> Σ−1 u < 0 = 1,
d

where at (a) we use the Portmanteau theorem (van der Vaart, 2000, Lemma 2.2 (i)) guaranteeing that xn → x if and only if
P(xn < t) → P(x < t) for all continuity points of t 7→ P(x < t). Step (b) is justified by noting that the covariance matrix
Σ is positive definite so that u> Σ−1 u > 0, and t 7→ P(u> Σ−1 u < t) (a step function) is continuous at 0.

F. Proof of Theorem 7
Recall Theorem 7,
Theorem 7 (A lower bound on the test power). Let NFSIC2 (X, Y ) := λn := nu> Σ−1 u. Let K be a kernel class for k, L
be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that
1. There exist finite Bk and Bl such that supk∈K supx,x0 ∈X |k(x, x0 )| ≤ Bk and supl∈L supy,y0 ∈Y |l(y, y0 )| ≤ Bl .
2. c̃ := supk∈K supl∈L supVJ ∈V kΣ−1 kF < ∞.


Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test power satisfies P λ̂n ≥ r ≥ L(λn ) where
2

2

L(λn ) = 1 − 62e−ξ1 γn (λn −r)

/n

2
2
− 2e−b0.5nc(λn −r) /[ξ2 n ]
2

2
2
− 2e−[(λn −r)γn (n−1)/3−ξ3 n−c3 γn n(n−1)] /[ξ4 n (n−1)] ,

1
, B ∗ is a
32 c21 J 2 B ∗
4B 2 J c̃2 , ξ4 := 28 B 4 J 2 c21 ,

b·c is the floor function, ξ1 :=

ξ3 := 8c1 B 2 J, c3 :=
fixed n, L(λn ) is increasing in λn .

constant depending on only Bk and Bl , ξ2 := 72c22 JB 2 , B := Bk Bl ,
√
√
c1 := 4B 2 J J c̃, and c2 := 4B J c̃. Moreover, for sufficiently large

An Adaptive Test of Independence with Analytic Kernel Embeddings

Overview of the proof We first derive a probabilistic bound for |λ̂n − λn |/n. The bound is in turn upper bounded by
an expression involving kû − uk2 and kΣ̂ − ΣkF . The difference kû − uk2 can be bounded by applying the bound for
U-statistics given in Serfling (2009, Theorem A, p. 201). For kΣ̂ − ΣkF , we decompose it into a sum of smaller components,
and bound each term with a product variant of the Hoeffding’s inequality (Lemma 9). L(λn ) is obtained by combining all
the bounds with the union bound.
F.1. Notations
p
Let hA, BiF := tr(A> B) denote the Frobenius inner product, and kAkF := tr(A> A) be the Frobenius norm. Write
z := (x, y) to denote a pair of points from X × Y. We write t := (v, w) to denote a pair of test locations from X × Y. For
brevity, an expectation over (x, y) (i.e., E(x,y)∼Pxy ) will be written as Ez or Exy . Define k̃(x, v) := k(x, v) − Ex0 k(x0 , v),
and ˜l(y, w) := l(y, w) − Ey0 l(y0 , w). Let B2 (r) := {x | kxk2 ≤ r} be a closed ball with radius r centered at the origin.
Similarly, define BF (r) := {A | kAkF ≤ r} to be a closed ball with radius r of J × J matrices under the Frobenius norm.
Denote the max operation by (x1 , . . . , xm )+ = max(x1 , . . . , xm ).
Pn P
1
For a product of marginal mean embeddings µx (v)µy (w), we write µ[
k(xi , v)l(yj , w)
x µy (v, w) := n(n−1)
Pn
Pn i=1 j6=i
1
1
to denote the unbiased plug-in estimator, and write µ̂x (v)µ̂y (w) := n i=1 k(xi , v) n j=1 l(yj , w) which is a biased
>
estimator. Define ûb (v, w) := µ̂xy (v, w) − µ̂x (v)µ̂y (w) so that ûb := ûb (t1 ), . . . , ûb (tJ ) where the superscript b
stands for “biased”. To avoid confusing with a positive definite kernel, we will refer to a U-statistic kernel as a core.
F.2. Proof
We will first derive a bound for P(|λ̂n − λn | ≥ t), which will then be reparametrized to get a bound for the target quantity
P(λ̂n ≥ r). We closely follow the proof in Jitkrittum et al. (2016, Section C.1) up to (12), then we diverge. We start by
considering |λ̂n − λn |/n.




|λ̂n − λn |/n = û> (Σ̂ + γn I)−1 û − u> Σ−1 u
 

−1
 >

−1
−1
>
>
> −1 

= û Σ̂ + γn I
û − u (Σ + γn I) u + u (Σ + γn I) u − u Σ u
 
 

−1
 >


−1 
−1
>

≤ û Σ̂ + γn I
û − u (Σ + γn I) u + u> (Σ + γn I) u − u> Σ−1 u
:= (F)1 + (F)2 .
We next bound (F1 ) and (F2 ) separately.

 

−1
 >
−1 
>

(F)1 = û Σ̂ + γn I
û − u (Σ + γn I) u
 

−1
 >
−1
−1
−1 
>
>
>

= û Σ̂ + γn I
û − û (Σ + γn I) û + û (Σ + γn I) û − u (Σ + γn I) u
 
 

−1
 >

−1 
−1
−1 
>

≤ û Σ̂ + γn I
û − û (Σ + γn I) û + û> (Σ + γn I) û − u> (Σ + γn I) u

  D
E 

−1
 

−1
>

 +  ûû> − uu> , (Σ + γn I)−1 
=  ûû , Σ̂ + γn I
− (Σ + γn I)

F

F

>

−1

≤ kûû kF k(Σ̂ + γn I)

−1

− (Σ + γn I)

>

>

−1

kF + kûû − uu kF k(Σ + γn I)

kF

= kûû> kF k(Σ̂ + γn I)−1 [(Σ + γn I) − (Σ̂ + γn I)](Σ + γn I)−1 kF + kûû> − ûu> + ûu> − uu> kF k(Σ + γn I)−1 kF
(a)

≤ kûû> kF k(Σ̂ + γn I)−1 kF kΣ − Σ̂kF kΣ−1 kF + kûû> − ûu> + ûu> − uu> kF kΣ−1 kF
√
(b)

J
kûk22 kΣ − Σ̂kF kΣ−1 kF + kû(û − u)> kF + k(û − u)u> kF kΣ−1 kF
≤
γn

An Adaptive Test of Independence with Analytic Kernel Embeddings

≤

√

J
kûk22 kΣ − Σ̂kF kΣ−1 kF + (kûk2 + kuk2 ) kû − uk2 kΣ−1 kF ,
γn

where at (a) we used k(Σ + γn I)−1 kF ≤ kΣ−1 kF , at (b) we used k(Σ̂ + γn I)−1 kF ≤

(5)
√

Jk(Σ̂ + γn I)−1 k2 ≤

√

J/γn .

For (F)2 , we have




−1
(F)2 = u> (Σ + γn I) u − u> Σ−1 u


 
=  uu> , (Σ + γn I)−1 − Σ−1 F 

≤ kuu> kF k(Σ + γn I)−1 − Σ−1 kF

= kuk22 k(Σ + γn I)−1 [Σ − (Σ + γn I)] Σ−1 kF
≤ γn kuk22 k(Σ + γn I)−1 kF kΣ−1 kF
(a)

≤ γn kuk22 kΣ−1 k2F ,

where at (a) we used k(Σ + γn I)−1 kF ≤ kΣ

−1

(6)

kF .

Combining (5) and (6), we have


 >

û (Σ̂ + γn I)−1 û − u> Σ−1 u
√
J
≤
kûk2 kΣ − Σ̂kF kΣ−1 kF + (kûk2 + kuk2 ) kû − uk2 kΣ−1 kF + γn kuk22 kΣ−1 k2F .
γn

(7)

Bounding kûk22 and kuk22 Here, we show that by the boundedness of the kernels k and l, it follows that kûk22 is bounded.
Recall that supx,x0 ∈X |k(x, x0 )| ≤ Bk , supy,y0 |l(y, y0 )| ≤ Bl , our notation t = (v, w) for the test locations, and
zi := (xi , yi ). We first show that the U-statistic core h is bounded.


1

0
0
0
0

|ht ((x, y), (x , y ))| =  (k(x, v) − k(x , v))(l(y, w) − l(y , w))
2
1
≤ (|k(x, v)| + |k(x0 , v)|) (|l(y, w)| + |l(y0 , w)|)
2
≤ 2Bk Bl := 2B,
(8)
where we define B := Bk Bl . It follows that

2
J
J
X
X
X
2
2

kûk22 =
htm (zi , zj ) ≤
[2Bk Bl ] = 4B 2 J,
n(n
−
1)
m=1
m=1
i<j
kuk22 =

J
X
m=1

2

[Ez Ez0 htm (z, z0 )] ≤ 4B 2 J.

Using the upper bounds on kûk22 , kuk22 ,(7) and the definition of c̃, we have


 >

û (Σ̂ + γn I)−1 û − u> Σ−1 u
√
√
J
4B 2 J c̃kΣ − Σ̂kF + 4B J c̃kû − uk2 + 4B 2 J c̃2 γn
≤
γn
c1
kΣ − Σ̂kF + c2 kû − uk2 + c3 γn ,
=:
γn
√
√
where we define c1 := 4B 2 J J c̃, c2 := 4B J c̃, and c3 := 4B 2 J c̃2 . This upper bound implies that
c1
|λ̂n − λn | ≤
nkΣ − Σ̂kF + c2 nkû − uk2 + c3 nγn .
γn
We will separately upper bound kΣ − Σ̂kF and kû − uk2 , and combine them with a union bound.

(9)

(10)

(11)

(12)

An Adaptive Test of Independence with Analytic Kernel Embeddings

F.2.1. B OUNDING kû − uk2
Let t∗ = arg maxt∈{t1 ,...,tJ } |û(t) − u(t)|. Recall that u = (u(t1 ), . . . , u(tJ ))> = (u1 , . . . , uJ )> .
kû − uk2 =

sup hb, û − ui2 ≤

b∈B2 (1)

sup

b∈B2 (1) j=1

≤ |û(t∗ ) − u(t∗ )| sup

J
X

≤
=
√

√

√

|bj ||û(tj ) − u(tj )|

|bj |

b∈B2 (1) j=1

(a)

J
X

J|û(t∗ ) − u(t∗ )| sup kbk2
b∈B2 (1)

∗

∗

J|û(t ) − u(t )|,

(13)

where at (a) we used kak1 ≤ Jkak2 for any a ∈ RJ . From (13), it can be seen that bounding kû − uk2 amounts to
bounding the difference of a U-statistic û(t∗ ) (see (4)) to its expectation u(t∗ ). Combining (13) and (12), we have
√
c1
|λ̂n − λn | ≤
nkΣ − Σ̂kF + c2 n J|û(t∗ ) − u(t∗ )| + c3 nγn .
(14)
γn
F.2.2. B OUNDING kΣ̂ − ΣkF
The plan is to write Σ̂ = Ŝ − ûb ûb> , Σ = S − uu> , so that kΣ̂ − ΣkF ≤ kŜ − SkF + kûb ûb> − uu> kF and bound
separately kŜ − SkF and kûb ûb> − uu> kF .


Recall that Σij = η(ti , tj ), η(t, t0 ) = Exy [ k̃(x, v)˜l(y, w) − u(v, w) k̃(x, v0 )˜l(y, w0 ) − u(v0 , w0 ) ] where k̃(x, v) =
k(x, v) − Ex0 k(x0 , v), and ˜l(y, w) = l(y, w) − Ey0 l(y0 , w). Its empirical estimator (see Proposition 6) is Σ̂ij = η̂(ti , tj )
where
η̂(t, t0 ) =

n


1X
[ k(xi , v)l(yi , w) − ûb (v, w) k(xi , v0 )l(yi , w0 ) − ûb (v0 , w0 ) ]
n i=1
n

=

1X
k(xi , v)l(yi , w)k(xi , v0 )l(yi , w0 ) − ûb (v, w)ûb (v0 , w0 ),
n i=1

Pn
Pn
k(x, v)
:=
k(x, v) − n1 i=1 k(xi , v), and l(y, w)
:=
l(y, w) − n1 i=1 l(yi , w).
We
Pn
∈
RJ×J such that Ŝij
note that n1 i=1 k(xi , v)l(yi , w)
=
ûb (v, w).
We define Ŝ
:=
Pn
1
m=1 k(xm , vi )l(ym , wi )k(xm , vj )l(yi , wj ), and define similarly its population counterpart S such that
n
Sij := Exy [k̃(x, v)˜l(y, w)k̃(x, v0 )˜l(y, w0 )]. We have
Σ̂ = Ŝ − ûb ûb> ,
Σ = S − uu> ,

kΣ̂ − ΣkF = kŜ − S − (ûb ûb> − uu> )kF
b b>

≤ kŜ − SkF + kû û

>

− uu kF .

(15)
(16)

With (16), (14) becomes
|λ̂n − λn | ≤

√
c1 n
c1 n b b>
kŜ − SkF +
kû û − uu> kF + c2 n J|û(t∗ ) − u(t∗ )| + c3 nγn .
γn
γn

We will further separately bound kŜ − SkF and kûb ûb> − uu> kF .
F.2.3. B OUNDING kûb ûb> − uu> kF
kûb ûb> − uu> kF = kûb ûb> − ûb u> + ûb u> − uu> kF

(17)

An Adaptive Test of Independence with Analytic Kernel Embeddings

≤ kûb (ûb − u)> kF + k(ûb − u)u> kF
= kûb k2 kûb − uk2 + kûb − uk2 kuk2
√
≤ 4B Jkûb − uk2 ,

√
where we used (10) and the fact that kûb k2 ≤ 2B J which can be shown similarly to (9) as
2
n X
n
J
X
X
1
2
2


kûb k22 =
[µ̂xy (vm , wm ) − µ̂x (vm )µ̂y (wm )] =
h
(z
,
z
)
≤
[2Bk Bl ] = 4B 2 J.
t
i
j
m
2
n
m=1
m=1
m=1
i=1 j=1
J
X

J
X



Let (ṽ, w̃) := t̃ = arg maxt∈{t1 ,...,tJ } |ûb (t) − u(t)|. We bound kûb − uk2 by
(a)

√

J|ûb (t̃) − u(t̃)|
√ 

= J µ̂xy (t̃) − µ̂x (ṽ)µ̂y (w̃) − u(t̃)
√ 


= J µ̂xy (t̃) − µ[
[
x µy (t̃) + µ
x µy (t̃) − µ̂x (ṽ)µ̂y (w̃) − u(t̃)
√ 
 √ 



J µ[
≤ J µ̂xy (t̃) − µ[
x µy (t̃) − u(t̃) +
x µy (t̃) − µ̂x (ṽ)µ̂y (w̃)
√ 
 √ 


= J û(t̃) − u(t̃) + J µ[
x µy (t̃) − µ̂x (ṽ)µ̂y (w̃) ,

kûb − uk2 ≤

(18)



where at (a) we used the same reasoning as in (13). The bias µ[
x µy (t̃) − µ̂x (ṽ)µ̂y (w̃) in the second term can be bounded
as


µ[

x µy (t̃) − µ̂x (ṽ)µ̂y (w̃)




n X
n X
n
X
X


1
1

k(xi , ṽ)l(yj , w̃) − 2
k(xi , ṽ)l(yj , w̃)
=
n(n
−
1)
n


i=1 j6=i
i=1 j=1




n
n
n X
n
n X
X
X
X


1
1
1

=
k(xi , ṽ)l(yj , w̃) −
k(xi , ṽ)l(yj , w̃)
k(xi , ṽ)l(yi , w̃) − 2
n(n
−
1)
n(n
−
1)
n


i=1 j=1
i=1
i=1 j=1





n
n
n
X


n
1 XX
1

= 1−
k(xi , ṽ)l(yj , w̃) +
k(xi , ṽ)l(yi , w̃)
2
n
−
1
n
n(n
−
1)


i=1 j=1
i=1

 


 

n
n X
n

X
X

 
1
n
1

+
≤  1 −
k(x
,
ṽ)l(y
,
w̃)
k(x
,
ṽ)l(y
,
w̃)

i
j
i
i
  n(n − 1)
2

n
−
1
n


i=1 j=1

i=1

B
B
2B
≤
+
=
.
n−1 n−1
n−1
Combining this upper bound with (18), we have

 8B 2 J
kûb ûb> − uu> kF ≤ 4BJ û(t̃) − u(t̃) +
.
n−1

(19)

With (19), (17) becomes
|λ̂n − λn | ≤

√
 c1 n 8B 2 J
c1 n
4BJc1 n 
kŜ − SkF +
û(t̃) − u(t̃) +
+ c2 n J|û(t∗ ) − u(t∗ )| + c3 nγn .
γn
γn
γn n − 1

F.2.4. B OUNDING kŜ − SkF

(20)

Pn
Recall that VJ = {t1 , . . . , tJ }, Ŝij = Ŝ(ti , tj ) = n1 m=1 k(xm , vi )l(ym , wi )k(xm , vj )l(ym , wj ), and Sij =
S(ti , tj ) = Exy [k̃(x, vi )˜l(y, wi )k̃(x, vj )˜l(y, wj )]. Let (t(1) , t(2) ) = arg max(s,t)∈VJ ×VJ |Ŝ(s, t) − S(s, t)|.

An Adaptive Test of Independence with Analytic Kernel Embeddings

kŜ − SkF =
≤

sup
B∈BF (1)

D
E
B, Ŝ − S

F

J X
J
X

sup

B∈BF (1) i=1 j=1

|Bij ||Ŝij − Sij |





≤ Ŝ(t(1) , t(2) ) − S(t(1) , t(2) )

sup

J X
J
X

B∈BF (1) i=1 j=1

|Bij |

(a)





≤ J Ŝ(t(1) , t(2) ) − S(t(1) , t(2) )

sup
B∈BF (1)

kBkF





= J Ŝ(t(1) , t(2) ) − S(t(1) , t(2) ) ,
where at (a) we used

PJ

(21)

PJ

|Aij | ≤ JkAkF for any matrix A ∈ RJ×J . We arrive at
 4BJc n 

c1 Jn 

1 
û(t̃) − u(t̃)
|λ̂n − λn | ≤
Ŝ(t(1) , t(2) ) − S(t(1) , t(2) ) +
γn
γn
√
c1 n 8B 2 J
+
+ c2 n J|û(t∗ ) − u(t∗ )| + c3 nγn .
γn n − 1
i=1

j=1

(22)





F.2.5. B OUNDING Ŝ(t, t0 ) − S(t, t0 )




Having an upper bound for Ŝ(t, t0 ) − S(t, t0 ) will allow us to bound (22). To keep the notations uncluttered, we will
define the following shorthands.
Expression

Shorthand

Expression

Shorthand

k(x, v)

a

l(y, w)

b

k(x, v0 )

a0

l(y, w )

b0

k(xi , v)

ai

l(yi , w)

bi

k(xi , v0 )

a0i

l(yi , w0 )

b0i

Ex∼Px k(x, v)

ã

Ey∼Py l(y, w)

b̃

Ex∼Px k(x, v0 )
Pn
1
i=1 k(xi , v)
n
Pn
1
0
i=1 k(xi , v )
n

ã0

Ey∼Py l(y, w0 )
Pn
1
i=1 l(yi , w)
n
P
n
1
0
i=1 l(yi , w )
n

b̃0

a
a

0

0

b
0

b

We will also use · to denote a empirical expectation over x, or y, or (x, y). PThe argument under · will detern
mine
take the expectation. For instance, aa0 = n1 i=1 k(xi , v)k(xi , v0 ) and aba0 =
Pnthe variable over which we
1
0
· i.e.,
i=1 k(xi , v)l(yi , w)k(xi , v ), and so on. We define in the same way for the population expectation using e
n
0
0
0
0
g
f
aa = Ex [k(x, v)k(x, v )] and aba = Exy [k(x, v)l(y, w)k(x, v )].
With these shorthands, we can rewrite Ŝ(t, t0 ) and S(t, t0 ) as
n

1X
0
(ai − a)(bi − b)(a0i − a0 )(b0i − b ),
n i=1
h
i
S(t, t0 ) = Exy (a − ã)(b − b̃)(a0 − ã0 )(b0 − b̃0 ) .
Ŝ(t, t0 ) =

By expanding S(t, t0 ), we have

S(t, t0 ) = Exy + aba0 b0 − aba0 b̃0 − abã0 b0 + abã0 b̃0

An Adaptive Test of Independence with Analytic Kernel Embeddings

− ab̃a0 b0 + ab̃a0 b̃0 + ab̃ã0 b0 − ab̃ã0 b̃0

− ãba0 b0 + ãba0 b̃0 + ãbã0 b0 − ãbã0 b̃0

+ ãb̃a0 b0 − ãb̃a0 b̃0 − ãb̃ã0 b̃0 + ãb̃ã0 b̃0



0 b0 − aba
g0 b̃0 − abb
g0 ã0 + abã
e 0 b̃0
^
= +aba
0 b0 b̃ + aa
f0 ã0 b̃ − ãb̃ã0 b̃0
f0 b̃b̃0 + ab
]
− aa

0 bb0 ã + a
0 bãb̃0 + ãã0 bb
f
f0 − ãb̃ã0 b̃0
− ag

0 b0 ãb̃ − ãb̃ã0 b̃0 − ãb̃ã0 b̃0 + ãb̃ã0 b̃0
+ ag

0 b0 − aba
g0 b̃0 − abb
g0 ã0 + abã
e 0 b̃0
^
= +aba
0 b0 b̃ + aa
0 b0 ãb̃
f0 ã0 b̃ + ag
f0 b̃b̃0 + ab
]
− aa

0 bb0 ã + a
0 bãb̃0 + ãã0 bb
f
f0 − 3ãb̃ã0 b̃0 .
− ag

The expansion of Ŝ(t, t0 ) can be done in the same way. By the triangle inequality, we have

 
 
 

 
0

 
0 b0  + aba0 b − aba
g0 b̃0  + abb0 a0 − abb
g0 ã0  + aba0 b0 − abã
e 0 b̃0 
^
Ŝ(t, t0 ) − S(t, t0 ) ≤ aba0 b0 − aba

 

 
 
0
 0 0
0 b0 b̃ + aa0 b b − aa
0 b0 ãb̃
f0 ã0 b̃ + a0 b0 ab − ag
f0 b̃b̃0  + ab0 a0 b − ab
]
aa b b − aa

 
 



0
 0 0
0 bb0 ã + a0 bab − a
0 bãb̃0  + a a0 bb0 − ãã0 bb
f
f0  + 3 aba0 b0 − ãb̃ã0 b̃0  .
a bb a − ag



0 b0  can be bounded by applying the Hoeffding’s inequality. Other terms can be bounded by
^
The first term aba0 b0 − aba
applying Lemma 9. Recall that we write (x1 , . . . , xm )+ for max(x1 , . . . , xm ).



0 b0  (1st term).
^
Bounding aba0 b0 − aba

Since −B 2 ≤ aba0 b0 ≤ B 2 , by the Hoeffding’s inequality (Lemma 14), we have





nt2
 0 0 ^

0
0
P aba b − aba b  ≤ t ≥ 1 − 2 exp − 4 .
2B



0

g0 b̃0  (2nd term).
Bounding aba0 b − aba

Let f1 (x, y) = aba0 = k(x, v)l(y, w)k(x, v0 ) and f2 (y) = b0 = l(y, w0 ). We



0

e 0 b̃0  (4th term).
Bounding aba0 b − abã

Let f1 (x, y) = ab = k(x, v)l(y, w), f2 (x) = a0 = k(x, v0 ) and f3 (y) = b0 =

note that |f1 (x, y)| ≤ (BBk , Bl )+ and |f2 (y)| ≤ (BBk , Bl )+ . Thus, by Lemma 9 with E = 2, we have





nt2
0

g0 b̃0  ≤ t ≥ 1 − 4 exp −
P aba0 b − aba
.
8(BBk , Bl )4+

l(y, w0 ). We can see that |f1 (x, y)|, |f2 (x)|, |f3 (y)| ≤ (B, Bk , Bl )+ . Thus, by Lemma 9 with E = 3, we have





nt2
 0 0 e 0 0
P aba b − abã b̃  ≤ t ≥ 1 − 6 exp −
.
18(B, Bk , Bl )6+


0


Bounding aba0 b − ãb̃ã0 b̃0  (last term). Let f1 (x) = a = k(x, v), f2 (y) = b = l(y, w), f3 (x) = a0 = k(x, v0 ) and

f4 (y) = b0 = l(y, w0 ). It can be seen that |f1 (x)|, |f2 (y)|, |f3 (x)|, |f4 (y)| ≤ (Bk , Bl )+ . Thus, by Lemma 9 with E = 4,
we have



 

nt2
0


P 3 aba0 b − ãb̃ã0 b̃0  ≤ t ≥ 1 − 8 exp −
.
32 · 32 (Bk , Bl )8+
Bounds for other terms can be derived in a similar way to yield




 0 0 g0 0 
rd
(3 term) P abb a − abb ã  ≤ t ≥ 1 − 4 exp −

nt2
8(BBl , Bk )4+


,

An Adaptive Test of Independence with Analytic Kernel Embeddings

(5

th

term)

(6th term)
(7th term)
(8th term)
(9th term)
(10th term)
(11th term)






nt2

 0 0
0
0
]
P aa b b − aa b b̃ ≤ t ≥ 1 − 4 exp −
,
8(BBk , Bl )4+





nt2
0

f0 b̃b̃0  ≤ t ≥ 1 − 6 exp −
,
P aa0 b b − aa
18(Bk2 , Bl )6+





nt2

f0 ã0 b̃ ≤ t ≥ 1 − 6 exp −
P ab0 a0 b − ab
,
18(B, Bk , Bl )6+





nt2
 0 0

0
0
g
P a b ab − a b ãb̃ ≤ t ≥ 1 − 6 exp −
,
18(B, Bk , Bl )6+





nt2

0 bb0 ã ≤ t ≥ 1 − 4 exp −
P a0 bb0 a − ag
,
8(BBl , Bk )4+





nt2
0

0 bãb̃0  ≤ t ≥ 1 − 6 exp −
P a0 bab − af
,
18(B, Bk , Bl )6+





nt2

f0  ≤ t ≥ 1 − 6 exp −
P a a0 bb0 − ãã0 bb
.
18(Bk , Bl2 )6+

By the union bound, we have





P Ŝ(t, t0 ) − S(t, t0 ) ≤ 12t









nt2
nt2
nt2
nt2
≥ 1 − 2 exp − 4 + 4 exp −
+
4
exp
−
+
6
exp
−
2B
8(BBk , Bl )4+
8(BBl , Bk )4+
18(B, Bk , Bl )6+








2
2
2
nt
nt
nt2
nt
+ 6 exp −
+ 6 exp −
+ 6 exp −
4 exp −
8(BBk , Bl )4+
18(Bk2 , Bl )6+
18(B, Bk , Bl )6+
18(B, Bk , Bl )6+








nt2
nt2
nt2
nt2
4 exp −
+
6
exp
−
+
6
exp
−
+
8
exp
−
8(BBl , Bk )4+
18(B, Bk , Bl )6+
18(Bk , Bl2 )6+
32 · 32 (Bk , Bl )8+









2
2
2
2
nt
nt
nt
nt
+
8
exp
−
+
24
exp
−
= 1 − 2 exp − 4 + 8 exp −
2B
8(BBk , Bl )4+
8(BBl , Bk )4+
18(B, Bk , Bl )6+






2
2
2
nt
nt
nt
+ 6 exp −
+ 6 exp −
+ 8 exp −
18(Bk2 , Bl )6+
18(Bk , Bl2 )6+
32 · 32 (Bk , Bl )8+









122 nt2
122 nt2
122 nt2
122 nt2
≥ 1 − 2 exp −
+
8
exp
−
+
8
exp
−
+
24
exp
−
B∗
B∗
B∗
B∗






2
2
2
2
2
2
12 nt
12 nt
12 nt
+ 6 exp −
+ 8 exp −
+ 6 exp −
B∗
B∗
B∗


122 nt2
,
= 1 − 62 exp −
B∗

where
B ∗ :=

1
max(2B 4 , 8(BBk , Bl )4+ , 8(BBl , Bk )4+ , 18(B, Bk , Bl )6+ , 18(Bk2 , Bl )6+ , 18(Bk , Bl2 )6+ , 32 · 32 (Bk , Bl )8+ ).
122

By reparameterization, it follows that





c1 Jn 
γ 2 t2

P
Ŝ(t, t0 ) − S(t, t0 ) ≤ t ≥ 1 − 62 exp − 2 n2 ∗ .
γn
c1 J nB




F.2.6. U NION B OUND FOR λ̂n − λn  AND F INAL L OWER B OUND
Recall from (22) that
 4BJc n 

c1 Jn 

1 
û(t̃) − u(t̃)
Ŝ(t(1) , t(2) ) − S(t(1) , t(2) ) +
γn
γn
2
√
c1 n 8B J
+
+ c2 n J|û(t∗ ) − u(t∗ )| + c3 nγn .
γn n − 1

|λ̂n − λn | ≤

(23)

An Adaptive Test of Independence with Analytic Kernel Embeddings

We will bound terms in (22) separately and combine all the bounds with the union bound. As shown in (8), the U-statistic
core h is bounded between −2B and 2B. Thus, by Lemma 13 (with m = 2), we have



 √
b0.5nct2
.
P c2 n J|û(t∗ ) − u(t∗ )| ≤ t ≥ 1 − 2 exp − 2 2
8c2 n JB 2
Bounding

c1 n 8B 2 J
γn n−1

(24)



1n 
û(t̃) − u(t̃). By Lemma 13 (with m = 2), it follows that
+ c3 nγn + 4BJc
γn



c1 n 8B 2 J
4BJc1 n 

P
û(t̃) − u(t̃) ≤ t
+ c3 nγn +
γn n − 1
γn

h
i2 
c1 n 8B 2 J
2
t
−
b0.5ncγ
−
c
nγ
3
n
n
γn n−1


≥ 1 − 2 exp −

2
7
4
2
2
2 B J c1 n

2 !
b0.5nc tγn (n − 1) − 8c1 B 2 nJ − c3 n(n − 1)γn2
= 1 − 2 exp −
27 B 4 J 2 c21 n2 (n − 1)2
2 !

(a)
tγn (n − 1) − 8c1 B 2 nJ − c3 n(n − 1)γn2
,
≥ 1 − 2 exp −
28 B 4 J 2 c21 n2 (n − 1)

(25)

where at (a) we used b0.5nc ≥ (n − 1)/2. Combining (23), (24), and (25) with the union bound (set T = 3t), we can
bound (22) with







γ2 T 2
b0.5ncT 2


P λ̂n − λn  ≤ T ≥ 1 − 62 exp − 2 2n 2 ∗ − 2 exp −
3 c1 J nB
72c22 n2 JB 2

2 !
T γn (n − 1)/3 − 8c1 B 2 nJ − c3 γn2 n(n − 1)
.
− 2 exp −
28 B 4 J 2 c21 n2 (n − 1)




Since λ̂n − λn  ≤ T implies λ̂n ≥ λn − T , a reparametrization with r = λn − T gives
 2





γ (λn − r)2
b0.5nc(λn − r)2
P λ̂n ≥ r ≥ 1 − 62 exp − n2 2 2 ∗ − 2 exp −
3 c1 J nB
72c22 n2 JB 2

2 !
(λn − r)γn (n − 1)/3 − 8c1 B 2 nJ − c3 γn2 n(n − 1)
− 2 exp −
28 B 4 J 2 c21 n2 (n − 1)
:= L(λn ).
Grouping constants into ξ1 , . . . ξ5 gives the result.
The lower bound L(λn ) takes the form




[(λn − Tα )C3 − C4 ]2
1 − 62 exp −C1 (λn − Tα )2 − 2 exp −C2 (λn − Tα )2 − 2 exp −
,
C5
where C1 , . . . , C5 are positive constants. For fixed large enough n such that λn > Tα , and fixed significance level α,
increasing λn will increase L(λn ). Specifically, since n is fixed, increasing u> Σ−1 u in λn = nu> Σ−1 u will increase
L(λn ).

G. Helper Lemmas
This section contains lemmas used to prove the main results in this work.
Q

QE
 E

Lemma 8 (Product to sum). Assume that |ai | ≤ B, |bi | ≤ B for i = 1, . . . , E. Then  i=1 ai − i=1 bi  ≤
P
E
B E−1 j=1 |aj − bj |.

An Adaptive Test of Independence with Analytic Kernel Embeddings

Proof.



 
 E−1

 Y

Y
 Y
E
E
E−1
E−2
E
 Y

Y
Y
Y
Y


E
 E
 


 ai −

bj −
ai bE −
bj 
ai bE  + 
ai bE−1 bE  + . . . + a1
bj  ≤  ai −


 

 j=2
i=1
i=1
i=1
j=1 
i=1
i=1
j=1 


 E−2 ! 
E−1 

Y
E
 Y

Y 






bj 
ai bE  + . . . + |a1 − b1 | 
≤ |aE − bE | 
ai  + |aE−1 − bE−1 | 






j=2

i=1

i=1

≤ |aE − bE |B E−1 + |aE−1 − bE−1 | B E−1 + . . . + |a1 − b1 | B E−1
= B E−1

E
X
j=1

|aj − bj |

applying triangle inequality, and the boundedness of ai and bi -s.
(i)

i
Lemma 9 (Product variant of the Hoeffding’s inequality). For i = 1, . . . , E, let {xj }nj=1
⊂ Xi be an i.i.d. sample from
a distribution Pi , and fi : Xi 7→ R be a measurable function. Note that it is possible that P1 = P2 = · · · = PE and
(1) 1
(E) E
{xj }nj=1
= · · · = {xj }nj=1
. Assume that |fi (x)| ≤ B < ∞ for all x ∈ Xi and i = 1, . . . , E. Write P̂i to denote an

(i)

i
empirical distribution based on the sample {xj }nj=1
. Then,
"
#
"
#
!


E
E
E
 Y

Y
X
ni T 2

(i)
(i) 
P 
Ex(i) ∼P̂i fi (x ) −
Ex(i) ∼Pi fi (x )  ≤ T ≥ 1 − 2
exp − 2 2E .


2E B

i=1

i=1

i=1

Proof. By Lemma 8, we have
" E
#
# "E
E 


 Y
X
Y



(i) 
(i)
Ex(i) ∼P̂i fi (x ) −
Ex(i) ∼Pi fi (x )  ≤ B E−1
Ex(i) ∼P̂i fi (x(i) ) − Ex(i) ∼Pi fi (x(i) ) .



i=1
i=1
i=1





By applying the Hoeffding’s inequality to each term in the sum, we have P Ex(i) ∼P̂i fi (x(i) ) − Ex(i) ∼Pi fi (x(i) ) ≤ t ≥


2
it
1 − 2 exp − 2n
. The result is obtained with a union bound.
2
4B

H. External Lemmas
In this section, we provide known results referred to in this work.
Lemma 10 (Chwialkowski et al. (2015, Lemma 1)). If k is a bounded, analytic kernel (in the sense given in Definition 1) on
Rd × Rd , then all functions in the RKHS defined by k are analytic.
Lemma 11 (Chwialkowski et al. (2015, Lemma 3)). Let Λ be an injective mapping from the space of probability measures
into a space of analytic functions on Rd . Define
d2VJ (P, Q) =

J
X
j=1

2

|[ΛP ](vj ) − [ΛQ](vj )| ,

where VJ = {vi }Ji=1 are vector-valued i.i.d. random variables from a distribution which is absolutely continuous with
respect to the Lebesgue measure. Then, dVJ (P, Q) is almost surely (w.r.t. VJ ) a metric.
Lemma 12 (Bochner’s theorem (Rudin, 2011)). A continuous function Ψ : Rd → R is positive definite if and only if it is
R
>
the Fourier transform of a finite nonnegative Borel measure ζ on Rd , that is, Ψ(x) = Rd e−ix ω dζ(ω), x ∈ Rd .

Lemma 13 (A bound for U-statistics (Serfling, 2009, Theorem A, p. 201)). Let h(x1 , . . . , xm ) be a U-statistic kernel for

n −1 P
an m-order U-statistic such that h(x1 , . . . , xm ) ∈ [a, b] where a ≤ b < ∞. Let Un = m
i1 <···<im h(xi1 , . . . , xim )

n
be a U-statistic computed with a sample of size n, where the summation is over the m
combinations of m distinct elements
{i1 , . . . , im } from {1, . . . , n}. Then, for t > 0 and n ≥ m,

P(Un − Eh(x1 , . . . , xm ) ≥ t) ≤ exp −2bn/mct2 /(b − a)2 ,

An Adaptive Test of Independence with Analytic Kernel Embeddings


P(|Un − Eh(x1 , . . . , xm )| ≥ t) ≤ 2 exp −2bn/mct2 /(b − a)2 ,
where bxc denotes the greatest integer which is smaller than or equal to x. Hoeffind’s inequality is a special case when
m = 1.
Lemma 14 (Hoeffding’s
inequality). Let X1 , . . . , Xn be i.i.d. random variables such that a ≤ Xi ≤ b almost surely.
Pn
Define X := n1 i=1 Xi . Then,





2nα2


P X − E[X] ≤ α ≥ 1 − 2 exp −
.
(b − a)2

References
[sup4] K. P. Chwialkowski, A. Ramdas, D. Sejdinovic, and A. Gretton. Fast Two-Sample Testing with Analytic Representations
of Probability Measures. In Advances in Neural Information Processing Systems (NIPS), pages 1981–1989. 2015.
[sup14] W. Jitkrittum, Z. Szabó, K. Chwialkowski, and A. Gretton. Interpretable Distribution Features with Maximum Testing
Power. 2016. URL http://arxiv.org/abs/1605.06796.
[sup3] W. Rudin. Fourier analysis on groups. John Wiley & Sons, 2011.
[sup20] R. J. Serfling. Approximation Theorems of Mathematical Statistics. John Wiley & Sons, 2009.

