Conditional Accelerated Lazy Stochastic Gradient Descent

Guanghui Lan * 1 Sebastian Pokutta * 1 Yi Zhou * 1 Daniel Zink * 1

Abstract

Compared to most other first-order methods, such as e.g.,
gradient descent algorithms and accelerated gradient algorithms (Nesterov, 1983; 2004), the CG method is computationally cheaper in some cases, since it only requires the
solution of a linear optimization subproblem (3) rather than
an often costly projection onto the feasible region X.

In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm
with optimal number of calls to a stochastic firstorder oracle and convergence rate O( ε12 ) improving over the projection-free, Online Frank-Wolfe
based stochastic gradient descent of (Hazan and
Kale, 2012) with convergence rate O( ε14 ).

1. Introduction
The conditional gradient method (also known as: FrankWolfe algorithm) proposed in (Frank and Wolfe, 1956),
gained much popularity in recent years due to its simple
projection-free scheme and fast practical convergence rates.
We consider the basic convex programming (CP) problem
f ∗ := min f (x),
x∈X

(1)

where X ⊆ Rn is a closed convex set and f : X → R is a
smooth convex function such that ∃L > 0,
kf 0 (x) − f 0 (y)k∗ ≤ Lkx − yk, ∀ x, y ∈ X.

(2)

The classic conditional gradient (CG) method solves (1) iteratively by minimizing a series of linear approximations of f
over the feasible set X. More specifically, given xk−1 ∈ X
at the k-th iteration, it updates xk according to the following
steps:
1) Call the first-order (FO) oracle to compute
(f (xk−1 ), f 0 (xk−1 )) and set pk = f 0 (xk−1 ).
2) Call the linear optimization (LO) oracle to compute
yk ∈ argminx∈X hpk , xi.

(3)

3) Set xk = (1 − λk )xk−1 + λk yk for some λk ∈ [0, 1].
*
Equal contribution 1 ISyE, Georgia Institute of Technology, Atlanta, GA. Correspondence to: Daniel Zink
<daniel.zink@gatech.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

There has been extensive and fruitful research on the general class of linear-optimization-based convex programming (LCP) methods (which covers the CG method and
its variants) and their application in machine learning (e.g.,
(Ahipasaoglu and Todd, 2013; Bach et al., 2012; Beck and
Teboulle, 2004; Cox et al., 2013; Clarkson, 2010; Freund
and Grigas, 2013; Hazan, 2008; Harchaoui et al., 2012;
Jaggi, 2011; 2013; Jaggi and Sulovský, 2010; Luss and
Teboulle, 2013; Shen et al., 2012; Hazan and Kale, 2012;
Lan, 2013; Lan and Zhou, 2014; Braun et al., 2016)). It
should be noted that even the computational cost for LO
oracle to solve the linear optimization subproblem (3) is
high for some complex feasible regions. Recently, several
approaches have been considered to address this issue. Jaggi
demonstrated practical speed up for the CG method by approximately solving (3) in (Jaggi, 2013). Braun, Pokutta,
and Zink in (Braun et al., 2016) proposed a class of modified CG methods, namely the lazy conditional gradient
(LCG) algorithms, which calls a weak separation oracle
rather than solving the linear subproblem (3) in the classical
CG method. In fact, the weak separation oracle is computationally more efficient than approximate minimization
used in (Jaggi, 2013), at the expense of not providing any
guarantee for function value improvement with respect to
(3). Furthermore, as shown in (Jaggi, 2013; Lan, 2013), the
total number of iterations for the LCP methods to find an
-solution of (1) (i.e., a point x̄ ∈ X, s.t. f (x̄) − f ∗ ≤ )
cannot be smaller than O(1/), which is not improvable
even when the objective function f is strongly convex.
Improved complexity results can only be obtained under
stronger assumptions on the LO oracle or the feasible set
(see, e.g., (Garber and Hazan, 2013; Lan, 2013)). However,
the O(1/) bound does not preclude the existence of more
efficient LCP algorithms for solving (1). Lan and Zhou in
(Lan and Zhou, 2014) proposed a class of conditional gradient sliding methods (CGS), which significantly improve
the complexity bounds in terms of the number of gradient
evaluations while maintaining optimal complexity bounds

Conditional Accelerated Lazy Stochastic Gradient Descent

for the LO oracle calls required by the LCP methods.

Contributions

Inspired by (Braun et al., 2016) and (Lan and Zhou, 2014),
in this paper we focus on a class of modified LCP methods
that require only improving solutions for a certain separation problem rather than solving the linear optimization
subproblem (3) explicitly through LO oracle calls while
simultaneously minimizing the number of gradient evaluations when performing weak separation over the feasible set
X. At first these two objectives seem to be incompatible as
(Braun et al., 2016) give up the dual guarantee to simplify
the oracle, while the dual guarantee of CG iterations is at the
core of the analysis in (Lan and Zhou, 2014). We overcome
this impasse by carefully modifying both techniques.

Our main contributions can be briefly summarized as follows. We consider stochastic smooth optimization, where
we have only access unbiased estimators of the gradients of
f via a stochastic first-order (SFO) oracle. By incorporating a modified LCG procedure (Braun et al., 2016) into a
modified CGS method (Lan and Zhou, 2014) we obtain a
new conditional accelerated lazy stochastic gradient descent
algorithm (CALSGD) and we show that the number of calls
to the weak separation oracle can be optimally bounded by
O(1/), while the optimal bound of O(1/2 ) on the total
number of calls to the SFO oracle can be maintained. In
addition, if the exact gradients of f can be accessed by a
FO oracle,
√ the latter bound can be significantly improved to
O(1/ ). In order to achieve the above we will present a
modified lazy conditional gradient method, and show that
the total number of iterations (or calls to the weak separation
oracle) performed by it can be bounded by O(1/) under
a stronger termination criterion, i.e., the primal-dual gap
function.

It should be mentioned that Hazan and Kale in (Hazan
and Kale, 2012) proposed the online Frank-Wolfe (OFW)
algorithm, which obtains O(1/4 ) rate of convergence for
stochastic problems. Indeed, if we consider the objective
function f (x) := E[F (x, ξ)] for stochastic optimization,
the OFW method can be applied to solve (1) by viewing the
iteratively observed function ft as the current realization of
the true objective function f , i.e., ft (·) = F (·, ξt ). Without
re-evaluating the (sub)gradients at the updated points, the
OFW obtains O(T −1/4 ) bound for any (smooth or nonsmooth) objective functions (see Theorem 4.4 in (Hazan and
Kale, 2012)), which implies O(1/4 ) rate of convergence
in terms of the number of (sub)gradient evaluations for
stochastic optimization. However, we can show that our
proposed algorithm obtains O(1/) (resp., O(1/2 )) rate
of convergence for smooth (resp., non-smooth) stochastic
problems, which is much better than the convergence rate of
the OFW method. We would like to stress that the stochastic
optimization bound in (Hazan and Kale, 2012, Theorem 4.1)
which gives a guarantee of O(1/2 ), requires to re-evaluate
all gradients at the current iterate and as such the number
of gradient evaluations required grows quadratically in t.
Moreover, Hazan and Luo (2016) proposed two methods for
solving the special case of Problem (1) of the form
m

1 X
min f (x) = min
fi (x),
x∈X
x∈X m
i=1
which allows for a potentially smaller number of SFO evaluations than O(1/ε2 ), the lower bound for the general
problem. The two methods Stochastic Variance-Reduced
Frank-Wolfe (SVRF) and Stochastic Variance-Reduced Conditional Gradient Sliding (STORC) are obtained by applying
the variance reduction idea of Johnson and Zhang (2013)
and Mahdavi et al. (2013) to the CG method and the Stochastic CGS method respectively. Both algorithms however need
a certain number of exact (or full) gradient evaluations leading to a potentially undesirable dependence on the number
of examples m.

We also consider strongly convex and smooth functions and
show that without enforcing any stronger assumptions on the
weak separation oracle or the feasible set X, the total number of calls to the FO (resp., SFO) oracle can be optimally
bounded by O(log 1/) (resp., O(1/)) for variants of the
proposed method to solve deterministic (resp., stochastic)
strongly convex and smooth problems. Furthermore, we
also generalize the proposed algorithms to solve an important class of non-smooth convex programming problems
with a saddle point structure. By adaptively approximating the original non-smooth problem via a class of smooth
functions, we are able to show that the deterministic version
of CALSGD can obtain an -solution within O(1/) number of linear operator evaluations and O(1/2 ) number of
calls to the weak separation oracle, respectively. The former
bound will increase to O(1/2 ) for non-smooth stochastic
optimization.
Finally, we demonstrate practical speed ups of CALSGD
through preliminary numerical experiments for the video
co-localization problem, the structured regression problem
and quadratic optimization over the standard spectrahedron;
an extensive study is beyond the scope of this paper and
left for future work. In all cases we report a substantial
improvements in performance.
In the main body of the paper we focus on the stochastic
smooth case; several other results and their proofs have been
relegated to the Supplementary Material.
1.1. Notation and Terminology
Let X ⊆ Rn be a convex compact set, and k · kX be the
norm associated with the inner product in Rn . For the sake

Conditional Accelerated Lazy Stochastic Gradient Descent

of simplicity, we often skip the subscript in the norm k · kX .
We define the diameter of the set X as
DX ≡ DX,k·k := max kx − yk.

(4)

x,y∈X

For a given norm k · k, we denote its conjugate by ksk∗ =
maxkxk≤1 hs, xi. For a linear operator A : Rn → Rm , we
use kAk to denote its operator norm defined as kAk :=
maxkxk≤1 kAxk. Let f : X → R be a convex function, we
denote its linear approximation at x by
lf (x; y) := f (x) + hf 0 (x), y − xi.

(5)

Clearly, if f satisfies (2), then
f (y) ≤ lf (x; y) +

L
2 ky

− xk2 , ∀ x, y ∈ X.

(6)

Notice that the constant L in (2) and (6) depends on k · k.
Moreover, we say f is smooth with curvature at most C, if
f (y) ≤ lf (x; y) +

C
2,

∀ x, y ∈ X.

(7)

2
It is clear that if X is bounded, we have C ≤ LDX
. In
the following we also use R++ to denote the set of strictly
positive reals.

2. Conditional Accelerated Lazy Stochastic
Gradient Descent
We now present a new method for stochastic gradient descent that is based on the stochastic conditional gradient
sliding (SCGS) method and the parameter-free lazy conditional gradient (LCG) procedure from Section 2.2, which
we refer to as the Conditional Accelerated Lazy Stochastic
Gradient Descent (CALSGD) method.
We consider the stochastic optimization problem:
f ∗ := min{f (x) = Eξ [F (x, ξ)]},

(8)

x∈X

where f (x) is a smooth convex function satisfying (2).

Algorithm 1 Conditional Accelerated Lazy Stochastic Gradient Descent (CALSGD)
Input: Initial point x0 ∈ X, iteration limit N , and weak
separation oracle accuracy α ≥ 1.
Let βk ∈ R++ , γk ∈ [0, 1], and ηk ∈ R+ , k = 1, 2, . . .,
be given and set y0 = x0 .
for k = 1, 2, . . . , N do
zk = (1 − γk )yk−1 + γk xk−1 ,
PBk 0
gk = B1k j=1
F (zk , ξk,j ),

(11)

xk = LCG(gk , βk , xk−1 , α, ηk ),

(13)

yk = (1 − γk )yk−1 + γk xk ,

(14)

Throughout this section, we assume that there exists a
stochastic first-order (SFO) oracle, which for a search point
zk ∈ X outputs a stochastic gradient F 0 (zk , ξk ), s.t.
(9)
(10)

If σ = 0, the stochastic gradient F 0 (zk , ξk ) is the exact
gradient at point zk , i.e., F 0 (zk , ξk ) = f 0 (zk ).
Our algorithmic framework is inspired by the SCGS method
by (Lan and Zhou, 2014). However, instead of applying
the classic CG method to solve the projection subproblem

(12)

where F 0 (zk , ξk,j ), j = 1, . . . , Bk , are stochastic gradients computed by the SFO at zk .
end for
Output: yN .

We hasten to make some observations about the CALSGD
method. Firstly, we apply mini-batches to estimate the
gradient at point zk , where the parameter {Bk } denotes the
batch sizes used to compute gk . It can be easily seen from
(9), (10), and (12) that
E[gk − f 0 (zk )] = 0 and E[kgk − f 0 (zk )k2∗ ] ≤

2.1. The Algorithm

E [F 0 (zk , ξk )] = f 0 (zk ),
 0

E kF (zk , ξk ) − f 0 (zk )k2∗ ≤ σ 2 .

appearing in the accelerated gradient (AG) method, the
CALSGD method utilizes a modified parameter-free LCG
algorithm (see Section 2.2) to approximately solve the subproblem ψ(x) defined in (16) and skips the computations
of the stochastic gradient F 0 (z, ξ) from time to time when
performing weak separation over the feasible region X. The
main advantages of our method are that it does not solve
a traditional projection problem and achieves the optimal
bounds on the number of calls to the SFO and LOsepX
oracles (see Oracle 1 in Subsection 2.2) for solving problem
(1)-(8). To the authors’ best knowledge, no such algorithms
have been developed before in the literature; we present the
algorithm below in Algorithm 1.

σ2
Bk ,

(15)

and hence gk is an unbiased estimator of f 0 (zk ). In fact,
PBk
letting SBk = j=1
(F 0 (zk , ξk,j ) − f 0 (zk )), from (9) and
(10), by induction, we have




E kSBk k2∗ = E kSBk −1 + F 0 (zk , ξk,Bk ) − f 0 (zk )k2∗

= E kSBk −1 k2∗ + kF 0 (zk , ξk,Bk ) − f 0 (zk )k2∗

+2hSBk −1 , F 0 (zk , ξk,Bk ) − f 0 (zk )i]


= E kSBk −1 k2∗


+ E kF 0 (zk , ξk,Bk ) − f 0 (zk )k2∗

PBk  0
= j=1
E kF (zk , ξk,j ) − f 0 (zk )k2∗ ≤ Bk σ 2 ,

Conditional Accelerated Lazy Stochastic Gradient Descent

which together with the fact that gk − f 0 (zk ) =
PBk
1
1
0
0
j=1 [F (zk , ξk,j ) − f (zk )] = Bk SBk , implies the
Bk
second relationship in (15).
Secondly, in view of the SCGS method in (Lan and Zhou,
2014), xk obtained in (13) should be an approximate solution to the gradient sliding subproblem
n
o
min ψk (x) := hgk , xi + β2k kx − xk−1 k2 ,
(16)
x∈X

such that for some ηk ≥ 0 we have
hψk0 (xk ), xk − xi = hgk + βk (xk − xk−1 ), xk − xi ≤ ηk ,
(17)
for all x ∈ X. If we solve the subproblem (16) exactly
(i.e., ηk = 0), then CALSGD will reduce to the accelerated stochastic approximation method by (Lan, 2009; 2012).
However, by employing the LCG procedure (see Procedure 1 in Subsection 2.2), we only need to use a weak
separation oracle, but still maintaining the optimal bounds
on stochastic first-order oracle as in (Lan, 2009; 2012; Lan
and Zhou, 2014).
Thirdly, observe that the CALSGD method so far is conceptual only as we have not yet specified the LCG procedure
and the parameters {Bk }, {βk }, {γk }, and {ηk }. We will
come back to this issue after introducing the LCG procedure
and establishing its main convergence properties.

Observe that the oracle has two output modes. In particular,
Oracle 1 first verifies whether there exists an improving
point y ∈ P with the required guarantee and if so it outputs this point, which we refer it as a positive call. If no
such point exists the oracle certifies this by providing the
maximizer y, which then also provides a new duality gap.
We refer to this case as a negative call. The computational
advantages of this oracle are that it can reuse previously
seen solutions y if they satisfy the improvement condition
and even if LO oracle has to be called, the optimization can
be terminated early once the improvement condition is satisfied. Finally, the parameter α allows to only approximately
satisfy the improvement condition making separation even
easier; in our applications we set the parameter α slightly
larger than 1.
We present the LCG procedure based on (Braun et al., 2016)
below. We adapted the parameter-free version to remove
any dependence on hard to estimate parameters. For any
smooth convex function φ, we define its duality gap as
gapφ,X (x) ≡ gapφ (x) := max ∇φ(x)T (x − y).
y∈X

(18)

Clearly, by convexity the duality gap is an upper bound on
f (x∗ ) − f (x). Given any accuracy parameter η ≥ 0, the
LCG procedure solves minx∈X φ(x) approximately with
accuracy η, i.e., it outputs a point ū ∈ X, s.t. gapφ (ū) ≤ η.

We present the LOsep oracle in Oracle 1 below.

Procedure 1 Parameter-free Lazy Conditional Gradients
(LCG) procedure
Input: access to gradients of smooth convex function φ,
u1 ∈ X vertex, LOsepX weak linear separation oracle,
accuracy α ≥ 1, duality gap bound η
Output: ū ∈ X with bounded duality gap, i.e., gapφ (ū) ≤
η
1: Φ0 ← maxu∈X ∇φ(u1 )T (u1 − u)
2: for t = 1 to T − 1 do
3:
vt ← LOsepX (∇φ(ut ), xt , Φt−1 , α)
4:
if not ∇φ(ut )T (ut − vt ) > Φt−1 /α then
5:
if Φt−1 = η then
6:
return ū = ut
7:
end if
8:
else
n
o
9:
Φt ← max Φt−1
,
η
{Update Φt }
2
10:
end if
11:
λt ← argmin φ((1 − λt )ut + λt vt )
12:
ut+1 ← (1 − λt )ut + λt vt
13: end for

Oracle 1 Weak Separation Oracle LOsepP (c, x, Φ, α)
Input: c ∈ Rn linear objective, x ∈ P point, α ≥ 1 accuracy, Φ > 0 objective value;
Output: y ∈ P vertex with either (1) cT (x − y) > Φ/α,
or (2) y = argmaxy∈P cT (x − z) ≤ Φ.

The LCG procedure is a parameter-free algorithm. Note
that while line search can be expensive in general, for our
subproblems, function evaluation is very cheap. The algorithm needs only one LO oracle call to estimate the
initial functional value gap at Line 1. Alternatively, this

2.2. The Parameter-free Lazy Conditional Gradient
Procedure
The classical CG method is a well-known projection-free
algorithm, which requires only the solution of a linear optimization subproblem (3) rather than the projection over
X per iteration. Therefore, it has computational advantages
over many other first-order methods when projection over
X being costly. The LCG procedure presented in this subsection, a modification of the vanilla LCG method in (Braun
et al., 2016), goes several steps further than CG and even
vanilla LCG method. Firstly, it replaces LO oracle by a
weaker separation oracle LOsep, which is no harder than
linear optimization and often much simpler. Secondly, it
uses a stronger termination criterion, the Frank-Wolfe gap
(cf. (18)), than vanilla LCG method. Finally, it maintains
the same order of convergence rate as the CG and the vanilla
LCG method.

Conditional Accelerated Lazy Stochastic Gradient Descent

can be also done approximately via binary search with
LOsep. The algorithm maintains a sequence, {Φt }, that
provides valid upper bounds for the functional value gap
at the current iterate, i.e., φ(ut ) − φ∗ ≤ 2Φt−1 (see Theorem 5.1 of (Braun et al., 2016)), and it halves the value
of Φt only when the current oracle call is negative. Finally,
our LCG procedure exits at Line 5 whenever LOsepX returns a negative call and Φt−1 = η, which ensures that
gapφ (ū) = maxy∈X h∇φ(ū), ū − yi ≤ η.
Theorem 2.1 below provides a bound for the total number of iterations (or calls to the LOsepX oracle) that the
LCG procedure requires to generate a point ū ∈ X with
gapφ (ū) ≤ η.

Theorem 2.1. Procedure 1 returns a point ū ∈ X such that
the duality gap at point ū is bounded by η, i.e., gapφ (ū) ≤ η.
Furthermore, the total number of iterations T (and hence
LOsepX calls) performed by Procedure 1 is at most
T ≤

(
κ+

8α2 Cφ
η

κ + 4α +

l
with κ := 4α log

Φ0
αCφ

m

+ 2,
4α2 Cφ
η

+ log

η < αCφ ;
+ 2,

η ≥ αCφ ,

(19)

Φ0
η .

Proof. From the observations above, it is clear that the duality gap at the output point ū is bounded by η.
Also observe that the procedure calls LOsepX once per
iteration. In order to demonstrate the bound in (19), we split
the LCG procedure into two phases, and bound the number
of iterations separately for each phase. Let Cφ denote the
curvature of the smooth convex function φ.
We say Procedure 1 is in the first phase whenever Φt−1 > η.
In view of Theorem 5.1 in (Braun et al., 2016), it is clear that
the number of iterations in the first phase can be bounded as
l
T1 ≤ 4α log

Φ0
αCφ

m

+

4α2 Cφ
η

+ log

Φ0
η .

Procedure 1 enters the second phase when Φt−1 ≤ η. Again
with the argumentation in Theorem 5.1 in (Braun et al.,
2016), we obtain that the total number of positive calls in
4α2 C
this phase can be bounded by η φ , if η < αCφ , or by 4α
if η ≥ αCφ . Moreover, the procedure exits whenever the
current LOsepX oracle call is a negative call. Hence, the
number of iterations in the second phase can be bounded by
(
T2 ≤

4α2 Cφ
η

+ 1,
4α + 1,

η < αCφ ;
η ≥ αCφ .

Thus, our bound in (19) can be obtained from the above two
bounds plus one more LO oracle call at Line 1.

2.3. The Convergence Properties of CALSGD
This subsection is devoted to establishing the main convergence properties of the CALSGD method. Since the
algorithm is stochastic, we will establish the convergence
results for finding a stochastic -solution, i.e., a point x̄ ∈ X
s.t. E[f (x̄) − f (x∗ )] ≤ . We first state a simple technical
result from (Lan and Zhou, 2014, Lemma 2.1) that we will
use.
Lemma 2.2. Let wt ∈ (0, 1], t = 1, 2, . . ., be given. Also
let us denote
(
1
t=1
Wt :=
(1 − wt )Wt−1 t ≥ 2.
Suppose that Wt > 0 for all t ≥ 2 and that the sequence
{δt }t≥0 satisfies
δt ≤ (1 − wt )δt−1 + Bt , t = 1, 2, . . . .
Then for any 1 ≤ l ≤ k, we have

Pk Bi 
l
δk ≤ Wk 1−w
δ
+
l−1
i=l Wi .
Wl
Theorem 2.3 describes the main convergence properties of
the CALSGD method (cf. Algorithm 1). The proof of this
theorem can be found in the Supplementary material A.
Theorem 2.3. Let Γk be defined as follows,
(
1
k=1
Γk :=
Γk−1 (1 − γk ) k ≥ 2.

(20)

Suppose that {βk } and {γk } in the CALSGD algorithm
satisfy
γ1 = 1 and Lγk ≤ βk , k ≥ 1.
(21)
a) If
β k γk
Γk

≥

βk−1 γk−1
Γk−1 ,

k ≥ 2,

(22)

then under assumptions (9) and (10), we have
E [f (yk ) − f (x∗ )] ≤

βk γk 2
2 DX
k h
X

+ Γk

ηi γi
Γi

+

γi σ 2
2Γi Bi (βi −Lγi )

i

,

i=1

(23)
where x∗ is an arbitrary optimal solution of (8) and
DX is defined in (4).
b) If
β k γk
Γk

≤

βk−1 γk−1
Γk−1 ,

k ≥ 2,

(24)

(rather than (36)) is satisfied, then the result in part a)
2
holds by replacing βk γk DX
with β1 Γk kx0 − x∗ k2 in
the first term of the RHS of (37).

Conditional Accelerated Lazy Stochastic Gradient Descent

c) Under the assumptions in part a) or b), the number of
inner iterations performed at the k-th outer iterations
is bounded by
(
Tk =

κ+
κ+

2
8α2 βk DX
+ 2,
ηk
2
4α2 βk DX
4α +
ηk

Φk
0

l
with κ := 4α log

2
αβk DX

2
ηk < αβk DX
;

2
ηk ≥ αβk DX
,
(25)
m
Φk
+ log ηk0 .

+ 2,

Now we provide two different sets of parameters
{βk }, {γk }, {ηk }, and {Bk }, which lead to optimal complexity bounds on the number of calls to the SFO and
LOsepX oracles.
Corollary 2.4. Suppose that {βk }, {γk }, {ηk }, and {Bk }
in the CALSGD method are set to
2
LDX

3
γk = k+2
, ηk = k(k+1) ,
m
l 2
3
and Bk = σ L(k+2)
, k ≥ 1,
2 D2

βk =

4L
k+2 ,

(26)

X

and we assume kf 0 (x∗ )k is bounded for any optimal solution x∗ of (8). Under assumptions (9) and (10), we have
E [f (yk ) − f (x∗ )] ≤

2
6LDX
(k+2)2

2
9LDX
2(k+1)(k+2) ,

∀k ≥ 1.
(27)
As a consequence, the total number of calls to the SFO and
LOsepX oracles performed by the CALSGD method for
finding a stochastic -solution of (1), respectively, can be
bounded by

q
2
LDX

O



+

+

σ

2

2
DX
2

,

(28)

and
O

q

2
LDX


log

2
LDX
Λ

+

2
LDX



with probability 1 − Λ.
(29)

Proof. It can be easily seen from (26) that (35) holds. Also
note that by (26), we have
Γk =

6
k(k+1)(k+2) ,

(30)

βk γk
Γk

=

2Lk(k+1)
,
k+2

which implies that (36) holds. It can also be easily checked
from (30) and (26) that
ηi γi
i=1 Γi

PN

k=1 Bk

≤

PN

k=1

σ 2 (k+2)3
2
L2 DX

+N ≤

σ 2 (N +3)4
2
4L2 DX

+ N.

We now provide a good estimation for Φk0 (cf. Line 1 in
LCG procedure) at the k-th outer iteration. In view of the
definition of Φk0 and ψ(·) (cf. (16)), we have,
Φk0 = hψk0 (xk−1 ), xk−1 − xi = hgk , xk−1 − xi.
q 2
Nσ
Moreover, let Ak := kgk − f 0 (zk )k∗ ≥ ΛB
, by Chebyk
shev’s inequality and (15), we obtain,
Prob{Ak } ≤

E[kgk −f 0 (zk )|2∗ ]ΛBk
N σ2

≤

Λ
N,

∀Λ < 1, k ≥ 1,

TN
which implies that Prob{ k=1 Āk } ≤ 1 − Λ. Hence, by
Cauchy-Schwarz and triangle inequalities, we have with
probability 1 − Λ,
Φk0 = hgk − f 0 (zk ), xk−1 − xi + hf 0 (zk ), xk−1 − xi}
q

0
0 ∗
0 ∗
N σ2
≤
ΛBk + kf (zk ) − f (x )k∗ + kf (x )k∗ DX

q
2
0 ∗
N
(31)
≤
Λk3 + 1 LDX + kf (x )k∗ DX ,
where the last inequality follows from (6) and (26).
2
Note that we always have ηk < αβk DX
. Therefore, it
follows from the bound in (39), (26), and (31) that the total
number of inner iterations can be bounded by

PN
PN h 
Φk
Φk
0
+ log ηk0
2 + 1
k=1 Tk ≤
k=1 4α log αβk DX
i
8α2 β D 2
+ ηkk X + 2

q

N 
X
kf 0 (x∗ )k∗
N
5α log 2k 2
≤
+
1
+
3
Λk
LDX
k=1


+32α2 k + (4α + 2)N
= O(N log

N2
Λ

+ N 2 + N ),

which implies that our bound in (29).

and hence

Pk

bound in (28) then immediately follows from this observation and the fact that the number of calls to the SFO oracle
is bounded by

≤

2
kLDX
,
2

Pk

γi
i=1 Γi Bi (βi −Lγi )

≤

2
kLDX
2σ 2 .

Using the bound in (37), we obtain (27), which implies that
thetotal number
 of outer iterations N can be bounded by
p
2 / under the assumptions (9) and (10). The
O
LDX

We now provide a slightly improved complexity bound on
the number of calls to the SFO oracle which depends on
the distance from the initial point to the set of optimal solutions, rather than the diameter DX . In order to obtain
this improvement, we need to estimate D0 ≥ kx0 − x∗ k
and to fix the number of iterations N in advance. This
result will play an important role for the analysis of the
CALSGD method to solve strongly convex problems (see
Supplementary Material C.1).

Conditional Accelerated Lazy Stochastic Gradient Descent

(32)

100
10−1
10−2
10−3

0

Under assumptions (9) and (10),
E [f (yN ) − f (x )] ≤

∀N ≥ 1.

As a consequence, the total number of calls to the SFO and
LOsepX oracles performed by the CALSGD method for
finding a stochastic -solution of (1), respectively, can be
bounded by

q
O

LD02


+

σ 2 D02
2

150 300 450
Iterations

,

(33)

and (29).
Proof. The proof is similar to Corollary 2.4, and hence
details are skipped.
It should be pointed out that the complexity bound for the
number of calls to the LOsep oracle in (29) is established
with probability 1 − Λ. However, the probability parameter
Λ only appears in the non-dominant term.

3. Experimental Results
We present preliminary experimental results showing the
performance of CALSGD compared to OFW for stochastic
optimization. As examples we use the video co-localization
problem, which can be solved by quadratic programming
over a path polytope, different structured regression problems, and quadratic programming over the standard spectrahedron. In all cases we use objective functions of the
2
form kAx − bk , with A ∈ Rm×n , i.e., m examples over a
feasible region of dimension n. For comparability we use a
batch size of 128 for all algorithms to compute each gradient and the full matrix A for the actual objective function
values. All graphs show the function value using a logscale
on the vertical axis.
In Figure 1 we compare the performance of three algorithms:
CALSGD, SCGS and OFW. As described above SCGS is
the non-lazy counterpart of CALSGD. In the four graphs
of Figure 1 we report the objective function value over the
number of iterations, the wall clock time in seconds, the
number of calls to the linear oracle, and the number of
gradient evaluations in that order. In all these measures, our
proposed algorithms outperform OFW by multiple orders
of magnitude. As expected in number of iterations and
number of gradient evaluations both versions CALSGD and
SCGS perform equally well, however in wall clock time and

101
100
10

100
10−1
10−2
0

150
300
450
Wall clock time

102
CALSGD
SCGS
OFW

−1

10−2
10−3

CALSGD
SCGS
OFW

101

10−3

600

102

8LD02
N (N +1) ,

Function value

∗

0

0 100 200 300 400 500 600
LP calls

Function value

3L
k ,

10

102
CALSGD
SCGS
OFW

1

Function value

2LD 2

2
γk = k+1
, ηk = N k 0 ,
m
l 2
2
and Bk = σ NL2(k+1)
, k ≥ 1.
2
D

βk =

102
Function value

Corollary 2.5. Suppose that there exists an estimate D0
s.t. kx0 − x∗ k ≤ D0 ≤ DX . Also assume that the outer
iteration limit N ≥ 1 is given. If

101
100

CALSGD
SCGS
OFW

10−1
10−2
10−3

0 100 200 300 400 500 600
Gradient evaluations

Figure 1. Performance of CALSGD and its non-lazy variant SCGS
on a structured regression problem compared to OFW. The feasible region is the flow-based formulation of the convex hull of
Hamiltonian cycles on 9 nodes and has dimension n = 162.

in the number of calls to the linear oracle we observe the
advantage of the weaker LOsep oracle over LO.
In Figure 5 and 4 we show the performance of CALSGD
on one video co-localization instance and one semi-definite
convex programming instance. Due to space limitations
we only report the function value over the number of iterations and wall clock time in seconds; see Supplementary
Material D for a detailed analysis as well as more examples.
Implementation details. Finally, we provide details of
the implementation of LOsep. In the case of the structured
regression problems and the quadratic optimizations over
the path polytope instances, we used Gurobi as a solver
and included callbacks to terminate whenever the required
improvement (given by Φt−1 ) is reached; our approach is
one out of many and other approaches could have been used
equally well. If the solver does not find a good enough
solution, it returns with a lower bound on the Wolfe gap,
which we use to update Φt . In the case of convex programming over the feasible region Sn := {X ∈ Rn×n | X <
0 and tr(X) = 1}, we compute a maximal eigenvector of
the gradient (which is a matrix in this case) and use the rank1 factor of the maximal eigenvector, which is an optimal
point. In this case, there is no early termination.
However, in all cases, we use caching, i.e., we store all
previously seen points and check if any of them satisfies
the improvement guarantee. If that is the case we do not
call Gurobi or the maximal eigenvector routine. The size of
the cache is very small in all experiments; alternatively one
could use cache strategies such as e.g., k-paging.

Conditional Accelerated Lazy Stochastic Gradient Descent

104
103
10

2

0

10
20
30
Iterations

10

6

105
10

4

10

3

102

104

0

15
30
45
Wall clock time

10

−1

10−2
10−3
10

−4

103
0

8
16
24
Iterations
CALSGD
OFW

107
10

0

101
100
10−1
10−2
10−3
10−4

500 1000 1500
Iterations

0

25
50
75 100
Wall clock time

2

108
CALSGD
OFW

Function value

Function value

10

105

10

40

108
7

106

100

CALSGD
OFW

102

6

Figure 4. Performance of CALSGD and OFW on a medium sized
convex programming instance with feasible region Sn := {X ∈
Rn×n | X < 0, tr(X) = 1} with n = 100 and m = 10000.
Similar to the results before in both iterations and wall clock time
our method performs better.

105
104
103

108

102

7

0

15
30
45
Wall clock time

Figure 2. Two small video co-localization instances. On the
left: road_paths_01_DC_a instance (n = 29682 and m =
10000). On the right: road_paths_01_DC_b instance (n =
29682 and m = 10000). Observe a significant difference in function value of multiple orders of magnitude after a few seconds.

10

108
CALSGD
OFW

Function value

105

CALSGD
OFW

107

101

Function value

106

8

103
CALSGD
OFW

102

Function value

10
CALSGD
OFW

107

Function value

Function value

10

8

Function value

103

106
10

5

10

4

10

3

10

2

0

10
20
30
Iterations

106
105
104
103
102

40

CALSGD
OFW

107

0

15
30
45
Wall clock time

Figure 5. Performance of CALSGD compared to OFW on a small
video co-localization instance. The dimension of the underlying
path polytope is n = 29682, the time limit is 50 seconds. Our
algorithm performs significantly better both in number of iterations
as well as in wall clock time.
109

106
105
104
103

0

107

107
CALSGD
OFW

105
104
0

8
16
Iterations

24

106

0

109

107
106
105
104
3

0

50 100 150 200
Wall clock time

CALSGD
OFW

108
107
106
105
104
103

0

50 100 150 200
Wall clock time

Figure 3. Two medium sized video co-localization instances. On
the left: road_paths_02_DE_a instance (n = 119520 and
m = 10000). On the right: road_paths_02_DE_b instance
(n = 119520 and m = 10000). Similar results as in Figure 2.

25
50
75
Iterations

0

20
40
60
Iterations

107
CALSGD
OFW

106

CALSGD
OFW

106

100

107
Function value

CALSGD
OFW

108

Function value

Function value

106

103

10
20
30
Iterations

109

10

107

Function value

107

CALSGD
OFW

108

0

1000 2000 3000 4000
Wall clock time

CALSGD
OFW

Function value

108

Function value

CALSGD
OFW

Function value

Function value

109

106

0

1000 2000 3000 4000
Wall clock time

Figure 6. Structured regression problem over the convex hull of
all Hamiltonian cycles of a graph on 11 nodes (n = 242) on the
left and 12 nodes (n = 288) on the right. We used a density of
d = 0.6 for A and m = 10000. On both instances CALSGD
achieves lower values much faster, both in number of iterations as
well as in wall clock time.

Conditional Accelerated Lazy Stochastic Gradient Descent

Acknowledgements
We would to thank Elad Hazan for providing references.
Research reported in this paper was partially supported by
NSF CAREER award CMMI-1452463.

References
S. Ahipasaoglu and M. Todd. A Modified Frank-Wolfe Algorithm for Computing Minimum-Area Enclosing Ellipsoidal Cylinders: Theory and Algorithms. Computational
Geometry, 46:494–519, 2013.
F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equivalence between herding and conditional gradient algorithms. In the 29th International Conference on Machine
Learning, 2012.
A. Beck and M. Teboulle. A conditional gradient method
with linear rate of convergence for solving convex linear
systems. Math. Methods Oper. Res., 59:235–247, 2004.
G. Braun, S. Pokutta, and D. Zink. Lazifying conditional
gradient algorithms. arXiv preprint arXiv:1610.05120,
2016.
Y. Chen, G. Lan, and Y. Ouyang. Optimal primal-dual
methods for a class of saddle point problems. SIAM
Journal on Optimization, 24(4):1779–1814, 2014.
K. L. Clarkson. Coresets, sparse greedy approximation, and
the frank-wolfe algorithm. ACM Trans. Algorithms, 6(4):
63:1–63:30, Sept. 2010.
B. Cox, A. Juditsky, and A. S. Nemirovski. Dual subgradient
algorithms for large-scale nonsmooth learning problems.
Manuscript, School of ISyE, Georgia Tech, Atlanta, GA,
30332, USA, 2013. submitted to Mathematical Programming, Series B.
M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval Research Logistics Quarterly, 3:95–110,
1956.
R. M. Freund and P. Grigas. New Analysis and Results for
the Frank-Wolfe Method. ArXiv e-prints, July 2013.
D. Garber and E. Hazan. A Linearly Convergent Conditional Gradient Algorithm with Applications to Online
and Stochastic Optimization. ArXiv e-prints, Jan 2013.
S. Ghadimi and G. Lan. Optimal stochastic approximation
algorithms for strongly convex stochastic composite optimization, I: a generic algorithmic framework. SIAM
Journal on Optimization, 22:1469–1492, 2012.
S. Ghadimi and G. Lan. Optimal stochastic approximation
algorithms for strongly convex stochastic composite optimization, II: shrinking procedures and optimal algorithms.
SIAM Journal on Optimization, 23:2061–2089, 2013.

Gurobi Optimization. Gurobi optimizer reference manual version 6.5, 2016. URL https://www.gurobi.
com/documentation/6.5/refman/.
Z. Harchaoui, A. Juditsky, and A. S. Nemirovski. Conditional gradient algorithms for machine learning. NIPS
OPT workshop, 2012.
E. Hazan. Sparse approximate solutions to semidefinite
programs. In E. Laber, C. Bornstein, L. Nogueira, and
L. Faria, editors, LATIN 2008: Theoretical Informatics, volume 4957 of Lecture Notes in Computer Science,
pages 306–316. Springer Berlin Heidelberg, 2008. ISBN
978-3-540-78772-3.
E. Hazan and S. Kale. Projection-free online learning. arXiv
preprint arXiv:1206.4657, 2012.
E. Hazan and H. Luo. Variance-reduced and projectionfree stochastic optimization. In Proceedings of The 33rd
International Conference on Machine Learning, pages
1263–1271, 2016.
M. Jaggi. Sparse Convex Optimization Methods for
Machine Learning. PhD thesis, ETH Zürich, 2011.
http://dx.doi.org/10.3929/ethz-a-007050453.
M. Jaggi. Revisiting frank-wolfe: Projection-free sparse
convex optimization. In the 30th International Conference on Machine Learning, 2013.
M. Jaggi and M. Sulovský. A simple algorithm for nuclear
norm regularized problems. In the 27th International
Conference on Machine Learning, 2010.
R. Johnson and T. Zhang. Accelerating stochastic gradient
descent using predictive variance reduction. In Advances
in Neural Information Processing Systems, pages 315–
323, 2013.
A. Joulin, K. Tang, and L. Fei-Fei. Efficient image and
video co-localization with frank-wolfe algorithm. In European Conference on Computer Vision, pages 253–268.
Springer, 2014.
G. Lan. Convex optimization under inexact first-order information. Ph.D. dissertation, School of Industrial and
Systems Engineering, Georgia Institute of Technology,
Atlanta, GA 30332, USA, 2009.
G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1):365–397,
2012.
G. Lan. The complexity of large-scale convex programming
under a linear optimization oracle. Technical Report,
2013. Available on http://www.optimization-online.org/.

Conditional Accelerated Lazy Stochastic Gradient Descent

G. Lan and Y. Zhou. Conditional gradient sliding for convex
optimization. Optimization-Online preprint (4605), 2014.
R. Luss and M. Teboulle. Conditional gradient algorithms
for rank one matrix approximations with a sparsity constraint. SIAM Review, 55:65–98, 2013.
M. Mahdavi, L. Zhang, and R. Jin. Mixed optimization for
smooth functions. In Advances in Neural Information
Processing Systems, pages 674–682, 2013.
Y. E. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence O(1/k 2 ).
Doklady AN SSSR, 269:543–547, 1983.
Y. E. Nesterov. Introductory Lectures on Convex Optimization: a basic course. Kluwer Academic Publishers, Massachusetts, 2004.
Y. E. Nesterov. Smooth minimization of nonsmooth functions. Mathematical Programming, 103:127–152, 2005.
C. Shen, J. Kim, L. Wang, and A. van den Hengel. Positive semidefinite metric learning using boosting-like algorithms. Journal of Machine Learning Research, 13:
1007–1036, 2012.

Conditional Accelerated Lazy Stochastic Gradient Descent

A. Omitted proofs

which implies that

Theorem A.1. Let Γk be defined as follows,
(
1
k=1
Γk :=
Γk−1 (1 − γk ) k ≥ 2.

1
2 kxk

a) If
≥

βk−1 γk−1
Γk−1 ,

k ≥ 2,

(36)

≤ 12 kxk−1 − xk2 − 12 kxk − xk2

E [f (yk ) − f (x )] ≤

βk γk 2
2 DX
k h
X

+ Γk

ηi γi
Γi

+

1
βk hgk , x

+

− xk i +

ηk
βk .

Combing the above two relations, we have
f (yk ) ≤ (1 − γk )f (yk−1 ) + γk lf (zk , xk ) + γk hgk , x − xk i


+ βk2γk kxk−1 − xk2 − kxk − xk2

then under assumptions (9) and (10), we have
∗

− hxk−1 − xk , xk − xi

(34)

Suppose that {βk } and {γk } in the CALSGD algorithm
satisfy
γ1 = 1 and Lγk ≤ βk , k ≥ 1.
(35)
β k γk
Γk

− xk−1 k2 = 12 kxk−1 − xk2 − 12 kxk − xk2

+ ηk γk −

γk (βk −Lγk )
kxk
2

− xk−1 k2

+ ηk γk −

γk (βk −Lγk )
kxk
2

− xk−1 k2 .

= (1 − γk )f (yk−1 ) + γk lf (zk , x) + γk hδk , x − xk i


+ βk2γk kxk−1 − xk2 − kxk − xk2

γi σ 2
2Γi Bi (βi −Lγi )

i

,

Using the above inequality and the fact that

i=1

(37)
where x∗ is an arbitrary optimal solution of (8) and
DX is defined in (4).

hδk ,x − xk i −

= hδk , x − xk−1 i

≤

βk−1 γk−1
Γk−1 ,

k ≥ 2,

≤ hδk , x − xk−1 i +

(38)

(rather than (36)) is satisfied, then the result in part a)
2
holds by replacing βk γk DX
with β1 Γk kx0 − x∗ k2 in
the first term of the RHS of (37).
c) Under the assumptions in part a) or b), the number of
inner iterations performed at the k-th outer iterations
is bounded by
(
2
8α2 βk DX
2
;
κ+
+ 2,
ηk < αβk DX
ηk
Tk =
2
4α2 βk DX
2
κ + 4α +
,
+ 2,
ηk ≥ αβk DX
ηk
(39)
l
m
Φk
Φk
0
with κ := 4α log αβk D
+ log ηk0 .
2
X

L
2 kyk

(βk −Lγk )
kxk
2
kδk k2∗
2(βk −Lγk ) ,

− xk−1 k2

we obtain for all x ∈ X,
f (yk ) ≤ (1 − γk )f (yk−1 ) + γk f (x) + ηk γk


+ βk2γk kxk−1 − xk2 − kxk − xk2
+ γk hδk , x − xk−1 i +

γk kδk k2∗
2(βk −Lγk ) .

f (yk ) − f (x) ≤ Γk (1 − γ1 ) [f (y0 ) − f (x)] + Γk
+ Γk

k
X

βi γi
2Γi

i=1

+ Γk

− zk k2

k
X
i=1

γi
Γi

+

2

+

βk γk
2 kxk

− xk−1 k2 −

γk (βk −Lγk )
kxk
2

ηi γi
Γi

i=1

h
hδi , x − xi−1 i +

kδi k2∗
2(βi −Lγi )

i

.

(41)

kxk − xk−1 k2

= (1 − γk )f (yk−1 ) + γk lf (zk ; xk )

k
X



kxk−1 − xk2 − kxk − xk2

= (1 − γk )lf (zk ; yk−1 ) + γk lf (zk ; xk )
Lγk2

(40)

Subtracting f (x) from both sides of (40) and applying
Lemma 2.2, we have

Proof. Let us denote δk,j = F 0 (zk , ξk,j )−f 0 (zk ) and δk ≡
PBk
gk − f 0 (zk ) = j=1
δk,j /Bk . We first show part a). In
view of (6), (11) and (14), we have
f (yk ) ≤ lf (zk ; yk ) +

− xk−1 k2

+ hδk , xk−1 − xk i −

b) If
β k γk
Γk

(βk −Lγk )
kxk
2

− xk−1 k2 ,

where the last inequality follows from the convexity of f (·).
Also observe that by (17), we have
hgk + βk (xk − xk−1 ), xk − xi ≤ ηk , ∀x ∈ X,

Also observe that
Pk βi γi
2
2
i=1 Γi (kxi−1 − xk − kxi − xk )

− xk2 − βΓk γkk kxk − xk2


Pk
γi−1
+ i=2 βΓi γi i − βi−1
kxi−1 − xk2
Γi−1


Pk
γi−1
2
2
+ i=2 βΓi γi i − βi−1
DX
=
≤ βΓ1 γ1 1 DX
Γi−1
=

β1 γ1
Γ1 kx0

βk γk 2
Γk DX ,

Conditional Accelerated Lazy Stochastic Gradient Descent

where the inequality follows from the third assumption in
(36) and the definition of DX in (4).
Therefore, from the above two relations and the fact that
γ1 = 1, we can conclude that
h
Pk
kδi k2∗
2
f (yk ) − f (x) ≤ βk2γk DX
+ Γk i=1 Γγii ηi + 2(βi −Lγ
i)
i
PBi −1
(42)
+ j=1 Bi hδi,j , x − xi−1 i .
Note that by our assumptions on SFO, the random variables
δi,j are independent of the search point xi−1 and hence
E[hδi,j , x∗ − xi−1 i] = 0. In addition, relation (15) implies
that E[kδi k2∗ ] ≤ σ 2 /Bi . Using the previous two observations and taking expectation on both sides of (42) (with
x = x∗ ) we obtain (37).
Similarly, Part b) follows from (41), the assumption that
γ1 = 1, and the fact that
Pk βi γi
2
2
i=1 Γi (kxi−1 − xk − kxi − xk )
≤

β 1 γ1
Γ1 kx0

− xk2 −

β k γk
Γk kxk

− xk2 ≤ β1 kx0 − xk2 ,
(43)

due to the assumptions in (35) and (38).
Let Φk0 denote the initial bound obtained in Line 1 of the
LCG procedure at the k-th outer iteration. The result in
Part c) follows immediately from (19) and the fact that
2
Cψk = βk DX
.

B. Deterministic CALSGD
Our goal in this section is to present a deterministic version
of CALSGD, which we refer to as CALGD. Instead of
calling the SFO oracle to compute the stochastic gradients,
we assume that we have access to the exact gradients of
f . Therefore, the CALGD method calls the FO oracle to
obtain the exact gradients f 0 (zk ) at the k-th outer iteration.
The CALGD method is formally described as follows.
Algorithm 2 The conditional accelerated lazy gradient descent (CALGD) method
This algorithm is the same as Algorithm 1 except that
steps (12) and (13) are replaced by
xk = LCG(f 0 (zk ), βk , xk−1 , α, ηk ).

(44)

Similarly to the stochastic case, we can easily see that xk
obtained in (44) is an approximate solution for the gradient
sliding subproblem


βk
0
2
min ψk (x) := hf (zk ), xi +
kx − xk−1 k
(45)
x∈X
2

such that for all x ∈ X
hψk0 (xk ), xk −xi = hf 0 (zk )+βk (xk −xk−1 ), xk −xi ≤ ηk ,
(46)
for some ηk ≥ 0.
Theorem B.1 describes the main convergence properties of
the above CALGD method.
Theorem B.1. Let Γk be defined as in (34). Suppose that
{βk } and {γk } in the CALGD algorithm satisfy (35).
a) If (36) is satisfied, then for any x ∈ X and k ≥ 1,
Pk
2
f (yk ) − f (x∗ ) ≤ βk2γk DX
+ Γk i=1 ηΓi γi i . (47)
where x∗ is an arbitrary optimal solution of (1) and
DX is defined in (4).
b) If (38) (rather than (36)) is satisfied, then for any x ∈
X and k ≥ 1,
Pk
f (yk ) − f (x∗ ) ≤ β12Γk kx0 − x∗ k2 + Γk i=1 ηΓi γi i .
(48)
c) Under the assumptions in either part a) or b), the
number of inner iterations performed at the k-th outer
iteration can be bounded by (39).
Proof. Since the convergence results stated in Theorem 2.3 cover the deterministic case when we set δk,j =
F 0 (zk , ξk,j )−f 0 (zk ) ≡ 0, Part a) immediately follows from
(37) with P
σ = 0. Similarly, Part b) follows from (41), (43)
Bi
and δi = j=1
δi,j = 0. The proof of Part c) is exactly the
same as that of Theorem 2.3.c).
Clearly, there exist various options to specify the parameters
{βk }, {γk }, and {ηk } so as to guarantee the convergence
of the CALGD method. In the following corollaries, we
provide two different parameter settings for {βk }, {γk }, and
{ηk }, which lead to optimal complexity bounds on the total
number of calls to the FO and LOsep oracles for smooth
convex optimization.
Corollary B.2. If {βk }, {γk }, and {ηk } in the CALGD
method are set to
3L
k+1 ,

3
k+2 ,

2
LDX
k(k+1) ,

∀k ≥ 1,
(49)
and we assume that kf 0 (x∗ )k is bounded for any optimal
solution x∗ of (1), then for any k ≥ 1,
βk =

γk =

and ηk =

f (yk ) − f (x∗ ) ≤

2
15LDX
2(k+1)(k+2) .

(50)

As a consequence, the total number of calls to the FO and
LOsep oracles performed by the CALGD method
p for finding

2 / and
an -solution of (1) can be bounded by O
LDX

2
O LDX
/ respectively.

Conditional Accelerated Lazy Stochastic Gradient Descent

Proof. It can be easily seen from (49) that (35) holds, Γk is
given by (30), and
β k γk
Γk

=

k(k+1)(k+2)
9L
(k+1)(k+2)
6

=

3Lk
2 ,

which implies that (36) is satisfied. It then follows from
Theorem B.1.a), (49), and (30) that
f (yk ) − f (x∗ ) ≤
=

2
9LDX
2(k+1)(k+2)

+

6
k(k+1)(k+2)

Pk

η i γi
i=1 Γi

2
15LDX
2(k+1)(k+2) ,

which implies that the total number of outer iterations performed by the CALGD
pmethod for finding an -solution can
2 /(2).
be bounded by N = 15LDX
We first provide a valid upper bound for Φk0

defined in Line 1
when the CALGD method enters the LCG procedure at the
k-th outer iteration. In view of the definitions of Φk0 and ψ(·)
at Line 1 and (45), respectively, we have, for any k ≥ 1,

C. Generalizations to other optimization
problems
We generalize the CALGD and CALSGD methods to solve
two other classes of problems frequently seen in machine
learning. In particular, we discuss the CALGD method with
a restarting technique for solving smooth and strongly convex problems in Subsection C.1, and in Subsection C.3 we
extend the CALGD method to solve a special class of nonsmooth problems. Discussions for the similar extensions for
CALSGD method can be found in Subsection C.2 and C.4.
C.1. Strongly convex optimization
In this subsection, we assume that the objective function f
is not only smooth (i.e., (6) holds), but also strongly convex,
that is, ∃ µ > 0 s.t.

Φk0 = hψk0 (xk−1 ), xk−1 − xi = hf 0 (zk ), xk−1 − xi
≤ (kf 0 (zk ) − f 0 (x∗ )k + kf 0 (x∗ )k)kxk−1 − xk

2
≤ LDX
+ kf 0 (x∗ )kDX ,

(51)

where the first inequality follows from Cauchy-Schwarz and
the triangle inequality, and the second inequality follows
2
from (2) and (4). Note that we always have ηk < αβk DX
.
2
Therefore, similar to the stochastic case, our O(LDX /)
bound immediately follows from the above relation, (39),
and (49).
As before in the stochastic case, we can slightly improve
the complexity bound on the calls to the FO oracle in terms
of the dependence on DX .
Corollary B.3. Suppose that there exists an estimate D0 ≥
kx0 − x∗ k and that the outer iteration limit N ≥ 1 is given.
If
2LD02
2
(52)
βk = 2L
k , γk = k+1 , ηk = N k ,
for k ≥ 1, then

f (y) − f (x) − hf 0 (x), y − xi ≥

µ
2 ky

− xk2 , ∀x, y ∈ X.
(55)

For simplicity, we first establish the convergence results
for the deterministic case, i.e., we have access to the exact
gradients of the objective function f .

respectively.

The shrinking conditional gradient method in (Lan, 2013)
needs to make additional assumptions on the LO oracle to
obtain a linear rate of convergence. However, we will show
now that CALGD (relying on the vanilla weak separation
oracle) can obtain a linear rate of convergence in terms of
2
the number of calls to the FO oracle and O(LDX
/) rate
of convergence in the total number of calls to the LOsep
oracle. In view of the lower complexity bound established
for the LO oracle to solve strongly convex problems in
(Jaggi, 2013; Lan, 2013), our bound for the LOsep oracle
is not improvable.

Proof. The proof is similar to Corollary B.2, and hence
omitted.

We are now ready to formally describe the CALGD method
for solving strongly convex problems, which is obtained by
properly restarting the CALGD method (Algorithm 2).

f (yN ) − f (x∗ ) ≤

6LD02
N (N +1) .

(53)

As a consequence, the total number of calls to the FO and
LOsep oracles performed by the CALGD method for finding
an -solution of (1) can be bound by
 q 
 2 
LDX
O D0 L
and O
(54)


Conditional Accelerated Lazy Stochastic Gradient Descent

Algorithm 3 The CALGD method for strongly convex problems
Input: Initial point p0 ∈ X and an estimate δ0 > 0
satisfying f (p0 ) − f (x∗ ) ≤ δ0 .
for s = 1, 2, . . . do
Call the CALGD method in Algorithm 2 with input
l q m
x0 = ps−1 and N = 2 6L
,
(56)
µ

2L
k ,

γk =

 2 s 
µDX 2 N
O
s=1
k=1
δ0
 2 2P

µDX N
S
s
=O
s=1 2
δ0

 2 2
 2 2
µDX N
µDX N
S+1
=
O
=O
2
,
δ0


PS PN
s=1

k=1 Ts,k ≤

PS PN

which implies our second bound in (58) due to the definitions of N and S in (56) and (59), respectively.

and parameters
βk =

Therefore, the total number of calls to the LOsep oracle can
be bounded by

2
k+1 ,

and ηk = ηs,k :=

8Lδ0 2−s
µN k ,

(57)
and let ps be its output solution.
end for

In Algorithm 3, we restart the CALGD p
method for smooth
optimization (i.e., Algorithm 2) every d2 6L/µe iterations.
We call each loop iteration a phase of the above CALGD
algorithm. Observe that {ηk } decrease by a factor of 2 as
s increments by 1, while {βk } and {γk } remain the same.
The following theorem shows the convergence of the above
variant of the CALGD method.
Theorem C.1. Assume (55) holds and let {ps } be generated
by Algorithm 3. Then,
f (ps ) − f (x∗ ) ≤ δ0 2−s ,

s ≥ 0.

As a consequence, the total number of calls to the FO and
LOsep oracles performed by this algorithm for finding an
-solution of problem (1) can be bounded by
nq 
n 2 o
o
LDX
δ0
L
, (58)
O
and O
µ log2 max 1, 

respectively.
Proof. Denote the total number of phases performed by
CALGD method to obtain an -solution of (1) by S. In view
of the complexity results obtained in Theorem 2.5 in (Lan
and Zhou, 2014), we conclude that


S = log2 max 1, δ0 .
(59)
The total number of calls to the FO oracle performed by
Algorithm 3 is clearly bounded by N S, which immediately
implies our first result in (58).
Now, let Ts,k denote the number of calls to the LOsep
oracle required at the k-th outer iteration in the s-th phase.
It follows from Theorem B.1.c), (51), and (57) that


 2 s 
β D2
µDX 2 N
Ts,k ≤ O ηks,kX = O
.
δ0

In view of classic complexity theory for convex optimization, the bound on the total number of calls to the FO oracle
(cf. first bound in (58)) is optimal for strongly convex optimization. Moreover, in view of the complexity results
established in (Lan, 2013) and the fact that the LOsep oracle is weaker than the LO oracle, the bound on the total
number of calls to the LOsep oracle (cf. second bound in
(58)) is not improvable either.
C.2. Strongly convex stochastic optimization
Similarly to the deterministic case we present an optimal
algorithm for solving stochastic smooth and strongly convex
problems.
Algorithm 4 The CALSGD method for solving strongly
convex problems
Input: Initial point p0 ∈ X and an estimate δ0 > 0
satisfying f (p0 ) − f (x∗ ) ≤ δ0 .
for s = 1, 2, . . . do
Call the CALSGD method in Algorithm 1 with input
l q m
x0 = ps−1 and N = 4 2L
,
(60)
µ
and parameters
βk =

3L
k ,

γk =

−s

02
ηk = ηs,k := 8Lδ
µN k ,
l 2
m
2
:= µσ4LN2 δ(k+1)
,
(61)
−s
02

2
k+1 ,

and Bk = Bs,k

and let ps be its output solution.
end for

The main convergence properties of Algorithm 4 are as
follows.
Theorem C.2. Assume that (55) holds and let {ps } be generated by Algorithm 4. Then,
E[f (ps ) − f (x∗ )] ≤ δ0 2−s ,

s ≥ 0.

Conditional Accelerated Lazy Stochastic Gradient Descent

As a consequence, the total number of calls to the SFO and
LOsep oracles performed by this algorithm for finding a
stochastic -solution of problem (1)-(8) can be bounded by
n 2 q 
o
δ0
log
max
1,
,
(62)
O σµ + L
2
µ

and
O

n

2
LDX


o

, with probability 1 − Λ,

Proof. In view of Corollary 2.5, and Theorem 3.4 in (Lan
and Zhou, 2014), the total number of phases, S, performed
by CALSGD method to find a stochastic -solution of problem (1)-(8) is bounded by (59). Since the number of outer
iterations in each phase is at most N , the total number of
calls to the SFO oracle is bounded by

PS PN
PS PN  µσ2 N (k+1)2
s=1
k=1 Bk ≤
s=1
k=1
4L2 δ0 2−s + 1
2
N (N +1)3 PS
s
≤ µσ 12L
2δ
s=1 2 + SN
0
≤

For the sake of simplicity, we consider the deterministic
case, i.e., the problem of interest is an important class of
saddle point problems with f given in the form of
n
o
f (x) = max hAx, yi − fˆ(y) ,
(64)
y∈Y

(63)

respectively.

µσ 2 N (N +1)3
3L2 

C.3. Non-smooth optimization: Saddle point problems

+ SN.

Moreover, similar to (31), we obtain a good estimator for
Φs,k
0 , for any 0 < Λ ≤ 1
q

s,k
2
0 ∗
4SL2 δ0
Φ0 ≤
Λµk2 2s + 1 LDX + kf (x )k∗ DX ,
with probability 1 − Λ. Let Ts,k denote the number of calls
to the LOsep oracle required at the k-th outer iteration in the
s-th phase of the CALSGD method. It follows from Theorem 2.3.c), the above relation, and (61) that with probability
1 − Λ,


 2 s 
Φs,k
β D2
µDX 2 N
0
Ts,k ≤ O log ηs,k
+ ηks,kX = O
δ0
holds. Therefore, the total number of calls to the LOsep
oracle is bounded by
 2 s 
PS PN
PS PN
µDX 2 N
T
≤
O
s,k
s=1
k=1
s=1
k=1
δ0


P
S
−1
2
s
= O µDX
N 2 δ0
s=1 2
 2 2
µDX N
,
=O

which implies the bound in (63), due to the definitions of N
and S in (60) and (59), respectively.
According to Theorem C.2, the total number of calls to the
SFO oracle is bounded by O(1/), which is optimal in view
of the classic complexity theory for strongly convex optimization (see (Ghadimi and Lan, 2012; 2013)). Moreover,
the total number of calls to the LOsep oracle is bounded by
O(1/), which is the same bound as for the CALGD method
for strongly convex optimization and hence not improvable.

where A : Rn → Rm denotes a linear operator, Y ∈ Rm is
a convex compact set, and fˆ : Y → R is a simple convex
function. Since the objective function f is non-smooth, we
cannot directly apply the CALGD method presented in the
previous section. However, as shown by Nesterov (Nesterov,
2005), the function f (·) in (64) can be closely approximated
by a class of smooth convex functions. More specifically,
let ω : Y → R be a given strongly convex function with
strongly convex modulus σω > 0, i.e.,
ω(y) ≥ ω(x) + hω 0 (x), y − xi +

σω
2 ky

− xk2 , ∀x, y ∈ Y,

and let us denote cω := argminy∈Y ω(y), W (y) := ω(y) −
ω(cω ) − h∇ω(cω ), y − cω i and
2
DY,W
:= max W (y).
y∈Y

It can be easily seen that
ky − cω k2 ≤

2
σω W (y)

≤

2
2
σω DY,W ,

∀y ∈ Y,

and hence that
ky1 − y2 k2 ≤

2
4
σω DY,W ,

∀y1 , y2 ∈ Y.

In view of these relations, the function f (·) in (64) can be
closely approximated by
n
o
2
fτ (x) := max hAx, yi − fˆ(y) − τ [W (y) − DY,W
] .
y∈Y

(65)
In particular, for any τ ≥ 0,
2
f (x) ≤ fτ (x) ≤ f (x) + τ DY,W
, ∀x ∈ X.

Moreover, Nesterov (Nesterov, 2005) shows that fτ (·) is
differentiable and its gradients are Lipschitz continuous with
the Lipschitz constant given by
Lτ :=

kAk2
τ σω .

(66)

Throughout this subsection, we assume that the feasible
region Y and the function fˆ are simple enough, so that the
subproblem in (65) is easy to solve. Therefore, the major
computational cost for gradient calculations of fτ lie in the
evaluations of the linear operator A and its adjoint operator
AT . We are now ready to present a variant of the CALGD
method, which can achieve optimal bounds on the number

Conditional Accelerated Lazy Stochastic Gradient Descent

of calls to the LOsep oracle and the number of evaluations
of the linear operators A and AT .
Algorithm 5 The CALGD method for solving saddle point
problems
This algorithm is the same as Algorithm 2 except that
(44) is replaced by
xk = LCG(fτ0 k (zk ), βk , xk−1 , α, ηk ),

(67)

for some τk ≥ 0.

Theorem C.3. Suppose that τ1 ≥ τ2 ≥ . . . ≥ 0. Also
assume that {βk } and {γk } satisfy (35) (with L replaced by
Lτk defined in (66)) and (36). Then, for all k ≥ 1,
βk γk 2
2 DX

Pk

γi
i=1 Γi


2
ηi + τi DY,W
,
(68)
where x∗ is an arbitrary optimal solution of (1)-(64). Moreover, the number of inner iterations performed at the k-th
outer iteration is bounded by (39).
+ Γk

Proof. The proof is similar to Theorem 4.1 in (Lan and
Zhou, 2014), and hence omitted.
We now provide two different sets of parameter settings
for {βk }, {γk }, {ηk }, and {τk } which can guarantee the
optimal convergence of the above variant of the CALGD
method for saddle point optimization. Specifically, Corollary C.4 gives a static setting for parameter {τk } under the
assumption that the outer iteration limit N ≥ 1 is given,
while a dynamic setting is provided in Corollary C.5.
Corollary C.4. Assume the outer iteration limit N ≥ 1 is
given. If
2kAkDX
√
τk ≡ τ = DY,W
(69)
σω N , k ≥ 1,
and {βk }, {γk }, and {ηk } used in Algorithm 5 are set to
βk =

3Lτk
k+1

, γk =

k

3
k+2 , and

ηk =

2
Lτk DX
,
k2

k ≥ 1, (70)

then the number of linear operator evaluations (for A and
AT ) and the number of calls to the LOsep oracle performed
by Algorithm 5 for finding an -solution of problem (1)-(64),
respectively, is bounded by
n
o
n
o
2
2
kAk2 DX
DY,W
kAkDX DY,W
√
O
and
O
.
(71)
2
σω 
σω 
Proof. In view of the result in Corollary 4.2 of (Lan and
Zhou, 2014), our first bound in (71) immediately follows.

X

which implies our second bound in (71).
Corollary C.5. Suppose that parameter {τk } is now set to
τk =

In Theorem C.3 we state the main convergence properties
of this modified CALGD method to solve the saddle point
problem in (1)-(64).

f (yk ) − f (x∗ ) ≤

Moreover, it follows from (51), (39), (69), (70) and (66) that
the total number of calls to the LOsep oracle is bounded by


2
PN
PN
βk DX
k=1 Tk ≤
k=1 O
ηk


2
PN
L k DX
k2
= O(N 2 ),
= k=1 O τk+1
2
Lτ D

2kAkDX
√
DY,W σω k ,

k ≥ 1,

(72)

and the parameters {βk }, {γk }, and {ηk } used in Algorithm 5 are set as in (70). Then, the number of linear operator evaluations (for A and AT ) and the number of calls to
the LOsep oracle performed by Algorithm 5 for finding an
-solution of problem (1)-(64) is bounded by the two bounds
as given in (71) respectively.
Proof. The proof is similar to the Corollary C.4, and hence
omitted.
In view of the discussions in (Chen et al., 2014), the obtained
bound on the total number of operator evaluations (cf. first
bound in (71)) is not improvable for solving the saddle
point problems in (1)-(64). Moreover, according to (Lan,
2013) and the fact that the LOsep oracle is weaker than LO
oracle, the O(1/2 ) bound on the total number of calls to
the LOsep is not improvable.
C.4. Non-smooth stochastic optimization: stochastic
saddle point problems
In this subsection, we briefly discuss stochastic saddle point
problems, i.e., only stochastic gradients of fτ (cf. (65)) are
available. In particular, we consider the situation when the
original objective function f in (1) is given by


f (x) = E maxhAξ x, yi − fˆ(y, ξ) ,
(73)
y∈Y

where fˆ(·, ξ) is simple concave function for all ξ ∈ Ξ and
Aξ is a random linear operator such that


E kAξ k2 ≤ L2A
(74)
We can solve this stochastic saddle point problem by replacing (67) with
xk = LCG(gk , xk−1 , βk , ηk ),
PBk 0
where gk = B1k j=1
Fτk (zk , ξk,j ) for some τk ≥ 0 and
Bk ≥ 1. By properly specifying {βk }, {ηk }, {τk }, and
{Bk }, we can show that the number of linear operator
evaluations (for Aξ and ATξ ) and the number of calls to

Conditional Accelerated Lazy Stochastic Gradient Descent

the LOsep oracle performed by this variant of CALSGD
method for finding a stochastic -solution of problem (1)(73) is bounded by
n 2 2 2 o
LA DX DY,W
,
O
σω 2

of all Hamiltonian cycles of graphs of different size. In
Figure 8 the polytopes are the standard formulation of the
cut problem and the Birkhoff polytope.

and

In this section we consider instances of the problem of
finding the minimum of a convex function over the standard
spectrahedron, which is defined as

O

n

2
2
L2A DX
DY,W
σω 2

o

with probability 1 − Λ respectively. This result can be
proved by combining the techniques in Section 2 and those
in Theorem C.3. However, we skip the details of these
developments for the sake of simplicity.

D. Experimental results
We now provide additional experimental results in this section. The setup of the problems is as described in Section 3,
2
i.e., we use kAx−bk as the objective function where A is a
m × n matrix with n being the dimension of the underlying
feasible region and m being the number of examples. In
each example we use a density parameter d specifying the
fraction of non-zero entries in A. We compute b = Ax∗
with some feasible point x∗ so that in all examples the optimal value is 0. Albeit the theoretical number of samples
Bk (see Equation (26)) needed for CALSGD we use a batch
size of 128 for each gradient computation in all algorithms
for comparability. The function values that we report are
calculated using the full matrix A, however since they are
not used by either algorithm, each algorithm has only the
information provided by the 128 examples sampled in that
specific round.
We implemented all algorithms using Python 2.7 using
Gurobi 7.0 (Gurobi Optimization, 2016) as the solver
for our linear models.
D.1. Video co-localization
Video co-localization is the problem of identifying and object over multiple frames of a video. As shown by (Joulin
et al., 2014) this problem can be solved by quadratic programming over a path/flow polytope. In Figures 2, 3 and
7 we show that our algorithm CALSGD performs significantly better than OFW on this type of instances. We use
path polytopes available at http://lime.cs.elte.
hu/~kpeter/data/mcf/road/. The non-zero entries of A in this section are chosen uniformly from [0, 1]
and the density parameter we used is d = 0.8.
D.2. Structured regression
For our structured regression instances we solve the objec2
tive function kAx − bk as described before over different
polytopes. In Figure 6 the feasible region is the convex hull

D.3. Convex optimization over spectrahedra

Sn := {X ∈ Rn×n | X < 0, tr(X) = 1}.
In this case the linear minimization problem for an objective
function C is solved by computing an eigenvector for the
largest eigenvalue of −C. We use the same method to
implement LOsepSn .
We show results on three different sized instances, in Figure 9 for n = 50, Figure 10 for n = 100 and in Figure 11
for n = 150.

Conditional Accelerated Lazy Stochastic Gradient Descent

1010
CALSGD
OFW

109
108
107
106
105
104
103

0

8
16
Iterations

Function value

Function value

1010

107
106
105
104
0

8
16
Iterations

24

1010
CALSGD
OFW

109
108
107
106
105
104
0

150
300
450
Wall clock time

Function value

Function value

1010

103

108

103

24

CALSGD
OFW

109

CALSGD
OFW

109
108
107
106
105
104
103

0

150
300
450
Wall clock time

Figure 7. Two large video co-localization instances. On the left: road_paths_03_NH_a instance (n = 262958 and m = 10000).
On the right: road_paths_03_NH_b instance (n = 262958 and m = 10000). CALSGD has a better performance in both, iterations
and wall clock time.

Conditional Accelerated Lazy Stochastic Gradient Descent

102
CALSGD
OFW

104
103
102
101
100
10−1

0

800 1600 2400
Iterations

CALSGD
OFW

Function value

Function value

105

101
100
10−1

CALSGD
OFW

104
103
102
101
100
10−1

1500 3000
Iterations

4500

102

0

2000 4000 6000 8000
Wall clock time

Function value

Function value

105

0

CALSGD
OFW

101
100
10−1

0

150
300
450
Wall clock time

Figure 8. Structured regression problem over the cut polytope for a graph on 23 vertices on the left and the Birkboff polytope containing
all doubly stochastic matrices of size 100 × 100 on the right. In both cases we used m = 10000 rows for the matrix A, on the left a
density of d = 0.6 and on the right d = 0.8. In both cases the number of iterations computed in the given time between CALSGD and
OFW is quite significant, however in both cases CALSGD achieves better function values in the smaller number of iterations. In the
example of the Birkhoff polytope it almost looks like as if OFW converges suboptimally, however this is due to the large number of
iterations required: the convergence rate of OFW as shown by (Hazan and Kale, 2012) is O(T −1/4 ), so if we compute the improvement
with logarithmic scale, from, e.g., iteration 1500 to iteration 4500, we get −1/4(log(1500) − log(4500)) ≈ 0.12 (the constants hidden
in the O-notation get canceled due to the logarithm and the difference) and therefore indeed fits to the observation on the graph.

Conditional Accelerated Lazy Stochastic Gradient Descent

103
CALSGD
OFW

102
101
10
10

0

−1

10−2
10−3
10−4

0

Function value

Function value

103

CALSGD
OFW

102
101
100
−1

10−2
10−3
10−4

100
10−1
10−2
0

1000 2000 3000 4000
Iterations

103

0

25
50
75 100
Wall clock time

Function value

Function value

103

10

101

10−3

2000 4000 6000 8000
Iterations

CALSGD
OFW

102

CALSGD
OFW

102
101
100
10−1
10−2
10−3

0

25
50
75 100
Wall clock time

Figure 9. Quadratic optimization over the standard spectrahedron of size n = 50. On the left we use m = 10000 on the right m = 20000.
In both cases CALSGD performs better than OFW both in iterations as well as in wall clock time. As described in Figure 8 the impression
of suboptimal convergence of OFW can be explained by the very high number of iterations required.

Conditional Accelerated Lazy Stochastic Gradient Descent

103
CALSGD
OFW

102
101
100
10−1
10−2
10−3
10−4

0

Function value

Function value

103

100
10−1
10−2
10−3
0

250 500 750 1000
Iterations

103
CALSGD
OFW

102
101
100
10−1
10−2
10−3
0

25
50
75 100
Wall clock time

Function value

Function value

103

10−4

101

10−4

500 1000 1500
Iterations

CALSGD
OFW

102

CALSGD
OFW

102
101
100
10−1
10−2
10−3
10−4

0

25
50
75 100
Wall clock time

Figure 10. Medium sized quadratic optimization over the standard spectrahedron (n = 100). Again we chose m = 10000 on the left and
m = 20000 on the right. The CALSGD method achieves values multiple orders of magnitude better within the given time window.

Conditional Accelerated Lazy Stochastic Gradient Descent

103
CALSGD
OFW

102
101
100
10−1
10−2
10−3
10−4

0

200 400 600
Iterations

Function value

Function value

103

100
10−1
10−2
10−3
0

100 200 300
Iterations

400

103
CALSGD
OFW

102
101
100
10−1
10−2
10−3
0

25
50
75 100
Wall clock time

Function value

Function value

103

10−4

101

10−4

800

CALSGD
OFW

102

CALSGD
OFW

102
101
100
10−1
10−2
10−3
10−4

0

25
50
75 100
Wall clock time

Figure 11. Large quadratic optimization over the standard spectrahedron (n = 150), with m = 10000 on the left and m = 20000 on the
right. The behaviour and the achieved objective function values are very similar to the medium size instances in Figure 10.

