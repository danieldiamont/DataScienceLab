000
001
002
003
004
005
006
007
008
009
010
011
012
013
014
015
016
017
018
019
020
021
022
023
024
025
026
027
028
029
030
031
032
033
034
035
036
037
038
039
040
041
042
043
044
045
046
047
048
049
050
051
052
053
054

Leveraging Union of Subspace Structure to Improve Constrained Clustering:
Supplementary Material

Anonymous Authors1

In this document, we provide the proofs to Theorem 1 and
Corollary 1, which appear in Section 3.1 of the main document. We also explain the optional U O S-E XPLORE initialization phase of the SUPERPAC algorithm.

1. Proofs of Technical Results
Theorem 1. Consider two d-dimensional subspaces S1 and
S2 . Let y = x + n, where x ∈ S1 and n ∼ N (0, σ 2 ID ).
Define
dist(y, S1 )
µ(y) =
.
dist(y, S2 )
Then
p
(1 − ε) σ 2 (D − d)
p
≤ µ(y)
(1 + ε) σ 2 (D − d) + dist(x, S2 )2
and
p
(1 + ε) σ 2 (D − d)
p
,
µ(y) ≤
(1 − ε) σ 2 (D − d) + dist(x, S2 )2
with probability at least 1 − 4e−cε
absolute constant.

2

(D−d)

, where c is an

Proof. The proof relies on theorem 5.2.1 from (Vershynin,
2016), restated below.
Theorem 2. (Concentration on Gauss space) Consider a
random vector X ∼ N (0, σ 2 ID ) and a Lipschitz function
f : RD → R. Then for every t ≥ 0,
!

2

P {|f (X) − Ef (X)| ≥ t} ≤ 2 exp −

ct

2

σ 2 kf kLip

,

where kf kLip is the Lipschitz constant of f .
*
Equal contribution 1 Anonymous Institution, Anonymous City,
Anonymous Region, Anonymous Country. Correspondence to:
Anonymous Author <anon.email@domain.com>.

First consider the numerator and note that y − P1 y =
P1⊥ y ∼ N (0, σ 2 P1⊥ ) with

2
E P1⊥ y  = σ 2 (D − d).
Let f (z) = kP zk2 , where P is an arbitrary projection
matrix. In this case, kf kLip = 1, as f is a composition of
1-Lipschitz functions, which is also 1-Lipschitz. Further, by
Exercise 5.2.5 of (Vershynin, 2016), we can replace E kXk2

1/2
2
by E kXk2
in the concentration inequality. Applying
Thm. 2 to the above, we see that



n
o
p
ct2
 ⊥ 

2

P  P1 y − σ (D − d) ≥ t ≤ 2 exp − 2 .
σ
(1)
Similarly, for the denominator, note that y − P2 y = P2⊥ y ∼
N (P2⊥ x, σ 2 P2⊥ ) with

2
E P2⊥ y  = σ 2 (D − d) + γ 2 .
Since P2⊥ y is no longer centered, we let g(z) = z + P2⊥ x,
which also has kgkLip = 1. Applying Thm. 2 to the centered random vector ȳ ∼ N (0, σ 2 P2⊥ ) with Lipschitz function h = f ◦ g, we have that



n
o
 p
ct2


P P2⊥ y  − σ 2 (D − d) + γ 2  ≥ t ≤ 2 exp − 2 .
σ
(2)
p
2 (D − d) in (1) and t
Letting
t
=
ε
σ
=
p
ε σ 2 (D − d) + γ 2 in (2) yields
p
p


(1 − ε) σ 2 (D − d) ≤ P1⊥ y  ≤ (1 + ε) σ 2 (D − d)
and
p


(1 − ε) σ 2 (D − d) + γ 2 ≤ P2⊥ y 
p
≤ (1 + ε) σ 2 (D − d) + γ 2 ,

each with probability at least 1 − 2 exp −cε2 (D − d)
(since γ > 0). Applying the union bound gives the statement
of the theorem.
Corollary 1. Suppose x1 ∈ S1 is such that
d

Preliminary work. Under review by the International Conference
on Machine Learning (ICML). Do not distribute.

2

2

dist(x1 , S2 ) = sin (φ1 ) + δ

1X 2
sin (φi )
d i=1

!
(3)

055
056
057
058
059
060
061
062
063
064
065
066
067
068
069
070
071
072
073
074
075
076
077
078
079
080
081
082
083
084
085
086
087
088
089
090
091
092
093
094
095
096
097
098
099
100
101
102
103
104
105
106
107
108
109

Leveraging Union of Subspace Structure to Improve Constrained Clustering

110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164

for some small δ ≥ 0; that is, x1 is close to the intersection
of S1 and S2 . Let x2 be a random point in S1 generated as
x2 = U1 w where U1 is a basis for S1 and w ∼ N (0, d1 Id ).
We observe yi = xi + ni , where ni ∼ N (0, σ 2 ), i = 1, 2.
If there exists τ > 1 such that
δ<

5 1
−
7 τ

and
τ



d
1
1X 2
sin2 (φ1 ) + σ 2 (D − d) <
sin (φi ) ,
6
d i=1

(4)

that is, the average angle is sufficiently larger than the
smallest angle, then
2

P {µ(y1 ) > µ(y2 )} ≥ 1 − e−c( 100 )
7

ds

2

− 4e−c( 50 )
1

(D−d)

where µ(y) is defined as in Thm. 1, c is an absolute constant,
Pd
and s = d1 i=1 sin2 (φi ).
Proof. We have from Thm. 1 that
p
(1 + ε) σ 2 (D − d)
p
µ(y2 ) ≤
(1 − ε) σ 2 (D − d) + γ22
and

2

with probability at least 1 − 4e−cε (D−d) . Therefore if we
get the upper bound of µ(y2 ) to be smaller than the lower
bound of µ(y1 ), we are done. Rearranging this desired
inequality we see that we need
(5)

where β = (1 − ε)/(1 + ε). Let ε be such that β 4 = 5/6,
and let γ12 = sin2 (φ1 ) + δs as in the theorem. Then we
wish to select δ to satisfy
δ<

5 2
6 γ2

− sin2 (φ1 ) − 16 σ 2 (D − d)
.
s

(6)

Applying concentration with γ22 , we have that γ22 ≥ (1 −
2
ξ)2 s with probability at least 1 − e−cξ ds where c is an
absolute constant. Therefore taking ξ to be such that (1 −
ξ)2 = 6/7, we require
− sin2 (φ1 ) − 61 σ 2 (D − d)
5 1
= −
s
7 τ
where we used the definition of τ in the theorem. To quantify
the probability we need the appropriate values for ε and ξ;
we lower bound both with simple fractions: 1/50 < ε where
4
((1 − ε)/(1 + ε)) = β = 5/6 and 7/100 < ξ where (1 −
ξ)2 = 6/7. Applying the union bound with the chosen
concentration values implies that µ(y1 ) > µ(y2 ) holds with
2
7
1 2
probability at least 1 − e−c( 100 ) ds − 4e−c( 50 ) (D−d) .
δ<

5
7s

Sort {Z1 , · · · , Znc } in order of most likely mustlink (via subspace residual for xT ), query xT against
representatives from Zk until must-link constraint is
found or k = nc . If no must-link constraint found,
set Z ← {Z1 , · · · , Znc , {xT }} and increment nc .
end while

2. U O S-E XPLORE Algorithm

p
(1 − ε) σ 2 (D − d)
p
≤ µ(y1 )
(1 + ε) σ 2 (D − d) + γ12

γ12 < β 4 γ22 − (1 − β 4 )σ 2 (D − d).

Algorithm 1 U O S-E XPLORE
Input: X = {x1 , x2 , . . . , xN }: data, K: number of
subspaces, d: dimension of subspaces, A: affinity matrix,
maxQueries: maximum number of pairwise comparisons
Estimate Labels: Ĉ ← S PECTRAL C LUSTERING(A,K)
Calculate Margin: Calculate margin and set
x∨ ← arg maxx∈X µ̂(x) (most confident point)
Initialize Certain Sets: Z1 ← x∨ , Z ← {Z1 },
numQueries ← 0, nc ← 1
while nc < K and numQueries < maxQueries do
Obtain Test Point: Choose xT as point of maximum
margin such that Ĉ(xT ) 6= Ĉ(x ∈ Zk ) for any k. If
no such xT exists, choose xT at random.
Assign xT to Certain Set:

In this section, we describe the process of initializing the
certain sets. Note that this step is not necessary, as we
could initialize all certain sets to be empty, but we found
it led to improved performance experimentally. A main
distinction between subspace clustering and the general
clustering problem is that in the UoS model points can lie
arbitrarily far from each other but still be on or near the
same subspace. For this reason, the E XPLORE algorithm
from (Basu et al., 2004) is unlikely to quickly find points
from different clusters in an efficient manner. Here we
define an analogous algorithm for the UoS case, termed
U O S-E XPLORE, with pseudocode given in Algorithm 1.
The goal of U O S-E XPLORE is to find K certain sets, each
containing as few points as possible (ideally a single point),
allowing us to more rapidly assign test points to certain
sets in the SUPERPAC algorithm. We begin by selecting
our test point xT as the most certain point, or the point of
maximum margin and placing it in its own certain set. We
then iteratively select xT as the point of maximum margin
that (1) is not in any certain set and (2) has a different
cluster estimate from all points in the certain sets. If no
such point exists, we choose uniformly at random from all
points not in any certain set. This point is queried against
a single representative from each certain set according to
the UoS model as above until either a must-link is found
or all set representatives have been queried, in which case
xT is added to a new certain set. This process is repeated
until either K certain sets have been created or a terminal
number of queries have been used. As points of maximum
margin are more likely to be correctly clustered than other

165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219

Leveraging Union of Subspace Structure to Improve Constrained Clustering

220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274

points in the set, we expect that by choosing points whose
estimated labels indicate they do not belong to any current
certain set, we will quickly find a point with no must-link
constraints. In our simulations, we found that this algorithm
finds at least one point from each cluster in nearly the lower
limit of K(K − 1)/2 queries on the Yale dataset.

References
Basu, Sugato, Banerjee, Arindam, and Mooney, Raymond J.
Active semi-supervision for pairwise constrained clustering. In Proc. SIAM Int. Conf. on Data Mining, 2004.
Vershynin, Roman. A Course in High Dimensional Probability. 2016. URL www-personal.umich.edu/
˜romanv/teaching/2015-16/626/HDP-book.
pdf.

275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329

