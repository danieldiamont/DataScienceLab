Supplementary Material for
“Efficient Regret Minimization in Non-Convex Games”

Elad Hazan 1 Karan Singh 1 Cyril Zhang 1

A. Proof of Theorem 4.4
Since each ft is β-smooth, it follows that each Ft is βdt = xt −xt+1 . Note that since the itsmooth. Define ∇f
η
erates (xt : t ∈ [T ]) depend on the gradient estimates,
dt . By βthe iterates are stochastic variables, as are ∇f
smoothness of Ft , we have
Ft,w (xt+1 ) − Ft,w (xt )
β
≤ h∇Ft,w (xt ), xt+1 − xt i + kxt+1 − xt k2
2
D
E
dt + η 2 β k∇f
dt k2
= − η ∇Ft,w (xt ), ∇f
2
D
E
dt − ∇Ft,w (xt )
= − ηk∇Ft,w (xt )k2 − η ∇Ft,w (xt ), ∇f
β
2
2β
+η
2
2β
+η
 2


k∇Ft,w (xt )k2
 D
E
dt − ∇Ft,w (xt )
2 ∇Ft,w (xt ), ∇f


dt − ∇Ft,w (xt )k2
k∇f

β 2
= − η − η k∇Ft,w (xt )k2
2
D
E
dt − ∇Ft,w (xt )
− (η − βη 2 ) ∇Ft,w (xt ), ∇f
+ η2

β d
2
+ η 2 k∇f
t − ∇f (xt )k .
2
dt is an average of
Additionally, we each observe that ∇f
w independently sampled unbiased gradient estimates of
variance σ 2 each. It follows as a consequence that
 

dt xt = ∇Ft,w (xt )
E ∇f
2
 

dt − ∇Ft,w (xt )k2 xt ≤ σ
E k∇f
w
1
Computer Science, Princeton University.
Correspondence to:
Elad Hazan <ehazan@princeton.edu>,
Karan Singh <karans@princeton.edu>,
Cyril Zhang
<cyril.zhang@princeton.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Now, applying E [·|xt ] on both sides, it follows that


β 2
η − η · Ek∇Ft,w (xt )k2
2
β σ2
≤ E [Ft,w (xt ) − Ft,w (xt+1 )] + η 2
.
2 w
Also, we note that
Ft+1,w (xt+1 ) − Ft,w (xt+1 )
=

w−1
w−1
1 X
1 X
ft+1−i (xt+1 ) −
ft−i (xt+1 )
w i=0
w i=0

=

w−2
w−1
1 X
1 X
ft−i (xt+1 ) −
ft−i (xt+1 )
w i=−1
w i=0

=

ft+1 (xt+1 ) − ft−w+1 (xt+1 )
2M
≤
w
w

Adding the last two inequalities, we proceed to sum the
above inequality over all time steps:
" T
#
T βη 2 2
T
X
2M + 2M
2
w + 2w σ
E
k∇Ft,w (xt )k ≤
.
2
η − βη2
t=1
Setting η = 1/β yields the claim from the theorem.
Finally, note that for each round the number of stochastic
gradient oracle calls required is w. Therefore, across all T
rounds, the number of noisy oracle calls is T w.

B. Proof of Theorem 5.1 (ii)
Following the technique from Theorem 3.1, for 2 ≤ t ≤ T ,
let τt be the number of iterations of the inner loop during
the execution of Algorithm 3 during round t − 1 (in order
to generate the iterate xt ). Then, we have the following
lemma:
Lemma B.1. For any 2 ≤ t ≤ T ,
Ft−1 (xt ) − Ft−1 (xt−1 ) ≤ −τt ·

δ3
.
2βw3

Proof. This follows by summing the inequality Lemma 5.3
for across all pairs of consecutive iterates of the inner loop

Efficient Regret Minimization in Non-Convex Games

within the same epoch, and noting that each term Φ(z) is
3
at least wδ 3 before the inner loop has terminated.
Finally, we write (understanding F0 (x0 ) := 0):
FT (xT ) =

T
X

Ft (xt ) − Ft−1 (xt−1 )

t=1

=

T
X

Ft−1 (xt ) − Ft−1 (xt−1 ) + ft (xt ) − ft−w (xt )

t=1

≤

T
X

[Ft−1 (xt ) − Ft−1 (xt−1 )] +

t=2

2M T
.
w

Using Lemma B.1, we have
FT (xT ) ≤

T
X
δ3
2M T
−
·
τt ,
w
2βw3 t=1

whence
T
X

2βw3
·
τ=
τt ≤
δ3
t=1



2M T
− FT (xT )
w




2βM
· 2T w2 + w3
3
δ
6M
≤ 2 · T w2 ,
β
≤

as claimed (recalling that we chose δ = β for this analysis).

C. Proof of Theorem 6.2
Summing up the definitions of w-regret bounds achieved
by each A, and truncating the first w − 1 terms, we get
k X
T
X

k∇K,η Fti (xit )k2 ≤

i=1 t=w

k
X

Rw,Ai (T ).

i=1

Thus, for some t between w and T inclusive, it holds that

2
" Pw−1
#
k 
k

˜
X
X

j=0 fi,t−j
i 
(x
)
=
k∇K,η Fti (xit )k2
∇
 K,η
t 


w
i=1

i=1

≤

k
X
i=1

Rw,Ai (T )
.
T −w

Thus, for the same t we have

 v
" Pw−1
#
k

 u
˜i,t−j
uX
f
Rw,Ai (T )

j=0
i 
max ∇K,η
(xt ) ≤ t
,

w
T −w
i∈[k] 
i=1

as claimed.

