Supplemental Material: Scaling Up Sparse Support Vector Machines by
Simultaneous Feature and Sample Reduction

Weizhong Zhang * 1 2 Bin Hong * 1 3 Wei Liu 2 Jieping Ye 3 Deng Cai 1 Xiaofei He 1 Jie Wang 3
1

2

State Key Lab of CAD&CG, Zhejiang University, China
Tencent AI Lab, Shenzhen, China, 3 University of Michigan, USA

In this supplement, we first present the detailed proofs of all the theorems in the main text and then report the rest experiment results which are omitted in the experiment section due to the space limitation.

A. Proof for Theorem 1
Proof. of Theorem 1:
(i) : Let XÌ„ = (xÌ„1 , xÌ„2 , ..., xÌ„n ) and z = 1 âˆ’ XÌ„T w, the primal problem (Pâˆ— ) then is equivalent to
n

min

wâˆˆRp ,zâˆˆRn

1X
Î±
||w||2 + Î²||w||1 +
`([z]i ),
2
n i=1

s.t. z = 1 âˆ’ XÌ„T w.
The Lagrangian then becomes
n

1X
1
Î±
`([z]i ) + h1 âˆ’ XÌ„T w âˆ’ z, Î¸i
L(w, z, Î¸) = ||w||2 + Î²||w||1 +
2
n i=1
n

(17)

n

=

Î±
1
1
1X
1
`([z]i ) âˆ’ hz, Î¸i + h1, Î¸i
||w||2 + Î²||w||1 âˆ’ hXÌ„Î¸, wi +
n
n
n
|2
{z
} n i=1
|
{z
}
:=f1 (w)

(18)

:=f2 (z)

We first consider the subproblem minw L(w, z, Î¸):
1
XÌ„Î¸ + Î²âˆ‚||w||1 â‡”
n
1
1
1
XÌ„Î¸ âˆˆ Î±w + Î²âˆ‚||w||1 â‡’ w = SÎ² ( XÌ„Î¸)
n
Î±
n

0 âˆˆ âˆ‚w L(w, z, Î¸) = âˆ‚w f1 (w) = Î±w âˆ’

(19)

By substituting (19) into f1 (w), we get
f1 (w) =

Î±
Î±
1
1
||w||2 + Î²||w||1 âˆ’ hÎ±w + Î²âˆ‚||w||1 , wi = âˆ’ ||w||2 = âˆ’ ||SÎ² ( XÌ„Î¸)||2 .
2
2
2Î±
n

(20)

Then, we consider the problem minz L(w, z, Î¸):
0 =âˆ‡[z]i L(w, z, Î¸) = âˆ‡[z]i f2 (z) =

ï£±
ï£²
ï£³

â‡’[Î¸]i =

ï£±
ï£²
ï£³

Thus, we have

0,
1
Î³ [z]i ,

1,

if [z]i < 0,
if 0 â‰¤ [z]i â‰¤ Î³,
if [z]i > Î³.

âˆ’ n1 [Î¸]i ,
1
1
Î³n [z]i âˆ’ n [Î¸]i ,
1
1
n âˆ’ n [Î¸]i ,

if [z]i < 0,
if 0 â‰¤ [z]i â‰¤ Î³,
if [z]i > Î³.
(21)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction


f2 (z) =

Î³
||Î¸||2 ,
âˆ’ 2n
âˆ’âˆž,

if [Î¸]i âˆˆ [0, 1], âˆ€i âˆˆ [n],
otherwise .

(22)

Combining Eq. (17), Eq. (20) and Eq. (22), we obtain the dual problem:
1
1
Î³
1
min n
||SÎ² ( XÌ„Î¸)||2 +
||Î¸||2 âˆ’ h1, Î¸i
n
2n
n
Î¸âˆˆ[0,1] 2Î±

(23)

(ii) : From Eq. (19) and Eq. (21), we get the KKT conditions:
1
1
SÎ² ( XÌ„T Î¸âˆ— (Î±, Î²))
Î±ï£± n
0,
ï£²
1
(1
âˆ’
hxÌ„
,
wâˆ— (Î±, Î²)i),
[Î¸âˆ— (Î±, Î²)]i =
i
ï£³ Î³
1,
wâˆ— (Î±, Î²) =

if 1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²)i < 0,
if 0 â‰¤ 1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²)i â‰¤ Î³, i = 1, ..., n.
if 1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²)i > Î³.

The proof is complete.

B. Proof for Lemma 1
Proof. of Theorem 1:
1) It is the conclusion of the analysis above.
2) After feature screening, the primal problem (Pâˆ— ) is scaled into:
n

min c

wÌƒâˆˆR|FÌ‚

|

1X
Î±
||wÌƒ||2 + Î²||wÌƒ||1 +
`(1 âˆ’ h[xÌ„i ]FÌ‚ c , wÌƒi),
2
n i=1

(scaled-P âˆ— -1)

Thus, we can easily derive out the dual problem of (scaled-P âˆ— -1):
1
1
Î³
1
min n DÌƒ(Î¸Ìƒ; Î±, Î²) =
||SÎ² ( FÌ‚ c [XÌ„]Î¸Ìƒ)||2 +
||Î¸Ìƒ||2 âˆ’ h1, Î¸Ìƒi.
2Î±
n
2n
n
Î¸Ìƒâˆˆ[0,Î±]

(scaled-Dâˆ— -1)

and also the KKT conditions:
1
1
SÎ² ( FÌ‚ c [XÌ„]Î¸Ìƒâˆ— (Î±, Î²))
Î±ï£± n
0,
ï£²
1
âˆ—
(1
âˆ’
h[xÌ„
]
[Î¸Ìƒâˆ— (Î±, Î²)]i =
i
FÌ‚ c , wÌƒ (Î±, Î²)),
ï£³ Î³
1,
wÌƒâˆ— (Î±, Î²) =

(scaled-KKT-1)
if 1 âˆ’ h[xÌ„i ]FÌ‚ c , wÌƒâˆ— (Î±, Î²)i < 0,
if 0 â‰¤ 1 âˆ’ h[xÌ„i ]FÌ‚ c , wÌƒâˆ— (Î±, Î²) â‰¤ Î³,
if 1 âˆ’ h[xÌ„i ]FÌ‚ c , wÌƒâˆ— (Î±, Î²) > Î³,

(scaled-KKT-2)

Then, it is obvious that wÌƒâˆ— (Î±, Î²) = [wâˆ— (Î±, Î²)]FÌ‚ c , since essentially, problem (scaled-P âˆ— -1) can be derived by substituting
0 to the weights for the eliminated features in problem (Pâˆ— ) and optimize over the rest weights.
Since the solutions wâˆ— (Î±, Î²) and Î¸âˆ— (Î±, Î²) satisfy the conditions KKT-1 and KKT-2 and h[xÌ„i ]FÌ‚ c , wÌƒâˆ— (Î±, Î²)i =
hxÌ„i , wâˆ— (Î±, Î²)i for all i , we know wÌƒâˆ— (Î±, Î²) and Î¸âˆ— (Î±, Î²) satisfy the conditions scaled-KKT-1 and scaled-KKT-2. So
they are the solutions of problems (scaled-P âˆ— -1) and (scaled-Dâˆ— -1). Thus, due to the uniqueness of the solution of problem (scaled-Dâˆ— -1), we have
Î¸âˆ— (Î±, Î²) = Î¸Ìƒâˆ— (Î±, Î²)

(24)

From 1) we have, [Î¸Ìƒâˆ— (Î±, Î²)]RÌ‚c = 0 and [Î¸Ìƒâˆ— (Î±, Î²)]LÌ‚c = 1. Therefore, from the dual problem (scaled-Dâˆ— ), we can see that
[Î¸Ìƒâˆ— (C, Î±)]DÌ‚c can be recovered from the following problem:
1
1
Î³
1
1
min c
||SÎ² ( GÌ‚1 Î¸Ì‚ + GÌ‚2 1)||2 +
||Î¸Ì‚||2 âˆ’ h1, Î¸Ì‚i,
|DÌ‚ | 2Î±
n
n
2n
n
Î¸Ì‚âˆˆ[0,1]
Since [Î¸Ìƒâˆ— (Î±, Î²)]DÌ‚c = [Î¸âˆ— (Î±, Î²)]DÌ‚c , the proof is therefore completed.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

C. Proof for Lemma 2
Proof. Due to the Î±-strong convexity of the objective P (w; Î±, Î²), we have
P (wâˆ— (Î±0 , Î²0 ); Î±, Î²0 ) â‰¥ P (wâˆ— (Î±, Î²0 ); Î±, Î²0 ) +

P (wâˆ— (Î±, Î²0 ); Î±0 , Î²0 ) â‰¥ P (wâˆ— (Î±0 , Î²0 ); Î±0 , Î²0 ) +

Î±
||wâˆ— (Î±0 , Î²0 ) âˆ’ wâˆ— (Î±, Î²0 )||2
2
Î±0
||wâˆ— (Î±0 , Î²0 ) âˆ’ wâˆ— (Î±, Î²0 )||2
2

which are equivalent to
n

Î±
1X
||wâˆ— (Î±0 , Î²0 )||2 + Î²0 ||wâˆ— (Î±0 , Î²0 )||1 +
`(1 âˆ’ hxÌ„i , wâˆ— (Î±0 , Î²0 )i)
2
n i=1
n
Î±
1X
â‰¥ ||wâˆ— (Î±, Î²0 )||2 + Î²0 ||wâˆ— (Î±, Î²0 )||1 +
`(1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²0 )i)
2
n i=1
Î±
+ ||wâˆ— (Î±0 , Î²0 ) âˆ’ wâˆ— (Î±, Î²0 )||2
2
n
Î±0
1X
||wâˆ— (Î±, Î²0 )||2 + Î²0 ||wâˆ— (Î±, Î²0 )||1 +
`(1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²0 )i)
2
n i=1
n

Î±0
1X
â‰¥ ||wâˆ— (Î±0 , Î²0 )||2 + Î²0 ||wâˆ— (Î±0 , Î²0 )||1 +
`(1 âˆ’ hxÌ„i , wâˆ— (Î±0 , Î²0 )i)
2
n i=1
Î±0
+
||wâˆ— (Î±0 , Î²0 ) âˆ’ wâˆ— (Î±, Î²0 )||2
2
Adding the above two inequalities together, we get
Î± âˆ’ Î±0
Î± âˆ’ Î±0
Î±0 + Î±
||wâˆ— (Î±0 , Î²0 )||2 â‰¥
||wâˆ— (Î±, Î²0 )||2 +
||wâˆ— (Î±0 , Î²0 ) âˆ’ wâˆ— (Î±, Î²0 )||2
2
2
2
(Î± âˆ’ Î±0 )2
Î±0 + Î± âˆ—
w (Î±0 , Î²0 )||2 â‰¤
||wâˆ— (Î±0 , Î²0 )||2
â‡’||wâˆ— (Î±, Î²0 ) âˆ’
2Î±
4Î±2

(25)

Substitute the prior that [wâˆ— (Î±, Î²0 )]FÌ‚ = 0 into (25), we get
Î±0 + Î± âˆ—
[w (Î±0 , Î²0 )]FÌ‚ c ||2
2Î±
(Î± âˆ’ Î±0 )2
(Î±0 + Î±)2
âˆ—
2
||w
(Î±
,
Î²
)||
âˆ’
||[wâˆ— (Î±0 , Î²0 )]FÌ‚ ||2 .
â‰¤
0
0
4Î±2
4Î±2
||[wâˆ— (Î±, Î²0 )]FÌ‚ c âˆ’

The proof is complete.

D. Proof for Lemma 3
Proof. Firstly, we need to extend the definition of D(Î¸; Î±, Î²) to Rn :

D(Î¸; Î±, Î²), if Î¸ âˆˆ [0, 1]n ,
DÌƒ(Î¸; Î±, Î²) =
+âˆž,
otherwise
Due to the strong convexity of objective DÌƒ(Î¸; Î±, Î²), we have
Î³ âˆ—
||Î¸ (Î±0 , Î²0 ) âˆ’ Î¸âˆ— (Î±, Î²0 )||2 ,
2n
Î³ âˆ—
DÌƒ(Î¸âˆ— (Î±, Î²0 ), Î±0 , Î²0 ) â‰¥ DÌƒ(Î¸âˆ— (Î±0 , Î²0 ), Î±0 , Î²0 ) +
||Î¸ (Î±0 , Î²0 ) âˆ’ Î¸âˆ— (Î±, Î²0 )||2 .
2n

DÌƒ(Î¸âˆ— (Î±0 , Î²0 ), Î±, Î²0 ) â‰¥ DÌƒ(Î¸âˆ— (Î±, Î²0 ), Î±, Î²0 ) +

Since Î¸âˆ— (Î±0 , Î²0 ), Î¸âˆ— (Î±, Î²0 ) âˆˆ [0, 1]n , the above inequalities are equivalent to

(26)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

1
1
Î³ âˆ—
1
||SÎ²0 ( XÌ„T Î¸âˆ— (Î±0 , Î²0 ))||2 +
||Î¸ (Î±0 , Î²0 )||2 âˆ’ h1, Î¸âˆ— (Î±0 , Î²0 )i
2Î±
n
2n
n
1
1
Î³ âˆ—
1
â‰¥ ||SÎ²0 ( XÌ„T Î¸âˆ— (Î±, Î²0 ))||2 +
||Î¸ (Î±, Î²0 )||2 âˆ’ h1, Î¸âˆ— (Î±, Î²0 )i
2Î±
n
2n
n
Î³ âˆ—
+
||Î¸ (Î±0 , Î²0 ) âˆ’ Î¸âˆ— (Î±, Î²0 )||2 ,
2n
1
1
Î³ âˆ—
1
||SÎ²0 ( XÌ„T Î¸âˆ— (Î±, Î²0 ))||2 +
||Î¸ (Î±, Î²0 )||2 âˆ’ h1, Î¸âˆ— (Î±, Î²0 )i
2Î±0
n
2n
n
1
Î³ âˆ—
1
Î³ âˆ—
1
||SÎ²0 ( XÌ„T Î¸âˆ— (Î±0 , Î²0 ))||2 +
||Î¸ (Î±0 , Î²0 )||2 âˆ’ h1, Î¸âˆ— (Î±0 , Î²0 )i +
||Î¸ (Î±0 , Î²0 ) âˆ’ Î¸âˆ— (Î±, Î²0 )||2 .
â‰¥
2Î±0
n
2n
n
2n
Adding the above two inequalities, we get
Î³(Î± âˆ’ Î±0 ) âˆ—
Î± âˆ’ Î±0
||Î¸ (Î±0 , Î²0 )||2 âˆ’
h1, Î¸âˆ— (Î±0 , Î²0 )
2n
n
Î³(Î± âˆ’ Î±0 ) âˆ—
Î± âˆ’ Î±0
Î³(Î±0 + Î±) âˆ—
||Î¸ (Î±, Î²0 )||2 âˆ’
h1, Î¸âˆ— (Î±, Î²0 )i +
||Î¸ (Î±0 , Î²0 ) âˆ’ Î¸âˆ— (Î±, Î²0 )||2
â‰¥
2n
n
2n
That is equivalent to
Î±0 + Î± âˆ—
Î± âˆ’ Î±0
1+
Î¸ (Î±0 , Î²0 ), Î¸âˆ— (Î±, Î²0 )i
Î³Î±
Î±
Î±0 âˆ—
Î± âˆ’ Î±0
â‰¤âˆ’
||Î¸ (Î±0 , Î²0 )||2 âˆ’
h1, Î¸âˆ— (Î±0 , Î²0 )i
Î±
Î³Î±
||Î¸âˆ— (Î±, Î²0 )||2 âˆ’ h

(27)

That is
||Î¸âˆ— (Î±, Î²0 ) âˆ’ (

Î± âˆ’ Î±0
Î±0 + Î± âˆ—
Î± âˆ’ Î±0 2 âˆ—
1
1+
Î¸ (Î±0 , Î²0 ))||2 â‰¤ (
) ||Î¸ (Î±0 , Î²0 ) âˆ’ 1||2
2Î³Î±
2Î±
2Î±
Î³

(28)

Substitute the priors that [Î¸âˆ— (Î±, Î²0 )]RÌ‚ = 0 and [Î¸âˆ— (Î±, Î²0 )]LÌ‚ = 1 into (28), we have
Î± âˆ’ Î±0
Î±0 + Î± âˆ—
1+
[Î¸ (Î±0 , Î²0 )]DÌ‚c )||2
2Î³Î±
2Î±
Î± âˆ’ Î±0 2 âˆ—
1
(2Î³ âˆ’ 1)Î± + Î±0
Î±0 + Î± âˆ—
) ||Î¸ (Î±0 , Î²0 ) âˆ’ 1||2 âˆ’ ||
1âˆ’
[Î¸ (Î±0 , Î²0 )]LÌ‚ ||2
â‰¤(
2Î±
Î³
2Î³Î±
2Î±
Î± âˆ’ Î±0
Î±0 + Î± âˆ—
âˆ’ ||
1+
[Î¸ (Î±0 , Î²0 )]RÌ‚ ||2 .
2Î³Î±
2Î±
||[Î¸âˆ— (Î±, Î²0 )]DÌ‚c âˆ’ (

The proof is complete.

E. Proof for Lemma 4
Before the proof of Lemma 4, we should prove that the optimization problem in (1) is equivalent to


1
si (Î±, Î²0 ) = max
|h[xÌ„i ]DÌ‚c , Î¸i + h[xÌ„i ]LÌ‚ , 1i| , i âˆˆ FÌ‚ c .
Î¸âˆˆÎ˜
n
To avoid notational confusion, we denote the feasible region Î˜ in (1) as Î˜Ìƒ. Then,




1

1  i 


max  [XÌ„Î¸]i  = max
xÌ„ Î¸
Î¸âˆˆÎ˜
n
n
Î¸âˆˆÎ˜Ìƒ



1  i
i
i

[xÌ„ ]DÌ‚c [Î¸]DÌ‚c + [xÌ„ ]LÌ‚ [Î¸]LÌ‚ + [xÌ„ ]RÌ‚ [Î¸]RÌ‚
= max
n
Î¸âˆˆÎ˜Ìƒ


1
i
i
= max
|h[xÌ„ ]DÌ‚c , [Î¸]DÌ‚c i + h[xÌ„ ]LÌ‚ , 1i| = si (Î±, Î²0 ).
Î¸âˆˆÎ˜
n

(29)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

The last equation holds since [Î¸]LÌ‚ = 1, [Î¸]RÌ‚ = 0 and [Î¸DÌ‚c ] âˆˆ Î˜.
Proof. of Lemma 4:
1
si (Î±, Î²0 ) = max { |h[xÌ„i ]DÌ‚c , Î¸i + h[xÌ„i ]LÌ‚ , 1i|}.
Î¸âˆˆB(c,r) n
1
= max { |h[xÌ„i ]DÌ‚c , ci + h[xÌ„i ]LÌ‚ , 1i + h[xÌ„i ]DÌ‚c , Î·i|}
n
Î·âˆˆB(0,r)

1
= |h[xÌ„i ]DÌ‚c , ci + h[xÌ„i ]LÌ‚ , 1| + k[xÌ„i ]DÌ‚c kr
n
The last equality holds since âˆ’k[xÌ„i ]DÌ‚c kr â‰¤ h[xÌ„i ]DÌ‚c , Î·i| â‰¤ k[xÌ„i ]DÌ‚c kr. The proof is complete.

F. Proof for Theorem 4
Proof. (1) It can be obtained from the the rule (R1).
(2) It is from the definition of FÌ‚.

G. Proof for Lemma 5
Firstly, we need to point out that the optimization problems in (2) and (3) are equivalent to the problems:
ui (Î±, Î²0 ) = max {1 âˆ’ h[xÌ„i ]FÌ‚ c , wi}, i âˆˆ DÌ‚c ,

(30)

li (Î±, Î²0 ) = min {1 âˆ’ h[xÌ„i ]FÌ‚ c , wi}, i âˆˆ DÌ‚c

(31)

wâˆˆW

wâˆˆW

They follow from the fact that [w]FÌ‚ c âˆˆ W and
{1 âˆ’ hw, xÌ„i i}
={1 âˆ’ h[w]FÌ‚ c , [xÌ„i ]FÌ‚ c i âˆ’ h[w]FÌ‚ , [xÌ„i ]FÌ‚ i}
={1 âˆ’ h[w]FÌ‚ c , [xÌ„i ]FÌ‚ c i} (since [w]FÌ‚ = 0).
Proof. of Lemma 5:
ui (Î±, Î²0 ) = max {1 âˆ’ h[xÌ„i ]FÌ‚ c , wi}
wâˆˆB(c,r)

= max {1 âˆ’ h[xÌ„i ]FÌ‚ c , ci âˆ’ h[xÌ„i ]FÌ‚ c , Î·i}
Î·âˆˆB(0,r)

=1 âˆ’ h[xÌ„i ]FÌ‚ c , ci + max {âˆ’h[xÌ„i ]FÌ‚ c , Î·i}
Î·âˆˆB(0,r)

=1 âˆ’ h[xÌ„i ]FÌ‚ c , ci + k[xÌ„i ]FÌ‚ c kr

li (Î±, Î²0 ) =

min {1 âˆ’ h[xÌ„i ]FÌ‚ c , wi}

wâˆˆB(c,r)

= min {1 âˆ’ h[xÌ„i ]FÌ‚ c , ci âˆ’ h[xÌ„i ]FÌ‚ c , Î·i}
Î·âˆˆB(0,r)

=1 âˆ’ h[xÌ„i ]FÌ‚ c , ci +

min {âˆ’h[xÌ„i ]FÌ‚ c , Î·i}

Î·âˆˆB(0,r)

=1 âˆ’ h[xÌ„i ]FÌ‚ c , ci âˆ’ k[xÌ„i ]FÌ‚ c kr
The proof is complete.

H. Proof for Theorem 5
Proof. (1) It can be obtained from the the rule (R2).

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(2) It is from the definitions of RÌ‚ and LÌ‚.

I. Proof for Theorem 2
Proof. of Theorem 2:
We prove this theorem by verifying that the solutions wâˆ— (Î±, Î²) = 0 and Î¸âˆ— (Î±, Î²) = 1 satisfy the conditions KKT-1 and
KKT-2.
Firstly, since Î² â‰¥ Î²max = || n1 XÌ„1||âˆž , we have SÎ² ( n1 XÌ„1) = 0. Thus wâˆ— (Î±, Î²) = 0 and Î¸âˆ— (Î±, Î²) = 1 satisfy the condition
KKT-1.
Then, for all i âˆˆ [n], we have
1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²)i = 1 âˆ’ 0 > Î³.
Thus wâˆ— (Î±, Î²) = 0 and Î¸âˆ— (Î±, Î²) = 1 satisfy the condition KKT-2. Hence, they are the solutions for the primal problem
(Pâˆ— ) and the dual problem (Dâˆ— ), respectively.

J. Proof for Theorem 3
Proof. of Theorem 3:
Similar with the proof of Theorem 2, we prove this theorem by verifying that the solutions wâˆ— (Î±, Î²) =
and Î¸âˆ— (Î±, Î²) = 1 satisfy the conditions KKT-1 and KKT-2.

1
1
âˆ—
Î± SÎ² ( n XÌ„Î¸ (Î±, Î²))

1. Case 1: Î±max (Î²) â‰¤ 0. Then for all Î± > 0, we have
min {1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²)i}

iâˆˆ[n]

1
1
1
1
= min {1 âˆ’ hxÌ„i , SÎ² ( XÌ„Î¸âˆ— (Î±, Î²))i} = min {1 âˆ’ hxÌ„i , SÎ² ( XÌ„1)i}
Î±
n
Î±
n
iâˆˆ[n]
iâˆˆ[n]
1
1
1
=1 âˆ’ max hxÌ„i , SÎ² ( XÌ„1)i = 1 âˆ’ (1 âˆ’ Î³) Î±max (Î²)
Î± iâˆˆ[n]
n
Î±
â‰¥1 > Î³
Then, L = [n] and wâˆ— (Î±, Î²) = Î±1 SÎ² ( n1 XÌ„Î¸âˆ— (Î±, Î²)) and Î¸âˆ— (Î±, Î²) = 1 satisfy the conditions KKT-1 and KKT-2.
Hence, they are the optimal solution for the primal and dual problems (Pâˆ— ) and (Dâˆ— ).
2. Case 2: Î±max (Î²) > 0. Then for any Î± â‰¥ Î±max (Î²), we have
min {1 âˆ’ hxÌ„i , wâˆ— (Î±, Î²)i}

iâˆˆ[n]

1
1
1
1
= min {1 âˆ’ hxÌ„i , SÎ² ( XÌ„Î¸âˆ— (Î±, Î²))i} = min {1 âˆ’ hxÌ„i , SÎ² ( XÌ„1)i}
Î±
n
Î±
n
iâˆˆ[n]
iâˆˆ[n]
1
1
1
=1 âˆ’ max hxÌ„i , SÎ² ( XÌ„1)i = 1 âˆ’ (1 âˆ’ Î³) Î±max (Î²) â‰¥ 1 âˆ’ (1 âˆ’ Î³) = Î³.
Î± iâˆˆ[n]
n
Î±
Thus, E âˆª L = [n] and wâˆ— (Î±, Î²) = Î±1 SÎ² ( n1 XÌ„Î¸âˆ— (Î±, Î²)) and Î¸âˆ— (Î±, Î²) = 1 satisfy the conditions KKT-1 and KKT-2.
Hence, they are the optimal solution for the primal and dual problems (Pâˆ— ) and (Dâˆ— ).
The proof is complete.

K. Proof for Theorem 6
Proof. of Theorem 6:
(1) Given the reference solutions pair wâˆ— (Î±iâˆ’1,j , Î²j ) and Î¸âˆ— (Î±iâˆ’1,j , Î²j ), if we do ISS first in SIFS and apply ISS and IFS
for infinite times. If after p times of triggering, no new inactive features or samples are identified, then we can denote the
sequence of FÌ‚, RÌ‚ and LÌ‚ as:
ISS

IF S

ISS

A
A
A
A
A
A
A
A
A
A
FÌ‚0A = RÌ‚A
0 = LÌ‚0 = âˆ… âˆ’â†’ FÌ‚1 , RÌ‚1 , LÌ‚1 âˆ’â†’ FÌ‚2 , RÌ‚2 , LÌ‚2 âˆ’â†’ ...FÌ‚p , RÌ‚p , LÌ‚p

IF S/ISS

âˆ’â†’

...

(32)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

A
A
A
A
A
A
A
with FÌ‚pA = FÌ‚p+1
= FÌ‚p+2
= ..., RÌ‚A
p = RÌ‚p+1 = RÌ‚p+2 = ..., and LÌ‚p = LÌ‚p+1 = LÌ‚p+2 = ...

(33)

In the same way, if we do IFS first in SIFS and no new inactive feature or samples are identified after q times of triggering
of ISS and IFS, then the sequence can be denoted as:
IF S

ISS

IF S

B
B
B
B
B
B
B
B
B
B
FÌ‚0B = RÌ‚B
0 = LÌ‚0 = âˆ… âˆ’â†’ FÌ‚1 , RÌ‚1 , LÌ‚1 âˆ’â†’ FÌ‚2 , RÌ‚2 , LÌ‚2 âˆ’â†’ ...FÌ‚q , RÌ‚q , LÌ‚q

IF S/ISS

âˆ’â†’

...

(34)

B
B
B
B
B
B
B
with FÌ‚qB = FÌ‚q+1
= FÌ‚q+2
= ..., RÌ‚B
q = RÌ‚q+1 = RÌ‚q+2 = ..., and LÌ‚q = LÌ‚q+1 = LÌ‚q+2 = ...

(35)

A
A
B
A
We first prove that FÌ‚kB âŠ† FÌ‚k+1
, RÌ‚B
k âŠ† RÌ‚k+1 and LÌ‚k âŠ† LÌ‚k+1 hold for all k â‰¥ 0 by induction.
A
B
A
B
B
B
1) When k = 0, the equalities FÌ‚0B âŠ† FÌ‚1A , RÌ‚B
0 âŠ† RÌ‚1 and LÌ‚0 âŠ† LÌ‚1 hold since FÌ‚0 = RÌ‚0 = LÌ‚0 = âˆ….
B
A
B
A
B
A
B
2) If FÌ‚k âŠ† FÌ‚k+1 , RÌ‚k âŠ† RÌ‚k+1 and LÌ‚k âŠ† LÌ‚k+1 hold, by the synergy effect of ISS and IFS, we have FÌ‚k+1
âŠ†
A
B
A
B
A
FÌ‚k+2 , RÌ‚k+1 âŠ† RÌ‚k+2 and LÌ‚k+1 âŠ† LÌ‚k+2 hold.
A
A
B
A
Thus, FÌ‚kB âŠ† FÌ‚k+1
, RÌ‚B
k âŠ† RÌ‚k+1 and LÌ‚k âŠ† LÌ‚k+1 hold for all k â‰¥ 0.
A
B
B
B
, RÌ‚A
Similar with the analysis in (1), we can also prove that FÌ‚kA âŠ† FÌ‚k+1
k âŠ† RÌ‚k+1 and LÌ‚k âŠ† LÌ‚k+1 hold for all k â‰¥ 0.
Combine (1) and (2), we can get

FÌ‚0B âŠ† FÌ‚1A âŠ† FÌ‚2B âŠ† FÌ‚3A ....

(36)

FÌ‚0A âŠ† FÌ‚1B âŠ† FÌ‚2A âŠ† FÌ‚3B ....
A
B
A
RÌ‚B
0 âŠ† RÌ‚1 âŠ† RÌ‚2 âŠ† RÌ‚3 ....
B
A
A
RÌ‚A
0 âŠ† RÌ‚1 âŠ† RÌ‚2 âŠ† RÌ‚3 ....
A
B
A
LÌ‚B
0 âŠ† LÌ‚1 âŠ† LÌ‚2 âŠ† LÌ‚3 ....
B
A
B
LÌ‚A
0 âŠ† LÌ‚1 âŠ† LÌ‚2 âŠ† LÌ‚3 ....

(37)
(38)
(39)
(40)
(41)

B
A
B
by the first equality of (33), (36) and (37), we can get FÌ‚pA = FÌ‚qB . Similarly, we can get RÌ‚A
p = RÌ‚q and LÌ‚p = LÌ‚q .
B
B
A
B
A
A
(2) If p is odd, then by (36), (38 and (40), we have FÌ‚p âŠ† FÌ‚p+1 , RÌ‚p âŠ† RÌ‚p+1 and LÌ‚p âŠ† LÌ‚p+1 . Thus q â‰¤ p + 1.
B
B
A
B
, RÌ‚A
Else if p is even, then by (37), (39) and (41), we have FÌ‚pA âŠ† FÌ‚p+1
p âŠ† RÌ‚p+1 and LÌ‚p âŠ† LÌ‚p+1 . Thus q â‰¤ p + 1.
Do the same analysis for q, we can get p â‰¤ q + 1.
Hence, |p âˆ’ q| â‰¤ 1.
The proof is complete.

L. Experiment Result
L.1. Verification of the Synergy Effect
Here, we verify the synergy effect between ISS and IFS in SIFS from the experiment results on the dataset real-sim. In
Fig. 4, SIFS performs ISS (sample screening) first, while in Fig. 5, it performs IFS (feature screening) first. All the rejection
ratios (Fig. 4(a)-(d)) of the 1st triggering of IFS when SIFS performs ISS first are much higher than (at least equal to) those
(Fig. 5(a)-(d)) when SIFS performs IFS first. In turn, all the rejection ratios (Fig. 5(e)-(h)) of the 1st triggering of ISS when
SIFS performs IFS first are also much higher than those (Fig. 4(e)-(h)) when SIFS performs ISS first. This demonstrates
that the screening result of ISS can reinforce the capability of IFS and vice versa, which is the so called synergy effect. At
last, in Fig. 5 and Fig. 4, we can see that the overall rejection ratios at the end of SIFS are the same, so no matter which (ISS
or IFS) we perform first in SIFS, SIFS has the same screening performances in the end. This is consistent with Theorem 6.
L.2. The Rest Experiment Result
Below, we report the rejection ratios of SIFS on syn1 (Fig. 6), syn3 (Fig. 7), rcv1-train (Fig. 8), rcv1-test(Fig. 9), url
(Fig. 10) and kddb (Fig. 11), which are omitted in the main text due to the space limitation.

Trigger 1
Trigger 2
Trigger 3

0.985

0.01 0.03 0.1

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

1

1

1

0
0.01 0.03 0.1

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

Rejection Ratio

(c) Î²/Î²max =0.5

Trigger 1
Trigger 2
Trigger 3

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

(e) Î²/Î²max =0.05

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

(d) Î²/Î²max =0.9

0.4 1

1
0.995
Trigger 1
Trigger 2
Trigger 3

0.99

0.985
0.01 0.03 0.1

0.4 1

Î±/Î±max

Î±/Î±max

(f) Î²/Î²max =0.1

0.4 1

Î±/Î±max

(b) Î²/Î²max =0.1

0.5

1

Î±/Î±max

(a) Î²/Î²max =0.05
Rejection Ratio

Rejection Ratio

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

0.99

1

Rejection Ratio

0.995

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(g) Î²/Î²max =0.5

(h) Î²/Î²max =0.9

0.01 0.03 0.1

0.4 1

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

(b) Î²/Î²max =0.1

1

1

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

1
0.995
Trigger 1
Trigger 2
Trigger 3

0.99

0.985
0.01 0.03 0.1

(d) Î²/Î²max =0.9

0.4 1

1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

(f) Î²/Î²max =0.1

0.4 1

Î±/Î±max

(c) Î²/Î²max =0.5

Î±/Î±max

(e) Î²/Î²max =0.05

0.4 1

1

Î±/Î±max

(a) Î²/Î²max =0.05

0.5

0.5

Î±/Î±max

Rejection Ratio

Rejection Ratio

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

0.97

0.5

Rejection Ratio

Trigger 1
Trigger 2
Trigger 3

0.98

1

Rejection Ratio

0.99

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Figure 4. Rejection ratios of SIFS on real-sim when it performs ISS first (first row: Feature Screening, second row: Sample Screening).

0.4 1

Î±/Î±max

(g) Î²/Î²max =0.5

(h) Î²/Î²max =0.9

0.4 1

0
0.01 0.03 0.1

Î±/Î±max

0.98
Trigger 1
Trigger 2
Trigger 3

0.4 1

Î±/Î±max

(e) Î²/Î²max =0.05

Rejection Ratio

Rejection Ratio

1

0.01 0.03 0.1

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

(a) Î²/Î²max =0.05

0.96

Trigger 1
Trigger 2
Trigger 3

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

1

1

0.98
Trigger 1
Trigger 2
Trigger 3

0.4 1

Î±/Î±max

(f) Î²/Î²max =0.1

0.98

0.96

Trigger 1
Trigger 2
Trigger 3

0.01 0.03 0.1

0.4 1

Î±/Î±max

(c) Î²/Î²max =0.5

0.01 0.03 0.1

1

Î±/Î±max

(b) Î²/Î²max =0.1

0.96

Rejection Ratio

0
0.01 0.03 0.1

0.5

1

0.4 1

Î±/Î±max

(g) Î²/Î²max =0.5

(d) Î²/Î²max =0.9
Rejection Ratio

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

0.5

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Figure 5. Rejection ratios of SIFS on real-sim when it performs IFS first(first row: Feature Screening, second row: Sample Screening).

1
0.995
0.99
0.985

Trigger 1
Trigger 2
Trigger 3

0.01 0.03 0.1

0.4 1

Î±/Î±max

(h) Î²/Î²max =0.9

Figure 6. Rejection ratios of SIFS on syn1 (first row: Feature Screening, second row: Sample Screening).

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

Î±/Î±max

(c) Î²/Î²max =0.5

(d) Î²/Î²max =0.9

1

1

1

1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

(e) Î²/Î²max =0.05

Trigger 1
Trigger 2
Trigger 3

Rejection Ratio

(b) Î²/Î²max =0.1
Rejection Ratio

(a) Î²/Î²max =0.05
Rejection Ratio

Rejection Ratio

Trigger 1
Trigger 2
Trigger 3

1

Rejection Ratio

0.5

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

0.4 1

0.5

Trigger 1
Trigger 2
Trigger 3

0
0.01 0.03 0.1

Î±/Î±max

(f) Î²/Î²max =0.1

0.4 1

Î±/Î±max

(g) Î²/Î²max =0.5

(h) Î²/Î²max =0.9

0.01 0.03 0.1

0.4 1

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

1

1

1

0
0.01 0.03 0.1

0.4 1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

Rejection Ratio

(c) Î²/Î²max =0.5

Round 1
Round 2
Round 3

0.4 1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

(e) Î²/Î²max =0.05

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

(d) Î²/Î²max =0.9
1
0.995
Round 1
Round 2
Round 3

0.99
0.01 0.03 0.1

0.4 1

0.4 1

Î±/Î±max

Î±/Î±max

(f) Î²/Î²max =0.1

0.4 1

Î±/Î±max

(b) Î²/Î²max =0.1

0.5

1

Î±/Î±max

(a) Î²/Î²max =0.05
Rejection Ratio

Rejection Ratio

Round 1
Round 2
Round 3

Rejection Ratio

0.985

0.5

1

Rejection Ratio

Round 1
Round 2
Round 3

0.99

1

Rejection Ratio

1
0.995

Rejection Ratio

Rejection Ratio

Figure 7. Rejection ratios of SIFS on syn3 (first row: Feature Screening, second row: Sample Screening).

(g) Î²/Î²max =0.5

(h) Î²/Î²max =0.9

0.01 0.03 0.1

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

1

1

1

0
0.01 0.03 0.1

0.4 1

Î±/Î±max

(e) Î²/Î²max =0.05

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

Î±/Î±max

(f) Î²/Î²max =0.1

Rejection Ratio

(c) Î²/Î²max =0.5

Round 1
Round 2
Round 3

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

Î±/Î±max

(b) Î²/Î²max =0.1

0.5

1

Î±/Î±max

(a) Î²/Î²max =0.05
Rejection Ratio

Rejection Ratio

Round 1
Round 2
Round 3

Rejection Ratio

Round 1
Round 2
Round 3

0.99

1

0.4 1

Î±/Î±max

(g) Î²/Î²max =0.5

(d) Î²/Î²max =0.9
Rejection Ratio

0.995

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Figure 8. Rejection ratios of SIFS on rcv1-train dataset (first row: Feature Screening, second row: Sample Screening).

1
0.995
Round 1
Round 2
Round 3

0.99
0.01 0.03 0.1

0.4 1

Î±/Î±max

(h) Î²/Î²max =0.9

Figure 9. Rejection ratios of SIFS on rcv1-test dataset (first row: Feature Screening, second row: Sample Screening).

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

Î±/Î±max

(c) Î²/Î²max =0.5

(d) Î²/Î²max =0.9

1

1

1

1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

(e) Î²/Î²max =0.05

Round 1
Round 2
Round 3

Rejection Ratio

(b) Î²/Î²max =0.1
Rejection Ratio

(a) Î²/Î²max =0.05
Rejection Ratio

Rejection Ratio

Round 1
Round 2
Round 3

1

Rejection Ratio

0.5

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

(f) Î²/Î²max =0.1

Round 1
Round 2
Round 3

0.4 1

Î±/Î±max

(g) Î²/Î²max =0.5

(h) Î²/Î²max =0.9

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

0.5

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

Î±/Î±max

0.4 1

Î±/Î±max

(c) Î²/Î²max =0.5

(d) Î²/Î²max =0.9

1

1

1

1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

Î±/Î±max

(e) Î²/Î²max =0.05

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

Î±/Î±max

(f) Î²/Î²max =0.1

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

Î±/Î±max

(g) Î²/Î²max =0.5

Rejection Ratio

(b) Î²/Î²max =0.1
Rejection Ratio

(a) Î²/Î²max =0.05
Rejection Ratio

Rejection Ratio

Round 1
Round 2
Round 3

1

Rejection Ratio

0.5

1

Rejection Ratio

1

Rejection Ratio

Rejection Ratio

Figure 10. Rejection ratios of SIFS on url dataset (first row: Feature Screening, second row: Sample Screening).

0.5

Round 1
Round 2
Round 3

0
0.01 0.03 0.1

0.4 1

Î±/Î±max

(h) Î²/Î²max =0.9

Figure 11. Rejection ratios of SIFS on kddb dataset (first row: Feature Screening, second row: Sample Screening).

