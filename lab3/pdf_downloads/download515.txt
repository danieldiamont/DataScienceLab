Supplementary Material

Proof of Theorem

is C = RS (h∗ ) + RT (h∗ ), where R denotes the expected
error on each hypothesis.
We
{ consider
}mt the pseudo-labeled target samples set Tl =
(xi , ŷi ) i=1 given false labels at the ratio of ρ. The distribution of the source samples is denoted as S; that of the
target samples, as T ; and that of the pseudo-labeled target samples, as Tl . The minimum shared error on S, Tl is
denoted as C ′ . Then, the following inequality holds:
∀h ∈ H, RT (h) ≤ RS (h) + 21 dH∆H (SX , TX ) + C

3.5
3

0.8

2.5
2

0.7

1.5
Accuracy of labeling method
Accuracy of learned network
Number of labeled samples

0.6

0.5

0

20

40

60

80

1

Number of samples

h∈H

4
0.9

Accuracy

We introduce the derivation of theorem of the main paper. The
joint hypothesis
is defined as h∗ =
( ideal
)
∗
∗
arg min RS (h ) + RT (h ) , and its corresponding error

!10 4
4.5

1

0.5
0
100

Number of steps

Figure 1. The behavior of our model when increasing the number
of steps up to 100. Our model achieves accuracy of about 97%.

≤ RS (h) + 12 dH∆H (SX , TX ) + C ′ + ρ
Proof. The probabiliy of false labels in the pseudo-labeled
set Tl is ρ. When we consider 0-1 loss function for l, the
difference between the error based on the true labeled set
and pseudo-labeled set is
{
1
yi ̸= ŷi
|l(h(xi ), yi ) − l(h(xi ), ŷi )| =
0
yi = ŷi

scenarios, the learning rate is set to 0.01. In the initial training step, The batchsize is set as 128. After the initial step,
the batchsize for training Ft , F is set as 128, the batchsize
for training F1 , F2 , F is set as 64 in all scenarios.

In MNIST→MNIST-M, the dropout rate used in the experiment is 0.2 for training Ft , 0.5 for training F1 , F2 .
The number of iteraions per one step is set 2000. In
MNIST→SVHN, we did not use dropout. We decreased
learning rate to 0.001 after step 10. The number of iteraions
Then, the difference in the expected error is,
per one step is set 3000. In SVHN→MNIST, the dropout
rate used in the experiment is 0.5. The number of itE[|l(h(xi ), yi ) − l(h(xi ), ŷi )|] ≤ |RTl (h) − RT (h)| ≤ ρ
eraions per one step is set 3000. In SYNDIGITS→SVHN,
the dropout rate used in the experiment is 0.5. The
From the characteritic of the loss function, the triangle innumber of iteraions per one step is set 5000.
In
equality will hold, then
SYNSIGNS→GTSRB, the dropout rate used in the experRS (h) + RT (h) = RS (h) + RT (h) − RTl (h) + RTl (h) iment is 0.5. The number of iteraions per one step is set
5000.
≤ RS (h) + RTl (h) + |RTl (h) − RT (h)|
≤ RS (h) + RTl (h) + ρ
From this result, the main inequality holds.

CNN Architectures and training detail
Four types of architectures are used for our method, which
is based on (Ganin & Lempitsky, 2014). The network
topology is shown in Figs 2, 3 and 4. The other hyperparameters are decided on the validation splits. In the all

Semi-supervised domain adaptation
experiments

In semi-supervised domain adaptation in MNIST→SVHN,
we used the same architecture we used in the unsupervised
setting. For the first step of training, we trained all networks solely on source samples. We add randomly selected
labeled target samples into pseudo-labeled target training
sets. Other hyperparameters are the same as the ones used
in unsupervised settings.

Submission and Formatting Instructions for ICML 2017

Supplementary experiments on
MNIST→MNIST-M
We observe the behavior of our model when increasing the
number of steps up to one hundred. We show the result in
Fig. 1. Our model’s accuracy gets about 97%. In our main
experiments, we set the number of steps thirty, but from this
experiment, further improvements can be expected when
the number of steps is increased.

References
Ganin, Yaroslav and Lempitsky, Victor. Unsupervised domain adaptation by backpropagation. In ICML, 2014.

Submission and Formatting Instructions for ICML 2017

F1: Labeling Network1!
FC 100 units
ReLU!

F: Shared Network!
conv
5x5x32
ReLU!

FC 10 units
Softmax!

F2: Labeling Network2!

conv
5x5x48
ReLU!

max-pool 2x2
2x2 stride

FC 100 units
ReLU!

max-pool 2x2
2x2 stride

FC 100 units
ReLU!

FC 100 units
ReLU!

FC 10 units
Softmax!

Ft : Target-specific network!
FC 100 units
ReLU!

FC 100 units
ReLU!

FC 10 units
Softmax!

Figure 2. The architecture used for MNIST→MNIST-M. We added BN layer in the last convolution layer and FC layers in F1 , F2 . We
also used dropout in our experiment.

F1: Labeling Network1!
FC 2048
units
ReLU!

F: Shared Network!

FC 10 units
Softmax!

F2: Labeling Network2!
conv
5x5x64
ReLU!

max-pool 3x3
2x2 stride

conv
5x5x64
ReLU!

max-pool 3x3
2x2 stride

conv
5x5x128
ReLU!

FC 3072
units
ReLU!

FC 2048
units
ReLU!

FC 10 units
Softmax!

Ft : Target-specific network!
FC 2048
units
ReLU!

FC 10 units
Softmax!

Figure 3. The architecture used for training SVHN. In MNIST→SVHN, we added a BN layer in the last FC layer in F . In
SVHN→MNIST, SYN Digits↔SVHN, we added BN layer in the last convolution layer in F and FC layers in F1 ,F2 and also used
dropout.

F1: Labeling Network1!
FC 512 units
ReLU!

FC 43 units
Softmax!

F: Shared Network!
F2: Labeling Network1!
conv
5x5x96
ReLU!

max-pool 2x2
2x2 stride

Conv
3x3x144
ReLU!

max-pool 2x2
2x2 stride

conv
5x5x256
ReLU!

max-pool 2x2
2x2 stride

FC 512 units
ReLU!

FC 43 units
Softmax!

Ft : Target-specific network!
FC 512 units
ReLU!

FC 43 units
Softmax!

Figure 4. The architecture used in the adaptation Synthetic Signs→GTSRB. We added a BN layer after the last convolution layer in F
and also used dropout.

