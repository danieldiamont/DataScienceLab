Guarantees for Greedy Maximization of Non-submodular Functions with Applications

Appendix
A. Organization of the Appendix
Appendix B presents the proofs for our approximation guarantees and its tightness for the G REEDY algorithm.
Appendix C provides details on existing notions of curvature and submodularity ratio, and relates it to the notions in this
paper.
Appendix D presents detailed proofs for bounding the submodularity ratio and curvature for various applications.
Appendix E gives details on the classical SDP formulation of the Bayesian A-optimality objective.
Appendix F provides proofs omitted in Section 6.
Appendix G provides information on more applications, including sparse modeling with strongly convex loss functions,
subset selection using the R2 objective and optimal budget allocation with combinatorial constraints.
Appendix H provides experimental results on subset selection with the R2 objective and additional results on experimental
design.

B. Proofs for Approximation Guarantee and Tightness Result (Section 2 and Section 3)
B.1. Proof of Remarks in Section 2
Proofs of Remark 1.
a) Because F is nondecreasing, and , G are defined as the largest scalars, , G 0. At the same time, both and G
can be at most 1 because the conditions in Def. 1 also have to hold for the case that ⌦\S (⌦\S t , respectively) is a singleton.
b) “ ) ”:

Let ⌦ \ S = {!1 , . . . , !k }, k

1. Submodularity implies

“ ( ”:

Pk

i=1

⇢!i (S)

⇢⌦ (S). Hence,

can take the largest value 1.

= 1 implies that (setting ⌦ \ S = {!i , !j }), for all !i , !j 2 V \ S, it holds that F ({!i } [ S) + F ({!j } [ S)
F ({!i , !j } [ S) + F (S), which is an equivalent way to define submodularity (Bach, 2013, Proposition 2.3).

Proof of Remark 2.
a) “If F (·) is nondecreasing, then ↵, ↵G 2 [0, 1]”;

When ⌦ = ;, ↵ is at least 0. From the definition, ↵G 0. Since F is nondecreasing, ⇢i (S \ {i} [ ⌦)
⇢ji (S i 1 [ ⌦) 0), and we defined ↵, ↵G to be the smallest scalar, it must hold that ↵, ↵G  1.

0 (respectively,

b) “For a nondecreasing function F (·), F (·) is supermodular iff ↵ = 0 ”;
“ ) ”:

If F is supermodular, it always holds that ⇢i (S \ {i} [ ⌦)
know that ↵ must be 0.
“ ( ”:

One can observe that ↵ = 0 is equivalent to
being supermodular.

⇢i (S \ {i}), combined with the fact that ↵ is at least 0, we

F (·) satisfying the diminishing returns property, which is equivalent to F (·)

c) “If F (·) is nondecreasing submodular, then ↵G  ↵ = ↵total .”

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

Since it always holds that ↵G  ↵, we only need to prove that ↵ = ↵total . Wlog., assume ⇢i (S \ {i}) > 0. Then,
⇢i (S \ {i} [ ⌦)
⇢i (S \ {i})
⌦,S✓V,i2S\⌦
⇢i (V \ {i})
= min
(diminishing returns, and taking ⌦ = V \ {i})
S✓V,i2S ⇢i (S \ {i})
⇢i (V \ {i})
= min
(diminishing returns, and taking S = {i})
i2V
⇢i (;)

1

↵=

=1

min

↵total .

So it holds that ↵G  ↵ = ↵total .
B.2. Proof of Lemma 1
Proof of Lemma 1. The proof needs the definitions of generalized curvature, submodularity ratio, and the selection rule of
the G REEDY algorithm.
Firstly, observe,
F (⌦ [ S t ) = F (⌦) +

X

i:ji

⇢ji (⌦ [ S i

2S t

X

= F (⌦) +

i:ji

= F (⌦) +

2S t \⌦

X

i:ji

2S t \⌦

1

⇢ji (⌦ [ S i

⇢ji (⌦ [ S i

)
1

)+
i:ji

1

|

).

X

2S t \⌦

⇢ji (⌦ [ S i
{z

= 0 because ji 2 ⌦

1

)

}

(7)

From the definition of the submodularity ratio,
F (⌦ [ S t )  F (S t ) +

1

X

⇢! (S t ).

(8)

!2⌦\S t

From the definition of curvature (for the greedy curvature, since it holds for S K 1 , it must also hold for S t ✓ S K
have,
X
X
⇢ji (⌦ [ S i 1 ) (1 ↵)
⇢ji (S i 1 ).
i:ji 2S t \⌦

i:ji 2S t \⌦

Combining (7) to (9), and remember that we use the shorthand ⇢t := ⇢jt (S t
F (⌦) = F (⌦ [ S t )
↵
=↵

X

i:ji

↵

i:ji

2S t \⌦

i:ji 2S t \⌦

X

i:ji

⇢i +

2S t \⌦

X

⇢ji (⌦ [ S i

⇢i + F (S t )

2S t \⌦

X

i:ji

X

i:ji

⇢i +

X

2S t \⌦

X

1

), it reads,

)

⇢i +

2S t \⌦

⇢i +

1

1

1

X

⇢! (S t )

!2⌦\S t

X

⇢! (S t )

(K

wt )⇢t+1 ,

!2⌦\S t

⇢i +

1

i:ji 2S t \⌦

where the last inequality is because of the selection rule of the G REEDY algorithm (⇢! (S t )  ⇢t+1 , 8!).

1

), we
(9)

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

B.3. Proof of Claim 1
Since the proof heavily relies on the structure of the constructed LPs, we restate it here: The worst-case approximation
ratio of the group PK,↵, ({l1 , ..., ls }) is
XK
R({l1 , ..., ls }) = min
xi , s.t. xi 0, i 2 [K] and
i=1

2

K/
6 ↵
6
6 ..
6 .
6
6 ↵
6
6 ↵
6
6 ↵
6
6 .
6 ..
6
6
6 ↵
6
6 ..
4 .
↵

row (0)
row (1)
..
.

row (l1 1)
row (l2 1)
row (q = lr )
..
.
row (ls
..
.

1)

row (K

1)

K/
..
.

..

.
···
···
···

↵
↵
↵
..
.
↵
..
.
↵

K/
1
1
..
.
1
..
.
1

···
···

0
(K

1)/
K r

1
..
.
1
..
.
1

..
.
↵
..
.
↵

..

.
···

K s+1

..
.
1

···

..

.
···

K

3 2
3
x1
7 6 x 7
7 6 2 7
7 6 . 7
7 6 . 7
7 6 . 7
7 6
7 6 x l1 7
7
7 6
7 6 x l2 7
7
7·6
7 6xq+1 7
7 6 . 7
7 6 . 7
7 6 . 7
7
7 6
7 6 x ls 7
7 6 . 7
7 4 . 7
. 5
5
s
xK

2 3
1
617
6 7
6 .. 7
6.7
6 7
617
6 7
617
6 7
617 (2)
6 7
6 .. 7
6.7
6 7
617
6 7
6.7
4 .. 5
1

For notational simplicity, w.l.o.g., assume that ji = i, i 2 [K]. Let the row index in (2) start from 0.
Proof of Claim 1.
Let ✏ =

⇤
)x⇤
q (K r)xq+1
K r+1

(K r+1

K r
⇤
K r+1 (xq

x⇤q+1 ) > 0.

0 since the only decreased entry is the q th entry, and one can easily see that yq⇤ = x⇤q

a) It is easy to see that y ⇤

✏

0.

b) “All of the constraints in (2) are still feasible for y .”
⇤

(i) For the rows 0 to (q
(ii) For the (q

2) in (2), there is no change, so they are still feasible.

th

1) and q th rows in (2), they are
K

↵x⇤1 + · · · + ↵(or 1)x⇤q

1

+

↵x⇤1 + · · · + ↵(or 1)x⇤q

1

+ x⇤q +

r+1

x⇤q

K

r

(10)

1

x⇤q+1

(11)

1

For (10), after plugging y ⇤ into its L.H.S., we get ↵y1⇤ + · · · + ↵(or 1)yq⇤ 1 + K r+1 yq⇤ , subtract from which the L.H.S.
of (11), we get


K r+1 ⇤
K r ⇤
↵y1⇤ + · · · + ↵(or 1)yq⇤ 1 +
yq
↵x⇤1 + · · · + ↵(or 1)x⇤q 1 + x⇤q +
xq+1
=

K

r+1

(x⇤q

✏)

x⇤q

K

r

x⇤q+1

= 0,
so ↵y1⇤ + · · · + ↵(or 1)yq⇤

1

+

K r+1 ⇤
yq

After increasing x⇤q+1 by ✏q+1 = ✏ K
(iii) For the rows q to (K

r,

1 and y ⇤ is feasible for (10).

the q th row in (2) is feasible since the change in its L.H.S. is

1) in (2), let us prove by induction.

For the base case, consider the (q + 1)th row in (2), it can be either,
↵x⇤1 + · · · + ↵(or 1)x⇤q

1

+ x⇤q + x⇤q+1 +

K

r

1

x⇤q+2

1

✏ + ✏ = 0.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

or
↵x⇤1 + · · · + ↵(or 1)x⇤q

1

K

+ x⇤q + ↵x⇤q+1 +

r

x⇤q+2

1

It can be easily verified that the (q + 1)th row in (2) is still feasible in both the above two situations. Let us use
denote the change of L.H.S. of the (q + u)th row after applying the changes.
For the inductive step, assume that the claim holds for u = u0 , i.e., the (q + u0 )th row in (2) is feasible or
(q + u0 )th row is,
K

(...same as (q + u0 + 1)th row) +

r

v

x⇤q+u0 +1

q+u

to

0. The

q+u0

1

where 0  v  u0 is some integer dependent on the structure of (2), but not affect the final analysis. Then the (q + u0 + 1)th
row can be either,
(...

K

same as (q + u0 )th row) + x⇤q+u0 +1 +

r

v

1

x⇤q+u0 +2

1 (case 1)

or
(...
In

same as (q + u0 )th row) + ↵x⇤q+u0 +1 +

K

r

v

x⇤q+u0 +2

1 (case 2)

(case 1), the L.H.S. of (q + u0 + 1)th row minus the L.H.S. of (q + u0 )th row is

K r v

K r v 1 ⇤
xq+u0 +2

x⇤q+u0 +1 , so

q+u0 +1

q+u0

=

K


r

v

K

r

= (K

(K

r

=



r

1

K

✏q+u0 +2

r

v

✏q+u0 +1

1 K r u0
K r v
K r u0 1
K r u0
v 1)
(K r v
K r u0 1
K r v
v 1)
(K r v
K r v 1

v

✏q+u0 +1
)

✏q+u0 +1
✏q+u0 +1

)

(since 0  v  u0 )

= 0.

so the (q + u0 + 1)th row is still feasible.
In (case 2), the L.H.S. of (q + u0 + 1)th row minus the L.H.S. of (q + u0 )th row is
↵)x⇤q+u0 +1 , so
q+u0 +1

q+u0

=

K

r

v

K

r

v

0.

✏q+u0 +2

(

(✏q+u0 +2

K

r

✏q+u0 +1 )

(since ✏q+u0 +2

v

K r v

x⇤q+u0 +2

(K

↵)✏q+u0 +1
(since ↵

0)

✏q+u0 +1 )

so the (q + u0 + 1)th row is feasible. Thus we finish proving Claim 1.
B.4. Proof of Claim 2
Proof of Claim 2. The change of the LP objective is
LP

✏ + ✏q+1 + ✏q+2 + · · · + ✏K

K
=✏ 1+
+
·
K r K r K
=

r
r

1

+ ··· +

K

K
r K
·

r
r

1

···

K

r
K

m+2
r m+1

,

r v

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

where inside the bracket there are m = K
bracket to be,
hr ( ) :=
First of all, since K

1+
r

K
K

r

+

K

r

q items except for the “

1”. For notational simplicity, let the sum inside the

K
K

r

·

r
r

1

+ ··· +

K

·

K
K

r
r

1

···

K

r
K

r

m+2
m+1

.

(12)

q = m, we have that

hr ( )  hr=q ( ) =

1+

m

+

m
m m
·

1

+ ··· +

m
m m
·

1

···

3
2

·

2
1

(13)

.

Let us merge the items in (13) from left to right one by one,
hr=q ( ) =

1+
m

=

m
m

=

m
···

=

m

(m

setting

m
m
3
2
·
+ ··· +
·
···
·
m m 1
m m 1
2
1
m
m
3
2
+
·
+ ··· +
·
···
·
m m 1
m m 1
2
1
m 1
m
3
2
+ ··· +
·
···
·
m 1
m m 1
2
1
+

)(m
m(m
to be 1



1) · · · (2
)(1
1) · · · 2 · 1

)

0

Then hr ( )  0, 8 2 (0, 1]. And it is easy to see that the equality holds if r = q and
So we have that

LP

= 1.

= ✏hr ( )  0, where the equality is achieved at “boundary” situation (r = q and

= 1).

B.5. Proof of Lemma 3
Proof of Lemma 3.
For notational simplicity, wlog., assume that ji = i, i 2 [K].
a) Firstly let us prove that R({l1 , ..., ls })

R(;).

The high-level idea is to change the structure of the constraint matrix in the LP associated with {l1 , ..., ls }, such that in
each change, the optimal LP objective value R never increases.
To better explain the proof, let us state the setup first of all. Let us call the elements inside the set ⌦⇤ \ S K = {l1 =
jm1 , l2 = jm2 , ..., ls = jms } the “joint elements”, which means that they are joint elements in ⌦⇤ and S K . Similarly, the
elements outside of ⌦⇤ \ S K are called the “disjoint” elements. For the joint elements, two elements li , lj being “adjacent”
means that li + 1 = lj . Mapping to the constraint matrix in (2), it means that the corresponding columns (column (li ) and
column (lj )) are adjacent with each other. So we also call the corresponding columns in the constraint matrix as “joint
columns”.
We prove part a) of Lemma 3 by two steps: In the first step, we try to make all of the joint elements inside {l1 , l2 , ..., ls } to
be adjacent with each other; In the second step, we get rid of the joint columns in the constraint matrix from left to right,
one by one. Specifically,
Step 1. Assume that some elements inside {l1 , l2 , ..., ls } are not adjacent, like the example in (2), where l2 and l3 are not
adjacent. Suppose that lr and lr+1 are not adjacent, which means lr + 1 < lr+1 . Denote p = lr for notational simplicity.
Let us use A to represent the constraint matrix in the constructed LP associated with {l1 , l2 , ..., lr 1 , lr , lr+1 , ..., ls }, let A0
represent the constraint matrix associated with {l1 , l2 , ..., lr 1 , lr + 1, lr+1 , ..., ls }. Notice that lr + 1 is a disjoint element
for A, but a joint element for A0 . Furthermore A and A0 only differ by columns p and p + 1 = lr + 1. Assume that
x⇤ 2 R K
+ is the optimal solution of the constructed LP with A as its constraint matrix. From Lemma 2, it must hold that
x⇤p  x⇤p+1 . Combining with the fact that Ax⇤ 1, one can easily verify that A0 x⇤ 1, which implies that,
R({l1 , l2 , ..., lr

1 , lr , lr+1 , ..., ls })

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

R({l1 , l2 , ..., lr

1 , lr

(14)

+ 1, lr+1 , ..., ls }).

The change from {l1 , l2 , ..., lr 1 , lr , lr+1 , ..., ls } to {l1 , l2 , ..., lr 1 , lr + 1, lr+1 , ..., ls } is essentially to swap the roles of
one originally disjoint element lr + 1 and the originally joint element lr . Repeatedly applying this operation for all 1 
r  s 1 such that lr + 1 < lr+1 , we can get that,
R({l1 , l2 , ..., lr
R({ls
Now the s joint elements inside {ls

s + 1, ls

1 , lr , lr+1 , ..., ls })

s + 1, ls

(15)

1, ls }).

s + 2, ..., ls

1, ls } are adjacent with each other.

s + 2, ..., ls

Step 2. Let B be the constraint matrix associated with {ls s + 1, ls s + 2, ..., ls 1, ls }, and B0 be the constraint
matrix associated with {ls s + 2, ..., ls 1, ls }. Note that B and B0 differ in the columns from ls s + 1 to the end.
Suppose the vector x⇤ is the optimal solution of the constructed LP with B as the constraint matrix. According to Lemma 2
we know that x⇤ls s+1  x⇤ls s+2  · · ·  x⇤ls  x⇤ls +1 . So one can easily verify that it must hold that B0 x⇤ 1, which
implies
s + 1, ls s + 2, ..., ls 1, ls })
R({ls s + 2, ..., ls 1, ls }).

R({ls

Apply this process repeatedly s times, one can reach that R({ls

s + 1, ls

(16)

s + 2, ..., ls

Combining step 1 and step 2, we prove part a) of Lemma 3.

⇣
⌘K
K ↵
1
b) Then let us prove that R(;) = ↵ 1
.
K

1, ls })

R(;).

The constructed LP associated with R(;) is,

R(;) = min

K
X

xi

i=1

subject to the constraints that,
0, 8i = 1, ..., K

xi
and

2K

6↵
6
6.
6.
6.
6
6↵
6
6↵
6
6↵
6
6.
6.
6.
6
6↵
6
6 ..
4.
↵

3

2 3
x1
7
7 6 x2 7
7 6 7
7 6 .. 7
7 6 . 7
7 6 7
7 6 xa 7
7 6 7
7 6 xb 7
7 6 7
7 · 6 xc 7
7 6 7
7 6 .. 7
7 6 . 7
7 6 7
7 6 xd 7
7 6 7
7 6 . 7
7 4 .. 5
5
xK
K

K

..
.
↵
↵
↵
..
.
↵
..
.
↵

..

.
···
···
···
···
···

0

K

↵
↵
..
.
↵
..
.
↵

K

↵
..
.
↵
..
.
↵

K

..
.
↵
..
.
↵

One can observe that the vector y 2 RK
+ such that yi =
row in (17) is tight, hence y is the optimal solution. So

..

.
···
···
⇣

K

..
.
↵

K

K

↵
K

"
1
R(;) =
yi =
1
↵
i=1
K
X

..

⌘i

✓

.
···
1

2 3
1
617
6 7
6 .. 7
6.7
6 7
617
6 7
617
6 7
617
6 7
6 .. 7
6.7
6 7
617
6 7
6.7
4 .. 5
1

(17)

, i = 1, ..., K satisfies all the constraints and every

K

↵
K

◆K #

.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

B.6. Proof for the Tightness Result
Proof of Lemma 4.
a) “When ↵ = 0, F (·) is supermodular”;
It is easy to see that ⇠i = 1/K, i 2 [K]. Since f (·) is convex, it can be easily verified that F (·) is supermodular.
b) “When

= 1, F (·) is submodular”;

Now f (x) = x. Assume there are T1 ✓ T2 ✓ V, t 2 V \T2 . Let T1 = S10 [⌦01 , T2 = S20 [⌦02 , where S10 , S20 ✓ S, ⌦01 , ⌦02 ✓
⌦. It holds that S10 ✓ S20 , ⌦01 ✓ ⌦02 . Now there are two cases:
1) t = ji 2 S. Then,

h
⇢ji (T1 ) = 1

i
↵
f (|⌦01 |) ⇠i ,
K

2
1 4
⇢!i (T1 ) =
1
K

X

Because f (·) is nondecreasing, so it holds ⇢ji (T1 )
2) t = !i 2 ⌦. It reads,

Because S10 ✓ S20 , so ⇢!i (T1 )

↵

⇢ji (T2 ).

ji 2S10

⇢!i (T2 ).

3

h
⇢ji (T2 ) = 1

2
1 4
⇢!i (T2 ) =
1
K

⇠i 5 ,

The above two situations prove the submodularity of F (T ) when
c) “F (T ) has submodularity ratio

i
↵
f (|⌦02 |) ⇠i
K

↵

X

ji 2S20

3

⇠i 5

= 1.

and curvature ↵”.

Let us assume T = A [ B and T 0 = A0 [ B 0 are two disjoint sets (T \ T 0 = ;), where A and A0 are subsets of S while B
and B 0 are subsets of ⌦. It is easy to see that A \ A0 = ;, B \ B 0 = ;.

First of all, for the
submodularity ratio, assume without loss of generality8 that ⇢T 0 (T ) > 0, so the submodularity ratio
P
i2T 0 ⇢i (T )
is = minT,T 0 ⇢ 0 (T ) .
T

One can see that,
⇢T 0 (T ) = F (T 0 [ T )

f (|B [ B 0 |)
=
K

and

X

⇢i (T ) =

i2T 0

X

F (T )
f (|B|)

⇢!i (T ) +

!i 2B 0

f (|B| + 1)
= |B 0 |
K
⇥
Because f (|B|)  f (|B [ B 0 |), so one has 1
when B 0 = ; or A0 = ;. Therefore,
P

8

(1

↵

X

ji 2A

X

h
⇠i ) + 1

ji 2A

⇢ji (T )

ji 2A0

0
f (|B|) @

↵
K

i X
↵
f (|B [ B 0 |)
⇠i
K
0

1

↵

X

ji 2A

f (|B [ B 0 |)

⇤P

1

h
⇠i A + 1

ji 2A0

⇥
⇠i  1

i X
↵
f (|B|)
⇠i .
K
0
ji 2A

↵
K

f (|B|)

⇤P

ji 2A0

⇠i , equality holds

⇣
⌘ ⇥
⇤P
P
f (|B|)
↵
|B 0 | f (|B|+1)
1 ↵
⇢i (T )
ji 2A ⇠i + 1
ji 2A0 ⇠i
K
K f (|B|)
= f (|B[B 0 |) f (|B|)
⇥
⇤
P
P
↵
0
⇢T 0 (T )
(1 ↵
ji 2A ⇠i ) + 1
ji 2A0 ⇠i
K
K f (|B [ B |)
⇣
⌘
⇥
⇤
P
P
f (|B|)
↵
|B 0 | f (|B|+1)
1 ↵
ji 2A ⇠i + 1
ji 2A0 ⇠i
K
K f (|B|)
⇥
⇤
P
P
0
f (|B[B |) f (|B|)
↵
(1 ↵
ji 2A ⇠i ) + 1
ji 2A0 ⇠i
K
K f (|B|)

i2T 0

If ⇢T 0 (T ) = 0, from monotonicity of F (·), it must hold

P

i2T 0

⇢i (T ) = 0, this case is not of interest in Def. 1.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

|B 0 |(f (|B| + 1) f (|B|))
,
f (|B [ B 0 |) f (|B|)
where

(18)

|B 0 | f (|B|+1)
K

(18)

comes
from the ⌘ fact:
f (·) is
⇣
P
f (|B[B 0 |) f (|B|)
1 ↵
⇠

(1
ji 2A i
K

convex and
P
↵
ji 2A ⇠i ).

f (|B|)

nondecreasing

in

[0, K],

thus

Now to continue with (18), one can verify that by setting B = ;, B 0 = ⌦, the minimum of (18) is achieved as , thus
proving the submodularity ratio to be .
Then for the curvature, for any t 2 T = A [ B, we want to lower bound
1) When t = ji 2 A, we have

⇢t (T \{t}[T 0 )
⇢t (T \{t}) .

There are two cases:

⇥
⇤
1 ↵K f (|B [ B 0 |) ⇠i
⇢ji (T \ {ji } [ T 0 )
⇤
= ⇥
⇢ji (T \ {ji })
1 ↵K f (|B|) ⇠i
=

↵
K

1

1

f (|B [ B 0 |)
.
f (|B|)

(19)

↵
K

Since f (·) is convex and nondecreasing in [0, K], it is easy to see that the minimum of (19) is achieved when B = ;, B 0 = ⌦
as 1 ↵.
2) When t = !i 2 B, we have,
⇢!i (T \ {!i } [ T 0 )
=
⇢!i (T \ {!i })

f (|B[B 0 |) f (|B[B 0 | 1) ⇥
1
K
f (|B|) f (|B| 1) ⇥
1
K

↵

↵
P
0
1 ↵
⇠
i0 2A[A0 i
P
1 ↵
i2A ⇠i
P
1 ↵+↵ ↵
0
0 ⇠ i0
P i 2A[A
=
1 ↵
i2A ⇠i

P

P

i0 2A[A0

i2A ⇠i

⇤

⇠ i0

⇤
(20)
(21)

where (20) is because f (·) is convex and nondecreasing in [0, K].
P
P
0
Since ↵ ↵
0 and ↵
i0 2A[A0 ⇠i
i2A ⇠i  0, continuing with (21) we have,
⇢!i (T 0 \ {!i } [ T )
⇢!i (T \ {!i })

1

↵.

The above two cases jointly prove that the objective in (3) has curvature ↵.

C. Existing Notions of Curvature and Submodularity Ratio
In this section we firstly discuss existing notions of curvature and submodularity ratio, then secondly we present the
relations to the notions in this paper.
C.1. Classical Notions of Curvature and Submodularity Ratio
The curvature of submodular functions measures how close a submodular set function is to being modular, and has been
used to prove improved theoretical results for constrained submodular minimization and learning of submodular functions
(Iyer et al., 2013). Earlier, it has been used to tighten bounds for submodular maximization subject to a cardinality
constraint (Conforti & Cornuéjols, 1984) or a matroid constraint (Vondrák, 2010).
Definition 3 (Curvature of submodular functions (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013)). The
total curvature F (which we term as ↵total in the main text) of a submodular function F and the curvature F (S) w.r.t. a
set S ✓ V are defined as,
F := 1

min
j2V

⇢j (V \ {j})
and
⇢j (;)

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

F (S) := 1

min
j2S

⇢j (S \ {j})
,
⇢j (;)

respectively. Assume without loss of generality that F ({j}) > 0, 8j 2 V. One can observe that F (S)  F . A modular
function has curvature F = 0, and a matroid rank function has maximal curvature F = 1. Vondrák (2010) also defines
the relaxed notion of curvature (which is called curvature with respect to the optimum) to be the smaller scalar ̄F (S) s.t,
X
⇢T (S) +
⇢j (S [ T \ {j}) (1 ̄F (S))⇢T (;), 8T ✓ V.
(22)
j2S[T

Iyer et al. (2013) propose two new notions of curvature, which are,
P
⇢T (S) + j2S[T ⇢j (S [ T \ {j})
̃F (S) := 1 min
,
T ✓V
⇢T (;)
P
j2S ⇢j (S \ {j})
P
̂F (S) := 1
.
j2S ⇢j (;)

Iyer et al. (2013) show that for submodular functions, it holds that ̂F (S)  F (S)  ̃F (S)  F .
Submodularity ratio. Informally, the submodularity ratio quantifies how close a set function is to being submodular (Das
& Kempe, 2011).
Definition 4 (Original submodularity ratio from Das & Kempe (2011)). Let F (·) be a non-negative nondecreasing set
function. The submodularity ratio of a set U w.r.t. an integer k is given by,
P
j2L ⇢j (S)
min
.
U,k := min
L✓U L,S:L\S=;,|S|k
⇢L (S)
C.2. Curvature of Non-submodular Functions and Relation to Our Results
Sviridenko et al. (2013) present a new notion of curvature for monotone set functions. We show how it is related to
our notion of curvature in Def. 2. We also show that our approximation factors using the combination of curvature and
submodularity ratio characterize the performance of G REEDY for solving problem (P) better.
Specifically, for a nondecreasing function F , Sviridenko et al. (2013, Section 8) define the curvature c as
1

c = min

min

j2V A,B2V\{j}

⇢j (A)
.
⇢j (B)

(23)

(Sviridenko et al., 2013, Theorem 8.1) show that for maximizing a nondecreasing function with bounded curvature c 2
[0, 1] under a matroid constraint, G REEDY enjoys an approximation guarantee of (1 c), and it is tight in terms of the
definition of c in (23). The following remark discusses the relation to our definition of curvature.
Remark 3. For a nondecreasing function F (·), it holds: a) c in (23) is always larger than the notion of curvature ↵ in
Def. 2, i.e., c ↵; b) For the G REEDY algorithm, there exists a class of functions for which the approximation guarantee
characterized by c (which is 1 c) is strictly smaller than the approximation guarantee characterized by the combination
of ↵ and (which is ↵ 1 (1 e ↵ ) according to Theorem 1).
Proof of Remark 3.
a) Note that the definition of curvature in Def. 2 is equivalent to the smallest scalar ↵ such that,
Now it is easy to see that c

8j 2 V, 8B ✓ A 2 V \ {j}, ⇢j (A)

(1

↵)⇢j (B).

↵.

b) Consider the class of functions in our tightness result in (3). From Lemma 4 we know that its curvature is ↵ and
submodularity ratio is . So its curvature c in (23) must be greater than or equal to ↵. Note that the approximation
guarantee characterized by c is 1 c  1 ↵. Taking ↵ = 1 in (3), the approximation guarantee of Sviridenko et al.
(2013) is 0. While our approximation guarantee is , for any 2 (0, 1], our approximation guarantee is strictly higher than
1 c.

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

C.3. Relation to Notions in This Work
- There are two versions of submodularity ratio in this paper: and G , G cannot be recovered from Def. 4. Our theory
can easily accommodate Def. 4: our approximation guarantee in Theorem 1 holds for Def. 4 as long as U contains ⌦⇤
and k
K. One benefit of the definition in this work (Def. 1) is that it better handles subtleties in Def. 4 where the
denominator could be 0.
- The curvature in this work is a natural extension of the classical ones for monotone nondecreasing submodular functions (Conforti & Cornuéjols, 1984).
- Note that classical notions of curvature measure how close a submodular set function is to being modular. The notions
of (generalized) curvature in Def. 2 measures how close a set function is to being supermodular.
- Our combinations of (generalized) curvature and submodularity ratio gives tight approximation guarantees for
G REEDY, and this combination is more expressive than the curvature by Sviridenko et al. (2013), as shown in Remark 3.

D. Proofs for Bounding Parameters of Applications
D.1. Proving Proposition 1
Proof of Proposition 1.
Notice that in this subsection, the matrix XS = [xv1 , . . . , xvs ] 2 Rd⇥|S| is the submatrix consisting the columns of X
indexed by the set S.
Our proof considers the spectral parameters of the matrix XS X>
S . For brevity, let us write B = ⇤ +
symmetric positive definite matrix, thus can be factorized as B = PDP 1 .

2

Let the eigenvalues of XS X>
S be
[d]. Then the eigenvalues of B are
Pd
1
i=1 2 + 2 i (S) .

>
i (XS XS ), 8i
1
1

1 (S)
2
2

+

···
0, where we use the notation that i (S) :=
d (S)
1
= PD 1 P 1 , and tr(B
i (S), i 2 [d]. One can see that B

XS X>
S . B is a

) = tr(D

2
)=

Let the singular values of XS be 1 (XS )
···
q (XS ), where q  min{d, |S|}. For notational simplicity, when
|S| < d, we still use the convention i (XS ) = 0, i = q + 1, ..., d to represent the zeros values. One has i2 (XS ) =
i (S), i = 1, ..., d. For notational simplicity, we use F (·) to represent FA (·) in the following.
Monotonicity. It can be easily seen that F (;) = 0. To prove that F (S) is monotone nondecreasing, one just needs to
show that 8! 2 V \ S, it holds that F ({!} [ S) F (S) 0. One can see that,
F ({!} [ S)

F (S) =

d
X

1
2

i=1

0

2 2 (X )
S
i

+

d
X

1
2

j=1

2 2 (X
S[{!} )
j

+

(Cauchy interlacing inequality of singular values).

Bounding parameters. Let us restate the assumption: The data points are normalized,
p i.e., kxi k = 1, 8i 2 V. Given
this assumption, it holds that the spectral norm of the data matrix kXk = max (X)  n, because of Weyl’s inequality.
–Bounding the submodularity ratio: We need to lower bound

P

⇢! (S)
⇢⌦ (S)

!2⌦\S

=

P

F ({!}[S) F (S)
.
F (⌦[S) F (S)

!2⌦\S

For the numerator, we have,
X

!2⌦\S

F ({!} [ S)

F (S) =

X

!2⌦\S

=

2
4

d
X

1
2

i=1

d
X X

!2⌦\S i=1

+

2 2 (X )
S
i
2

(

2

+

d
X
j=1

2
i (XS[{!} )
2 2 (X ))( 2 +
S
i

[

1
2

+

2 2 (X
S[{!} )
j

2
i (XS )]
2 2 (X
S[{!} ))
i

3
5

Guarantees for Greedy Maximization of Non-submodular Functions with Applications
2

(

d
X X

2 2
2
max (X))

+

2

2
i (XS[{!} )

[

2
i (XS )]

!2⌦\S i=1
2

=(
=(

2

=(

2

=(

2

2

=(

2

=

2

+
+

2

+

2

+

2

(

2

2

kXk2 )

2

2

2

kXk )

2

kXk2 )

2

+

2

X

[ i (S [ {!})

i (S)]

2

[tr(XS[{!} X>
S[{!} )

tr(XS X>
S )]

2

>
[tr(XS X>
S + x! x! )

tr(XS X>
S )]

2

tr(x! x>
!)

2

kx! k2

!2⌦\S

2

2

d
X X

!2⌦\S i=1

kXk )

2

+

kXk2 )

X

!2⌦\S

X

(linearity of the trace )

!2⌦\S

X

!2⌦\S
2

kXk )

2

(normalization of the data points)

|⌦ \ S|

(24)

For the denominator, one has,
F (⌦ [ S)

F (S) =

d
X
i=1



d
X

1
2

2 2 (X )
S
i

+

d
X

1
2

j=1

+

|⌦\S|

X

1
2

i=d |⌦\S|+1

2 2 (X )
S
i

+

1

2

Combining (24) and (25) yields,
P

1
2

j=1

1
)
+ 2 kXk2
2
kXk2
= |⌦ \ S| 2 2
.
( + 2 kXk2 )
 |⌦ \ S|(

2 2 (X
S[⌦ )
j

(interlacing inequality of singular values)

2

!2⌦\S

F ({!} [ S)

F (⌦ [ S)

(25)

F (S)

F (S)

d
X

2

i0 =1
2

(

(

2

kXk2 (

2

2

+

kXk2 )

2

2 kXk2

|⌦ \ S|

–Bounding the curvature: We want to lower bound 1
For the numerator, one has,
F (S \ {i} [ ⌦) =

2

|⌦ \ S|
=

F (S [ ⌦)

2 2 (X
S[⌦ )
j

+

2( 2+

2
2

+

2 kXk2 )

2 kXk2 )

.

↵, which corresponds to lower bounding

+

2
2

+

1
2 (X
S\{i}[⌦ )
i0
kXk2 )

2

d
X
j=1

F (S[⌦) F (S\{i}[⌦)
.
F (S) F (S\{i})

1
2

+

2 2 (X
S[⌦ )
j

(similar derivation as in (24)) .

For the denominator, one has (similar derivation as in (25)),
F (S)

F (S \ {i}) =


d
X

i0 =1
2

+

d
X

1
2

2 2 (X
S\{i} )
i0

+

j=1

1
2 2 (X
S\{i} )
d

1
2

2

+

+

2 2 (X )
S
j

1
2 2 (X )
S
1

(Cauchy interlacing inequality)

(26)

Guarantees for Greedy Maximization of Non-submodular Functions with Applications
2

kXk2
.
2( 2 +
2 kXk2 )



(27)

Combining (26) and (27) we get,
F (S [ ⌦)
F (S)

2

F (S \ {i} [ ⌦)
F (S \ {i})

kXk2 (

2

+

2 kXk2 )

.

D.2. Proofs for Determinantal Functions of Square Submatrix
Proof of Proposition 2.
Notice that in this subsection, the matrix ⌃S is the square submatrix of ⌃, with both its rows and columns indexed by S.
a) We want to prove that F (·) is supermodular. Assume that A ✓ B ✓ V and i 2 V \ B, then
⇢i (A) = det(I + 2 ⌃A[{i} ) det(I + 2 ⌃A )
X
X
=
det(( 2 ⌃)S )
det(( 2 ⌃)S )
=

X

(Kulesza & Taskar, 2012, Theorem 2.1)

S✓A

S✓A[{i}

2

det((

⌃)S[{i} )

S✓A



X

2

det((

(⌃ is positive semidefinite)

⌃)S[{i} )

S✓B
2

= det(I +

⌃B[{i} )

det(I +

2

⌃B )

= ⇢i (B),
which proves that F (·) is supermodular.
b) We want to lower bound

P

⇢! (S)
⇢⌦ (S)

!2⌦\S

=

P

F ({!}[S) F (S)
.
F (⌦[S) F (S)

!2⌦\S

For the numerator, one has,
X

!2⌦\S

F ({!} [ S)

F (S) =

|S[{!}|

X

Y

i=1

!2⌦\S

=

|S[{!}| (AS[{!} )

X

|S[{!}| (AS[{!} )

!2⌦\S

X

(

!2⌦\S

|S|
Y

j (AS )

j=1

X

!2⌦\S

=

i (AS[{!} )
|S|
Y

i (AS[{!} )

i=1
|S|
Y

j (AS )

j=1

i (AS )

i=1

|S[{!}| (AS[{!} )

|S|
Y

1)

|S|
Y

j (AS )

(Cauchy interlacing inequality)

j=1
|S|
Y

(28)

i (AS ).

i=1

For the denonimator, it holds,
F (⌦ [ S)

F (S) =

|⌦[S|

Y

i (A⌦[S )

|⌦\S|

@

Y

j=1

Y

j (A⌦[S )

j=1

i=|⌦\S|

0

|⌦\S|

j (AS[⌦ )

1

1A

|S|
Y

i (AS )

i=1
|S|
Y

i=1

i (AS )

(Cauchy interlacing inequality).

(29)

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

Combining (28) and (29) gives,
P

!2⌦\S

F ({!} [ S)

F (⌦ [ S)

P

F (S)

F (S)
=

P

Q|S|
1) i=1 i (AS )
⌘Q
|S|
1
i=1 i (AS )

!2⌦\S ( |S[{!}| (AS[{!} )

⇣Q

|⌦\S|
j=1

j (AS[⌦ )

!2⌦\S ( |S[{!}| (AS[{!} )

K(
QK

⇣Q

|⌦\S|
j=1

1)

n

1

j

j=1

⌘
1

j (AS[⌦ )

,

1)

where the last inequality comes from that |⌦ \ S|  K.
D.3. LP with Combinatorial Constraints

D.3.1. T WO EXAMPLES WHERE F (S) IS NON - SUBMODULAR
1), Considering the following LP:
max 4x1 +
s.t. 2x1 +

x2 +
x2
x2 +

4x3
2x3

x 1 , x2 , x3

(30)

2
2

0.

For this LP, one can easily see that F ({1, 2}) = 4, F ({2}) = 2, F ({1, 2, 3}) = 8, F ({2, 3}) = 4, thus F ({1, 2})
F ({2}) < F ({1, 2, 3}) F ({2, 3}), which shows F is non-submodular.
2), Considering the following LP:
max 10x1 +
s.t. x1 +
2x1 +
2x2 +

12x2 +
2x2 +
x2 +
2x2 +

12x3
2x3
2x3
x3

 20
 20
 20

x 1 , x2 , x 3

(31)

0.

For this LP, one can see that F ({1, 2}) = 120, F ({2}) = 120, F ({1, 2, 3}) = 136, F ({2, 3}) = 120, thus F ({1, 2})
F ({2}) < F ({1, 2, 3}) F ({2, 3}). But this one has degenerate basic feasible solutions.
D.3.2. P ROVING P ROPOSITION 3
To prove Proposition 3, we first need to present the setup. The LP corresponding to F (S) is,
(LPS )

max
s.t.

hdS , xS i
A S xS  b
xS

(32)

0.

m⇥|S|

where the columns of AS 2 R+
are the columns of A indexed by the set S. xS (respectively, dS ) is the subvector of
x (respectively, d) indexed by S. To apply the optimality condition of a LP in the standard form, let us change (LPS ) to
be the following standard LP by introducing the slack variable ⇠ 2 Rm ,
(LPS⇤ )

min
s.t.

hcS , xS i
A S xS + I m ⇠ = b
xS

where cS :=

0, ⇠

0.

> >
dS . Let us denote Ā := [AS , Im ] 2 Rm⇥(|S|+m) , x̄ := [x>
S,⇠ ] .

(33)

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

Let (x(S) , ⇠ (S) ) denote the optimal solution of (LPS⇤ ). The corresponding basis of of (LPS⇤ ) is B (S) , which is a subset of
V [ {⇠1 , · · · , ⇠m }, and |B (S) | = m.
According to Bertsimas & Tsitsiklis (1997, Chapter 3.1), the optimality condition for (LPS⇤ ) is: Given a basic feasible
1
solution (x, ⇠) with the basis as B, the reduced cost is c̄j = cj c>
B ĀB Ā·j . 1) If (x, ⇠) is optimal and non-degenerate,
then c̄j 0, 8j; 2) If c̄j 0, 8j, then (x, ⇠) is optimal.
Proof of Proposition 3. First of all, let us detail the non-degenerancy assumption.
Non-degenerancy assumption: The basic feasible solutions of the correpsonding LP in standard form (LPS⇤ ) is nondegenerate 8S ✓ V.
a) It is easy to see that F (;) = 0, and F (S) is nondecreasing.
P

⇢! (S)

b) For the submodularity ratio, we want to lower bound !2⌦\S
. There could be in total four situations:
⇢⌦ (S)
P
1) !2⌦\S ⇢! (S) = 0 but ⇢⌦ (S) > 0. We will prove that this situation cannot happen, or in the other words,
P
F (S) = 0 implies that F (⌦ [ S) F (S) = 0 as well.
!2⌦\S F ({!} [ S)

First of all, since F (S) is nondecreasing, so F ({!} [ S) F (S) = 0, 8!. We know that (x(S) , ⇠ (S) ) is the optimal
⇤
solution of (LPS⇤ ), and (x(S) , ⇠ (S) ) is a basic feasible solution of (LPS[{!}
), so (x(S) , ⇠ (S) ) is also the optimal solution
⇤
⇤
of (LPS[{!} ). Since (LPS[{!} ) is non-degenerate, according to the optimality condition, the reduced cost of x! : c̄! must
be greater than or equal zero.
⇤
Now we know that c̄!
0, 8! 2 ⌦\S, and (x(S) , ⇠ (S) ) is a basic feasible solution of (LPS[⌦
) as well, again using the
(S) (S)
⇤
optimality condition, we know that (x , ⇠ ) is optimal for (LPS[{⌦} ). So F (⌦ [ S) F (S) = 0.
P
2) !2⌦\S ⇢! (S) = 0 and ⇢⌦ (S) = 0. The submodularity ratio is 1 in this situation.
P
3) !2⌦\S ⇢! (S) > 0 and ⇢⌦ (S) = 0. This can be ignored since we want a lower bound.
P
4) !2⌦\S ⇢! (S) > 0 and ⇢⌦ (S) > 0. This situation gives the lower bound:

P

!2⌦\S

⇢! (S)

⇢⌦ (S)

max!2⌦\S ⇢! (S)
F (V)
minS✓V,!2V\S,⇢! (S)>0 ⇢! (S)
F (V)
=: 0 > 0.

E. Details about SDP Formulation of Bayesian A-optimality Objective
The SDP formulation used in this paper is consistent with that from Boyd & Vandenberghe (2004, Chapter 7.5) and Krause
et al. (2008). To make this work self-contained, we present the details here.
Firstly, maximizing the Bayesian A-optimality objective is equivalent to,
min

S✓V,|S|K

tr((⇤ +

2

XS X>
S)

1

)

(34)

By introducing binary variables mj , j 2 [n], (34) is equivalent to,
min tr((⇤ +

2

n
X

mj xj x>
j )

1

)

j=1

s.t. mj 2 {0, 1}, j 2 [n], m1 + · · · + mn  K

(35)

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

A proper relaxation is (relaxing the variables

j

= mj /K, j 2 [n]),
2

min tr((⇤ +

n
X

>
1
)
j xj xj )

(36)

j=1

s.t.

2 Rn+ , 1> = 1.

According to the Schur complement lemma, the relaxed formulation (36) is equivalent to the following SDP problem,
min 1> u

Pn
⇤ + 2 j=1
s.t.
e>
k
u2Rd

>
j xj xj

ek
⌫ 0,
uk

k = 1, · · · , d

(SDP)

2 Rn+ , 1> = 1,

where ek 2 Rd is the k th standard basis vector. According to Krause et al. (2008), after solving the (SDP) problem we sort
the entries of in descending order, and select the largest K coordinates as the indices of the K elements to be selected.

F. Proofs and Details in Related Work (Section 6)
Remark 4. For a set function F (·): a) Its submodularity ratio is lower-bounded away from 0 and its curvature ↵ is
upper-bounded away from 1 does not imply that it is weakly submodular; b) F (·) is weakly submodular does not imply
that its submodularity ratio is lower-bounded away from 0 and its curvature ↵ is upper-bounded away from 1.
Proof of Remark 4.
For argument a): Let F (S) := |S|4 , S ✓ V, which is a supermodular function, so the curvature is 0 (upper-bounded away
from 1). The submodualrity ratio can be lower bounded by n 3 . But it is not weakly submodular according to Proposition
3.11 in Borodin et al. (2014).
For argument b): Let us take a minimum cardinality function with k = 2, i.e., F (S) = B > 0 iff. |S|
2, otherwise
F (S) = 0. According to Proposition 3.5 in Borodin et al. (2014), it is weakly submodular, but it is easy to see that its
submodualrity ratio is 0.
More on submodularity index. It is defined as (equivalent to that in Zhou & Spanos (2016)):
⇣ X
⌘
min min
⇢! (S) ⇢⌦ (S) .
⌦,S✓V |⌦\S|K

!2⌦\S

G. More Applications
G.1. Subset Selection Using the R2 Objective
Subset selection aims to estimate a predictor variable Z using linear regression on a small subset from the set of observation
variables V = {X1 , ..., Xn }. Let C to be the covariance matrix among the observation variables {X1 , ..., Xn }. We use
b to denote the covariances between Z and the Xi , with entries bi = Cov(Z, Xi ). Assuming there are m observations,
let us arrange the data of all the observation variables to be a design matrix X 2 Rm⇥n , with each column representing
the observations of one variable. Given aPbudget parameter K, subset selection tries to find a set S ✓ V of at most
K elements, and a linear predictor Z 0 = i2S ↵i Xi = X·S ↵S , in order to maximize the squared multiple corrleation
E[(Z Z ) ]
, it measures the fraction of variance of Z explained by variables in S. Assume Z is normalized
RZ,S = Var(Z) Var(Z)
to have variance 1, and it is well-known that the optimal regression coefficients are ↵S = (CS ) 1 bS , so the R2 objective
can be formulated as,
0 2

2
F (S) := RZ,S
= b>
S (CS )

1

bS , S ✓ V.

(37)

Das & Kempe (2011) show that the submodularity ratio of F in (37) can be lower bounded by min (C), which is the
smallest eigenvalue of C. The theoretical results in this work suggests that the approximation guarantees for maximizing
F in (37) can be further improved by analyzing the curvature parameters. The experimental results in Appendix H.2
demonstrates that it is promising to upper bound the curvature parameters of (37) (possibly with regular assumptions) .

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

G.2. Sparse Modeling with Strongly Convex Loss Functions
Sparse modeling aims to build a model with a small subset of at most K features, out of in total n features. Let f (x) :
Rn 7! R to be the loss function, the corresponding objective is,
min f (x) s.t. |supp(x)|  K.
Assume f (x) is m-strongly convex and has Lipschitz continuous gradient with parameter L, which is equilavent to say
that g(x) := f (x) is m-strongly concave and has L-Lipschitz continuous gradient. Then for all x, y 2 dom(f ) it holds,
L
m
ky xk2  g(y) + g(x) + hrg(x), y xi  ky xk2 .
2
2
In solving this problem, the G REEDY algorithm maximizes the corresponding auxiliary set function,
F (S) :=

max

supp(x)✓S

(38)

(39)

g(x), S ✓ [n]

Elenberg et al. (2016) analyzed the approximation guarantees of G REEDY by bounding the submodularity ratio of F (S).
Specifically,
Lemma 5 (Paraphrasing Theorem 1 in Elenberg et al. (2016)). The submodularity ratio of F (S) in (39) is lower bounded
by m
L.
By further bounding the curvature parameters of the auxiliary set function in (39), one can get improved approximation
guarantees according to our theoretical findings.
G.3. Optimal Budget Allocation with Combinatorial Constraints
Optimal budget allocation (Soma et al., 2014) is a special case of the influence maximization problem, it aims to distribute
the budget (e.g., space of an inline advertisement, or time for a TV advertisement) among the customers, and to maximize
the expected influence on the potential customers. A concrete application is for the search marketing advertiser bidding
task, in which vendors bid for the right to appear alongside the results of different search keywords. Let xis 2 R+ to be
the volume of advertising space allocated to the advertiser i to show his ad alongside query keyword s. Bian et al. (2017)
present continuous DR-submodular objectives to model this problem with continuous assignments.
The search engine company (e.g., Google and Yahoo) needs to distribute the budget (ad space) to all vendors to maximize
their influence on the customers, while respecting various continuous and combinatorial constraints. For the continuous
constraints, for instance, each vendor has a specified budget limit for advertising, and the ad space associated with each
search keyword can not be too large. These continuous constraints can be formulated as a convex set P. For combinatorial
constraints, each vendor needs to obey the Internet regulations of sensitive search keywords in his country, so the search
engine company can only choose a subset of “legal” keywords for a specific vendor. The combinatorial constraints can be
arranged as a matroid M = (V, I). Hence the problem in general can be formulated as,
max

x2P and supp(x)2I

g(x),

where g(x) is the total influence modeled by a DR-submodular function. For one of its possible forms, one can refer to
Bian et al. (2017). The G REEDY algorithm solves this problem by maximizing the following auxiliary set function F (S)
while respecting the combinatorial constraints,
max F (S), where F (S) :=
S2I

max

supp(x)✓S,x2P

g(x).

(40)

By studying the submodularity ratio and curvature parameters of F (S) in (40), one could obtain theoretical guarantees of
the G REEDY algorithm according to Theorem 1 in this work.

H. More Experimental Results
H.1. Bayesian A-optimality Experiments
We put the results on a randomly generated dataset, to illustrate what does the proved bounds looks like. In the synthetic
experiments we generate random observations from a multivariate Gaussian distribution with correlation 0.5. Fig. 8 shows

Guarantees for Greedy Maximization of Non-submodular Functions with Applications

1

1

0.8

0.8

0.6

0.6

2
1.5
OPT
Greedy
SDP

1

0.4

Bounds

2.5

Parameters

A-optimality objective

3

G

0.2

0.5

0.2

G

0
2

4

6

8

10

0.4

total
G

1/

G

(1-exp(-

G G

))

G G

)/K) K ]

[1-((K-

0

12

2

4

6

K

8

10

12

2

K

(a) A-optimality objective

1/

4

6

8

10

12

K

(b) Greedy submodularity ratio and curvature

(c) Approximation bounds

Figure 8: Function value, parameters and approximation bounds of experimental design on synthetic data. Correlation: 0.5
the results (function value, parameters and approximation bounds) for one randomly generated data set with d = 6 features
and n = 12 observations. Specifically, Fig. 8c traces the two approximation bounds from Theorem1 (and Lemma 3): one
⇣
⌘K
K ↵
curve shows the constant-factor bound ↵ 1 (1 e ↵ ) and the other the K-dependent bound ↵1 1
. We
K
observe that both bounds give reasonable predictions of the performance of G REEDY.
H.2. Subset Selection Using the R2 Objective

0.9
R2 objective

2

R objective

0.8
0.6
0.4

OPT
Greedy

2

4

6

0.8
0.7
0.6

OPT
Greedy

0.5
8

10

2

4

K
total

G

G

total

1.1

1.1

1

1

Parameters

Parameters

6

8

10

K

0.9
0.8
0.7

G

G

0.9
0.8
0.7

2

4

6

K

(a) Correlation: 0.05

8

10

2

4

6

K

(b) Correlation: 0.5

Figure 9: Results for R2 objective on synthetic data.
for G REEDY.

8

10

For details on this task please refer to Das & Kempe (2011) or Appendix G.1. We did synthetic experiments to illustrate that our theory
can give a refined explanation of the
performance of G REEDY. We generate random observations from a multivariate standard Gaussian distribution
with different correlations. We used
n = 10 features and m = 100 observations. The target regression coefficients ↵ 2 Rn were generated
as a random vector with uniformly
distributed entries in [0, 1]. Standard
Gaussian noise was added to generate
the observation of predictor variable
Z. The results are shown in Fig. 9,
with first column showing the results
with correlation as 0.05, the second
column with correlation as 0.5. One
can see that the mean of the greedy
curvature and submodularity ratio take
values in (0, 1), which can be used to
give improved approximation bounds

