Compressed Sensing using Generative Models

8. Appendix A
Lemma 8.1. Given S ✓ Rn , y 2 Rm , A 2 Rm⇥n , and
, , ✏1 , ✏2 > 0, if matrix A satisfies the S-REC(S, , ),
then for any two x1 , x2 2 S, such that kAx1 yk  ✏1
and kAx2 yk  ✏2 , we have
kx1

x2 k 

✏1 + ✏2 +

.

is sufficient to ensure that

Proof.
kx1

kAxk2
Proof. Observe that for any x 2 Rn ,
is
kxk2
✓
◆
1 1
subgamma p ,
. Thus, for any f > 0,
m m
✓r
◆
4
2
2
2 2
2
✏ 2+
log
max
log , log
m
f
m
f m
f

P (kAxk
x2 k 
=



1
1
1

(kAx1

Ax2 k + ) ,

(k(Ax1

y)

(k(Ax1

y)k + k(Ax2

✏1 + ✏2 +

(Ax2

y)k + ) ,
y)k + ) ,

.

8.1. Proof of Lemma 4.1
Definition 2. A random variable X is said to be
subgamma( , B) if 8✏ 0, we have
⇣ 2
⌘
2
P (|X E[X]| ✏)  2 max e ✏ /(2 ) , e ✏/(2B) .

then for any x 2 S, if x0 = arg minxb2G(M ) kx x
bk, we
0
⌦(m)
have kA(x x )k = O( ) with probability 1 e
.

Note that for any given point x0 in S, if we try to find its
nearest neighbor of that point in an -net on S, then the
difference between the two is at most the . In words, this
lemma says that even if we consider measurements made
on these points, i.e. a linear projection using a random matrix A, then as long as there are enough measurements, the
difference between measurements is of the same order . If
the point x0 was in the net, then this can be easily achieved
by Johnson-Lindenstrauss Lemma. But to argue that this
is true for all x0 in S, which can be an uncountably large
set, we construct a chain of nets on S. We now present the
formal proof.

p

1 + ✏kxk  f.

Now, let M = M0 ✓ M1 ✓ M2 , · · · ✓ Ml be a chain
of epsilon nets of B k (r) such that Mi is a i /L-net and
i
i = 0 /2 , with 0 = . We know that there exist nets
such that
✓
◆
✓
◆
4Lr
4Lr
log |Mi |  k log
 ik + k log
.
i

0

Let Ni = G(Mi ). Then due to Lipschitzness of G, Ni ’s
form a chain of epsilon nets such that Ni is a i -net of S =
G(B k (r)), with |Ni | = |Mi |.
For i 2 {0, 1, 2 · · · , l
Ti = {xi+1

1}, let

xi | xi+1 2 Ni+1 , xi 2 Ni }.

Thus,
|Ti |  |Ni+1 ||Ni |.

Lemma 8.2. Let G : Rk ! Rn be an L-Lipschitz function. Let B k (r) be the L2 -ball in Rk with radius r,
k
S = G(B k (r)),
✓ and◆M be a /L-net on B (r) such that
4Lr
|M |  k log
. Let A be a Rm⇥n random matrix
with IID Gaussian entries with zero mean and variance
1/m. If
✓
◆
Lr
m = ⌦ k log
,

(1 + ✏)kxk)  P kAxk

=) log |Ti |  log |Ni+1 | + | log |Ni |,
✓
◆
4Lr
 (2i + 1)k + 2k log
,
0
✓
◆
4Lr
 3ik + 2k log
.
0

Now assume m = 3k log

✓

log(fi ) =

4Lr
0

◆

,

(m + 4ik),

and
4
2
log ,
m
fi
4
16ik
=2+
log 2 + 4 +
,
m
m
16ik
= O(1) +
.
m

✏i = 2 +

By choice of fi and ✏i , we have 8i 2 [l

1], 8t 2 Ti ,

P (kAtk > (1 + ✏i )ktk)  fi .

Compressed Sensing using Generative Models

Thus by union bound, we have

to choose x0 = x0 , we get that with probability 1 e

P (kAtk  (1 + ✏i )ktk, 8i, 8t 2 Ti )

l 1
X

1

i=0

|Ti |fi .

x0 )k = kA(x

kA(x



i=0

,

x0 )k,

kA(xi+1

xi )k + kAxf k,

= O( 0 ) + kAkkxf k,

Now,

= O( ).

log(|Ti |fi ) = log(|Ti |) + log(fi ),
✓
◆
4Lr
 k log
ik,

Lemma. Let G : Rk ! Rn be L-Lipschitz. Let

0

=

=)

l 1
X
i=0

m/3

|Ti |fi  e

m/3

e

m/3

 2e

ik.

l 1
X

e

ik

,

i=0

✓

m/3

1
1 e

.

B k (r) = {z | z 2 Rk , kzk  r}

1

◆

be an L2 -norm ball in Rk . For ↵ < 1, if
✓
◆
k
Lr
m=⌦
log
,
↵2

,

then a random matrix A 2 Rm⇥n with IID entries such that
1
Aij ⇠ N 0, m
satisfies the S-REC(G(B k (r)), 1 ↵, )
2
with 1 e ⌦(↵ m) probability.

Observe that for any x 2 S, we can write
x = x0 + (x1
x

l 1
X

⌦(m)

x0 =

l 1
X

(xi+1

x0 ) + (x2

x1 ) . . . (xl

xl

1)

xi ) + xf .

i=0

+ xf . Proof. We construct a -net, N , on B k (r). There exists a
L
net such that
✓
◆
4Lr
log |N |  k log
.

where xi 2 Ni and xf = x xl . We also get kxi+1 xi k 
f
i , and kx k  l due to properties of epsilon-nets.
Since each xi+1
2e m/3 , we have
l 1
X
i=0

kA(xi+1

xi 2 Ti , with probability at least 1

xi )k =

l 1
X

(1 + ✏i )k(xi+1

xi )k,

Since N is a

-cover of B k (r), due to the L-Lipschitz
L
property of G(·), we get that G(N ) is a -cover of
G(B k (r)).
Let T denote the pairwise differences between the elements
in G(N ), i.e.,

i=0



l 1
X

T = {G(z1 )

(1 + ✏i ) i ,

i=0

✓
◆
l 1
X
1
16ik
= 0
O(1)
+
,
2i
m
i=0
◆
l 1✓
16k X i
= O( 0 ) + 0
,
m i=0 2i
= O( 0 ).

p
We know that kAk  2 + n/m with probability at least
1 2e m/2 (Corollary 5.35 (Vershynin,✓2010)).
r By◆setting
n
0
l = log(n), we get that, kAkkxf k  2 +
=
m 2l
O( 0 ) with probability 1 2e m/2 .
Combining these two results, and noting that it is possible

G(z2 ) | z1 , z2 2 N }.

Then,
|T |  |N |2 ,

=) log |T |  2 log |N |,
✓
◆
4Lr
 2k log
.
For any z, z 0 2 B k , 9 z1 , z2 2 N , such that G(z1 ), G(z2 )
are -close to G(z) and G(z 0 ) respectively. Thus, by triangle inequality,
kG(z)

G(z 0 )k  kG(z)

kG(z1 )
kG(z2 )

 kG(z1 )

G(z1 )k+
G(z2 )k+
G(z 0 )k,
G(z2 )k + 2 .

Compressed Sensing using Generative Models

Again by triangle inequality,
kAG(z1 )

AG(z2 )k  kAG(z1 )

AG(z)k+
AG(z 0 )k+

kAG(z)
0

kAG(z )

AG(z2 )k.

Now, by Lemma 8.2, with probability 1
e ⌦(m) ,
0
kAG(z1 ) AG(z)k = O( ), and kAG(z ) AG(z2 )k =
O( ). Thus,
kAG(z1 )

AG(z2 )k  kAG(z)

0

AG(z )k + O( ).

n
By⇥ the Johnson-Lindenstrauss
⇤ Lemma,2for a fixed x 2 R ,
2
2
P kAxk < (1 ↵)kxk < exp( ↵ m). Therefore, we
can union bound over all vectors in T to get

P(kAxk2

(1

↵)kxk2 , 8x 2 T )

1

e

⌦(↵2 m)

.

Since ↵ < 1, and z1 , z2 2 N , G(z1 ) G(z2 ) 2 T , we have
p
(1 ↵)kG(z1 ) G(z2 )k  1 ↵kG(z1 ) G(z2 )k,
 kAG(z1 )

AG(z2 )k.

Combining the three results above we get that with proba2
bility 1 e ⌦(↵ m) ,

hyperplanes H = {h1 , h2 , . . . , hc 1 } ⇢ Rk , and a new
hyperplane hc is added. hc intersects H at (c 1) different
(k 2)-faces given by F = {fj | fj = hj \ hc , 1  j 
(c 1)}. The (k 2)-faces in F partition hc into f (c
1, k 1) different (k 1)-faces. Additionally, each (k 1)face in hc divides an existing k-face into two. Hence the
number of new k-faces introduced by the addition of hc is
f (c 1, k 1). This gives the recursion
f (c, k) = f (c

1, k) + f (c

= f (c
k

1, k) + O(c

1, k
k 1

1),

),

= O(c ).

Lemma. Let G : Rk ! Rn be a d-layer neural network,
where each layer is a linear transformation followed by a
pointwise non-linearity. Suppose there are at most c nodes
per layer, and the non-linearities are piecewise linear with
at most two pieces, and let
✓
◆
1
m=⌦
kd
log
c
↵2

for some ↵ < 1.
Then a random matrix A 2
1
Rm⇥n with IID entries Aij ⇠ N (0, m
) satisfies the
2
0
(1 ↵)kG(z) G(z )k  (1 ↵)kG(z1 ) G(z2 )k + O( ), S-REC(G(Rk ), 1 ↵, 0) with 1 e ⌦(↵ m) probability.
 kAG(z1 ) AG(z2 )k + O( ),
Proof. Consider the first layer of G. Each node in this layer
 kAG(z) AG(z 0 )k + O( ).
can be represented as a hyperplane in Rk , where the points
on the hyperplane are those where the input to the node
Thus, A satisfies S-REC(S, 1 ↵, ) with probability 1
switches from one linear piece to the other. Since there are
2
e ⌦(↵ m) .
at most c nodes in this layer, by Lemma 8.3, the input space
is partitioned by at most c different hyperplanes, into O(ck )
k-faces. Applying this over the d layers of G, we get that
the input space Rk is partitioned into at most ckd sets.
8.2. Proof of Lemma 4.2
Lemma 8.3. Consider c different k 1 dimensional hyperplanes in Rk . Consider the k-dimensional faces (hereafter
called k-faces) generated by the hyperplanes, i.e. the elements in the partition of Rk such that relative to each hyperplane, all points inside a partition are on the same side.
Then, the number of k-faces is O(ck ).
Proof. Proof is by induction, and follows (Matoušek,
2002).
Let f (c, k) denote the number of k faces generated in Rk
by c different (k 1)-dimensional hyperplanes. As a base
case, let k = 1. Then (k 1)-dimensional hyperplanes are
just points on a line. c points partition R into c + 1 pieces.
This gives f (c, 1) = O(c).
Now, assuming that f (c, k 1) = O(ck 1 ) is true, we need
to show f (c, k) = O(ck ). Assume we have (c 1) different

Recall that the non-linearities are piecewise linear, and the
partition boundaries were made precisely at those points
where the non-linearities change from one piece to another.
This means that within each set of the input partition, the
output is a linear function of the inputs. Thus G(Rk ) is a
union of ckd different k-faces in Rn .
We now use an oblivious subspace embedding to bound
the number of measurements required to embed the range
of G(·). For a single k-face S ✓ Rn , a random matrix
1
A 2 Rm⇥n with IID entries such that Aij ⇠ N 0, m
2
satisfies S-REC(S, 1 ↵, 0) with probability 1 e ⌦(↵ m)
if m = ⌦(k/↵2 ).
Since the range of G(·) is a union of ckd different kfaces, we can union bound over all of them, such that
A satisfies the S-REC(G(Rk ), 1 ↵, 0) with probabil2
ity 1 ckd e ⌦(↵ m) . Thus, we get that A satisfies the

Compressed Sensing using Generative Models

S-REC(G(Rk ), 1
if

↵, 0) with probability 1
✓
◆
kd log c
m=⌦
.
↵2

⌦(↵2 m)

e

8.3. Proof of Lemma 4.3
Lemma. Let A 2 Rm⇥n by drawn from a distribution that
(1) satisfies the S-REC(S, , ) with probability 1 p and
(2) has for every fixed x 2 Rn , kAxk  2kxk with probability 1 p. For any x⇤ 2 Rn and noise ⌘, let y = Ax⇤ + ⌘.
Let x
b approximately minimize ky Axk over x 2 S, i.e.,
ky

Ab
xk  min ky
x2S

Axk + ✏.

Then
kb
x

x⇤ k 

✓

4

◆
+ 1 min kx⇤
x2S

with probability 1

1

xk +

(2k⌘k + ✏ + )

2p.

Proof. Let x = arg minx2S kx⇤ xk. Then we have by
Lemma 8.1 and the hypothesis on x
b that
kx

x
bk 



kAx

yk + kAb
x

2kAx

yk + ✏ +

2kA(x

yk +

,

,

x⇤ )k + 2k⌘k + ✏ +

,

as long as A satisfies the S-REC, as happens with probability 1 p. Now, since x and x⇤ are independent of A, by
assumption we also have kA(x x⇤ )k  2kx x⇤ k with
probability 1 p. Therefore
kx⇤

x
bk  kx

x⇤ k +

4kx

x⇤ k + 2k⌘k + ✏ +

Proof. Consider any linear layer with input x, weight matrix W and bias vector b. Thus, f (x) = W x + b. Now for
any two x1 , x2 ,
kf (x1 )

f (x2 )k = kW x1 + b
= kW (x1

 kW kk(x1

W x2 + bk,

x2 )k,

 cwmax k(x1

x2 )k,
x2 )k.

Let fi (·), i 2 [d] denote the function for the i-th layer in G.
Since each layer is a composition of a linear function and
a non-linearity, by Lemma 8.4, have that fi is M cwmax Lipschitz.
Since G = f1 f2 . . . fd , by repeated application
of Lemma 8.4, we get that G is L-Lipschitz with L =
d
(M cwmax ) .

9. Appendix B
9.1. Noise tolerance
To understand the noise tolerance of our algorithm, we do
the following experiment: First we fix the number of measurements so that Lasso does as well as our algorithm.
From Fig. 1a, and Fig. 1b we see that this point is at
m = 500 for MNIST and m = 2500 for celebA. Now, we
look at the performance as the noise level increases. Hyperparameters are kept fixed as we change the noise level
for both Lasso and for our algorithm.
In Fig. 8a, we show the results on the MNIST dataset. In
Fig. 8a, we show the results on celebA dataset. We observe
that our algorithm has more noise tolerance than Lasso.
9.2. Scaling with latent dimension

as desired.

8.4. Lipschitzness of Neural Networks
Lemma 8.4. Consider any two functions f and g. If f is
Lf -Lipschitz and g is Lg -Lipschitz, then their composition
f g is Lf Lg -Lipschitz.
Proof. For any two x1 , x2 ,
kf (g(x1 ))

Lemma 8.5. If G is a d-layer neural network with at most
c nodes per layer, all weights  wmax in absolute value,
and M -Lipschitz non-linearity after each layer, then G(·)
d
is L-Lipschitz with L = (M cwmax ) .

f (g(x2 ))k  Lf kg(x1 )
 Lf Lg kx1

g(x2 )k,
x2 k.

In the experiments in Sec. 6.3, we saw that the representation error was a major component of the total error, and
thus a better generative model might be helpful. Recall that
a generative model is a function G : R ! Rn . Thus, one
way to make the generative model more powerful is to increase the size of the latent space k.
In this section we present some experiments that investigate how the representation error scales as we use different
values of k. We keep the rest of the architecture and hyperparameters fixed as we change k. For comparison, we also
plot the representation error of a k-sparse wavelet as we
change k. Figure 9 shows the plots for the celebA dataset.
We observe that for small values of k, our method is far

VA(, m 500

0.20
0.15
0.10
0.05
0.00 -2
10

10 -1
10 1
10 0
Standard devLatLRn Rf nRLVe

10 2

LDssR (DCT), m 2500

0.40

LDssR (WDveleW), m 2500

0.35

DCGA1, m 2500

0.30
0.25
0.20
0.15
0.10
0.05
0.00 -2
10

10 -1
10 1
10 0
SWDnGDrG GevLDWLRn Rf nRLse

(a) Results on MNIST.

10 2

(b) Results on celebA.
q

E[k⌘k2 ]). The vertical bars

0.12

LaVVR
VA(
VA(+5eg
)Lxed A
Learned A

0.10
0.08
0.06
0.04

50

0.00

25

0.02
10

5eFRnVtruFtLRn errRr (per pLxel)

Figure 8. Noise tolerance. We show a plot of per pixel reconstruction error as we vary the noise level (
indicate 95% confidence intervals.

750

VA(, m 100

300
400
500

0.25

0.45

200

LaVVR, m 500

100

0.30

5ecRnsWrucWLRn errRr (Ser SLxel)

5ecRnVtructLRn errRr (Ser SLxel)

Compressed Sensing using Generative Models

1umEer Rf meaVurementV

Figure 9. Results on celebA. We show per pixel representation error vs the latent dimension of the generative model. The vertical
bars indicate 95% confidence intervals.

Figure 10. Results for end to end model on MNIST. We show per
pixel reconstruction error vs number of measurements. ‘Fixed
A’ and ‘Learned A’ are two end to end models. The end to end
models get noiseless measurements, while the other models get
noisy ones. The vertical bars indicate 95% confidence intervals.

9.3. Other models
9.3.1. E ND TO END TRAINING ON MNIST
superior to k-sparse wavelet. This suggests that neural network based generative models make effective use of the
latent space by constructing excellent representations. We
see that as we increase k, the error starts to plateau for our
method while it goes to zero for k-sparse wavelet model.
This suggests that beyond a point, some other factor in our
model, such as the architecture of the DCGAN, starts to
become the bottleneck. It is possible that the results for our
method can be improved by more careful hyperparameter
tuning for each k.

Instead of using a generative model to reconstruct the image, another approach is to learn from scratch a mapping
that takes the measurements and outputs the original image. A major drawback of this approach is that it necessitates learning a new network if get a different set of measurements.
If we use a random matrix for every new image, the input
to the network is essentially noise, and the network does
not learn well. Instead we use a fixed measurement matrix.
We explore two approaches. First is to randomly sample

Learned30

Learned20

Learned10

FLxed30

FLxed20

FLxed10

2rLgLnaO

Compressed Sensing using Generative Models

Figure 11. MNIST End to end learned model. Top row are original images. The next three are recovered by model with fixed random
A, with 10, 20 and 30 measurements. Bottom three rows are with learned A and 10, 20 and 30 measurements.

and fix the measurement matrix and learn the rest of the
mapping. In the second approach, we jointly optimize the
measurement matrix as well.
We do this for 10, 20 and 30 measurements for the MNIST
dataset. We did not use additive noise. The reconstruction
errors are shown in Fig. 10. The reconstructed images can
be seen in Fig. 11.
9.4. More results
Here, we show more results on the reconstruction task,
with varying number of measurements on both MNIST and
celebA. Fig. 12 shows reconstructions on MNIST with 25,
100 and 400 measurements. Fig. 13, Fig. 14 and Fig. 15
show results on celebA dataset.

Compressed Sensing using Generative Models

(a) 25 measurements

(b) 100 measurements

(c) 400 measurements
Figure 12. Reconstruction on MNIST. In each image, top row is ground truth, middle row is Lasso, bottom row is our algorithm.

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

Compressed Sensing using Generative Models

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

(a) 50 measurements

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

(b) 100 measurements

(c) 200 measurements
Figure 13. Reconstruction on celebA. In each image, top row is ground truth, subsequent two rows show reconstructions by Lasso (DCT)
and Lasso (Wavelet) respectively. The bottom row is the reconstruction by our algorithm.

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

Compressed Sensing using Generative Models

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

(a) 500 measurements

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

(b) 1000 measurements

(c) 2500 measurements
Figure 14. Reconstruction on celebA. In each image, top row is ground truth, subsequent two rows show reconstructions by Lasso (DCT)
and Lasso (Wavelet) respectively. The bottom row is the reconstruction by our algorithm.

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

Compressed Sensing using Generative Models

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

(a) 5000 measurements

DCGAN

LDsso (WDveOeW)

LDsso (DCT)

OrLgLnDO

(b) 7500 measurements

(c) 10000 measurements
Figure 15. Reconstruction on celebA. In each image, top row is ground truth, subsequent two rows show reconstructions by Lasso (DCT)
and Lasso (Wavelet) respectively. The bottom row is the reconstruction by our algorithm.

