Online Learning to Rank in Stochastic Click Models

A. Notation
Symbol
↵(d)
↵max
A
P↵
A
bmax
Bb,`
ct (k)
cb,` (d)
ĉb,` (d)
c̄b,` (d)
D
T

˜`
Ib
K
len(b)
L
Lb,` (d)
n`
nb,`
⇧K (D)
r(R, A, X)
r(R, ↵, )
R = (d1 , . . . , dK )
R⇤ = (1, . . . , K)
R(R, A, X)
R(T )
T
Ub,` (d)
(R, k)
⇤
(k)
X
P

Definition
Attraction probability of item d
Highest attraction probability, ↵(1)
Binary attraction vector, where A(d) is the attraction indicator of item d
Distribution over binary attraction vectors
Set of active batches
Index of the last created batch
Items in stage ` of batch b
Indicator of the click on position k at time t
Number of observed clicks on item d in stage ` of batch b
Estimated probability of clicking on item d in stage ` of batch b
Probability of clicking on item d in stage ` of batch b, E [ĉb,` (d)]
Ground set of items [L] such that ↵(1) . . . ↵(L)
log T + 3 log log T
2 `
Interval of positions in batch b
Number of positions to display items
Number of positions to display items in batch b
Number of items
Lower confidence bound of item d, in stage ` of batch b
Number of times that each item is observed in stage `
Number of observations of item d in stage ` of batch b
Set of all K-tuples with distinct elements from D
Reward of list R, for attraction and examination indicators A and X
Expected reward of list R
List of K items, where dk is the k-th item in R
Optimal list of K items
Regret of list R, for attraction and examination indicators A and X
Expected cumulative regret in T steps
Horizon of the experiment
Upper confidence bound of item d, in stage ` of batch b
Examination probability of position k in list R
Examination probability of position k in the optimal list R⇤
Binary examination matrix, where X(R, k) is the examination indicator of position k in list R
Distribution over binary examination matrices

Online Learning to Rank in Stochastic Click Models

B. Proof of Theorem 1
Let Rb,` be the stochastic regret associated with stage ` of batch b. Then the expected T -step regret of MergeRank can be
decomposed as
" 2K T 1
#
XX
R(T )  E
Rb,`
b=1 `=0

because the maximum number of batches is 2K. Let
¯ b,` (d) =

c̄b,` (d)
↵(d)

(12)

be the average examination probability of item d in stage ` of batch b. Let
⇢
Eb,` = Event 1: 8d 2 Bb,` : c̄b,` (d) 2 [Lb,` (d), Ub,` (d)] ,

Event 2: 8Ib 2 [K]2 , d 2 Bb,` , d⇤ 2 Bb,` \ [K] s.t. = ↵(d⇤ ) ↵(d) > 0 :
16K
n`
log T =) ĉb,` (d)  ¯ b,` (d)[↵(d) + /4] ,
⇤ (I (1))(1
↵max ) 2
b
Event 3: 8Ib 2 [K]2 , d 2 Bb,` , d⇤ 2 Bb,` \ [K] s.t.
n`

⇤ (I

16K
↵max )
b (1))(1

2

= ↵(d⇤ )

log T =) ĉb,` (d⇤ )

↵(d) > 0 :

¯ b,` (d⇤ )[↵(d⇤ )

/4]

be the “good event” in stage ` of batch b, where c̄b,` (d) is the probability of clicking on item d in stage ` of batch b, which
is defined in (8); ĉb,` (d) is its estimate, which is defined in (7); and both ⇤ and ↵max are defined in Section 5.3. Let Eb,`
be the complement of event Eb,` . Let E be the “good event” that all events Eb,` happen; and E be its complement, the “bad
event” that at least one event Eb,` does not happen. Then the expected T -step regret can be bounded from above as
R(T )  E

" 2K T 1
XX
b=1 `=0

#

Rb,` 1{E} + T P (E) 

2K
X
b=1

E

"T 1
X
`=0

#

Rb,` 1{E} + 4KL(3e + K) ,

where the second inequality is from Lemma 2. Now we apply Lemma 7 to each batch b and get that
"T 1
#
2K
X
X
192K 3 L
E
Rb,` 1{E} 
log T .
(1 ↵max ) min
b=1

This concludes our proof.

`=0

Online Learning to Rank in Stochastic Click Models

C. Upper Bound on the Probability of Bad Event E
Lemma 2. Let E be defined as in the proof of Theorem 1 and T
P (E) 

5. Then

4KL(3e + K)
.
T

Proof. By the union bound,
P (E) 

2K T
X
X1

P (Eb,` ) .

b=1 `=0

Now we bound the probability of each event in Eb,` and then sum them up.
Event 1
The probability that event 1 in Eb,` does not happen is bounded as follows. Fix Ib and Bb,` . For any d 2 Bb,` ,
P (c̄b,` (d) 2
/ [Lb,` (d), Ub,` (d)])  P (c̄b,` (d) < Lb,` (d)) + P (c̄b,` (d) > Ub,` (d))
⌃
⌥
2e log(T log3 T ) log n`

T log3 T
⌃ 2
⌥
2e log T + log(log3 T ) log T

T log3 T
⌃
⌥
2e 2 log2 T

T log3 T
6e

,
T log T
where the second inequality is by Theorem 10 of Garivier & Cappe (2011), the third
⌃ inequality
⌥ is from T n` , the fourth
inequality is from log(log3 T )  log T for T 5, and the last inequality is from 2 log2 T  3 log2 T for T 3. By the
union bound,
P (9d 2 Bb,` s.t. c̄b,` (d) 2
/ [Lb,` (d), Ub,` (d)]) 

6eL
T log T

for any Bb,` . Finally, since the above inequality holds for any Bb,` , the probability that event 1 in Eb,` does not happen is
bounded as above.
Event 2
The probability that event 2 in Eb,` does not happen is bounded as follows. Fix Ib and Bb,` , and let k = Ib (1). If the event
does not happen for items d and d⇤ , then it must be true that
16K
↵max )

n`

⇤ (k)(1

2

log T ,

ĉb,` (d) > ¯ b,` (d)[↵(d) +

/4] .

From the definition of the average examination probability in (12) and a variant of Hoeffding’s inequality in Lemma 8, we
have that
P (ĉb,` (d) > ¯ b,` (d)[↵(d) +
From Lemma 9, ¯ b,` (d)

⇤

/4])  exp [ n` DKL( ¯ b,` (d)[↵(d) +

/4] k c̄b,` (d))] .

(k)/K (Lemma 3), and Pinsker’s inequality, we have that

exp [ n` DKL( ¯ b,` (d)[↵(d) +

/4] k c̄b,` (d))]  exp [ n` ¯ b,` (d)(1 ↵max )DKL(↵(d) +

⇤
(k)(1 ↵max ) 2
 exp n`
.
8K

/4 k ↵(d))]

Online Learning to Rank in Stochastic Click Models

From our assumption on n` , we conclude that

⇤
(k)(1
exp n`

↵max )
8K

2

 exp[ 2 log T ] =

1
.
T2

Finally, we chain all above inequalities and get that event 2 in Eb,` does not happen for any fixed Ib , Bb,` , d, and d⇤ with
probability of at most T 2 . Since the maximum numbers of items d and d⇤ are L and K, respectively, the event does not
happen for any fixed Ib and Bb,` with probability of at most KLT 2 . In turn, the probability that event 2 in Eb,` does not
happen is bounded by KLT 2 .
Event 3
This bound is analogous to that of event 2.
Total probability
The maximum number of stages in any batch in BatchRank is log T and the maximum number of batches is 2K. Hence,
by the union bound,
✓
◆
6eL
KL KL
4KL(3e + K)
P (E) 
+ 2 + 2 (2K log T ) 
.
T log T
T
T
T
This concludes our proof.

Online Learning to Rank in Stochastic Click Models

D. Upper Bound on the Regret in Individual Batches
Lemma 3. For any batch b, positions Ib , stage `, set Bb,` , and item d 2 Bb,` ,
⇤

(k)
 ¯ b,` (d) ,
K

where k = Ib (1) is the highest position in batch b.
Proof. The proof follows from two observations. First, by Assumption 6, ⇤ (k) is the lowest examination probability of
position k. Second, by the design of DisplayBatch, item d is placed at position k with probability of at least 1/K.
Lemma 4. Let event E happen and T
5. For any batch b, positions Ib , set Bb,0 , item d 2 Bb,0 , and item d⇤ 2 Bb,0 \
⇤
[K] such that = ↵(d ) ↵(d) > 0, let k = Ib (1) be the highest position in batch b and ` be the first stage where
r
⇤ (k)(1
↵max )
˜` <
.
K
Then Ub,` (d) < Lb,` (d⇤ ).
Proof. From the definition of n` in BatchRank and our assumption on ˜ ` ,
16
log T >
˜2

n`
Let µ = ¯ b,` (d) and suppose that Ub,` (d)
and event 2 in Eb,` ,
DKL(ĉb,` (d) k Ub,` (d))
From Lemma 9, µ

⇤

16K
↵max )

µ[↵(d) +

/2] holds. Then from this assumption, the definition of Ub,` (d),
/2]) 1{ĉb,` (d)  µ[↵(d) +

DKL(ĉb,` (d) k µ[↵(d) +
DKL(µ[↵(d) +

2

/4] k µ[↵(d) +

/4] k µ[↵(d) +

From the definition of Ub,` (d), T
n` =

/2])

µ(1
⇤

(k)(1

↵max )
8K

2

5, and above inequalities,
16K log T
↵max )

⇤ (k)(1

⇤

⇤

DKL(ĉb,` (d⇤ ) k Lb,` (d⇤ ))

⇤

DKL(ĉb,` (d⇤ ) k µ⇤ [↵(d⇤ )
⇤

⇤

DKL(µ [↵(d )

.

/2] holds. Then from this assumption,

⇤

/2]) 1{ĉb,` (d⇤ )
⇤

2

/2] holds.

On the other hand, let µ = ¯ b,` (d ) and suppose that Lb,` (d )  µ [↵(d )
the definition of Lb,` (d⇤ ), and event 3 in Eb,` ,

⇤

/2)

.

log T + 3 log log T
2 log T


DKL(ĉb,` (d) k Ub,` (d))
DKL(ĉb,` (d) k Ub,` (d))

⇤

/4 k ↵(d) +

↵max )DKL(↵(d) +

This contradicts to (13), and therefore it must be true that Ub,` (d) < µ[↵(d) +

⇤

/4] k µ [↵(d )

µ⇤ [↵(d⇤ )

/2]}

/2]) .

(k)/K (Lemma 3), and Pinsker’s inequality, we have that

DKL(µ⇤ [↵(d⇤ )

/4] k µ⇤ [↵(d⇤ )

From the definition of Lb,` (d⇤ ), T
n` =

/2]}

/2]) .

(k)/K (Lemma 3), and Pinsker’s inequality, we have that

DKL(µ[↵(d) +

From Lemma 9, µ⇤

(13)

log T .

`

⇤ (k)(1

/2])

µ⇤ (1
⇤

↵max )DKL(↵(d⇤ )

(k)(1

↵max )
8K

/4 k ↵(d⇤ )

2

/2)

.

5, and above inequalities,

log T + 3 log log T
2 log T


⇤
DKL(ĉb,` (d) k Lb,` (d ))
DKL(ĉb,` (d⇤ ) k Lb,` (d⇤ ))

16K log T
↵max )

⇤ (k)(1

2

.

Online Learning to Rank in Stochastic Click Models

This contradicts to (13), and therefore it must be true that Lb,` (d⇤ ) > µ⇤ [↵(d⇤ )

/2] holds.

Finally, based on inequality (11),
µ⇤ =

c̄b,` (d⇤ )
↵(d⇤ )

c̄b,` (d)
= µ,
↵(d)

and item d is guaranteed to be eliminated by the end of stage ` because
Ub,` (d) < µ[↵(d) +

/2]

µ ↵(d⇤ ) µ↵(d)
2
⇤
⇤
µ
↵(d
) µ↵(d)
= µ⇤ ↵(d⇤ )
2
 µ⇤ [↵(d⇤ )
/2]
 µ↵(d) +

⇤

< Lb,` (d⇤ ) .
This concludes our proof.

Lemma 5. Let event E happen and T
5. For any batch b, positions Ib where Ib (2) = K, set Bb,0 , and item d 2 Bb,0
such that d > K, let k = Ib (1) be the highest position in batch b and ` be the first stage where
r
⇤ (k)(1
↵max )
˜` <
K
for

= ↵(K)

↵(d). Then item d is eliminated by the end of stage `.

Proof. Let B + = {k, . . . , K}. Now note that ↵(d⇤ ) ↵(d)
for any d⇤ 2 B + . By Lemma 4, Lb,` (d⇤ ) > Ub,` (d) for
⇤
+
any d 2 B ; and therefore item d is eliminated by the end of stage `.

Lemma 6. Let E happen and T
5. For any batch b, positions Ib , and set Bb,0 , let k = Ib (1) be the highest position in
batch b and ` be the first stage where
r
⇤ (k)(1
↵max )
˜` <
max
K
for

max

= ↵(s)

↵(s + 1) and s =

arg max

[↵(d)

↵(d + 1)]. Then batch b is split by the end of stage `.

d2{Ib (1),...,Ib (2) 1}
⇤
+
Proof. Let B + = {k, . . . , s} and B = Bb,0 \ B + . Now note that ↵(d⇤ ) ↵(d)
max for any (d , d) 2 B ⇥ B .
⇤
⇤
+
By Lemma 4, Lb,` (d ) > Ub,` (d) for any (d , d) 2 B ⇥ B ; and therefore batch b is split by the end of stage `.

Lemma 7. Let event E happen and T

5. Then the expected T -step regret in any batch b is bounded as
"T 1
#
X
96K 2 L
E
Rb,` 
log T .
(1 ↵max ) max
`=0

Proof. Let k = Ib (1) be the highest position in batch b. Choose any item d 2 Bb,0 and let

= ↵(k)

↵(d).

First, we show that the expected per-step regret of any item d is bounded by (k) when event E happens. Since event E
happens, all eliminations and splits up to any stage ` of batch b are correct. Therefore, items 1, . . . , k 1 are at positions
1, . . . , k 1; and position k is examined with probability ⇤ (k). Note that this is the highest examination probability in
batch b (Assumption 4). Our upper bound follows from the fact that the reward is linear in individual items (Section 3.1).
⇤

We analyze two cases. First, suppose that
the number of steps in a stage is at most

 2K

⇤ (k)(1

max

for

16K
↵max )

max

in Lemma 6. Then by Lemma 6, batch b splits when

2
max

log T .

Online Learning to Rank in Stochastic Click Models

By the design of DisplayBatch, any item in stage ` of batch b is displayed at most 2n` times. Therefore, the maximum
regret due to item d in the last stage before the split is
32K ⇤ (k)
⇤ (k)(1
↵max )
Now suppose that

> 2K

max .

2
max

log T 

64K 2 max
log T =
(1 ↵max ) 2max
(1

64K 2
↵max )

log T .
max

This implies that item d is easy to distinguish from item K. In particular,

↵(K)

↵(d) =

(↵(k)

↵(K))

K

max

2

,

where the equality is from the identity
= ↵(k)

↵(d) = ↵(k)

↵(K) + ↵(K)

↵(d) ;

the first inequality is from ↵(k) ↵(K)  K max , which follows from the definition of max and k 2 [K]; and the last
inequality is from our assumption that K max < /2. Now we apply the derived inequality and, by Lemma 5 and from
the design of DisplayBatch, the maximum regret due to item d in the stage where that item is eliminated is
⇤ (k)(1

32K ⇤ (k)
↵max )(↵(K)

↵(d))2

The last inequality is from our assumption that

log T 
> 2K

128K
(1 ↵max )

log T 

64
(1

↵max )

log T .
max

max .

Because the lengths of the stages quadruple and BatchRank resets all click estimators at the beginning of each stage, the
maximum expected regret due to any item d in batch b is at most 1.5 times higher than that in the last stage, and hence
"T 1
#
X
96K 2 |Bb,0 |
E
Rb,` 
log T .
(1 ↵max ) max
`=0

This concludes our proof.

Online Learning to Rank in Stochastic Click Models

E. Technical Lemmas
Lemma 8. Let (X1 )ni=1 be n i.i.d. Bernoulli random variables, µ̄ =
P (µ̄
for any " 2 [0, 1

µ], and

i=1

Xi , and µ = E [µ̄]. Then

µ + ")  exp[ nDKL(µ + " k µ)]

P (µ̄  µ

for any " 2 [0, µ].

Pn

")  exp[ nDKL(µ

" k µ)]

Proof. We only prove the first claim. The other claim follows from symmetry.
From inequality (2.1) of Hoeffding (1963), we have that
"✓
◆µ+" ✓
◆1
µ
1 µ
P (µ̄ µ + ") 
µ+"
1 (µ + ")
for any " 2 [0, 1 µ]. Now note that
"✓
◆µ+" ✓
◆1
µ
1 µ
µ+"
1 (µ + ")

#
(µ+") n

 
= exp n (µ + ") log
= exp





#
(µ+") n

µ
+ (1
µ+"

n (µ + ") log

µ+"
+ (1
µ

(µ + ")) log

1 µ
1 (µ + ")

(µ + ")) log

1

(µ + ")
1 µ

= exp[ nDKL(µ + " k µ)] .

This concludes the proof.
Lemma 9. For any c, p, q 2 [0, 1],
c(1

(14)

max {p, q})DKL(p k q)  DKL(cp k cq)  cDKL(p k q) .

Proof. The proof is based on differentiation. The first two derivatives of DKL(cp k cq) with respect to q are
@
c(q
DKL(cp k cq) =
@q
q(1

@2
c2 (q p)2 + cp(1
DKL(cp k cq) =
2
@q
q 2 (1 cq)2

p)
,
cq)

and the first two derivatives of cDKL(p k q) with respect to q are
@
c(q
[cDKL(p k q)] =
@q
q(1

@2
c(q
[cDKL(p k q)] =
2
@q

p)
,
q)

p)2 + cp(1
q 2 (1 q)2

cp)

p)

;

.

The second derivatives show that DKL(cp k cq) and cDKL(p k q) are convex in q for any p. Their minima are at q = p.
Now we fix p and c, and prove (14) for any q. The upper bound is derived as follows. Since
DKL(cp k cx) = cDKL(p k x) = 0

when x = p, the upper bound holds when cDKL(p k x) increases faster than DKL(cp k cx) for any p < x  q, and when
@
cDKL(p k x) decreases faster than DKL(cp k cx) for any q  x < p. This follows from the definitions of @x
DKL(cp k cx)
@
@
@
and @x [cDKL(p k x)]. In particular, both derivatives have the same sign and @x DKL(cp k cx)  @x [cDKL(p k x)] for
any feasible x 2 [min {p, q} , max {p, q}].
The lower bound is derived as follows. The ratio of
@
@x [cDKL(p k x)]
@
@x DKL(cp k cx)

@
@x [cDKL(p k x)]

=

1
1

and

@
@x DKL(cp k cx)

cx
1


x
1 x
1

is bounded from above as

1
max {p, q}

for any x 2 [min {p, q} , max {p, q}]. Therefore, we get a lower bound on DKL(cp k cx) when we multiply cDKL(p k x)
by 1 max {p, q}.

