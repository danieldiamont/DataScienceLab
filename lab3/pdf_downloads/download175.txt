Supplementary Material for â€œiSurvive: An Interpretable, Event-time
Prediction Model for mHealthâ€
Walter H. Dempsey * 1 Alexander Moreno * 2 Christy K. Scott 3 Michael L. Dennis 3 David H. Gustafson 4
Susan A. Murphy 1 James M. Rehg 2

A. Prior Work on Interpretable Latent State
Models
We highlight key differences between the present work and
an interpretable, latent state model introduced by (Lian
et al., 2014). In it, the model has one sequence of latent
(K binary events) states (e.g., progression in a movie); each
user experiences the same sequence of latent states but may
react differently, resulting in a user-specific intensity function that produces a response to the latent process. In our
model, on the other hand, the latent state process evolves
independently from user to user. Thus the participants do
not experience the same sequence of latent states. This is a
key difference for our mobile health application.
In Lian et al. (2014), interpretability is achieved post-hoc
and is not a built in feature of the model. Comparison of
latent sources to movie scenes is performed and intuitive
statements are made such as â€One source relates to enhanced arousal intensity during a plane crash, key plot turning points, a climax, and a surprise denouement.â€ We, on
the other hand, will provide a systematic method of relating one latent state to particular emissions so that the states
are easily interpreted, allowing us to make statements such
as the 30-minute probability of lapse at high risk and low
engagement is 74.4%.

B. Graphical Models for Section 2.3
The graphical model for the example discussed in Section 2.3 is given by {S1 â†’ O1 , S1 â†’ O2 , S2 â†’ O2 },
where x â†’ y denotes a directed arc from x to y.
*

1

2

Equal contribution University of Michigan Georgia Institute
of Technology 3 Lighthouse Institute 4 University of WisconsinMadison. Correspondence to: Alexander Moreno <alexander.f.moreno@gatech.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

!"

!"

!#

!#

$"

$"

$#

$#

Figure 1. Graphical model for sec. 2.3. Recall that s1 corresponds
to stress and s2 corresponds to craving. Then o1 is conditionally
independent of s2 , given s1 .

C. Details on Weighted Fisher Scoring
We have assumed complete factorization of the observation
component; therefore, we only investigate the kth observa(n)
tion component. Let Ok denote the kth observation component for the nth participant. Then the final term of the
expected complete log-likelihood is given by
Vn
N X
h
i
X
E log p(O(n) (tv ) = ov | S(tv )) | On [t]; QÌ‚, Î¦Ì‚ .
n=1 v=1



Let Î³v (s) = p S(tv ) = s | On [t]; QÌ‚, Î¦Ì‚ . Then the expected complete log-likelihood is equal to
Vn X
N X
X
s

n=1 v=1

=

XX
o

Î³v (s) log p(O(n) (tv ) = ov | S(tv ) = s)

b(s) log p(O = o | S(tv ) = s).

s

P
where o is a sum over the observed values of the observation process (e.g., if the process isP
binary
P then the sum is
over o = 0 and o = 1) and b(s) = v n Î³v (s) (i.e., the
sum of all weights where the latent process is equal to the
latent state s âˆˆ S.
Given weights {b(s)}sâˆˆS , we now show how to obtain the
maximum-likelihood estimates via a weighted version of
Fisher scoring. Let Î·Ì‚0 denote the current estimate of the

Supplementary Material for â€œiSurvive: An Interpretable, Event-time Prediction Model for mHealthâ€

linear predictor, and ÂµÌ‚0 the fitted value using the link function Î· = g(Âµ). Then define
 
dÎ·
z0 = Î·Ì‚0 + (y âˆ’ ÂµÌ‚0 )
dÂµ 0

We observe N independent and identically distributed observation sequences, oi = (oi (ti,1 ), . . . , oi (ti,ki )) for i =
1, . . . , n. Then the re-scaled log-likelihood of such a sample is given by

where the derivative of the link is evaluated at ÂµÌ‚0 . Weights
are defined by
 2
dÎ·
W0âˆ’1 = bÌ„
V0
dÂµ 0

lN (Î¸) =

where V0 is the variance function evaluated at ÂµÌ‚0 , and bÌ„ =
(b(1), . . . , b(S)) is the vector of weights associated with
the EM-algorithm. Then regress z0 on the covariates with
weight W0 to give new estimates Î²Ì‚; from these form a new
estimate Î·Ì‚. Repeat until changes are sufficiently small.

By Jensenâ€™s inequality we have
"N
#

X
1
p(S = s, Xi = xi ; Î¸0 )
lN (Î¸) â‰¥ ES | xi ;Î¸0
log
N
p(S = s | Xi = xi ; Î¸0 )
i=1
X
N
1
= ES | xi ;Î¸
log (p(S = s, Xi = xi ; Î¸0 ))
N
i=1

N
X
0
log (p(S = s | Xi = xi ; Î¸ ))
âˆ’

D. EM Convergence â€“ Technical Details
Here we provide technical details related to Lemma 1 from
the main body of the paper; we assume the event process
is observed via self-report and therefore can be thought of
as part of the observation process. When the event process
is measured continuously, the below theory can easily be
adjusted to include the necessary third term. We omit this
for brevity as the conclusions are not changed.
We assume the observation schedule always starts with an
observation at baseline (i.e, t1 = 0). The next observation times ti are (potentially stochastic) functions of the
observed history Hi ; therefore the observation schedule
satisfies the restrictive conditional independence assumption. The study is of length Î¾ and therefore the observation
schedule for each participant t is a random subset of the
interval [0, Î¾].
We assume the probability of observing the sequence o =
(o(t1 ), . . . , o(tk )) is given by:
Z
p(X = x; Î¸) = p(S = s, O = o; Î¸)
s

Z
=

p(s(t1 ))
s

Â·

K
Y

k
Y

Z

N
1 X
log
p(S = s, Xi = xi ; Î¸)
N i=1
s



N
1 X
p(S = s, Xi = xi ; Î¸0 )
=
log ES | xi ;Î¸
.
N i=1
p(S = s | Xi = xi ; Î¸0 )

i=1

=QN (Î¸ | Î¸0 ) âˆ’ HN (Î¸0 ).
For a choice of Î¸0 , the E-step computes the function Î¸ â†’
QN (Î¸ | Î¸0 ). The M-step then maximizes the Q-function
for fixed Î¸0 :
MN (Î¸0 ) = arg max QN (Î¸ | Î¸0 )
Î¸âˆˆâ„¦

For the HMM, we can decompose the Q-function as
(s)

(s)

QN (Î¸ | Î¸0 ) = QN (Î¸s | Î¸0 ) + QN (Î¸x | Î¸0 ).
That is, we can decompose the Q-function into a component only dependent on Î¸s and one only dependent on Î¸x .
This implies the M-step is also decomposable and we can
define
(s)

(s)

MN (Î¸0 ) = arg max QN (Î¸s | Î¸0 ) and
Î¸s âˆˆâ„¦s

(x)

(x)

MN (Î¸0 ) = arg max QN (Î¸x | Î¸0 )
Î¸x âˆˆâ„¦s

p(s(ti ) | s(tiâˆ’1 ); Î¸z )

i=2

p(o(ti ) | s(ti ); Î¸o )p(ti | Hi ; Ï‰)

i=1

where S is a latent Markov process, and Î¸ =
(Î¸s , Î¸x ) parametrize the latent Markov process and the
â€œmeasurement-error modelsâ€ respectively. Note that the
final term is independent of the latent process and therefore factors outside the summation; under the assumption
of variational independence we see that the observation
schedule component of the joint probability will not impact
maximum likelihood estimation with respect to Î¸.

Under the assumption of independent and identically distributed, the law of large numbers ensures that as the same
size N increases, the sample-based Q-function approaches
its expectation:


QÌƒ(Î¸ | Î¸0 ) = E[QN (Î¸ | Î¸0 )] = E ES | X,Î¸0 [log(p(S, X; Î¸))]
where the outside expectation is over the distribution of o
(both the observation values and schedule). We call QÌƒ the
â€œpopulation Q-functionâ€. The â€œpopulation M-functionâ€ is
defined as
MÌƒ (Î¸0 ) = arg max QÌƒ(Î¸ | Î¸0 ).
Î¸âˆˆâ„¦

Supplementary Material for â€œiSurvive: An Interpretable, Event-time Prediction Model for mHealthâ€

Proof. By definition, the M -functions satisfy the following
optimality condition

D.1. Analysis of EM Algorithm
D.1.1. P OPULATION L EVEL A LGORITHM
Let Î¸? denote the maximum likelihood estimate (MLE) of
the population likelihood E[lN (Î¸)]. Here we assume the
MLE is unique. The MLE is a fixed point of the population
M -function â€“ i.e., it automatically satisfies
Î¸? = MÌƒ (Î¸? ).
We make the following conditions on the population Qfunction.
Assumption 1 (Conditions on QÌƒ). We assume the following conditions on the population Q-function for Î¸ âˆˆ â„¦:
1. We assume QÌƒ(s) is Î»s -strongly concave:

for Î¸s , Î¸s0 âˆˆ â„¦s and Î¸? âˆˆ â„¦.
2. We assume QÌƒ

and
hâˆ‡Q(s) (M (s) (Î¸) | Î¸), M (s) (Î¸s? ) âˆ’ M (s) (Î¸)i â‰¤ 0.
Recall the optimal Î¸s? is a fixed point of the M (s) -operator
(i.e., M (s) (Î¸s? ) = Î¸s? ). Combining these inequalities yields
hâˆ‡Q(s) (M (s) (Î¸? ) | Î¸? ) âˆ’ âˆ‡Q(s) (M (s) (Î¸) | Î¸),
Î¸s? âˆ’ M (s) (Î¸)i â‰¥ 0.

Q(s) (Î¸s | Î¸? ) âˆ’ Q(s) (Î¸s0 | Î¸? ) âˆ’ hâˆ‡Q(s) (Î¸s | Î¸? ),
Î»s
kÎ¸s âˆ’ Î¸s0 k22 .
Î¸s âˆ’ Î¸s0 i â‰¥
2
Switching places of Î¸s and Î¸s0 and adding the resulting inequality to the above yields

is Î»x -strongly concave

QÌƒ(x) (Î¸x | Î¸? ) âˆ’ QÌƒ(x) (Î¸x0 | Î¸? ) âˆ’ hâˆ‡QÌƒ(x) (Î¸x ), Î¸x âˆ’ Î¸x0 i
Î»x
â‰¤ âˆ’ kÎ¸x âˆ’ Î¸x0 k22
2
for Î¸x , Î¸x0 âˆˆ â„¦x and Î¸? âˆˆ â„¦.

hâˆ‡Q(s) (Î¸s | Î¸? ) âˆ’ âˆ‡Q(s) (Î¸s0 | Î¸? ),
Î¸s âˆ’ Î¸s0 i â‰¥ Î»s kÎ¸s âˆ’ Î¸s0 k22 .

kâˆ‡x QÌƒ(x) (Î¸x | (Î¸s? , Î¸x? )) âˆ’ âˆ‡x QÌƒ(x) (Î¸x | (Î¸s0 , Î¸x? ))k2
â‰¤ Lx,s kÎ¸s? âˆ’ Î¸s0 k

hâˆ‡Q(s) (M (s) (Î¸) | Î¸? ) âˆ’ âˆ‡Q(s) (M (s) (Î¸? ) | Î¸? ),
M (s) (Î¸) âˆ’ M (s) (Î¸? )i â‰¥ Î»s kM (s) (Î¸) âˆ’ M (s) (Î¸? )k22
= Î»s kM (s) (Î¸) âˆ’ Î¸s? k22 .

kâˆ‡x QÌƒ(x) (Î¸x | (Î¸s? , Î¸x? )) âˆ’ QÌƒ(x) (Î¸x | (Î¸s? , Î¸x0 ))k2
â‰¤ Lx,x kÎ¸x? âˆ’ Î¸x0 k
0

4. For each Î¸s âˆˆ â„¦s and Î¸ âˆˆ â„¦
kâˆ‡s QÌƒ

(Î¸s |

(Î¸s? , Î¸x? ))

kâˆ‡s QÌƒ

(Î¸x |

(Î¸s? , Î¸x? ))

Also, equation (1) implies
hâˆ‡Q(s) (M (s) (Î¸? ) | Î¸? ) âˆ’ âˆ‡Q(s) (M (s) (Î¸) | Î¸? ),

(x)

âˆ’ âˆ‡x QÌƒ

(Î¸x |

(Î¸s0 , Î¸x? ))k2

â‰¤ Lx,s kÎ¸s? âˆ’ Î¸s0 k
(x)

(2)

Substitute M (s) (Î¸) = Î¸s and Î¸s? = M (s) (Î¸? ) = Î¸s0 into
equation (2) yields

3. For each Î¸x âˆˆ â„¦x and Î¸0 âˆˆ â„¦

(z)

(1)

Recall Î»s -strong concavity, states that

QÌƒ(s) (Î¸s | Î¸? ) âˆ’ QÌƒ(s) (Î¸s0 | Î¸? ) âˆ’ hâˆ‡QÌƒ(s) (Î¸s ), Î¸s âˆ’ Î¸s0 i
Î»s
â‰¤ âˆ’ kÎ¸s âˆ’ Î¸s0 k22
2
(x)

hâˆ‡Q(s) (M (s) (Î¸? ) | Î¸? ), M (s) (Î¸s? ) âˆ’ M (s) (Î¸)i â‰¥ 0

âˆ’ QÌƒ (Î¸x | (Î¸s? , Î¸x0 ))k2
â‰¤ Lx,x kÎ¸x? âˆ’ Î¸x0 k
(x)

Î¸s? âˆ’ M (s) (Î¸)i
â‰¥ hâˆ‡Q(s) (M (s) (Î¸) | Î¸) âˆ’ âˆ‡Q(s) (M (s) (Î¸) | Î¸? ),
Î¸s? âˆ’ M (s) (Î¸)i.

Lemma 1. Under the above assumptions, for Î¸ âˆˆ â„¦ and
pair (L, Î»), then population M -function satisfies
 
L
kÎ¸ âˆ’ Î¸? k.
kM (Î¸) âˆ’ Î¸k â‰¤
Î»

Take the left-hand side of the above inequality. Then the
second set of assumptions leads to

Define L = max{Ls,x , Ls,s } + max{Lx,s , Lx,x } and Î» =
min{Î»s , Î»x }. Then for any point starting Î¸0 âˆˆ â„¦, the population EM-algorithm satisfies
 t
L
kÎ¸(t) âˆ’ Î¸? k2 â‰¤
kÎ¸0 âˆ’ Î¸? k2
Î»

â‰¤kâˆ‡Q(s) (M (s) (Î¸) | Î¸? ) âˆ’ âˆ‡Q(s) (M (s) (Î¸) | Î¸? )k22

for every t = 1, 2, . . ..

hâˆ‡Q(s) (M (s) (Î¸? ) | Î¸? ) âˆ’ âˆ‡Q(s) (M (s) (Î¸) | Î¸? ),
Î¸s? âˆ’ M (s) (Î¸)i
kÎ¸s? âˆ’ M (s) (Î¸)k22
â‰¤ Ls,x Â· kÎ¸x? âˆ’ Î¸x k22 + Ls,s Â· kÎ¸s? âˆ’ Î¸s k22



kÎ¸s? âˆ’ M (s) (Î¸)k2

â‰¤ max{Ls,x , Ls,s } kÎ¸? âˆ’ Î¸k22 kÎ¸s? âˆ’ M (s) (Î¸)k2

Supplementary Material for â€œiSurvive: An Interpretable, Event-time Prediction Model for mHealthâ€

where the second inequality is application of CauchySchwarz, the third is due to item 3 in Assumptions 1. Combining the inequalities yields the equation
Î»s kÎ¸s? âˆ’ M (s) (Î¸)k2 â‰¤ max{Ls,x , Ls,s }kÎ¸? âˆ’ Î¸k2
The identical argument applied to the x-component yields
Î»x kÎ¸x? âˆ’ M (x) (Î¸)k2 â‰¤ max{Lx,x , Lx,s }kÎ¸? âˆ’ Î¸k2 .
Combining these with Î» = min(Î»s , Î»x ) and L =
max{Ls,x , Ls,s } + max{Lx,x , Lx,s } yields
 
L
?
kÎ¸? âˆ’ Î¸k2
kÎ¸ âˆ’ M (Î¸)k2 â‰¤
Î»

D.2. Finite Sample-size Statistical Guarantees for EM
Algorithm
Definition 1 (Sample-to-population M -function gap). For
a particular sample size N â‰¥ 0 and constant Î´ âˆˆ (0, 1),
define (x) (N, Î´) by:
(x)

sup p(kMN (Î¸) âˆ’ M (x) (Î¸)k2 â‰¥ (x) (N, Î´)) â‰¤ Î´

Î¸âˆˆâ„¦

and (z) (N, Î´) by:
(s)

sup p(kMN (Î¸) âˆ’ M (s) (Î¸)k2 â‰¥ (s) (N, Î´)) â‰¤ Î´.

Î¸âˆˆâ„¦

Lemma 2 below is the technical version of Lemma 1 in the
main body of the paper; we end with a proof of this result.
Lemma 2. The population M -function satisfies Assumptions 1 for all Î¸ âˆˆ â„¦. Then for sample size sufficiently
large to ensure
x (N, Î´) + s (N, Î´) â‰¤ (1 âˆ’ Îº) r âˆ’ Îº max kÎ¸s âˆ’ Î¸s? k
Î¸s âˆˆâ„¦s

Then for Î¸0 âˆˆ â„¦, with probability 1âˆ’2Î´, the EM-algorithm
satisfies
kÎ¸Ì‚(t) âˆ’ Î¸? k â‰¤ Îºt kÎ¸0 âˆ’ Î¸? k +

1
(x (N, Î´) + s (N, Î´))
1âˆ’Îº

Proof. Define (N, Î´) = x (N, Î´) + x (N, Î´). With probability at least 1 âˆ’ 2Î´
kÎ¸Ì‚(t+1) âˆ’ Î¸? k = kMN (Î¸Ì‚(t) ) âˆ’ Î¸? k
â‰¤ kMN (Î¸Ì‚(t) ) âˆ’ MÌƒ (Î¸Ì‚(t) )k + kMÌƒ (Î¸Ì‚(t) ) âˆ’ Î¸? k
â‰¤ x (N, Î´) + x (N, Î´) + Îº kÎ¸Ì‚(t) âˆ’ Î¸? k.

Iterating on this we have
" tâˆ’1 #
X
kÎ¸Ì‚(t+1) âˆ’ Î¸? k â‰¤
Îºs (N, Î´) + Îºt kÎ¸0 âˆ’ Î¸? k
s=0

â‰¤

1
(N, Î´) + Îºt kÎ¸0 âˆ’ Î¸? k.
1âˆ’Îº

To complete the proof, note the sample size based inequality ensures that each iteration stays within the space â„¦.

E. Additional Details for the RSS Case Study
Here we present additional details regarding the subset of
data analyzed from the recovery support studies on individuals with substance use disorders (SUDs). We start with
a more detailed description of each component of the reduced observation O(t) = (O1 (t), O2 (t), O3 (t), Y (t)).
â€¢ O1 (t): Ordinal response to the question â€Rate the
extent to which certain feelings help with/support
your recovery.â€ Original response was on a 0â€“7 scale.
Based on studying the distribution of responses we
collapsed into a three-level ordinal response:
(0, 1, 2) â†’ 0, (3, 4, 5) â†’ 1, and (6) â†’ 2.
Here 2 is translates to current feelings greatly helping
with/supporting your recovery. The fraction of observations per level is (0.10, 0.26, 0.64). Figure 2 below
presents histograms across participants of the fraction
of EMAs when a participant responds with a particular rating.
â€¢ O2 (t): a 3-level ordinal variable related to EMI usage
by the following mapping:
None â†’ 0, 1â€“3 times â†’ 1, and 4+ â†’ 2
The
fraction
of
observations
per
level
is (0.76, 0.15, 0.09).
Figure 3 below presents
histograms across participants of the fraction of
EMAs when a participant had a particular level of
EMI usage.
â€¢ O3 (t): an indicator of whether the participant kept all
default answers to all questions within the self-report.
The fraction of observations per level is (0.61, 0.39).
Figure 3 below presents a histogram (across participants) of the fraction of EMAs when the participant
responded with all default answers.
â€¢ Y(t): the binary indicator of use of drugs/alcohol in
past 30 minutes. The fraction of observations per
level is (0.94, 0.06) (i.e., 6% of all EMAs record
drug/alcohol use within the ). Figure 3 below presents
a histogram (across participants) of the fraction of
EMAs when the participant responded yes to having
used drugs/alcohol within the past 30 minutes.

0.2

0.8

0.4
0.6
Fraction of EMAs with rating = 1

0.8

0.4
0.6
Fraction of EMAs with rating = 2

0.8

Frequency
0.2

0.4
0.6
Fraction of EMAs with rating = 0

1.0

0.2

0.8

1.0

1.0

45
40
35
30
25
20
15
10
5
0
0.0

0.4
0.6
Fraction of EMAs with EMI usage = 0

Frequency

45
40
35
30
25
20
15
10
5
0
0.0

0.2

45
40
35
30
25
20
15
10
5
0
0.0

0.2

0.8

1.0

45
40
35
30
25
20
15
10
5
0
0.0

0.4
0.6
Fraction of EMAs with EMI usage = 1

Frequency

Frequency

45
40
35
30
25
20
15
10
5
0
0.0

Frequency

45
40
35
30
25
20
15
10
5
0
0.0

Frequency

Supplementary Material for â€œiSurvive: An Interpretable, Event-time Prediction Model for mHealthâ€

0.2

0.4
0.6
Fraction of EMAs with EMI usage = 2

0.8

1.0

1.0

Figure 3. Histograms across participants related to O2 (t)

Figure 2. Histograms across participants related to O1 (t)

18

We assumed 2 latent binary sourcesâ€“i.e., S(t) =
(S1 (t), S2 (t)) with Si (t) âˆˆ {0, 1} for each i âˆˆ {1, 2}. We
describe how we think about each and then show how this
interpretation is achieved via link restriction.

16
14

â€¢ S2 (t): We define this to be engagement. Here engagement is defined in a very specific manner: â€œthinkingâ€
through the EMA self-report and not simply filling a
self-report with default answers. Here S2 (t) is thought
to be uniquely associated with O3 (t).
We now specify the models for each observation component conditional on S(t).
â€¢ Model for O2 (t): Hierarchical model dependent on
S2 (t). If S2 (t) = 1 then the participant is not currently engaged, and therefore the response is independent of S1 (t). In this case, the following proportional
odds model defines the relationship:
(1,1)

logit(p(O2 (t) â‰¤ j | S1 (t), S2 (t) = 1]) = Ï†j

.

Frequency

12

â€¢ S1 (t): We define this as risk. Here S1 (t) is thought to
be uniquely associated with O1 , the ordinal response
on how current feelings help with support a participantâ€™s recovery.

10
8
6
4
2
0
0.0

0.2

0.4
0.6
0.8
Fraction of EMAs with all default answers

1.0

Figure 4. Histogram related to O3 (t)

If S2 (t) = 1 then the participant is currently engaged,
and therefore the response depends on S1 (t). In this
case, the following proportional odds model defines
the relationship:
(1,0)

logit(p(O2 (t) â‰¤ j | S1 (t), S2 (t) = 0]) = Ï†j

(1)

+S1 (t)Ï†1 .

â€¢ Model for O2 (t): Proportional odds model defined by

Supplementary Material for â€œiSurvive: An Interpretable, Event-time Prediction Model for mHealthâ€
45

0.6

40
0.5

35

0.4
Brier Score

Frequency

30

Comparison of prediction accuracy
iSurvive
Current O (Logistic)
Event at prior time (Logistic)
Complete (Logistic)
Current O (kSVM)
Event at prior time (kSVM)
Complete (kSVM)

25
20

0.3

15
0.2

10
5
0
0.0

0.1

0

0.2
0.4
0.6
0.8
Fraction of EMAs with self-reported alcohol/drug use

the following relationship:
(2)

logit(p(O2 (t) â‰¤ j | S(t)]) = Ï†j

(2)

+ S1 (t)Ï†1 + S2 (t)Ï†2 .
â€¢ Model for O3 (t): GLM with logit link function (i.e.,
logistic regression) defined by the following relationship:
(3)

(3)

logit(E[O3 (t) | S(t)]) = Ï†0 + S2 (t)Ï†2 .
â€¢ Model for Y (t): GLM with logit link function (i.e.,
logistic regression) defined by the following relationship:
(4)

(4)

2

Window length (in days)

3

4

5

Figure 6. Cross-validated complete log-loss on recovery support
services study for several discriminative models and iSurvive

Figure 5. Histogram related to Y (t)

(2)

1

1.0

(4)

logit(E[Y (t) | S(t)]) = Ï†0 + S1 (t)Ï†1 + S2 (t)Ï†2 .
A log-linear model can also be fit with an â€œexposureâ€ parameter of 30-minutes. This would allow for
the user to make predictions for future self-reported
drug/alcohol use over different windows. Since this
was not necessary for the case study, we did not pursue this further.

F. Additional details on Synthetic
Experiments
Table 1. Results from 10 runs of the synthetic experiments. We
deleted a single extreme outlier from the c = 2 case.
c
Binary MAE Count Weights Norm Q rel. error
1
0.056Â±0.029 0.109Â±0.087
0.269Â±0.048
2
0.039Â±0.028 0.120Â±0.102
0.222Â±0.104
3
0.010Â±0.006 0.068Â±0.023
0.180Â±0.044

We assume the latent process has p = 3 sources: for ease
of understanding we consider them to be stress, craving,

and engagement. We generate a random Q matrix in the
following way: we sample from a dirichlet with parameters K = 8 and Î±1 , Â· Â· Â· , Î±K = 8. We then zero out the
diagonals, set them to be the negative of the sum of the remaining terms in the corresponding rows, and multiply the
entire matrix by 5. For the observations, we have three ordinal ratings, each having a binary response. We also have
count data to represent number of times EMIs (Ecological
Momentary Interventions) are accessed via the application.
The emission model for the ratings data is a GLM similar
to equation (3) given by
logit (E[Ok (t) | S]) = Ï†0 S(t)

(3)

while for EMI the counts between two observations follow
a homogeneous poisson process with mean rate parameter
Î» such that
log(Î») = Ï†0 S(t)

(4)

We use logit emissions in order to see the effect of smart
initialization, as the lack of strong concavity far from the
true parameters can provide a challenge without it.
For
the
EMA,
we
use
Ï†
=
(Ï†baseline , Ï†stress , Ï†craving , Ï†engagement ). Each EMA
is only associated with one variable, and the true relationship between the baseline parameter and a latent
variable weight is fixed. For the stress question this would
be Ï† = c(1, âˆ’2, 0, 0), while for craving it would be
Ï† = c(1, 0, âˆ’2, 0), where c is a scalar. The weights are
the same magnitudes across questions. With this setup,
c becomes a parameter that controls the emission noise.
Larger values of c correspond to lower emission noise.
Particularly, c = 1, 2, 3 correspond to probabilities 0.73,
0.88, and 0.95 for observing a 0. For the EMI we set the
true weights to Ï† = c(0.4, 0.1, 0.2, 0.3). We set c = 3
for the true value, and c = 0.25 with the true relative
proportions for the smart initialization. When not using

Supplementary Material for â€œiSurvive: An Interpretable, Event-time Prediction Model for mHealthâ€

smart initialization, we set c = 0 and initialize Q randomly
the same way we initialized the true Q.
Table 1 shows the results from running the algorithm with
smart initialization 10 times for each of c = 1, 2, 3 with
N = 50, T = 252 and 125 iterations. We deleted a single
extreme outlier for c = 2, and found that the differences
between the accuracy of c = 1 and c = 2 were not statistically significant at 95%, but the differences between c = 3
and either of c = 1 or c = 2 were for the binary case and
count weights. The difference for Q in the latter case was
just outside the 95% interval. For the terms shown, the binary MAE represents the mean absolute error between the
true probability of observing a 0 and the learned probability across all states. The count weight norms represent the
norm of the difference between generating and estimated
parameters, and the Q relative error is the same as in (Liu
et al., 2015).

References
Lian, W., Rao, V.A., Eriksson, B., and Carin, L. Modeling correlated arrival events with latent semi-markov
processes. In Proceedings of the 29th International Conference on Machine Learning, 2014.
Liu, Y., Song, L., Li, F., Li, S., and Rehg, J. Efficient
continuous-time hidden markov model for disease modeling. In Proceedings for Advances in Neural Information Processing Systems (NIPS), 2015.

