Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

A. Proofs for Realizable Setting
Proof of Lemma 3. Let âˆ† := w
b âˆ’ wâˆ— be the difference between the true answer and solution to the optimization problem.
âˆ—
Let S to be the support of w and let S c = [d] \ S be the complements of S. Consider the permutation i1 , . . . , idâˆ’k of
S c for which |âˆ†(ij )| â‰¥ |âˆ†(ij+1 )| for all j. That is, the permutation dictated by the magnitude of the entries of âˆ† outside
of S. We split S c into subsets of size k according to this permutation: Define Sj , for j â‰¥ 1 as {i(jâˆ’1)k+1 , . . . , ijk }. For
convenience we also denote by S01 the set S âˆª S1 .
Now, consider the matrix XS01 âˆˆ RtÃ—|S01 | whose columns are those of X with indices S01 . The Restricted Isometry
Property of X dictates that for any vector c âˆˆ RS01 ,
1
(1 âˆ’ ) kck2 â‰¤ âˆš kXS01 ck2 â‰¤ (1 + ) kck2 .
n
Let V âŠ† Rt be the subspace of dimension |S01 | that is the image of the linear operator XS01 , and let PV âˆˆ RtÃ—t be the
projection matrix onto that subspace. We have, for any vector z âˆˆ Rt that

1 
(1 âˆ’ ) kPV zk â‰¤ âˆš XST01 z  â‰¤ (1 + ) kPV zk
n
We apply this to z = Xâˆ† and conclude that
kPV Xâˆ†k â‰¤ âˆš


 T
1
XS Xâˆ†
01
t(1 âˆ’ )

We continue to lower bound the quantity of kPV Xâˆ†k. We decompose PV Xâˆ† as
X
PV Xâˆ† = PV Xâˆ†(S01 ) +
PV Xâˆ†(Sj )

(10)

(11)

jâ‰¥2

Now, according to the definition of V we that there exist vectors {cj }jâ‰¥2 in R|S01 | for which
PV Xâˆ†(Sj ) = XS01 cj
We now invoke Lemma 1.1 from (Candes & Tao, 2005) stating that for any S 0 , S 00 with |S 0 | + |S 00 | â‰¤ 3k it holds that
1
hXS 0 c, XS 00 c0 i â‰¤ (2 âˆ’ 2 ) kck2 kc0 k2
n
We apply this for S01 , Sj , j â‰¥ 2 and conclude that
âˆš
2 t
2
kPV Xâˆ†(Sj )k2 = hPV Xâˆ†(Sj ), Xâˆ†(Sj )i â‰¤ 2t kcj k2 Â· kâˆ†(Sj )k â‰¤
kPV Xâˆ†(Sj )k2 Â· kâˆ†(Sj )k2 .
1âˆ’
Dividing through by kPV Xâˆ†(Sj )k2 , we get
âˆš
2 t
kPV Xâˆ†(Sj )k â‰¤
kâˆ†(Sj )k .
(12)
1âˆ’
Let us now bound the sum kâˆ†(Sj )k. By the definition of Sj we know that any element i âˆˆ Sj has the property âˆ†(i) â‰¤
(1/k) kâˆ†(Sjâˆ’1 )k1 . Hence
X
âˆš X
âˆš
kâˆ†(Sj )k â‰¤ (1/ k)
kâˆ†(Sj )k1 = (1/ k) kâˆ†(S c )k1
âˆ€c, c0

jâ‰¥2

jâ‰¥1

We now combine this inequality with Equations (10), (11) and (12)

1
âˆ’
XST Xâˆ† â‰¥ 1âˆš
kPV Xâˆ†k
01
t
t
1âˆ’
1âˆ’X
â‰¥ âˆš kPV Xâˆ†(S01 )k âˆ’ âˆš
kPV Xâˆ†(Sj )k
n
t
jâ‰¥2
X
1âˆ’
â‰¥ âˆš kXâˆ†(S01 )k âˆ’ 2
kâˆ†(Sj )k
t
jâ‰¥2
1âˆ’
2
â‰¥ âˆš kXâˆ†(S01 )k âˆ’ âˆš kâˆ†(S c )k1
t
k

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

The third inequality holds since Xâˆ†(S01 ) âˆˆ V hence PV Xâˆ†(S01) = Xâˆ†(S01 ). We continue to bound the expression by
claiming that kâˆ†(S)k1 â‰¥ kâˆ†(S c )k1 . This holds since in S c , w
bS c = âˆ†(S c ) hence
kwâˆ— k1 = kw
b âˆ’ âˆ†(S c ) âˆ’ âˆ†(S)k1 â‰¤ kwk
b 1 + (kâˆ†(S)k1 âˆ’ kâˆ†(S c )k1 )
Now, the optimality of w
b implies kwk
b 1 â‰¤ kwâˆ— k1 , hence indeed kâˆ†(S)k1 â‰¥ kâˆ†(S c )k1 .
c

kâˆ†(S )k1 â‰¤ kâˆ†(S)k1 â‰¤

âˆš

âˆš

k kâˆ†(S)k2 â‰¤ kâˆ†(S01 )k2 â‰¤

k
âˆš kXâˆ†(S01 )k
(1 âˆ’ ) t

We continue the chain of inequalities

1
2
XST Xâˆ† â‰¥ 1âˆšâˆ’  kXâˆ†(S01 )k âˆ’ âˆš
kâˆ†(S c )k1
01
t
n
k
!
âˆš
1âˆ’
2
k
âˆš
â‰¥ kXâˆ†(S01 )k âˆš âˆ’ âˆš Â·
n
k (1 âˆ’ ) n
=

(1 âˆ’ )2 âˆ’ 2
âˆš kXâˆ†(S01 )k
(1 âˆ’ ) t

Rearranging we conclude that
1
âˆš kXâˆ†(S01 )k
(1 âˆ’ ) t
 T

1
XS Xâˆ†
â‰¤
01
2
((1 âˆ’ ) âˆ’ 2)t
âˆš

2k 
X T Xâˆ†
â‰¤
âˆž
(1 âˆ’ 4)t
s


dk log(d/Î´)
d
âˆ—
Ïƒ+
kw k1
â‰¤C
tk0
k0

kâˆ†(S01 )k â‰¤

(RIP of X)

(since for any z âˆˆ R2k , kzk2 â‰¤

âˆš

2k kzkâˆž )

(Lemma 14 and  < 1/5)

c
for some constant C. We continue our bound on kâˆ†k by showing that kâˆ†(S01
)k â‰¤ kâˆ†(S01 )k
2

(i)

2

c
kâˆ†(S01
)k2 â‰¤ kâˆ†(S c )k1 Â·

X 1
1
1
2
2
2
â‰¤ kâˆ†(S c )k1 â‰¤ kâˆ†(S)k1 â‰¤ kâˆ†(S)k2 .
2
j
k
k

jâ‰¥k+1

Inequality (i) holds due to the following: Let Î±i be the absolute value of the iâ€™th largest (in absolute value) element of
2
c
c
âˆ†(S
holds that Î±i â‰¤ kâˆ†(S c )k1 /i. Now, according to the definition of S01 we have that kâˆ†(S01
)k2 =
P ). It obviously
2
jâ‰¥k+1 Î±i and the inequality follows. Hence,
c
kâˆ†(S01
)k2 â‰¤ kâˆ†(S)k2 â‰¤ kâˆ†(S01 )k2 .

We conclude that
kâˆ†k2 â‰¤

âˆš

s
2 kâˆ†(S01 )k2 â‰¤ C

dk log(d/Î´)
tk0


Ïƒ+

d
kwâˆ— k1
k0

for some universal constant C > 0. Since kâˆ†(S)k1 â‰¥ kâˆ†(S c )k1 and |S| â‰¤ k we get that
âˆš
âˆš
kâˆ†k1 â‰¤ 2 kâˆ†(S)k1 â‰¤ 2 k kâˆ†(S)k2 â‰¤ 2 k kâˆ†k2
and the claim follows.



Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

Proof of Lemma 4. Let S be the support of wâˆ— . We can decompose the square of the left hand side as
2

X
X
X

 e
2
(w(i)
b âˆ’ wâˆ— (i))2 +
(w(i))
b
(wâˆ— (i))2 .
+
b S) âˆ’ wâˆ—  =
w(
2

e
iâˆˆSâˆ©S

e
iâˆˆS\S

e
iâˆˆS\S

We upper bound the last sum on the right hand side as
X
X
2
(wâˆ— (i))2 =
[(w(i)
b âˆ’ wâˆ— (i)) + (w(i))]
b
e
iâˆˆS\S

e
iâˆˆS\S

X

â‰¤2

2
(w(i)
b âˆ’ wâˆ— (i))2 + (w(i))
b

e
iâˆˆS\S

X

â‰¤2

(w(i)
b âˆ’ wâˆ— (i))2 + 2

e
iâˆˆS\S

X

2
(w(i))
b
,

e
iâˆˆS\S

where first inequality follows from the elementary inequality (a + b)2 â‰¤ 2a2 + 2b2 and the second inequality is due to the
e = |Se \ S|. Hence,
fact that Se contains top k entries of w
b in absolute value and |S \ S|

2
X
X
X
 e

2
b S) âˆ’ wâˆ—  =
(w(i)
b âˆ’ wâˆ— (i))2 +
(w(i))
b
+
(wâˆ— (i))2
w(
2

e
iâˆˆSâˆ©S

â‰¤

X

e
iâˆˆS\S

(w(i)
b âˆ’ wâˆ— (i))2 + 2

e
iâˆˆSâˆ©S

X

â‰¤2

X

e
iâˆˆS\S

(w(i)
b âˆ’ wâˆ— (i))2 + 3

e
iâˆˆS\S
âˆ—

2

(w(i)
b âˆ’ w (i)) + 2

e
iâˆˆSâˆ©S

X

X

2
(w(i))
b

e
iâˆˆS\S
âˆ—

2

(w(i)
b âˆ’ w (i)) + 3

e
iâˆˆS\S

X

2
(w(i))
b

e
iâˆˆS\S

X
X
2
=2
(w(i)
b âˆ’ wâˆ— (i))2 + 3
(w(i))
b
iâˆˆS

â‰¤3

e
iâˆˆS\S

d
X
(w(i)
b âˆ’ wâˆ— (i))2
i=1
2

= 3 kw
b âˆ’ wâˆ— k2 .
Taking square root finishes the proof.
Lemma 14. There exists a universal constant C > 0 such that, with probability at least 1 âˆ’ Î´, the convex program (3) is
feasible and its optimal solution w
b satisfies
s




1 T

d log(d/Î´)
d
âˆ— 
âˆ—
 Xt Xt (w
b
âˆ’
w
)
â‰¤
C
Ïƒ
+
kw
k
1 .
t

tk0
k0
âˆž
We note that the above lemma is beyond simple triangle inequality on the feasibility constraints, as the left hand side
bt .
depends on actual design matrix Xt which we do not observe, instead of X
b = Xt and D
b = D
b t , and also let Î· =
Proof. To simplify notation, we drop subscript t. Namely, let X = Xt , X
(Î·1 , Î·2 , . . . , Î·t ) be the vector of noise variables.
First, we show that wâˆ— satisfies the constraint of (3) with probability at least 1 âˆ’ Î´. We upper bound





1 T



b (Y âˆ’ Xw
b âˆ— ) + 1 Dw
b âˆ— =  1 X
b T (X âˆ’ X)
b + 1D
b wâˆ— + 1 X
b T Î·
 X
t

 t

t
t
t
âˆž
âˆž

 


 1 T

1
1

b (X âˆ’ X)
b + D
b wâˆ—  + X
b T Î·
â‰¤
X

 t

t
t
âˆž
âˆž

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

We first bound the left summand. By Lemma 15, we have



 
 1 T



b (X âˆ’ X)
b + 1D
b wâˆ—  â‰¤ kwâˆ— k Â·  1 X
b T (X âˆ’ X)
b + 1D
b
 X
1 
 t

t
t
t âˆž
âˆž


 

1
1 T

1 b
âˆ—
T b
b
b



â‰¤ kw k1  X (X âˆ’ X) +  (X âˆ’ X) (X âˆ’ X) âˆ’ D
t
t
t âˆž
âˆž
s
d3 log(d/Î´)
â‰¤ kwâˆ— k1 C Â·
.
tk0 3
For the right summand, since Î· is vector of i.i.d Gaussians with variance Ïƒ 2 , with probability at least 1 âˆ’ Î´,



Ïƒp
1
 bT 
b 
log(d/Î´) Â· max X
X Î·  â‰¤ C
(i) 
t
t
iâˆˆ[d]
2
âˆž
b
b(2) , . . . , X
b(d) are the columns of X.
b Since the absolute value of the entries of X
b is at most d/k0 , we have
where
X

X(1) ,p
b 
X(i)  â‰¤ td/k0 and thus
2
s

1
d log(d/Î´)
 bT 
.
X Î·  â‰¤ CÏƒ
t
tk0
âˆž
Combining the inequalities so far provides
s




1 T

1
b (Y âˆ’ Xw
b âˆ— ) + Dw
b âˆ—  â‰¤ C d log(d/Î´) Ïƒ + d kwâˆ— k
 X
1
t

t
tk0
k0
âˆž
and hence conclude the constraint of the optimization problem (3) is satisfied (at least) by wâˆ— and thus the optimization
problem is feasible.
Now consider the vector âˆ† := w
b âˆ’ wâˆ— , we have











1 T
bT X
b âˆ’ D)âˆ†
b  +  1 (X
bT X
b âˆ’D
b âˆ’ X T X)âˆ†
 X Xâˆ† â‰¤  1 (X
t
t



t
âˆž
âˆž

âˆž 

1
1 T


T
b b
b 
 b

â‰¤
 t (X X âˆ’ D)âˆ† +  t (X âˆ’ X) Xâˆ†
âˆž
âˆž



 
1 T
 1


1b
T b
b
b



+  X (X âˆ’ X)âˆ† +  (X âˆ’ X) (X âˆ’ X) âˆ’ D âˆ†
 .
t
t
t
âˆž
âˆž
According to Lemma 15 we have
s
s





1 T

1 T
d
log(d/Î´)
d log(d/Î´)
âˆ—
b âˆ’ X)âˆ† â‰¤  X (X
b âˆ’ X) kâˆ†k â‰¤ C
 X (X
(kw k1 + kwk
b 1 ) â‰¤ 2C
Â· kwâˆ— k1
1

t

t
tk
tk
0
0
âˆž
âˆž
where
the last inequality
is by the optimality of w.
b
The same argument provides an identical bound for


1 b

b
 (X âˆ’ X)T Xâˆ† . The last summand can also be bounded by using Lemma 15 and the optimality of w.
t

âˆž

s

 
3
 1

1
T
b âˆ’ X) (X
b âˆ’ X) âˆ’ D
b âˆ† â‰¤ 2C d log(d/Î´) Â· kwâˆ— k
 (X
1
 t

t
tk0 3
âˆž
Finally, according to the feasibility of w
b and wâˆ— we may bound the first summand
s

 


 1 T

1
b X
bâˆ’ D
b âˆ† â‰¤ 2C d log(d/Î´) Ïƒ + d kwâˆ— k ,
 X
1

 t
t
tk0
k0
âˆž
and reach the final bound.

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

Lemma 15. For any t â‰¥ t0 , with probability at least 1 âˆ’ Î´, the following two inequalities hold
s


1

d3 log(d/Î´)
1
T
b
b
b 
 (X
,
 t t âˆ’ Xt ) (Xt âˆ’ Xt ) âˆ’ t Dt  â‰¤ C
tk0 3
âˆž
s


1 T

bt âˆ’ Xt ) â‰¤ C d log(d/Î´) ,
 Xt (X
t

tk0
âˆž
where kÂ·kâˆž denotes the maximum of the absolute values of the entries of a matrix.
Proof. Throughout we use that |xs (i)| â‰¤ 1 for all i âˆˆ [d] and all s âˆˆ [t], and (2) (b
xs (i) âˆ’ xs (i))2 âˆ’ 1t Dii is unbiased with
2
3
absolute value of at most (d/k0 ) and variance of at most (d/k0 ) . For the first term, letâ€™s bound


1 b
b âˆ’ X) âˆ’ 1 D
b
(X âˆ’ X)T (X
t
t

t


ij

1X
1b
=
(b
xs (i) âˆ’ xs (i))(b
xs (j) âˆ’ xs (j)) âˆ’ D
ij
t s=1
t

For i = j, we have
"
E

1
(b
xs (i) âˆ’ xs (i)) âˆ’ Dii
t
2

2 #



â‰¤ E (b
xs (i) âˆ’ xs (i))4 â‰¤ (d/k0 )3



1
1
xs (i) âˆ’ xs (i))2 âˆ’ Dii = 0
(b
xs (i) âˆ’ xs (i))2 âˆ’ Dii â‰¤ (d/k0 )2 , E (b
t
t
Hence, by Bernsteinâ€™s inequality, for any v > 0,

" t
#


1 X

1
v2 t


2
(b
xs (i) âˆ’ xs (i)) âˆ’ Dii  > v â‰¤ 2 exp âˆ’
Pr 
.
t

t
(d/k0 )3 + (d/k0 )2 v/3
s=1
It follows that for any Î´ > 0, with probability at least 1 âˆ’ Î´ it holds for all i âˆˆ [d] that,
 t


1 X
1


2
(b
xs (i) âˆ’ xs (i)) âˆ’ Dii  â‰¤ O

t

t
s=1
b ii âˆ’ Dii ) â‰¤ O
Similarly we have 1t (D



log(d/Î´)d2
tk0 2

+

q

log(d/Î´)d3
tk0 3

log(d/Î´)d2
+
tk0 2

s

log(d/Î´)d3
tk0 3

!
.


.

For i 6= j we use an analogous argument, only now the variance term in Bernsteinâ€™s inequality is (d/k0 )2 rather than
(d/k0 )3 , hence only reach a tighter bound.
For the second term, we again bound via Bernsteinâ€™s inequality as


ï£«s
ï£¶

t
1 T b
1X
d
log(d/Î´)
d
log(d/Î´)
ï£¸
X (X âˆ’ X)
=
xs (i)(b
xs (j) âˆ’ xs (j)) â‰¤ O ï£­
+
t
t s=1
tk0
tk0
ij

âˆš
The claim now follows by noticing that for large enough t, the dominating terms are those that scale as 1/ t.
Proof of Theorem 2. By Lemma 3,
ï£«s
kwt+1 âˆ’ wâˆ— k2 â‰¤ O ï£­

ï£¶
d k log(d/Î´)
d
(Ïƒ +
kwâˆ— k1 )ï£¸ .
k0
t
k0

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

We have
RegretT (wâˆ— ) âˆ’ Regrett0 (wâˆ— ) =

T
X

(yt âˆ’ hxt , wt i)2 âˆ’ (yt âˆ’ hxt , wâˆ— i)2

t=t0 +1

=

T
X

(hxt , wâˆ— âˆ’ wt i + Î·t )2 âˆ’ Î·t2

t=t0 +1

=

T
X

(hxt , wâˆ— âˆ’ wt i + 2Î·t ) hxt , wâˆ— âˆ’ wt i

t=t0 +1

=

T
X

2

2Î·t hxt , wâˆ— âˆ’ wt i + (hxt , wâˆ— âˆ’ wt i) ,

t=t0 +1

where we used that yt = hxt , wt i + Î·t . To bound the regret we require the upper bound, that occurs with probability of at
least 1 âˆ’ Î´,
ï£« s
ï£¶


q
(ii)
(i)
d
log(log(T
)d/Î´)
d
ï£¸.
Ïƒ+
âˆ€t â‰¥ t0
|hxt , wâˆ— âˆ’ wt i| â‰¤ kxt kâˆž kwt âˆ’ wâˆ— k0 Â· kwt âˆ’ wâˆ— k2 â‰¤ O ï£­k Â·
k0
t
k0
p
Inequality (i) holds since ha, bi â‰¤ ka(S)k2 Â· kbk2 with S being the support of b and ka(S)k2 â‰¤ kakâˆž |S|. Inequality
(ii) follows from Lemma 3 and Lemma 4, and a union bound over the dlog(T )e many times the vector wt is updated. Now,
for the left summand of the regret bound we have by Martingale concentration inequality that w.p. 1 âˆ’ Î´
ï£« v
ï£¶
u
T
T
X
X
u
2
2Î·t hxt , wt âˆ’ wâˆ— i â‰¤ O ï£­Ïƒ tlog(1/Î´)
hxt , wt âˆ’ wâˆ— i ï£¸
t=t0 +1

t=t0 +1

ï£« s
= O ï£­Ïƒ

d log(d log(T )/Î´)
log(1/Î´) log(T )k 2 Â·
k0

ï£¶

2
d
ï£¸.
Ïƒ+
k0

The right summand is bounded as
T
X
t=t0

d log(d log(T )/Î´)
hxt , w âˆ’ wt i = O k Â·
k0
+1
2

âˆ—

2



d
Ïƒ+
k0

2

!
Â· log(T ) .

Clearly, the right summand dominates the left one.
It remains to bound the regret in first t0 rounds. Since wt = 0 for t â‰¤ t0 , we have
Regrett0 (wâˆ— ) =

t0
X


 p
2
2Î·t hxt , wâˆ— i + (hxt , wâˆ— i) â‰¤ O Ïƒ t0 log(1/Î´) + t0 .

t=1
2

Here, we used that | hxt , wâˆ— i | â‰¤ 1 since kxt kâˆž â‰¤ 1 and kwâˆ— k1 â‰¤ 1. We also used that Î·t hxt , wâˆ— i âˆ¼ N (0, Ïƒ 2 hxt , wâˆ— i )
and Î·1 hx1 , wâˆ— i , Î·2 hx2 , wâˆ— i , . . . , Î·t0 hxt0 , wâˆ— i are independent. Thus their sum is a Gaussian with variance at most Ïƒ 2 t0 .
Collecting all the terms along with Lemma 16, bounding the difference RegretT âˆ’ RegretT (wâˆ— ), gives
!

2
p
d
2 d log(d log(T )/Î´)
RegretT â‰¤ t0 + t0 log(1/Î´) + k Â·
Ïƒ+
Â· log(T )
k0
k0

(13)

Lemma 16. In the realizable case, w.p. at least 1 âˆ’ Î´ we have for any sequence of wt that RegretT âˆ’ RegretT (wâˆ— ) =
O(Ïƒ 2 k log(d/Î´)).

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

Proof. It is an easy exercise to show that RegretT âˆ’ RegretT (wâˆ— ) is equal to the regret on an algorithm that always plays
wâˆ— . We thus continue to bound the regret of wâˆ— .
Let âˆ† âˆˆ Rd be the difference between wâˆ— and wÌƒ, the empirical optimal solution for the sparse regression problem. The
loss associated with wâˆ— is clearly kÎ·k2 , where Î· is the noise term y = Xwâˆ— + Î·. The loss associated with wÌƒ is
kX(wâˆ— + âˆ†) âˆ’ Xwâˆ— âˆ’ Î·k2 = kÎ· âˆ’ Xâˆ†k2 = kÎ· âˆ’ XSÌƒ âˆ†k2
where SÌƒ is the support of âˆ†, having a cardinality of at most 2k. The closed form solution for the least-squares problem
dictates that
kÎ· âˆ’ XSÌƒ âˆ†k2 â‰¥ kÎ· âˆ’ XSÌƒ XSÌƒâ€  Î·k2 = kÎ·k2 âˆ’ kXSÌƒ XSÌƒâ€  Î·k2 .
Here, Aâ€  is the pseudo inverse of a matrix A and XS is the matrix obtained from the columns of X whose indices are in
S. It follows that the regret of wâˆ— is bounded by
kXSÌƒ XSÌƒâ€  Î·k2
for some subset SÌƒ of size at most 2k. To bound this quantity we use a high probability bound for kXS XSâ€  Î·k2 for a fixed
set S, and take a union bound over all possible sets of cardinality 2k. For a fixed set S we have that kXS XSâ€  Î·k2 /Ïƒ 2 is
distributed according to the Ï‡22k distribution. The tail bounds of this distribution suggest that
i
h
âˆš
Pr kXS XSâ€  Î·k2 > 2kÏƒ 2 + 2Ïƒ 2 2kx + 2Ïƒ 2 x â‰¤ exp(âˆ’x)
meaning that with probability at least 1 âˆ’ Î´/d2k we have
p
kXS XSâ€  Î·k2 < 2kÏƒ 2 + 2Ïƒ 2 2k Â· 2k Â· log(d/Î´) + 2Ïƒ 2 Â· 2k Â· log(d/Î´) = O(Ïƒ 2 k log(d/Î´))
Taking a union bound over all possible subsets of size â‰¤ 2k we get that w.p. at least 1 âˆ’ Î´ the regret of wâˆ— is at most
O(Ïƒ 2 k log(d/Î´)).

B. Proofs for Agnostic Setting
We begin with an auxiliary lemma for Lemma 10, informally proving that for any matrix XÌ„ with BBRCNP (Definition 6)
and vector y, the set function
g(S) = inf ky âˆ’ XÌ„wk2
wâˆˆRS

is weakly supermodular. Its proof can be found in (Boutsidis et al., 2015), yet for completeness we provide it here as well.
Lemma 17. [Lemma 5 in (Boutsidis et al., 2015)] Let XÌ„ be a matrix whose columns have 2-norm at most 1 and y be a
vector with kykâˆž â‰¤ 1 of dimension matching the number of rows in X. the set function
g(S) = inf ky âˆ’ Xwk2
wâˆˆRS

is Î±-weakly supermodular for sparsity k for Î± = maxS:|S|â‰¤k 1/Ïƒmin (XS )2 , where XS is the submatrix of X obtained by
choosing the columns indexed by S, and Ïƒmin (A) is the smallest singular value of A.
Proof. Firstly, the well known closed form solution for the least-squares problem informs us that
g(S) = inf ky âˆ’ Xwk2 ,
wâˆˆRS
T

= y [I âˆ’ (XST )â€  XST ]y.
We
notation Aâ€  for the pseudoinverse
P use the
P âˆ’1 Tof a matrix A. That is, if the singular value decomposition of A is A =
T
â€ 
Ïƒ
u
v
with
Ïƒ
>
0
then
A
=
i
i i i i
i Ïƒi vi ui .
Let us first estimate g(S) âˆ’ g(T ), for sets S âŠ‚ T . For brevity, define HS as the projection matrix XS XSâ€  projecting onto
the column space of XS . Denote by ZT \S the matrix whose columns are those of XT \S projected away from the span of
XS , and normalized. Namely, writing xi as the iâ€™th column of X, Î¶i = k(I âˆ’ HS )xi k, zi = (I âˆ’ HS )xi /Î¶i , and ZT \S â€™s

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

columns are {zi }iâˆˆT \S . Notice that the columns of ZT \S and XS are orthogonal, hence according to the Pythagorean
theorem it holds that
g(S) = kyk2 âˆ’ kHS yk2 , g(T ) = kyk2 âˆ’ kHS yk2 âˆ’ kZT \S ZTâ€  \S yk2
/ S it holds that g(S)âˆ’g(S âˆª{j}) =
meaning that g(S)âˆ’g(T ) = kZT \S ZTâ€  \S yk2 . In particular, this implies that for any j âˆˆ
T
2
(zj y) , since zj is a unit vector. Let us now decompose g(S) âˆ’ g(T ).
g(S) âˆ’ g(T ) = kZT \S ZTâ€  \S yk2 = k(ZTT \S )â€  ZTT \S yk2 â‰¤ k(ZTT \S )â€  k2 Â· kZTT \S yk2
The norm used in the last inequality is the matrix operator norm. We now bound both factors of the product on the RHS
separately. For the first factor, we claim that k(ZTT \S )â€  k = kZTâ€  \S k â‰¤ kXTâ€  k. To see this, consider a vector w âˆˆ R|T \S| ,
P
for convenience denote its entries by {w(i)}iâˆˆT \S , and write zi = (xi âˆ’ jâˆˆS Î±ij xj )/Î¶i . We have
ZT \S w =

X

zi w(i) =

iâˆˆT \S

X

xi w(i)/Î¶i âˆ’

X
jâˆˆS

iâˆˆT \S

xj

X

w(i)Î±ij /Î¶i = XT w0

iâˆˆT \S

for the vector w0 âˆˆ R|T | defined as w0 (i) = w(i)/Î¶i for i âˆˆ T \ S and w0 (j) = âˆ’

P

iâˆˆT \S

w(i)Î±ij /Î¶i for j âˆˆ S. Since

Î¶i â‰¤ kxi k â‰¤ 1 we must have kw k â‰¥ kwk. Consider now the unit vector w for which kZT \S wk = kZTâ€  \S kâˆ’1 , that is, the
unit norm singular vector corresponding to the smallest non-zero singular value of ZT \S . For this w, and its corresponding
vector w0 , we have
0

kZTâ€  \S kâˆ’1 = kZT \S wk = kXT w0 k â‰¥ Ïƒmin (XT )kw0 k â‰¥ Ïƒmin (XT )kwk = Ïƒmin (XT ).
It follows that
k(ZTT \S )â€  k2 = kZTâ€  \S k2 â‰¤ 1/Ïƒmin (XT )2
We continue to bound the right factor of product.
X
X
kZTT \S yk2 =
(ziT y)2 =
g(S) âˆ’ g(S âˆª {i}).
iâˆˆT \S

iâˆˆT \S

By combining the inequalities we obtained the required result:
g(S) âˆ’ g(T ) â‰¤ 1/Ïƒmin (XT )2

 X

g(S) âˆ’ g(S âˆª {i}).

iâˆˆT \S

Proof of Lemma 10. We would like to apply Lemma 17 on the design matrix X. The only catch is that the columns of X
may not be bounded by 1 in norm. To remedy this, let j be the index of the column with the maximum norm and consider
the matrix XÌ„ = kX1j k X instead (here, Xj is the j-th column of X; note that Xj = Xej for the j-th standard basis vector
ej ). Now, for any subset S of coordinates,
inf ky âˆ’ XÌ„wk2 = inf ky âˆ’ Xwk2 .

wâˆˆRS

wâˆˆRS

Thus, we conclude that the set function of interest, g(S) = inf wâˆˆRS ky âˆ’ Xwk2 , is Î±-weakly supermodular for sparsity k
for Î± = maxS:|S|â‰¤k kXÌ„Sâ€  k22 . For any subset of coordinates S of size at most k, let w be a unit norm right singular vector of
kXe k
XÌ„S corresponding to the smallest singular value, so that kXÌ„Sâ€  k2 = kXÌ„S1 wk . But kXÌ„S1 wk = kXwj0 k , where w0 is the vector
w extended to all coordinates by padding with zeros.
Since the restricted condition number of X for sparsity k is bounded by Îº we conclude that
holds for any subset S of size at most k, we conclude that Î± â‰¤ Îº2 .

kXej k
kXw0 k

â‰¤ Îº. Since this bound

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

Proof of Lemma 11. By the Î±-weak supermodularity of g, we have
X
g(âˆ…) âˆ’ g(V ) â‰¤ Î± Â·
[g(âˆ…) âˆ’ g({j})]
jâˆˆV

â‰¤ Î±|V | Â· [(g(âˆ…) âˆ’ g(V )) âˆ’ (g({j âˆ— }) âˆ’ g(V ))].
Rearranging, we get the claimed bounds.
The following lemma gives a useful property of weakly supermodular functions.
Lemma 18. Let g(Â·) be a (k, Î±)-weakly supermodular set function and U be a subset with |U | < k. Then g 0 (S) :=
g(U âˆª S) is (k âˆ’ |U |, Î±)-weakly supermodular.
Proof. For any two subsets S âŠ† T with |T | â‰¤ k âˆ’ |U |, we have
X

g 0 (S) âˆ’ g 0 (T ) = g(U âˆª S) âˆ’ g(U âˆª T ) â‰¤ Î±

[g(U âˆª S) âˆ’ g(U âˆª S âˆª {j})]

jâˆˆ(T âˆªU )\(SâˆªU )

â‰¤Î±

X

X

[g(U âˆª S) âˆ’ g(U âˆª S âˆª {j})] = Î±

[g 0 (S) âˆ’ g 0 (S âˆª {j})].

jâˆˆT \S

jâˆˆT \S

(i)

(i)

(i)

Proof of Lemma 12. For i âˆˆ {0, 1, . . . , k1 }, define the set function gb as gb (S) = gb (S âˆª Vb ).
First, we analyze the performance of the BEXP algorithms. Fix any i âˆˆ [k1 ] and consider BEXP(i) . Conceptually, for any
(iâˆ’1)
(i)
j âˆˆ [d], the loss of expert j at the end of mini-batch b is gb (Vb
âˆª j) (note that this loss is only evaluated for j âˆˆ Ub
in the algorithm).P
To bound the regret, we need to bound the magnitude of the losses. Note that for any subset S, we have
0 â‰¤ gb (S) â‰¤ B1 tâˆˆTb yt2 â‰¤ 1. Thus, the regret guarantee of BEXP (Theorem 8) implies that for any i âˆˆ [k1 ] and any
j âˆˆ [d], we have
ï£®
ï£¹
T /B
T /B
q
X
X
(iâˆ’1)
(i)
(iâˆ’1)
Eï£°
gb (Vb
âˆª {jb })ï£» â‰¤
gb (Vb
âˆª {j}) + 2 dk1 klog(d)T
.
0B
b=1

b=1
(iâˆ’1)

The expectation above is conditioned on the randomness in Vb
, for b âˆˆ [T /B]. Rewriting the above inequality using
(iâˆ’1)
(i)
(i)
(iâˆ’1)
(i)
the g
and g functions, and using the fact that Vb
âˆª {jb } = Vb , we get
ï£®
ï£¹
T /B
T /B
q
X (i)
X (iâˆ’1)
Eï£°
gb (âˆ…)ï£» â‰¤
gb
({j}) + 2 dk1 klog(d)T
.
(14)
0B
b=1

b=1

Next, since we assumed that the sequence of feature vectors satisfies BBRCNP with parameters (, k1 + k), Lemma 10
1+
. By Lemma 18, the set
implies that the set function gb defined in (6) is (k1 + k, Îº2 )-weakly supermodular for Îº = 1âˆ’
(i)

(i)

function gb is (k, Îº2 )-weakly supermodular (since |Vb | â‰¤ k1 ).
It is easy to check that the sum of weakly supermodular functions is also weakly supermodular (with the same parameters),
PT /B (iâˆ’1)
PT /B (iâˆ’1)
and hence b=1 gb
is also (k, Îº2 )-weakly supermodular. Hence, by Lemma 11, if j âˆ— = arg minj b=1 gb
({j}),
we have, for any subset V of size at most k,
T /B

X

(iâˆ’1)

gb

(iâˆ’1)

({j âˆ— }) âˆ’ gb

(V ) â‰¤ (1 âˆ’

b=1
(iâˆ’1)

Since gb (V ) â‰¥ gb (V âˆª Vb

(iâˆ’1)

) = gb

b=1

(iâˆ’1)

gb

(iâˆ’1)

âˆ’ gb

(V )].

(V ), the above inequality implies that

T /B

X

T /B
X (iâˆ’1)
1
)[
gb
(âˆ…)
Îº2 |V |
b=1

({j âˆ— }) âˆ’ gb (V ) â‰¤ (1 âˆ’

T /B
X (iâˆ’1)
1
)[
gb
(âˆ…)
Îº2 |V |
b=1

âˆ’ gb (V )].

Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP

Combining this bound with (14) for j = j âˆ— , we get
ï£®
ï£¹
T /B
X (i)
Eï£°
gb (âˆ…) âˆ’ gb (V )ï£» â‰¤ (1 âˆ’
b=1

T /B
X (iâˆ’1)
1
)[
gb
(âˆ…)
Îº2 |V |
b=1

âˆ’ gb (V )] + 2

q

dk1 log(d)T
k0 B

.

Applying this bound recursively for i âˆˆ [k1 ] and simplifying, we get
ï£®
ï£¹
T /B
T /B
q
X (0)
X (k )
gb (âˆ…) âˆ’ gb (V )] + 2Îº2 |V | dk1 klog(d)T
.
Eï£°
gb 1 (âˆ…) âˆ’ gb (V )ï£» â‰¤ (1 âˆ’ Îº21|V | )k1 [
0B
b=1

b=1
(k1 )

Using the definitions of gb

(0)

and gb , and the fact that |V | â‰¤ k, we get the claimed bound.

