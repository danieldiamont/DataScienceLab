Differentially Private Learning of Graphical Models

A. Extra Proofs
Proof of Proposition 3. It is well known that the local sensitivity of any contingency table with respect to our definition
of nbrs(X) is one. This is easy to see from the definition of nC following Eq. (2): each individual contributes a count of
exactly one to each clique contingency table. Since there are |C| tables, the local sensitivity is exactly |C| for all data sets,
and, therefore, the sensitivity is the same.
Proof of Proposition 4. Note that nC (iC ) is a sum of N iid indicator variables, so nC (iC ) ⇠ Binomial N, µC (iC ) , and
Var nc (iC ) = N µC (iC ) 1 µC (iC ) . Now let z ⇠ Laplace(|C|/✏) and write:
µ̄C (iC ) =

1
nC (iC ) + z
N

⇥
⇤
Recall that E[z] = 0 and Var(z) = 2|C|2 /✏2 . We see immediately that E[µ̄C (iC )] = E nC (iC )/N = µC (iC ). Therefore,
the estimator is unbiased and its mean-squared error is equal to its variance. Since nC (iC ) and z are independent, we have:
Var nC (iC )
Var(z)
+
2
N
N2
µC (iC ) 1 µC (iC )
2|C|2
=
+ 2 2
N
N ✏

Var µ̄C (iC ) =

ˆ converges to p(x; ✓) follows from Proposition 2 and the consistency of the marginals, as long as the
The fact that p(x; ✓)
true marginals µ lie in the interior of the marginal polytope M. However, this is guaranteed because the true distribution
p(x; ✓) is strictly positive.
Proof of Proposition 5. After applying Stirling’s approximation to log p(n; ✓) we obtain (Nguyen et al., 2016):
X
X
log h(n) ⇡ H(n) = N log N +
ĤC
⌫(S)ĤS
C2C

where we define ĤA =
rewrite it as:

P

iA 2X |A|

ĤA =
=

(7)

S2S

nA (iA ) log nA (iA ) for any A 2 C [ S. The term ĤA is a scaled entropy. We can
⇣ n (i )
⌘
A A
·N
N
N
iA
X
X
N
µ̂A (iA ) log µ̂A (iA ) N
µ̂A (iA ) log N
N

X nA (iA )

log

iA

= N HA

iA

N log N

where HA is now the entropy of the empirical marginal distribution µ̂A = nA /N . Since the total multiplicity of the
separators is one less than the number of cliques, when we substitute back into Eq. (7), all of the N log N terms cancel,
and we are left only with
⇣ X
⌘
X
H(n) = N ·
HA
⌫(S)HA
C2C(T )

S2S(T )

But, from standard arguments about the decomposition of entropy on junction trees, the term in parentheses is exactly the
entropy of distribution q defined as:
Y Y
µ̂C (xC )
C2C iC 2X |C|

q(x) = Y

Y

S2S iS 2X |S|

µ̂S (xS )⌫(S)

,

which factors according to C and can be written as p(x; ✓) for parameters ✓ derived from the marginal probabilities.
Although the mapping from parameters to distributions is many-to-one, for any maginals µ̂, there is a unique distribution
p(x; ✓) in the model family that has marginals µ̂ (Wainwright & Jordan, 2008), so this uniquely defines q(x) as stated in
the Proposition.

