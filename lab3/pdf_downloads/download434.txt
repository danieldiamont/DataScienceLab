Regularising Non-linear Models Using Feature Side-information

6. Appendix
6.1. Modified Backpropogation
For notion simplicity, we consider stochastic gradient descent. The objective function we want to minimize is as
following:
E = L(y, φ(x)) + λ1

! ∂φ(x) ∂φ(x)
||
−
||2 Sij (12)
∂x
∂x
i
j
ij

Notice that the objective function includes derivative of the
learned function with respect to the input features, if we use
neural network to learn the model, the conventional backpropagation algorithm can’t be applied directly. Therefore,
we developed a modified version of the backpropagation
algorithm to find the gradient of the objective.
We keep the notation consistent with the notation used in
the book of (Bishop, 1995). n is the total layers (including
input and out put layer) number of the network, ak is the
pre-activation units in layer k, k1 is the number of hidden
units in hidden layer k, m is the number of output units,
and h(x) stands for the non-linear activation function.
ak = wk zk−1 + bk

G1mg =

∂a1m
=
∂a1g

(

1 if m=g
0 others

(19)

And Gk for all k can be achieved during forward path by
the following forward propagation equation and G1
!
k
′ k−1
Gkmg =
Wml
Gk−1
) ∀k = 2, 3, ..., n (20)
lg h (al
l

k
Bljg
=

δ for all k can be achieved by the following backpropagation equation.

n
Bljg
=

Defining the term δ in such a away, we can rewrite the regularizer term in equation (12) as following:
!
||(W1 (:, i)) − W1 (:, j))T δ 1 ||2 Sij
(17)

(21)

n
∂δlj
= h′′ (anl )1lj Gnlg
∂a1g

(22)

Bk for all k can be obtained by the following propagating
equation during backward path using Bn as following:

k
Bljg
= h′′ (akl )Gklg

δ k = ((Wk+1 )T δ k+1 )⊙h′ (ak ) ∀k = 1, 2, ..., n−1 (15)
Where ⊙ stands for the element wise multiplication of a
column vector to every column of the matrix.
⎡ ′ n
⎤
h (a1 )
0
...
0
⎢ 0
h′ (an2 ) ...
0 ⎥
⎥
δn = ⎢
(16)
⎣ ...
⎦
...
...
′ n
0
0
... h (am )

k
∂δlj
∀k = 1, 2, ..., n
∂a1g

We know that:

k1

k

ij

(18)

We know that:

n

To find the gradient of (12), we define δ k as the Jacobian
of the learned function with respect to pre-activations at the
layer k:
⎡ ∂φ
⎤
∂φ2
1
m
· · · . ∂φ
h
k
∂ak
∂a
∂a
1
1
⎢ ∂φ11
∂φ2
∂φm ⎥
⎢ ∂ak
···
k
k ⎥
∂a
∂a
⎢
2
2
2 ⎥
δk = ⎢ .
(14)
⎥
.
.
..
..
⎢ ..
⎥
⎣
⎦
∂φ1
∂φ2
∂φm
···
∂ak
∂ak
∂ak
k1

∂akm
∀k = 1, 2, 3, ..., n
∂a1g

(13)

zk = h(ak )

k1

Gkmg =

Define Bk which gives the derivative of the δ k with respect
to the pre-activation units in the first hidden layers:

z0 = x

φ(x) = z

If the network only has one hidden layer, we can derive
derivative of the regularizer with respect to weights using
δ and (15). When hidden layer’s number is more than one,
we need to introduce two more term, one to the backward
path and one to the forward path: Define Gk as the jacobian of pre-activation unit at layer k with respect to preactivation at first hidden layer, note layer k = 1 corresponding to first hidden layer.

)

k+1 k+1
p δpj Wpl

+ h′ (akl )

)

k+1 k+1
p Wpl Bpjg

(23)

∀k = 1, 2, ..., n − 1
Finally, the gradient of the regularizer, i.e. second term of
the equation (12), can be calculated as follwoing:
For k = 1, i.e. first hidden layer:
)
) 1
∂R
1
T 1
1
1 = 4λ1
s Sms j (W (:, m) − W (:, s)) δ (:, j)δ (lj)
∂Wlm
(24)
)
)
)
1 0
+2λ1 ks Sks j (W1 (:, k) − W 1 (:, s))T δ1 (:, j) g (W 1 (g, k) − W 1 (g, s))Bljg
zm )
For k = 2, ..., n:
∂R
k
∂Wlm

)

= 2λ1

g (W

1

)

ks Sks

)

j (W

1

(:, k) − W1 (:, s))T δ 1 (:, j)

k−1 k
k−1
(g, k) − W 1 (g, s))(zm
Bljg + δljk h′ (ak−1
m )Gmg )

(25)

Regularising Non-linear Models Using Feature Side-information

Gradient with respect to bias term, for all k = 1, ..., n:
!
!
∂R
=
2λ
S
(W1 (:, k) − W1 (:, s))T δ 1 (:, j)
1
ks
∂bklm
j
ks
(26)
!
1
k
(W (g, k) − W 1 (g, s))Bljg
g

The gradient of the first part of the objective which is some
loss function we chose, is same as in the standard Backpropagation algorithm, here we just need to rewrite it in
terms of the newly defined δ. For example, if we use sigmoid on all layers as activation function and cross entropy
loss, we have the following:
E=−

m
!
i=1

(yi log φ(x)i + (1 − yi ) log(1 − φ(x)i ))

∂E
φ−y
= δk
(zk−1 )T
k
∂W
φ(1 − φ)
∂E
φ−y
= δk
∂bk
φ(1 − φ)

(27)
(28)
(29)

Now we can find the gradient of the loss with respect
to weights in all layers. Compared to the conventional
back propagation algorithm, except we have δ term which
is defined differently than the conventional backprop
algorithm, we have one more extra term Bk to add to the
backward path and one more term Gh to the forward path.

