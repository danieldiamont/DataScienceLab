Convolutional Sequence to Sequence Learning

A. Weight Initialization
We derive a weight initialization scheme tailored to the GLU
activation function similar to Glorot & Bengio (2010); He et al.
(2015b) by focusing on the variance of activations within the
network for both forward and backward passes. We also detail
how we modify the weight initialization for dropout.

With (7) and V ar[yla 1]=V ar[ylb 1]=V ar[yl 1], this results
in
⇥ ⇤ 1
⇥
⇤2 1
⇥
⇤
V ar xl  V ar yl 1 + V ar yl 1 .
(15)
16
4
We initialize the embedding matrices in our network with small
variances (around 0.01), which allows us to dismiss the quadratic
term and approximate the GLU output variance with

A.1. Forward Pass
Assuming that the inputs xl of a convolutional layer l and its
weights Wl are independent and identically distributed (i.i.d.),
the variance of its output, computed as yl =Wlxl +bl , is
⇥ ⇤
⇥
⇤
V ar yl =nl V ar wl xl
(3)
where nl is the number inputs to the layer. For one-dimensional
convolutional layers with kernel width k and input dimension
c, this is kc. We adopt the notation in (He et al., 2015b), i.e. yl ,
wl and xl represent the random variables in yl , Wl and xl . With
wl and xl independent from each other and normally distributed
with zero mean, this amounts to
⇥ ⇤
⇥ ⇤
⇥ ⇤
V ar yl =nl V ar wl V ar xl .
(4)
xl is the result of the GLU activation function yla 1 (ylb 1) with
yl 1 = (yla 1,ylb 1), and yla 1,ylb 1 i.i.d. Next, we formulate
upper and lower bounds in order to approximate V ar[xl ]. If
yl 1 follows a symmetric distribution with mean 0, then
⇥ ⇤
⇥
⇤
V ar xl =V ar yla 1 (ylb 1)
(5)
⇥ a
⇤
⇥
⇤
2
=E yl 1 (ylb 1)
E 2 yla 1 (ylb 1)
(6)
⇥
⇤
a
b
2
=V ar[yl 1]E (yl 1) .
(7)

A lower bound is given by (1/4)V ar[yla 1] when expanding
(6) with E 2[ (ylb 1)]=1/4:
⇥ ⇤
⇥
⇤
V ar xl =V ar yla 1 (ylb 1)
(8)
⇥ a ⇤ 2⇥
⇤
b
=V ar yl 1 E (yl 1) +
(9)
⇥
⇤
⇥
⇤
V ar yla 1 V ar (ylb 1)
⇥
⇤
⇥
⇤
⇥
⇤
1
= V ar yla 1 +V ar yla 1 V ar (ylb 1) (10)
4
and V ar[yla 1]V ar[ (ylb 1)] > 0. We utilize the relation
(x)2  (1/16)x2 1/4 + (x) (Appendix B) to provide an
upper bound on E[ (x)2]:
⇥1 2
x
16
1
= E[x2]
16

E[ (x)2]E

⇤
1
+ (x)
4
1
+E[ (x)]
4

(11)
(12)

With x⇠N (0,std(x)), this yields
E

⇥

⇤ 1 ⇥ ⇤ 1 1
(x)2  E x2
+
16
4 2
⇥ ⇤ 1
1
= V ar x + .
16
4

(13)
(14)

1
V ar[xl ]⇡ V ar[yl
4

1 ].

(16)

If L network layers of equal size and with GLU activations are
combined, the variance of the final output yL is given by
0
1
L
Y
1
V ar[yL]⇡V ar[y1]@
nl V ar[wl ]A.
(17)
4
l=2

Following (He et al., 2015b), we aim to satisfy the condition
⇥ ⇤
1
nl V ar wl =1,8l
4

(18)

so that the activations in a network are neither exponentially
magnified
p nor reduced. This is achieved by initializing Wl from
N (0, 4/nl ).

A.2. Backward Pass

The gradient of a convolutional layer is computed via backpropagation as xl = Ŵlyl . Considering separate gradients
yla and ylb for GLU, the gradient of x is given by
xl = Ŵla yla + Ŵlb ylb.

(19)

Ŵ corresponds to W with re-arranged weights to enable
back-propagation. Analogously to the forward pass, xl , ŵl
and yl represent the random variables for the values in xl,
Ŵl and yl, respectively. Note that W and Ŵ contain the same
values, i.e. ŵ =w. Similar to (3), the variance of xl is
⇣
⌘
V ar[ xl ]= n̂l V ar[wla]V ar[ yla]+V ar[wlb]V ar[ ylb] .
(20)
Here, n̂l is the number of inputs to layer l +1. The gradients
for the GLU inputs are:
yla = xl+1 (ylb) and
ylb =

xl+1yla 0(ylb).

(21)
(22)

The approximation for the forward pass can be used for
V ar[ yla], and for estimating V ar[ ylb] we assume an upper
bound on E[ 0(ylb)2] of 1/16 since 0(ylb)2[0, 14 ]. Hence,
1
1
V ar[ xl+1] V ar[ xl+1]V ar[ylb)]
4
16
(23)
1
V ar[ ylb]
V ar[ xl+1]V ar[yla]
(24)
16

V ar[ yla]

Convolutional Sequence to Sequence Learning

We observe relatively small gradients in our network, typically
around 0.001 at the start of training. Therefore, we approximate
by discarding the quadratic terms above, i.e.
1
V ar[ yla]⇡ V ar[ xl+1]
4
V ar[ ylb]⇡0
1
V ar[ xl ]⇡ n̂l V ar[wla]V ar[ xl+1]
4

(25)
(26)
(27)

As for the forward pass, the above result can be generalized to
backpropagation through many successive layers, resulting in
0
1
L
Y
1
V ar[ x2]⇡V ar[ xL+1]@
n̂l V ar[wla]A
4

1
nl V ar[wl ]V ar[xl ] and
4p
1
V ar[ xl ]⇡ nl V ar[wla]V ar[ xl+1].
4p

V ar[xl+1]⇡

(32)
(33)

This amounts to a modified initialization of Wl from ap
normal
distribution with zero mean and a standard deviation of 4p/n.
For layers without a succeedingpGLU activation function,
we initialize weights from N (0, p/n) to calibrate for any
immediately preceding dropout application.

B. Upper Bound on Squared Sigmoid
(28)

l=2

and a similar condition, i.e. (1/4)n̂l V ar[wla] = 1. In the
networks we consider, successions of convolutional layers usually
operate on the same number of inputs so that most cases nl = n̂l .
Note that Wlb is discarded in the approximation; however, for
the sake of consistency we use the same initialization for Wla
and Wlb.
For arbitrarily large variances of network inputs and activations,
our approximations are invalid; in that case, the initial values for
Wla and Wlb would have to be balanced for the input distribution
to be retained. Alternatively, methods that explicitly control
the variance in the network, e.g. batch normalization (Ioffe &
Szegedy, 2015) or layer normalization (Ba et al., 2016) could
be employed.
A.3. Dropout
Dropout retains activations in a neural network with a probability
p and sets them to zero otherwise (Srivastava et al., 2014). It is
common practice to scale the retained activations by 1/p during
training so that the weights of the network do not have to be
modified at test time when p is set to 1. In this case, dropout
amounts to multiplying activations x by a Bernoulli random variable r where Pr[r =1/p]=p and Pr[r =0]=1 p (Srivastava
et al., 2014). It holds that E[r] = 1 and V ar[r] = (1 p)/p. If
x is independent of r and E[x]=0, the variance after dropout is
V ar[xr]=E[r]2V ar[x]+V ar[r]V ar[x]
✓
◆
1 p
= 1+
V ar[x]
p
1
= V ar[x]
p

now be approximated with

(29)
(30)
(31)

Assuming that a the input of a convolutional layer has been
subject to dropout with a retain probability p, the variations of
the forward and backward activations from§A.1 and§A.2 can

The sigmoid function (x) can be expressed as a hyperbolic
tangent by using the identity tanh(x) = 2 (2x) 1. The
derivative of tanh is tanh0(x) = 1 tanh2(x), and with
tanh(x)2[0,1],x 0 it holds that
Z

0

tanh0(x)1,x 0
Z x
x
0
tanh (x)dx
1dx

(34)
(35)

0

tanh(x)x,x 0

(36)

We can express this relation with (x) as follows:
1
2 (x) 1 x,x 0
2

(37)

Both terms of this inequality have rotational symmetry w.r.t 0,
and thus
✓ ◆2
1
2
2 (x) 1 
x 8x
(38)
2
1
1
, (x)2  x2
+ (x).
(39)
16
4

C. Attention Visualization
Figure 3 shows attention scores for a generated sentence from
the WMT’14 English-German task. The model used for this plot
has 8 decoder layers and a 80K BPE vocabulary. The attention
passes in different decoder layers capture different portions of
the source sentence. Layer 1, 3 and 6 exhibit a linear alignment.
The first layer shows the clearest alignment, although it is slightly
off and frequently attends to the corresponding source word of
the previously generated target word. Layer 2 and 8 lack a clear
structure and are presumably collecting information about the
whole source sentence. The fourth layer shows high alignment
scores on nouns such as “festival”, “way” and “work” for both the
generated target nouns as well as their preceding words. Note that
in German, those preceding words depend on gender and object
relationship of the respective noun. Finally, the attention scores in
layer 5 and 7 focus on “built”, which is reordered in the German
translation and is moved from the beginning to the very end of

Convolutional Sequence to Sequence Learning

the sentence. One interpretation for this is that as generation
progresses, the model repeatedly tries to perform the re-ordering.
“aufgebaut” can be generated after a noun or pronoun only, which
is reflected in the higher scores at positions 2, 5, 8, 11 and 13.

Convolutional Sequence to Sequence Learning
Layer 1

Layer 2

Layer 3

Layer 4

Layer 5

Layer 6

Layer 7

Layer 8

Figure 3. Attention scores for different decoder layers for a sentence translated from English (y-axis) to German (x-axis). This model uses 8 decoder
layers and a 80k BPE vocabulary.

