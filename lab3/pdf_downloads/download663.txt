Supplementary Material:
Adaptive Consensus ADMM for Distributed Optimization

Zheng Xu 1 Gavin Taylor 2 Hao Li 1 MaÌrio A. T. Figueiredo 3 Xiaoming Yuan 4 Tom Goldstein 1

This is the supplemental material for Adaptive Consensus ADMM (ACADMM) (Xu et al., 2017c). We provide
details of proofs and experimental settings, in addition to
more results. Our proof generalizes the variational inequality approach in (He et al., 2000; He & Yuan, 2012; 2015;
Xu et al., 2017b).

1. Proof of lemmas

Let y = y âˆ— , z = z âˆ— in VI (S4), and y = y k+1 , z = z k+1 in
VI (13), and sum the two equalities together to get
âˆ—
(âˆ†zk+1
)T â„¦(âˆ†zk+ , T k ) â‰¥

(S5)

âˆ—
(âˆ†zk+1
)T (F (z âˆ— ) âˆ’ F (z k+1 )).

Since F (z) is monotone, the right hand side is nonnegative. Now, substitute â„¦(âˆ†zk+ , T k ) into (S5) to get
âˆ’ (Aâˆ†uâˆ—k+1 )T T k (Bâˆ†vk+ )

1.1. Proof of Lemma 1 (17)

(S6)

+ (âˆ†Î»âˆ—k+1 )T (T k )âˆ’1 âˆ†Î»+
k â‰¥ 0.

Proof. By using the updated dual variable Î»k+1 in (10), VI
(15) can be rewritten as
âˆ€v, g(v) âˆ’ g(v k+1 ) âˆ’ (Bv âˆ’ Bv k+1 )T Î»k+1 â‰¥ 0. (S1)

If we use the feasibility constraint of optimal solution
(Auâˆ— + Bv âˆ— = b) and the dual update formula (10), we
have
k
âˆ—
T k Aâˆ†uâˆ—k+1 = âˆ†Î»+
(S7)
k âˆ’ T Bâˆ†vk+1 .
Substitute this into (S6) yields

Similarly, in the previous iteration,

âˆ—
(Bâˆ†vk+1
)T T k Bâˆ†vk+ + (âˆ†Î»âˆ—k+1 )T (T k )âˆ’1 âˆ†Î»+
k

âˆ€v, g(v) âˆ’ g(v k ) âˆ’ (Bv âˆ’ Bv k )T Î»k â‰¥ 0.

(S8)

â‰¥ (Bâˆ†vk+ )T âˆ†Î»+
k

(S2)

The proof (18) is concluded by applying (17) to (S8).
Let v = v k in (S1) and v = v k+1 in (S2), and sum the two
inequalities together. We conclude
(Bv k+1 âˆ’ Bv k )T (Î»k+1 âˆ’ Î»k ) â‰¥ 0.

(S3)

1.3. Proof of Lemma 1 (19)
Proof.
kâˆ†zkâˆ— k2H k = kz âˆ— âˆ’ z k k2H k
âˆ—

=

1.2. Proof of Lemma 1 (18)

=
Proof. VI (16) can be rewritten as
â‰¥

Ï†(y) âˆ’ Ï†(y k+1 )+
(z âˆ’ z

k+1 T

)

F (z

k+1

)+

â„¦(âˆ†zk+ , T k )



â‰¥ 0, (S4)

where â„¦(âˆ†zk+ , T k ) = (âˆ’AT T k Bâˆ†vk+ ; 0; (T k )âˆ’1 âˆ†Î»+
k ).
1
University of Maryland, College Park 2 United States Naval
Academy, Annapolis, 3 Universidade de Lisboa, Portugal 4 Hong
Kong Baptist University, Hong Kong. Correspondence to: Zheng
Xu <xuzhustc@gmail.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

(S9)
z k k2H k

(S10)

âˆ—
kâˆ†zk+1
+ âˆ†zk+ k2H k
âˆ—
kâˆ†zk+1
k2H k + kâˆ†zk+ k2H k
âˆ—
+ 2(âˆ†zk+1
)T H k âˆ†zk+
âˆ—
kâˆ†zk+1
k2H k + kâˆ†zk+ k2H k .

(S11)

= kz âˆ’ z

k+1

+z

k+1

âˆ’

(S12)
(S13)

Eq. (18) is used for the inequality in (S13), and Eq. (19)
is derived by rearranging the order of kâˆ†zkâˆ— k2H k â‰¥
âˆ—
kâˆ†zk+1
k2H k + kâˆ†zk+ k2H k .
1.4. Proof of Lemma 2
Proof. Applying the observation
1
(ka âˆ’ dk2H âˆ’ ka âˆ’ ck2H )
2
(S14)
1
+ (kc âˆ’ bk2H âˆ’ kc âˆ’ dk2H ),
2

(a âˆ’ b)T H(c âˆ’ d) =

Supplementary Material for Adaptive Consensus ADMM

we have

Hence
âˆ—
kâˆ†zk+1
k2H k â‰¤(1 + (Î· k )2 )kâˆ†zkâˆ— k2H kâˆ’1

(zÌƒ k+1 âˆ’ z)T H k âˆ†zk+ = (zÌƒ k+1 âˆ’ z)H k (z k+1 âˆ’ z k ) (S15)
1
= (kzÌƒ k+1 âˆ’ z k k2H k âˆ’ kzÌƒ k+1 âˆ’ z k+1 k2H k )+
2
(S16)
1
(kz k+1 âˆ’ zk2H k âˆ’ kz k âˆ’ zk2H k ).
2

â‰¤
â‰¤

We now consider
kzÌƒ k+1 âˆ’ z k+1 k2H k = kzÌƒ k+1 âˆ’ z k + z k âˆ’ z k+1 k2H k (S17)
=kzÌƒ k+1 âˆ’ z k k2H k + kâˆ†zk+ k2H k âˆ’
2(zÌƒ k+1 âˆ’ z k )T H k âˆ†zk+ ,

=2(zÌƒ

k T

âˆ’z ) H

k

âˆ†zk+

âˆ’

kâˆ†zk+ k2H k .

(S19)

â‰¤

(S20)

(S21)

=(zÌƒ k+1 âˆ’ z k )T (2I âˆ’ M k )T H k M k (zÌƒ k+1 âˆ’ z k ) (S22)
=kÎ»Ì‚k+1 âˆ’ Î»k k2(T k )âˆ’1 â‰¥ 0.

(S23)

=

k
Y

(1 + (Î· t )2 )kz âˆ’ z âˆ— k2H 0

t=1
âˆ
Y

(1 + (Î· t )2 )kz âˆ’ z âˆ— k2H 0

t=1
CÎ·Î 

kz âˆ’ z âˆ— k2H 0 < âˆ.

kz âˆ’ z k k2H k â‰¤ (1 + (Î· k )2 )kz âˆ’ z k k2H kâˆ’1 .

(S33)
(S34)

(S35)
(S36)
(S37)
(S38)

(S39)

Then we have
l
X

1
(kz k+1 âˆ’zk2H k âˆ’kz k âˆ’zk2H k ). (S24)
2

(S32)

Let z 0 = z k in Lemma 3, we have

Combining (S16) and (S23), we conclude
(zÌƒ k+1 âˆ’z)T H k âˆ†zk+ â‰¥

kâˆ†z1âˆ— k2H 0 < âˆ.

kz âˆ’ z âˆ— k2H k â‰¤ (1 + (Î· k )2 )kz âˆ’ z âˆ— k2H kâˆ’1

(S18)

We then substitute âˆ†zk+ with M k (zÌƒ k+1 âˆ’ z k ) in (12),
kzÌƒ k+1 âˆ’ z k k2H k âˆ’ kzÌƒ k+1 âˆ’ z k+1 k2H k

(1 + (Î· t )2 )kâˆ†z1âˆ— k2H 0

(S31)

Let z 0 = z âˆ— in Lemma 3, we have

â‰¤

k+1

(1 + (Î· t )2 )kâˆ†z1âˆ— k2H 0

t=1
âˆ
Y

t=1
= CÎ·Î 

and get
kzÌƒ k+1 âˆ’ z k k2H k âˆ’ kzÌƒ k+1 âˆ’ z k+1 k2H k

k
Y

(kz âˆ’ z k k2H k âˆ’ kz âˆ’ z k k2H kâˆ’1 )

(S40)

k=1

â‰¤

l
X

(Î· k )2 kz âˆ’ z k k2H kâˆ’1

(S41)

k=1

1.5. Proof of Lemma 3

=

Proof. Assumption 1 implies (22), which suggests the diagonal matrices T k and T kâˆ’1 satisfy
k

k 2

T â‰¤(1 + (Î· ) )T
k âˆ’1

(T )

â‰¤

(S25)
â‰¤

(S26)

=kB(v âˆ’ v 0 )k2T k + kÎ» âˆ’ Î»0 k2(T k )âˆ’1
k 2

â‰¤(1 + (Î· ) )(kB(v âˆ’ v

0

)k2T kâˆ’1 +

kÎ» âˆ’ Î»0 k2(T kâˆ’1 )âˆ’1 )
â‰¤(1 + (Î· k )2 )kz âˆ’ z 0 k2H kâˆ’1 .

(S42)

l
X

(Î· k )2 (kz âˆ’ z âˆ— k2H kâˆ’1 + kâˆ†zkâˆ— k2H kâˆ’1 )

(S43)

k=1

Then we have
kz âˆ’ z 0 k2H k

(Î· k )2 kz âˆ’ z âˆ— + z âˆ— âˆ’ z k k2H kâˆ’1

k=1

kâˆ’1

â‰¤(1 + (Î· k )2 )(T kâˆ’1 )âˆ’1 .

l
X

(S27)
(S28)

â‰¤

l
X

(Î· k )2 (CÎ·Î  kz âˆ’ z âˆ— k2H 0 + CÎ·Î  kâˆ†z1âˆ— k2H 0 )

k=1
âˆ
X

(Î· k )2 (CÎ·Î  kz âˆ’ z âˆ— k2H 0 + CÎ·Î  kâˆ†z1âˆ— k2H 0 )

k=1
=CÎ·Î£ (CÎ·Î  kz âˆ’ z âˆ— k2H 0 + CÎ·Î  kâˆ†z1âˆ— k2H 0 )
=CÎ·Î£ CÎ·Î  (kz âˆ’ z âˆ— k2H 0 + kâˆ†z1âˆ— k2H 0 ) < âˆ.

(S44)
(S45)
(S46)
(S47)

(S29)

The inequality (S25) is used to get from (S27) to (S28).

1.7. Proof of equivalence of generalized ADMM and
DRS in Section 5.1

1.6. Proof of Lemma 4

Proof. The optimality condition for ADMM step (8) is

Proof. From (27) we know
âˆ—
kâˆ†zk+ k2H k +kâˆ†zk+1
k2H k â‰¤ (1+(Î· k )2 )kâˆ†zkâˆ— k2H kâˆ’1 . (S30)

0 âˆˆ âˆ‚f (uk+1 ) âˆ’ AT (Î»k + T k (b âˆ’ Auk+1 âˆ’ Bv k )),
|
{z
}
Î»Ì‚k+1

(S48)

Supplementary Material for Adaptive Consensus ADMM
3

3

10

ENReg-S1
ENReg-S2
Logreg-S1
Logreg-S2
SVM-S1
SVM-S2

Iterations

Iterations

10

2

10

101

2

10

ENReg-S1
ENReg-S2
Logreg-S1
Logreg-S2
SVM-S1
SVM-S2

0

0.2

0.4

0.6

0.8

1

1

10
102

Correlation threshold

104

106

108

1010

Convergence constant paramter

parameter cor .

which is equivalent to AT Î»Ì‚k+1 âˆˆ âˆ‚f (uk+1 ). By exploiting properties of the Fenchel conjugate (Rockafellar,
1970), we get uk+1 âˆˆ âˆ‚f âˆ— (AT Î»Ì‚k+1 ). A similar argument using the optimality condition for (9) leads to v k+1 âˆˆ
âˆ‚g âˆ— (B T Î»k+1 ). Recalling the definition of fË†, gÌ‚ in (42), we
arrive at
Auk+1 âˆ’ b âˆˆ âˆ‚ fË†(Î»Ì‚k+1 ) and Bv k+1 âˆˆ âˆ‚gÌ‚(Î»k+1 ). (S49)
k

1.8. Proposition for proof in Section 5.2
Proposition 1 (Spectral DRS (Xu et al., 2017a)). Suppose
the Douglas-Rachford splitting steps are used,
0 âˆˆ (Î»Ì‚k+1 âˆ’ Î»k )/Ï„ k + âˆ‚ fË†(Î»Ì‚k+1 ) + âˆ‚gÌ‚(Î»k )
0 âˆˆ (Î»k+1 âˆ’ Î»k )/Ï„ k + âˆ‚ fË†(Î»Ì‚k+1 ) + âˆ‚gÌ‚(Î»k+1 ),

ENRegression-Synthetic1

102

CADMM
RB-ADMM
AADMM
CRB-ADMM
ACADMM

âˆ‚gÌ‚(Î») = Î² Î» + Î¦,

101
10-2

10-1

100

101

102

Regularizer

Figure 3: ACADMM is robust to regularizer parameter Ï in EN
regression problem.

(S50)

3.1. Sampling data matrices from Gaussian(s)

(S51)

For Synthetic1, on each compute node i, we create a data
matrix Di âˆˆ Rni Ã—d with ni samples and d features using
a standard normal distribution. For Synthetic2, we build 10
Gaussian feature sets {Di }. On each node, we then randomly choose an index ji , and randomly select two Gaussian parameters Âµ1 , . . . , Âµ10 âˆˆ R and Ïƒ1 , . . . , Ïƒ10 âˆˆ R. We
then introduce heterogeneity across nodes by computing

and assume the subgradients are locally linear,
and

103

k+1

We can then use simple algebra to verify Î» , Î»Ì‚
in (10)
and âˆ‚ fË†(Î»Ì‚k+1 ), âˆ‚gÌ‚(Î»k+1 ) in (S49) satisfy the generalized
DRS steps (43, 44).

âˆ‚ fË†(Î»Ì‚) = Î± Î»Ì‚ + Î¨

Figure 2: ACADMM is robust to the convergence threshold Ccg .

Iterations

Figure 1: ACADMM is robust to the correlation threshold hyper-

(S52)

where Î±, Î² âˆˆ R, Î¨, Î¦ âŠ‚ Rp . Then, the minimal residual
âˆš
of fË†(Î»k+1 )+ gÌ‚(Î»k+1 ) is obtained by setting Ï„ k = 1/ Î± Î².

Di â† Di âˆ— Ïƒji + Âµji .

(S53)

2. More experimental results
We provide more experimental results demonstrating the
robustness of ACADMM in Fig. 1, Fig. 2 and Fig. 3.

3. Synthetic problems in experiments
We provide the details of the synthetic data used in our experiments.

3.2. Correlation for Elastic Net regression
Following standard method used to test elastic net regression in (Zou & Hastie, 2005), we introduce correlations
into the datasets. We start by building a random Gaussian dataset Di on each node. We then select the number
of active features as 0.6d. Then we randomly select three

Supplementary Material for Adaptive Consensus ADMM

Gaussian vectors vi,1 , vi,2 , vi,3 âˆˆ Rni . We then compute
âˆ€j âˆˆ{1, 2, . . . , 0.2d},
Di [:, j] â† Di [:, j] + vi,1 ,
âˆ€j âˆˆ{0.2d + 1, 0.2d + 2, . . . , 0.4d},
Di [:, j] â† Di [:, j] + vi,2 ,
âˆ€j âˆˆ{0.4d + 1, 0.4d + 2, . . . , 0.6d},
Di [:, j] â† Di [:, j] + vi,3 ,

(S54)
(S55)
(S56)

where Di [:, j] denotes the jth column of Di .
3.3. Regression measurement
We use a groundtruth vector x âˆˆ Rd , where the first 0.6d
features are 1 and the rest are 0, and generate measurements
for the regression problem as
Di x = ci

(S57)

where Di is random Gaussian.
3.4. Classification labels
For classification problems, we add a constant dconst to the
active features on half of the feature vectors stored on each
node. This means we compute
Di [0.5ni : ni , 1 : 0.6d] â† Di [0.5ni : ni , 1 : 0.6d]+dconst .
We then create a ground truth label vector ci âˆˆ Rni , which
contains 1 for the permuted feature vectors, and âˆ’1 for the
rest.

References
He, Bingsheng and Yuan, Xiaoming. On the o(1/n) convergence rate of the douglas-rachford alternating direction
method. SIAM Journal on Numerical Analysis, 50(2):
700â€“709, 2012.
He, Bingsheng and Yuan, Xiaoming. On non-ergodic convergence rate of Douglas-Rachford alternating direction
method of multipliers. Numerische Mathematik, 130:
567â€“577, 2015.
He, Bingsheng, Yang, Hai, and Wang, Shengli. Alternating
direction method with self-adaptive penalty parameters
for monotone variational inequalities. Jour. Optim. Theory and Appl., 106(2):337â€“356, 2000.
Rockafellar, R. Convex Analysis. Princeton University
Press, 1970.
Xu, Zheng, Figueiredo, Mario AT, and Goldstein, Thomas.
Adaptive ADMM with spectral penalty parameter selection. AISTATS, 2017a.

Xu, Zheng, Figueiredo, Mario AT, Yuan, Xiaoming, Studer,
Christoph, and Goldstein, Thomas. Adaptive relaxed
ADMM: Convergence theory and practical implementation. CVPR, 2017b.
Xu, Zheng, Taylor, Gavin, Li, Hao, Figueiredo, Mario AT,
Yuan, Xiaoming, and Goldstein, Tom. Adaptive consensus ADMM for distributed optimization. ICML, 2017c.
Zou, Hui and Hastie, Trevor. Regularization and variable
selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):
301â€“320, 2005.

