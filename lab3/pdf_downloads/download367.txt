Fast k-Nearest Neighbour Search via Prioritized DCI
Supplementary Material

Ke Li 1 Jitendra Malik 1

7. Analysis

For there to be n0 extraneous points, there must be n0 k
silly extraneous points. Therefore, the probability that there
are n0 extraneous points is upper bounded by the probability that there are n0 k silly extraneous points.

We present proofs that were omitted from the main paper
below.
N

N0

Theorem 1. Let vil i=1 and vis0 i0 =1 be sets of vectors such that vil 2 >
vis0 2 8i 2 [N ], i0 2
0
0
[N ]. Furthermore, let uij i2[N ],j2[M ] be random uniformly distributed unit vectors such that u0i1 , . . . , u0iM
are independent for any given i. Consider the events
N
9vis0 s.t. maxj hvil , u0ij i  vis0 2 i=1 . The probability that at least k 0 of these events occur is at
PN
M
s
most k10 i=1 1 ⇡2 cos 1 vmax
/ vil 2
, where
2
s
s
0
0
vmax
=
max
v
.
Furthermore,
if
k
=
N , it is
0
i
2
n i 2
o
M
2
1
s
l
at most mini2[N ] 1 ⇡ cos
vmax 2 / vi 2
.

Since points are retrieved from the composite index in
the order of increasing maximum projected distance to the
query, for any pair of points p and p0 , if p is retrieved before
p0 , then maxj {|hp q, ujl i|}  maxj {|hp0 q, ujl i|},
m
where {ujl }j=1 are the projection directions associated
with the constituent simple indices of the composite index.
By Theorem 1, if we take vil
N0

q

n
,
i=2k+1

k

k

for some p0 2 p(i) q i=1 . In other words, this is
the probability of there being n0
k points that are
not the 2k-nearest neighbours whose maximum projected distances are no greater than the distance from
some P
k-nearest neighbours to the query, which is at most
n
1
2
1
p(k) q 2 / p(i) q 2
i=2k+1 1
n0 k
⇡ cos

M

s
Lemma 1, Pr(Ei )  1 ⇡2 cos 1 vmax
/ vil 2
.
2
It follows from Lemma 2 that theP probability that
N
k 0 of Ei ’s occur is at most k10 i=1 Pr (Ei ) 
P
M
N
1
2
1
s
vmax
/ vil 2
. If k 0 =
i=1 1
k0
⇡ cos
2
TN
N , we use the ⇣fact that ⌘i0 =1 Ei0 ✓ Ei 8i, which
TN
0
implies that Pr
 mini2[N ] Pr (Ei ) 
i0 =1 Ei
n
o
M
s
l
mini2[N ] 1 ⇡2 cos 1 vmax
/
v
.
i 2
2

Proof. Points that are not the true k-nearest neighbours but
are retrieved before some of them will be referred to as
extraneous points and are divided into two categories: reasonable and silly. An extraneous point is reasonable if it
is one of the 2k-nearest neighbours, and is silly otherwise.

to be p(i)

vis0 i0 =1 to be p(i) q i=1 , M to be m, u0ij j2[M ]
to be {ujl }j2[m] for all i 2 [N ] and k 0 to be n0 k, we
obtain an upper bound for the probability of there being
n
a subset of p(i) i=2k+1 of size n0 k such that for all
points p in the subset, maxj {|hp q, ujl i|}  kp0 qk2

Proof. The event that 9vis0 s.t. maxj hvil , u0ij i

vis0 2 is equivalent to the event that maxj hvil , u0ij i 
s
maxi0 vis0 2
=
vmax
.
Take Ei to be the
2
l
s
event that maxj hvi , u0ij i

vmax
.
By
2

Lemma 3. Consider points in the order they are retrieved from a composite index that consists of m
simple indices.
The probability that there are at
least n0 points that are not the true k-nearest neighbours P
but are retrieved before some of them is at most
n
1
2
1
p(k) q 2 / p(i) q 2
i=2k+1 1
n0 k
⇡ cos

N
i=1

m

.

Since the event that maxj {|hp q, ujl i|}

maxj {|hp0 q, ujl i|} is contained in the event that
maxj {|hp q, ujl i|}  kp0 qk2 for any p, p0 , this is
also an upper bound for the probability of there being
n0
k points that are not the 2k-nearest neighbours
whose maximum projected distances do not exceed those
of some of the k-nearest neighbours, which by definition
is the probability that there are n0
k silly extraneous
points. Since this probability is no less than the probability
that there are n0 extraneous points, the upper bound also
applies to this probability.
m

.

Lemma 4. Consider point projections in a composite
index that consists of m simple indices in the order they
are visited. The probability that n0 point projections that
are not of the true k-nearest neighbours are visited before
all true P
k-nearest neighbours have been retrieved is at most
n
m
2
1
p(k) q 2 / p(i) q 2 .
i=2k+1 1
n0 mk
⇡ cos

Fast k-Nearest Neighbour Search via Prioritized DCI

Proof. Projections of points that are not the true k-nearest
neighbours but are visited before the k-nearest neighbours
have all been retrieved will be referred to as extraneous projections and are divided into two categories: reasonable
and silly. An extraneous projection is reasonable if it is
of one of the 2k-nearest neighbours, and is silly otherwise.
For there to be n0 extraneous projections, there must be
n0 mk silly extraneous projections, since there could be
at most mk reasonable extraneous projections. Therefore,
the probability that there are n0 extraneous projections is
upper bounded by the probability that there are n0 mk
silly extraneous projections.
Since point projections are visited in the order of increasing projected distance to the query, each extraneous silly
projection must be closer to the query projection than the
maximum projection of some k-nearest neighbour.
By
p

Theorem

1,

(2k+b(i 1)/mc+1)

if
q

we

take

m(n 2k)
,
i=1

mk
, M to be
i=1
m(n 2k)
u(i mod m),l i=1
and k 0 to be

p(b(i

1)/mc+1)

N
to
i=1
0
s N
vi0 i0 =1 to
N
1, {u0i1 }i=1 to

vil

q

be
be
be

n0
mk, we obtain an upper bound for the probability of there being
n0 mk point projections that are not of the 2k-nearest
neighbours whose distances to their respective query
projections are no greater than the true distance between the query and✓some k-nearest
✓ neighbour,
◆◆which is
(k)
P
p
q
k
k
n
1
2
1
2
.
i=2k+1 m 1
n0 mk
⇡ cos
kp(i) qk2
Because maximum projected distances are no more than
true distances, this is also an upper bound for the probability of there being n0 mk silly extraneous projections.
Since this probability is no less than the probability that
there are n0 extraneous projections, the upper bound also
applies to this probability.
Lemma
relative
P

5.
On
a
dataset
with
global
sparsity
(k, ),
the
quantity
m
n
2
1
p(k) q 2 / p(i) q 2
is
i=2k+1 1
⇡ cos
at most O k max(log(n/k), (n/k)1 m log2 ) .
Proof. By definition of global relative sparsity, for all i
2k + 1, p(i) q 2 >
p(k) q 2 . A recursive ap0
plication shows that for all i
2i k + 1, p(i) q 2 >
i0
p(k) q 2 .
Applying the fact that 1 (2/⇡) cos
and the above observation yields:
0
n
X
B
@1

i=2k+1

2
cos
⇡

1

0
B
@

p(k)
p(i)

1

(x)  x 8x 2 [0, 1]

q
q

11 m

2 CC

2

AA

0
n
p(k)
X
B

@
p(i)
i=2k+1
<

dlog2 (n/k)e 1

X

q
q
0

2i k

1m

2C

2

A

i0 m

i0 =1

p
m
If
2, this quantity
p is at most k log2 (n/k). On the
other hand, if 1  < m 2, this quantity can be simplified
to:
! ✓
✓
◆ ✓
◆
◆
k

=O

2

2

m

m

k

✓

2
m

dlog2 (n/k)e 1

◆dlog2 (n/k)e

✓ ⇣ ⌘
n 1
=O k
k

m log2

◆

1

1 /

2

1

m

!

Pn
Therefore,
p(k) q 2 / p(i)
i=2k+1
O k max(log(n/k), (n/k)1 m log2 ) .

q

m
2



Lemma 6. For a dataset with global relative sparsity (k, ) and a given composite index consisting of m simple indices, there is some k0 2
⌦(k max(log(n/k), (n/k)1 m log2 )) such that the probability that the candidate points retrieved from the composite
index do not include some of the true k-nearest neighbours
is at most some constant ↵0 < 1.
Proof. We will refer to the true k-nearest neighbours that
are among first k0 points retrieved from the composite index as true positives and those that are not as false negatives. Additionally, we will refer to points that are not true
k-nearest neighbours but are among the first k0 points retrieved as false positives.
When not all the true k-nearest neighbours are among the
first k0 candidate points, there must be at least one false
negative and so there can be at most k 1 true positives.
Consequently, there must be at least k0 (k 1) false positives. To find an upper bound on the probability of the existence of k0 (k 1) false positives in terms of global relative sparsity, we apply Lemma 3 with n0 set to k0 (k 1),
followed by Lemma 5. We conclude that this probability
1
is at most k0 2k+1
O k max(log(n/k), (n/k)1 m log2 ) .
Because the event that not all the true k-nearest neighbours
are among the first k0 candidate points is contained in the
event that there are k0 (k 1) false positives, the former
is upper bounded by the same quantity. So, we can choose
some k0 2 ⌦(k max(log(n/k), (n/k)1 m log2 )) to make
it strictly less than 1.
Lemma 7. For a dataset with global relative sparsity (k, ) and a given composite index consisting of m simple indices, there is some k1 2

Fast k-Nearest Neighbour Search via Prioritized DCI

⌦(mk max(log(n/k), (n/k)1 log2 )) such that the probability that the candidate points retrieved from the composite
index do not include some of the true k-nearest neighbours
is at most some constant ↵1 < 1.
Proof. We will refer to the projections of true k-nearest
neighbours that are among first k1 visited point projections
as true positives and those that are not as false negatives.
Additionally, we will refer to projections of points that are
not of the true k-nearest neighbours but are among the first
k1 visited point projections as false positives.
When a k-nearest neighbour is not among the candidate
points that have been retrieved, some of its projections
must not be among the first k1 visited point projections.
So, there must be at least one false negative, implying that
there can be at most mk 1 true positives. Consequently,
there must be at least k1 (mk 1) false positives. To
find an upper bound on the probability of the existence of
k1 (mk 1) false positives in terms of global relative
sparsity, we apply Lemma 4 with n0 set to k1 (mk 1),
followed by Lemma 5. We conclude that this probability
m
is at most k1 2mk+1
O k max(log(n/k), (n/k)1 log2 ) .
Because the event that some true k-nearest neighbour is
missing from the candidate points is contained in the event
that there are k1 (mk 1) false positives, the former is
upper bounded by the same quantity. So, we can choose
some k1 2 ⌦(mk max(log(n/k), (n/k)1 log2 )) to make
it strictly less than 1.
Theorem 2. For a dataset with global relative sparsity (k, ), for any ✏ > 0, there is some L,
k0 2 ⌦(k max(log(n/k), (n/k)1 m log2 )) and k1 2
⌦(mk max(log(n/k), (n/k)1 log2 )) such that the algorithm returns the correct set of k-nearest neighbours with
probability of at least 1 ✏.

⇣
mk log m max(log(n/k), (n/k)1

1/d0

)

⌘⌘

time to re-

trieve the k-nearest neighbours at query time, where d0 denotes the intrinsic dimensionality.
Proof. Computing projections of the query point along all
ujl ’s takes O(dm) time, since L is a constant. Searching
in the binary search trees/skip lists Tjl ’s takes O(m log n)
time. The total number of point projections that are
visited is at most ⇥(mk max(log(n/k), (n/k)1 log2 )).
Because determining the next point to visit requires
popping and pushing a priority queue, which takes
O(log m) time, the total time spent on visiting points
is O(mk log m max(log(n/k), (n/k)1 log2 )).
The
total number of candidate points retrieved is at most
⇥(k max(log(n/k), (n/k)1 m log2 )).
Because true
distances are computed for every candidate point,
the total time spent on distance computation is
O(dk max(log(n/k), (n/k)1 m log2 )).
We can find
the k closest points to the query among the candidate
points using a selection algorithm like quickselect,
which takes O(k max(log(n/k), (n/k)1 m log2 )) time
on average. Since the time for visiting points and
for computing distances dominates, the entire algorithm takes O(dk max(log(n/k), (n/k)1 m log2 ) +
mk log m max(log(n/k), (n/k)1 log2 )) time. Substituting 1/d0 for log2 yields the desired expression.
Theorem 4. For a given number of simple indices m, the
algorithm takes O(m(dn + n log n)) time to preprocess the
data points in D at construction time.
Proof. Computing projections of all n points along all ujl ’s
takes O(dmn) time, since L is a constant. Inserting all n
points into mL self-balancing binary search trees/skip lists
takes O(mn log n) time.

Proof. For a given composite index, by Lemma 6, there
is some k0 2 ⌦(k max(log(n/k), (n/k)1 m log2 )) such
that the probability that some of the true k-nearest
neighbours are missed is at most some constant ↵0 <
1.
Likewise, by Lemma 7, there is some k1 2
⌦(mk max(log(n/k), (n/k)1 log2 )) such that this probability is at most some constant ↵1 < 1. By choosing such k0 and k1 , this probability is therefore at most
min{↵0 , ↵1 } < 1. For the algorithm to fail, all composite indices must miss some k-nearest neighbours. Since
each composite index is constructed independently, the alL
gorithm fails with probability of at most (min{↵0 , ↵1 }) ,
and so must succeed with probability of at least 1
L
(min{↵0 , ↵1 }) . Since min{↵0 , ↵1 } < 1, there is some
L
L that makes 1 (min{↵0 , ↵1 })
1 ✏.

Theorem 5. The algorithm requires O(m(d + log n)) time
to insert a new data point and O(m log n) time to delete a
data point.

Theorem 3. For a given
⇣ number of simple indices m,0 the
algorithm takes O dk max(log(n/k), (n/k)1 m/d )+

Proof. The only additional information that needs to be
stored are the mL binary search trees or skip lists. Since

Proof. In order to insert a data point, we need to compute
its projection along all ujl ’s and insert it into each binary
search tree or skip list. Computing the projections takes
O(md) time and inserting them into the corresponding selfbalancing binary search trees or skip lists takes O(m log n)
time. In order to delete a data point, we simply remove
its projections from each of the binary search trees or skip
lists, which takes O(m log n) time.
Theorem 6. The algorithm requires O(mn) space in addition to the space used to store the data.

Fast k-Nearest Neighbour Search via Prioritized DCI

(a)

(b)

Figure 3. Memory usage of different algorithms on (a) CIFAR-100 and (b) MNIST. Lower values are better.

n entries are stored in each binary search tree/skip list, the
total additional space required is O(mn).

8. Experiments
Figure 3 shows the memory usage of different algorithms
on CIFAR-100 and MNIST.

