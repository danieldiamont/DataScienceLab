Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

A. Fisher information matrix for the Normal Distribution
Under regularity conditions (Wasserman, 2013), the Fisher information matrix can also be obtained from the second-order
partial derivatives of the log-likelihood function
I(θ) = −E[

∂ 2 l(θ)
],
∂θ2

(D1)

where l(θ) = log πθ (a|s). This gives us the Fisher information for the Normal distribution
� 2
�
2
I(µ, σ) = −Ea∼πθ
= −Ea∼πθ

�

∂ l
∂µ2
∂2l
∂σ∂µ

∂ l
∂µ∂σ
∂2l
∂σ 2

−2 (a−µ)
σ3
−3(a−µ)2
+
4
σ

− σ12
−2 (a−µ)
σ3

1
σ2

�

=

�

1
σ2

0

0
2
σ2

(D2)
�

.

B. Fisher information matrix for the Beta Distribution
To see how variance changes as the policy converges and becomes more deterministic, let us ﬁrst compute the partial
derivative of log πθ (a|s) with respect to shape parameter α
�
�
Γ(α + β) α−1
∂
∂ log πθ (a|s)
=
log
a
(1 − a)β−1
∂α
∂α
Γ(α)Γ(β)
∂
(log Γ(α + β) − log Γ(α) + (α − 1) log a)
=
∂α
= log a + ψ(α + β) − ψ(α) ,
where ψ(·) =

d
dz

log Γ(·) is the digamma function. Similar results can also be derived for β. From (D1), we have
�
�
2
2
I(α, β) = −Ea∼πθ

= −Ea∼πθ
=

�

�

∂ l
∂α2
∂2l
∂β∂α

∂ l
∂α∂β
∂2l
∂β 2

∂
∂α (ψ(α + β) − ψ(α))
∂
∂α (ψ(α + β))

ψ � (α) − ψ � (α + β)
−ψ � (α + β)

where ψ � (z) = ψ (1) (z) and ψ (m) (z) =
goes to 0 as z → ∞.

dm+1
dz m+1

∂
∂β (ψ(α

+ β))
∂
∂β (ψ(α + β) − ψ(β))
�
�
−ψ (α + β)
,
ψ � (β) − ψ � (α + β)

�

log Γ(z) is the polygamma function of order m. Figure 7 shows how ψ � (z)

Figure 7. Graphs of the�digamma function, the polygamma function, and the harmonic series. The harmonic series and the digamma
function are related by zk=1 k1 = ψ(z + 1) + γ, where γ = 0.57721 · · · is the Euler-Mascheroni constant.

