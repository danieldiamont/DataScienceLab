Supplementary Material for
Being Robust (in High Dimensions) Can Be Practical
Ilias Diakonikolas
CS, USC
diakonik@usc.edu

Gautam Kamath
EECS & CSAIL, MIT
g@csail.mit.edu

Jerry Li
EECS & CSAIL, MIT
jerryzli@mit.edu

1

Daniel M. Kane
CSE & Math, UCSD
dakane@cs.ucsd.edu

Ankur Moitra
Math & CSAIL, MIT
moitra@mit.edu

Alistair Stewart
CS, USC
alistais@usc.edu

Omitted Details from Section 3

1.1

Robust Mean Estimation for Sub-Gaussian Distributions

In this section, we use our filter technique to give a near sample-optimal computationally efficient
algorithm to robustly estimate the mean of a sub-gaussian density with a known covariance matrix,
thus proving Theorem 3.1.
We emphasize that the algorithm and its analysis is essentially identical to the filtering algorithm
given in Section 8.1 of [DKK+ 16] for the case of a Gaussian N (µ, I). The only difference is a
weaker definition of the “good set of samples” (Definition 2) and a simple concentration argument
(Lemma 1) showing that a random set of uncorrupted samples of the appropriate size is good
with high probability. Given these, the analysis of this subsection follows straightforwardly from
the analysis in Section 8.1 of [DKK+ 16] by plugging in the modified parameters. For the sake of
completeness, we provide the details below.
We start by formally defining sub-gaussian distributions:
Definition 1. A distribution P on R with mean µ, is sub-gaussian with parameter ν > 0 if
EX∼P [exp(λ(X − µ))] ≤ exp(νλ2 /2)
for all λ ∈ R. A distribution P on Rd with mean vector µ is sub-gaussian with parameter ν > 0,
if for all unit vectors v, the one-dimensional random variable v · X, X ∼ P , is sub-gaussian with
parameter ν.
We will use the following simple fact about the concentration of sub-gaussian random variables:
Fact 1. If P is sub-gaussian on Rd with mean vector µ and parameter ν > 0, then for any unit
vector v ∈ Rd we have that PrX∼P [|v · (X − µ)| ≥ T ] ≤ exp(−t2 /2ν).
The following theorem is a high probability version of Theorem 3.1:
Theorem 2. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG ,
covariance matrix I, and ε, τ > 0. Let S 0 be an ε-corrupted set of samples from G of size
Ω((d/ε2 )poly log(d/ετ )). There exists an efficient algorithm that, on input S 0 and
p ε > 0, returns a
mean vector µ
b so that with probability at least 1 − τ we have kb
µ − µG k2 = O(ε log(1/ε)).
1

1 P
1 P
G
G T
Notation. We will denote µS = |S|
X∈S X and MS = |S|
X∈S (X − µ )(X − µ ) for the
sample mean and modified sample covariance matrix of the set S.

We start by defining our modified notion of good sample, i.e, a set of conditions on the uncorrupted set of samples under which our algorithm will succeed.
Definition 2. Let G be an identity covariance sub-gaussian in d dimensions with mean µG and
covariance matrix I and ε, τ > 0. We say that a multiset S of elements in Rd is (ε, τ )-good with
respect to G if the following conditions are satisfied:
p
(i) For all x ∈ S we have kx − µG k2 ≤ O( d log(|S|/τ )).
(ii) For every affine function L : Rd → R such that L(x) = v · (x − µG ) − T , kvk2 = 1, we have
that |PrX∈u S [L(X) ≥ 0] − PrX∼G [L(X) ≥ 0]| ≤ T 2 log dεlog( d ) .
(
ετ )
(iii) We have that kµS − µG k2 ≤ ε.
(iv) We have that kMS − Ik2 ≤ ε.
We show in the following subsection that a sufficiently large set of independent samples from G
is (ε, τ )-good (with respect to G) with high probability. Specifically, we prove:
Lemma 1. Let G be sub-gaussian distribution with parameter ν = Θ(1) and with identity covariance,
and ε, τ > 0. If the multiset S is obtained by taking Ω((d/ε2 )poly log(d/ετ )) independent samples
from G, it is (ε, τ )-good with respect to G with probability at least 1 − τ.
We require the following definition that quantifies the extent to which a multiset has been
corrupted:
Definition 3. Given finite multisets S and S 0 we let ∆(S, S 0 ) be the size of the symmetric difference
of S and S 0 divided by the cardinality of S.
The starting point of our algorithm will be a simple NaivePrune routine (Section 4.3.1 of
[DKK+ 16]) that removes obvious outliers, i.e., points which are far from the mean. Then, we
iterate the algorithm whose performance guarantee is given by the following:
Proposition 1. Let G be a sub-gaussian distribution on Rd with parameter ν = Θ(1), mean µG ,
covariance matrix I, ε > 0 be sufficiently small and τ > 0. Let S be an (ε, τ )-good set
p with respect
to G. Let S 0 be any multiset with ∆(S, S 0 ) ≤ 2ε and for any x, y ∈ S 0 , kx − yk2 ≤ O( d log(d/ετ )).
There exists a polynomial time algorithm Filter-Sub-Gaussian-Unknown-Mean that, given S 0
and ε > 0, returns one of the following:
p
(i) A mean vector µ
b such that kb
µ − µG k2 = O(ε log(1/ε)).
def

d
(ii) A multiset S 00 ⊆ S 0 such that ∆(S, S 00 ) ≤ ∆(S, S 0 )−ε/α, where α = d log(d/ετ ) log(d log( ετ
)).

We start by showing how Theorem 2 follows easily from Proposition 1.
Proof of Theorem 2. By the definition of ∆(S, S 0 ), since S 0 has been obtained from S by corrupting
an ε-fraction of the points in S, we have that ∆(S, S 0 ) ≤ 2ε. By Lemma 1, the set S of uncorrupted
samples is (ε, τ )-good with respect to G with probability at least 1 − τ. We henceforth condition on
this event.
p
Since S is (ε, τ )-good, all x ∈ S have kx − µG k2 ≤ O( d log |S|/τ ). Thus, the NaivePrune
procedure does not remove from S 0 any member of S. Hence, its output, S 00 , has ∆(S, S 00 ) ≤ ∆(S, S 0 )
2

p
and for any x ∈ S 00 , there is a y p
∈ S with kx − yk2 p
≤ O( d log |S|/τ ). By the triangle inequality,
for any x, z ∈ S 00 , kx − zk2 ≤ O( d log |S|/τ ) = O( d log(d/ετ )).
Then, we iteratively apply the Filter-Sub-Gaussian-Unknown-Meanpprocedure of Proposition 1 until it terminates returning a mean vector µ with kb
µ − µG k2 = O(ε log(1/ε)). We claim
that we need at most O(α) iterations for this to happen. Indeed, the sequence of iterations results
in a sequence of sets Si0 , so that ∆(S, Si0 ) ≤ ∆(S, S 0 )−i·ε/α. Thus, if we do not output the empirical
mean in the first 2α iterations, in the next iteration there are no outliers left and the algorithm
terminates outputting the sample mean of the remaining set.
1.1.1

Algorithm Filter-Sub-Gaussian-Unknown-Mean: Proof of Proposition 1

In this subsection, we describe the efficient algorithm establishing Proposition 1 and prove its
0
correctness. Our algorithm calculates the empirical mean vector µS and empirical covariance matrix
0
Σ. If the matrix Σ has no large eigenvalues, it returns µS . Otherwise, it uses the eigenvector v ∗
0
corresponding to the maximum magnitude eigenvalue of Σ and the mean vector µS to define a
filter. Our efficient filtering procedure is presented in detailed pseudocode below.
Algorithm 1 Filter algorithm for a sub-gaussian with unknown mean and identity covariance
1: procedure Filter-Sub-Gaussian-Unknown-Mean(S 0 , ε, τ )
input: A multiset S 0 such that there exists an (ε, τ )-good S with ∆(S, S 0 ) ≤ 2ε
output: Multiset S 00 or mean vector µ
b satisfying Proposition 1
0
S
2:
Compute the sample mean µ = EX∈u S 0 [X] and the sample covariance matrix Σ , i.e.,
0
0
Σ = (Σi,j )1≤i,j≤d with Σi,j = EX∈u S 0 [(Xi − µSi )(Xj − µSj )].
3:
Compute approximations for the largest absolute eigenvalue of Σ − I, λ∗ := kΣ − Ik2 , and
the associated unit eigenvector v ∗ .
0
4:
if kΣ − Ik2 ≤ O(ε log(1/ε)), then return µS .
5:
end if
p
6:
Let δ := 3 εkΣ − Ik2 . Find T > 0 such that
h
i
ε
0
.
Pr 0 |v ∗ · (X − µS )| > T + δ > 8 exp(−T 2 /2ν) + 8
d
2
X∈u S
T log d log( ετ
)
0

return the multiset S 00 = {x ∈ S 0 : |v ∗ · (x − µS )| ≤ T + δ}.
8: end procedure
7:

1.1.2

Proof of Correctness of Filter-Sub-Gaussian-Unknown-Mean

By definition, there exist disjoint multisets L, E, of points in Rd , where L ⊂ S, such that S 0 =
0
(S \ L) ∪ E. With this notation, we can write ∆(S, S 0 ) = |L|+|E|
|S| . Our assumption ∆(S, S ) ≤ 2ε is
equivalent to |L| + |E| ≤ 2ε · |S|, and the definition of S 0 directly implies that (1 − 2ε)|S| ≤ |S 0 | ≤
(1 + 2ε)|S|. Throughout the proof, we assume that ε is a sufficiently small constant.
0
We define µG , µS , µS , µL , and µE to be the means of G, S, S 0 , L, and E, respectively.
Our analysis will make essential use of the following matrices:
• MS 0 denotes EX∈u S 0 [(X − µG )(X − µG )T ],
• MS denotes EX∈u S [(X − µG )(X − µG )T ],
• ML denotes EX∈u L [(X − µG )(X − µG )T ], and
3

• ME denotes EX∈u E [(X − µG )(X − µG )T ].
Our analysis will hinge on proving the important claim that Σ−I is approximately (|E|/|S 0 |)ME .
This means two things for us. First, it means that if the positive errors align in some direction
(causing ME to have a large eigenvalue), there will be a large eigenvalue in Σ − I. Second, it says
that any large eigenvalue of Σ − I will correspond to an eigenvalue of ME , which will give an explicit
direction in which many error points are far from the empirical mean.
Useful Structural Lemmas. We begin by noting that we have concentration bounds on G and
therefore, on S due to its goodness.


Fact 3. Let w ∈ Rd be any
unit vector, then for any T > 0, PrX∼G |w · (X − µG )| > T ≤

2 exp(−T 2 /2ν) and PrX∈u S |w · (X − µG )| > T ≤ 2 exp(−T 2 /2ν) + T 2 log dεlog( d ) .
(
ετ )
Proof. The first line is Fact 1, and the former follows from it using the goodness of S.
By using the above fact, we obtain the following simple claim:
Claim 1. Let w ∈ Rd be any unit vector, then for any T > 0, we have that:
0

0

Pr [|w · (X − µS )| > T + kµS − µG k2 ] ≤ 2 exp(−T 2 /2ν).

X∼G

and

0

0

Pr [|w · (X − µS )| > T + kµS − µG k2 ] ≤ 2 exp(−T 2 /2ν) +

X∈u S

0

T 2 log

ε
.
d
)
d log( ετ
0

Proof. This follows from Fact 3 upon noting that |w · (X − µS )| > T + kµS − µG k2 only if
|w · (X − µG )| > T .
We can use the above facts to prove concentration bounds for L. In particular, we have the
following lemma:
Lemma 2. We have that kML k2 = O (log(|S|/|L|) + ε|S|/|L|).
Proof. Since L ⊆ S, for any x ∈ Rd , we have that
|S| · Pr (X = x) ≥ |L| · Pr (X = x) .
X∈u S

X∈u L

(1)

Since ML is a symmetric matrix, we have kML k2 = maxkvk2 =1 |v T ML v|. So, to bound kML k2 it
suffices to bound |v T ML v| for unit vectors v. By definition of ML , for any v ∈ Rd we have that
|v T ML v| = EX∈u L [|v · (X − µG )|2 ].

4

For unit vectors v, the RHS is bounded from above as follows:
Z ∞




Pr |v · (X − µG )| > T T dT
EX∈u L |v · (X − µG )|2 = 2
0 X∈u L
Z √
O(

d log(d/ετ ))

Pr [|v · (X − µG )| > T ]T dT

=2
0

Z

√

O(

X∈u L
d log(d/ετ ))

≤2
0
Z 4√ν log(|S|/|L|)







|S|
min 1,
· Pr |v · (X − µG )| > T T dT
|L| X∈u S

T dT
0
O(

Z
+ (|S|/|L|)

√

4

√

d log(d/ετ )) 

ν log(|S|/|L|)

exp(−T 2 /2ν) +


ε

T dT
d
T 2 log d log( ετ
)

 log(|S|/|L|) + ε · |S|/|L| ,
where the second line follows from the fact that kvk2 = 1, L ⊂ S, and S satisfies condition (i) of
Definition 2; the third line follows from (1); and the fourth line follows from Fact 3.
As a corollary, we can relate the matrices MS 0 and ME , in spectral norm:
Corollary 1. We have that MS 0 − I = (|E|/|S 0 |)ME + O(ε log(1/ε)), where the O(ε log(1/ε)) term
denotes a matrix of spectral norm O(ε log(1/ε)).
Proof. By definition, we have that |S 0 |MS 0 = |S|MS − |L|ML + |E|ME . Thus, we can write
MS 0 = (|S|/|S 0 |)MS − (|L|/|S 0 |)ML + (|E|/|S 0 |)ME
= I + O(ε) + O(ε log(1/ε)) + (|E|/|S 0 |)ME ,
where the second line uses the fact that 1 − 2ε ≤ |S|/|S 0 | ≤ 1 + 2ε, the goodness of S (condition (iv)
in Definition 2), and Lemma 2. Specifically, Lemma 2 implies that (|L|/|S 0 |)kML k2 = O(ε log(1/ε)).
Therefore, we have that
MS 0 = I + (|E|/|S 0 |)ME + O(ε log(1/ε)) ,
as desired.
We now establish a similarly useful bound on the difference between the mean vectors:
p
p
0
Lemma 3. We have that µS −µG = (|E|/|S 0 |)(µEp−µG )+O(ε log(1/ε)), where the O(ε log(1/ε))
term denotes a vector with `2 -norm at most O(ε log(1/ε)).
Proof. By definition, we have that
0

|S 0 |(µS − µG ) = |S|(µS − µG ) − |L|(µL − µG ) + |E|(µE − µG ).
Since S is a good set, by condition (iii) of Definition 2, we have kµS − µG k2 = O(ε). Since
1 − 2ε ≤ |S|/|S 0 | ≤ 1 + 2ε, it follows that (|S|/|S 0 |)kµS − µG k2 = O(ε).
inequality
p Using the valid

p
L
G
2
L
G
kML k2 ≥ kµ − µ k2 and Lemma 2, we obtain that kµ − µ k2 ≤ O
log(|S|/|L|) + ε|S|/|L| .
Therefore,


p
p
p
(|L|/|S 0 |)kµL − µG k2 ≤ O (|L|/|S|) log(|S|/|L|) + ε|L|/|S| = O(ε log(1/ε)) .
5

In summary,

p
0
µS − µG = (|E|/|S 0 |)(µE − µG ) + O(ε log(1/ε)) ,

as desired. This completes the proof of the lemma.
By combining the above, we can conclude that Σ − I is approximately proportional to ME .
More formally, we obtain the following corollary:
Corollary 2. We have Σ − I = (|E|/|S 0 |)ME + O(ε log(1/ε)) + O(|E|/|S 0 |)2 kME k2 , where the
additive terms denote matrices of appropriately bounded spectral norm.
0

0

Proof. By definition, we can write Σ − I = MS 0 − I − (µS − µG )(µS − µG )T . Using Corollary 1 and
Lemma 3, we obtain:
Σ − I = (|E|/|S 0 |)ME + O(ε log(1/ε)) + O((|E|/|S 0 |)2 kµE − µG k22 ) + O(ε2 log(1/ε))
= (|E|/|S 0 |)ME + O(ε log(1/ε)) + O(|E|/|S 0 |)2 kME k2 ,
where the second line follows from the valid inequality kME k2 ≥ kµE − µG k22 . This completes the
proof.
0

Case of Small Spectral Norm. We are now ready to analyze the case that the mean vector µS
def
is returned by the algorithm in Step 4. In this case, we have that λ∗ = kΣ − Ik2 = O(ε log(1/ε)).
Hence, Corollary 2 yields that
(|E|/|S 0 |)kME k2 ≤ λ∗ + O(ε log(1/ε)) + O(|E|/|S 0 |)2 kME k2 ,
which in turns implies that
(|E|/|S 0 |)kME k2 = O(ε log(1/ε)) .
On the other hand, since kME k2 ≥ kµE − µG k22 , Lemma 3 gives that
p
p
p
0
kµS − µG k2 ≤ (|E|/|S 0 |) kME k2 + O(ε log(1/ε)) = O(ε log(1/ε)).
This proves part (i) of Proposition 1.
Case of Large Spectral Norm. We next show the correctness of the algorithm when it returns
a filter in Step 6.
def
We start by proving that if λ∗ = kΣ − Ik2 > Cε log(1/ε), for a sufficiently large universal
constant C, then a value T satisfying the condition in Step 6 exists. We first note that that kME k2
is appropriately large. Indeed, by Corollary 2 and the assumption that λ∗ > Cε log(1/ε) we deduce
that
(|E|/|S 0 |)kME k2 = Ω(λ∗ ) .
(2)
Moreover, using the inequality kME k2 ≥ kµE − µG k22 and Lemma 3 as above, we get that
p
p
0
kµS − µG k2 ≤ (|E|/|S 0 |) kME k2 + O(ε log(1/ε)) ≤ δ/2 ,
p
def √
where we used the fact that δ = ελ∗ > C 0 ε log(1/ε).
Suppose for the sake of contradiction that for all T > 0 we have that
h
i
ε
0
.
Pr 0 |v ∗ · (X − µS )| > T + δ ≤ 8 exp(−T 2 /2ν) + 8
d
2
X∈u S
T log d log( ετ
)
6

(3)

Using (3), we obtain that for all T > 0 we have that


Pr

X∈u

S0


|v ∗ · (X − µG )| > T + δ/2 ≤ 8 exp(−T 2 /2ν) + 8

T 2 log

ε
.
d
d log( ετ
)

(4)

Since E ⊆ S 0 , for all x ∈ Rd we have that |S 0 | PrX∈u S 0 [X = x] ≥ |E| PrY ∈u E [Y = x]. This fact
combined with (4) implies that for all T > 0
!
 ∗

ε
G
0
2
 . (5)
Pr |v · (X − µ )| > T + δ/2  (|S |/|E|) exp(−T /2ν) +
d
X∈u E
T 2 log d log( ετ
)
We now have the following sequence of inequalities:
Z ∞
 ∗



G 2
kME k2 = EX∈u E |v · (X − µ )| = 2
Pr |v ∗ · (X − µG )| > T T dT
0 X∈u E
Z O(√d log(d/ετ ))


=2
Pr |v ∗ · (X − µG )| > T T dT
X∈u E
0


Z O(√d log(d/ετ ))
 ∗

|S 0 |
G
≤2
min 1,
· Pr |v · (X − µ )| > T T dT
|E| X∈u S 0
0
Z 4√ν log(|S 0 |/|E|)+δ
Z O(√d log(d/ετ )) 
0

T dT + (|S |/|E|) √
exp(−T 2 /2ν) +
0
0

4

0

ν log(|S |/|E|)+δ


ε

T dT
d
T 2 log d log( ετ
)

0

2

 log(|S |/|E|) + δ + O(1) + ε · |S |/|E|
 log(|S 0 |/|E|) + ελ∗ + ε · |S 0 |/|E| .
Rearranging the above, we get that
(|E|/|S 0 |)kME k2  (|E|/|S 0 |) log(|S 0 |/|E|) + (|E|/|S 0 |)ελ∗ + ε = O(ε log(1/ε) + ε2 λ∗ ).
Combined with (2), we obtain λ∗ = O(ε log(1/ε)), which is a contradiction if C is sufficiently large.
Therefore, it must be the case that for some value of T the condition in Step 6 is satisfied.
The following claim completes the proof:
def

d
Claim 2. Fix α = d log(d/ετ ) log(d log( ετ
)). We have that ∆(S, S 00 ) ≤ ∆(S, S 0 ) − 2ε/α .

Proof. Recall that S 0 = (S \ L) ∪ E, with E and L disjoint multisets such that L ⊂ S. We can
similarly write S 00 = (S \ L0 ) ∪ E 0 , with L0 ⊇ L and E 0 ⊂ E. Since
∆(S, S 0 ) − ∆(S, S 00 ) =

|E \ E 0 | − |L0 \ L|
,
|S|

it suffices to show that |E \ E 0 | ≥ |L0 \ L| + ε|S|/α. Note that |L0 \ L| is the number of points rejected
by the filter that lie in S ∩ S 0 . Note that the fraction of elements of S that are removed to produce
S 0 )| > T + δ) is at most 2 exp(−T 2 /2ν) + ε/α. This follows from Claim 1
S 00 (i.e., satisfy |v ∗ · (x − µp
and the fact that T = O( d log(d/ετ )).
Hence, it holds that |L0 \ L| ≤ (2 exp(−T 2 /2ν) + ε/α)|S|. On the other hand, Step 6 of the
algorithm ensures that the fraction of elements of S 0 that are rejected by the filter is at least

7

8 exp(−T 2 /2ν) + 8ε/α). Note that |E \ E 0 | is the number of points rejected by the filter that lie in
S 0 \ S. Therefore, we can write:
|E \ E 0 | ≥ (8 exp(−T 2 /2ν) + 8ε/α)|S 0 | − (2 exp(−T 2 /2ν) + ε/α)|S|
≥ (8 exp(−T 2 /2ν) + 8ε/α)|S|/2 − (2 exp(−T 2 /2ν) + ε/α)|S|
≥ (2 exp(−T 2 /2ν) + 3ε/α)|S|
≥ |L0 \ L| + 2ε|S|/α ,
where the second line uses the fact that |S 0 | ≥ |S|/2 and the last line uses the fact that |L0 \L|/|S| ≤
2 exp(−T 2 /2ν) + ε/α. Noting that log(d/ετ ) ≥ 1, this completes the proof of the claim.
1.1.3

Proof of Lemma 1

Proof. Let N = Ω((d/ε2 )poly log(d/ετ )) be the number
of samples drawn from G. For (i), the
p
probability that a coordinate of a sample is at least 2ν log(N d/3τ ) is at most τ /3dN
p by Fact 1. By
a union bound, the probability that all coordinates
of
all
samples
are
smaller
than
2ν log(N d/3τ )
p
p
is at least 1 − τ /3. In this case, kxk2 ≤ 2νd log(N d/3τ ) = O( dν log(N ν/τ )).
After translating by µG , we note that (iii) follows immediately from Lemmas 4.3 of [DKK+ 16]
and (iv) follows from Theorem 5.50 of [Ver10], as long as N = Ω(ν 4 d log(1/τ )/ε2 ), with probability
at least 1 − τ /3. It remains to show that, conditioned on (i), (ii) holds with probability at least
1 − τ /3.
p
To simplify some expressions, let δ := ε/(log(d log d/ετ )) and R = C d log(|S|/τ ). We need to
show that for all unit vectors v and all 0 ≤ T ≤ R that




G
G
 Pr [|v · (X − µ )| > T ] − Pr [|v · (X − µ ) > T ≥ 0] ≤ δ .
(6)
X∈u S
 T2
X∼G
Firstly, we show that for all unit vectors v and T > 0




G
G
 Pr [|v · (X − µ )| > T ] − Pr [|v · (X − µ )| > T ≥ 0] ≤ δ/4ν ln(1/δ)
X∈u S

X∼G
with probability at least 1 − τ /6. Since the VC-dimension of the set of all halfspaces is d + 1, this
2
follows from the VC inequality [DL01], since we have
p more than Ω(d/(δ/(4ν log(1/δ)) ) samples.
We thus only need to consider the case when T ≥ 4ν ln(1/δ).
p
Lemma 4. For any fixed unit vector v and T > 4ν ln(1/δ), except with probability exp(−N δ/6Cν),
we have that
δ
Pr [|v · (X − µG )| > T ] ≤
,
X∈u S
CT 2
where C = 8.
Proof. Let E be the event that |v · (X − µG )| > T . Since G is sub-gaussian, Fact 1 yields that
PrG [E] = PrY ∼G [|v · (X − µG )| ≤ exp(−T 2 /2ν). Note that, thanks to our assumption on T , we
have that T ≤ exp(T 2 /4ν)/2C, and therefore T 2 PrG [E] ≤ exp(−T 2 /4ν)/2C ≤ δ/2C.

8

Consider ES [exp(t2 /3ν · N PrS [E])]. Each individual sample sample Xi for 1 ≤ i ≤ N , is an
independent copy of Y ∼ G, and hence:
"
#


n
X
1Xi ∈E )
ES exp(T 2 /3ν · N Pr[E]) = ES exp(T 2 /3ν ·
S

i=1

=

=

N
Y
i=1
N
Y

"
2

EXi exp(T /3ν ·

n
X

#
1Xi ∈E )

i=1

exp(T 2 /3ν · Pr[E])

i=1

G


≤ exp N T 2 /3ν · exp(−T 2 /2ν)
≤ exp(N/3ν · δ/2C) = exp(N δ/6Cν) .
δ
2
Note that if PrS [E] > CνT
2 , then exp(T /3ν · N PrS [E]) = exp(N δ/3Cν). By Markov’s inequality,
this happens with probability at most exp(−N δ/6Cν).
O(d) . By a
Now let C be a 1/2-cover in Euclidean distance for the
p set of unit vectors of size 2
0
0
union bound, for all v ∈ C and T a power of 2 between 4ν ln(1/δ) and R, we have that

Pr [|v 0 · (X − µG )| > T 0 ] ≤

X∈u S

δ
8T 2

except with probability
2O(d) log(R) exp(−N δ/6Cν) = exp (O(d) + log log R − N δ/6Cν) ≤ τ /6 .
p
However, for any unit vector v and 4ν ln(1/δ) ≤ T ≤ R, there is a v 0 ∈ C and such a T 0 such
that for all x ∈ Rd , we have |v · (X − µG )| ≥ |v 0 · (X − µG )|/2, and so |v 0 · (X − µG )| > 2T 0 implies
|v 0 · (X − µG )| > T.
Then, by a union bound, (6) holds simultaneously for all unit vectors v and all 0 ≤ T ≤ R, with
probability a least 1 − τ /3. This completes the proof.

1.2

Robust Mean Estimation Under Second Moment Assumptions

In this section, we use our filtering technique to give a near sample-optimal computationally efficient
algorithm to robustly estimate the mean of a density with a second moment assumption. We show:
Theorem 4. Let P be a distribution on Rd with unknown mean vector µP and unknown covariance
matrix ΣP  I. Let S be an ε-corrupted set of samples from P of size Θ((d/ε) log d). Then there
√
exists an algorithm that given S, with probability 2/3, outputs µ
b with kb
µ − µP k2 ≤ O( ε) in time
poly(d/ε).
Note that Theorem 3.2 follows straightforwardly from the above (divide every sample by σ, run
the algorithm of Theorem 4, and multiply its output by σ).
As usual in our filtering framework, the algorithm will iteratively look at the top eigenvalue
and eigenvector of the sample covariance matrix and return the sample mean if this eigenvalue
is small (Algorithm 2). The main difference between this and the filter algorithm for the subgaussian case is how we choose the threshold for the filter. Instead of looking for a violation of
a concentration inequality, here we will choose a threshold at random (with a bias towards higher
9

thresholds). The reason is that, in this setting, the variance in the direction we look for a filter in
needs to be a constant multiple larger – instead of the typical Ω̃(ε) relative for the sub-gaussian
case. Therefore, randomly choosing a threshold weighted towards higher thresholds suffices to throw
out more corrupted samples than uncorrupted samples in expectation. Although it is possible to
reject many good samples this way, the algorithm still only rejects a total of O(ε) samples with high
probability.
We would like our good set of samples to have mean close to that of P and bounded variance
in all directions. This motivates the following definition:
Definition 4. We call a set S ε-good for a distribution P with mean µP and covariance ΣP  I if
√
the mean µS and covariance ΣS of S satisfy kµS − µP k2 ≤ ε and kΣS k2 ≤ 2.
However, since we have no assumptions about higher moments, it may be be possible for outliers
to affect our sample covariance too much. Fortunately, such outliers have small probability and do
not contribute too much to the mean, so we will later reclassify them as errors.
Lemma 5. Let S be N = Θ((d/ε) log d) samples drawn from P . Then, with probability at least
9/10, a random X ∈u S satisfies
√
(i) kE[X] − µP k2 ≤ ε/3,
h
p i
(ii) Pr kX − µP k2 ≥ 80 d/ε ≤ ε/160,
h
(iii) E kX − µP k2 · 1kX−µP k

√

2 ≥80

i
d/ε

≤

√

ε/2, and

 h
i


(iv) E (X − µP )(X − µP )T · 1kX−µP k ≤80√d/ε  ≤ 3/2.
2
2

Proof. For (i), note that
ES [kE[X] − µP k22 ] =

X

ES [(E[X]i − µPi )2 ] ≤ d/N ≤ ε/360 ,

i

and so by Markov’s inequality, with probability at least 39/40, we have kE[X] − µP k22 ≤ ε/9.
For (ii), similarly to (i), note that
X
E[kY − µP k22 ] =
E[(Yi − µPi )2 ] ≤ d ,
i

p
for Y ∼ P . By Markov’s inequality, Pr[kY − µP k2 ≥ 80 d/ε] ≤ ε/160 with probability at least
39/40.
For (iii), note that by an application of the Cauchy-Schwarz inequality
q
p
√
P
√
E[|Y − µ |1kY −µP k ≥80 d/ε ] ≤ E[(Y − µP )2 ] Pr[kY − µP k2 ≥ 80 d/ε] ≤ ε/80 .
2

Thus,
ES [E[|X − µP |1kX−µP k

√

2 ≥80

d/ε

]] ≤

√

ε/80 ,

and by Markov’s inequality, with probability at least 39/40
h
i √
E |X − µP |1kX−µP k ≥80√d/ε ≤ ε/2 .
2

For (iv), we require the following Matrix Chernoff bound:
10

Lemma 6 (Part of Theorem 5.1.1 of [T+ 15]). Consider a sequence
of d × d positive semi-definite
P
max
random matrices Xk with kXk k2 ≤ L for all k. Let µ
= k k E[Xk ]k2 . Then, for θ > 0,
 #
"
X 


E 
Xk  ≤ (eθ − 1)µmax /θ + L log(d)/θ ,


k

2

and for any δ > 0,

"
#
X 
max


Pr 
Xk  ≥ (1 + δ)µmax ≤ d(eδ /(1 + δ)1+δ )µ /L .


k

2

We apply this lemma with Xk = (xk − µP )(xk − µP )T 1kx

k −µ

√

P k ≤80
2

d/ε

for {x1 , . . . , xN } = S. Note

that kXk k2 ≤ (80)2 d/ε = L and that µmax ≤ N kΣP k2 ≤ N .
Suppose that µmax ≤ N/80. Then, taking θ = 1, we have


X 


E[
Xk  ] ≤ (e − 1)N/80 + O(d log(d)/ε) .


k

2

P
By Markov’s inequality, except with probability 39/40, we have k k Xk k2 ≤ N + O(d log(d)/ε) ≤
3N/2, for N a sufficiently high multiple of d log(d)/ε.
Suppose that µmax ≥ N/80, then we take δ = 1/2 and obtain

"
#
X 


Pr 
Xk  ≥ 3µmax 2 ≤ d(e3/2 /(5/2)3/2 )N ε/20d .


k

2

P
For N a sufficiently high multiple of d log(d)/ε, we getP
that Pr[k k Xk k2 ≥ 3µmax /2] ≤ 1/40. Since
µmax ≤ N , we have with probability at least 39/40, k k Xk k2 ≤ 3N/2.
P
Noting that k k Xk k2 /N = kE[1kX−µP k ≤80√d/ε (X − µP )(X − µP )T ]k2 , we obtain (iv). By a
2

union bound, (i)-(iv) all hold simultaneously with probability at least 9/10.
Now we can get a 2ε-corrupted good set from an ε-corrupted set of samples satisfying Lemma
5, by reclassifying outliers as errors:
Lemma 7. Let S = R ∪ E \ L, where R is a set of N = Θ(d log d/ε) samples drawn from P
and E and L are disjoint sets with |E|, |L| ≤ ε. Then, with probability 9/10, we can also write
S = G ∪ E 0 \ L0 , where G ⊆ R is ε-good, L0 ⊆ L and E 0 ⊆ E 0 has |E 0 | ≤ 2ε|S|.
p
Proof. Let G = {x ∈ R : kxk2 ≤ 80 d/ε}. Since R satisfies (ii) of Lemma 5, |G|−|R| ≤ ε|R|/160 ≤
ε|S|. Thus, E 0 = E ∪ (R \ G) has |E 0 | ≤ 3ε/2. Note that (iv) of Lemma 5 for R in terms of G is
exactly |G|kΣG k2 /|R| ≤ 3/2, and so kΣG k2 ≤ 3|R|/2|G| ≤ 2.
√
It remains to check that kµG −µP k2 ≤ ε. But note that (iii) of Lemma 5 is exactly EX∈uR [kX −
√
µP k2 1X∈R\G ] ≤ ε/2, and we have


√
|G|EX∈uG [kX − µP k2 ] − |R|EX∈uR [kX − µP k2 ] ≤ |R| ε/2 ,
√
√
and since by (i), EX∈uR [kX − µP k2 ] ≤ ε/3, it follows that EX∈uG0 [kX − µP k2 ] ≤ ε.
An iteration of FilterUnder2ndMoment may throw out more samples from G than corrupted
samples. However, in expectation, we throw out many more corrupted samples than from the good
set:
11

Algorithm 2 Filter under second moment assumptions
1: function FilterUnder2ndMoment(S)
2:
Compute µS , ΣS , the mean and covariance matrix of S.
3:
Find the eigenvector v ∗ with highest eigenvalue λ∗ of ΣS .
4:
if λ∗ ≤ 9 then
5:
return µS
6:
else
7:
Draw Z from the distribution on [0, 1] with probability density function 2x.
8:
Let T = Z max{|v ∗ · x − µS | : x ∈ S}.
9:
Return the set S 0 = {x ∈ S : |v ∗ · (X − µS )| < T }.
10:
end if
11: end function
Proposition 2. If we run FilterUnder2ndMoment on a set S = G ∪ E \ L for some ε-good set G
√
and disjoint E, L with |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, then either it returns µS with kµS − µP k2 ≤ O( ε),
or else it returns a set S 0 ⊂ S with S 0 = G ∪ E 0 \ L0 for disjoint E 0 and L0 . In the latter case we
have EZ [|E 0 | + 2|L0 |] ≤ |E| + 2|L|.
For D ∈ {G, E, L, S}, let µD be the mean of D and MD be the matrix EX∈u D [(X−µS )(X−µS )T ].
p
Lemma 8. If G is an ε-good set with x ≤ 40 d/ε for x ∈ S ∪ G, then kMG k2 ≤ 2kµG − µS k22 + 2 .
Proof. For any unit vector v, we have
v T MG v = EX∈u G [(v · (X − µS ))2 ]
= EX∈u G [(v · (X − µG ) + v · (µP − µG ))2 ]
= v T ΣG v + (v · (µG − µS ))2
≤ 2 + 2kµG − µS k22 .

Lemma 9. We have that |L|kML k2 ≤ 2|G|(1 + kµG − µS k22 ) .
Proof. Since L ⊆ G, for any unit vector v, we have
|L|v T ML v = |L|EX∈u L [(v · (X − µS ))2 ]
≤ |G|EX∈u G [(v · (X − µS ))2 ]
≤ 2|G|(1 + kµG − µS k22 ) .

Lemma 10. kµG − µS k2 ≤

p
√
2εkMS k2 + 12 ε.

Proof. We have that |E|ME ≤ |S|MS + |L|ML and so
|E|kME k2 ≤ |S|kMS k2 + 2|G|(1 + kµG − µS k22 ) .
By Cauchy Schwarz, we have that kME k2 ≥ kµE − µS k22 , and so
q
p
|E|kµE − µS k2 ≤ |S|kMS k2 + 2|G|(1 + kµG − µS k22 ) .
12

By Cauchy-Schwarz and Lemma 9, we have that
q
p
p
L
S
|L|kµ − µ k2 ≤ |L|kML k2 ≤ 2|G|(1 + kµG − µS k22 ) .
Since |S|µS = |G|µG + |E|µE − |L|µL and |S| = |G| + |E| − |L|, we get
|G|(µG − µS ) = |E|(µE − µS ) − |L|(µE − µS ) .
Substituting into this, we obtain
q
q
|G|kµG − µS k2 ≤ |E||S|kMS k2 + 2|E||G|(1 + kµG − µS k22 ) + 2|L||G|(1 + kµG − µS k22 ) .
√
√
√
Since for x, y > 0, x + y ≤ x + y, we have
p
p
p
|G|kµG − µS k2 ≤ |E||S|kMS k2 + ( 2|E||G| + 2|L||G|)(1 + kµG − µS k2 ) .
Since ||G| − |S|| ≤ ε|S| and |E| ≤ 2ε|S|, |L| ≤ 9ε|S|, we have
p
√
kµG − µS k2 ≤ 2εkMS k2 + (6 ε)(1 + kµG − µS k2 ) .
√
Moving the kµG − µS k2 terms to the LHS, using 6 ε ≤ 1/2, gives
p
√
kµG − µS k2 ≤ 2εkMS k2 + 12 ε .

Since λ∗ = kMS k2 , the correctness if we return the empirical mean is immediate.
√
Corollary 3. If λ∗ ≤ 9, we have that kµG − µS k2 = O( ε).
From now on, we assume λ∗ > 9. In this case we have kµG − µS k22 ≤ O(ελ∗ ). Using Lemma 8,
we have
kMG k2 ≤ 2 + O(ελ∗ ) ≤ 2 + λ∗ /5
for sufficiently small ε. Thus, we have that
v ∗T MS v ∗ ≥ 4v ∗T MG v ∗ .

(7)

Now we can show that in expectation, we throw out many more corrupted points from E than
from G \ L:
Lemma 11. Let S 0 = G ∪ E 0 \ L0 for disjoint E 0 , L0 be the set of samples returned by the iteration.
Then we have EZ [|E 0 | + 2|L0 |] ≤ |E| + 2|L|.
Proof. Let a = maxx∈S |v ∗ · x − µS |. Firstly, we look at the expected number of samples we reject:


0
S
EZ [|S |] − |S| = EZ |S| Pr [|X − µ | ≥ aZ]
X∈u S
Z 1


= |S|
Pr |v ∗ · (X − µS )| ≥ ax 2xdx
X∈u S
Z0 a


= |S|
Pr |v ∗ · (X − µS )| ≥ T (2T /a)dT
0 X∈u S


= |S|EX∈u S (v ∗ · (X − µS ))2 /a
= (|S|/a) · v ∗T MS v ∗ .
13

Next, we look at the expected number of false positive samples we reject, i.e., those in L0 \ L.




EZ [|L0 |] − |L| = EZ (|G| − |L|) Pr
|X − µS | ≥ T
X∈u G\L


∗
S
≤ EZ |G| Pr [|v · (X − µ )| ≥ aZ]
X∈u G
Z 1
= |G|
Pr [|v ∗ · (X − µS )| ≥ ax]2x dx
0 X∈u G
Z a
Pr [|v ∗ · (X − µS )| ≥ T ](2T /a) dT
= |G|
X∈
uG
Z0 ∞
Pr [|v ∗ · (X − µS )| ≥ T ](2T /a) dT
≤ |G|
X∈
uG
0
 ∗

= |G|EX∈u G (v · (X − µS ))2 /a
= (|G|/a) · v ∗T MG v ∗ .
Using (7), we have |S|v ∗T MS v ∗ ≥ 4|G|v ∗T MG v ∗ and so EZ [S 0 ] − S ≥ 3(EZ [L0 ] − L). Now consider
that |S 0 | = |G|+|E 0 |−|L0 | = |S|−|E|+|E 0 |+|L|−|L0 |, and thus |S 0 |−|S| = |E|−|E 0 |+|L0 |−|L|. This
yields that |E|−EZ [|E 0 |] ≥ 2(EZ [L0 ]−L), which can be rearranged to EZ [|E 0 |+2|L0 |] ≤ |E|+2|L|.
Proof of Proposition 2. If λ∗ ≤ 9, then we return the mean in Step 5, and by Corollary 3, kµS −
√
µP k2 ≤ O( ε).
If λ∗ > 9, then we return S 0 . Since at least one element of S has |v ∗ · X| = maxx∈S |v ∗ · X|,
whatever value of Z is drawn, we still remove at least one element, and so have S 0 ⊂ S. By Lemma
11, we have EZ [|E 0 | + 2|L0 |] ≤ |E| + 2|L|.
Proof of Theorem 4. Our input is a set S of N = Θ((d/ε) log d) ε-corrupted samples so that with
probability 9/10, S is a 2ε-corrupted set of ε-good samples for P by Lemmas 5 and 7. We have
a set S = G ∪ E 0 \ L, where G0 is an ε-good set, |E| ≤ 2ε, and |L| ≤ ε. Then, we iteratively
apply FilterUnder2ndMoment until it outputs an approximation to the mean. Since each
iteration removes a sample, this must happen within N iterations. The algorithm takes at most
poly(N, d) = poly(d, 1/ε) time.
As long as we can show that the conditions of Proposition 2 hold in each iteration, it ensures that
√
S
kµ − µP k2 ≤ O( ε). However, the condition that |L| ≤ 9ε|S| need not hold in general. Although
in expectation we reject many more samples in E than G, it is possible that we are unlucky and
reject many samples in G, which could make L large in the next iteration. Thus, we need a bound
on the probability that we ever have |L| > 9ε.
We analyze the following procedure: We iteratively run FilterUnder2ndMoment starting
with a set Si ∪ Ei \ Li of samples with S0 = S and producing a set Si+1 = G ∪ Ei+1 \ Li+1 . We stop
if we output an approximation to the mean or if |Li+1 | ≥ 13ε|S|. Since we do now always satisfy
the conditions of Proposition 2, this gives that EZ [|Ei+1 | + |Li+1 |] = |Ei | + 2|Li |. This expectation
is conditioned on the state of the algorithm after previous iterations, which is determined by Si .
Thus, if we consider the random variables Xi = |Ei | + 2|Li |, then we have E[Xi+1 |Si ] ≤ Xi , i.e.,
the sequence Xi is a sub-martingale with respect to Xi . Using the convention that Si+1 = Si , if
we stop in less than i iterations, and recalling that we always stop in N iterations, the algorithm
fails if and only if |LN | > 9ε|S|. By a simple induction or standard results on sub-martingales, we
have E[XN ] ≤ X0 . Now X0 = |E0 | + 2|L0 | ≤ 3ε|S|. Thus, E[XN ] ≤ 3ε|S|. By Markov’s inequality,
except with probability 1/6, we have XN ≤ 18ε|S|. In this case, |LN | ≤ XN /2 ≤ 9ε|S|. Therefore,
the probability that we ever have |Li | > 9ε is at most 1/6.
14

By a union bound, the probability that the uncorrupted samples satisfy Lemma 5 and Proposition 2 applies to every iteration is at least 9/10 − 1/6 ≥ 2/3. Thus, with at least 2/3 probability,
√
the algorithm outputs a vector µ
b with kb
µ − µP k2 ≤ O( ε).

1.3

Robust Covariance Estimation

In this subsection, we give a near sample-optimal efficient robust estimator for the covariance of
a zero-mean Gaussian density, thus proving Theorem 3.3. Our algorithm is essentially identical
to the filtering algorithm given in Section 8.2 of [DKK+ 16]. As in Section 1.1 the only difference
is a weaker definition of the “good set of samples” (Definition 5) and a concentration argument
(Lemma 3) showing that a random set of uncorrupted samples of the appropriate size is good with
high probability. Given these, the analysis of this subsection follows straightforwardly from the
analysis in Section 8.2 of [DKK+ 16] by plugging in the modified parameters.
The algorithm Filter-Gaussian-Unknown-Covariance to robustly estimate the covariance
of a mean 0 Gaussian in [DKK+ 16] is as follows:
Algorithm 3 Filter algorithm for a Gaussian with unknown covariance matrix.
1: procedure Filter-Gaussian-Unknown-Covariance(S 0 , ε, τ )
input: A multiset S 0 such that there exists an (ε, τ )-good set S with ∆(S, S 0 ) ≤ 2ε
output: Either a set S 00 with ∆(S, S 00 ) < ∆(S, S 0 ) or the parameters of a Gaussian G0 with
dT V (G, G0 ) = O( log(1/)).
Let C > 0 be a sufficiently large universal constant.
2:
Let Σ0 be the matrix EX∈u S 0 [XX T ] and let G0 be the mean 0 Gaussian with covariance
matrix Σ0 .
3:
if there is any x ∈ S 0 so that xT (Σ0 )−1 x ≥ Cd log(|S 0 |/τ ) then
4:
return S 00 = S 0 − {x : xT (Σ0 )−1 x ≥ Cd log(|S 0 |/τ )}.
5:
end if
6:
Compute an approximate eigendecomposition of Σ0 and use it to compute Σ0−1/2
7:
Let x(1) , . . . , x(|S 0 |) be the elements of S 0 .
⊗2
.
8:
For i = 1, . . . , |S 0 |, let y(i) = Σ0−1/2 x(i) and z(i) = y(i)
0|
P
|S
T .
9:
Let TS 0 = −I [ I [T + (1/|S 0 |) i=1 z(i) z(i)
∗
10:
Approximate the top eigenvalue λ and corresponding unit eigenvector v ∗ of TS 0 ..
11:
Let p∗ (x) = √12 ((Σ0−1/2 x)T v ∗] (Σ0−1/2 x) − tr(v ∗] ))
12:
13:
14:
15:
16:

if λ∗ ≤ (1 + C log2 (1/))QG0 (p∗ ) then
return G0
end if
Let µ be the median value of p∗ (X) over X ∈ S 0 .
Find a T ≥ C 0 so that
Pr (|p∗ (X) − µ| ≥ T + 4/3) ≥ Tail(T, d, ε, τ )

X∈u S 0

return S 00 = {X ∈ S 0 : |p∗ (X) − µ| < T }.
18: end procedure
17:

In [DKK+ 16], we take Tail(T, d, ε, τ ) = 12 exp(−T )+3/(d log(N/τ ))2 , where N = Θ((d log(d/ετ ))6 /ε2 )
is the number of samples we took there.
15

To get a near sample-optimal algorithms, we will need a weaker definition of a good set. To
use this, we will need to weaken the tail bound in the algorithm to Tail(T, d, ε, τ ) = ε/(T 2 log2 (T )),
when T ≥ 10 log(1/ε). For T ≤ 10 log(1/ε), we take Tail(T, d, ε, τ ) = 1 so that we always choose
T ≥ 10 log(1/ε). It is easy to show that the integrals of this tail bound used in the proofs of Lemma
8.19 and Claim 8.22 of [DKK+ 16] have similar bounds. Thus, our analysis here will sketch that
these tail bounds hold for a set of Ω(d2 log5 (d/ετ )/ε2 ) samples from the Guassian.
Firstly, we state the new, weaker, definition of a good set:
Definition 5. Let G be a Gaussian in Rd with mean 0 and covariance Σ. Let  > 0 be sufficiently
small. We say that a multiset S of points in Rd is ε-good with respect to G if the following hold:
√
1. For all x ∈ S, xT Σ−1 x < d + O( d log(d/ε)).
2. We have that kΣ−1/2 Cov(S)Σ−1/2 − IkF = O(ε).
3. For all even degree-2 polynomials p, we have that Var(p(S)) = Var(p(G))(1 + O(ε)).
4. For p an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1, and for any T >
10 log(1/ε) we have that
Pr (|p(x)| > T ) ≤ ε/(T 2 log2 (T )).
x∈u S

It is easy to see that the algorithm and analysis of [DKK+ 16] can be pushed through using the
above weaker definition. That is, if S is a good set, then G can be recovered to Õ(ε) error from an
ε-corrupted version of S. Our main task will be to show that random sets of the appropriate size
are good with high probability.
Proposition 3. Let N be a sufficiently large constant multiple of d2 log5 (d/ε)/ε2 . Then a set S of
N independent samples from G is ε-good with respect to G with high probability.
Proof. First, note that it suffices to prove this when G = N (0, I).
Condition 1 follows by standard concentration bounds on kxk22 .
Condition 2 follows by estimating the entry-wise error between Cov(S) and I.
Condition 3 is slightly more involved. Let {pi } be an orthonormal basis for the set of even,
degree-2, mean-0 polynomials with respect to G. Define the matrix Mi,j = Ex∈u S [pi (x)pj (x)] − δi,j .
This condition is equivalent to kM k2 = O(ε). Thus, it suffices to show that for every v with kvk2 = 1
that v T M v = O(ε). It actually suffices
P to consider a cover of such v’s. Note that this cover will be
2)
O(d
of size 2
. For each v, let pv = i vi pi . We need to show that Var(pv (S)) = 1 + O(ε). We can
2
show this happens with probability 1 − 2−Ω(d ) , and thus it holds for all v in our cover by a union
bound.
Condition 4 is substantially the most difficult of these conditions to prove. Naively, we would
want to find a cover of all possible p and all possible T , and bound the probability that the desired
condition fails. Unfortunately, the best a priori bound on Pr(|p(G)| > T ) are on the order of
2
exp(−T ). As our cover would need to be of size 2d or so, to make this work with T = d, we would
require on the order of d3 samples in order to make this argument work.
However, we will note that this argument is sufficient to cover the case of T < 10 log(1/ε) log2 (d/ε).
Fortunately, most such polynomials p satisfy much better tail bounds. Note that any even, mean
zero polynomial p can be written in the form p(x) = xT Ax − tr(A) for some matrix A. We call
A the associated matrix to p. We note by the Hanson-Wright inequality that Pr(|p(G)| > T ) =
exp(−Ω(min((T /kAkF )2 , T /kAk2 ))). Therefore, the tail bounds above are only as bad as described
when A has a single large eigenvalue. To take advantage of this, we will need to break p into parts
based on the size of its eigenvalues. We begin with a definition:
16

Definition 6. Let Pk be the set of even, mean-0, degree-2 polynomials, so that the associated matrix
A satisfies:
1. rank(A) ≤ k
√
2. kAk2 ≤ 1/ k.
√
√
Note that for p ∈ Pk that |p(x)| ≤ |x|2 / k + k.
Importantly, any polynomial can be written in terms of these sets.
Lemma 12. Let p be an even, degree-2 polynomial with E[p(G)] = 0, Var(p(G)) = 1. Then if
t = blog2 (d)c, it is possible to write p = 2(p1 + p2 + . . . + p2t + pd ) where pk ∈ Pk .
Proof. Let A be the associated matrix to p. Note that kAkF = Varp = 1. Let Ak be the matrix
corresponding to the top k eigenvalues of A. We now let p1 be the polynomial associated to A1 /2,
p2 be associated to (A2 − A1 )/2, p4 be associated to (A4 − A2 )/2, and so on. It is clear that
p = 2(p1 + p2 + . . . + p2t + pd ). It is also clear that the matrix associated
to pk has rank at most
√
k. If the matrix associated to pk had an eigenvalue more than
1/
k,
it
would
need to be the case
√
nd
that the k/2 largest eigenvalue of A had size at least 2/ k. This is impossible since the sum of
the squares of the eigenvalues of A is at most 1.
This completes our proof.
We will also need covers of each of these sets Pk .
Lemma 13. For each k, there exists a set Ck ⊂ Pk so that
1. For each p ∈ Pk there exists a q ∈ Ck so that kp(G) − q(G)k2 ≤ (ε/d)2 .
2. |Ck | = 2O(dk log(d/ε)) .
Pk
λi vi viT , for
Proof. We note that any such p is associated to a matrix A of the form A =
i=1
√
P
k
λi ∈ [0, 1/ k] and vi orthonormal. It suffices to let q correspond to the matrix A0 = i=1 µi wi wiT
for with |λi − µi | < (ε/d)3 and |vi − wi | < (ε/d)3 for all i. It is easy to let µi and wi range over
covers of the interval and the sphere with appropriate errors. This gives a set of possible q’s of
size 2O(dk log(d/ε)) as desired. Unfortunately, some of these q will not be in Pk as they will have
eigenvalues that are too large. However, this is easily fixed by replacing each such q by the closest
element of Pk . This completes our proof.
We next will show that these covers are sufficient to express any polynomial.
Lemma 14. Let p be an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1. It is
possible to write p as a sum of O(log(d)) elements of some Ck plus another polynomial of L2 norm
at most ε/d.
Proof. Combining the above two lemmas we have that any such p can be written as
t

p = (q1 + p1 ) + (q2 + p2 ) + . . . (q2t + p2t ) + (qd + pd ) = q1 + q2 + . . . + q 2 + q d + p0 ,
where qk above is in Ck and kpk (G)k2 < (ε/d)2 . Thus, p0 = p1 + p2 + . . . + p2t + pd has kp0 (G)k2 ≤
(ε/d). This completes the proof.

17

p
The key observation now is that if |p(x)| ≥ T for kxk2 ≤ d/ε, then writing p = q1 + q2 + q4 +
. . . + qd + p0 as above, it must be the case that |qk (x)| > (T − 1)/(2 log(d)) for some k. Therefore,
to prove our main result, it suffices to show that, with high probability over the choice of S, for
any T ≥ 10 log(1/ε) log2 (d/ε) and any q ∈ Ck for some k, that Prx∈u S (|q(x)| > T /(2 log(d))) <
ε/(2T 2 log2 (T ) log(d)). Equivalently, it suffices to show that for T ≥ 10 log(1/ε) log(d/ε) it holds
Prx∈u S (|q(x)| > T /(2 log(d))) < ε/(2T 2 log2 (T ) log2 (d)). Note
p that this holds automatically for
T > (d/ε), as p(x) cannot possibly be that large for kxk2 ≤ d/ε. Furthermore, note that losing a
constant factor in the probability, it suffices to show this only for T a power of 2. √
Therefore, it suffices to show for every k ≤ d, every q ∈ Ck and every d/ kε  T 
log(1/ε) log(d/ε) that with probability at least 1 − 2−Ω(dk log(d/ε)) over the choice of S we have
that Prx∈u S (|q(x)| > T )  ε/(T 2 log4 (d/ε)). However, by the Hanson-Wright inequality, we have
that
√
Pr(|q(G)| > T ) = exp(−Ω(min(T 2 , T k))) < (ε/(T 2 log4 (d/ε)))2 .
Therefore, by Chernoff bounds, the probability that more than a ε/(T 2 log4 (d/ε))-fraction of the
elements of S satisfy this property is at most
√
√
exp(−Ω(min(T 2 , T k))|S|ε/(T 2 log4 (d/ε))) = exp(−Ω(|S|ε/(log4 (d/ε)) min(1, k/T )))
≤ exp(−Ω(|S|ε2 /(log4 (d/ε))k/d))
≤ exp(−Ω(dk log(d/ε))) ,
as desired.
This completes our proof.

2
2.1

Omitted Details from Section 5
Full description of the distributions for experiments

Here we formally describe the distributions we used in our experiments. In all settings, our goal
was to find noise distributions so that noise points were not “obvious” outliers, in the sense that
there is no obvious pointwise pruning process which could throw away the noise points, which still
gave the algorithms we tested the most difficulty. We again remark that while other algorithms had
varying performances depending on the noise distribution, it seemed that the performance of ours
was more or less unaffected by it.
Distribution for the synthetic mean experiment Our uncorrupted points were generated by
N (µ, I), where µ is the all-ones vector. Our noise distribution is given as
1
1
N = Π1 + Π2 ,
2
2
where Π1 is the product distribution over the hypercube where every coordinate is 0 or 1 with
probability 1/2, and Π2 is a product distribution where the first coordinate is ether 0 or 12 with equal
probability, the second coordinate is −2 or 0 with equal probability, and all remaining coordinates
are zero.
Distribution for the synthetic covariance experiment For the isotropic synthetic covariance
experiment, our uncorrupted points were generated by N (0, I), and the noise points were all zeros.
For the skewed synthetic covariance experiment, our uncorrupted points were generated by N (0, I +
18

10e1 eT1 ), where e1 is the first unit vector, and our noise points were generated as follows: we took
a fixed random rotation of points of the form Yi ∼ Π, where Π is a product distribution whose
first d/2 coordinates are ±0.5 with probability 1/2, and whose next d/2 − 1 coordinates are each
0.8 × Ai , where for each coordinate i, Ai is an independent random integer between −2 and 2, and
whose last coordinate is a uniformly random integer between [−10, 10].
Setup for the semi-synthetic geographic experiment We took the 20 dimensional data
from [NJB+ 08], which was diagonalized, and randomly rotated it. This was to simulate the higher
dimensional case, since the singular vectors that [NJB+ 08] obtained did not seem to be sparse or
analytically sparse. Our noise was distributed as Π, where Π is a product distribution whose first
d/2 coordinates are each uniformly random integers between 0 and 2 and whose last d/2 coordinates
are each uniformly randomly either 2 or 3, all scaled by a factor of 1/24.

2.2

Comparison with other robust PCA methods on semi-synthetic data

In addition to comparing our results with simple pruning techniques, as we did in Figure 3 in the
main text, we also compared our algorithm with implementations of other robust PCA techniques
from the literature with accessible implementations. In particular, we compared our technique with
RANSAC-based techniques, LRVCov, two SDPs ([CLMW11, XCS10]) for variants of robust PCA,
and an algorithm proposed by [CLMW11] to speed up their SDP based on alternating descent. For
the SDPs, since black box methods were too slow to run on the full data set (as [CLMW11] mentions,
black-box solvers for the SDPs are impractical above perhaps 100 data points), we subsample the
data, and run the SDP on the subsampled data. For each of these methods, we ran the algorithm
on the true data points plus noise, where the noise was generated as described above. We then take
the estimate of the covariance it outputs, and project the data points onto the top two singular
values of this matrix, and plot the results in Figure 1.
Similar results occurred for most noise patterns we tried. We found that only our algorithm
and LRVCov were able to reasonably reconstruct Europe, in the presence of this noise. It is hard to
judge qualitatively which of the two maps generated is preferable, but it seems that ours stretches
the picture somewhat less than LRVCov.

19

Original Data

Filter Projection

-0.2

-0.2

-0.1

-0.1

0

0

0.1

0.1

0.2

0.2

0.3

0.3

-0.15

-0.1

-0.05

0

0.05

0.1

0.15

0.2

0.15

0.1

0.05

RANSAC Projection

0.2

0

-0.05

-0.1

-0.15

-0.2

0.1

0.15

0.2

0.1

0.15

0.2

LRV Projection
-0.2

0.15
-0.1
0.1

0.05

0

0

0.1

-0.05
0.2
-0.1
0.3
-0.15
-0.2

-0.1

0

0.1

0.2

0.3

-0.15

-0.1

-0.05

0.2

0

0.05

CLMW SDP Projection

CLMW ADMM Projection
0.3

0.15

0.2
0.1

0.05

0.1

0

0

-0.05

-0.1
-0.1

-0.2
-0.15
-0.2

-0.1

0

0.1

0.2

-0.15

0.3

-0.1

-0.05

0

0.05

XCS Projection
0.3

0.2

0.1

0

-0.1

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

0.15

0.2

Figure 1: Comparison with other robust methods on the Europe semi-synthetic data. From left to
right, top to bottom: the original projection without noise, what our algorithm recovers, RANSAC,
LRVCov, the ADMM method proposed by [CLMW11], the SDP proposed by [XCS10] with subsampling, and the SDP proposed by [CLMW11] with subsampling.
20

References
[CLMW11] E. J. Candès, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J.
ACM, 58(3):11, 2011.
[DKK+ 16] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proceedings
of FOCS’16, 2016. Full version available at https://arxiv.org/pdf/1604.06443.pdf.
[DL01]

L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer
Series in Statistics, Springer, 2001.

[NJB+ 08]

J. Novembre, T. Johnson, K. Bryc, Z. Kutalik, A. R. Boyko, A. Auton, A. Indap, K. S.
King, S. Bergmann, M. R. Nelson, et al. Genes mirror geography within europe. Nature,
456(7218):98–101, 2008.

[T+ 15]

J. A. Tropp et al. An introduction to matrix concentration inequalities. Foundations
and Trends in Machine Learning, 8(1-2):1–230, 2015.

[Ver10]

R. Vershynin. Introduction to the non-asymptotic analysis of random matrices, 2010.

[XCS10]

H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. In Advances in
Neural Information Processing Systems, pages 2496–2504, 2010.

21

