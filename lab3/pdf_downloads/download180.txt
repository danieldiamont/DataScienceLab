Supplementary Material for
Being Robust (in High Dimensions) Can Be Practical
Ilias Diakonikolas
CS, USC
diakonik@usc.edu

Gautam Kamath
EECS & CSAIL, MIT
g@csail.mit.edu

Jerry Li
EECS & CSAIL, MIT
jerryzli@mit.edu

1

Daniel M. Kane
CSE & Math, UCSD
dakane@cs.ucsd.edu

Ankur Moitra
Math & CSAIL, MIT
moitra@mit.edu

Alistair Stewart
CS, USC
alistais@usc.edu

Omitted Details from Section 3

1.1

Robust Mean Estimation for Sub-Gaussian Distributions

In this section, we use our filter technique to give a near sample-optimal computationally efficient
algorithm to robustly estimate the mean of a sub-gaussian density with a known covariance matrix,
thus proving Theorem 3.1.
We emphasize that the algorithm and its analysis is essentially identical to the filtering algorithm
given in Section 8.1 of [DKK+ 16] for the case of a Gaussian N (Âµ, I). The only difference is a
weaker definition of the â€œgood set of samplesâ€ (Definition 2) and a simple concentration argument
(Lemma 1) showing that a random set of uncorrupted samples of the appropriate size is good
with high probability. Given these, the analysis of this subsection follows straightforwardly from
the analysis in Section 8.1 of [DKK+ 16] by plugging in the modified parameters. For the sake of
completeness, we provide the details below.
We start by formally defining sub-gaussian distributions:
Definition 1. A distribution P on R with mean Âµ, is sub-gaussian with parameter Î½ > 0 if
EXâˆ¼P [exp(Î»(X âˆ’ Âµ))] â‰¤ exp(Î½Î»2 /2)
for all Î» âˆˆ R. A distribution P on Rd with mean vector Âµ is sub-gaussian with parameter Î½ > 0,
if for all unit vectors v, the one-dimensional random variable v Â· X, X âˆ¼ P , is sub-gaussian with
parameter Î½.
We will use the following simple fact about the concentration of sub-gaussian random variables:
Fact 1. If P is sub-gaussian on Rd with mean vector Âµ and parameter Î½ > 0, then for any unit
vector v âˆˆ Rd we have that PrXâˆ¼P [|v Â· (X âˆ’ Âµ)| â‰¥ T ] â‰¤ exp(âˆ’t2 /2Î½).
The following theorem is a high probability version of Theorem 3.1:
Theorem 2. Let G be a sub-gaussian distribution on Rd with parameter Î½ = Î˜(1), mean ÂµG ,
covariance matrix I, and Îµ, Ï„ > 0. Let S 0 be an Îµ-corrupted set of samples from G of size
â„¦((d/Îµ2 )poly log(d/ÎµÏ„ )). There exists an efficient algorithm that, on input S 0 and
p Îµ > 0, returns a
mean vector Âµ
b so that with probability at least 1 âˆ’ Ï„ we have kb
Âµ âˆ’ ÂµG k2 = O(Îµ log(1/Îµ)).
1

1 P
1 P
G
G T
Notation. We will denote ÂµS = |S|
XâˆˆS X and MS = |S|
XâˆˆS (X âˆ’ Âµ )(X âˆ’ Âµ ) for the
sample mean and modified sample covariance matrix of the set S.

We start by defining our modified notion of good sample, i.e, a set of conditions on the uncorrupted set of samples under which our algorithm will succeed.
Definition 2. Let G be an identity covariance sub-gaussian in d dimensions with mean ÂµG and
covariance matrix I and Îµ, Ï„ > 0. We say that a multiset S of elements in Rd is (Îµ, Ï„ )-good with
respect to G if the following conditions are satisfied:
p
(i) For all x âˆˆ S we have kx âˆ’ ÂµG k2 â‰¤ O( d log(|S|/Ï„ )).
(ii) For every affine function L : Rd â†’ R such that L(x) = v Â· (x âˆ’ ÂµG ) âˆ’ T , kvk2 = 1, we have
that |PrXâˆˆu S [L(X) â‰¥ 0] âˆ’ PrXâˆ¼G [L(X) â‰¥ 0]| â‰¤ T 2 log dÎµlog( d ) .
(
ÎµÏ„ )
(iii) We have that kÂµS âˆ’ ÂµG k2 â‰¤ Îµ.
(iv) We have that kMS âˆ’ Ik2 â‰¤ Îµ.
We show in the following subsection that a sufficiently large set of independent samples from G
is (Îµ, Ï„ )-good (with respect to G) with high probability. Specifically, we prove:
Lemma 1. Let G be sub-gaussian distribution with parameter Î½ = Î˜(1) and with identity covariance,
and Îµ, Ï„ > 0. If the multiset S is obtained by taking â„¦((d/Îµ2 )poly log(d/ÎµÏ„ )) independent samples
from G, it is (Îµ, Ï„ )-good with respect to G with probability at least 1 âˆ’ Ï„.
We require the following definition that quantifies the extent to which a multiset has been
corrupted:
Definition 3. Given finite multisets S and S 0 we let âˆ†(S, S 0 ) be the size of the symmetric difference
of S and S 0 divided by the cardinality of S.
The starting point of our algorithm will be a simple NaivePrune routine (Section 4.3.1 of
[DKK+ 16]) that removes obvious outliers, i.e., points which are far from the mean. Then, we
iterate the algorithm whose performance guarantee is given by the following:
Proposition 1. Let G be a sub-gaussian distribution on Rd with parameter Î½ = Î˜(1), mean ÂµG ,
covariance matrix I, Îµ > 0 be sufficiently small and Ï„ > 0. Let S be an (Îµ, Ï„ )-good set
p with respect
to G. Let S 0 be any multiset with âˆ†(S, S 0 ) â‰¤ 2Îµ and for any x, y âˆˆ S 0 , kx âˆ’ yk2 â‰¤ O( d log(d/ÎµÏ„ )).
There exists a polynomial time algorithm Filter-Sub-Gaussian-Unknown-Mean that, given S 0
and Îµ > 0, returns one of the following:
p
(i) A mean vector Âµ
b such that kb
Âµ âˆ’ ÂµG k2 = O(Îµ log(1/Îµ)).
def

d
(ii) A multiset S 00 âŠ† S 0 such that âˆ†(S, S 00 ) â‰¤ âˆ†(S, S 0 )âˆ’Îµ/Î±, where Î± = d log(d/ÎµÏ„ ) log(d log( ÎµÏ„
)).

We start by showing how Theorem 2 follows easily from Proposition 1.
Proof of Theorem 2. By the definition of âˆ†(S, S 0 ), since S 0 has been obtained from S by corrupting
an Îµ-fraction of the points in S, we have that âˆ†(S, S 0 ) â‰¤ 2Îµ. By Lemma 1, the set S of uncorrupted
samples is (Îµ, Ï„ )-good with respect to G with probability at least 1 âˆ’ Ï„. We henceforth condition on
this event.
p
Since S is (Îµ, Ï„ )-good, all x âˆˆ S have kx âˆ’ ÂµG k2 â‰¤ O( d log |S|/Ï„ ). Thus, the NaivePrune
procedure does not remove from S 0 any member of S. Hence, its output, S 00 , has âˆ†(S, S 00 ) â‰¤ âˆ†(S, S 0 )
2

p
and for any x âˆˆ S 00 , there is a y p
âˆˆ S with kx âˆ’ yk2 p
â‰¤ O( d log |S|/Ï„ ). By the triangle inequality,
for any x, z âˆˆ S 00 , kx âˆ’ zk2 â‰¤ O( d log |S|/Ï„ ) = O( d log(d/ÎµÏ„ )).
Then, we iteratively apply the Filter-Sub-Gaussian-Unknown-Meanpprocedure of Proposition 1 until it terminates returning a mean vector Âµ with kb
Âµ âˆ’ ÂµG k2 = O(Îµ log(1/Îµ)). We claim
that we need at most O(Î±) iterations for this to happen. Indeed, the sequence of iterations results
in a sequence of sets Si0 , so that âˆ†(S, Si0 ) â‰¤ âˆ†(S, S 0 )âˆ’iÂ·Îµ/Î±. Thus, if we do not output the empirical
mean in the first 2Î± iterations, in the next iteration there are no outliers left and the algorithm
terminates outputting the sample mean of the remaining set.
1.1.1

Algorithm Filter-Sub-Gaussian-Unknown-Mean: Proof of Proposition 1

In this subsection, we describe the efficient algorithm establishing Proposition 1 and prove its
0
correctness. Our algorithm calculates the empirical mean vector ÂµS and empirical covariance matrix
0
Î£. If the matrix Î£ has no large eigenvalues, it returns ÂµS . Otherwise, it uses the eigenvector v âˆ—
0
corresponding to the maximum magnitude eigenvalue of Î£ and the mean vector ÂµS to define a
filter. Our efficient filtering procedure is presented in detailed pseudocode below.
Algorithm 1 Filter algorithm for a sub-gaussian with unknown mean and identity covariance
1: procedure Filter-Sub-Gaussian-Unknown-Mean(S 0 , Îµ, Ï„ )
input: A multiset S 0 such that there exists an (Îµ, Ï„ )-good S with âˆ†(S, S 0 ) â‰¤ 2Îµ
output: Multiset S 00 or mean vector Âµ
b satisfying Proposition 1
0
S
2:
Compute the sample mean Âµ = EXâˆˆu S 0 [X] and the sample covariance matrix Î£ , i.e.,
0
0
Î£ = (Î£i,j )1â‰¤i,jâ‰¤d with Î£i,j = EXâˆˆu S 0 [(Xi âˆ’ ÂµSi )(Xj âˆ’ ÂµSj )].
3:
Compute approximations for the largest absolute eigenvalue of Î£ âˆ’ I, Î»âˆ— := kÎ£ âˆ’ Ik2 , and
the associated unit eigenvector v âˆ— .
0
4:
if kÎ£ âˆ’ Ik2 â‰¤ O(Îµ log(1/Îµ)), then return ÂµS .
5:
end if
p
6:
Let Î´ := 3 ÎµkÎ£ âˆ’ Ik2 . Find T > 0 such that
h
i
Îµ
0
.
Pr 0 |v âˆ— Â· (X âˆ’ ÂµS )| > T + Î´ > 8 exp(âˆ’T 2 /2Î½) + 8
d
2
Xâˆˆu S
T log d log( ÎµÏ„
)
0

return the multiset S 00 = {x âˆˆ S 0 : |v âˆ— Â· (x âˆ’ ÂµS )| â‰¤ T + Î´}.
8: end procedure
7:

1.1.2

Proof of Correctness of Filter-Sub-Gaussian-Unknown-Mean

By definition, there exist disjoint multisets L, E, of points in Rd , where L âŠ‚ S, such that S 0 =
0
(S \ L) âˆª E. With this notation, we can write âˆ†(S, S 0 ) = |L|+|E|
|S| . Our assumption âˆ†(S, S ) â‰¤ 2Îµ is
equivalent to |L| + |E| â‰¤ 2Îµ Â· |S|, and the definition of S 0 directly implies that (1 âˆ’ 2Îµ)|S| â‰¤ |S 0 | â‰¤
(1 + 2Îµ)|S|. Throughout the proof, we assume that Îµ is a sufficiently small constant.
0
We define ÂµG , ÂµS , ÂµS , ÂµL , and ÂµE to be the means of G, S, S 0 , L, and E, respectively.
Our analysis will make essential use of the following matrices:
â€¢ MS 0 denotes EXâˆˆu S 0 [(X âˆ’ ÂµG )(X âˆ’ ÂµG )T ],
â€¢ MS denotes EXâˆˆu S [(X âˆ’ ÂµG )(X âˆ’ ÂµG )T ],
â€¢ ML denotes EXâˆˆu L [(X âˆ’ ÂµG )(X âˆ’ ÂµG )T ], and
3

â€¢ ME denotes EXâˆˆu E [(X âˆ’ ÂµG )(X âˆ’ ÂµG )T ].
Our analysis will hinge on proving the important claim that Î£âˆ’I is approximately (|E|/|S 0 |)ME .
This means two things for us. First, it means that if the positive errors align in some direction
(causing ME to have a large eigenvalue), there will be a large eigenvalue in Î£ âˆ’ I. Second, it says
that any large eigenvalue of Î£ âˆ’ I will correspond to an eigenvalue of ME , which will give an explicit
direction in which many error points are far from the empirical mean.
Useful Structural Lemmas. We begin by noting that we have concentration bounds on G and
therefore, on S due to its goodness.


Fact 3. Let w âˆˆ Rd be any
unit vector, then for any T > 0, PrXâˆ¼G |w Â· (X âˆ’ ÂµG )| > T â‰¤

2 exp(âˆ’T 2 /2Î½) and PrXâˆˆu S |w Â· (X âˆ’ ÂµG )| > T â‰¤ 2 exp(âˆ’T 2 /2Î½) + T 2 log dÎµlog( d ) .
(
ÎµÏ„ )
Proof. The first line is Fact 1, and the former follows from it using the goodness of S.
By using the above fact, we obtain the following simple claim:
Claim 1. Let w âˆˆ Rd be any unit vector, then for any T > 0, we have that:
0

0

Pr [|w Â· (X âˆ’ ÂµS )| > T + kÂµS âˆ’ ÂµG k2 ] â‰¤ 2 exp(âˆ’T 2 /2Î½).

Xâˆ¼G

and

0

0

Pr [|w Â· (X âˆ’ ÂµS )| > T + kÂµS âˆ’ ÂµG k2 ] â‰¤ 2 exp(âˆ’T 2 /2Î½) +

Xâˆˆu S

0

T 2 log

Îµ
.
d
)
d log( ÎµÏ„
0

Proof. This follows from Fact 3 upon noting that |w Â· (X âˆ’ ÂµS )| > T + kÂµS âˆ’ ÂµG k2 only if
|w Â· (X âˆ’ ÂµG )| > T .
We can use the above facts to prove concentration bounds for L. In particular, we have the
following lemma:
Lemma 2. We have that kML k2 = O (log(|S|/|L|) + Îµ|S|/|L|).
Proof. Since L âŠ† S, for any x âˆˆ Rd , we have that
|S| Â· Pr (X = x) â‰¥ |L| Â· Pr (X = x) .
Xâˆˆu S

Xâˆˆu L

(1)

Since ML is a symmetric matrix, we have kML k2 = maxkvk2 =1 |v T ML v|. So, to bound kML k2 it
suffices to bound |v T ML v| for unit vectors v. By definition of ML , for any v âˆˆ Rd we have that
|v T ML v| = EXâˆˆu L [|v Â· (X âˆ’ ÂµG )|2 ].

4

For unit vectors v, the RHS is bounded from above as follows:
Z âˆ




Pr |v Â· (X âˆ’ ÂµG )| > T T dT
EXâˆˆu L |v Â· (X âˆ’ ÂµG )|2 = 2
0 Xâˆˆu L
Z âˆš
O(

d log(d/ÎµÏ„ ))

Pr [|v Â· (X âˆ’ ÂµG )| > T ]T dT

=2
0

Z

âˆš

O(

Xâˆˆu L
d log(d/ÎµÏ„ ))

â‰¤2
0
Z 4âˆšÎ½ log(|S|/|L|)







|S|
min 1,
Â· Pr |v Â· (X âˆ’ ÂµG )| > T T dT
|L| Xâˆˆu S

T dT
0
O(

Z
+ (|S|/|L|)

âˆš

4

âˆš

d log(d/ÎµÏ„ )) 

Î½ log(|S|/|L|)

exp(âˆ’T 2 /2Î½) +


Îµ

T dT
d
T 2 log d log( ÎµÏ„
)

 log(|S|/|L|) + Îµ Â· |S|/|L| ,
where the second line follows from the fact that kvk2 = 1, L âŠ‚ S, and S satisfies condition (i) of
Definition 2; the third line follows from (1); and the fourth line follows from Fact 3.
As a corollary, we can relate the matrices MS 0 and ME , in spectral norm:
Corollary 1. We have that MS 0 âˆ’ I = (|E|/|S 0 |)ME + O(Îµ log(1/Îµ)), where the O(Îµ log(1/Îµ)) term
denotes a matrix of spectral norm O(Îµ log(1/Îµ)).
Proof. By definition, we have that |S 0 |MS 0 = |S|MS âˆ’ |L|ML + |E|ME . Thus, we can write
MS 0 = (|S|/|S 0 |)MS âˆ’ (|L|/|S 0 |)ML + (|E|/|S 0 |)ME
= I + O(Îµ) + O(Îµ log(1/Îµ)) + (|E|/|S 0 |)ME ,
where the second line uses the fact that 1 âˆ’ 2Îµ â‰¤ |S|/|S 0 | â‰¤ 1 + 2Îµ, the goodness of S (condition (iv)
in Definition 2), and Lemma 2. Specifically, Lemma 2 implies that (|L|/|S 0 |)kML k2 = O(Îµ log(1/Îµ)).
Therefore, we have that
MS 0 = I + (|E|/|S 0 |)ME + O(Îµ log(1/Îµ)) ,
as desired.
We now establish a similarly useful bound on the difference between the mean vectors:
p
p
0
Lemma 3. We have that ÂµS âˆ’ÂµG = (|E|/|S 0 |)(ÂµEpâˆ’ÂµG )+O(Îµ log(1/Îµ)), where the O(Îµ log(1/Îµ))
term denotes a vector with `2 -norm at most O(Îµ log(1/Îµ)).
Proof. By definition, we have that
0

|S 0 |(ÂµS âˆ’ ÂµG ) = |S|(ÂµS âˆ’ ÂµG ) âˆ’ |L|(ÂµL âˆ’ ÂµG ) + |E|(ÂµE âˆ’ ÂµG ).
Since S is a good set, by condition (iii) of Definition 2, we have kÂµS âˆ’ ÂµG k2 = O(Îµ). Since
1 âˆ’ 2Îµ â‰¤ |S|/|S 0 | â‰¤ 1 + 2Îµ, it follows that (|S|/|S 0 |)kÂµS âˆ’ ÂµG k2 = O(Îµ).
inequality
p Using the valid

p
L
G
2
L
G
kML k2 â‰¥ kÂµ âˆ’ Âµ k2 and Lemma 2, we obtain that kÂµ âˆ’ Âµ k2 â‰¤ O
log(|S|/|L|) + Îµ|S|/|L| .
Therefore,


p
p
p
(|L|/|S 0 |)kÂµL âˆ’ ÂµG k2 â‰¤ O (|L|/|S|) log(|S|/|L|) + Îµ|L|/|S| = O(Îµ log(1/Îµ)) .
5

In summary,

p
0
ÂµS âˆ’ ÂµG = (|E|/|S 0 |)(ÂµE âˆ’ ÂµG ) + O(Îµ log(1/Îµ)) ,

as desired. This completes the proof of the lemma.
By combining the above, we can conclude that Î£ âˆ’ I is approximately proportional to ME .
More formally, we obtain the following corollary:
Corollary 2. We have Î£ âˆ’ I = (|E|/|S 0 |)ME + O(Îµ log(1/Îµ)) + O(|E|/|S 0 |)2 kME k2 , where the
additive terms denote matrices of appropriately bounded spectral norm.
0

0

Proof. By definition, we can write Î£ âˆ’ I = MS 0 âˆ’ I âˆ’ (ÂµS âˆ’ ÂµG )(ÂµS âˆ’ ÂµG )T . Using Corollary 1 and
Lemma 3, we obtain:
Î£ âˆ’ I = (|E|/|S 0 |)ME + O(Îµ log(1/Îµ)) + O((|E|/|S 0 |)2 kÂµE âˆ’ ÂµG k22 ) + O(Îµ2 log(1/Îµ))
= (|E|/|S 0 |)ME + O(Îµ log(1/Îµ)) + O(|E|/|S 0 |)2 kME k2 ,
where the second line follows from the valid inequality kME k2 â‰¥ kÂµE âˆ’ ÂµG k22 . This completes the
proof.
0

Case of Small Spectral Norm. We are now ready to analyze the case that the mean vector ÂµS
def
is returned by the algorithm in Step 4. In this case, we have that Î»âˆ— = kÎ£ âˆ’ Ik2 = O(Îµ log(1/Îµ)).
Hence, Corollary 2 yields that
(|E|/|S 0 |)kME k2 â‰¤ Î»âˆ— + O(Îµ log(1/Îµ)) + O(|E|/|S 0 |)2 kME k2 ,
which in turns implies that
(|E|/|S 0 |)kME k2 = O(Îµ log(1/Îµ)) .
On the other hand, since kME k2 â‰¥ kÂµE âˆ’ ÂµG k22 , Lemma 3 gives that
p
p
p
0
kÂµS âˆ’ ÂµG k2 â‰¤ (|E|/|S 0 |) kME k2 + O(Îµ log(1/Îµ)) = O(Îµ log(1/Îµ)).
This proves part (i) of Proposition 1.
Case of Large Spectral Norm. We next show the correctness of the algorithm when it returns
a filter in Step 6.
def
We start by proving that if Î»âˆ— = kÎ£ âˆ’ Ik2 > CÎµ log(1/Îµ), for a sufficiently large universal
constant C, then a value T satisfying the condition in Step 6 exists. We first note that that kME k2
is appropriately large. Indeed, by Corollary 2 and the assumption that Î»âˆ— > CÎµ log(1/Îµ) we deduce
that
(|E|/|S 0 |)kME k2 = â„¦(Î»âˆ— ) .
(2)
Moreover, using the inequality kME k2 â‰¥ kÂµE âˆ’ ÂµG k22 and Lemma 3 as above, we get that
p
p
0
kÂµS âˆ’ ÂµG k2 â‰¤ (|E|/|S 0 |) kME k2 + O(Îµ log(1/Îµ)) â‰¤ Î´/2 ,
p
def âˆš
where we used the fact that Î´ = ÎµÎ»âˆ— > C 0 Îµ log(1/Îµ).
Suppose for the sake of contradiction that for all T > 0 we have that
h
i
Îµ
0
.
Pr 0 |v âˆ— Â· (X âˆ’ ÂµS )| > T + Î´ â‰¤ 8 exp(âˆ’T 2 /2Î½) + 8
d
2
Xâˆˆu S
T log d log( ÎµÏ„
)
6

(3)

Using (3), we obtain that for all T > 0 we have that


Pr

Xâˆˆu

S0


|v âˆ— Â· (X âˆ’ ÂµG )| > T + Î´/2 â‰¤ 8 exp(âˆ’T 2 /2Î½) + 8

T 2 log

Îµ
.
d
d log( ÎµÏ„
)

(4)

Since E âŠ† S 0 , for all x âˆˆ Rd we have that |S 0 | PrXâˆˆu S 0 [X = x] â‰¥ |E| PrY âˆˆu E [Y = x]. This fact
combined with (4) implies that for all T > 0
!
 âˆ—

Îµ
G
0
2
 . (5)
Pr |v Â· (X âˆ’ Âµ )| > T + Î´/2  (|S |/|E|) exp(âˆ’T /2Î½) +
d
Xâˆˆu E
T 2 log d log( ÎµÏ„
)
We now have the following sequence of inequalities:
Z âˆ
 âˆ—



G 2
kME k2 = EXâˆˆu E |v Â· (X âˆ’ Âµ )| = 2
Pr |v âˆ— Â· (X âˆ’ ÂµG )| > T T dT
0 Xâˆˆu E
Z O(âˆšd log(d/ÎµÏ„ ))


=2
Pr |v âˆ— Â· (X âˆ’ ÂµG )| > T T dT
Xâˆˆu E
0


Z O(âˆšd log(d/ÎµÏ„ ))
 âˆ—

|S 0 |
G
â‰¤2
min 1,
Â· Pr |v Â· (X âˆ’ Âµ )| > T T dT
|E| Xâˆˆu S 0
0
Z 4âˆšÎ½ log(|S 0 |/|E|)+Î´
Z O(âˆšd log(d/ÎµÏ„ )) 
0

T dT + (|S |/|E|) âˆš
exp(âˆ’T 2 /2Î½) +
0
0

4

0

Î½ log(|S |/|E|)+Î´


Îµ

T dT
d
T 2 log d log( ÎµÏ„
)

0

2

 log(|S |/|E|) + Î´ + O(1) + Îµ Â· |S |/|E|
 log(|S 0 |/|E|) + ÎµÎ»âˆ— + Îµ Â· |S 0 |/|E| .
Rearranging the above, we get that
(|E|/|S 0 |)kME k2  (|E|/|S 0 |) log(|S 0 |/|E|) + (|E|/|S 0 |)ÎµÎ»âˆ— + Îµ = O(Îµ log(1/Îµ) + Îµ2 Î»âˆ— ).
Combined with (2), we obtain Î»âˆ— = O(Îµ log(1/Îµ)), which is a contradiction if C is sufficiently large.
Therefore, it must be the case that for some value of T the condition in Step 6 is satisfied.
The following claim completes the proof:
def

d
Claim 2. Fix Î± = d log(d/ÎµÏ„ ) log(d log( ÎµÏ„
)). We have that âˆ†(S, S 00 ) â‰¤ âˆ†(S, S 0 ) âˆ’ 2Îµ/Î± .

Proof. Recall that S 0 = (S \ L) âˆª E, with E and L disjoint multisets such that L âŠ‚ S. We can
similarly write S 00 = (S \ L0 ) âˆª E 0 , with L0 âŠ‡ L and E 0 âŠ‚ E. Since
âˆ†(S, S 0 ) âˆ’ âˆ†(S, S 00 ) =

|E \ E 0 | âˆ’ |L0 \ L|
,
|S|

it suffices to show that |E \ E 0 | â‰¥ |L0 \ L| + Îµ|S|/Î±. Note that |L0 \ L| is the number of points rejected
by the filter that lie in S âˆ© S 0 . Note that the fraction of elements of S that are removed to produce
S 0 )| > T + Î´) is at most 2 exp(âˆ’T 2 /2Î½) + Îµ/Î±. This follows from Claim 1
S 00 (i.e., satisfy |v âˆ— Â· (x âˆ’ Âµp
and the fact that T = O( d log(d/ÎµÏ„ )).
Hence, it holds that |L0 \ L| â‰¤ (2 exp(âˆ’T 2 /2Î½) + Îµ/Î±)|S|. On the other hand, Step 6 of the
algorithm ensures that the fraction of elements of S 0 that are rejected by the filter is at least

7

8 exp(âˆ’T 2 /2Î½) + 8Îµ/Î±). Note that |E \ E 0 | is the number of points rejected by the filter that lie in
S 0 \ S. Therefore, we can write:
|E \ E 0 | â‰¥ (8 exp(âˆ’T 2 /2Î½) + 8Îµ/Î±)|S 0 | âˆ’ (2 exp(âˆ’T 2 /2Î½) + Îµ/Î±)|S|
â‰¥ (8 exp(âˆ’T 2 /2Î½) + 8Îµ/Î±)|S|/2 âˆ’ (2 exp(âˆ’T 2 /2Î½) + Îµ/Î±)|S|
â‰¥ (2 exp(âˆ’T 2 /2Î½) + 3Îµ/Î±)|S|
â‰¥ |L0 \ L| + 2Îµ|S|/Î± ,
where the second line uses the fact that |S 0 | â‰¥ |S|/2 and the last line uses the fact that |L0 \L|/|S| â‰¤
2 exp(âˆ’T 2 /2Î½) + Îµ/Î±. Noting that log(d/ÎµÏ„ ) â‰¥ 1, this completes the proof of the claim.
1.1.3

Proof of Lemma 1

Proof. Let N = â„¦((d/Îµ2 )poly log(d/ÎµÏ„ )) be the number
of samples drawn from G. For (i), the
p
probability that a coordinate of a sample is at least 2Î½ log(N d/3Ï„ ) is at most Ï„ /3dN
p by Fact 1. By
a union bound, the probability that all coordinates
of
all
samples
are
smaller
than
2Î½ log(N d/3Ï„ )
p
p
is at least 1 âˆ’ Ï„ /3. In this case, kxk2 â‰¤ 2Î½d log(N d/3Ï„ ) = O( dÎ½ log(N Î½/Ï„ )).
After translating by ÂµG , we note that (iii) follows immediately from Lemmas 4.3 of [DKK+ 16]
and (iv) follows from Theorem 5.50 of [Ver10], as long as N = â„¦(Î½ 4 d log(1/Ï„ )/Îµ2 ), with probability
at least 1 âˆ’ Ï„ /3. It remains to show that, conditioned on (i), (ii) holds with probability at least
1 âˆ’ Ï„ /3.
p
To simplify some expressions, let Î´ := Îµ/(log(d log d/ÎµÏ„ )) and R = C d log(|S|/Ï„ ). We need to
show that for all unit vectors v and all 0 â‰¤ T â‰¤ R that




G
G
 Pr [|v Â· (X âˆ’ Âµ )| > T ] âˆ’ Pr [|v Â· (X âˆ’ Âµ ) > T â‰¥ 0] â‰¤ Î´ .
(6)
Xâˆˆu S
 T2
Xâˆ¼G
Firstly, we show that for all unit vectors v and T > 0




G
G
 Pr [|v Â· (X âˆ’ Âµ )| > T ] âˆ’ Pr [|v Â· (X âˆ’ Âµ )| > T â‰¥ 0] â‰¤ Î´/4Î½ ln(1/Î´)
Xâˆˆu S

Xâˆ¼G
with probability at least 1 âˆ’ Ï„ /6. Since the VC-dimension of the set of all halfspaces is d + 1, this
2
follows from the VC inequality [DL01], since we have
p more than â„¦(d/(Î´/(4Î½ log(1/Î´)) ) samples.
We thus only need to consider the case when T â‰¥ 4Î½ ln(1/Î´).
p
Lemma 4. For any fixed unit vector v and T > 4Î½ ln(1/Î´), except with probability exp(âˆ’N Î´/6CÎ½),
we have that
Î´
Pr [|v Â· (X âˆ’ ÂµG )| > T ] â‰¤
,
Xâˆˆu S
CT 2
where C = 8.
Proof. Let E be the event that |v Â· (X âˆ’ ÂµG )| > T . Since G is sub-gaussian, Fact 1 yields that
PrG [E] = PrY âˆ¼G [|v Â· (X âˆ’ ÂµG )| â‰¤ exp(âˆ’T 2 /2Î½). Note that, thanks to our assumption on T , we
have that T â‰¤ exp(T 2 /4Î½)/2C, and therefore T 2 PrG [E] â‰¤ exp(âˆ’T 2 /4Î½)/2C â‰¤ Î´/2C.

8

Consider ES [exp(t2 /3Î½ Â· N PrS [E])]. Each individual sample sample Xi for 1 â‰¤ i â‰¤ N , is an
independent copy of Y âˆ¼ G, and hence:
"
#


n
X
1Xi âˆˆE )
ES exp(T 2 /3Î½ Â· N Pr[E]) = ES exp(T 2 /3Î½ Â·
S

i=1

=

=

N
Y
i=1
N
Y

"
2

EXi exp(T /3Î½ Â·

n
X

#
1Xi âˆˆE )

i=1

exp(T 2 /3Î½ Â· Pr[E])

i=1

G


â‰¤ exp N T 2 /3Î½ Â· exp(âˆ’T 2 /2Î½)
â‰¤ exp(N/3Î½ Â· Î´/2C) = exp(N Î´/6CÎ½) .
Î´
2
Note that if PrS [E] > CÎ½T
2 , then exp(T /3Î½ Â· N PrS [E]) = exp(N Î´/3CÎ½). By Markovâ€™s inequality,
this happens with probability at most exp(âˆ’N Î´/6CÎ½).
O(d) . By a
Now let C be a 1/2-cover in Euclidean distance for the
p set of unit vectors of size 2
0
0
union bound, for all v âˆˆ C and T a power of 2 between 4Î½ ln(1/Î´) and R, we have that

Pr [|v 0 Â· (X âˆ’ ÂµG )| > T 0 ] â‰¤

Xâˆˆu S

Î´
8T 2

except with probability
2O(d) log(R) exp(âˆ’N Î´/6CÎ½) = exp (O(d) + log log R âˆ’ N Î´/6CÎ½) â‰¤ Ï„ /6 .
p
However, for any unit vector v and 4Î½ ln(1/Î´) â‰¤ T â‰¤ R, there is a v 0 âˆˆ C and such a T 0 such
that for all x âˆˆ Rd , we have |v Â· (X âˆ’ ÂµG )| â‰¥ |v 0 Â· (X âˆ’ ÂµG )|/2, and so |v 0 Â· (X âˆ’ ÂµG )| > 2T 0 implies
|v 0 Â· (X âˆ’ ÂµG )| > T.
Then, by a union bound, (6) holds simultaneously for all unit vectors v and all 0 â‰¤ T â‰¤ R, with
probability a least 1 âˆ’ Ï„ /3. This completes the proof.

1.2

Robust Mean Estimation Under Second Moment Assumptions

In this section, we use our filtering technique to give a near sample-optimal computationally efficient
algorithm to robustly estimate the mean of a density with a second moment assumption. We show:
Theorem 4. Let P be a distribution on Rd with unknown mean vector ÂµP and unknown covariance
matrix Î£P  I. Let S be an Îµ-corrupted set of samples from P of size Î˜((d/Îµ) log d). Then there
âˆš
exists an algorithm that given S, with probability 2/3, outputs Âµ
b with kb
Âµ âˆ’ ÂµP k2 â‰¤ O( Îµ) in time
poly(d/Îµ).
Note that Theorem 3.2 follows straightforwardly from the above (divide every sample by Ïƒ, run
the algorithm of Theorem 4, and multiply its output by Ïƒ).
As usual in our filtering framework, the algorithm will iteratively look at the top eigenvalue
and eigenvector of the sample covariance matrix and return the sample mean if this eigenvalue
is small (Algorithm 2). The main difference between this and the filter algorithm for the subgaussian case is how we choose the threshold for the filter. Instead of looking for a violation of
a concentration inequality, here we will choose a threshold at random (with a bias towards higher
9

thresholds). The reason is that, in this setting, the variance in the direction we look for a filter in
needs to be a constant multiple larger â€“ instead of the typical â„¦Ìƒ(Îµ) relative for the sub-gaussian
case. Therefore, randomly choosing a threshold weighted towards higher thresholds suffices to throw
out more corrupted samples than uncorrupted samples in expectation. Although it is possible to
reject many good samples this way, the algorithm still only rejects a total of O(Îµ) samples with high
probability.
We would like our good set of samples to have mean close to that of P and bounded variance
in all directions. This motivates the following definition:
Definition 4. We call a set S Îµ-good for a distribution P with mean ÂµP and covariance Î£P  I if
âˆš
the mean ÂµS and covariance Î£S of S satisfy kÂµS âˆ’ ÂµP k2 â‰¤ Îµ and kÎ£S k2 â‰¤ 2.
However, since we have no assumptions about higher moments, it may be be possible for outliers
to affect our sample covariance too much. Fortunately, such outliers have small probability and do
not contribute too much to the mean, so we will later reclassify them as errors.
Lemma 5. Let S be N = Î˜((d/Îµ) log d) samples drawn from P . Then, with probability at least
9/10, a random X âˆˆu S satisfies
âˆš
(i) kE[X] âˆ’ ÂµP k2 â‰¤ Îµ/3,
h
p i
(ii) Pr kX âˆ’ ÂµP k2 â‰¥ 80 d/Îµ â‰¤ Îµ/160,
h
(iii) E kX âˆ’ ÂµP k2 Â· 1kXâˆ’ÂµP k

âˆš

2 â‰¥80

i
d/Îµ

â‰¤

âˆš

Îµ/2, and

 h
i


(iv) E (X âˆ’ ÂµP )(X âˆ’ ÂµP )T Â· 1kXâˆ’ÂµP k â‰¤80âˆšd/Îµ  â‰¤ 3/2.
2
2

Proof. For (i), note that
ES [kE[X] âˆ’ ÂµP k22 ] =

X

ES [(E[X]i âˆ’ ÂµPi )2 ] â‰¤ d/N â‰¤ Îµ/360 ,

i

and so by Markovâ€™s inequality, with probability at least 39/40, we have kE[X] âˆ’ ÂµP k22 â‰¤ Îµ/9.
For (ii), similarly to (i), note that
X
E[kY âˆ’ ÂµP k22 ] =
E[(Yi âˆ’ ÂµPi )2 ] â‰¤ d ,
i

p
for Y âˆ¼ P . By Markovâ€™s inequality, Pr[kY âˆ’ ÂµP k2 â‰¥ 80 d/Îµ] â‰¤ Îµ/160 with probability at least
39/40.
For (iii), note that by an application of the Cauchy-Schwarz inequality
q
p
âˆš
P
âˆš
E[|Y âˆ’ Âµ |1kY âˆ’ÂµP k â‰¥80 d/Îµ ] â‰¤ E[(Y âˆ’ ÂµP )2 ] Pr[kY âˆ’ ÂµP k2 â‰¥ 80 d/Îµ] â‰¤ Îµ/80 .
2

Thus,
ES [E[|X âˆ’ ÂµP |1kXâˆ’ÂµP k

âˆš

2 â‰¥80

d/Îµ

]] â‰¤

âˆš

Îµ/80 ,

and by Markovâ€™s inequality, with probability at least 39/40
h
i âˆš
E |X âˆ’ ÂµP |1kXâˆ’ÂµP k â‰¥80âˆšd/Îµ â‰¤ Îµ/2 .
2

For (iv), we require the following Matrix Chernoff bound:
10

Lemma 6 (Part of Theorem 5.1.1 of [T+ 15]). Consider a sequence
of d Ã— d positive semi-definite
P
max
random matrices Xk with kXk k2 â‰¤ L for all k. Let Âµ
= k k E[Xk ]k2 . Then, for Î¸ > 0,
 #
"
X 


E 
Xk  â‰¤ (eÎ¸ âˆ’ 1)Âµmax /Î¸ + L log(d)/Î¸ ,


k

2

and for any Î´ > 0,

"
#
X 
max


Pr 
Xk  â‰¥ (1 + Î´)Âµmax â‰¤ d(eÎ´ /(1 + Î´)1+Î´ )Âµ /L .


k

2

We apply this lemma with Xk = (xk âˆ’ ÂµP )(xk âˆ’ ÂµP )T 1kx

k âˆ’Âµ

âˆš

P k â‰¤80
2

d/Îµ

for {x1 , . . . , xN } = S. Note

that kXk k2 â‰¤ (80)2 d/Îµ = L and that Âµmax â‰¤ N kÎ£P k2 â‰¤ N .
Suppose that Âµmax â‰¤ N/80. Then, taking Î¸ = 1, we have


X 


E[
Xk  ] â‰¤ (e âˆ’ 1)N/80 + O(d log(d)/Îµ) .


k

2

P
By Markovâ€™s inequality, except with probability 39/40, we have k k Xk k2 â‰¤ N + O(d log(d)/Îµ) â‰¤
3N/2, for N a sufficiently high multiple of d log(d)/Îµ.
Suppose that Âµmax â‰¥ N/80, then we take Î´ = 1/2 and obtain

"
#
X 


Pr 
Xk  â‰¥ 3Âµmax 2 â‰¤ d(e3/2 /(5/2)3/2 )N Îµ/20d .


k

2

P
For N a sufficiently high multiple of d log(d)/Îµ, we getP
that Pr[k k Xk k2 â‰¥ 3Âµmax /2] â‰¤ 1/40. Since
Âµmax â‰¤ N , we have with probability at least 39/40, k k Xk k2 â‰¤ 3N/2.
P
Noting that k k Xk k2 /N = kE[1kXâˆ’ÂµP k â‰¤80âˆšd/Îµ (X âˆ’ ÂµP )(X âˆ’ ÂµP )T ]k2 , we obtain (iv). By a
2

union bound, (i)-(iv) all hold simultaneously with probability at least 9/10.
Now we can get a 2Îµ-corrupted good set from an Îµ-corrupted set of samples satisfying Lemma
5, by reclassifying outliers as errors:
Lemma 7. Let S = R âˆª E \ L, where R is a set of N = Î˜(d log d/Îµ) samples drawn from P
and E and L are disjoint sets with |E|, |L| â‰¤ Îµ. Then, with probability 9/10, we can also write
S = G âˆª E 0 \ L0 , where G âŠ† R is Îµ-good, L0 âŠ† L and E 0 âŠ† E 0 has |E 0 | â‰¤ 2Îµ|S|.
p
Proof. Let G = {x âˆˆ R : kxk2 â‰¤ 80 d/Îµ}. Since R satisfies (ii) of Lemma 5, |G|âˆ’|R| â‰¤ Îµ|R|/160 â‰¤
Îµ|S|. Thus, E 0 = E âˆª (R \ G) has |E 0 | â‰¤ 3Îµ/2. Note that (iv) of Lemma 5 for R in terms of G is
exactly |G|kÎ£G k2 /|R| â‰¤ 3/2, and so kÎ£G k2 â‰¤ 3|R|/2|G| â‰¤ 2.
âˆš
It remains to check that kÂµG âˆ’ÂµP k2 â‰¤ Îµ. But note that (iii) of Lemma 5 is exactly EXâˆˆuR [kX âˆ’
âˆš
ÂµP k2 1XâˆˆR\G ] â‰¤ Îµ/2, and we have


âˆš
|G|EXâˆˆuG [kX âˆ’ ÂµP k2 ] âˆ’ |R|EXâˆˆuR [kX âˆ’ ÂµP k2 ] â‰¤ |R| Îµ/2 ,
âˆš
âˆš
and since by (i), EXâˆˆuR [kX âˆ’ ÂµP k2 ] â‰¤ Îµ/3, it follows that EXâˆˆuG0 [kX âˆ’ ÂµP k2 ] â‰¤ Îµ.
An iteration of FilterUnder2ndMoment may throw out more samples from G than corrupted
samples. However, in expectation, we throw out many more corrupted samples than from the good
set:
11

Algorithm 2 Filter under second moment assumptions
1: function FilterUnder2ndMoment(S)
2:
Compute ÂµS , Î£S , the mean and covariance matrix of S.
3:
Find the eigenvector v âˆ— with highest eigenvalue Î»âˆ— of Î£S .
4:
if Î»âˆ— â‰¤ 9 then
5:
return ÂµS
6:
else
7:
Draw Z from the distribution on [0, 1] with probability density function 2x.
8:
Let T = Z max{|v âˆ— Â· x âˆ’ ÂµS | : x âˆˆ S}.
9:
Return the set S 0 = {x âˆˆ S : |v âˆ— Â· (X âˆ’ ÂµS )| < T }.
10:
end if
11: end function
Proposition 2. If we run FilterUnder2ndMoment on a set S = G âˆª E \ L for some Îµ-good set G
âˆš
and disjoint E, L with |E| â‰¤ 2Îµ|S|, |L| â‰¤ 9Îµ|S|, then either it returns ÂµS with kÂµS âˆ’ ÂµP k2 â‰¤ O( Îµ),
or else it returns a set S 0 âŠ‚ S with S 0 = G âˆª E 0 \ L0 for disjoint E 0 and L0 . In the latter case we
have EZ [|E 0 | + 2|L0 |] â‰¤ |E| + 2|L|.
For D âˆˆ {G, E, L, S}, let ÂµD be the mean of D and MD be the matrix EXâˆˆu D [(Xâˆ’ÂµS )(Xâˆ’ÂµS )T ].
p
Lemma 8. If G is an Îµ-good set with x â‰¤ 40 d/Îµ for x âˆˆ S âˆª G, then kMG k2 â‰¤ 2kÂµG âˆ’ ÂµS k22 + 2 .
Proof. For any unit vector v, we have
v T MG v = EXâˆˆu G [(v Â· (X âˆ’ ÂµS ))2 ]
= EXâˆˆu G [(v Â· (X âˆ’ ÂµG ) + v Â· (ÂµP âˆ’ ÂµG ))2 ]
= v T Î£G v + (v Â· (ÂµG âˆ’ ÂµS ))2
â‰¤ 2 + 2kÂµG âˆ’ ÂµS k22 .

Lemma 9. We have that |L|kML k2 â‰¤ 2|G|(1 + kÂµG âˆ’ ÂµS k22 ) .
Proof. Since L âŠ† G, for any unit vector v, we have
|L|v T ML v = |L|EXâˆˆu L [(v Â· (X âˆ’ ÂµS ))2 ]
â‰¤ |G|EXâˆˆu G [(v Â· (X âˆ’ ÂµS ))2 ]
â‰¤ 2|G|(1 + kÂµG âˆ’ ÂµS k22 ) .

Lemma 10. kÂµG âˆ’ ÂµS k2 â‰¤

p
âˆš
2ÎµkMS k2 + 12 Îµ.

Proof. We have that |E|ME â‰¤ |S|MS + |L|ML and so
|E|kME k2 â‰¤ |S|kMS k2 + 2|G|(1 + kÂµG âˆ’ ÂµS k22 ) .
By Cauchy Schwarz, we have that kME k2 â‰¥ kÂµE âˆ’ ÂµS k22 , and so
q
p
|E|kÂµE âˆ’ ÂµS k2 â‰¤ |S|kMS k2 + 2|G|(1 + kÂµG âˆ’ ÂµS k22 ) .
12

By Cauchy-Schwarz and Lemma 9, we have that
q
p
p
L
S
|L|kÂµ âˆ’ Âµ k2 â‰¤ |L|kML k2 â‰¤ 2|G|(1 + kÂµG âˆ’ ÂµS k22 ) .
Since |S|ÂµS = |G|ÂµG + |E|ÂµE âˆ’ |L|ÂµL and |S| = |G| + |E| âˆ’ |L|, we get
|G|(ÂµG âˆ’ ÂµS ) = |E|(ÂµE âˆ’ ÂµS ) âˆ’ |L|(ÂµE âˆ’ ÂµS ) .
Substituting into this, we obtain
q
q
|G|kÂµG âˆ’ ÂµS k2 â‰¤ |E||S|kMS k2 + 2|E||G|(1 + kÂµG âˆ’ ÂµS k22 ) + 2|L||G|(1 + kÂµG âˆ’ ÂµS k22 ) .
âˆš
âˆš
âˆš
Since for x, y > 0, x + y â‰¤ x + y, we have
p
p
p
|G|kÂµG âˆ’ ÂµS k2 â‰¤ |E||S|kMS k2 + ( 2|E||G| + 2|L||G|)(1 + kÂµG âˆ’ ÂµS k2 ) .
Since ||G| âˆ’ |S|| â‰¤ Îµ|S| and |E| â‰¤ 2Îµ|S|, |L| â‰¤ 9Îµ|S|, we have
p
âˆš
kÂµG âˆ’ ÂµS k2 â‰¤ 2ÎµkMS k2 + (6 Îµ)(1 + kÂµG âˆ’ ÂµS k2 ) .
âˆš
Moving the kÂµG âˆ’ ÂµS k2 terms to the LHS, using 6 Îµ â‰¤ 1/2, gives
p
âˆš
kÂµG âˆ’ ÂµS k2 â‰¤ 2ÎµkMS k2 + 12 Îµ .

Since Î»âˆ— = kMS k2 , the correctness if we return the empirical mean is immediate.
âˆš
Corollary 3. If Î»âˆ— â‰¤ 9, we have that kÂµG âˆ’ ÂµS k2 = O( Îµ).
From now on, we assume Î»âˆ— > 9. In this case we have kÂµG âˆ’ ÂµS k22 â‰¤ O(ÎµÎ»âˆ— ). Using Lemma 8,
we have
kMG k2 â‰¤ 2 + O(ÎµÎ»âˆ— ) â‰¤ 2 + Î»âˆ— /5
for sufficiently small Îµ. Thus, we have that
v âˆ—T MS v âˆ— â‰¥ 4v âˆ—T MG v âˆ— .

(7)

Now we can show that in expectation, we throw out many more corrupted points from E than
from G \ L:
Lemma 11. Let S 0 = G âˆª E 0 \ L0 for disjoint E 0 , L0 be the set of samples returned by the iteration.
Then we have EZ [|E 0 | + 2|L0 |] â‰¤ |E| + 2|L|.
Proof. Let a = maxxâˆˆS |v âˆ— Â· x âˆ’ ÂµS |. Firstly, we look at the expected number of samples we reject:


0
S
EZ [|S |] âˆ’ |S| = EZ |S| Pr [|X âˆ’ Âµ | â‰¥ aZ]
Xâˆˆu S
Z 1


= |S|
Pr |v âˆ— Â· (X âˆ’ ÂµS )| â‰¥ ax 2xdx
Xâˆˆu S
Z0 a


= |S|
Pr |v âˆ— Â· (X âˆ’ ÂµS )| â‰¥ T (2T /a)dT
0 Xâˆˆu S


= |S|EXâˆˆu S (v âˆ— Â· (X âˆ’ ÂµS ))2 /a
= (|S|/a) Â· v âˆ—T MS v âˆ— .
13

Next, we look at the expected number of false positive samples we reject, i.e., those in L0 \ L.




EZ [|L0 |] âˆ’ |L| = EZ (|G| âˆ’ |L|) Pr
|X âˆ’ ÂµS | â‰¥ T
Xâˆˆu G\L


âˆ—
S
â‰¤ EZ |G| Pr [|v Â· (X âˆ’ Âµ )| â‰¥ aZ]
Xâˆˆu G
Z 1
= |G|
Pr [|v âˆ— Â· (X âˆ’ ÂµS )| â‰¥ ax]2x dx
0 Xâˆˆu G
Z a
Pr [|v âˆ— Â· (X âˆ’ ÂµS )| â‰¥ T ](2T /a) dT
= |G|
Xâˆˆ
uG
Z0 âˆ
Pr [|v âˆ— Â· (X âˆ’ ÂµS )| â‰¥ T ](2T /a) dT
â‰¤ |G|
Xâˆˆ
uG
0
 âˆ—

= |G|EXâˆˆu G (v Â· (X âˆ’ ÂµS ))2 /a
= (|G|/a) Â· v âˆ—T MG v âˆ— .
Using (7), we have |S|v âˆ—T MS v âˆ— â‰¥ 4|G|v âˆ—T MG v âˆ— and so EZ [S 0 ] âˆ’ S â‰¥ 3(EZ [L0 ] âˆ’ L). Now consider
that |S 0 | = |G|+|E 0 |âˆ’|L0 | = |S|âˆ’|E|+|E 0 |+|L|âˆ’|L0 |, and thus |S 0 |âˆ’|S| = |E|âˆ’|E 0 |+|L0 |âˆ’|L|. This
yields that |E|âˆ’EZ [|E 0 |] â‰¥ 2(EZ [L0 ]âˆ’L), which can be rearranged to EZ [|E 0 |+2|L0 |] â‰¤ |E|+2|L|.
Proof of Proposition 2. If Î»âˆ— â‰¤ 9, then we return the mean in Step 5, and by Corollary 3, kÂµS âˆ’
âˆš
ÂµP k2 â‰¤ O( Îµ).
If Î»âˆ— > 9, then we return S 0 . Since at least one element of S has |v âˆ— Â· X| = maxxâˆˆS |v âˆ— Â· X|,
whatever value of Z is drawn, we still remove at least one element, and so have S 0 âŠ‚ S. By Lemma
11, we have EZ [|E 0 | + 2|L0 |] â‰¤ |E| + 2|L|.
Proof of Theorem 4. Our input is a set S of N = Î˜((d/Îµ) log d) Îµ-corrupted samples so that with
probability 9/10, S is a 2Îµ-corrupted set of Îµ-good samples for P by Lemmas 5 and 7. We have
a set S = G âˆª E 0 \ L, where G0 is an Îµ-good set, |E| â‰¤ 2Îµ, and |L| â‰¤ Îµ. Then, we iteratively
apply FilterUnder2ndMoment until it outputs an approximation to the mean. Since each
iteration removes a sample, this must happen within N iterations. The algorithm takes at most
poly(N, d) = poly(d, 1/Îµ) time.
As long as we can show that the conditions of Proposition 2 hold in each iteration, it ensures that
âˆš
S
kÂµ âˆ’ ÂµP k2 â‰¤ O( Îµ). However, the condition that |L| â‰¤ 9Îµ|S| need not hold in general. Although
in expectation we reject many more samples in E than G, it is possible that we are unlucky and
reject many samples in G, which could make L large in the next iteration. Thus, we need a bound
on the probability that we ever have |L| > 9Îµ.
We analyze the following procedure: We iteratively run FilterUnder2ndMoment starting
with a set Si âˆª Ei \ Li of samples with S0 = S and producing a set Si+1 = G âˆª Ei+1 \ Li+1 . We stop
if we output an approximation to the mean or if |Li+1 | â‰¥ 13Îµ|S|. Since we do now always satisfy
the conditions of Proposition 2, this gives that EZ [|Ei+1 | + |Li+1 |] = |Ei | + 2|Li |. This expectation
is conditioned on the state of the algorithm after previous iterations, which is determined by Si .
Thus, if we consider the random variables Xi = |Ei | + 2|Li |, then we have E[Xi+1 |Si ] â‰¤ Xi , i.e.,
the sequence Xi is a sub-martingale with respect to Xi . Using the convention that Si+1 = Si , if
we stop in less than i iterations, and recalling that we always stop in N iterations, the algorithm
fails if and only if |LN | > 9Îµ|S|. By a simple induction or standard results on sub-martingales, we
have E[XN ] â‰¤ X0 . Now X0 = |E0 | + 2|L0 | â‰¤ 3Îµ|S|. Thus, E[XN ] â‰¤ 3Îµ|S|. By Markovâ€™s inequality,
except with probability 1/6, we have XN â‰¤ 18Îµ|S|. In this case, |LN | â‰¤ XN /2 â‰¤ 9Îµ|S|. Therefore,
the probability that we ever have |Li | > 9Îµ is at most 1/6.
14

By a union bound, the probability that the uncorrupted samples satisfy Lemma 5 and Proposition 2 applies to every iteration is at least 9/10 âˆ’ 1/6 â‰¥ 2/3. Thus, with at least 2/3 probability,
âˆš
the algorithm outputs a vector Âµ
b with kb
Âµ âˆ’ ÂµP k2 â‰¤ O( Îµ).

1.3

Robust Covariance Estimation

In this subsection, we give a near sample-optimal efficient robust estimator for the covariance of
a zero-mean Gaussian density, thus proving Theorem 3.3. Our algorithm is essentially identical
to the filtering algorithm given in Section 8.2 of [DKK+ 16]. As in Section 1.1 the only difference
is a weaker definition of the â€œgood set of samplesâ€ (Definition 5) and a concentration argument
(Lemma 3) showing that a random set of uncorrupted samples of the appropriate size is good with
high probability. Given these, the analysis of this subsection follows straightforwardly from the
analysis in Section 8.2 of [DKK+ 16] by plugging in the modified parameters.
The algorithm Filter-Gaussian-Unknown-Covariance to robustly estimate the covariance
of a mean 0 Gaussian in [DKK+ 16] is as follows:
Algorithm 3 Filter algorithm for a Gaussian with unknown covariance matrix.
1: procedure Filter-Gaussian-Unknown-Covariance(S 0 , Îµ, Ï„ )
input: A multiset S 0 such that there exists an (Îµ, Ï„ )-good set S with âˆ†(S, S 0 ) â‰¤ 2Îµ
output: Either a set S 00 with âˆ†(S, S 00 ) < âˆ†(S, S 0 ) or the parameters of a Gaussian G0 with
dT V (G, G0 ) = O( log(1/)).
Let C > 0 be a sufficiently large universal constant.
2:
Let Î£0 be the matrix EXâˆˆu S 0 [XX T ] and let G0 be the mean 0 Gaussian with covariance
matrix Î£0 .
3:
if there is any x âˆˆ S 0 so that xT (Î£0 )âˆ’1 x â‰¥ Cd log(|S 0 |/Ï„ ) then
4:
return S 00 = S 0 âˆ’ {x : xT (Î£0 )âˆ’1 x â‰¥ Cd log(|S 0 |/Ï„ )}.
5:
end if
6:
Compute an approximate eigendecomposition of Î£0 and use it to compute Î£0âˆ’1/2
7:
Let x(1) , . . . , x(|S 0 |) be the elements of S 0 .
âŠ—2
.
8:
For i = 1, . . . , |S 0 |, let y(i) = Î£0âˆ’1/2 x(i) and z(i) = y(i)
0|
P
|S
T .
9:
Let TS 0 = âˆ’I [ I [T + (1/|S 0 |) i=1 z(i) z(i)
âˆ—
10:
Approximate the top eigenvalue Î» and corresponding unit eigenvector v âˆ— of TS 0 ..
11:
Let pâˆ— (x) = âˆš12 ((Î£0âˆ’1/2 x)T v âˆ—] (Î£0âˆ’1/2 x) âˆ’ tr(v âˆ—] ))
12:
13:
14:
15:
16:

if Î»âˆ— â‰¤ (1 + C log2 (1/))QG0 (pâˆ— ) then
return G0
end if
Let Âµ be the median value of pâˆ— (X) over X âˆˆ S 0 .
Find a T â‰¥ C 0 so that
Pr (|pâˆ— (X) âˆ’ Âµ| â‰¥ T + 4/3) â‰¥ Tail(T, d, Îµ, Ï„ )

Xâˆˆu S 0

return S 00 = {X âˆˆ S 0 : |pâˆ— (X) âˆ’ Âµ| < T }.
18: end procedure
17:

In [DKK+ 16], we take Tail(T, d, Îµ, Ï„ ) = 12 exp(âˆ’T )+3/(d log(N/Ï„ ))2 , where N = Î˜((d log(d/ÎµÏ„ ))6 /Îµ2 )
is the number of samples we took there.
15

To get a near sample-optimal algorithms, we will need a weaker definition of a good set. To
use this, we will need to weaken the tail bound in the algorithm to Tail(T, d, Îµ, Ï„ ) = Îµ/(T 2 log2 (T )),
when T â‰¥ 10 log(1/Îµ). For T â‰¤ 10 log(1/Îµ), we take Tail(T, d, Îµ, Ï„ ) = 1 so that we always choose
T â‰¥ 10 log(1/Îµ). It is easy to show that the integrals of this tail bound used in the proofs of Lemma
8.19 and Claim 8.22 of [DKK+ 16] have similar bounds. Thus, our analysis here will sketch that
these tail bounds hold for a set of â„¦(d2 log5 (d/ÎµÏ„ )/Îµ2 ) samples from the Guassian.
Firstly, we state the new, weaker, definition of a good set:
Definition 5. Let G be a Gaussian in Rd with mean 0 and covariance Î£. Let  > 0 be sufficiently
small. We say that a multiset S of points in Rd is Îµ-good with respect to G if the following hold:
âˆš
1. For all x âˆˆ S, xT Î£âˆ’1 x < d + O( d log(d/Îµ)).
2. We have that kÎ£âˆ’1/2 Cov(S)Î£âˆ’1/2 âˆ’ IkF = O(Îµ).
3. For all even degree-2 polynomials p, we have that Var(p(S)) = Var(p(G))(1 + O(Îµ)).
4. For p an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1, and for any T >
10 log(1/Îµ) we have that
Pr (|p(x)| > T ) â‰¤ Îµ/(T 2 log2 (T )).
xâˆˆu S

It is easy to see that the algorithm and analysis of [DKK+ 16] can be pushed through using the
above weaker definition. That is, if S is a good set, then G can be recovered to OÌƒ(Îµ) error from an
Îµ-corrupted version of S. Our main task will be to show that random sets of the appropriate size
are good with high probability.
Proposition 3. Let N be a sufficiently large constant multiple of d2 log5 (d/Îµ)/Îµ2 . Then a set S of
N independent samples from G is Îµ-good with respect to G with high probability.
Proof. First, note that it suffices to prove this when G = N (0, I).
Condition 1 follows by standard concentration bounds on kxk22 .
Condition 2 follows by estimating the entry-wise error between Cov(S) and I.
Condition 3 is slightly more involved. Let {pi } be an orthonormal basis for the set of even,
degree-2, mean-0 polynomials with respect to G. Define the matrix Mi,j = Exâˆˆu S [pi (x)pj (x)] âˆ’ Î´i,j .
This condition is equivalent to kM k2 = O(Îµ). Thus, it suffices to show that for every v with kvk2 = 1
that v T M v = O(Îµ). It actually suffices
P to consider a cover of such vâ€™s. Note that this cover will be
2)
O(d
of size 2
. For each v, let pv = i vi pi . We need to show that Var(pv (S)) = 1 + O(Îµ). We can
2
show this happens with probability 1 âˆ’ 2âˆ’â„¦(d ) , and thus it holds for all v in our cover by a union
bound.
Condition 4 is substantially the most difficult of these conditions to prove. Naively, we would
want to find a cover of all possible p and all possible T , and bound the probability that the desired
condition fails. Unfortunately, the best a priori bound on Pr(|p(G)| > T ) are on the order of
2
exp(âˆ’T ). As our cover would need to be of size 2d or so, to make this work with T = d, we would
require on the order of d3 samples in order to make this argument work.
However, we will note that this argument is sufficient to cover the case of T < 10 log(1/Îµ) log2 (d/Îµ).
Fortunately, most such polynomials p satisfy much better tail bounds. Note that any even, mean
zero polynomial p can be written in the form p(x) = xT Ax âˆ’ tr(A) for some matrix A. We call
A the associated matrix to p. We note by the Hanson-Wright inequality that Pr(|p(G)| > T ) =
exp(âˆ’â„¦(min((T /kAkF )2 , T /kAk2 ))). Therefore, the tail bounds above are only as bad as described
when A has a single large eigenvalue. To take advantage of this, we will need to break p into parts
based on the size of its eigenvalues. We begin with a definition:
16

Definition 6. Let Pk be the set of even, mean-0, degree-2 polynomials, so that the associated matrix
A satisfies:
1. rank(A) â‰¤ k
âˆš
2. kAk2 â‰¤ 1/ k.
âˆš
âˆš
Note that for p âˆˆ Pk that |p(x)| â‰¤ |x|2 / k + k.
Importantly, any polynomial can be written in terms of these sets.
Lemma 12. Let p be an even, degree-2 polynomial with E[p(G)] = 0, Var(p(G)) = 1. Then if
t = blog2 (d)c, it is possible to write p = 2(p1 + p2 + . . . + p2t + pd ) where pk âˆˆ Pk .
Proof. Let A be the associated matrix to p. Note that kAkF = Varp = 1. Let Ak be the matrix
corresponding to the top k eigenvalues of A. We now let p1 be the polynomial associated to A1 /2,
p2 be associated to (A2 âˆ’ A1 )/2, p4 be associated to (A4 âˆ’ A2 )/2, and so on. It is clear that
p = 2(p1 + p2 + . . . + p2t + pd ). It is also clear that the matrix associated
to pk has rank at most
âˆš
k. If the matrix associated to pk had an eigenvalue more than
1/
k,
it
would
need to be the case
âˆš
nd
that the k/2 largest eigenvalue of A had size at least 2/ k. This is impossible since the sum of
the squares of the eigenvalues of A is at most 1.
This completes our proof.
We will also need covers of each of these sets Pk .
Lemma 13. For each k, there exists a set Ck âŠ‚ Pk so that
1. For each p âˆˆ Pk there exists a q âˆˆ Ck so that kp(G) âˆ’ q(G)k2 â‰¤ (Îµ/d)2 .
2. |Ck | = 2O(dk log(d/Îµ)) .
Pk
Î»i vi viT , for
Proof. We note that any such p is associated to a matrix A of the form A =
i=1
âˆš
P
k
Î»i âˆˆ [0, 1/ k] and vi orthonormal. It suffices to let q correspond to the matrix A0 = i=1 Âµi wi wiT
for with |Î»i âˆ’ Âµi | < (Îµ/d)3 and |vi âˆ’ wi | < (Îµ/d)3 for all i. It is easy to let Âµi and wi range over
covers of the interval and the sphere with appropriate errors. This gives a set of possible qâ€™s of
size 2O(dk log(d/Îµ)) as desired. Unfortunately, some of these q will not be in Pk as they will have
eigenvalues that are too large. However, this is easily fixed by replacing each such q by the closest
element of Pk . This completes our proof.
We next will show that these covers are sufficient to express any polynomial.
Lemma 14. Let p be an even degree-2 polynomial with E[p(G)] = 0 and Var(p(G)) = 1. It is
possible to write p as a sum of O(log(d)) elements of some Ck plus another polynomial of L2 norm
at most Îµ/d.
Proof. Combining the above two lemmas we have that any such p can be written as
t

p = (q1 + p1 ) + (q2 + p2 ) + . . . (q2t + p2t ) + (qd + pd ) = q1 + q2 + . . . + q 2 + q d + p0 ,
where qk above is in Ck and kpk (G)k2 < (Îµ/d)2 . Thus, p0 = p1 + p2 + . . . + p2t + pd has kp0 (G)k2 â‰¤
(Îµ/d). This completes the proof.

17

p
The key observation now is that if |p(x)| â‰¥ T for kxk2 â‰¤ d/Îµ, then writing p = q1 + q2 + q4 +
. . . + qd + p0 as above, it must be the case that |qk (x)| > (T âˆ’ 1)/(2 log(d)) for some k. Therefore,
to prove our main result, it suffices to show that, with high probability over the choice of S, for
any T â‰¥ 10 log(1/Îµ) log2 (d/Îµ) and any q âˆˆ Ck for some k, that Prxâˆˆu S (|q(x)| > T /(2 log(d))) <
Îµ/(2T 2 log2 (T ) log(d)). Equivalently, it suffices to show that for T â‰¥ 10 log(1/Îµ) log(d/Îµ) it holds
Prxâˆˆu S (|q(x)| > T /(2 log(d))) < Îµ/(2T 2 log2 (T ) log2 (d)). Note
p that this holds automatically for
T > (d/Îµ), as p(x) cannot possibly be that large for kxk2 â‰¤ d/Îµ. Furthermore, note that losing a
constant factor in the probability, it suffices to show this only for T a power of 2. âˆš
Therefore, it suffices to show for every k â‰¤ d, every q âˆˆ Ck and every d/ kÎµ  T 
log(1/Îµ) log(d/Îµ) that with probability at least 1 âˆ’ 2âˆ’â„¦(dk log(d/Îµ)) over the choice of S we have
that Prxâˆˆu S (|q(x)| > T )  Îµ/(T 2 log4 (d/Îµ)). However, by the Hanson-Wright inequality, we have
that
âˆš
Pr(|q(G)| > T ) = exp(âˆ’â„¦(min(T 2 , T k))) < (Îµ/(T 2 log4 (d/Îµ)))2 .
Therefore, by Chernoff bounds, the probability that more than a Îµ/(T 2 log4 (d/Îµ))-fraction of the
elements of S satisfy this property is at most
âˆš
âˆš
exp(âˆ’â„¦(min(T 2 , T k))|S|Îµ/(T 2 log4 (d/Îµ))) = exp(âˆ’â„¦(|S|Îµ/(log4 (d/Îµ)) min(1, k/T )))
â‰¤ exp(âˆ’â„¦(|S|Îµ2 /(log4 (d/Îµ))k/d))
â‰¤ exp(âˆ’â„¦(dk log(d/Îµ))) ,
as desired.
This completes our proof.

2
2.1

Omitted Details from Section 5
Full description of the distributions for experiments

Here we formally describe the distributions we used in our experiments. In all settings, our goal
was to find noise distributions so that noise points were not â€œobviousâ€ outliers, in the sense that
there is no obvious pointwise pruning process which could throw away the noise points, which still
gave the algorithms we tested the most difficulty. We again remark that while other algorithms had
varying performances depending on the noise distribution, it seemed that the performance of ours
was more or less unaffected by it.
Distribution for the synthetic mean experiment Our uncorrupted points were generated by
N (Âµ, I), where Âµ is the all-ones vector. Our noise distribution is given as
1
1
N = Î 1 + Î 2 ,
2
2
where Î 1 is the product distribution over the hypercube where every coordinate is 0 or 1 with
probability 1/2, and Î 2 is a product distribution where the first coordinate is ether 0 or 12 with equal
probability, the second coordinate is âˆ’2 or 0 with equal probability, and all remaining coordinates
are zero.
Distribution for the synthetic covariance experiment For the isotropic synthetic covariance
experiment, our uncorrupted points were generated by N (0, I), and the noise points were all zeros.
For the skewed synthetic covariance experiment, our uncorrupted points were generated by N (0, I +
18

10e1 eT1 ), where e1 is the first unit vector, and our noise points were generated as follows: we took
a fixed random rotation of points of the form Yi âˆ¼ Î , where Î  is a product distribution whose
first d/2 coordinates are Â±0.5 with probability 1/2, and whose next d/2 âˆ’ 1 coordinates are each
0.8 Ã— Ai , where for each coordinate i, Ai is an independent random integer between âˆ’2 and 2, and
whose last coordinate is a uniformly random integer between [âˆ’10, 10].
Setup for the semi-synthetic geographic experiment We took the 20 dimensional data
from [NJB+ 08], which was diagonalized, and randomly rotated it. This was to simulate the higher
dimensional case, since the singular vectors that [NJB+ 08] obtained did not seem to be sparse or
analytically sparse. Our noise was distributed as Î , where Î  is a product distribution whose first
d/2 coordinates are each uniformly random integers between 0 and 2 and whose last d/2 coordinates
are each uniformly randomly either 2 or 3, all scaled by a factor of 1/24.

2.2

Comparison with other robust PCA methods on semi-synthetic data

In addition to comparing our results with simple pruning techniques, as we did in Figure 3 in the
main text, we also compared our algorithm with implementations of other robust PCA techniques
from the literature with accessible implementations. In particular, we compared our technique with
RANSAC-based techniques, LRVCov, two SDPs ([CLMW11, XCS10]) for variants of robust PCA,
and an algorithm proposed by [CLMW11] to speed up their SDP based on alternating descent. For
the SDPs, since black box methods were too slow to run on the full data set (as [CLMW11] mentions,
black-box solvers for the SDPs are impractical above perhaps 100 data points), we subsample the
data, and run the SDP on the subsampled data. For each of these methods, we ran the algorithm
on the true data points plus noise, where the noise was generated as described above. We then take
the estimate of the covariance it outputs, and project the data points onto the top two singular
values of this matrix, and plot the results in Figure 1.
Similar results occurred for most noise patterns we tried. We found that only our algorithm
and LRVCov were able to reasonably reconstruct Europe, in the presence of this noise. It is hard to
judge qualitatively which of the two maps generated is preferable, but it seems that ours stretches
the picture somewhat less than LRVCov.

19

Original Data

Filter Projection

-0.2

-0.2

-0.1

-0.1

0

0

0.1

0.1

0.2

0.2

0.3

0.3

-0.15

-0.1

-0.05

0

0.05

0.1

0.15

0.2

0.15

0.1

0.05

RANSAC Projection

0.2

0

-0.05

-0.1

-0.15

-0.2

0.1

0.15

0.2

0.1

0.15

0.2

LRV Projection
-0.2

0.15
-0.1
0.1

0.05

0

0

0.1

-0.05
0.2
-0.1
0.3
-0.15
-0.2

-0.1

0

0.1

0.2

0.3

-0.15

-0.1

-0.05

0.2

0

0.05

CLMW SDP Projection

CLMW ADMM Projection
0.3

0.15

0.2
0.1

0.05

0.1

0

0

-0.05

-0.1
-0.1

-0.2
-0.15
-0.2

-0.1

0

0.1

0.2

-0.15

0.3

-0.1

-0.05

0

0.05

XCS Projection
0.3

0.2

0.1

0

-0.1

-0.2

-0.15

-0.1

-0.05

0

0.05

0.1

0.15

0.2

Figure 1: Comparison with other robust methods on the Europe semi-synthetic data. From left to
right, top to bottom: the original projection without noise, what our algorithm recovers, RANSAC,
LRVCov, the ADMM method proposed by [CLMW11], the SDP proposed by [XCS10] with subsampling, and the SDP proposed by [CLMW11] with subsampling.
20

References
[CLMW11] E. J. CandÃ¨s, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? J.
ACM, 58(3):11, 2011.
[DKK+ 16] I. Diakonikolas, G. Kamath, D. M. Kane, J. Li, A. Moitra, and A. Stewart. Robust
estimators in high dimensions without the computational intractability. In Proceedings
of FOCSâ€™16, 2016. Full version available at https://arxiv.org/pdf/1604.06443.pdf.
[DL01]

L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer
Series in Statistics, Springer, 2001.

[NJB+ 08]

J. Novembre, T. Johnson, K. Bryc, Z. Kutalik, A. R. Boyko, A. Auton, A. Indap, K. S.
King, S. Bergmann, M. R. Nelson, et al. Genes mirror geography within europe. Nature,
456(7218):98â€“101, 2008.

[T+ 15]

J. A. Tropp et al. An introduction to matrix concentration inequalities. Foundations
and Trends in Machine Learning, 8(1-2):1â€“230, 2015.

[Ver10]

R. Vershynin. Introduction to the non-asymptotic analysis of random matrices, 2010.

[XCS10]

H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. In Advances in
Neural Information Processing Systems, pages 2496â€“2504, 2010.

21

