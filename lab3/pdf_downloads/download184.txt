Sharp Minima Can Generalize For Deep Nets
Supplementary Material

Laurent Dinh 1 Razvan Pascanu 2 Samy Bengio 3 Yoshua Bengio 1 4

A

Radial transformations

We show an elementary transformation to locally perturb the
geometry of a finite-dimensional vector space and therefore
affect the relative flatness between a finite number minima,
at least in terms of spectral norm of the Hessian. We define
the function:
∀δ > 0, ∀ρ ∈]0, δ[,∀(r, r̂) ∈ R+ ×]0, δ[,

 r
ψ(r, r̂, δ, ρ) = 1 r ∈
/ [0, δ] r + 1 r ∈ [0, r̂] ρ
r̂


r−δ
+ 1 r ∈]r̂, δ] (ρ − δ)
+δ
r̂ − δ

ρ
0
ψ (r, r̂, δ, ρ) = 1 r ∈
/ [0, δ] + 1 r ∈ [0, r̂]
r̂
 ρ−δ
+ 1 r ∈]r̂, δ]
r̂ − δ

(a) ψ(r, r̂, δ, ρ)

For a parameter θ̂ ∈ Θ and δ > 0, ρ ∈]0, δ[, r̂ ∈]0, δ[,
inspired by the radial flows (Rezende & Mohamed, 2015)
in we can define the radial transformations


ψ kθ − θ̂k2 , r̂, δ, ρ

∀θ ∈ Θ, g −1 (θ) =
θ − θ̂ + θ̂
kθ − θ̂k2
with Jacobian
∀θ ∈ Θ, (∇g −1 )(θ) = ψ 0 (r, r̂, δ, ρ) In
 δ(r̂ − ρ)
(θ − θ̂)T (θ − θ̂)
− 1 r ∈]r̂, δ] 3
r (r̂ − δ)
 δ(r̂ − ρ)
+ 1 r ∈]r̂, δ]
In ,
r(r̂ − δ)
with r = kθ − θ̂k2 .
First, we can observe in Figure 1 that these transformations
are purely local: they only have an effect inside the ball
1

Université of Montréal, Montréal, Canada 2 DeepMind, London, United Kingdom 3 Google Brain, Mountain View, United
States 4 CIFAR Senior Fellow. Correspondence to: Laurent Dinh
<laurent.dinh@umontreal.ca>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

(b) g −1 (θ)

Figure 1: An example of a radial transformation on a 2dimensional space. We can see that only the area in blue
and red, i.e. inside B2 (θ̂, δ), are affected. Best seen with
colors.
B2 (θ̂, δ). Through these transformations, you can arbitrarily
perturb the ranking between several minima in terms of
flatness as described in subsection 5.1.

Sharp Minima Can Generalize For Deep Nets

B

Considering the bias parameter

When we consider the bias parameter for a one (hidden)
layer neural network, the non-negative homogeneity property translates into
y = φrect (x · θ1 + b1 ) · θ2 + b2
= φrect (x · αθ1 + αb1 ) · α

−1

θ2 + b2 ,

which results in conclusions similar to section 4.
For a deeper rectified neural network, this property results
in



y = φrect φrect · · · φrect (x · θ1 + b1 ) · · · · θK−1 + bK−1
· θ K + bK


= φrect φrect · · · φrect (x · α1 θ1 + α1 b1 ) · · ·
· αK−1 θK−1 +

K−1
Y


αk bK−1 · αK θK + bK

k=1

QK
for k=1 αk = 1. This can decrease the amount of eigenvalues of the Hessian that can be arbitrarily influenced.

C

Rectified neural network and
Lipschitz continuity

Relative to recent works (Hardt et al., 2016; Gonen &
Shalev-Shwartz, 2017) assuming Lipschitz continuity of the
loss function to derive uniform stability bound, we make
the following observation:
Theorem 1. For a one-hidden layer rectified neural network
of the form
y = φrect (x · θ1 ) · θ2 ,
if L is not constant, then it is not Lipschitz continuous.
Proof. Since a Lipschitz function is necessarily absolutely
continuous, we will consider the cases where L is absolutely
continuous. First, if L has zero gradient almost everywhere,
then L is constant.
Now, if there is a point θ with non-zero gradient, then by
writing

Without loss of generality, we consider (∇θ1 L)(θ1 , θ2 ) 6= 0.
Then the limit of the norm
k(∇L)(αθ1 , α−1 θ2 )k22 = α−2 k(∇θ1 L)(θ1 , θ2 )k22
+ α2 k(∇θ2 L)(θ1 , θ2 )k22
of the gradient goes to +∞ as α goes to 0. Therefore, L is
not Lipschitz continuous.
This result can be generalized to several other models containing a one-hidden layer rectified neural network, including deeper rectified networks.

D

Euclidean distance and input
representation

A natural consequence of subsectio 5.2 is that metrics relying on Euclidean metric like mean square error or Earthmover distance will rank very differently models depending
on the input representation chosen. Therefore, the choice
of input representation is critical when ranking different
models based on these metrics. Indeed, bijective transformations as simple as feature standardization or whitening
can change the metric significantly.
On the contrary, ranking resulting from metrics like fdivergence and log-likelihood are not perturbed by bijective
transformations because of the change of variables formula.

E

Eigenspectrum of Hessian

In section 4.2, we show how to manipulate the spectral radius and trace of the Hessian as a notion of sharpness. In
However, some notion of sharpness might take into account
the entire eigenspectrum of the Hessian as opposed to its
largest eigenvalue, for instance, Chaudhari et al. (2017) describe the notion of wide valleys, allowing the presence of
very few large eigenvalues. We can generalize the transformations between observationally equivalent parameters
to deeper neural networks with K − 1 hidden
QKlayers: for
αk > 0, Tα : (θk )k≤K 7→ (αk θk )k∈K with k=1 αk = 1.
If we define

 −1
α1 In1
0
···
0


0
α2−1 In2 · · ·
0


Dα = 

..
..
.
.
..
..


.
.
0

(∇L)(θ1 , θ2 ) = [(∇θ1 L)(θ1 , θ2 )
(∇θ2 L)(θ1 , θ2 )],
we have
(∇L)(αθ1 , α−1 θ2 ) = [α−1 (∇θ1 L)(θ1 , θ2 )
α(∇θ2 L)(θ1 , θ2 )].

0

···

−1
αK
InK

then the first and second derivatives at Tα (θ) will be

(∇L) Tα (θ) =(∇L)(θ)Dα

(∇2 L) Tα (θ) =Dα (∇2 L)(θ)Dα .
We will show to which extent
 you can increase several
eigenvalues of (∇2 L) Tα (θ) by varying α.

Sharp Minima Can Generalize For Deep Nets

Definition 1. For each n × n matrix A, we define the vector
λ(A) of sorted singular values of A with their multiplicity
λ1 (A) ≥ λ2 (A) ≥ · · · ≥ λn (A).
If A is symmetric positive semi-definite, λ(A) is also the
vector of its sorted eigenvalues.
Theorem 2. For a (K − 1)-hidden layer rectified neural
network of the form
y = φrect (φrect (· · · φrect (x · θ1 ) · · · ) · θK−1 ) · θK ,
and critical point θ = (θk )k≤K being a minimum
 for L,
2
such that (∇2 L)(θ) has
rank
r
=
rank
(∇
L)(θ)
, ∀M >


0, ∃α > 0 such that r − mink≤K (nk ) eigenvalues are
greater than M .

√
Proof. For simplicity, we will note M the principal
square root of a symmetric
√ positive-semidefinite matrix
M . The eigenvalues of M are the square root of the
eigenvalues of M and are its
p singular values. By definition, the singular values of (∇2 L)(θ)Dα are the square
root of the eigenvalues of Dα (∇2 L)(θ)Dα . Without loss
of generality, we consider mink≤K (nk ) = nK and choose
−1
∀k
and αK = β K−1 . Since Dα and
p < K, αk = β
2
(∇ L)(θ) are positive symmetric semi-definite matrices,
we can apply the multiplicative Horn inequalities
(Klyachko,
p
2000) on singular values of the product (∇2 L)(θ)Dα :
∀i ≤ n,j ≤ (n − nK ),


λi+j−n (∇2 L)(θ)Dα2 ≥ λi (∇2 L)(θ) β 2 .
By choosing β

>

r

M

 , since we have


∀i ≤ r, λi (∇2 L)(θ) ≥ λr (∇2 L)(θ) > 0 we can
conclude that
λr (∇2 L)(θ)

∀i ≤ (r − nK ),


λi (∇2 L)(θ)Dα2 ≥ λi+nk (∇2 L)(θ) β 2

≥ λr (∇2 L)(θ) β 2 > M.

It means that there exists
 an observationally
 equivalent parameter with at least r − mink≤K (nk ) arbitrarily large
eigenvalues. Since Sagun et al. (2016) seems to suggests that
rank deficiency in the Hessian is due to over-parametrization


of the model, one could conjecture that r − mink≤K (nk )
can be high for thin and deep neural networks, resulting in
a majority of large eigenvalues. Therefore, it would still
be possible to obtain an equivalent parameter with large
Hessian eigenvalues, i.e. sharp in multiple directions.

References
Chaudhari, Pratik, Choromanska, Anna, Soatto, Stefano, LeCun, Yann, Baldassi, Carlo, Borgs, Christian, Chayes, Jennifer, Sagun, Levent, and Zecchina, Riccardo. Entropy-sgd:
Biasing gradient descent into wide valleys. In ICLR’2017,
arXiv:1611.01838, 2017.
Gonen, Alon and Shalev-Shwartz, Shai. Fast rates for empirical
risk minimization of strict saddle problems. arXiv preprint
arXiv:1701.04271, 2017.
Hardt, Moritz, Recht, Ben, and Singer, Yoram. Train faster, generalize better: Stability of stochastic gradient descent. In Balcan,
Maria-Florina and Weinberger, Kilian Q. (eds.), Proceedings
of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pp.
1225–1234. JMLR.org, 2016. URL http://jmlr.org/
proceedings/papers/v48/hardt16.html.
Klyachko, Alexander A. Random walks on symmetric spaces
and inequalities for matrix spectra. Linear Algebra and its
Applications, 319(1-3):37–59, 2000.
Rezende, Danilo Jimenez and Mohamed, Shakir. Variational inference with normalizing flows. In Bach, Francis R. and Blei,
David M. (eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July
2015, volume 37 of JMLR Workshop and Conference Proceedings, pp. 1530–1538. JMLR.org, 2015. URL http://jmlr.
org/proceedings/papers/v37/rezende15.html.
Sagun, Levent, Bottou, Léon, and LeCun, Yann. Singularity of
the hessian in deep learning. arXiv preprint arXiv:1611.07476,
2016.

