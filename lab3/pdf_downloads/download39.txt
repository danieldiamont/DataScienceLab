Wasserstein Generative Adversarial Networks

A. Why Wasserstein is indeed weak
We now introduce our notation. Let X âŠ† Rd be a compact set (such as [0, 1]d the space of images). We define Prob(X ) to
be the space of probability measures over X . We note
Cb (X ) = {f : X â†’ R, f is continuous and bounded}
Note that if f âˆˆ Cb (X ), we can define kf kâˆž = maxxâˆˆX |f (x)|, since f is bounded. With this norm, the space (Cb (X ), k Â·
kâˆž ) is a normed vector space. As for any normed vector space, we can define its dual
Cb (X )âˆ— = {Ï† : Cb (X ) â†’ R, Ï† is linear and continuous}
and give it the dual norm kÏ†k = supf âˆˆCb (X ),kf kâˆž â‰¤1 |Ï†(f )|.
With this definitions, (Cb (X )âˆ— , k Â· k) is another normed space. Now let Âµ be a signed measure over X , and let us define the
total variation distance
kÂµkT V = sup |Âµ(A)|
AâŠ†X

where the supremum is taken over the Borel sets in X . Since the total variation is a norm, then if we have Pr and PÎ¸ two
probability distributions over X ,
Î´(Pr , PÎ¸ ) := kPr âˆ’ PÎ¸ kT V
is a distance in Prob(X ) (called the total variation distance).
We can consider
Î¦ : (Prob(X ), Î´) â†’ (Cb (X )âˆ— , k Â· k)
where Î¦(P)(f ) := Exâˆ¼P [f (x)] is a linear function over Cb (X ). The Riesz Representation theorem ((Kakutani, 1941),
Theorem 10) tells us that Î¦ is an isometric immersion. This tells us that we can effectively consider Prob(X ) with the
total variation distance as a subset of Cb (X )âˆ— with the norm distance. Thus, just to accentuate it one more time, the total
variation over Prob(X ) is exactly the norm distance over Cb (X )âˆ— .
Let us stop for a second and analyze what all this technicality meant. The main thing to carry is that we introduced a
distance Î´ over probability distributions. When looked as a distance over a subset of Cb (X )âˆ— , this distance gives the norm
topology. The norm topology is very strong. Therefore, we can expect that not many functions Î¸ 7â†’ PÎ¸ will be continuous
when measuring distances between distributions with Î´. As we will show later in Theorem 2, Î´ gives the same topology as
the Jensen-Shannon divergence, pointing to the fact that the JS is a very strong distance, and is thus more propense to give
a discontinuous loss function.
Now, all dual spaces (such as Cb (X )âˆ— and thus Prob(X )) have a strong topology (induced by the norm), and a weak*
topology. As the name suggests, the weak* topology is much weaker than the strong topology. In the case of Prob(X ),
the strong topology is given by the total variation distance, and the weak* topology is given by the Wasserstein distance
(among others) (Villani, 2009).

B. Assumption definitions
Assumption 1. Let g : Z Ã— Rd â†’ X be locally Lipschitz between finite dimensional vector spaces. We will denote gÎ¸ (z)
itâ€™s evaluation on coordinates (z, Î¸). We say that g satisfies assumption 1 for a certain probability distribution p over Z if
there are local Lipschitz constants L(Î¸, z) such that
Ezâˆ¼p [L(Î¸, z)] < +âˆž

C. Proofs of things
Proof of Theorem 1. Let Î¸ and Î¸0 be two parameter vectors in Rd . Then, we will first attempt to bound W (PÎ¸ , PÎ¸0 ), from
where the theorem will come easily. The main element of the proof is the use of the coupling Î³, the distribution of the joint
(gÎ¸ (Z), gÎ¸0 (Z)), which clearly has Î³ âˆˆ Î (PÎ¸ , PÎ¸0 ).

Wasserstein Generative Adversarial Networks

By the definition of the Wasserstein distance, we have
Z
W (PÎ¸ , PÎ¸0 ) â‰¤

kx âˆ’ yk dÎ³
X Ã—X

= E(x,y)âˆ¼Î³ [kx âˆ’ yk]
= Ez [kgÎ¸ (z) âˆ’ gÎ¸0 (z)k]
If g is continuous in Î¸, then gÎ¸ (z) â†’Î¸â†’Î¸0 gÎ¸0 (z), so kgÎ¸ âˆ’ gÎ¸0 k â†’ 0 pointwise as functions of z. Since X is compact, the
distance of any two elements in it has to be uniformly bounded by some constant M , and therefore kgÎ¸ (z) âˆ’ gÎ¸0 (z)k â‰¤ M
for all Î¸ and z uniformly. By the bounded convergence theorem, we therefore have
W (PÎ¸ , PÎ¸0 ) â‰¤ Ez [kgÎ¸ (z) âˆ’ gÎ¸0 (z)k] â†’Î¸â†’Î¸0 0
Finally, we have that
|W (Pr , PÎ¸ ) âˆ’ W (Pr , PÎ¸0 )| â‰¤ W (PÎ¸ , PÎ¸0 ) â†’Î¸â†’Î¸0 0
proving the continuity of W (Pr , PÎ¸ ).
Now let g be locally Lipschitz. Then, for a given pair (Î¸, z) there is a constant L(Î¸, z) and an open set U such that
(Î¸, z) âˆˆ U , such that for every (Î¸0 , z 0 ) âˆˆ U we have
kgÎ¸ (z) âˆ’ gÎ¸0 (z 0 )k â‰¤ L(Î¸, z)(kÎ¸ âˆ’ Î¸0 k + kz âˆ’ z 0 k)
By taking expectations and z 0 = z we
Ez [kgÎ¸ (z) âˆ’ gÎ¸0 (z)k] â‰¤ kÎ¸ âˆ’ Î¸0 kEz [L(Î¸, z)]
whenever (Î¸0 , z) âˆˆ U . Therefore, we can define UÎ¸ = {Î¸0 |(Î¸0 , z) âˆˆ U }. Itâ€™s easy to see that since U was open, UÎ¸ is as
well. Furthermore, by assumption 1, we can define L(Î¸) = Ez [L(Î¸, z)] and achieve
|W (Pr , PÎ¸ ) âˆ’ W (Pr , PÎ¸0 )| â‰¤ W (PÎ¸ , PÎ¸0 ) â‰¤ L(Î¸)kÎ¸ âˆ’ Î¸0 k
for all Î¸0 âˆˆ UÎ¸ , meaning that W (Pr , PÎ¸ ) is locally Lipschitz. This obviously implies that W (Pr , PÎ¸ ) is everywhere
continuous, and by Radamacherâ€™s theorem we know it has to be differentiable almost everywhere.
The counterexample for item 3 of the Theorem is indeed Example 1.
Proof of Corollary 1. We begin with the case of smooth nonlinearities. Since g is C 1 as a function of (Î¸, z) then for any
fixed (Î¸, z) we have L(Î¸, Z) â‰¤ kâˆ‡Î¸,x gÎ¸ (z)k +  is an acceptable local Lipschitz constant for all  > 0. Therefore, it
suffices to prove
Ezâˆ¼p(z) [kâˆ‡Î¸,z gÎ¸ (z)k] < +âˆž
QH
If H is the number of layers we know that âˆ‡z gÎ¸ (z) = k=1 Wk Dk where Wk are the weight matrices and Dk is are the
diagonal Jacobians
Qof the nonlinearities.
  Let fi:j be the application of layers i to j inclusively (e.g. gÎ¸ = f1:H ). Then,
H
âˆ‡Wk gÎ¸ (z) =
i=k+1 Wi Di Dk f1:kâˆ’1 (z). We recall that if L is the Lipschitz constant of the nonlinearity, then
Qkâˆ’1
kDi k â‰¤ L and kf1:kâˆ’1 (z)k â‰¤ kzkLkâˆ’1 i=1 Wi . Putting this together,
! !
H
H
H
Y
X
Y
kâˆ‡z,Î¸ gÎ¸ (z)k â‰¤ k
Wi D i k +
k
Wi Di Dk f1:kâˆ’1 (z)k
i=1
H

â‰¤L

K
Y
i=H

If C1 (Î¸) = LH

Q

H
i=1

k=1

kWi k +

H
X
k=1

i=k+1
H

kzkL

kâˆ’1
Y
i=1

!
kWi k

H
Y

!
kWi k

i=k+1


Q
 Q

PH
kâˆ’1
H
kWi k and C2 (Î¸) = k=1 LH
i=1 kWi k
i=k+1 kWi k then
Ezâˆ¼p(z) [kâˆ‡Î¸,z gÎ¸ (z)k] â‰¤ C1 (Î¸) + C2 (Î¸)Ezâˆ¼p(z) [kzk] < +âˆž

finishing the proof

Wasserstein Generative Adversarial Networks

Proof of Theorem 2.
1.

â€¢ (Î´(Pn , P) â†’ 0 â‡’ JS(Pn , P) â†’ 0) â€” Let Pm be the mixture distribution Pm = 12 Pn + 12 P (note that Pm
depends on n). It is easily verified that Î´(Pm , Pn ) â‰¤ Î´(Pn , P), and in particular this tends to 0 (as does Î´(Pm , P)).
We now show this for completeness. Let Âµ be a signed measure, we define kÂµkT V = supAâŠ†X |Âµ(A)|. for all
Borel sets A. In this case,
Î´(Pm , Pn ) = kPm âˆ’ Pn kT V
1
1
= k P + Pn âˆ’ Pn kT V
2
2
1
= kP âˆ’ Pn kT V
2
1
= Î´(Pn , P) â‰¤ Î´(Pn , P)
2
dPn
Let fn = dP
be the Radon-Nykodim derivative between Pn and the mixture. Note that by construction for
m
every Borel set A we have Pn (A) â‰¤ 2Pm (A). If A = {fn > 3} then we get
Z
Pn (A) =
fn dPm â‰¥ 3Pm (A)
A

which implies Pm (A) = 0. This means that fn is bounded by 3 Pm (and therefore Pn and P)-almost everywhere.
We could have done this for any constant larger than 2 but for our purposes 3 will sufice.
Let  > 0 fixed, and An = {fn > 1 + }. Then,
Z
Pn (An ) =
fn dPm â‰¥ (1 + )Pm (An )
An

Therefore,
Pm (An ) â‰¤ Pn (An ) âˆ’ Pm (An )
â‰¤ |Pn (An ) âˆ’ Pm (An )|
â‰¤ Î´(Pn , Pm )
â‰¤ Î´(Pn , P).
Which implies Pm (Am ) â‰¤ 1 Î´(Pn , P). Furthermore,
Pn (An ) â‰¤ Pm (An ) + |Pn (An ) âˆ’ Pm (An )|
1
â‰¤ Î´(Pn , P) + Î´(Pn , Pm )

1
â‰¤ Î´(Pn , P) + Î´(Pn , P)



1
â‰¤
+ 1 Î´(Pn , P)

We now can see that
Z
KL(Pn kPm ) =

log(fn ) dPn
Z
â‰¤ log(1 + ) +

log(fn ) dPn

An

â‰¤ log(1 + ) + log(3)Pn (An )


1
â‰¤ log(1 + ) + log(3)
+ 1 Î´(Pn , P)


Wasserstein Generative Adversarial Networks

Taking limsup we get 0 â‰¤ lim sup KL(Pn kPm ) â‰¤ log(1 + ) for all  > 0, which means KL(Pn kPm ) â†’ 0.
dP
, and
In the same way, we can define gn = dP
m
2Pm ({gn > 3}) â‰¥ P({gn > 3}) â‰¥ 3Pm ({gn > 3})
meaning that Pm ({gn > 3}) = 0 and therefore gn is bounded by 3 almost everywhere for Pn , Pm and P. With
the same calculation, Bn = {gn > 1 + } and
Z
P(Bn ) =
gn dPm â‰¥ (1 + )Pm (Bn )
Bn

so Pm (Bn ) â‰¤ 1 Î´(P, Pm ) â†’ 0, and therefore P(Bn ) â†’ 0. We can now show
Z
KL(PkPm ) = log(gn ) dP
Z
â‰¤ log(1 + ) +
log(gn ) dP
Bn

â‰¤ log(1 + ) + log(3)P(Bn )
so we achieve 0 â‰¤ lim sup KL(PkPm ) â‰¤ log(1 + ) and then KL(PkPm ) â†’ 0. Finally, we conclude
JS(Pn , P) =
â€¢ (JS(Pn , P) â†’ 0 â‡’ Î´(Pn , P) â†’ 0)
we get

â€”

1
1
KL(Pn kPm ) + KL(PkPm ) â†’ 0
2
2
by a simple application of the triangular and Pinskerâ€™s inequalities

Î´(Pn , P) â‰¤ Î´(Pn , Pm ) + Î´(P, Pm )
r
r
1
1
KL(Pn kPm ) +
KL(PkPm )
â‰¤
2
2
p
â‰¤ 2 JS(Pn , P) â†’ 0
2. This is a long known fact that W metrizes the weak* topology of (C(X ), k Â· kâˆž ) on Prob(X ), and by definition this
is the topology of convergence in distribution. A proof of this can be found (for example) in (Villani, 2009).
3. This is a straightforward application of Pinskerâ€™s inequality
r
1
Î´(Pn , P) â‰¤
KL(Pn kP) â†’ 0
2
r
1
Î´(P, Pn ) â‰¤
KL(PkPn ) â†’ 0
2
4. This is trivial by recalling the fact that Î´ and W give the strong and weak* topologies on the dual of (C(X ), k Â· kâˆž )
when restricted to Prob(X ).

Proof of Theorem 3. Let us define
V (fËœ, Î¸) = Exâˆ¼Pr [fËœ(x)] âˆ’ Exâˆ¼PÎ¸ [fËœ(x)]
= Exâˆ¼P [fËœ(x)] âˆ’ Ezâˆ¼p(z) [fËœ(gÎ¸ (z))]
r

where fËœ lies in F = {fËœ : X â†’ R , fËœ âˆˆ Cb (X ), kfËœkL â‰¤ 1} and Î¸ âˆˆ Rd .

Wasserstein Generative Adversarial Networks

Since X is compact, we know by the Kantorovich-Rubinstein duality (Villani, 2009) that there is an f âˆˆ F that attains the
value
W (Pr , PÎ¸ ) = sup V (fËœ, Î¸) = V (f, Î¸)
fËœâˆˆF

âˆ—

Let us define X (Î¸) = {f âˆˆ F : V (f, Î¸) = W (Pr , PÎ¸ )}. By the above point we know then that X âˆ— (Î¸) is non-empty. We
know by a simple envelope theorem ((Milgrom & Segal, 2002), Theorem 1) that
âˆ‡Î¸ W (Pr , PÎ¸ ) = âˆ‡Î¸ V (f, Î¸)
for any f âˆˆ X âˆ— (Î¸) when both terms are well-defined.
Let f âˆˆ X âˆ— (Î¸), which we knows exists since X âˆ— (Î¸) is non-empty for all Î¸. Then, we get
âˆ‡Î¸ W (Pr , PÎ¸ ) = âˆ‡Î¸ V (f, Î¸)
= âˆ‡Î¸ [Exâˆ¼Pr [f (x)] âˆ’ Ezâˆ¼p(z) [f (gÎ¸ (z))]
= âˆ’âˆ‡Î¸ Ezâˆ¼p(z) [f (gÎ¸ (z))]
under the condition that the first and last terms are well-defined. The rest of the proof will be dedicated to show that
âˆ’âˆ‡Î¸ Ezâˆ¼p(z) [f (gÎ¸ (z))] = âˆ’Ezâˆ¼p(z) [âˆ‡Î¸ f (gÎ¸ (z))]

(4)

when the right hand side is defined. For the reader who is not interested in such technicalities, he or she can skip the rest
of the proof.
Since f âˆˆ F, we know that it is 1-Lipschitz. Furthermore, gÎ¸ (z) is locally Lipschitz as a function of (Î¸, z). Therefore,
f (gÎ¸ (z)) is locally Lipschitz on (Î¸, z) with constants L(Î¸, z) (the same ones as g). By Radamacherâ€™s Theorem, f (gÎ¸ (z))
has to be differentiable almost everywhere for (Î¸, z) jointly. Rewriting this, the set A = {(Î¸, z) : f â—¦ g is not differentiable}
has measure 0. By Fubiniâ€™s Theorem, this implies that for almost every Î¸ the section AÎ¸ = {z : (Î¸, z) âˆˆ A} has measure 0.
Letâ€™s now fix a Î¸0 such that the measure of AÎ¸0 is null (such as when the right hand side of equation (4) is well defined).
For this Î¸0 we have âˆ‡Î¸ f (gÎ¸ (z))|Î¸0 is well-defined for almost any z, and since p(z) has a density, it is defined p(z)-a.e. By
assumption 1 we know that
Ezâˆ¼p(z) [kâˆ‡Î¸ f (gÎ¸ (z))|Î¸0 k] â‰¤ Ezâˆ¼p(z) [L(Î¸0 , z)] < +âˆž
so Ezâˆ¼p(z) [âˆ‡Î¸ f (gÎ¸ (z))|Î¸0 ] is well-defined for almost every Î¸0 . Now, we can see
Ezâˆ¼p(z) [f (gÎ¸ (z))] âˆ’ Ezâˆ¼p(z) [f (gÎ¸0 (z))] âˆ’ h(Î¸ âˆ’ Î¸0 ), Ezâˆ¼p(z) [âˆ‡Î¸ f (gÎ¸ (z))|Î¸0 ]i
kÎ¸ âˆ’ Î¸0 k

= Ezâˆ¼p(z)

f (gÎ¸ (z)) âˆ’ f (gÎ¸0 (z)) âˆ’ h(Î¸ âˆ’ Î¸0 ), âˆ‡Î¸ f (gÎ¸ (z))|Î¸0 i
kÎ¸ âˆ’ Î¸0 k

(5)



By differentiability, the term inside the integral converges p(z)-a.e. to 0 as Î¸ â†’ Î¸0 . Furthermore,
k

f (gÎ¸ (z)) âˆ’ f (gÎ¸0 (z)) âˆ’ h(Î¸ âˆ’ Î¸0 ), âˆ‡Î¸ f (gÎ¸ (z))|Î¸0 i
k
kÎ¸ âˆ’ Î¸0 k
kÎ¸ âˆ’ Î¸0 kL(Î¸0 , z) + kÎ¸ âˆ’ Î¸0 kkâˆ‡Î¸ f (gÎ¸ (z))|Î¸0 k
â‰¤
kÎ¸ âˆ’ Î¸0 k
â‰¤ 2L(Î¸0 , z)

and since Ezâˆ¼p(z) [2L(Î¸0 , z)] < +âˆž by assumption 1, we get by dominated convergence that Equation 5 converges to 0 as
Î¸ â†’ Î¸0 so
âˆ‡Î¸ Ezâˆ¼p(z) [f (gÎ¸ (z))] = Ezâˆ¼p(z) [âˆ‡Î¸ f (gÎ¸ (z))]
for almost every Î¸, and in particular when the right hand side is well defined. Note that the mere existance of the left hand
side (meaning the differentiability a.e. of Ezâˆ¼p(z) [f (gÎ¸ (z))]) had to be proven, which we just did.

Wasserstein Generative Adversarial Networks

D. Related Work
Thereâ€™s been a number of works on the so called Integral Probability Metrics (IPMs) (MuÌˆller, 1997). Given F a set of
functions from X to R, we can define
dF (Pr , PÎ¸ ) = sup Exâˆ¼Pr [f (x)] âˆ’ Exâˆ¼PÎ¸ [f (x)]

(6)

f âˆˆF

as an integral probability metric associated with the function class F. It is easily verified that if for every f âˆˆ F we have
âˆ’f âˆˆ F (such as all examples weâ€™ll consider), then dF is nonnegative, satisfies the triangular inequality, and is symmetric.
Thus, dF is a pseudometric over Prob(X ).
While IPMs might seem to share a similar formula, as we will see different classes of functions can yield to radically
different metrics.
â€¢ By the Kantorovich-Rubinstein duality (Villani, 2009), we know that W (Pr , PÎ¸ ) = dF (Pr , PÎ¸ ) when F is the set of
1-Lipschitz functions. Furthermore, if F is the set of K-Lipschitz functions, we get K Â· W (Pr , PÎ¸ ) = dF (Pr , PÎ¸ ).
â€¢ When F is the set of all continuous functions bounded between -1 and 1, we retrieve dF (Pr , PÎ¸ ) = Î´(Pr , PÎ¸ ) the total
variation distance (MuÌˆller, 1997). This already tells us that going from 1-Lipschitz to 1-Bounded functions drastically
changes the topology of the space, and the regularity of dF (Pr , PÎ¸ ) as a loss function (as by Theorems 1 and 2).
â€¢ Energy-based GANs (EBGANs) (Zhao et al., 2016) can be thought of as the generative approach to the total variation
distance. This connection is stated and proven in depth in Appendix E. At the core of the connection is that the
discriminator will play the role of f maximizing equation (6) while its only restriction is being between 0 and m for
some constant m. This will yield the same behaviour as being restricted to be between âˆ’1 and 1 up to a constant
scaling factor irrelevant to optimization. Thus, when the discriminator approaches optimality the cost for the generator
will aproximate the total variation distance Î´(Pr , PÎ¸ ).
Since the total variation distance displays the same regularity as the JS, it can be seen that EBGANs will suffer
from the same problems of classical GANs regarding not being able to train the discriminator till optimality and thus
limiting itself to very imperfect gradients.
â€¢ Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) is a specific case of integral probability metrics when
F = {f âˆˆ H : kf kH â‰¤ 1} for H some Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel
k : X Ã— X â†’ R. As proved on (Gretton et al., 2012) we know that MMD is a proper metric and not only a
pseudometric when the kernel is universal. In the specific case where H = L2 (X , m) for m the normalized Lebesgue
measure on X , we know that {f âˆˆ Cb (X ), kf kâˆž â‰¤ 1} will be contained in F, and therefore Î´(Pr , PÎ¸ ) â‰¤ dF (Pr , PÎ¸ ),
so the regularity of the MMD distance as a loss function will be at least as bad as the one of the total variation.
Nevertheless this is a very extreme case, since we would need a very powerful kernel to approximate the whole L2 .
However, even Gaussian kernels are able to detect tiny noise patterns as recently evidenced by (Sutherland et al.,
2017). This points to the fact that especially with low bandwidth kernels, the distance might be close to a saturating
regime similar as with total variation or the JS. This obviously doesnâ€™t need to be the case for every kernel, and
figuring out how and which different MMDs are closer to Wasserstein or total variation distances is an interesting
topic of research.
The great aspect of MMD is that via the kernel trick there is no need to train a separate network to maximize equation
(6) for the ball of a RKHS. However, this has the disadvantage that evaluating the MMD distance has computational
cost that grows quadratically with the amount of samples used to estimate the expectations in (6). This last point makes
MMD have limited scalability, and is sometimes inapplicable to many real life applications because of it. There are
estimates with linear computational cost for the MMD (Gretton et al., 2012) which in a lot of cases makes MMD very
useful, but they also have worse sample complexity.
â€¢ Generative Moment Matching Networks (GMMNs) (Li et al., 2015; Dziugaite et al., 2015) are the generative counterpart of MMD. By backproping through the kernelized formula for equation (6), they directly optimize dM M D (Pr , PÎ¸ )
(the IPM when F is as in the previous item). As mentioned, this has the advantage of not requiring a separate network
to approximately maximize equation (6). However, GMMNs have enjoyed limited applicability. Partial explanations
for their unsuccess are the quadratic cost as a function of the number of samples and vanishing gradients for lowbandwidth kernels. Furthermore, it may be possible that some kernels used in practice are unsuitable for capturing

Wasserstein Generative Adversarial Networks

very complex distances in high dimensional sample spaces such as natural images. This is properly justified by the
fact that (Ramdas et al., 2014) shows that for the typical Gaussian MMD test to be reliable (as in itâ€™s power as a statistical test approaching 1), we need the number of samples to grow linearly with the number of dimensions. Since the
MMD computational cost grows quadratically with the number of samples in the batch used to estimate equation (6),
this makes the cost of having a reliable estimator grow quadratically with the number of dimensions, which makes it
very inapplicable for high dimensional problems. Indeed, for something as standard as 64x64 images, we would need
minibatches of size at least 4096 (without taking into account the constants in the bounds of (Ramdas et al., 2014)
which would make this number substantially larger) and a total cost per iteration of 40962 , over 5 orders of magnitude
more than a GAN iteration when using the standard batch size of 64.
That being said, these numbers can be a bit unfair to the MMD, in the sense that we are comparing empirical sample
complexity of GANs with the theoretical sample complexity of MMDs, which tends to be worse. However, in the
original GMMN paper (Li et al., 2015) they indeed used a minibatch of size 1000, much larger than the standard 32 or
64 (even when this incurred in quadratic computational cost). While estimates that have linear computational cost as
a function of the number of samples exist (Gretton et al., 2012), they have worse sample complexity, and to the best
of our knowledge they havenâ€™t been yet applied in a generative context such as in GMMNs.
On another great line of research, the recent work of (Montavon et al., 2016) has explored the use of Wasserstein distances
in the context of learning for Restricted Boltzmann Machines for discrete spaces. The motivations at a first glance might
seem quite different, since the manifold setting is restricted to continuous spaces and in finite discrete spaces the weak
and strong topologies (the ones of W and JS respectively) coincide. However, in the end there is more in commmon than
not about our motivations. We both want to compare distributions in a way that leverages the geometry of the underlying
space, and Wasserstein allows us to do exactly that.
Finally, the work of (Genevay et al., 2016) shows new algorithms for calculating Wasserstein distances between different
distributions. We believe this direction is quite important, and perhaps could lead to new ways to evaluate generative
models.

Wasserstein Generative Adversarial Networks

E. Energy-based GANs optimize total variation
In this appendix we show that under an optimal discriminator, energy-based GANs (EBGANs) (Zhao et al., 2016) optimize
the total variation distance between the real and generated distributions.
Energy-based GANs are trained in a similar fashion to GANs, only under a different loss function. They have a discriminator D who tries to minimize
LD (D, gÎ¸ ) = Exâˆ¼Pr [D(x)] + Ezâˆ¼p(z) [[m âˆ’ D(gÎ¸ (z))]+ ]
for some m > 0 and [x]+ = max(0, x) and a generator network gÎ¸ thatâ€™s trained to minimize
LG (D, gÎ¸ ) = Ezâˆ¼p(z) [D(gÎ¸ (z))] âˆ’ Exâˆ¼Pr [D(x)]
Very importantly, D is constrained to be non-negative, since otherwise the trivial solution for D would be to set everything
to arbitrarily low values. The original EBGAN paper used only Ezâˆ¼p(z) [D(gÎ¸ (z))] for the loss of the generator, but this is
obviously equivalent to our definition since the term Exâˆ¼Pr [D(x)] does not dependent on Î¸ for a fixed discriminator (such
as when backproping to the generator in EBGAN training) and thus minimizing one or the other is equivalent.
We say that a measurable function Dâˆ— : X â†’ [0, +âˆž) is optimal for gÎ¸ (or PÎ¸ ) if LD (Dâˆ— , gÎ¸ ) â‰¤ LD (D, gÎ¸ ) for all other
measurable functions D. We show that such a discriminator always exists for any two distributions Pr and PÎ¸ , and that
under such a discriminator, LG (Dâˆ— , gÎ¸ ) is proportional to Î´(Pr , PÎ¸ ). As a simple corollary, we get the fact that LG (Dâˆ— , gÎ¸ )
attains its minimum value if and only if Î´(Pr , PÎ¸ ) is at its minimum value, which is 0, and Pr = PÎ¸ (Theorems 1-2 of
(Zhao et al., 2016)).
Theorem 4. Let Pr be a the real data distribution over a compact space X . Let gÎ¸ : Z â†’ X be a measurable function
(such as any neural network). Then, an optimal discriminator Dâˆ— exists for Pr and PÎ¸ , and
LG (Dâˆ— , gÎ¸ ) =

m
Î´(Pr , PÎ¸ )
2

Proof. First, we prove that there exists an optimal discriminator. Let D : X â†’ [0, +âˆž) be a measurable function, then
D0 (x) := min(D(x), m) is also a measurable function, and LD (D0 , gÎ¸ ) â‰¤ LD (D, gÎ¸ ). Therefore, a function Dâˆ— : X â†’
[0, +âˆž) is optimal if and only if Dâˆ— 0 is. Furthermore, it is optimal if and only if LD (Dâˆ— , gÎ¸ ) â‰¤ LD (D, gÎ¸ ) for all D :
X â†’ [0, m]. We are then interested to see if thereâ€™s an optimal discriminator for the problem min0â‰¤D(x)â‰¤m LD (D, gÎ¸ ).
Note now that if 0 â‰¤ D(x) â‰¤ m we have
LD (D, gÎ¸ ) = Exâˆ¼Pr [D(x)] + Ezâˆ¼p(z) [[m âˆ’ D(gÎ¸ (z))]+ ]
= Exâˆ¼Pr [D(x)] + Ezâˆ¼p(z) [m âˆ’ D(gÎ¸ (z))]
= m + Exâˆ¼Pr [D(x)] âˆ’ Ezâˆ¼p(z) [D(gÎ¸ (z))]
= m + Exâˆ¼Pr [D(x)] âˆ’ Exâˆ¼PÎ¸ [D(x)]
Therefore, we know that
inf
0â‰¤D(x)â‰¤m

LD (D, gÎ¸ ) = m +
=m+
=m+

inf
0â‰¤D(x)â‰¤m

inf

Exâˆ¼Pr [D(x)] âˆ’ Exâˆ¼PÎ¸ [D(x)]

m
âˆ’m
2 â‰¤D(x)â‰¤ 2

Exâˆ¼Pr [D(x)] âˆ’ Exâˆ¼PÎ¸ [D(x)]

m
inf
Exâˆ¼Pr [f (x)] âˆ’ Exâˆ¼PÎ¸ [f (x)]
2 âˆ’1â‰¤f (x)â‰¤1

The interesting part is that
inf

âˆ’1â‰¤f (x)â‰¤1

Exâˆ¼Pr [f (x)] âˆ’ Exâˆ¼PÎ¸ [f (x)] = âˆ’Î´(Pr , PÎ¸ )

(7)

and there is an f âˆ— : X â†’ [âˆ’1, 1] such that Exâˆ¼Pr [f âˆ— (x)] âˆ’ Exâˆ¼PÎ¸ [f âˆ— (x)] = âˆ’Î´(Pr , PÎ¸ ). This is a long known fact, found
m
âˆ—
for example in (Villani, 2009), but we prove it later for completeness. In that case, we define Dâˆ— (x) = m
2 f (x) + 2 . We

Wasserstein Generative Adversarial Networks

then have 0 â‰¤ D(x) â‰¤ m and
LD (Dâˆ— , gÎ¸ ) = m + Exâˆ¼Pr [Dâˆ— (x)] âˆ’ Exâˆ¼PÎ¸ [Dâˆ— (x)]
m
= m + Exâˆ¼Pr [Dâˆ— (x)] âˆ’ Exâˆ¼PÎ¸ [f âˆ— (x)]
2
m
= m âˆ’ Î´(Pr , PÎ¸ )
2
=
inf
LD (D, gÎ¸ )
0â‰¤D(x)â‰¤m

This shows that Dâˆ— is optimal and LD (Dâˆ— , gÎ¸ ) = m âˆ’

m
2 Î´(Pr , PÎ¸ ).

Furthermore,

LG (Dâˆ— , gÎ¸ ) = Ezâˆ¼p(z) [Dâˆ— (gÎ¸ (z))] âˆ’ Exâˆ¼Pr [Dâˆ— (x)]
= âˆ’LD (Dâˆ— , gÎ¸ ) + m
m
= Î´(Pr , Pg )
2
concluding the proof.
For completeness, we now show a proof for equation (7) and the existence of said f âˆ— that attains the value of the infimum.
Take Âµ = Pr âˆ’ PÎ¸ , which is a signed measure, and (P, Q) its Hahn decomposition. Then, we can define f âˆ— := 1Q âˆ’ 1P .
By construction, then
Z
Exâˆ¼Pr [f âˆ— (x)]âˆ’Exâˆ¼PÎ¸ [f âˆ— (x)] = f âˆ— dÂµ = Âµ(Q)âˆ’Âµ(P ) = âˆ’(Âµ(P )âˆ’Âµ(Q)) = âˆ’kÂµkT V = âˆ’kPr âˆ’PÎ¸ kT V = âˆ’Î´(Pr , PÎ¸ )
Furthermore, if f is bounded between -1 and 1, we get
Z
|Exâˆ¼Pr [f (x)] âˆ’ Exâˆ¼PÎ¸ [f (x)]| = |

Z
f dPr âˆ’

f dPÎ¸ |

Z
= | f dÂµ|
Z
Z
â‰¤ |f | d|Âµ| â‰¤ 1 d|Âµ|
= |Âµ|(X ) = kÂµkT V = Î´(Pr , PÎ¸ )
Since Î´ is positive, we can conclude Exâˆ¼Pr [f (x)] âˆ’ Exâˆ¼PÎ¸ [f (x)] â‰¥ âˆ’Î´(Pr , PÎ¸ ).

Wasserstein Generative Adversarial Networks

F. Generatorâ€™s cost during normal GAN training

Figure 9: Cost of the generator during normal GAN training, for an MLP generator (upper left) and a DCGAN generator (upper
right). Both had a DCGAN discriminator. Both curves have increasing error. Samples get better for the DCGAN but the cost of the
generator increases, pointing towards no significant correlation between sample quality and loss. Bottom: M LP with both generator
and discriminator. The curve goes up and down regardless of sample quality. All training curves were passed through the same median
filter as in Figure 4.

G. Further comments on clipping and batch normalization
In this appendix we provide further informal insights into the behaviour of weight clipping and batch normalization in the
context of GANs and WGANs.
G.1. Weight clipping
One may wander what would happen if one were to use weight clipping in a standard GAN. Disregarding the use of the
cross-entropy vs difference loss, the central difference would be the use of a sigmoid in the end of the discriminator. This
brings into place the use of the Dudley metric:
dF (Pr , PÎ¸ ) = sup Exâˆ¼Pr [f (x)] âˆ’ Exâˆ¼PÎ¸ [f (x)]
f âˆˆF

where
F = {f : X â†’ R, f continuous and kf kâˆž + kf kL â‰¤ 1}
This is similar to the class of 1-Lipschitz functions, only we restrict how high the values of f can be. This metric is easily
shown to be equivalent to the one with
F 00 = {f : X â†’ R, f continuous and kf kâˆž â‰¤ 1, kf kL â‰¤ K}
or the one with
F 0 = {f : X â†’ R, f continuous and 0 â‰¤ f â‰¤ 1, kf kL â‰¤ K}

(8)

This last family is essentially the family of functions we would achieve by adding a sigmoid to the critic of the WGAN,
moving us closer to the standard GAN realm. An easy and very interesting result is that dF and dF 0 have the same topology
as the Wasserstein distance (Villani, 2009), which hints that adding clipping to a GAN lands us closer to a WGAN than
the standard GAN (since the cost function between distributions has the exact same regularity as that of a WGAN, and
drastically different from a normal GAN, see Theorems 1, 2).

Wasserstein Generative Adversarial Networks

That being said, while the topology of Wasserstein and the Dudley metric is the same, some things are not. There are many
distances that yield the weak topology, but Wasserstein has a number of differences against the rest. These are perfectly
explained in pages 110 and 111 of (Villani, 2009), but we highlight the main idea here. At the core of it, Wasserstein is
better at representing long distances between samples. The saturation behaviour of the sigmoid, or what happens when K
in (8) is large, shows that if the real samples are far away from the fake ones, f can saturate at 1 in the real and 0 in the fake
constantly, providing no usable gradient. Thus, Dudley and Wasserstein have the same behaviour in close samples, but
Wasserstein avoids saturations and provides gradients even when samples are far away. However, if K (i.e. the clipping)
is small enough (such as the 0.01 we use in practice), the saturating regime will never enter in place, so Dudley and
Wasserstein will behave in the same way.
To conclude, if the clipping is small enough, the network is quite literally a WGAN, and if itâ€™s large it will saturate and fail
to take into account information between samples that are far away (much like a normal GAN when the discriminator is
trained till optimum).
As to the similarities between the difference vs cross-entropy on the loss of the discriminator or critic: if the supports of
Pr and PÎ¸ are essentially disjoint (which was shown to happen in (Arjovsky & Bottou, 2017) in the usual setting of low
dimensional supports), with both cost functions the f will simply be trained to attain the maximum value possible in the
real and the minimum possible in the fake, without surpassing the Lipschitz constraint. Therefore, CE and the difference
might behave more similarly than we expect in the typical â€˜learning low dimensional distributionsâ€™ setting, provided we
have a strong Lipschitz constraint.
G.2. Batch normalization
It is not clear that batch normalization (BN) is a Lipschitz function with a constant independent of the parameters, since
we are dividing by the standard deviation, which depends on the inputs and the parameters. In this subsection we explain
why BN still behaves in a Lipschitz way, and fits in with the theoretical support of our method.
Let x be the input of a BN layer. If there is a positive c âˆˆ R for which V (x)1/2 > c (the variance is uniformly bounded
below during training), then this c âˆˆ R becomes a Lipschitz constant on the BN layer thatâ€™s independent of the model
parameters, as we wanted. For V (x) to not be bounded below as training progresses, it has to go arbitrarily close to 0. In
this case, x has to converge (up to taking a subsequence) to itâ€™s mean, so the term x âˆ’ E[x] in the numerator of batchnorm
comes the constant 0 (which is obviously 1-Lipschitz) due to the  in the division.
will go to 0, and therefore V xâˆ’E[x]
[x]1/2 +
This will also further render the activation x inactive, which the network has no incentive to do.
While this argument is handwavy, one can formalize it and prove very simple bounds that depend only on . By increasing
 one can enforce a stronger Lipschitz constant, and we could have for example clamped the denominator of the BN to
attain a value large enough. However, in practice in all our runs the variance never surpassed low thresholds, and this
clamping of the BN division was simply never set into effect. Thus, we empirically never saw a break in the Lipschitness
of our BN layers.

H. Sheets of samples

Wasserstein Generative Adversarial Networks

Figure 10: WGAN algorithm: generator and critic are DCGANs.

Figure 11: Standard GAN procedure: generator and discriminator are DCGANs.

Wasserstein Generative Adversarial Networks

Figure 12: WGAN algorithm: generator is a DCGAN without batchnorm and constant filter size. Critic is a DCGAN.

Figure 13: Standard GAN procedure: generator is a DCGAN without batchnorm and constant filter size. Discriminator is a DCGAN.

Wasserstein Generative Adversarial Networks

Figure 14: WGAN algorithm: generator is an MLP with 4 hidden layers of 512 units, critic is a DCGAN.

Figure 15: Standard GAN procedure: generator is an MLP with 4 hidden layers of 512 units, discriminator is a DCGAN.

