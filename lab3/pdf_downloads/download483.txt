Geometry of Neural Networks

Supplemental Material: Geometry of Neural Network Loss Surfaces via
Random Matrix Theory
1. Computing the normalized index
One way to obtain an expression for the normalized index is to rewrite eqn. (18) as f (G) = z (where f (G) = RH (G) +
1/G), so that G = f −1 (z). Integrating the inverse of a function requires only integration of the function itself (Laisant,
1905),
Z
f −1 (z)dz = zf −1 (z) − F ◦ f −1 (z) + C ,

(S1)

where F is the antiderivative of f . This relation gives,
 i
1 h
(S2)
α(, φ) = 1 − Im GH (0)2 + log GH (0) − log 1 − φGH (0) /φ .
π
An explicit representation of GH (0) and thus α(, φ) is possible by solving the cubic equation in eqn. (18). The full result
is very long and unenlightening, but we find that for small α,


  − c 3/2
1

, c =
(1 − 20φ − 8φ2 + (1 + 8φ)3/2 ) ,
(S3)
α(, φ) ≈ α0 (φ) 

c
16
where c is the critical value of  below which all critical points are minimizers.

2. On the assumption that the weights are I.I.D. random normal variables
2500

16
14

2000

Hist ogram

Hist ogram

12
1500

1000

10
8
6
4

500
2
0
− 0.3

− 0.2

− 0.1

0.0

Weight Values

0.1

0.2

0.3

0
− 1.5

− 1.0

− 0.5

0.0

0.5

1.0

1.5

Weight Values

Figure S1. Histogram of weight matrix entries (W (1) left, W (2) right) after training a 50 hidden-unit single-layer ReLU network on a
subset of 500 grayscaled CIFAR-10 images. Left, right histograms have a total of 51,200 and 500 entries.

3. Spectral density of H1 for single-hidden-layer ReLU networks
From (Dupic & Castillo, 2014) and referring to eqn. (27), the density can be written as
α2 |λ|  α2 λ2 α 
α 
ρH1 (λ) = 1 − min 1,
δ(λ) +
ρc
,
,
2
2
2 2
where 1/α = φ/2 = n/m,
√

3
√
r+ − r− 1x∈[θ(1−α)x− ,x+ ] ,
ρc (x, α) =
3
6πx 2
and,
q
p
3
r± =
9(2 + α)(x − ξ0 ) ± 6 3(x − x− )x(x+ − x) ,
√
8 + 20α − α2 ± α(8 + α)3/2
x± =
,
8
2(−1 + α)3
ξ0 = −
.
9(2 + α)

(S4)

(S5)

(S6)
(S7)
(S8)

Geometry of Neural Networks

4. Free independence and the evolution of eigenvalues over training
We plot the eigenvalues of the Hessian H0 + H1 and the transformed Hessian H0 + QH1 QT as the parameters evolve over
training. The training set is CIFAR-10 downsampled to 4x4 images, grayscaled and whitened. We train a 16-20-16 ReLU
autoencoding network without biases on the first 150 images of the dataset using full-batch gradient descent with learning
rate 0.05. The parameters are initialized as zero-mean Gaussians with variance 2 over the number of incoming units.
step = 0

step = 1

0.6

0.5

step = 2
0.6

0.5

0.4

0.4

0.4

ρ(λ)

0.3
0.3

0.2
0.1
5

10

0.3

0.2

0.2

0.1

0.1
2

-2

4

6

λ

8

2

10

step = 8

step = 16

0.4

ρ(λ)

ρ(λ)

ρ(λ)

10

0.8

0.4
0.2

2

4

6

8

0.6
0.4
0.2

2

10

4

6

8

10

2

4

λ

λ
step = 32
1.4

1.0

1.2

step = 128

1.0

ρ(λ)

0.8
0.6

0.4

0.5

0.4

0.2

8

1.5

1.0

0.6

6

λ

step = 64

1.2

0.8

0.2
2

4

6

8

2

4

6

8

2

λ

λ

step = 256

step = 512

step = 1024

1.5

1.5

1.5

1.0

1.0

1.0

0.5

0.5

2

4

6

8

2

4

6

8

2

λ
step = 4096

step = 2048

1.0

1.0

1.0

0.5

0.5

8

ρ(λ)

1.5

6

4

6

8

step = 8192

1.5

λ

8

λ

1.5

4

6

0.5

λ

2

4

λ

ρ(λ)

ρ(λ)

8

0.6

0.2

ρ(λ)

6

1.0

0.8
0.6

ρ(λ)

4

λ

λ

step = 4

0.8

H = H0 + H1
H = H0 + QH1QT

0.5

0.5

2

4

6

8

λ

Figure S2. Evolution of the eigenvalues of the Hessian over training.

2

4

λ

6

8

