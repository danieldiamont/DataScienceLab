Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

Supplementary Material

S1. Adaptive Decoding
Limited training data in practical settings can limit the inferential accuracy of the learned autoencoder model, and we
may have x0 ‰ DpEpx0 qq for a given to-be-revised sequence x0 (particularly if pX px0 q is low). In this case, even when
z ˚ “ Epx0 q solves our latent-factor optimization, our R EVISE procedure can return a different sequence than x0 (despite
not expecting any associated outcome-improvement).
To ensure that our methods simply return the initial x0 when no superior revision can be identified,
we replace¯ our decoder
´
px0 q
px q
model pD px | zq with an adaptive variant pDx0 px | zq that is efficiently defined once x0 “ s1 , . . . , sTx0 is specified
0
at test time. Like before, we write Dx0 pzq to denote the (beam-search approximated) most-likely decoding with respect to
pDx0 . Recall from our definition in (4), ⇡t is the vector of symbol-probabilities output by our decoder RNN D to compute
px0 q

pD . Using the indexing notation ⇡t rst s to denote the decoder RNN’s approximation of ppst , | s1 . . . , st´1 q, we let ⇡t
denote particular conditional-probability values output by D when the initial hidden state is z “ Epx0 q.
For any x “ ps1 , . . . , sT q P X , we define:

#
⇡t rss `
⇡
rt rst s where for t “ 1, . . . , T, s P S : ⇡
rt rss “
pDx0 px | zq “
⇡t rss ´
t“1
T
π

and

px0 q
t

px0 q

“ max ⇡t
sPS

px0 q

rss ´ ⇡t

px0 q

rst

px0 q
t
1 px0 q
|S| t

px q

if s “ st 0
otherwise

(15)

s • 0 for t “ 1, . . . , Tx0

px q

At each time step, the t 0 measure any probability gap between the most likely symbol under pD and the actual sequence
x0 when our decoder model D is applied to Epx0 q. Thus, the definition in (15) ensures Dx0 pEpx0 qq “ x0 . When revising
px q
sequences using this adaptive decoding procedure, we compute all t 0 by first decoding from Epx0 q before beginning
the latent z-optimization in the R EVISE procedure. These values are stored so that we can subsequently decode from the
optimal latent-configuration z ˚ with respect to pDx0 rather than pD .
According to our adaptive decoding definition, x0 is more likely than any other sequence under pDx0 px | Epx0 qq, and pDx0
is very easy to derive from pD (no additional model besides our original D is needed). Furthermore, the (beam-search)
maximizer of pDx0 can be used to decode from any latent z values, resulting in a mapping that is slightly more biased
toward x0 than decoding with respect to pD . Finally, we note that if x˚ is produced by Dx0 rather than D, Theorem 3
continues to hold if we replace D with Dx0 in assumption (A6). Theorems 1 and 2 remain valid without any change, since:
pDx0 px˚ | z ˚ q • pDx0 px0 | z ˚ q and pDx0 px0 | z ˚ q ´ pD px0 | z ˚ q • pDx0 px˚ | z ˚ q ´ pD px˚ | z ˚ q
together imply that pD px˚ | z ˚ q • pD px0 | z ˚ q, as required for expression (16) in our original proofs.

Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

S2. Experiment Details and Additional Results
Automatic differentiation in TensorFlow is used to obtain gradients for both our revision procedure and the (stochastic)
learning of neural network parameters. Throughout our applications, the GRU input is a vector-representation of each
symbol in the sequence, taken from a dictionary of embeddings that is learned jointly with the neural network parameters
via the Adam optimization algorithm of Kingma & Ba (2015). To ensure the decoder can actually generate variable-length
sequences, a special †End° symbol is always included in S and appended at the end of each sequence in the training data.
Note that all ↵-values stated in the text were actually first rescaled by p2⇡q´d{2 before the R EVISE procedure (to avoid
confounding from the choice of latent-dimensionality d in the relationship between the listed ↵ and characteristics of the
resulting revisions).
S2.1. Simulation Study
When sampling a sequence for this simulation, we first draw its length uniformly from the range [10,20], and subsequently
draw the symbols at each position following the probabilistic grammar of Table S1. Before its quality is evaluated, any
proposed sequence whose length violates the [10,20] range is either truncated or extended via repeated duplication of the
last symbol. In all models we apply, the encoder/decoder GRUs operate on input-embeddings of size 8, and the outcomeprediction model F is a feedforward network with one tanh hidden layer of size 128.
Rule
st “ A | st´1 “ C
st “ B | st´1 “ A
st “ D | st´3 “ D
st “ E | st´5 “ E
st “ J | st´2 “ H, st´1 “ I
st “ I | st´2 “ I, st´1 “ H
st “ B | st´3 “ B, st´2 “ C
st “ F | st´1 “ F, t • 11
s7 “ G | s6 “ F
s8 “ G | s7 “ F
s5 “ C
s10 “ C
s15 “ C
s20 “ C

Probability
0.50
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.95
0.50
0.50
0.50
0.50
0.50

Table S1. Probabilistic grammar used to generate sequences ps1 , . . . , sT q in our simulation. All events not listed here are assumed to
occur randomly (uniformly among the remaining probability mass). When one or more conditioning statements are valid at a given t,
we renormalize the probabilities for st | s1 , . . . , st´1 before sampling the next character.

In the S EARCH procedure, evaluating 100 candidates took similar computation time as a typical run of our R EVISE algorithm. Note that in this small scale simulation study, S EARCH is able to examine a nontrivial subset of the possible
sequences around x0 . However, exponentially more randomly generated revisions would be needed to retain the performance of this S EARCH approach under longer sequences with larger vocabularies, whereas the computational complexity
of our R EVISE procedure scales linearly with such increases. Whereas the S EARCH method changes nearly every given
initial sequence by a relatively similar amount, our R EVISE procedure tends to either make larger changes or no change
at all. As is desirable, our approach (particularly with adaptive decoding) tends to favor no change for x0 where the corresponding latent posterior has high uncertainty, both because the VAE training objective urges all decodings in a large
region around Epx0 q to heavily favor x0 and the invariance term Linv encourages F to be more flat in such regions.
S2.2. Improving Sentence Positivity
For simplicity, our analysis of the beer reviews only considers sentences that are short (§ 30 words) and entirely composed
of words that appear in • 100 other sentences. This restricts the size of the vocabulary to |S| « 5, 500. In this analysis,
the S EARCH procedure is allowed to score 1000 candidate sequences, which is now far slower than our R EVISE algorithm.
In our models, GRUs E and D employ an embedding layer of size 128, the latent representations (and GRU hidden states
ht ) have d “ 256 dimensions, and F is feedforward network with one hidden layer of the same size (and tanh activations)

Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

log p X (x*)
6

5

4

3

2

− 10 − 10 − 10 − 10 − 10 −10 −1
log α

10

−50

−40

−30

− log p X (x 0)

−20

2.0
1.0

●
●

0.0

d (x 0, x*)

−60 −50 −40 −30 −20

− log p X (x*)

∆Y (x*)

(C)
3.0

(B)
Latent (Euclidean) Distance

(A)

●

0

2

4

6

8

10

12

Edit Distance

Figure S1. Behavior of the R EVISE procedure in our simulation study. (A) Relationship between ↵ and properties of revised sequence
(averaged over same 1000 initial sequences x0 „ pX , with units rescaled so that all curves share the same range): outcome improvement
(black), edit distance (blue), marginal log-likelihood (red). (B) Likelihood of each original sequences vs. its revised version, when
log ↵ “ ´10000. The diagonal red line depicts the identity relationship y “ x. (C) Boxplot of ||z ˚ ´ Epx0 q||2 values for each
resulting value of dpx0 , x˚ q observed when log ↵ “ ´10000. Note there were very few revisions where dpx0 , x˚ q ° 8.

followed by a sigmoid output layer. The language model L shares the same GRU architecture as our decoder network D .
Examining the R EVISE output, we find that punctuation patterns are quite often perfectly preserved in revisions (this is
interesting since all punctuation characters are simply treated as elements of the vocabulary in the sequences). There exist
many initialization-points where if unconstrained gradient ascent is run for a vast number of iterations with a large step-size,
the resulting decoding produces the sentence: “excellent excellent excellent excellent excellent excellent excellent.”, which
is has near-optimal VADER sentiment but low marginal likelihood. Starting from other z-initializations, the decoding
which results from a massive shift in the latent space often reverts to repetitions of a safe choice where each decoded word
has high marginal likelihood, such as: “the the a the the the a the” or “tasting tasting tasting tasting tasting tasting tasting ”.
S2.3. Revising Modern Text in the Language of Shakespeare
Sentences used in this analysis were taken either from the concatenated works of Shakespeare (Karpathy, 2015) or from
various more contemporary texts (non-Shakespeare-authored works from the Brown, Reuters, Gutenberg, and FrameNet
corpora in Python’s NLTK library (Bird et al., 2009)). Here, we use the same architecture for networks F , E, D as in the
previous beer-reviews application.

Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

Model

Sentence

x0
log ↵ “ ´10000
A DAPTIVE
log ↵ “ ´1
inv “ pri “ 0
S EARCH

caramel, fruit, sweetness, and a soft floral bitterness.
caramel, fresh, sweetness, quite soft and a good bitterness.
caramel, fresh, sweetness, quite soft and a good bitterness.
caramel, fruit sweetness, and a soft floral nose.
caramel, fruit sweetness, and a soft floral and tangy nose.
caramel, fruit sweetness, and a soft floral, cocoa.

x0
log ↵ “ ´10000
A DAPTIVE
log ↵ “ ´1
inv “ pri “ 0
S EARCH

Y

px˚ q

L px

˚

q

dpx˚ , x0 q

+1.88
+1.88
+1.17
+1.17
+ 1.17

-5.1
-5.1
+0.2
-16.4
-7.0

6
6
1
3
2

i like to support san diego beers.
i love to support craft beers!
i like to support san diego beers.
i like to support craft beers!
i like to support you know.
i like to super support san diego.

+0.5
0
+0.1
0
+0.7

+1.6
0
+2.6
+3.7
-2.9

4
0
3
3
2

x0
log ↵ “ ´10000
A DAPTIVE
log ↵ “ ´1
inv “ pri “ 0
S EARCH

good carbonation makes for a smooth drinking experience.
good carbonation makes a great smooth drinking stuff.
good carbonation makes a great smooth drinking stuff.
good carbonation makes for great smooth drinking.
good carbonation makes for a smooth drinking like experience.
good carbonation makes for a drinking nice experience!

+1.1
+1.1
+ 1.1
+0.7
+0.9

-1.1
-1.1
+3.0
-9.2
-4.1

3
3
2
1
3

x0
log ↵ “ ´10000
A DAPTIVE
log ↵ “ ´1
inv “ pri “ 0
S EARCH

i’m not sure how old the bottle is.
i definitely enjoy how old is the bottle is.
i definitely enjoy how old is the bottle is.
i’m sure not sure how old the bottle is.
i’m sure better is the highlights when cheers.
i ’m not sure how the bottle is love.

+3.0
+3.0
+2.5
+3.3
+2.3

-3.6
-3.6
-6.8
-9.2
-3.3

4
4
1
6
2

x0
log ↵ “ ´10000
A DAPTIVE
log ↵ “ ´1
inv “ pri “ 0
S EARCH

what a great afternoon!
what a great afternoon!
what a great afternoon!
what a great afternoon!
what a great afternoon lace!
what a solid great!

0
0
0
0
+0.19

0
0
0
-8.2
-7.1

0
0
0
1
2

x0
log ↵ “ ´10000
A DAPTIVE
log ↵ “ ´1
inv “ pri “ 0
S EARCH

the finish is a nice hoppy bitter, with ample spice.
the finish is a nice hoppy plant, with ample spice and great mouthfeel.
the finish is a nice hoppy plant, with ample spice.
the finish is a nice hoppy plant, with ample spice.
the finish is a nice hoppy bitter, with ample spice.
the finish is a nice hoppy bitter best, with ample spice.

+2.5
+1.3
+1.3
0
+2.0

-6.4
-0.8
-0.8
0
-7.9

4
1
1
0
1

Table S2. Additional examples of held-out beer reviews x0 (in bold) revised to improve their VADER sentiment. Underneath each sentence, we show the revision produced by each different method along with the true outcome improvement
˚
˚
Y px q “ ErY | X “ x s ´ ErY | X “ x0 s (rescaled by the standard deviation of outcomes in the training data), estimated change
in marginal likelihood L px˚ q “ log Lpx˚ q ´ log Lpx0 q, and Levenshtein (edit) distance dpx˚ , x0 q.

Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

# Steps

Sentence

x0
100
1000
5000
10000
x˚

you find the evidence of that in the chart on this page.
you find the evidence of that in the chart on this page.
you find the chart of action in this page.
you find the chart of the chart that page of action in this page.
find you in this page of the way of your highness.
you speak of the chart in this page of the lord.

x0
100
1000
5000
10000
x˚

somewhere, somebody is bound to love us.
somewhere, somebody is bound to love us.
courage, honey, somebody is bound to love us!
courage man; ’tis love that is lost to us.
thou, within courage to brush and such us brush.
courage man; somebody is bound to love us.

x0
100
1000
5000
10000
x˚

the story of the fatal crash is not fully known
the story of the injured is not known.
the story of our virtue is not yet known.
the story of our virtue is not given me yet.
the virtue of our story is not yet.
the story of our virtue is not yet known.

x0
100
1000
5000
10000
x˚

this is the root issue for which the united states should stand.
this is the root issue which is an issue on the united states.
the root issue is that the dialogue itself should stand provided.
the general is for the root chief held for which is thy tale.
this the shallow is sworn thee. shallow for thee.
the root issue is the national dialogue from thine.

x0
100
1000
5000
10000
x˚

there is no such magic in man-made laws.
there is no such magic of man in such magic.
there is no magic of man in such magic.
there is no magic question with such a man in man.
there is no magic in revolution and made no such india.
there is no magic in such noble birth;

x0
100
1000
5000
10000
x˚

check the quality of the water.
check the quality of the water.
check the quality of thy water.
check the quality of thy quality.
check the king of gloucester.
check the quality of thy water.

x0
100
1000
5000
10000
x˚

what are you doing here?
what are you doing here?
what are you doing here?
cardinal what does thou live here?
cardinal what does thou live here?
does thou live here?

Table S3. Adaptive decoding from various latent Z configurations encountered at the indicated number of (unconstrained) gradient steps
from Epx0 q, for the model trained to distinguish sentences from Shakespeare vs. contemporary authors. Shown first and last are the
initial sequence x0 and the revision x˚ returned by our R EVISION procedure (constrained with log ↵ “ ´10000).

Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

S3. Proofs and Auxiliary Lemmas
Proof of Theorem 1.
By the definition of x˚ , we have:
pD px˚ | z ˚ q • pD px0 | z ˚ q

(16)

˚

ppz | x0 q
¨ pX px0 q
ppz ˚ | x˚ q
qE pz ˚ | x0 q
•
¨ pX px0 q with probability • 1 ´
ppz ˚ | x˚ q

ùñ pX px˚ q •

by Bayes’ rule

by assumptions (A1) and (A2) combined via the union bound. Finally, from the definitions in R EVISE, we have that
z ˚ P Cx0 , which implies qE pz ˚ | x˚ q • ↵.
Lemma 1. If (A1) holds, then for z ˚ defined in R EVISE: z ˚ P BR p0q with probability • 1 ´

2

(over x0 „ pX ).

Proof. Recall that BR p0q is defined as the Euclidean ball of radius R centered around 0. We show:
||z ˚ ´ Epx0 q|| §
and with probability • 1 ´ 2 :

||Epx0 q|| §

1
R
2

(17)

1
R
2

(18)

Subsequently, the triangle inequality completes the proof.
To prove (17), we recall that from our definition in (3): qE pz | x0 q is a Gaussian distribution with mean Epx0 q and diagonal
covariance ⌃z|x where each entry is § 1. Furthermore, the definitions in R EVISE ensure z ˚ P Cx0 ùñ qpz ˚ | x0 q • ↵.
Defining K “ ´2 logrp2⇡qd{2 |⌃z|x |1{2 ↵s which specifies the level-↵ isocontour of the N p0, ⌃z|x q density, we have:
qpz ˚ | Epx0 q • ↵

˚
ùñ pz ˚ ´ Epx0 qqT ⌃´1
z|x pz ´ Epx0 qq § K
b
1
ùñ ||z ˚ ´ Epx0 q|| § K ¨ max p⌃z|x q § R1
2
where max p⌃z|x q is the largest eigenvalue of ⌃z|x and

max p⌃z|x q

§ 1, |⌃z|x |1{2 § 1 for our qE pz | xq.

Now, define R “ tx P X : Epxq ° 12 Ru, and let Zr „ qZ as defined in (10). To prove (18), we note that for all x P R:
qE pz | xq is a diagonal Gaussian distribution centered around Epxq which has norm ° R{2. Thus:
4

¨ pX pRq †

ÿª

qE pz | xq dz ppxq “

1
xPR ||z||• 2 R

ˆ
˙
1
§ Pr ||Z|| • R
2
ˆ
˙
1
§ Pr ||Z|| • R2
2

ˆ
˙
1
r
¨ Pr ||Z|| • R
2

by the second condition in (A1)
as we defined R • R2

Since Z „ N p0, Iq under our prior, ||Z||2 „ 2d .
Applying the Chernoff bound to the tail of the 2 distribution (Dasgupta & Gupta, 2002), we thus obtain:
ˆ
˙ „
ˆ
˙⇢d{2 „
ˆ
˙⇢d{2
1
1
1
1
Pr ||Z||2 • R22 § R22 ¨ exp 1 ´ R22
§ exp 1 ´ R22
4
4
4
16
which implies pX pRq † {2 by our definition of R2 .

Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

Proof of Theorem 2.
For ✏ P p0, 1s, let B✏ pzq denote the ✏-ball centered at z. We have:
ª
pX px˚ q “ pD px˚ | zqpZ pzq dz
• PrpZ P B✏ pz ˚ qq rpD px˚ | z ˚ q ´ L✏s

assuming z ˚ P BR p0q, which occurs with probability • 1 ´ {2 by Lemma 1

• PrpZ P B✏ pz ˚ qq rpD px0 | z ˚ q ´ L✏s
„
⇢
ppz ˚ | x0 q
“ PrpZ P B✏ pz ˚ qq
p
px
q
´
L✏
X
0
pZ pz ˚ q
„
⇢
qE pz ˚ | x0 q
• PrpZ P B✏ pz ˚ qq
p
px
q
´
L✏
X
0
pZ pz ˚ q

by (16)

assuming z ˚ P BR p0q and x0 satisfies the (A1) inequality, which occurs with probability • 1 ´ by the union bound

PrpZ P B✏ pz ˚ qq
r ↵pX px0 q ´ L✏s
pZ pz ˚ q
min pZ pz ˚ ` q
|| ||“✏
•
VolpB✏ pz ˚ qq r ↵pX px0 q ´ L✏s
pZ pz ˚ q
ˆ
˙
‰
1“ ˚
2
• exp ´ ||z ||✏ ` ✏
VolpB✏ pz ˚ qq r ↵pX px0 q ´ L✏s
2
•

since pZ pz ˚ q † 1 and z ˚ P Cx0 ùñ qE pz ˚ | x0 q • ↵
where Volp¨q denotes the Lebesgue measure

by exploiting the fact that pZ “ N p0, Iq and subsequent application of the Cauchy-Schwarz inequality
ˆ
˙
||z ˚ || ` 1
• exp ´
¨ VolpB✏ pz ˚ qq ¨ r ↵pX px0 q ´ L✏s
for any ✏ P p0, 1s
2
ˆ
˙
R`1
• exp ´
¨ VolpB✏ pz ˚ qq ¨ r ↵pX px0 q ´ L✏s
since we already assumed z ˚ P BR p0q.
2
We conclude the proof by selecting ✏ “

↵pd`1q
Lpd`2q pX px0 q

which maximizes the lower bound given above.

Proof of Theorem 3.
Suppose for x0 P R, the corresponding revision x˚ R E. Then:
PrpX P E

X Rq § 1 ´ pX px˚ q ´ PrpX P EzRq
§ 1 ´  ´ PrpX P EzRq

Since (A5) implies PrpX P E C q † , we also have:
PrpX P E

X Rq “ 1 ´ PrpX P E C q ´ PrpX P EzRq
° 1 ´  ´ PrpX P EzRq

which is a contradiction. Thus, we must have x˚ P E if x0 P R, which occurs with probability • 1 ´ {2.

Lemma 1 ensures that under (A1): z ˚ P BR p0q with probability • 1 ´ {2, implying |F pz ˚ q ´ F pEpDpz ˚ qqq| § ✏inv with
the same probability. Consequently, we have:
F pz ˚ qq ´ F pEpx0 qq § F pEpDpz ˚ qq ´ F pEpx0 qq ` ✏inv
˚

§ F pEpx qq ´ ErY | X “ x0 s ` ✏inv ` ✏mse

§ ErY | X “ x˚ s ´ ErY | X “ x0 s ` ✏inv ` 2✏mse

with probability • 1 ´

with probability • 1 ´

The inequality in the other direction is proved via similar reasoning.

2

with probability • 1 ´
2

2

´  by the union bound

´´

2

by the union bound

Sequence to Better Sequence: Continuous Revision of Combinatorial Structures

Additional References for the Supplementary Material
Bird, S., Klein, E., and Loper, E. Natural Language Processing with Python. O’Reilly Media, 2009.
Dasgupta, S. D. A. and Gupta, A. K. An elementary proof of a theorem of johnson and lindenstrauss. Random Structures
and Algorithms, 22:60–65, 2002.
Karpathy, A. The unreasonable effectiveness of recurrent neural networks. Andrej Karpathy blog, 2015. URL karpathy.
github.io.
Kingma, D. and Ba, J. Adam: A method for stochastic optimization. International Conference on Learning Representations, 2015.

