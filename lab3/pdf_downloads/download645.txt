Unifying Task Specification in Reinforcement Learning

Martha White 1

Abstract
Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task
specification, as well as obfuscate connections
between different task settings, such as episodic
and continuing. In this work, we introduce the
RL task formalism, that provides a unification
through simple constructs including a generalization to transition-based discounting. Through a
series of examples, we demonstrate the generality
and utility of this formalism. Finally, we extend
standard learning constructs, including Bellman
operators, and extend some seminal theoretical
results, including approximation errors bounds.
Overall, we provide a well-understood and sound
formalism on which to build theoretical results
and simplify algorithm use and development.

1. Introduction
Reinforcement learning is a formalism for trial-and-error
interaction between an agent and an unknown environment.
This interaction is typically specified by a Markov decision
process (MDP), which contains a transition model, reward
model, and potentially discount parameters γ specifying a
discount on the sum of future values in the return. Domains
are typically separated into two cases: episodic problems
(finite horizon) and continuing problems (infinite horizon).
In episodic problems, the agent reaches some terminal state,
and is teleported back to a start state. In continuing problems, the agent interaction is continual, with a discount to
ensure a finite total reward (e.g., constant γ < 1).
This formalism has a long and successful tradition, but is
limited in the problems that can be specified. Progressively
there have been additions to specify a broader range of ob1
Department of Computer Science, Indiana University. Correspondence to: Martha White <martha@indiana.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

jectives, including options (Sutton et al., 1999), state-based
discounting (Sutton, 1995; Sutton et al., 2011) and interest functions (Reza and Sutton, 2010; Sutton et al., 2016).
These generalizations have particularly been driven by offpolicy learning and the introduction of general value functions for Horde (Sutton et al., 2011; White, 2015), where
predictive knowledge can be encoded as more complex prediction and control tasks. Generalizations to problem specifications provide exciting learning opportunities, but can also
reduce clarity and complicate algorithm development and
theory. For example, options and general value functions
have significant overlap, but because of different terminology and formalization, the connections are not transparent.
Another example is the classic divide between episodic and
continuing problems, which typically require different convergence proofs (Bertsekas and Tsitsiklis, 1996; Tsitsiklis
and Van Roy, 1997; Sutton et al., 2009) and different algorithm specifications.
In this work, we propose a formalism for reinforcement
learning task specification that unifies many of these generalizations. The focus of the formalism is to separate the specification of the dynamics of the environment and the specification of the objective within that environment. Though
natural, this represents a significant change in the way tasks
are currently specified in reinforcement learning and has
important ramifications for simplifying implementation, algorithm development and theory. The paper consists of two
main contributions. First, we demonstrate the utility of this
formalism by showing unification of previous tasks specified in reinforcement learning, including options, general
value functions and episodic and continuing, and further
providing case studies of utility. We demonstrate how to
specify episodic and continuing tasks with only modifications to the discount function, without the addition of states
and modifications to the underlying Markov decision process. This enables a unification that significantly simplifies
implementation and easily generalizes theory to cover both
settings. Second, we prove novel contraction bounds on the
Bellman operator for these generalized RL tasks, and show
that previous bounds for both episodic and continuing tasks
are subsumed by this more general result. Overall, our goal
is to provide an RL task formalism that requires minimal
modifications to previous task specification, with significant
gains in simplicity and unification across common settings.

Unifying Task Specification in Reinforcement Learning

2. Generalized problem formulation
We assume the agent interacts with an environment formalized by a Markov decision process (MDP): (S, A, Pr)
where S is the set of states, n = |S|; A is the set of actions;
and Pr : S × A × S → [0, 1] is the transition probability
function where Pr(s, a, s0 ) is the probability of transitioning
from state s into state s0 when taking action a. A reinforcement learning task (RL task) is specified on top of these
transition dynamics, as the tuple (P, r, γ, i) where
1. P is a set of policies π : S × A → [0, 1];
2. the reward function r : S × A × S → R specifies
reward received from (s, a, s0 );
3. γ : S ×A×S → [0, 1] is a transition-based discount
function1 ;
4. i : S → [0, ∞) is an interest function that specifies the
user defined interest in a state.
Each task could have different reward functions within the
same environment. For example, in a navigation task within
an office, one agent could have the goal to navigate to the
kitchen and the other the conference room. For a reinforcement learning task, whether prediction or control, a set or
class of policies is typically considered. For prediction (policy evaluation), we often select one policy and evaluate its
long-term discounted reward. For control, where a policy is
learned, the set of policies may consist of all policies parameterized by weights that specify the action-value from states,
with the goal to find the weights that yield the optimal policy. For either prediction or control in an RL task, we often
evaluate the return of a policy: the cumulative discounted
reward obtained from following that policy


∞
i−1
X
Y

Gt =
γ(st+j , at+j , st+1+j ) Rt+1+i
i=0

j=0

Q−1
where j=0 γ(st+j , at+j , st+1+j ) := 1. Note that this
subsumes the setting with a constant discount γc ∈ [0, 1),
by using γ(s, a, s0 ) = γc for every s, a, s0 and so giving
Qi−1
i
0
j=0 γ(st+j , at+j , st+1+j ) = γc for i > 0 and γc = 1
for i = 0. As another example, the end of the episode,
γ(s, a, s0 ) = 0, making the product of these discounts
zero and so terminating the recursion. We further explain how transition-based discount enables specification of
episodic tasks and discuss the utility of the generalization to
transition-based discounting throughout this paper. Finally,
the interest function i specifies the degree of importance
1

We describe a further probabilistic generalization in Appendix
A; much of the treatment remains the same, but the notation becomes cumbersome and the utility obfuscated.

of each state for the task. For example, if an agent is only
interested in learning an optimal policy for a subset of the
environment, the interest function could be set to one for
those states and to zero otherwise.
We first explain the specification and use of such tasks, and
then define a generalized Bellman operator and resulting
algorithmic extensions and approximation bounds.
2.1. Unifying episodic and continuing specification
The RL task specification enables episodic and continuing
problems to be easily encoded with only modification to the
transition-based discount. Previous approaches, including
the absorbing state formulation (Sutton and Barto, 1998b)
and state-based discounting (Sutton, 1995; Reza and Sutton,
2010; Sutton et al., 2011)(van Hasselt, 2011, Section 2.1.1),
require special cases or modifications to the set of states
and underlying MDP, coupling task specification and the
dynamics of the environment.
We demonstrate how transition-based discounting seamlessly enables episodic or continuing tasks to be specified in
an MDP via a simple chain world. Consider the chain world
with three states s1 , s2 and s3 in Figure 1. The start state is
s1 and the two actions are right and left. The reward is -1 per
step, with termination occurring when taking action right
from state s3 , which causes a transition back to state s1 .
The discount is 1 for each step, unless specified otherwise.
The interest is set to 1 in all states, which is the typical case,
meaning performance from each state is equally important.
Figure 1a depicts the classical approach to specifying
episodic problems using an absorbing state, drawn as a
square. The agent reaches the goal—transitioning right from
state s3 —then forever stays in the absorbing state, receiving
a reward of zero. This encapsulates the definition of the
return, but does not allow the agent to start another episode.
In practice, when this absorbing state is reached, the agent
is “teleported" to a start state to start another episode. This
episodic interaction can instead be represented the same way
as a continuing problem, by specifying a transition-based
discount γ(s3 , right, s1 ) = 0. This defines the same return,
but now the agent simply transitions normally to a start state,
and no hypothetical states are added.
To further understand the equivalence, consider the updates
made by TD (see equation (3)). Assume linear function
approximation with feature function x : S → Rd , with
weights w ∈ Rd . When the agent takes action right from
s3 , the agent transitions from s3 to s1 with probability one
and so γt+1 = γ(s3 , right, s1 ) = 0. This correctly gives
δt = rt+1 + γt+1 x(s1 )> w − x(s3 )> w = rt+1 − x(s3 )> w
and correctly clears the eligibility trace for the next step
et+1 = λt+1 γt+1 et + x(s1 ) = x(s1 ).

Unifying Task Specification in Reinforcement Learning

The stationary distribution is also clearly equal to the original episodic task, since the absorbing state is not used in the
computation of the stationary distribution.
Another strategy is to still introduce hypothetical states, but
use state-based γ, as discussed in Figure 1c. Unlike absorbing states, the agent does not stay indefinitely in the
hypothetical state. When the agent goes right from s3 , it
transitions to hypothetical state s4 , and then transition deterministically to the start state s1 , with γs (s4 ) = 0. As before,
we get the correct update, because γt+1 = γs (s4 ) = 0. Because the stationary distribution has some non-zero probability in the hypothetical state s4 , we must set x(s4 ) = x(s1 )
(or x(s4 ) = 0). Otherwise, the value of the hypothetical
state will be learned, wasting function approximation resources and potentially modifying the approximation quality
of the value in other states. We could have tried state-based
discounting without adding an additional state s4 . However, this leads to incorrect value estimates, as depicted in
Figure 1d; the relationship between transition-based and
state-based is further discussed in Appendix B.1. Overall,
to keep the specification of the RL task and the MDP separate, transition-based discounting is necessary to enable the
unified specification of episodic and continuing tasks.
2.2. Options as RL tasks
The options framework (Sutton et al., 1999) generically
covers a wide range of settings, with discussion about macroactions, option models, interrupting options and intra-option
value learning. These concepts at the time merited their
own language, but with recent generalizations can be more
conveniently cast as RL subtasks.
Proposition 1. An option, defined as the tuple (Sutton et al.,
1999, Section 2) (π, β, I) with policy π : S × A → [0, 1],
termination function β : S → [0, 1] and an initiation set
I ⊂ S from which the option can be run, can be equivalently
cast as an RL task.
This proof is mainly definitional, but we state it as an
explicit proposition for clarity. The discount function
γ(s, a, s0 ) = 1 − β(s0 ) for all s, a, s0 specifies termination. The interest function, i(s) = 1 if s ∈ I and i(s) = 0
otherwise, focuses learning resources on the states of interest. If a value function for the policy is queried, it would
only make sense to query it from these states of interest.
If the policy for this option is optimized for this interest
function, the policy should only be run starting from s ∈ I,
as elsewhere will be poorly learned. The rewards for the RL
task correspond to the rewards associated with the MDP.
RL tasks generalize options, by generalizing termination
conditions to transition-based discounting and by providing
degrees of interest rather than binary interest. Further, the
policies associated with RL subtasks can be used as macro-

s1

s2

s3

s4

1

(a) Absorbing state formulation.
s1

s2

s3

γ(s3 , right, s1 ) = 0
(b) Transition-based termination, γ(s3 , right, s1 ) = 0.

γs (s4 ) = 0
s1

s2

s3

s4

1
(c) State-based termination with γs (s4 ) = 0.
s1

s2

s3

γs (s1 ) = 0 or γs (s3 ) = 0
(d) Incorrect state-based termination.

Figure 1: Three different ways to represent episodic problems as continuing problems. For (c), the state-based discount cannot represent the episodic chain problem without adding states. To see why, consider the two cases
for representing termination: γs (s1 ) = 0 or γs (s3 ) = 0.
For simplicity, assume that π(s, right) = 0.75 for all
states s ∈ {s1 , s2 , s3 } and transitions are deterministic. If
γs (s3 ) = 0, then the value for taking action right from s2 is
r(s2 , right, s3 ) + γs (s3 )vπ (s3 ) = −1 and the value for taking action right from s3 is r(s3 , right, s1 )+γs (s1 )vπ (s1 ) 6=
−1, which are both incorrect. If γs (s1 ) = 0, then the value
of taking action right from s3 is −1 + γs (s1 )vπ (s1 ) = −1,
which is correct. However, the value of taking action left
from s2 is −1 + γs (s1 )vπ (s1 ) = −1, which is incorrect.

actions, to specify a semi-Markov decision process (Sutton
et al., 1999, Theorem 1).
2.3. General value functions
In a similar spirit of abstraction as options, general value
functions were introduced for single predictive or goaloriented questions about the world (Sutton et al., 2011).
The idea is to encode predictive knowledge in the form
of value function predictions: with a collection or horde
of prediction demons, this constitutes knowledge (Sutton
et al., 2011; Modayil et al., 2014; White, 2015). The work
on Horde (Sutton et al., 2011) and nexting (Modayil et al.,
2014) provide numerous examples of the utility of the types
of questions that can be specified by general value functions,
and so by RL tasks, because general value functions can

Unifying Task Specification in Reinforcement Learning

naturally can be specified as an RL task.
The generalization to RL tasks provide additional benefits for predictive knowledge. The separation into underlying MDP dynamics and task specification is particularly
useful in off-policy learning, with the Horde formalism,
where many demons (value functions) are learned off-policy.
These demons share the underlying dynamics, and even feature representation, but have separate prediction and control
tasks; keeping these separate from the MDP is key for avoiding complications (see Appendix B.2). Transition-based
discounts, over state-based discounts, additionally enable
the prediction of a change, caused by transitioning between
states. Consider the taxi domain, described more fully in
Section 3, where the agent’s goal is to pick up and drop
off passengers in a grid world with walls. The taxi agent
may wish to predict the probability of hitting a wall, when
following a given policy. This can be encoded by setting
γ(s, a, s) = 0 if a movement action causes the agent to
remain in the same state, which occurs when trying to move
through a wall. In addition to episodic problems and hard termination, transition-based questions also enable soft termination for transitions. Hard termination uses γ(s, a, s0 ) = 0
and soft termination γ(s, a, s0 ) =  for some small positive
value . Soft terminations can be useful for incorporating
some of the value of a policy right after the soft termination.
If two policies are equivalent up to a transition, but have
very different returns after the transition, a soft termination
will reflect that difference. We empirically demonstrate the
utility of soft termination in the next section.

3. Demonstration in the taxi domain
To better ground this generalized formalism and provide
some intuition, we provide a demonstration of RL task specification. We explore different transition-based discounts in
the taxi domain (Dietterich, 2000; Diuk et al., 2008). The
goal of the agent is to take a passenger from a source platform to a destination platform, depicted in Figure 2. The
agent receives a reward of -1 on each step, except for successful pickup and drop-off, giving reward 0. We modify
the domain to include the orientation of the taxi, with additional cost for not continuing in the current orientation.
This encodes that turning right, left or going backwards are
more costly than going forwards, with additional negative
rewards of -0.05, -0.1 and -0.2 respectively. This additional
cost is further multiplied by a factor of 2 when there is a
passenger in the vehicle. For grid size g and the number
of pickup/dropoff locations l, the full state information is
a 5-tuple: (x position of taxi ∈ {1, . . . , g}, y position of
taxi ∈ {1, . . . , g}, location of passenger ∈ {1, . . . , l + 1},
location of destination ∈ {1, . . . , l}, orientation of car
∈ {N, E, S, W } ). The location of the passenger can be in
one of the pickup/drop-off locations, or in the taxi. Optimal

policies and value functions are computed iteratively, with
an extensive number of iterations.
Figure 2 illustrates three policies for one part of the taxi domain, obtained with three different discount functions. The
optimal policy is learned using a soft-termination, which
takes into consideration the importance of approaching the
passenger location with the right orientation to minimize
turns after picking up the passenger. A suboptimal policy is
in fact learned with hard termination, as the policy prefers to
greedily minimize turns to get to the passenger. For further
details, refer to the caption in Figure 2.
We also compare to a constant γ, which corresponds to
an average reward goal, as demonstrated in Equation (8).
The table in Figure 2(e) summarizes the results. Though in
theory it should in fact recognize the relative values of orientation before and after picking up a passenger, and obtain the
same solution as the soft-termination policy, in practice we
find that numerical imprecision actually causes a suboptimal
policy to be learned. Because most of the rewards are negative per step, small differences in orientation can be more
difficult to distinguish amongst for an infinite discounted
sum. This result actually suggests that having multiple subgoals, as one might have with RL subtasks, could enable
better chaining of decisions and local evaluation of the optimal action. The utility of learning with a smaller γc has
been previously described (Jiang et al., 2015), however, here
we further advocate that enabling γ that provides subtasks
is another strategy to improve learning.

4. Objectives and algorithms
With an intuition for the specification of problems as RL
tasks, we now turn to generalizing some key algorithmic
concepts to enable learning for RL tasks. We first generalize
the definition of the Bellman operator for the value function.
For a policy π : S × A → [0, 1], define Pπ , Pπ,γ ∈ Rn×n
and rπ , vπ ∈ Rn , indexed by states s, s0 ∈ S,
X

Pπ (s, s0 ) :=

π(s, a)Pr(s, a, s0 )

a∈A
0

Pπ,γ (s, s ) :=

X

π(s, a)Pr(s, a, s0 )γ(s, a, s0 )

a∈A

rπ (s) :=

X

π(s, a)

a∈A

vπ (s) := rπ (s) +

X

Pr(s, a, s0 )r(s, a, s0 )

s0 ∈S

X

Pπ,γ (s, s0 )vπ (s0 ).

s0 ∈S

where vπ (s) is the expected return, starting from a state
s ∈ S. To compute a value function that satisfies this recursion, we define a Bellman operator. The Bellman operator
has been generalized to include state-based discounting and
a state-based trace parameter2 (Sutton et al., 2016, Eq. 29).
2
A generalization to state-based trace parameters has been
considered (Sutton, 1995; Sutton and Barto, 1998b; Reza and

Unifying Task Specification in Reinforcement Learning

(b)

(a)
0

(c)

(e)

(d)

-1.2

-1.4

-1.4

-1.2

-1

-1

-1.2

-1

-1.2

2

-1

-1

3

-1

-1

4

-1.2

4

3

4

1

Car

0

1

2

3

4

3

4

3

T OTAL P ICKUP A DDED C OST
D ROPOFF
FOR T URNS

-1.4

-1.1

AND

T RANS -S OFT
T RANS -H ARD
S TATE - BASED
γc = 0.1
γc = 0.3
γc = 0.5
γc = 0.6
γc = 0.7
γc = 0.8
γc = 0.9
γc = 0.99

7.74 ± 0.03
7.73 ± 0.03
0.00 ± 0.00
0.00 ± 0.00
0.02 ± 0.01
0.04 ± 0.01
0.03 ± 0.01
7.12 ± 0.03
7.34 ± 0.03
3.52 ± 0.06
0.01 ± 0.01

5.54 ± 0.01
5.83 ± 0.01
18.8 ± 0.02
2.48 ± 0.01
2.49 ± 0.01
2.51 ± 0.01
2.49 ± 0.01
4.52 ± 0.01
4.62 ± 0.01
4.57 ± 0.02
2.45 ± 0.01

Figure 2: (a) The taxi domain, where the pickup/drop-off platforms are at (0,0), (0,4), (3,0) and (4,4). The Passenger P is at the source
platform (4,4), outlined in black. The Car starts in (2,3), with orientation E as indicated the arrow, needs to bring the passenger to
destination D platform at (3,0), outlined in blue. In (b) - (d), there are simulated trajectories for policies learned using hard and soft
termination.
(b) The optimal strategy, with γ(Car in source, Pickup, P in Car) = 0.1 and a discount 0.99 elsewhere. The sequence of taxi locations are
(3, 3), (3, 4), (4, 4), (4, 4) with Pickup action, (4, 3), (4, 2), (4, 1), (4, 0), (3, 0). Successful pickup and drop-off with total reward −7.7.
(c) For γ(Car in source, Pickup, P in Car) = 0, the agent does not learn the optimal strategy. The agent minimizes orientation cost to
the subgoal, not accounting for orientation after picking up the passenger. Consequently, it takes more left turns after pickup, resulting
in more total negative reward. The sequence of locations are (3, 3), (4, 3), (4, 4), (4, 4) with Pickup action, (3, 4), (3, 3), (3, 2), (3, 1),
(3, 0). Successful pickup and drop-off with total reward −8.
(d) For state-based γ(Car in source and P in Car) = 0, the agent remains around the source and does not complete a successful drop-off.
The sequence of locations are (3, 3), (4, 3), (4, 4), (4, 4) with Pickup action, (4, 3), (4, 4), (4, 3).... The agent enters the source and
pickups up the passenger. When it leaves to location (4,3), its value function indicates better value going to (4,4) because the negative
return will again be cutoff by γ(Car in source and P in Car) = 0, even without actually performing a pickup. Since the cost to get to the
destination is higher than the −2.6 return received from going back to (4, 4), the agent stays around (4, 4) indefinitely.
(e) Number of successful passenger pickup and dropoff, as well as additional cost incurred from turns, over 100 steps, with 5000 runs,
reported for a range of constant γc and the policies in Figure 2. Due to numerical imprecision, several constant discounts do not get close
enough to the passenger to pickup or drop-off. The state-based approach, that does not add additional states for termination, oscillates
after picking up the passenger, and so constantly gets negative reward.

We further generalize the definition to the transition-based
setting. The trace parameter λ : S × A × S → [0, 1] influences the fixed point and provides a modified (biased)
return, called the λ-return; this parameter is typically motivated as a bias-variance trade-off parameter (Kearns and
Singh, 2000). Because the focus of this work is on generalizing the discount, we opt for a simple constant λc in
the main body of the text; we provide generalizations to
transition-based trace parameters in the appendix.
The generalized Bellman operator T(λ) : Rn → Rn is
T(λ) v := rλπ + Pλπ v,
where

Pλπ
rλπ

∀v ∈ Rn

(1)

:= (I − λc Pπ,γ )

−1

Pπ,γ (1 − λc )

(2)

:= (I − λc Pπ,γ )

−1

rπ

To see why this is the definition of the Bellman operator,
we define the expected λ-return, vπ,λ ∈ Rn for a given
approximate value function, given by a vector v ∈ Rn .
X
vπ,λ (s) := rπ (s)+
Pπ,γ (s, s0 ) [(1−λc )v(s0 )+λc vπ,λ (s0 )]
s0 ∈S

= rπ (s) + (1 − λc )Pπ,γ (s, :)v + λc Pπ,γ (s, :)vπ,λ .
Sutton, 2010; Sutton et al., 2014; Yu, 2012).

Continuing the recursion, we obtain3
"∞
#
X
vπ,λ =
(λc Pπ,γ )i (rπ + (1 − λc )Pπ,γ v)
i=0

= (I − λc Pπ,γ )−1 (rπ + (1 − λc )Pπ,γ v) = T(λ) v
The fixed point for this formula satisfies T(λ) v = v for the
Bellman operator defined in Equation (1).
To see how this generalized Bellman operator modifies the
algorithms, we consider the extension to temporal difference algorithms. Many algorithms can be easily generalized by replacing γc or γs (st+1 ) with transition-based
γ(st , at , st+1 ). For example, the TD algorithm is generalized by setting the discount on each step to γt+1 =
γ(st , at , st+1 ),
wt+1 = wt + αt δt et

. for some step-size αt
>

δt := rt+1 + γt+1 x(st+1 ) w − x(st )> w

(3)

et = γt λc et−1 + x(st ).
3
with maximum eigenvalue less than 1,
P∞For ai matrix M −1
. We show in Lemma 3 that Pπ,γ satisi=0 M = (I − M)
fies this condition, implying λc Pπ,γ satisfies this condition and so
this infinite sum is well-defined.

Unifying Task Specification in Reinforcement Learning

The generalized TD fixed-point, under linear function approximation, can be expressed as a linear system Aw = b
A = X> D(I − λc Pπ,γ ) (I − Pπ,γ )X
−1

b = X> D(I − λc Pπ,γ ) rπ
−1

where each row in X ∈ Rn×d corresponds to features for
a state, and D ∈ Rn×n is a diagonal weighting matrix.
Typically, D = diag(dµ ), where dµ ∈ Rn is the stationary
distribution for the behavior policy µ : S × A → [0, 1]
generating the stream of interaction. In on-policy learning,
dµ = dπ . With the addition of the interest function, this
weighting changes to D = diag(dµ ◦ i), where ◦ denotes
element-wise product (Hadamard product). More recently,
a new algorithm, emphatic TD (ETD) (Mahmood et al.,
2015; Sutton et al., 2016) specified yet another weighting,
D = M where M = diag(m) with m = (I − Pλπ )−1 (dµ ◦
i). Importantly, even for off-policy sampling, with this
weighting, A is guaranteed to be positive definite. We show
in the next section that the generalized Bellman operator for
both the on-policy and emphasis weighting is a contraction,
and further in the appendix that the emphasis weighting
with a transition-based trace function is also a contraction.

√
For weighted norm kvkD = v> Dv, if we can take the
λ
square root of D,

 the induced matrix norm is kPπ kD =
D1/2 Pλπ D1/2  , where the spectral norm k·k is the
sp
sp
largest singular value of the matrix. For simplicity of notation below, define sD := kPλπ kD . For any diagonalizable,
nonnegative matrix D, the projection ΠD : V → Fv onto
Fv exists and is defined ΠD z = argminv∈Fv kz − vkD .
5.1. Approximation bound
We first prove that the generalized Bellman operator in
Equation 1 is a contraction. We extend the bound from
(Tsitsiklis and Van Roy, 1997; Hallak et al., 2015) for constant discount and constant trace parameter, to the general
transition-based setting. The normed difference to the true
value function could be defined by multiple weightings. A
well-known result is that for D = Dπ the Bellman operator is a contraction for constant γc and λc (Tsitsiklis and
Van Roy, 1997); recently, this has been generalized for a
variant of ETD to M, still with constant parameters (Hallak
et al., 2015). We extend this result for transition-based γ for
both Dπ and the transition-based emphasis matrix M.
Lemma 1. For D = Dπ or D = M,
sD = kPλπ kD < 1.

5. Generalized theoretical properties
In this section, we provide a general approach to incorporating transition-based discounting into approximation bounds.
Most previous bounds have assumed a constant discount.
For example, ETD was introduced with state-based γs ; however, (Hallak et al., 2015) analyzed approximation error
bounds of ETD using a constant discount γc . By using matrix norms on Pπ,γ , we generalize previous approximation
bounds to both the episodic and continuing case.

Proof: For D = M: let ξ ∈ Rn be the vector of row sums
for Pλπ : Pλπ 1 = ξ. Then for any v ∈ V, with v 6= 0,
!2
X
X
λ
2
λ
0
0
kPπ vkM =
m(s)
Pπ (s, s )v(s )

=

s∈S

s0 ∈S

X

2

m(s)ξ(s)

X Pλ (s, s0 )
π
v(s0 )
ξ(s)
0

s ∈S

s∈S

Define the set of bounded vectors for the general space
of value functions V = {v ∈ Rn : kvkDµ < ∞}. Let
Fv ⊂ V be a subspace of possible solutions, e.g., Fv =
{Xw|w ∈ Rd , kwk2 < ∞}.

X Pλ (s, s0 )
π
≤
v(s0 )2
m(s)ξ(s)2
ξ(s)
s∈S
s0 ∈S
X
X
0 2
=
v(s )
m(s)ξ(s)Pλπ (s, s0 )

A1. The action space A and state space S are finite.


= v diag (m ◦ ξ)> Pλπ v

A2. For polices µ, π : S × A → [0, 1], there exist unique
invariant distributions dµ , dπ such that dπ Pπ = dπ
and dµ Pµ = dµ . This assumption is typically satisfied
by assuming an ergodic Markov chain for the policy.
A3. There exist transition s, a, s0 such that γ(s, a, s0 ) < 1
and π(s, a)P (s, a, s0 ) > 0. This assumptions states
that the policy reaches some part of the space where
the discount is less than 1.
A4. Assume for any v ∈ Fv , if v(s) = 0 for all s ∈ S
where i(s) > 0, then v(s) = 0 for all s ∈ S s.t. i(s) =
0. For linear function approximation, this requires
F = span{x(s) : s ∈ S, i(s) 6= 0}.

!2

X

s0 ∈S
>

s∈S

where the first inequality follows from Jensen’s inequality,
because Pλπ (s, :) is normalized. Now because ξ has entries
that are less than 1, because the row sums of Pλπ are less
than 1 as shown in Lemma 4, and because each of the values
in the above product are nonnegative,

v> diag (m ◦ ξ)> Pλπ v

≤ v> diag m> Pλπ v

= v> diag m> (Pλπ − I) + m> v

= v> diag −(dπ ◦ i)> + m> v


= v> diag m> v − v> diag (dπ ◦ i)> v
< kvk2M

Unifying Task Specification in Reinforcement Learning

The last inequality is a strict inequality because dπ ◦ i has
at least one positive entry where v has a positive entry.
Otherwise, if v(s) = 0 everywhere with i(s) > 0, then
v = 0, which we assumed was not the case.
Therefore, kPλπ vkM < kvkM for any v 6= 0, giving
kPλ
π vkM
< 1. This exact same
kPλπ kM := maxv∈Rn ,v6=0 kvk
M
proof follows through verbatim for the generalization of Pλπ
to transition-based trace λ.
For D = Dπ : Again, we use Jensen’s inequality, but now
rely on the property dπ Pπ = dπ . Because of Assumption
A3, for some s < 1, for any non-negative v+ ,
XX
dπ Pπ,γ v+ =
dπ (s)Pr(s, a, :) ◦ γ(s, a, :)v+
s

≤s

where the last inequality follows from Lemma 1. By the
Banach Fixed Point theorem, because the Bellman operator
is a contraction under D, it has a unique fixed point.

Theorem 1. If D satisfies sD < 1, then there exists v ∈ Fv
such that ΠD T(λ) v = v, and the error to the true value
function is bounded as
kv − v∗ kD ≤ (1 − sD )−1 kΠD v∗ − v∗ kD .

For constant discount γc ∈ [0, 1) and constant trace parameter λc ∈ [0, 1], this bound reduces to the original bound
(Tsitsiklis and Van Roy, 1997, Lemma 6):
(1 − sD )−1 ≤

a

XX
s

dπ (s)Pr(s, a, :)v+ = sdπ v.

a

Therefore, because vectors Pπ,γ v+ are also non-negative,
!
∞
X
dπ Pλπ v+ = dπ
(Pπ,γ λc )k Pπ,γ (1 − λc ) v+
≤ (1 − λc )

1 − γc λc
.
1 − γc

Proof: Let v be the unique fixed point of ΠD T(λ) , which
exists by Lemma 2.
kv − v∗ kD ≤ kv − ΠD v∗ kD + kΠD v∗ − v∗ kD
= kΠT(λ) v − ΠD v∗ kD + kΠD v∗ − v∗ kD

k=0
∞
X

(4)

≤ kT(λ) v − v∗ kD + kΠD v∗ − v∗ kD

k

(sλc ) dπ Pπ,γ v+

= kT(λ) (v − v∗ )kD + kΠD v∗ − v∗ kD

k=0

= kPλπ (v − v∗ )kD + kΠD v∗ − v∗ kD

−1

≤ (1 − λc )(1 − sλc ) sdπ v+

= kPλπ kD kv − v∗ kD + kΠD v∗ − v∗ kD

and so
kPλπ vk2Dπ ≤

X

dπ (s)ξ(s)2

s ∈S

s∈S

=

X

0 2

v(s )

s0 ∈S

≤

X

X Pλ (s, s0 )
π
v(s0 )2
ξ(s)
0

X

dπ (s)ξ(s)Pλπ (s, s0 )

s∈S
0 2

v(s )

s0 ∈S

X

dπ (s)Pλπ (s, s0 )

s∈S

X

≤

s(1−λc )
1−λc s

≤

s0 ∈S
2
s−sλc
1−λc s kvkDπ

d(s0 )v(s0 )2

s−sλc
1−λc s

where
< 1 since s < 1.

Lemma 2. Under assumptions A1-A3, the Bellman operator T(λ) in Equation (1) is a contraction under a norm
weighted by D = Dπ or D = Mπ , i.e., for v ∈ V
kT(λ) vkD < kvkD .
Further, because the projection ΠD is a contraction,
ΠD T(λ) is also a contraction and has a unique fixed point
ΠD T(λ) v = v for v ∈ Fv .

= sD kv − v∗ kD + kΠD v∗ − v∗ kD
where the second inequality is due to kΠT(λ) vkD ≤
kT(λ) vkD , the second equality due to T(λ) v∗ = v∗ and
the third equality due to T(λ) v − T(λ) v∗ = Pλπ (v − v∗ )
because the rπ cancels. By rearranging terms, we get
(1 − sD )kv − v∗ kDπ ≤ kΠv∗ − v∗ kDπ
and since sD < 1, we get the final result.
For constant γc < 1 and λc , because Pπ,γ = γPπ
sD = kPλπ kD
1/2

= kD

∞
X

!
γci λic Piπ

i=0
∞
X

≤ γc (1 − λc )

γc (1 − λc )Pπ D1/2 k2

1/2
γci λic kD1/2 Pi+1
k2
π D

i=0

= γc (1 − λc )

∞
X

γci λic kPi+1
π kD

i=0

Proof: Because any vector v can be written v = v1 − v2 ,
kT(λ) vkD = kT(λ) (v1 − v2 )kD = kPλπ (v1 − v2 )kD
≤ kPλπ kD kvkD
< kvkD

≤ γc (1 − λc )

∞
X

γci λic

i=0

γc (1 − λc )
=
1 − γc λc



Unifying Task Specification in Reinforcement Learning

We provide generalizations to transition-based trace parameters in the appendix, for the emphasis weighting, and also
discuss issues with generalizing to state-based termination
for a standard weighting with dπ . We show that for any
transition-based discounting function λ : S×A×S → [0, 1],
the above contraction results hold under emphasis weighting. We then provide a general form for an upper bound
on kPλπ kDπ for transition-based discounting, based on the
contraction properties of two matrices within Pλπ . We further provide an example where the Bellman operator is
not a contraction even under the simpler generalization to
state-based discounting, and discuss the requirements for
the transition-based generalizations to ensure a contraction
with weighting dπ . This further motivates the emphasis
weighting as a more flexible scheme for convergence under general setting—both off-policy and transition-based
generalization.
5.2. Properties of TD algorithms
Using this characterization of Pλπ , we can re-examine previous results for temporal difference algorithms that either
used state-based or constant discounts.
Convergence of Emphatic TD for RL tasks. We can extend previous convergence results for ETD, for learning
value functions and action-value functions, for the RL task
formalism. For policy evaluation, ETD and ELSTD, the
least-squares version of ETD that uses the above defined
A and b with D = M, have both been shown to converge
with probability one (Yu, 2015). As an important component of this proof is convergence in expectation, which relies
on A being positive definite. In particular, for appropriate
step-sizes αt (see (Yu, 2015)), if A is positive definite, the
iterative update is convergent wt+1 = wt + αt (b − Awt ).
For the generalization to transition-based discounting, convergence in expectation extends for the emphatic algorithms.
We provide these details in the appendix for completeness,
with theorem statement and proof in Appendix F and pseudocode in Appendix D.
Convergence rate of LSTD(λ). Tagorti and Scherrer
(2015) recently provided convergence rates for LSTD(λ)
for continuing tasks, for some γc < 1. These results can be
extended to the episodic setting with the generic treatment
of Pλπ . For example, in (Tagorti and Scherrer, 2015, Lemma
1), which describes the sensitivity of LSTD, the proof extends by replacing the matrix (1 − λc )γc Pπ (I − λc γc Pπ )−1
(which they call M in their proof) with the generalization
1
Pλπ , resulting instead in the constant 1−s
in the bound
D
1−λc γc
rather than 1−γc . Further, this generalizes convergence
rate results to emphatic LSTD, since M satisfies the required convergence properties, with rates dictated by sM
rather than sDµ for standard LSTD.

Insights into sD . Though the generalized form enables
unified episodic and continuing results, the resulting bound
parameter sD is more difficult to interpret than for constant
c λc
γc , λc . With λc increasing to one, the constant 1−γ
1−γc in
the upper bound decreased to one. For γc decreasing to zero,
the bound also decreases to one. These trends are intuitive,
as the problem should be simpler when γc is small, and bias
should be less when λc is close to one. More generally,
however, the discount can be small or large for different
transitions, making it more difficult to intuit the trend.
To gain some intuition for sD , consider a random policy in
the taxi domain, with sD summarized in Table 1. As λc
goes to one, sD goes to zero and so (1 − sD )−1 goes to one.
Some outcomes of note are that 1) hard or soft termination
for the pickup results in the exact same sD ; 2) for a constant
gamma of γc = 0.99, the episodic discount had a slightly
smaller sD ; and 3) increasing λc has a much stronger effect
than including more terminations. Whereas, when we added
random terminations, so that from 1% and 10% of the states,
termination occurred on at least one path within 5 steps or
even more aggressively on every path within 5 steps, the
values of sD were similar.
λc

0.0

0.5

0.9

0.99

0.999

E PISODIC TAXI

0.989
0.990
0.989
0.987
0.978
0.898

0.979
0.980
0.978
0.975
0.956
0.815

0.903
0.908
0.898
0.887
0.813
0.468

0.483
0.497
0.467
0.439
0.304
0.081

0.086
0.090
0.086
0.086
0.042
0.009

γc = 0.99
1% SINGLE PATH
10% SINGLE PATH
1% ALL PATHS
10% ALL PATHS

Table 1: The sD values for increasing λc , with discount
settings described in the text.

6. Discussion and conclusion
The goal of this paper is to provide intuition and examples
of how to use the RL task formalism. Consequently, to avoid
jarring the explanation, technical contributions were not emphasized, and in some cases included only in the appendix.
For this reason, we would like to highlight and summarize the technical contributions, which include 1) the introduction of the RL task formalism, and of transition-based
discounts; 2) an explicit characterization of the relationship between state-based and transition-based discounting;
and 3) generalized approximation bounds, applying to both
episodic and continuing tasks; and 4) insights into—and
issues with—extending contraction results for both statebased and transition-based discounting. Through intuition
from simple examples and fundamental theoretical extensions, this work provides a relatively complete characterization of the RL task formalism, as a foundation for use in
practice and theory.

Unifying Task Specification in Reinforcement Learning

Acknowledgements
Thanks to Hado van Hasselt for helpful discussions about
transition-based discounting, and probabilistic discounts.

References
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-dynamic
programming. Athena Scientific Press, 1996.
Thomas G Dietterich. Hierarchical Reinforcement Learning
with the MAXQ Value Function Decomposition. Journal
of Artificial Intelligence Research, 2000.
Carlos Diuk, Andre Cohen, and Michael L Littman. An
object-oriented representation for efficient reinforcement
learning. In International Conference on Machine Learning, 2008.

Richard S Sutton, Hamid Maei Reza, Doina Precup, and
Shalab Bhatnagar. Fast gradient-descent methods for
temporal-difference learning with linear function approximation. International Conference on Machine Learning,
2009.
Richard S Sutton, Joseph Modayil, Michael Delp, Thomas
Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning
knowledge from unsupervised sensorimotor interaction.
In International Conference on Autonomous Agents and
Multiagent Systems, 2011.
Richard S Sutton, Ashique Rupam Mahmood, Doina Precup,
and Hado van Hasselt. A new Q(lambda) with interim forward view and Monte Carlo equivalence. In International
Conference on Machine Learning, 2014.

Assaf Hallak, Aviv Tamar, Rémi Munos, and Shie Mannor.
Generalized Emphatic Temporal Difference Learning:
Bias-Variance Analysis. CoRR abs/1509.05172, 2015.

Richard S Sutton, Ashique Rupam Mahmood, and Martha
White. An emphatic approach to the problem of off-policy
temporal-difference learning. The Journal of Machine
Learning Research, 2016.

Nan Jiang, Alex Kulesza, Satinder P Singh, and Richard L
Lewis. The Dependence of Effective Planning Horizon
on Model Accuracy. International Conference on Autonomous Agents and Multiagent Systems, 2015.

Manel Tagorti and Bruno Scherrer. On the Rate of Convergence and Error Bounds for LSTD(λ). In International
Conference on Machine Learning, 2015.

Michael J Kearns and Satinder P Singh. Bias-Variance
error bounds for temporal difference updates. In Annual
Conference on Learning Theory, 2000.
Ashique Rupam Mahmood, Huizhen Yu, Martha White, and
Richard S Sutton. Emphatic temporal-difference learning.
In European Workshop on Reinforcement Learning, 2015.
Joseph Modayil, Adam White, and Richard S Sutton. Multitimescale nexting in a reinforcement learning robot.
Adaptive Behavior - Animals, Animats, Software Agents,
Robots, Adaptive Systems, 2014.
Hamid Maei Reza and Richard S Sutton. GQ (λ): A general gradient algorithm for temporal-difference prediction
learning with eligibility traces. In AGI, 2010.
Richard S Sutton. TD models: Modeling the world at a
mixture of time scales. In International Conference on
Machine Learning, 1995.
Richard S Sutton and A G Barto. Introduction to reinforcement learning. MIT Press, 1998a.
Richard S Sutton and A G Barto. Reinforcement Learning:
An Introduction. MIT press, 1998b.
Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artificial intelligence, 1999.

Johnathan N Tsitsiklis and Benjamin Van Roy. An analysis
of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 1997.
Hado Philip van Hasselt. Insights in Reinforcement Learning. PhD thesis, Hado van Hasselt, 2011.
Harm van Seijen and Rich Sutton. True online TD(lambda).
In International Conference on Machine Learning, 2014.
Adam White. Developing a predictive approach to knowledge. PhD thesis, University of Alberta, 2015.
Huizhen Yu. Least Squares Temporal Difference Methods:
An Analysis under General Conditions. SIAM Journal on
Control and Optimization, 2012.
Huizhen Yu. On convergence of emphatic temporaldifference learning. In Annual Conference on Learning
Theory, 2015.

Unifying Task Specification in Reinforcement Learning

A. More general formulation with
probabilistic discounts
In the introduction of transition-based discounting, we could
have instead assumed that we had a more general probability model: Pr(r, γ|s, a, s0 ). Now, both the reward and discount are not just functions of states and action, but also are
stochastic. This generalization in fact, does not much alter
the treatment in this paper. This is because, when taking the
expectations for value function, the Bellman operator and
the A matrix, we are left again with γ(s, a, s0 ). To see why,
X
vπ (s) =
π(s, a)Pr(s, a, s0 )E[r + γvπ (s0 )|s, a, s0 ]
a,s0

=

X

π(s, a)Pr(s, a, s0 )E[r|s, a, s0 ]

a,s0

to assume x(fsas0 ) = x(s0 ) for fsas0 ∈ F.
Theorem 2. For a given transition-based MDP
(Pr, r, S, A, γ) and policy π, assume that the stationary distribution dπ exists. Define state-based MDP
(P̄r, r̄, S̄, A, γ̄s ) with extended π̄, all as above. Then the
stationary distribution d̄π for π̄ exists and satisfies
P

d̄π (s)
d̄π (i)

i∈S

= dπ (s).

(5)

∀s ∈ S, v̄π (s) = vπ (s) and π̄(s,
P a) = π(s, a) for all
s ∈ S, a ∈
A
with
π
=
argmin
π
s∈S dπ (s)vπ (s); π̄ =
P
argminπ i∈S̄ d̄π (s)v̄π (s)
B.2. Advantages of transition-based discounting over
state-based discounting

for γ(s, a, s0 ) = E[γ|s, a, s0 ].

Though the two have equal representational abilities, there
are several disadvantages of state-based discounting that
compound to make the more general transition-based discount strictly more desirable. The disadvantages of using an
induced state-based MDP, rather than the original transitionbased MDP, arises from the addition of states and include
the following.

B. Relationship between state-based and
transition-based discounting

Compactness. In the worst-case, for a transition-based
MDP with n states, the induced state-based MDP can have
|A|n2 + n states.

+

X

π(s, a)Pr(s, a, s0 )E[γ|s, a, s0 ]vπ (s0 )

a,s0

= rπ (s) +

X

Pπ,γ (s, s0 )vπ (s0 )

s0

In this section, we show that for any MDP with transitionbased discounting, we can construct an equivalent MDP
with state-based discounting. The MDPs are equivalent in
the sense that learned policies and value functions learned
in either MDP would have equal values when evaluated
on the states in the original transition-based MDP. This
equality ignores practicality of learning in the larger induced
state-based MDP, and at the end of this section, we discuss
advantages of the more compact transition-based MDP.
B.1. Equivalence result
The equivalence is obtained by introducing hypothetical
states for each transition. The key is then to prove that
the stationary distribution for the state-based MDP, with
additional hypothetical states, provides the same solution
even with function approximation. For each triplet s, a, s0 ,
add a new hypothetical state fsas0 , with set F comprised of
these additional states. Each transition now goes through
a hypothetical state, fsas0 , and allows the discount in the
hypothetical state to be set to γ(s, a, s0 ). The induced statebased MDP has state set S̄ = S ∪ F with |S̄| = |A|n2 + n.
We define the other models in the proof in Appendix B.3.
The choice of action in the hypothetical states is irrelevant.
To extend the policy π, we arbitrarily choose that the policy
uniformly selects actions when in the hypothetical states
and define π̄(s, a) = π(s, a) for s ∈ S and π̄(s, a) = 1/|A|
otherwise. For linear function approximation, we also need

Problem definition changes for different discounts. For
the same underlying MDP, multiple learners with different
discount functions would have different induced state-based
MDPs. This complicates code and reduces opportunities for
sharing variables and computation.
Overhead. Additional states must be stored, with additional
algorithmic updates in those non-states, or cases to avoid
these updates, and the need to carefully set features for
hypothetical states. This overhead is both computational as
well as conceptual, as it complicates the code.
Stationary distribution. This distribution superfluously
includes hypothetical states and requires renormalization to
obtain the stationary distribution for the original transitionbased MDP.
Off-policy learning. In off-policy learning, one goal is to
learn many value functions with different discounts (White,
2015). As mentioned above, these learners may have different induced state-based MDPs, which complicates implementation and even theory. To theoretically characterize a
set of off-policy learners, it would be necessary to consider
different induced state-based MDPs. Further, sharing information, such as the features, is again complicated by using
induced state-based MDP rather than a single transitionbased MDP, with varying discount functions.
Specification of algorithms. Often algorithms are introduced either for the episodic case (e.g., true-online TD (van

Unifying Task Specification in Reinforcement Learning

Seijen and Sutton, 2014)) or the continuing case (e.g., the
lower-variance version of ETD (Hallak et al., 2015)). When
kept separately, with explicit loops over episodes, the algorithm itself is different (e.g., Sarsa (Sutton and Barto, 1998a,
Figure 8.8)); or, if a state-based approach is used, fake states
and fake transitions would have to be explicitly added to
make the update the same for continuing or episodic. For
the generalized formulation, the only difference is the γt+1
that is passed to the algorithm; the algorithm itself remains
exactly the same in the two settings. As a minor example,
for episodic problems, there is typically an explicit (errorprone) step to clear traces; with generalized discounting, the
traces are automatically cleared at the end of an episode by
γt+1 .
Experimental design. When presenting results for episodic
and continuing problem, often the former uses number of
episodes and the later number of steps. In reality both simply
consist of a trajectory of information, with the former having
γt+1 = 0 for some steps. A unified view with number of
steps enables more consistent presentation of results across
domains. Related to this difference, a common but rarely
discussed decision when implementing episodic tasks is the
cut-off for the maximum number of steps in an episode. If
set too small, an algorithm that takes longer to reach the
goal in the first few episodes, but then learns more quickly
afterwards, could be unfairly penalized. Instead learning
could be limited to some maximum number of steps to
constrain learning time similarly for both continuing and
episodic problems, where multiple episodes could occur
within that maximum number of steps.

and state-based discount function γ̄s : S̄ → [0, 1]

γ(s, a, s0 ) i = fsas0
γ̄s (i) =
1
otherwise
Theorem 2 For a given transition-based MDP
(Pr, r, S, A, γ) and policy π, assume that the stationary distribution dπ exists. Define state-based MDP
(P̄r, r̄, S̄, A, γ̄s ) with extended π̄, all as above. Then the
stationary distribution d̄π for π̄ exists and satisfies
d̄π (s)
= dπ (s)
i∈S d̄π (i)

(6)

P

and for all s ∈ S, v̄π (s) = vπ (s).
2

2

(n+|A|n )×(n+|A|n )
Proof: Define
where
P matrix P̄π ∈ R
P̄π (i, j) = a∈A π̄(i, a)P̄r(i, a, j), giving

 π(i, a) Pr(i, a, s0 ) i ∈ S, j = fias0
1
i = fsaj , a ∈ A, j ∈ S
P̄π (i, j) =

0
otherwise

Define
d̄π (i) :=

1
c



dπ (i)
i∈S
dπ (s)π(s, a) Pr(s, a, s0 ) i = fsas0

where c > 0 is a normalizer to ensure that 1> d̄π = 1. Now
we need to show that d̄π P̄π = d̄π . For any j ∈ S,
d̄π P̄π (:, j) =


X
1 X
dπ (s)P̄π (s, j) +
d̄π (f )P̄π (f, j)
c
f ∈F

s∈S

B.3. Proof of Theorem 2
This prove illustrates the representability relationship between transition-based discounting and state-based discounting. This equivalence could be obtained more compactly if
γ(s, a, s0 ) is not different for every s0 ; however, the proof
becomes much more involved. Since our main goal is to simply show a representability result, we opt for interpretability.
Note that, in addition, the result in Theorem 2 fills a gap
in the previous theory, which indicated that state-based discounting could be used to represent episodic problems, but
did not explicitly demonstrate that the stationary distribution
would be equivalent (see (Yu, 2015)).
Define transition probabilities P̄r : S̄ × S̄ → [0, 1]

 Pr(i, a, s0 ) i ∈ S, j = fias0
1
j ∈ S, i = fsaj
P̄r(i, a, j) =

0
otherwise

Case 1: j ∈ S
For the first component, because P̄π (s, j) = 0 by definition
of P̄r, we get
X
dπ (s)P̄π (s, j) = 0
s∈S

For the second component, because P̄π (fsaj , j) = 1,
X
d̄π (fsas0 )P̄π (fsas0 , j)
fsas0 ∈F

=

X

=

X

=
=


r̄(i, a, j) =

r(i, a, s0 ) i ∈ S, j = fias0
0
otherwise

d̄π (fsaj )

fsaj ∈F

X

dπ (s)

s∈S

rewards

d̄π (fsaj )P̄π (fsaj , j)

fsaj ∈F

X

X

π(s, a)Pr(s, a, j)

a∈A

dπ (s)Pπ (s, j)

s∈S

= dπ (j)

Unifying Task Specification in Reinforcement Learning

where the last line follows from the definition of the stationary distribution. Therefore, for j ∈ S
d̄π P̄π (:, j) =

1
dπ (j) = d̄π (j)
c

Case 2: j = fsas0 ∈ F
For the first component, because P̄π (i, fsas0 ) = 0 for all
i 6= s and because P̄π (s, fsas0 ) = π(s, a)Pr(s, a, s0 ) by
construction,
X

dπ (i)P̄π (i, fsas0 ) = dπ (s)P̄π (s, fsas0 )

i∈S

= dπ (s)π(s, a)Pr(s, a, s0 )
= c d̄π (fsas0 ).
For the second component, because P̄π (f, j) = 0 for all
f, j ∈ F, we get
X

d̄π (f )P̄π (f, j) = 0.

f ∈F

Therefore, for j = fss0 ∈ F, d̄π P̄π (:, j) = d̄π (j).
Finally, clearly by normalizing the first component of d̄π
over s ∈ S, we get the same proportion across states as in
dπ , satisfying (6).
To see why v̄π (s) = vπ (s) for all s ∈ S, first notice that

With this equivalence, it is clear that
X
d̄π (i)v̄π (i)
i∈S̄

=

1X
1 X
dπ (s)vπ (s) +
dπ (s)Pπ (s, s0 )vπ (s0 )
c
c
fss0 ∈F

s∈S

1XX
dπ (s)Pπ (s, s0 )vπ (s0 )
c
c
s∈S
s∈S s0 ∈S
1 X
1X
dπ (s)vπ (s) +
dπ (s0 )vπ (s0 )
=
c
c 0
s∈S
s ∈S
2X
dπ (s)vπ (s)
=
c
=

1X

dπ (s)vπ (s) +

s∈S

Therefore, optimizing either results in the same policy. 

C. Discounting and average reward for
control
The common wisdom is that discounting is useful for asking
predictive questions, but for control, the end goal is average
reward. One of the main reasons for this view is that it has
been previously shown that, for a constant discount, optimizing the expected return is equivalent to optimizing average
reward. This can be easily seen by expanding the expected
return weighting according to the stationary distribution for
a policy, given constant discount γc < 1,
dπ vπ = dπ (rπ + Pπ,γ vπ )

(7)

= dπ rπ + γc dπ Pπ vπ

r̄π (i) =

rπ (i) i ∈ S
0
otherwise

and for any fsas0 ∈ F
v̄π (fsas0 ) = 0 +

X

P̄π (fsas0 , j)γ̄s (j)v̄π (j)

j∈S̄

= v̄π (s0 ).
Now for any s ∈ S,
v̄π (s) = r̄π (s) +

X

P̄π (s, j)γ̄s (j)v̄π (j)

j∈S̄

= rπ (s) +

X

P̄π (s, fsas0 )γ̄s (fsas0 )v̄π (fsas0 )

fsas0 ∈F

= rπ (s) +

XX
s0 ∈S

Pr(s, a, s0 )γ(s, a, s0 )v̄π (s0 )

a∈A

Therefore, because it satisfies the same fixed point equation,
v̄π (s) = vπ (s) for all s ∈ S.

= dπ rπ + γc dπ vπ
1
=⇒ dπ vπ =
dπ rπ .
1 − γc

(8)

Therefore, the constant γc < 1 simply scales the average reward objective, so optimizing either provides the
same policy. This argument, however, does not extend to
transition-based discounting, because γ(s, a, s0 ) can significantly change weighting in returns in a non-uniform way,
affecting the choice of the optimal policy. We demonstrate
this in the case study for the taxi domain in Section 3.

D. Algorithms
We show how to write generalized pseudo-code for two algorithms: true-online TD (λ) and ELSTDQ(λ). We choose
these two algorithms because they generally demonstrate
how one would extend to transition-based γ, and further
previously had a few unclear points in their implementation.
For TO-TD, the pseudo-code has been given for episodic
tasks (van Seijen and Sutton, 2014), rather than more generally, and has treated vold carefully at the beginning of
episodes, which is not necessary. LSTDQ has typically only

Unifying Task Specification in Reinforcement Learning

Algorithm 1 True-online TD(λ)
w ← 0, e ← 0, vold ← 0
Obtain initial x0
while agent interacting with environment, t = 0, 1, . . .
do
Obtain next feature vector xt+1 , reward rt+1 and
discount γt+1
v̂ = w> xt
v̂ 0 = w> xt+1
δ ← rt+1 + γt+1 v̂ 0 − v̂
e ← e + xt
w ← w + α(δ + v̂ − vold )e − α(v̂ − vold )xt
vold ← v̂ 0
e ← γt+1 λt+1 e − αγt+1 λt+1 (e> xt+1 )xt+1
return w

E. Lemmas
To bound the maximum eigenvalues of the discountweighted transition matrix, we first provide the following
lemma. This lemma is independently interesting, in that it
explicitly verifies the previous Assumption 2.1 (Yu, 2015).
Lemma 3. Under Assumption A3, the maximum eigenvalue
of Pπ,γ is less than 1:
r(Pπ,γ ) < 1.
Proof:
Part 1: We first show that r(M̃) < 1 for

Mkl − δ if i = k, j = l
M̃ =
Mkl
otherwise.
for any i, j, and any 0 < δ < Mij .

been written for a batch of data, without importance sampling; we provide an ELSTDQ variant here with importance
sampling, where LSTDQ is a special case using M = 1.
There are a few other implementation details that merit clarification. We use the notation γt+1 for γ(st , at , st+1 ), and
λt+1 for λ(st , at , st+1 ). Further, unlike previous pseudocode, we do not reinitialize vold specially at the start of an
episode (i.e., when γt+1 = 0). This is because the value of
vold is not relevant for the next step after γt+1 = 0. The eligibility trace is zeroed, and so α(δ+v̂−vold )e−α(v̂−vold )x =
αδx. Finally, for both algorithms, we stage the updates to
the traces. This is to avoid saving both γt , λt and γt+1 , λt+1
across timesteps.

Algorithm 2 ELSTDQ(λ)
A ← 0, b ← 0, e ← 0
F ← 0, M ← 0
Obtain initial action-value feature vector x0 (implicitly
x(s0 , a0 )) and action a0
while agent interacting with environment, t = 0, 1, . . .
do
Obtain next action-value feature vector xt+1 , action
at+1 reward rt+1 and discount γt+1
t+1 ,at+1 )
ρt+1 ← π(x
µ(xt+1 ,at+1 )
F ← γt+1 F + i(xt )
M ← λt+1 i(xt ) + (1 − λt+1 )F
e ← e + M xt
>
A ← A + e (xt − ρt+1 γt+1 xt+1 )
b ← b + ert+1
e ← γt+1 λt+1 ρt+1 e
F ← ρt+1 F
return A−1 b
// The solution w to the linear system

For M = Pπ , we know that r(M) = 1 and that, by assumption, Pπ is irreducible. We know that M̃ is still irreducible,
because the connectivity is not changed (since no additional
entries are zeroed). By the Perron-Frobenius theorem, we
know that the eigenvector x that corresponds to the maximum eigenvalue r(M̃) has strictly positive entries and so
δxi xj > 0. Therefore,
x> M̃x =

X

xk Mkl xl + xi (Mij − δ)xj

k,l:(k,l)6=(i,j)

= x> Mx − δxi xj
< x> Mx.
We know that x> Mx ≤ 1, because r(M) = 1 and x is a
unit vector. Therefore, using the fact that r(M̃) = x> M̃x
by Courant-Fischer-Weyl4 , we get
r(M̃) = x> M̃x < 1.
Part 2: Next we show that further reducing entries, even
to zero values, will not increase the maximum eigenvalue.
This follows simply from the fact that non-negative matrices
are guaranteed to have a non-negative eigenvector x that
corresponds to the maximum eigenvalue.
To see why, for notational convenience, we now let M be
the matrix where entry i, j in Pπ was reduced by δ. Let M̃
further reduce an entry by δ, now potentially to a minimum
value of 0, so that M̃ is guaranteed to be non-negative (rather
than strictly positive). Using the same argument as above,
4
The Courant-Fischer-Weyl min-max principle states that
the maximum eigenvalue r(M) of a matrix M corresponds to
argmaxx:x6=0 x> Mx/(x> x), where the corresponding x that
gives the maximum is an eigenvector of M. Therefore, for this x,
x> Mx = r(M) and kxk22 = x> x = 1.

Unifying Task Specification in Reinforcement Learning

we obtain that for the non-negative eigenvector of M̃

For t > 0: Assume that
t
X

x> M̃x = x> Mx − δxi xj
≤ r(M)

(Pπ,γ,λ )k Pπ,γ,1−λ 1 ≤ 1.

k=0

because δxi xj ≥ 0. Therefore, with further reduction,
r(M̃) cannot increase and so r(M̃) ≤ r(M) < 1.
Part 3: Finally, we can see that for any γ and Pπ as given
under Assumptions 2 and 3, M = Pπ,γ satisfies the above
construction.

Now we additionally provide definitions for the extension to
transition-based discounts. To do so, we will need to define

Then
t+1
X

(Pπ,γ,λ )k Pπ,γ,1−λ 1

k=0

= Pπ,γ,1−λ 1 +

" t+1
X

#
k

(Pπ,γ,λ ) Pπ,γ,1−λ 1

k=1

"
Pπ,γ,λ (s, s0 ) :=

X

π(s, a)Pr(s, a, s0 )γ(s, a, s0 )λ(s, a, s0 )

= Pπ,γ,1−λ 1 + Pπ,γ,λ

π(s, a)Pr(s, a, s0 )γ(s, a, s0 )(1 − λ(s, a, s0 ))

≤ Pπ,γ,1−λ 1 + Pπ,γ,λ 1

X
a∈A

≤1

Then we obtain the following generalized definition of
and necessary properties for convergence within ETD.

Pλπ

Lemma 4. Under Assumption A3, I − Pπ,γ,λ is nonsingular and the matrix
−1

Pλπ = (I − Pπ,γ,λ )

Pπ,γ,1−λ

is non-negative and has rows that sum to no greater than 1,
0 ≤ Pλπ ≤ 1

Pλπ 1 ≤ 1.

and

Proof:
By Lemma 3, we know r(Pπ,γ ) < 1. Because
0 ≤ λ(s, a, s0 ) ≤ 1 for all (s, a, s0 ), this means that
r(Pπ,γ,1−λ ) < 1. Therefore I − Pπ,γ,λ is non-singular
and so Pλπ is well-defined.
Notice that I − Pπ,γ,λ is a non-singular M-matrix, since the
maximum eigenvalue of Pπ,γ,λ is less than one and entrywise Pπ,γ,λ ≥ 0. Therefore, the inverse of I − Pπ,γ,λ is
−1
positive, making (I − Pπ,γ,λ ) Pπ,γ,1−λ a positive matrix.
The fact that the matrix has entries that are less than or equal
to 1 follows from showing Pλπ 1 ≤ 1 below.
To show that the matrix rows always sum to less than 1, we
use a simple inductive argument. Since
=

(Pπ,γ,λ )k Pπ,γ,1−λ 1

= Pπ,γ 1

= Pπ,γ − Pπ,γ,λ

Pλπ

#

k=0

a∈A

Pπ,γ,1−λ (s, s0 ) :=

t
X

∞
X



completing the proof.

F. Convergence of emphatic algorithms for
the RL task formalism
We start with convergence in expectation of ETD for
transition-based discounts. The results for state-based
MDPs should automatically extend to transition-based
MDPs, due to the equivalence proved in Section B.1. However, in an effort to similarly generalize the writing of the
theoretical analysis to the more general transition-based
MDP setting, as we did for algorithms and implementation,
we explicitly extend the proof for transition-based MDPs.
Theorem 3. Assume the value function is approximated
using linear function approximation: v(s) = x(s)> w. For
X with linearly independent columns (i.e. linearly independent features), with an interest function i : S → (0, ∞) and
M = diag(m) for m = (I − Pλπ )−1 (d ◦ i), the matrix A
is positive definite.
A := X> M(I − Pπ,γ,λ ) (I − Pπ,γ )X
−1

= X> M(I − Pλπ )X
is positive definite.
Proof:

k

(Pπ,γ,λ ) Pπ,γ,1−λ

First, we write an equivalent definition for Pλπ ,

k=0

we
Pt simply kneed to show
k=0 (Pπ,γ,λ ) Pπ,γ,1−λ 1 ≤ 1.

that

For the base case, t = 0: clearly
Pπ,γ,1−λ 1 ≤ 1

for

every

t,

−1

Pπ,γ,1−λ

−1

(Pπ,γ − Pπ,γ,λ )

−1

(Pπ,γ − I + I − Pπ,γ,λ )

Pλπ = (I − Pπ,γ,λ )
= (I − Pπ,γ,λ )
= (I − Pπ,γ,λ )

= I − (I − Pπ,γ,λ )−1 (I − Pπ,γ ).

Unifying Task Specification in Reinforcement Learning

Since X is a full rank matrix, to prove that

Further, this MDP with the assumptions on the subspace
produced by the state-action features satisfies the conditions
of Theorem 3, and so Aq is also positive definite.

A = X> M(I − Pλπ )X
is positive definite, we need to prove that M(I − Pλπ ) is
positive definite.
As in (Sutton et al., 2016, Theorem 1), (Yu, 2015, Proposition C.1), we need to show that for M(I − Pλπ ), (a) the
diagonal entries are nonnegative, (b) the off-diagonal entries
are nonpositive (c) its row sums are nonnegative and (d)
the columns sums are are positive. The requirements (a)
- (c) follow from Lemma 4, because M is a non-negative
diagonal weighting matrix. To show (d), first if i(s) > 0 for
all s ∈ S, the vector of columns sums is
>

1 M(I −

Pλπ )

>

= m (I −

Pλπ )

= (dπ ◦ i)

>

which always has positive entries.
Otherwise, if i(s) = 0 for some s ∈ S, we can prove that
M(I − Pλπ ) is positive definite using the same argument
as in (Yu, 2015, Corollary C.1). The proof nicely encapsulates Pλπ generically as a matrix Q. We simply have to
ensure that the inverse of I − Pλπ exists and that Pλπ has
entries less than or equal to 1, both of which were showed
in Lemma 4. The first condition is to have well-defined matrices, and the second to ensure that Q has a block-diagonal
structure. Therefore, under Assumption 4, we can follow
the same proof as (Yu, 2015, Corollary C.1) to ensure that
A is positive definite.

For the proofs for ELSTDQ, the main difference is in using
action-value functions. We construct the augmented space,
with states S̄ = S × A and

Similarly, the other properties of the Bellman operator
and the weighted norm on Pλ,q
π extend, giving a unique
fixed point for the action-value Bellman operator Pλ,q
π and
kPλ,q
k
<
1.
Mq
π
Corollary 1. Assume the action-value function is approximated using linear function approximation: x(s, a)> w. For
X with linearly independent columns (i.e. linearly independent features), Aq is positive definite.

G. Issues with transition-based trace without
emphatic weighting
A natural goal is to similarly generalize the contraction
properties of Pλπ under the weighting dπ , from constant λc
to transition-based trace. To do so, unlike under emphatic
weighting, we need to restrict the set of possible trace functions. Notice that, because of Assumption A3, for some
sλ < 1 and s1−λ < 1, for any non-negative v+ ,
dπ Pπ,γ,λ v+
XX
=
dπ (s)Pr(s, a, :) ◦ γ(s, a, :) ◦ λ(s, a, :)v+
s

≤ sλ

a

XX
s

dπ (s)Pr(s, a, :)v+ = sλ dπ v+

a

and similarly

Pπ,γ,q ((s, a), (s, a0 )) := P (s, a, s0 )γ(s, a, s0 )π(s0 , a0 )

dπ Pπ,γ,1−λ v+ ≤ s1−λ dπ v+ .

Pπ,γ,λ,q ((s, a), (s, a0 )) := P (s, a, s0 )γ(s, a, s0 )λ(s, a, s0 )π(s0 , a0 )

The generalized bound on the dπ weighted norm is given in
the following lemma.

giving
iq ((s, a)) := i(s)
dµ,q ((s, a)) := dµ (s)µ(s, a)
X
rq ((s, a)) :=
Pr(s, a, s0 )r(s, a, s0 )

Lemma 5.
kPλπ kDπ ≤

s0 ∈S

:= (I − Pπ,γ,q )−1 (Pπ,γ,q − Pπ,γ,λ,q )

−1
Mq := diag dµ,q ◦ iq (I − Pλ,q
.
π )

s1−λ
.
1 − sλ

Pλ,q
π

Then
λ,q
Aq := X>
q Mq (I − Pπ )X

bq := X>
q Mq (I − Pπ,γ,q )rq .
The projected Bellman operator is defined as
ΠMq T (λ,q) q = rq + Pλ,q
π q where ELSTD(λ) converges to the projected Bellman operator fixed point
ΠMq T (λ,q) q = q.

Now, the norm is only a contraction if s1−λ < 1 − sλ . As
we have seen, for constant trace, this inequality holds, since
s1−λ = s(1 − λ) and sλ = sλ for some s < 1. In general,
however, there are instances where this is not true. We
provide such an example below.5
Consider a 2-state MDP, with uniform probabilities of transitioning and uniform policy, and so dπ = [0.5, 0.5]. Let
γc = 0.99 and set λ to be 0.9 when entering state s1 and 0
5

ple.

Thanks to an anonymous reviewer for pointing out this exam-

Unifying Task Specification in Reinforcement Learning

when entering state s2 . Then for any v+ ,


0.9 0
dπ Pπ,γ,λ v+ = γc dπ Pπ
v+
0 0


0.9 0
= γc dπ
v+
0 0


1.0 0
= γc 0.9dπ
v+
0 0
≤ γc 0.9dπ v+
where for v+ = [v 0]> for any v ≥ 0, this bound is tight.
Similarly,


0.1 0
dπ Pπ,γ,1−λ v+ = γc dπ Pπ
v+
0 1.0
≤ γc dπ v+ .
where for v+ = [0 v]> for any v ≥ 0, this bound is tight.
Therefore, sλ = 0.9γc and s1−λ = γc , and so we get
1 − sλ = 0.9γc < s1−λ = γc , which makes the upper
bound in the above lemma 1.1̄. Computing Pλπ ,


0.0893 0.8927
Pλπ =
.
0.0893 0.8927
we can see that this is not a contraction.

