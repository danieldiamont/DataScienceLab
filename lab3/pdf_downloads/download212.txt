Communication-efficient Algorithms for
Distributed Stochastic Principal Component Analysis

Dan Garber 1 Ohad Shamir 2 Nathan Srebro 3

Abstract
We study the fundamental problem of Principal
Component Analysis in a statistical distributed
setting in which each machine out of m stores
a sample of n points sampled i.i.d. from a single unknown distribution. We study algorithms
for estimating the leading principal component
of the population covariance matrix that are both
communication-efficient and achieve estimation
error of the order of the centralized ERM solution that uses all mn samples. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumptions, for the PCA objective, simply
averaging the local ERM solutions cannot guarantee error that is consistent with the centralized
ERM. We show that this unfortunate phenomena
can be remedied by performing a simple correction step which correlates between the individual
solutions, and provides an estimator that is consistent with the centralized ERM for sufficientlylarge n. We also introduce an iterative distributed
algorithm that is applicable in any regime of n,
which is based on distributed matrix-vector products. The algorithm gives significant acceleration
in terms of communication rounds over previous
distributed algorithms, in a wide regime of parameters.

1. Introduction
Principal Component Analysis (PCA) (Pearson, 1901;
Hotelling, 1933; Jolliffe, 2002) is one of the most celebrated and popular techniques in data analysis and ma1
Technion - Israel Institute of Technology, Haifa, Israel 2 Weizmann Institute of Science, Rehovot, Israel
3
Toyota Technological Institute, Illinois, USA. Correspondence to:
Dan Garber <dangar@technion.ac.il>, Ohad
Shamir <ohad.Shamir@weizmann.ac.il>, Nathan Srebro
<nati@ttic.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

chine learning. For data that consists of N vectors in
RdP
, x1 , ..., xN , with normalized covariance matrix XÃÇ =
N
1
>
i=1 xi xi , The PCA method finds the k-dimensional
N
subspace (which corresponds to the span of the top k principal components) such that the projection of the data onto
the subspace has largest variance, i.e., it is the solution to
the optimization problem:
max

W‚ààRd√ók ,WT W=I

kXÃÇWk2F .

(1)

PCA is often considered in a statistical setting in which the
assumption is that the input vectors are not arbitrary but
sampled i.i.d. from some fixed but unknown distribution
with certain general characteristics D. Then, it is often of
interest to use the observed sample to estimate the top k
principal components of the population covariance matrix,
rather then that of the sample, which leads to the modified
optimization problem:


max
kEx‚àºD xx> Wk2F .
(2)
W‚ààRd√ók ,WT W=I

Of course the empirical estimation problem (1) and the
population estimation problem (2) are well connected, and
it is well-known that under mild assumptions on the distribution D and given a sufficiently large sample, we can
guarantee small estimation error in (2) by solving optimization problem (1).
In this work we consider the problem of estimating the first
principal component (i.e., k = 1) in a statistical and distributed setting. We assume the availability of m machines,
each of which stores a sample of n vectors sampled i.i.d
from a fixed distribution D over Rd , and we are interested
in algorithms that can be applied efficiently to solve Problem (2) for k = 1, with estimation error that approaches
that of a centralized algorithm, which has access to all mn
samples and does not pay for communication between machines. Indeed, when considering the efficiency of algorithms, we will mainly focus on the amount of communication between machines they require, since this is often
the most expensive resource in distributed computing. We
note that the i.i.d. assumption is standard in many applications of PCA, and can be leveraged to get more efficient
algorithms than when the data partition is arbitrary. Also,
we will make a standard assumption that the population covariance matrix has a non-zero additive gap between the

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

first and second eigenvalues, which makes the problem of
estimating the leading principal component meaningful.
A main challenge that often arises in many computational
settings of principal components is that it leads to inherently non-convex optimization problems. While many
times these problems turn out to admit efficient algorithms,
the rich toolbox of optimization and statistical estimation
procedures developed for convex problems often cannot be
directly applied to problems such as (1) and (2). Instead,
one often needs to consider a specialized and more involved
analysis, to get analogous convergence results for the PCA
problem. This for instance was the case in a recent wave
of results that applied concepts such as stochastic gradient updates (Balsubramani et al., 2013; Shamir, 2016a; Jain
et al., 2016b; Allen Zhu & Li, 2016b) and variance reduction (Shamir, 2015; 2016c; Garber & Hazan, 2015; Garber
et al., 2016; Allen Zhu & Li, 2016a) to the PCA problem.
This is also the case in our distributed setting. For instance,
(Zhang et al., 2013) proposed communication-efficient algorithms for a distributed statistical estimation settings,
similar to ours, but under convexity assumptions. The authors show that under their assumptions, in a wide regime
of parameters (namely when the per-machine sample size
n is large enough), then a simple averaging of the empirical
risk minimizers (ERM), computed locally on each machine,
leads to estimation error of the population parameters of
the order the centralized ERM solution. While averaging
makes perfect sense in a convex setting, it is clear that it
can completely fail in a non-convex setting. Indeed, we
show that already for the PCA problem with k = 1, simply
averaging the local ERM solutions (and normalizing to obtain a unit vector as required), cannot improve significantly
over the estimation error of any single machine. We then
show that a simple fix to the above scheme, namely correlating the directions of individual ERM solutions, remedies this phenomena and results in estimation error similar
to that of the centralized ERM solution. Much like the results of (Zhang et al., 2013), this result only holds in the
regime when the per-machine sample size n is sufficiently
large. As discussed, due to the inherent non-convexity of
the PCA objective, this approach requires a novel analysis
tailored to the PCA problem. In this context, we view this
work as an initiation of a research effort to understand how
to efficiently aggregate statistical estimators in a distributed
non-convex setting.
A second line of results for distributed estimation under
convexity assumptions consider iterative algorithms that
perform multiple communication rounds and are based on
distributed gradient computations (some examples include
(Shamir et al., 2014; Zhang & Lin, 2015; Lee et al., 2015;
Shamir, 2016b; Jaggi et al., 2014; Reddi et al., 2016)). The
benefit of these methods is that (a) they provide meaningful
estimation error guarantees in a much wider regime of pa-

rameters than the ‚Äúone-shot‚Äù aggregation methods (namely
in terms of the number of samples per machine), and (b),
due to their iterative nature, they allow to approximate the
centralized ERM solution arbitrary well. Unfortunately,
these methods, all of which rely heavily on convexity assumption, cannot be directly applied to the PCA problem.
Towards designing efficient distributed iterative methods
for our PCA setting, we consider the application of the
recently proposed method of Shift-and-Invert power iterations (S&I) for PCA (Garber & Hazan, 2015; Garber et al.,
2016). The S&I method reduces the problem of computing
the leading eigenvector of a real positive semidefinite matrix to that of approximately solving a small number (i.e.
poly-logarithmic in the problem parameters) of systems of
linear equations. These in turn, could be efficiently solved
by arbitrary distributed convex solvers. We show that coupling the S&I method with the stochastic pre-conditioning
technique for linear systems proposed in (Zhang & Lin,
2015) and well known fast gradient methods such as the
conjugate gradient method, gives state-of-the-art guarantees in terms of communication costs, and provides a significant improvement over distributed variants of classical
fast eigenvector algorithms such as power iterations and the
faster Lanczos algorithm. Much like its convex counterparts, which only rely on distributed gradient computations
and simple vector aggregations, our iterative method only
relies on distributed matrix-vector products, i.e., it requires
each machine to only send products of its local empirical
covariance matrix with some input vector.
Beyond the results described so far, (Liang et al., 2014;
Boutsidis et al., 2016) studied distributed algorithms for
PCA in a deterministic setting in which the partition of
the data across machines is arbitrary and communication
is measured in terms of number of transmitted bits. The
approximation guarantees provided in these works are in
terms of the projection of the data onto the leading principal components (instead of alignment between the estimate
and the optimal solution, studied in this paper). Applying
these results to our setting will give a number of communication rounds that scales like poly(‚àí1 Œ¥ ‚àí1 ), where  is the
desired error and Œ¥ is the population eigengap. In our setting,  will scale with the inverse of the size of the sample,
i.e.,  ‚âà (mn)‚àí1 , which for these algorithms will result in
amount of communication that is polynomial in the size of
the data. In contrast, we will be interested in algorithms
whose communication costs does not scale with n at all. In
this context we note that, by focusing on algorithms that
either perform simple aggregation of local ERM solutions,
or perform only distributed matrix-vector products with the
empirical covariance matrix, we can circumvent the need to
measure communication explicitly in terms of the number
of bits transmitted, which often burdens the analysis of natural algorithms, such as those proposed here.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

2. Preliminaries
2.1. Notation and problem setting
We write vectors in Rd in boldface lower-case letters (e.g.,
v), matrices in boldface upper-case letters (e.g., X), and
scalars are written as lightface letters (e.g., c). We let k ¬∑
k denote the standard Euclidean norm for vectors and the
spectral norm for matrices.
We consider the following statistical distributed setting.
Let D be a distribution over vectors in Rd with squared
`2 norm at most b, for some b > 0. We consider a setting
in which m machines, numbered 1...m, are each given a
dataset of n samples drawn i.i.d. from D. We let v1 denote
a leading eigenvector of the population covariance matrix
X = Ex‚àºD [xx> ]. Our goal is to efficiently (mainly in
terms of communication) find an estimate w for v1 , i.e., a
unit vector that maximizes the product (v1> w)2 with high
probability. Towards this end, we assume that the population covariance matrix X has a non-zero eigengap Œ¥, i.e.,
Œ¥ := Œª1 (X) ‚àí Œª2 (X) > 0, where Œªi (¬∑) denotes the ith
largest eigenvalue of a symmetric real matrix. Note that
Œ¥ > 0 is necessary for v1 to be uniquely defined (up to
sign).
In addition, we let XÃÇi denote the empirical covariance matrix of the sample stored on machine i for every i ‚àà [m],
Pn
(i) (i)>
(i)
(i)
i.e., XÃÇi = n1 j=1 xj xj , where x1 ...xn are the
samples stored on machine i. We let XÃÇ denote the empirical covariance matrix
Pmof the union of points across all
1
machines i.e., XÃÇ = m
i=1 XÃÇi .
Our model of communication assumes that the m machines
work in rounds during which a central machine (w.l.o.g.
machine 1) can send a single vector in Rd to all other machines, or every machine can send either the leading eigenvector of its local empirical covariance matrix, or the product of a single input vector with its local covariance, to machine 1. We will measure communication complexity in
terms of number of such rounds required to achieve a certain estimation error.
2.1.1. T HE CENTRALIZED SOLUTION
Our primary benchmark for measuring performance will be
the centralized empirical risk minimizer which is the leading eigenvector of the aggregated empirical covariance matrix XÃÇ.
The following standard result bounds the error of the centralized ERM.
Lemma 1 (Risk of centralized ERM). Fix p ‚àà (0, 1). Suppose that Œ¥ > 0 and let vÃÇ1 denote the leading eigenvector
of XÃÇ, i.e., vÃÇ1 ‚àà arg maxv:kvk=1 v> XÃÇv. Then it holds w.p.
at least 1 ‚àí p that

32b2 ln(d/p)
.
(3)
mnŒ¥ 2
Lemma 1 is a direct consequence of the following standard concentration argument for random matrices, and the
Davis-Kahan sin(Œ∏) theorem (whose proof is given in the
appendix for completeness):
Theorem 1 (Matrix Hoeffding, see (Tropp, 2012)). Let D
be a distribution over vectors with squared `2 norm
Pn at most
b, and let X = Ex‚àºD [xx> ]. Let XÃÇ = n1 i=1 xi x>
i ,
where x1 , ..., xn are
 sampled i.i.d.
 from D. Then,
 2it holds

 n
that ‚àÄ > 0 : Pr kXÃÇ ‚àí Xk ‚â•  ‚â§ d ¬∑ exp ‚àí 16b
.
2
1 ‚àí (v1> vÃÇ1 )2 ‚â§ ERM (p) :=

Theorem 2 (Davis-Kahan sin(Œ∏) theorem). Let X, Y be
symmetric real d√ód matrices with leading eigenvectors vX
and vY respetively. Also, suppose that Œ¥(X) := Œª1 (X) ‚àí
2
2
>
Œª2 (X) > 0. Then it holds that 1 ‚àí vX
vY ‚â§ 2 kX‚àíYk
Œ¥(X)2 .
2.2. Informal statement of main results and previous
algorithms
We now informally describe our main results, followed by a
detailed description of previous approaches that are directly
applicable to our setting. The algorithmic results (both new
and old) are summarized in Table 1.
2.2.1. M AIN RESULTS
Failure of simple averaging of local ERM solutions We
show that a natural approach of simply averaging the individual leading eigenvectors of the empirical covariance
matrices XÃÇi (and normalizing the obtain a unit vector)
cannot significantly improve (beyond logarithmic factors)
over the performance of any of the individual eigenvectors.
(i)
More concretely, if we let vÃÇ1 denote the leading eigenvector of XÃÇi for any i ‚àà [m], and we denote their average
Pm (i)
1
by vÃÑ1 = m
i=1 vÃÇ1 , then there exists a distribution D
over vectors with magnitude O(1) and covariance eigengap Œ¥ = 1, such that
"
 > 2 #
 
vÃÑ1 v1
1
‚àÄm, n : ED 1 ‚àí
=‚Ñ¶
,
kvÃÑ1 k
n
See Theorem 3 in Section 3 for the complete and formal
argument.
A successful single communication round algorithm via
correlation of individual ERM solutions We show that
if prior to averaging the local ERM solutions, as suggested
above, we correlate their directions by aligning them according to any single machine (say machine number 1),
Pm
(i)> (1)
(i)
1
vÃÇ1 )vÃÇ1 , then this
i.e., we let vÃÑ1 = m
i=1 sign(vÃÇ1
guarantees that for any p ‚àà (0, 1), w.p. at least 1 ‚àí p,
 
 Ô£∂
Ô£´
dm
 > 2
2
4 2 dm
b
ln
b
ln
p
p
vÃÑ1 v1
Ô£∏ . (4)
1‚àí
= OÔ£≠
+
kvÃÑ1 k
Œ¥ 2 mn
Œ¥ 4 n2

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis
Method
Centralized ERM
Distributed Power Method
Distributed Lanczos
‚ÄúHot-potato‚Äù SGD

1 ‚àí (w> v1 )2 w.p. 3/4
2
ln d
ERM = Œò( bŒ¥2 mn
)
ERM ¬∑ (1 + o(1))
ERM ¬∑ (1 + o(1))
O(ERM)


# communcation rounds
OÃÉ(Œª1 /Œ¥)
p
OÃÉ( Œª1 /Œ¥)
m

ERM ¬∑ (1 + o(1))

OÃÉ(min{(b/Œ¥)1/2 n‚àí1/4 , m1/4 })

O(ERM ) + O

Average of ERMs with sign-fixing (Theorem 4)
Distributed Shift&Invert + precond. linear systems (Theorem 6)

b4 ln2 d
Œ¥ 4 n2

1

Table 1. Comparison of estimation error and number of communication rounds. For simplicity we fix the failure probability to p = 1/4
and assume mn is in the regime in which Lemma 1 is meaningful, i.e, mn = ‚Ñ¶(b2 Œ¥ ‚àí2 ln d). The OÃÉ(¬∑) suppresses logarithmic factors
in b, d, 1/p, 1/ERM . For the result of Theorem 4 we assume the regime m = O(d). The sub-constant o(1) factors could be made, in
principle, arbitrary small in all relevant results by trading approximation with communication.

See Theorem 4 in Section 3 for the complete and formal
result.
In particular, in the likely scenario when m = O(d/p)
2
we have that w.p. at least 1 ‚àí p, 1 ‚àí vÃÑ1> v1 /kvÃÑ1 k =
ERM (p)) ¬∑ O 1 + m2 ¬∑ ERM (p) , where ERM (p)) is defined in Eq. (3). Another related interpretation of the results is that the bound in Eq. (4) is comparable with ERM

(up to poly-log factors) when n = ‚Ñ¶ Œ¥ ‚àí2 b2 m ln(dm/p) .
We also show a matching lower bound that the bound in
Eq. (4) is tight (up to poly-log factors) for this aggregation
method.
A multi communication round algorithm We present a
distributed algorithm based on the Shift-and-Invert framework for leading eigenvector computation (Garber &
Hazan, 2015; Garber et al., 2016) which is applied to explicitly solving the centralized ERM problem. We show
that for any p ‚àà (0, 1), when mn = ‚Ñ¶(b2 ln(d/p)/Œ¥ 2 ) (i.e.,
when Lemma 3 is meaningful), the algorithm produces a
solution w such that w.p. at least 1 ‚àí p,
1 ‚àí (v1> w)2 ‚â§ ERM (p)) ¬∑ (1 + o(1)) ,

(5)

where ERM (p)) is
‚àö defined in Eq. (3). The algorithm performs overall OÃÉ( bŒ¥ ‚àí1/2 n‚àí1/4 ) distributed matrix-vector
products with the centralized empirical covariance matrix
XÃÇ 1 . The OÃÉ(¬∑) notation hides poly-logarithmic factors in
1/p, 1/Œ¥, d, 1/ERM (p). See Theorem 6 in Section 4 for the
complete and formal result.
We note that in particular, under our assumption that mn =
‚Ñ¶ÃÉ(b2 /Œ¥ 2 ), it holds that the number of distributed matrixvector products is upper bounded by OÃÉ(m1/4 ). Moreover,
in the regime n = ‚Ñ¶(b2 Œ¥ ‚àí2 ), we can see that the number
of distributed matrix-vector products depends only polylogarithmically on the problem parameters.
In general, the sub-constant o(1) factor in (5) could be
made arbitrarily small by trading the approximation error
1

i.e., on each round, each machine i sends the product of an
input vector in Rd with its local covariance matrix XÃÇi .

with the number of distributed matrix-vector products.
2.2.2. P REVIOUS ALGORITHMS
Distributed versions of classical iterative algorithms:
Classical fast iterative algorithms for computing the leading eigenvector of a positive semidefinite matrix, such as
the well-known Power Method and the Lanczos Algorithm, require iterative multiplications of the input matrix (XÃÇ in our case) with the current estimate. It is thus
straightforward to implement these algorithms in our distributed setting, by multiplying the same vector with the
covariance matrices at each machine, and averaging the
result. Thus, by well-known convergence guarantees of
these two methods, we will have that for a fixed  > 0,
these methods produce a unit vector w such that, for any
p ‚àà (0, 1), 1 ‚àí (w> vÃÇ1 )2 ‚â§  w.p. at least 1 ‚àí p, after p
O(ŒªÃÇ1 Œ¥ÃÇ ‚àí1 ln(d/p)) rounds for the Power Method and
O( ŒªÃÇ1 Œ¥ÃÇ ‚àí1 ln(d/p)) for the Lanczos Algorithm, where
ŒªÃÇ1 , Œ¥ÃÇ denote the leading eigenvalue and eigengap of XÃÇ,
respectively. Moreover, in the regime of mn in which
Lemma 1 is meaningful, we can replace ŒªÃÇ1 , Œ¥ÃÇ with Œª1 , Œ¥
in the above bounds, and the result will still hold with high
probability.
Simple calculations show that in the regime of mn in which
Lemma 1 is meaningful, it holds that our Shift-and-Invertbased algorithm outperforms distributed Lanczos (in terms
of worst-case guarantees) whenever n = ‚Ñ¶ÃÉ(b2 /Œª21 ).
‚ÄúHot potato‚Äù SGD: Another straightforward approach is
to apply a sequential algorithm for direct risk minimization that can process the data-points one by one, such as
stochastic gradient descent (SGD), by passing its state from
one machine to the next, after completing a full pass over
the machine‚Äôs data. Clearly, this process of making a full
pass over the data of a certain machine before sending the
final estimate to the next one, requires overall m communication rounds in order to make a full pass over all mn
points. SGD for PCA was studied in several results in
recent years (Balsubramani et al., 2013; Shamir, 2016a;c;

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

Jain et al., 2016a; Allen Zhu & Li, 2016b). For instance
applying the result of (Jain et al., 2016a) in this way will
result in a final estimate w satisfying

 2
b ln d
>
2
w.p. at least 3/4. (6)
1 ‚àí (w v1 ) = O
Œ¥ 2 mn
We note that in the regime in which the bound in (6) is
meaningful it holds that the number of communication
rounds of our Shift-and-Invert-based algorithm is upperbounded by OÃÉ(m1/4 ) which for sufficiently large m dominates the communication complexity of SGD.

3. Single Communication Round Algorithms
via ERM on Each Machine
In this section we consider distributed algorithms that require only a single round of communication. Naturally for
this regime, all algorithms will be based on aggregating the
ERM solutions of the individual machines, i.e., each machine i only sends the leading eigenvector of its empirical covariance matrix XÃÇi to a centralized machine (without
loss of generality, machine 1) which it turn combines them
to a single unit vector in some manner.
3.1. Simple averaging of eigenvectors fail
Perhaps the simplest method to aggregate the individual
eigenvectors of each machine is to average them, and then
normalize to obtain a unit vector. For instance, in the
distributed statistical setting considered in (Zhang et al.,
2013), in which the objective is strongly convex, it was
shown that simply averaging the individual ERM solutions
leads, in a meaningful regime of parameters, to estimation
error of the order of the centralized ERM solution. However, here we show that for PCA, in which the objective is
certainly not convex, this approach fails practically in any
regime, in the sense that the error of the returned aggregated solution can be no better than that returned by any
single machine.
Theorem 3. There exists a distribution over vectors in R2
with `2 norm bounded by a universal constant for which the
eigengap in the covariance matrix is 1 (i.e., Œ¥ = 1), such
(i)
that if each machine i returns an estimate vÃÇ1 which is
an unbiased leading eigenvector of XÃÇi (i.e., both outcomes
(i)
(i)
‚àívÃÇ1 , +vÃÇ1 are equally likely), then the aggregated vector
P
(i)
m
1
vÃÑ1 = m
i=1 vÃÇ1 satisfies
"

2 #
vÃÑ1
‚àÄm, n : E 1 ‚àí
, v1
= ‚Ñ¶(1/n).
kvÃÑ1 k

The proof is given in the appendix.

3.2. Averaging with Sign Fixing
As evident from the statement of Theorem 3, an important
assumption is that each machine produces an unbiased estimate, in the sense that the sign of the outcome is uniform
and independent of the other machines. This hints that correlating the signs of the different estimates can circumvent
the lower bound result in Theorem 3. It turns out that this
is indeed the case, as captured by the following theorem:
Theorem 4. Let wÃÉi be the leading eigenvector of XÃÇi for
any i ‚àà [m], and consider
the unit vector
Pm
>
i=1 sign(wÃÉi wÃÉ1 )wÃÉi
.
(7)
w = Pm
k i=1 sign(wÃÉi> wÃÉ1 )wÃÉi k
Then, for any p ‚àà (0,Ô£´
1), it holds
 at least 1 ‚àí p that
Ô£∂
 w.p.
dm
2
b log p
b4 log2 dm
p
Ô£∏.
1 ‚àí (v1> w)2 = O Ô£≠
+
Œ¥ 2 mn
Œ¥ 4 n2
For ease of presentation, throughout the rest of this section
we denote the correlated vector wÃÇi = sign(wÃÉi> wÃÉ1 )wÃÉi for
any i ‚àà [m].
The main step towards proving Theorem 4 is to consider
each wÃÇi as an approximately unbiased perturbation of the
true leading eigenvector v1 and to upper bound the magnitude of this perturbation. This is carried out in the following much more general and self-contained lemma, which
might be of independent interest. The proof is given in the
appendix.
Lemma 2. Let A be a positive semidefinite matrix with
some fixed leading eigenvector v1 , a leading eigenvalue Œª1
and an eigengap Œ¥ := Œª1 (A) ‚àí Œª2 (A) > 0. Let AÃÇ be some
positive semidefinite matrix such that kAÃÇ ‚àí Ak ‚â§ Œ¥/4.
Then there is a unique leading eigenvector vÃÇ1 of AÃÇ such
that hvÃÇ1 , vi ‚â• 0, and


ckAÃÇ ‚àí Ak2


,
vÃÇ1 ‚àí v1 ‚àí (Œª1 I ‚àí A)‚Ä† (AÃÇ ‚àí A)v1  ‚â§
Œ¥2
where ‚Ä† denotes the pseudo-inverse, and c is a positive numerical constant.
Lemma 2 is central to the proof of the following Lemma,
of which the proof of Theorem 4 is an easy consequence.
We defer the proof of both the Lemma and that of Theorem
4 to the appendix.
Lemma 3. The following two conditions hold with probability at least 1‚àíp‚àíd exp(‚àíŒ¥ 2 n/cb2 ), for some numerical
constants c, c0 > 0:
‚Ä¢ The leading eigenvalue of every XÃÇi is simple, i.e.,
Œª1 (XÃÇi ) ‚àí Œª2 (XÃÇi ) > 0.
‚Ä¢ Fixing v1 , there exist unique leading eigeni
vectors vÃÇ1i , . . . , vÃÇm
of XÃÇ1 , . . . , XÃÇm , such that
 1 Pm i
i
maxi kvÃÇ1 ‚àí v1 k ‚â§ 41 , and  m
i=1 vÃÇ1 ‚àí
q

 2

2

v1  ‚â§ c0 b log(2dm/p)
+ b log(2dm/p)
.
Œ¥2 n
Œ¥ 2 mn

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

3.3. Lower Bound for Sign Fixing

min {FŒª,w (z) :=

z‚ààRd

We now show that the result of Theorem 4 is tight up to
poly-logarithmic factors and cannot be improved in general:
Theorem 5. For any Œ¥ ‚àà (0, 1) and d > 1, there exist a
distribution over vectors in Rd (of norm at most a universal
constant) with eigengap Œ¥ in the covariance matrix, such
that for any number of machines m and for per-machine
sample size any n sufficiently larger than 1/Œ¥ 2 , the aggrePm (i)
1
gated vector vÃÑ1 = m
i=1 vÃÇ1 (even after sign fixing with
the population
eigenvector
v ) satisfies
"


2 # 1

vÃÑ1
1
1
E 1‚àí
, e1
= ‚Ñ¶
+
kvÃÑ1 k
Œ¥ 2 mn Œ¥ 4 n2
The proof is given in the appendix.

4. A Multi-round Algorithm based on
Shift-and-Invert Iterations
In this section we move on to consider distributed algorithms that perform multiple communication rounds. The
main motivation, beyond improving some poly-logarithmic
factors in the estimation error, is to obtain a result that does
not require the per-machine sample size n to grow with the
number of machines m, as in the result of Theorem 4.
Towards this end we consider the use of the Shift-andInvert meta-algorithm, originally described in (Garber &
Hazan, 2015; Garber et al., 2016), to explicitly solve the
centralized ERM objective, i.e., find a unit vector that is an
approximate solution to maxv:kvk=1 v> XÃÇv.
Throughout this section we let ŒªÃÇ1 , Œ¥ÃÇ denote the leading
eigenvalue and eigengap of XÃÇ, respectively. Also, we assume without loss of generality that b = 1 (i.e., all data
points lie in the unit Euclidean ball).
Since our approach is to approximate the population risk
by approximating the empirical risk, we state the following simple lemma for completeness (a proof is given in the
appendix).
Lemma 4 (Risk of approximated-ERM for PCA). Let w
be a unit vector such that (w> vÃÇ1 )2 ‚â• 1 ‚àí , for some fixed
 > 0, where vÃÇ1 is the leading eigenvector ‚àö
of XÃÇ. Then it
holds that 1 ‚àí (w> v1 )2 ‚â§ 1 ‚àí (w> vÃÇ1 )2 + 2.
4.1. The Shift-and-Invert meta-algorithm
The Shift-and-Invert algorithm (Garber & Hazan, 2015;
Garber et al., 2016) efficiently reduces the problem of
computing the leading eigenvector of a positive semidefinite matrix XÃÇ to that of approximately-solving a polylogarithmic number of linear systems, i.e., finding approximate minimizers of convex quadratic optimization problems of the form

1 >
z (ŒªI ‚àí XÃÇ)z ‚àí z> w},
2

(8)

where Œª > Œª1 (XÃÇ) is a shifting parameter. The algorithm is
essentially based on applying power iterations to a shifted
and inverted matrix (ŒªI ‚àí XÃÇ)‚àí1 , where the shifting parameter Œª is carefully chosen. The algorithm that implements
this reduction, originally described in (Garber & Hazan,
2015), is given below (see Algorithm 1).
Algorithm 1 S HIFT- AND -I NVERT P OWER M ETHOD
1: Input: estimate Œ¥ÃÉ for the gap Œ¥ÃÇ, accuracy  ‚àà (0, 1),

failure probability p
 

2: Set: m1 ‚Üê d8 ln 144d/p2 e, m2 ‚Üê d 32 ln 18d
p2  e
n  m1 +1  m2 +1 o
1
3: Set: Àú ‚Üê min 16
Œ¥ÃÉ/8
, 4 Œ¥ÃÉ/8
4: Set: Œª(0) ‚Üê 1 + Œ¥ÃÉ , wÃÇ0 ‚Üê random unit vector, s ‚Üê 0
5: repeat
6:
s ‚Üê s + 1 , Ms ‚Üê (Œª(s‚àí1) I ‚àí XÃÇ)
7:
for t = 1...m1 do
8:
Find approx. minimizer - wÃÇt of FŒª(s‚àí1) ,wÃÇt‚àí1 (z)
9:
10:
11:
12:

such that kwÃÇt ‚àí M‚àí1
Àú
s wÃÇt‚àí1 k ‚â§ 
end for
ws ‚Üê wÃÇm1 /kwÃÇm1 k
Find approx. minimizer - vs of FŒª(s‚àí1) ,ws (z) such
that kvs ‚àí M‚àí1
Àú
s ws k ‚â§ 
‚àÜs ‚Üê 12 ¬∑ w> v1s ‚àíÀú , Œª(s) ‚Üê Œª(s‚àí1) ‚àí ‚àÜ2s
s

13: until ‚àÜs ‚â§ Œ¥ÃÉ
14: Œª(f ) ‚Üê Œª(s) , Mf ‚Üê (Œª(f ) I ‚àí XÃÇ)
15: for t = 1...m2 do
16:
Find approx. minimizer - wÃÇt of FŒª(f ) ,wÃÇt‚àí1 (z) such

Àú
that kwÃÇt ‚àí M‚àí1
f wÃÇt‚àí1 k ‚â§ 
17: end for
18: Return: wf ‚Üê wÃÇm2 /kwÃÇm2 k
Lemma 5 (Efficient reduction of top eigenvector to convex optimization; originally Theorem 4.2 in (Garber &
Hazan, 2015)). Suppose that Œ¥ÃÇ := Œª1 (XÃÇ) ‚àí Œª2 (XÃÇ) > 0
and suppose that the estimate Œ¥ÃÉ in Algorithm 1 satisfies
Œ¥ÃÉ ‚àà [Œ¥ÃÇ/2, 3Œ¥ÃÇ/4]. Then, with probability at least 1 ‚àí p,
Algorithm 1 finds a unit vector wf such that (wf> vÃÇ1 )2 ‚â•
1 ‚àí , and the total number of optimization problems of
the form (8) solved 
during the run of the algorithm,
  is upd
‚àí1
per bounded by O ln(d/p) ln(Œ¥ÃÇ ) + ln p
. Moreover, throughout the run of the algorithm it holds that
1 + Œ¥ÃÇ ‚â• Œª(s) ‚àí ŒªÃÇ1 = ‚Ñ¶(Œ¥ÃÇ).
Remark: the purpose of the repeat-until loop in Algorithm 1 is to efficiently find a shifting parameter Œª(f ) such
that c1 Œ¥ÃÇ ‚â§ Œª(f ) ‚àí ŒªÃÇ1 ‚â§ c2 Œ¥ÃÇ for some universal constants
c2 > c1 > 0. When n satisfies n = ‚Ñ¶(Œ¥ ‚àí2 ln(d/p)),
we can directly find (w.h.p) such a shifting parameter, by

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

simply estimating ŒªÃÇ1 , Œ¥ÃÇ from the data of a single machine.
Also, we can take wÃÇ0 to be the leading eigenvector of any
single machine, since this will already have a constant correlation with vÃÇ1 . Thus, for such n, the total number of
optimization problems can be reduced to O(ln(p‚àí1 ‚àí1 )).
Algorithm 1 is a meta-algorithm in the sense that the choice
of solver for the optimization problems min FŒª,w is unspecified, and any solver will do. A simple calculation
shows that a naive application of either the conjugate gradient method or Nesterov‚Äôs accelerated gradient method to
solve these optimization problems in a distributed manner,
i.e., the computation of the gradient vector
q is distributed

across machines, will require overall OÃÉ ŒªÃÇ1 /Œ¥ÃÇ communication rounds, which does not give any improvement over
the distributed Lanczos approach, described in Subsection
2.2.2. However, this can be substantially improved by taking advantage of the fact that the data on all machines is
sampled i.i.d. from the same distribution. In particular,
we present below an approach based on applying a preconditioner to the optimization Problem (8), in the spirit of
the one described in (Zhang & Lin, 2015).
4.2. Faster Distributed Approximation of Linear
Systems via Local Preconditioning
Let M = ŒªI‚àí XÃÇ, for some shift parameter Œª > ŒªÃÇ1 , and define the pre-conditioning matrix C = (Œª+¬µ)I‚àí XÃÇ1 , where
¬µ is required so C is invertible. Consider now solving the
following modified quadratic problem:
1
FÃÉŒª,w (y) := y> C‚àí1/2 MC‚àí1/2 y ‚àí y> C‚àí1/2 w. (9)
2
Note that if y‚àó is the optimal solution to Problem (9), i.e.,
y‚àó = C1/2 M‚àí1 C1/2 C‚àí1/2 w = C1/2 M‚àí1 w,
then z‚àó := C‚àí1/2 y‚àó is the optimal solution to Problem (8).
The idea behind choosing C this way is very intuitive. Ideally we could have chosen C = M, making the condition number of FÃÉŒª,w equal to Œ∫(FÃÉŒª,w ) = 1, which is the
best we can hope for. The problem of course is that this
requires us to explicitly compute M‚àí1/2 , which is more
challenging then just computing the leading eigenvector of
XÃÇ. The next best thing is thus to choose C based only
on the data available on any single machine, which allows
computing C‚àí1/2 without additional communication overhead, and leads to the choice described above. The following lemma, rephrased from (Zhang & Lin, 2015), quantifies
exactly how such a choice of C helps in improving the condition number of the new optimization problem, Problem
(9). The proof is given in the appendix.
Lemma 6. Suppose
 that ¬µ ‚â•
 kXÃÇ ‚àí XÃÇ1 k. Then, FÃÉŒª,w (y)
is 1-smooth and

Œª‚àíŒªÃÇ1
(Œª‚àíŒªÃÇ1 )+2¬µ

-strongly convex. In particu-

lar, Œ∫(FÃÉŒª,w ) ‚â§ 1 + 2¬µ/(Œª ‚àí ŒªÃÇ1 ). Moreover, fixing yÃÉ ‚àà Rd ,

if we let zÃÉ := C‚àí1/2 yÃÉ, then it holds that kzÃÉ ‚àí M‚àí1 wk ‚â§
(Œª ‚àí ŒªÃÇ1 )‚àí1/2 kyÃÉ ‚àí C1/2 M‚àí1
pwk. In particular, for any
p ‚àà (0, 1), if we set ¬µ = 4 ln(d/p)/n, then the above
holds with probability at least 1 ‚àí p, where this probability
depends only on the randomness in XÃÇ1 .
4.2.1. S OLVING THE PRE - CONDITIONED LINEAR
SYSTEMS

We now discuss the application of gradient-based algorithms for finding an approximate minimizer of the preconditioned problem, Problem (9), in our distributed setting. Towards this end we require a distributed implementation for the first-order oracle of FÃÉŒª,w (y) (i.e., computation of the value and gradient vector at a queried point).
A straight-forward implementation of the first-order oracle
in our distributed setting is given in Algorithm 2.
Algorithm 2 Distributed First-Order Oracle for FÃÉŒª,w (y)
1: Input: shift parameter Œª > 0, regularization parameter
¬µ > 0, vector w ‚àà Rd , query vector y ‚àà Rd
2: send yÃÉ := C‚àí1/2 y to machines {2, . . . , m} for C :=
(Œª + ¬µ)I ‚àí XÃÇ1 {executed on machine 1}
3: for i = 1...m do
Àú i := XÃÇi yÃÉ to machine 1 {executed on each
4:
send ‚àá
machine i}
5: end for
Àú
Àú := 1 Pm ‚àá
6: aggregate ‚àá
i=1 i {executed on machine 1}
m
1
Àú ‚àí
7: compute FÃÉŒª,w (y) = 2 (Œªy> C‚àí1 y ‚àí y> C‚àí1/2 ‚àá)
> ‚àí1/2
y C
w {executed on machine 1}
Àú ‚àí C‚àí1/2 w
8: compute ‚àáFÃÉŒª,w (y) = ŒªC‚àí1 y ‚àí C‚àí1/2 ‚àá
{executed on machine 1}
9: return: (FÃÉŒª,w (y), ‚àáFÃÉŒª,w (y))
We have the following lemma, the proof of which is deferred to the appendix.
Lemma 7. Fix some Œª > Œª1 (XÃÇ) and w ‚àà Rd , and let
1 ‚â• ¬µ > 0 be as in Lemma 6. Fix  > 0. Consider the
following two-step algorithm:
1. Apply either the conjugate gradient method or Nesterov‚Äôs accelerated method with the distributed firstorder oracle described in Algorithm 2 to find yÃÉ ‚àà Rd
such that FÃÉŒª,w (yÃÉ) ‚àí miny‚ààRd FÃÉŒª,w (y) ‚â§ 0
2. Return zÃÉ = C‚àí1/2 yÃÉ.

‚àí1
2¬µ
Then, for 0 = 2 1 + Œª‚àí
(Œª ‚àí ŒªÃÇ1 ) it holds that
ŒªÃÇ
1

kzÃÉ ‚àí (ŒªI ‚àí XÃÇ1 )‚àí1 wk ‚â§ , and the total number distributed matrix-vector products with the empirical covariance matrix XÃÇ required to compute zÃÉ is upper-bounded by
O

q

1 + 2¬µ(Œª ‚àí ŒªÃÇ1 )‚àí1 ln 1 +

2¬µ
Œª ‚àí ŒªÃÇ1




kwk/[(Œª ‚àí ŒªÃÇ1 )]
.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

We now state our main result for this section, which is a
simple consequence of the previous lemmas. The full proof
is given in the appendix.
Theorem 6. Fix  ‚àà (0, 1) and p ‚àà
p(0, 1). Suppose that
mn = ‚Ñ¶(Œ¥ ‚àí2 ln(d/p)). Set ¬µ = 4 ln(3d/p)/n. Applying the Shift-and-Invert algorithm, Algorithm 1, with the
parameters , p/3, and applying the algorithm in Lemma
7 with the parameter ¬µ, to approximately solve the linear
systems, yields with probability at least 1 ‚àí p a unit vector
wf such that (wf> vÃÇ1 )2 ‚â• 1 ‚àí , after executing at most
Ô£´s p
OÔ£≠

" 

ln(d/p)
d
‚àö
ln
ln
p2
Œ¥ n

+ ln

2



d
p2



 
1
ln
Œ¥

!
p
ln(d/p)
‚àö
Œ¥2 n
s
=

OÃÉ

1
‚àö

!

Œ¥ n

distributed matrix-vector products with the empirical covariance matrix XÃÇ.

5. Experiments
To validate some of our theoretical findings we conducted experiments with single-round algorithms on synthetic data. We generated synthetic datasets using two distributions. For both distributions we used the covariance
matrix X = UŒ£U> with U being a random d √ó d orthonormal matrix and Œ£ is diagonal satisfying: Œ£(1, 1) =
1, Œ£(2, 2) = 0.8, ‚àÄj ‚â• 3 : Œ£(j, j) = 0.9¬∑Œ£(j ‚àí1, j ‚àí1),
i.e., Œ¥ = 0.2. One dataset was generated according to the
normal distributions N (0, X), and for p
the second datasets
we generated samples by taking x = 3/2X1/2 y where
y ‚àº U [‚àí1, 1]. In both cases we set d = 300.
Beyond the single-round algorithms that are based on
aggregating the individual ERM solutions described so
far, we propose an additional natural aggregation approach, based on aggregating the individual projection ma(i)
trices. More concretely, letting {vÃÇ1 }m
i=1 denote the leading eigenvectors of the individual machines, let PÃÑ1 :=
Pm (i) (i)>
1
. We then take the final estimate w to
i=1 vÃÇ1 vÃÇ1
m
be the leading eigenvector of the aggregated matrix PÃÑ1 .
Note that as with the sign-fixing based aggregation, this
approach also resolves the sign-ambiguity in the estimates
produced by the different machines, which circumvents the
lower bound result of Theorem 3.
For both datasets we fixed the number of machines to
m = 25. We tested the estimation error (i.e., the value
1 ‚àí (w> v1 )2 where v1 is the leading eigenvector of X and
w is the estimator) of five benchmarks vs. the per-machine
sample size n: the centralized solution vÃÇ1 , the average
of the individual (unbiased) ERM solutions (normalized to
unit norm),the average of ERM solutions with sign-fixing,
and the leading eigenvector of the averaged projection ma-

trix. We also plotted the average loss of the individual ERM
solutions. Results are averaged over 400 independent runs.
The results for the normal distribution appear in Figure 1.
The results for the uniform-based distribution are very similar and are deferred to the appendix. We can see that, as
our lower bound in Theorem 3 suggests, simply averaging
and normalizing the individual ERM solutions has significantly worse performance than the centralized ERM solution. Perhaps surprisingly, the performance of this estimator is even worse than the average error of an estimate
computed using only a single machine. We see that both
aggregation methods that are based on correlating the individual ERM solutions, namely the sign-fixing-based estimator, and the proposed averaging-of-projections heuristic,
are asymptotically consistent with the centralized ERM.
In particular, the averaging-of-projections scheme, at least
empirically, significantly outperforms the sign-fixing approach, which justifies further theoretical investigation of
this heuristic. For the sign fixing approach, we can see that
as suggested by our bounds, the estimator is not consistent
with the centralized ERM solution for small values of n.
0.9

centralized ERM
avg. of ERMs
sign-fix avg. of ERMs.
projection avg.
avg. machine loss

0.8

0.7

0.6

avg. error

4.3. Putting it all together

0.5

0.4

0.3

0.2

0.1

0
0

100

200

300

400

500

600

n

Figure 1. Estimation error vs. the per-machine sample size n for
a normal distribution.

6. Discussion
We presented communication-efficient algorithms for distributed statistical estimation of principal components. Focusing on our results for methods based on a single communication round, we initiated a study of how to correctly aggregate distributed ERM solutions in a non-convex setting.
An important take-home message of our work is that in a
non-convex setting, simply averaging the local solutions is
not a good idea. On the positive side, we show that a very
simple correction (i.e., sign-fixing) is possible by leveraging the specific structure of the problem at hand. It is thus
interesting to develop a richer theory of how to perform
such aggregations in more involved non-convex problems.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

References
Eigenvalues and eigenvectors of 2x2 matrices.
http://www.math.harvard.edu/archive/
21b_fall_04/exhibits/2dmatrices/.

Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Praneeth, and Sidford, Aaron. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja‚Äôs algorithm. arXiv preprint
arXiv:1602.06929, 2016b.

Allen Zhu, Zeyuan and Li, Yuanzhi. Even faster SVD decomposition yet without agonizing pain. In Advances
in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems
2016, December 5-10, 2016, Barcelona, Spain, pp. 974‚Äì
982, 2016a.

Jolliffe, IT. Principal component analysis. 2002. Springverlag, New York, 2002.

Allen Zhu, Zeyuan and Li, Yuanzhi. Fast global convergence of online PCA. CoRR, abs/1607.07837, 2016b.

Liang, Yingyu, Balcan, Maria-Florina F, Kanchanapally,
Vandana, and Woodruff, David. Improved distributed
principal component analysis. In NIPS, 2014.

Balsubramani, Akshay, Dasgupta, Sanjoy, and Freund,
Yoav. The fast convergence of incremental PCA. In
Advances in Neural Information Processing Systems 26:
27th Annual Conference on Neural Information Processing Systems 2013, pp. 3174‚Äì3182, 2013.
Boutsidis, Christos, Woodruff, David P, and Zhong, Peilin.
Optimal principal component analysis in distributed and
streaming models. In Proceedings of the 48th Annual
ACM SIGACT Symposium on Theory of Computing, pp.
236‚Äì249. ACM, 2016.
Garber, Dan and Hazan, Elad. Fast and simple pca via
convex optimization. arXiv preprint arXiv:1509.05647,
2015.
Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Faster eigenvector computation via shift-andinvert preconditioning. CoRR, abs/1605.08754, 2016.
Golub, Gene H and Pereyra, Victor. The differentiation
of pseudo-inverses and nonlinear least squares problems
whose variables separate. SIAM Journal on numerical
analysis, 10(2):413‚Äì432, 1973.
Hotelling, H. Analysis of a complex of statistical variables
into principal components. J. Educ. Psych., 24, 1933.
Jaggi, Martin, Smith, Virginia, TakaÃÅc, Martin, Terhorst,
Jonathan, Krishnan, Sanjay, Hofmann, Thomas, and Jordan, Michael I. Communication-efficient distributed
dual coordinate ascent. In Advances in Neural Information Processing Systems, pp. 3068‚Äì3076, 2014.
Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Praneeth, and Sidford, Aaron. Matching matrix bernstein with little memory: Near-optimal finite sample guarantees for oja‚Äôs algorithm. arXiv preprint
arXiv:1602.06929, 2016a.

Lee, Jason D., Ma, Tengyu, and Lin, Qihang. Distributed
stochastic variance reduced gradient methods. CoRR,
abs/1507.07595, 2015.

Magnus, Jan R. On differentiating eigenvalues and eigenvectors. Econometric Theory, 1(02):179‚Äì191, 1985.
Pearson, K. On lines and planes of closest fit to systems of
points in space. Philosophical Magazine, 2(6):559‚Äì572,
1901.
Reddi, Sashank J., KonecnyÃÅ, Jakub, RichtaÃÅrik, Peter,
PoÃÅczos, BarnabaÃÅs, and Smola, Alexander J. AIDE:
fast and communication efficient distributed optimization. CoRR, abs/1608.06879, 2016.
Shamir, Ohad. A stochastic PCA and SVD algorithm with
an exponential convergence rate. In Proceedings of the
32nd International Conference on Machine Learning,
ICML 2015, Lille, France, 6-11 July 2015, pp. 144‚Äì152,
2015.
Shamir, Ohad. Convergence of stochastic gradient descent
for PCA:. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York
City, NY, USA, June 19-24, 2016, pp. 257‚Äì265, 2016a.
Shamir, Ohad. Without-replacement sampling for stochastic gradient methods. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 46‚Äì54, 2016b.
Shamir, Ohad. Fast stochastic algorithms for svd and pca:
Convergence properties and convexity. In Proceedings of
The 33rd International Conference on Machine Learning, pp. 248‚Äì256, 2016c.
Shamir, Ohad, Srebro, Nathan, and Zhang, Tong.
Communication-efficient distributed optimization using
an approximate newton-type method. In Proceedings of
the 31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pp. 1000‚Äì
1008, 2014.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

Tropp, Joel A. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389‚Äì434, 2012.
Yu, Yi, Wang, Tengyao, and Samworth, Richard J. A useful variant of the davis‚Äìkahan theorem for statisticians.
Biometrika, 102(2):315‚Äì323, 2015.
Zhang, Yuchen and Lin, Xiao. Disco: Distributed optimization for self-concordant empirical loss. In Proceedings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 362‚Äì370, 2015.
Zhang, Yuchen, Duchi, John C, and Wainwright, Martin J.
Communication-efficient algorithms for statistical optimization. Journal of Machine Learning Research, 14:
3321‚Äì3363, 2013.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

A. Proofs Omitted from Section 3
A.1. Proof of Theorem 3
Proof. Consider the following distribution over R2 .


1
x = e1 +
,
2

1 , 2 ‚àº U {‚àí1, +1},

where e1 is the first standard basis vector in R2 .
The population covariance matrix and the empirical covariance matrix of a sample of size n are clearly given by




2 0
2 yn
X=
,
XÃÇ(n) =
,
0 1
yn 1
where yn is a random variable which is the average of n U {‚àí1, +1} random variables. By elementary calculations we
have that the leading eigenvector of XÃÇ(n) is given by
!
2yn
p
,
vÃÇ1 = œÉ ¬∑ C(yn ) ¬∑ 1,
1 + 1 + 4yn2
where
Ô£´
C(yn ) := Ô£≠1 +

2y
p n
1 + 1 + 4yn2

!2 Ô£∂‚àí1/2
Ô£∏

‚àö
is the normalization factor that guarantees that vÃÇ1 is a unit vector. In particular, it holds that 1/ 2 ‚â§ C(yn ) ‚â§ 1. The
random variable œÉ ‚àº U {‚àí1, +1} is independent of yn and determines the sign of vÃÇ1 , which follows from our assumption
that vÃÇ1 is generated by unbiased ERM.
Pm (i)
(1)
(m)
1
Consider now the average of m such unit vectors vÃÇ1 ..vÃÇ1 given by vÃÑ = m
i=1 vÃÇ1 and the normalized estimate
vÃÑ1 /kvÃÑ1 k, and recall that the leading eigenvector of the population covariance matrix is e1 . It holds that
h

vÃÑ1
vÃÑ1 (2)2
vÃÑ1 (1)2
=
1
‚àí
.
, e1 i2 =
vÃÑ1 (1)2 + vÃÑ1 (2)2
vÃÑ1 (1)2 + vÃÑ1 (2)2
kvÃÑ1 k

(10)

Towards upper-bounding the RHS of (10) in expectation, the main step is to lower bound the random variable |vÃÑ1 (2)| using
Chebyshev‚Äôs inequality.
It holds that
E[|vÃÑ1 (2)|]

=

=
(a)

=

‚â•
(b)

=
(c)

Ô£π
Ô£Æ

 X


 1 (i) 
 1 m (i) 2C(yn(i) )yn(i) 
Ô£ª
q
E  vÃÇ1 (2) = E Ô£∞
œÉ

m
(i)2 
 m i=1
1 + 1 + 4yn
Ô£π
Ô£Æ
 X

 1 m (i) 2C(yn(i) )|yn(i) | 

Ô£ª
Ô£∞
q
E 
œÉ

(i)2 
 m i=1
1 + 1 + 4yn

Ô£πÔ£π
Ô£Æ
Ô£Æ
 X

 1 m (i) 2C(yn(i) )|yn(i) | 
 | {œÉ (i) }Ô£ªÔ£ª
q
E{œÉ(i) } Ô£∞E{y(i) } Ô£∞
œÉ

n
m
(i)2
 i=1
1 + 1 + 4yn 
Ô£Æ
Ô£πÔ£π
Ô£Æ


m
(i)
(i)
X


2C(y
)|y
|
1
n
n
(i)
(i) Ô£ªÔ£ª

Ô£∞
Ô£∞
q
E{œÉ(i) } E{y(i) }
œÉ
| {œÉ } 
n
m
(i)2


i=1
1 + 1 + 4yn
#
"
"
#


m
1 X

2C(yn )|yn |
1


p
E{œÉ(i) } 
= Œò ‚àö
,
œÉ (i)  ¬∑ Eyn
m

mn
1 + 1 + 4yn2 (d)
i=1

(11)

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis
(i)
œÉ (i) yn

q

(i)2
(i)
(i)
where (a) follows since
‚àºœÉ
and C(yn )/(1 + 1 + 4yn ) depends only on |yn |, (b) follows from the
(i)
triangle inequality, and (c) follows since {œÉ (i) }i‚àà[m] and {yn }i‚àà[m] are independent random variables. Finally, it is easy
Pm (i)
to verify that (d) follows since i=1 œÉ /m is the average of m U {‚àí1, +1} random variables andphence its expected
‚àö
‚àö
absolute value is Œò(1/ m). Similarly the expected absolute value of yn is Œò(1/ n) and C(yn )/(1 + 1 + 4yn2 ) is lower
(i)

(i)
|yn |

bounded by a positive constant.
Also, observe that
Ô£Æ
!2 Ô£π
2 #
1
1 (i)
1
2C(yn )yn
Ô£ª
p
= E[vÃÇ1 (2)2 ] = E Ô£∞
E[vÃÑ1 (2)2 ] = E
vÃÇ (2)
m 1
m
m
1 + 1 + 4yn2


1
1
2
‚â•
E[yn ] = Œò
,
2m
mn
‚àö
where the inequality follows since |yn | ‚â§ 1 and 1/ 2 ‚â§ C(yn ) ‚â§ 1.
"

(12)

Combining Eq. (11) and Eq. (12), we have by an application of Chebyshev‚Äôs inequality to the random variable |vÃÑ1 (2)| that
there exists universal constants c1 > 0 such that


1
1
Pr |vÃÑ1 (2)| ‚â§ ‚àö
‚â§ .
(13)
4
c1 mn
Also, it is easy to verify that
E[vÃÑ1 (1)2 ] = O(1/m),

E[vÃÑ1 (2)2 ] = O(1/m).

Thus, by a simple application of Markov‚Äôs inequality we have that there exists a universal constant c2 > 0 such that


1
1
2
2
Pr max{vÃÑ1 (1) , vÃÑ1 (2) } ‚â•
(14)
‚â§ .
c3 m
4
Using Eq. (10), (13) and (14) we finally have that




 
vÃÑ1 (2)2
vÃÑ1
1
2
E h
, e1 i = 1 ‚àí E
=1‚àí‚Ñ¶
.
kvÃÑ1 k
vÃÑ1 (1)2 + vÃÑ1 (2)2
n

A.2. Proof of Lemma 2
Proof. The proof is based on viewing AÃÇ as an unbiased perturbation of the matrix A, and computing a Taylor expansion
of vÃÇ1 around v1 . For notational convenience, let E = AÃÇ ‚àí A, and define A(t) = A + tE for t ‚àà [0, 1]. Also, define Œª(t)
to be the leading eigenvalue of A(t).
First, we note that for any t ‚àà [0, 1], A(t) has an eigengap of at least Œ¥/2 between its first two eigenvalues (since by
Weyl‚Äôs inequality, its eigenvalues are at most ktEk ‚â§ kEk ‚â§ Œ¥/4 different than A, and we know that A has an eigengap
of Œ¥). Therefore, the leading eigenvalue of A(t) is simple. This means that the function v(t), which equals the leading
eigenvector of A(t), is uniquely defined up to a sign. This sign will be chosen so that hv(t), v1 i ‚â• 0, which makes v(t)
unique and well-defined2 . By Theorem 1 in (Magnus, 1985), we have that both Œª(t) and v(t) are infinitely differentiable
at any t ‚àà [0, 1], and satisfy3
Œª0 (t) = v(t)> Ev(t) , v0 (t) = (Œª(t)I ‚àí A(t))‚Ä† Ev(t) .
2

Note thatp
ties are impossible, since that can only happen if hv(t), v1 i = 0, yet by applying the Davis-Kahan sin(Œ∏) theorem
(Theorem 2), 1 ‚àí hv(t), v1 i2 ‚â§ 2kA(t)‚àíAk
‚â§ 2kEk
‚â§ 12 .
Œ¥
Œ¥
3
Formally speaking, the theorem only ensures v(t), Œª(t) exist and are infinitely differentiable in some open neighborhood of t.
However, since the result holds for any t ‚àà [0, 1], and the proof implies that these functions are unique in each such neighborhood
(where the uniqueness of v(t) holds once we fixed the sign as above), it follows that the same holds in all of t ‚àà [0, 1].

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

We will also need to bound the second derivative of v(t). By the product rule and the equations above, this derivative
equals

‚àÇ
‚àÇ
(Œª(t)I ‚àí A(t))‚Ä† Ev(t) + (Œª(t)I ‚àí A(t))‚Ä† E v(t)
‚àÇt
‚àÇt

‚àÇ
‚Ä†
‚Ä†
=
(Œª(t)I ‚àí A(t)) Ev(t) + (Œª(t)I ‚àí A(t)) E(Œª(t)I ‚àí A(t))‚Ä† Ev(t).
‚àÇt

v00 (t) =

(15)

To compute the derivative above, we apply the chain rule. The derivative of a pseudo-inverse B‚Ä† of a matrix-valued
function B = B(t) with respect to t (assuming B and hence its pseudo-inverse is symmetric for all t) is given by (see
Theorem 4.3 in (Golub & Pereyra, 1973))






2 ‚àÇ
2
‚àÇ
‚àÇ
B B‚Ä† + B‚Ä†
B (I ‚àí BB‚Ä† ) + (I ‚àí B‚Ä† B)
B B‚Ä† .
‚àíB‚Ä†
‚àÇt
‚àÇt
‚àÇt
This formula is true assuming the rank of B is constant in some open neighborhood of t. Applying
this for B =
‚àÇ
 (Œª(t)I ‚àí A(t)) =
Œª(t)I
‚àí
A(t)
(which
indeed
has
a
fixed
rank
of
d
‚àí
1
by
the
eigengap
assumption),
noting
that
‚àÇt


v(t)> Ev(t)I ‚àí E ‚â§ 2kEk, and using the facts that kv(t)k = 1, kI ‚àí B‚Ä† Bk ‚â§ 1,kI ‚àí BB‚Ä† k ‚â§ 1 and
k(Œª(t)I ‚àí A(t))‚Ä† k ‚â§ 2/Œ¥ (since the smallest non-zero eigenvalue of Œª(t)I ‚àí A(t) is at least Œ¥/2), we have that


‚àÇ

24 ¬∑ kEk
‚Ä† 

.
 ‚àÇt (Œª(t)I ‚àí A(t))  ‚â§
Œ¥2
Plugging this into (15), and again using the fact that k(Œª(t)I ‚àí A(t))‚Ä† k ‚â§ 2/Œ¥, we get that
kv00 (t)k ‚â§

ckEk2
Œ¥2

for some numerical constant c.
By a first-order Taylor expansion of v(t) with an explicit remainder term4 ,
Z
1 1
v(1) = v(0) + v0 (0) +
(1 ‚àí t)2 v00 (t)dt ,
2 t=0
which by the equations above and the definition of v(t) implies that
Z
1 1
vÃÇ1 = v1 + (Œª1 I ‚àí A)‚Ä† Ev1 +
(1 ‚àí t)2 v00 (t)dt .
2 t=0
This implies


vÃÇ1 ‚àí v1 ‚àí (Œª1 I ‚àí A)‚Ä† Ev1  ‚â§ 1
2
0

2

2

Z

1

(1 ‚àí t)2 kv00 (t)kdt ‚â§

t=0

ckEk2
2Œª2

Z

1

(1 ‚àí t)2 dt,

t=0

0

which is at most c kEk /Œª for some appropriate numerical constant c . Plugging back E = AÃÇ‚àíA, the result follows.
A.3. Proof of Lemma 3
Proof. Using the matrix Hoeffding inequality (Theorem 1) and a union bound, we that


 2 
Œ¥
Œ¥ n
Pr ‚àÉi, kXÃÇi ‚àí Xk >
‚â§ md exp ‚àí 0 2
12
cb

(16)

for some constant c0 > 0. Thus, with high probability, maxi kXÃÇi ‚àí Xk ‚â§ Œ¥/12. By Weyl‚Äôs inequality, it follows that the
eigenvalues of X and XÃÇi are at most Œ¥/12 apart, and since X has an eigengap of Œ¥ between its two leading eigenvalues,
Since v(t), v0 (t), v00 (t) are all vectors, this is a direct consequence of the standard Taylor expansion of the scalar function t 7‚Üí
v(t)j , mapping t to the j-th coordinate of v(t), using the fact that this mapping is differentiable to any order (see Theorem 1 in (Magnus,
1985), and in particular twice continuously differentiable.
4

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

it follows that XÃÇi has an eigengap of at least Œ¥ ‚àí Œ¥/12 ‚àí Œ¥/12 > 0, which proves the first part of the lemma. To
handle the second part, note that by a variant of the Davis-Kahan sinŒ∏ theorem (see Corollary 1 in (Yu et al., 2015)), if
maxi kXÃÇi ‚àí Xk ‚â§ Œ¥/12, then the leading eigenvectors vÃÇ1i of XÃÇi (after choosing the sign appropriately, i.e. hvÃÇ1i , v1 i ‚â• 0)
are all at a distance of at most 1/4 from v1 . Moreover, by Lemma 2,
m
m

c 1 X
1 X
 i

kXÃÇi ‚àí Xk2 .
vÃÇ1 ‚àí v1 ‚àí (Œª1 I ‚àí X)‚Ä† (XÃÇi ‚àí X)v1  ‚â§ 2 ¬∑
m i=1
Œ¥ m i=1
By the triangle inequality, this implies

m
1 X

vÃÇ1i ‚àí v1 ‚àí (Œª1 I ‚àí X)‚Ä†

m
i=1

! 
m
m

c 1 X
1 X

(XÃÇi ‚àí X) v1  ‚â§ 2 ¬∑
kXÃÇi ‚àí Xk2 ,

m i=1
Œ¥ m i=1

and therefore (as kv1 k = 1),




m
m
m
1 X


  X

c 1 X



i
2
‚Ä†  1

vÃÇ1 ‚àí v1  ‚â§ 2 ¬∑
kXÃÇi ‚àí Xk + (Œª1 I ‚àí X) ¬∑ 
XÃÇi ‚àí X .

m
m


Œ¥
m
i=1
i=1
i=1

(17)

Since
X has an eigengap of Œ¥, it follows that the minimal non-zero eigenvalue of Œª1 I ‚àí X is at least Œ¥, and therefore

(Œª1 I ‚àí X)‚Ä†  ‚â§ 1/Œ¥. As to the other terms, recall that XÃÇi is the average of n i.i.d. matrices with mean X, and 1 Pm XÃÇi
i=1
m
is the average of mn such i.i.d. matrices. Thus, by a matrix Hoeffding inequality (Theorem 1) and a union bound, it holds
with probability at least 1 ‚àí p that
r
b2 log(2dm/p)
‚àÄi, kXÃÇi ‚àí Xk ‚â§ c1
n
as well as


r
m

1 X
b2 log(2dm/p)


XÃÇi ‚àí X ‚â§ c1


m
mn
i=1
for some constant c1 . Combining
 2  this with (16) using a union bound, and plugging into (17), it follows that with probability
at least 1 ‚àí p ‚àí d exp ‚àí cŒ¥0 bn2 ,


r
m

1 X
cc21 b2 log(2dm/p)
b2 log(2dm/p)


i
+ c1
.
vÃÇ1 ‚àí v1  ‚â§

2

m
Œ¥ n
Œ¥ 2 mn
i=1
Slightly simplifying, the result follows.
A.4. Proof of Theorem 4
Proof of Thm. 4. The proof is an easy consequence of Lemma 3. Assuming the events in the lemma occur, we have that
the leading eigenvalues of X as well as XÃÇi for all i are simple, hence the leading eigenvectors are all unique up to a sign.
In particular, let v1 be the eigenvector closest to wÃÉ1 = wÃÇ1 , with ties broken arbitrarily, so that kwÃÇ1 ‚àí v1 k ‚â§ kwÃÇ1 + v1 k.
This implies that wÃÇ1 = vÃÇ11 (where vÃÇ11 is as defined in Lemma 3), since otherwise, by the inequality above,
p we would get
k ‚àí vÃÇ11 ‚àí v1 k ‚â§ k ‚àí vÃÇ11 + v1 k, which implies in turn hvÃÇ11 , v1 i ‚â§ 0, contradicting the fact that kvÃÇ11 ‚àí v1 k = 2 ‚àí 2hvÃÇ1 , v1 i
is at most 1/4 by Lemma 3.
Having established that wÃÇ1 = vÃÇ11 , we note that by Lemma 3 and the triangle inequality, for any i > 1,
kvÃÇ1i ‚àí vÃÇ11 k ‚â§

1
1
and therefore kvÃÇ1i ‚àí wÃÇ1 k ‚â§ .
2
2

As vÃÇ1i , wÃÇ1 are unit vectors, this implies that kvÃÇ1i ‚àí wÃÇ1 k < k ‚àí vÃÇ1i ‚àí wÃÇ1 k. Since for any i, we have wÃÇi ‚àà {‚àívÃÇ1i , vÃÇ1i }, with
i
i
the sign chosen based on which vector is closest to wÃÇ1 , it follows
 that wÃÇi = vÃÇ1 for all i. Applying Lemma 3 with wÃÇi = vÃÇ1 ,
2
2
we get that with probability at least 1 ‚àí p ‚àí d exp ‚àíŒ¥ n/cb ,
m
1 X



wÃÇi ‚àí v1 

m i=1

‚â§ c0

 b2 log(2dm/p)

r
+

Œ¥2 n
b2 log(2dm/p) 
.
Œ¥ 2 mn

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

Squaring both sides and using the fact that (x + y)2 ‚â§ 2x2 + 2y 2 , we get that
m
1 X
2


wÃÇi ‚àí v1 

m i=1

‚â§

2(c0 )2

 b4 log2 (2dm/p)

Œ¥ 4 n2
b2 log(2dm/p) 
+
.
(18)
Œ¥ 2 mn

This holds with probability at least 1 ‚àí p ‚àí d exp ‚àíŒ¥ 2 n/cb2 . To simplify things a bit, note that we can assume
d exp(‚àíŒ¥ 2 n/cb2 ) ‚â§ p without loss of generality, since otherwise the bound in the displayed equation above is at least
a constant and therefore trivially true (holds with probability 1) if we make the constant c0 sufficiently large. Therefore, we
can argue that (18) (with an appropriate c0 ) holds with probability at least 1 ‚àí 2p. Absorbing the 2 factor into the p term,
slightly increasing c0 appropriately, and simplifying a bit, the result finally follows from the simple observation that

1
2 ‚àí kw ‚àí v1 k2 ‚â•
2
m
m

1 X
2 
1 X 
1

2


2 ‚àí 2w ‚àí
wÃÇi  ‚àí 2
wÃÇi ‚àí v1 
2
m i=1
m i=1
m
1 X
2


wÃÇi ‚àí v1  ,
‚â• 1 ‚àí 2
m i=1

(v1> w)2 =

where the first inequality follows from the triangle inequality and the inequality (a + b)2 ‚â§ P
2a2 + 2b2 , and the second
m
1
inequality follows since v1 is a unit vector, and by definition, w is the unit vector closest to m i=1 wÃÇi .
A.5. Proofs of Theorem 5
The proof is a combination of the following two lemmas, each proves one of the lower bounds. We first state the two
lemmas and then prove them.
Lemma 8. For any Œ¥ ‚àà (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) such that the
covariance matrix has eigengap Œ¥, and for any number of machines m and per-machine sample size n, the aggregated
Pm (i)
1
vector vÃÑ1 = m
i=1 vÃÇ1 (even after sign fixing) satisfies





1
1
vÃÑ1
2
, e1 i
= ‚Ñ¶ min
,
.
E 1‚àíh
kvÃÑ1 k
m Œ¥ 2 mn
Lemma 9. For any Œ¥ ‚àà (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) with eigengap
Œ¥ in the covariance matrix, such that for any number of machines m and for per-machine sample size any n sufficiently
Pm (i)
1
larger than 1/Œ¥ 2 , the aggregated vector vÃÑ1 = m
i=1 vÃÇ1 (even after sign fixing with the population eigenvector v1 )
satisfies




1
vÃÑ1
2
E 1‚àíh
, e1 i
= ‚Ñ¶
.
kvÃÑ1 k
Œ¥ 4 n2
proof of Lemma 8. We will prove the result for d = 2 (i.e. a distribution in R2 ). This is without loss of generality, since
we can always embed the distribution below in Rd for any d > 2 (say, by having all coordinates other than the first two
identically zero).
‚àö
Consider the distribution defined by the random vector x = 1 + Œ¥e1 +œÉe2 , where œÉ is uniformly distributed on {‚àí1, +1},
and e1 = (1, 0), e2 = (0, 1) are the standard basis vectors. Clearly, the population covariance matrix is


1+Œ¥ 0
X := E[xx> ] =
,
0
1
with a leading eigenvector (1, 0). Let us now consider the distribution of the output of a machine i. Given n samples, the
empirical covariance matrix is


n
‚àö
1X
1 + Œ¥ yn
XÃÇ(n) =
, yn := 1 + Œ¥ ¬∑
i ,
yn
1
n i=1

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

where i are i.i.d. and uniformly distributed on {‚àí1, +1}. Using a standard formula for the leading eigenvector of a 2 √ó 2
matrix (eig), we have that the leading eigenvector (and hence the output of any machine i) is of the form
!
r
1
Œ¥2
Œ¥
2
vÃÇ1 =
uÃÇ where uÃÇ :=
+
+ yn , yn .
(19)
kuÃÇk
2
4
Note that with this formula, the leading eigenvector is always closer to (1, 0) than (‚àí1, 0), and converges to (1, 0) as
n ‚Üí ‚àû. Thus, we can view the random variable vÃÇ(i) as the output of any machine i, given n samples and after fixing the
sign.
Pm (i)
1
Consider now the average of m such vectors given by vÃÑ = m
i=1 vÃÇ1 . Using (19), we have that
Ô£Æ
!2 Ô£π
m
m
X
X
1
1
(i)
(i)
Ô£ª= 1
vÃÇ2
E[(vÃÇ2 )2 ] = E[(vÃÇ(2))2 ]
E[vÃÑ1 (2)2 ] = E Ô£∞
m i=1
m2 i=1
m
Ô£π
Ô£Æ
yn2
1 Ô£∞
Ô£ª.
q
(20)
E
=
m
Œ¥2
Œ¥2
2
2
+ 2y + Œ¥
+y
n

2

4

n

By definition of yn and recalling that Œ¥ ‚àà [0, 1], we have that there exist universal constants c1 , c2 > 0 such that with
constant probability it holds that c1 /n ‚â• yn2 ‚â• c2 /n. Using this fact and considering the two cases 1/n ‚â• Œ¥ 2 and
1/n < Œ¥ 2 in the RHS of Eq. (20) separately, we can see that


1
1
E[vÃÑ1 (2)2 ] = ‚Ñ¶
min{1, 2 } .
(21)
m
Œ¥ n
Using Eq. (21) we have that


vÃÑ1
2
, e1 i
E h
kvÃÑ1 k




vÃÑ1 (1)2
vÃÑ1 (2)2
= E
=1‚àíE
vÃÑ1 (1)2 + vÃÑ1 (2)2
vÃÑ1 (1)2 + vÃÑ1 (2)2





1
1
2
‚â§ 1 ‚àí E vÃÑ1 (2) = 1 ‚àí ‚Ñ¶ min
,
,
m Œ¥ 2 mn


where the inequality follows since kvÃÑ1 k ‚â§ 1.

proof of Lemma 9. As in Lemma 8, we prove the result for d = 2, however, using a different construction. Consider the
defined by the random vector
‚àö
x = 1 + Œ¥ ¬∑ e1 + Œæ ¬∑ e2 ,
where Œæ is an independent random variable defined as:
 ‚àö
2 ‚àö
Œæ=
‚àí1/ 2

w.p. 1/3
w.p. 2/3

‚àö
It is easy to verify that E[Œæ] = 0, E[Œæ 2 ] = 1, E[Œæ 3 ] = 1/ 2. As we shall see, choosing Œæ to be asymmetric (as opposed to 
in the proof of Lemma 9) will be key to our construction. Clearly, the population covariance and the empirical covariance
of a sample of size n are given by we have




1+Œ¥ 0
1 + Œ¥ yn
X = E[xx> ] =
,
XÃÇ(n) =
,
0
1
yn
zn
where
yn :=

‚àö

n

1+Œ¥¬∑

1X
Œæi ,
n i=1

n

zn :=

1X 2
Œæ ,
n i=1 i

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

with Œæ1 , . . . , Œæn being i.i.d. copies of the random variable Œæ.
(1)

(m)

Clearly the leading eigenvector of X is e1 = (1, 0). Consider now vÃÇ1 , . . . , vÃÇ1 to be the leading eigenvectors of m i.i.d.
(1)
m)
empirical covariance matrices of n samples, XÃÇ(n) , . . . , XÃÇ(n) , and let vÃÑ1 denote their average after sign-fixings according
to the leading eigenvector of the population covariance e1 . In the following, we let vÃÇji denote the jth coordinate in the
(i)

eigenvector vÃÇ1 .
It holds that


vÃÑ1
2
E h
, e1 i
kvÃÑ1 k




vÃÑ1 (1)2
vÃÑ1 (2)2
= E
=1‚àíE
vÃÑ1 (1)2 + vÃÑ1 (2)2
vÃÑ1 (1)2 + vÃÑ1 (2)2
Ô£Æ
!2 Ô£π
m
X


1
‚â§ 1 ‚àí E vÃÑ1 (2)2 = 1 ‚àí E Ô£∞
sign(vÃÇ1i )vÃÇ2i Ô£ª
m i=1


!2
m

1 X 
i i
‚â§ 1‚àí
E sign(vÃÇ1 )vÃÇ2
m i=1
2
= 1 ‚àí E[sign(vÃÇ11 )vÃÇ21 ] ,

(22)

where the first inequality follows since kvÃÑ1 k ‚â§ 1, the second inequality follows from Jensen‚Äôs inequality, and the last
(1)
(m)
equality follows from the fact that vÃÇ1 , . . . , vÃÇ1 are i.i.d. random variables. From this chain of inequalities, it follows
2
that it is enough to lower bound E[sign(vÃÇ11 )vÃÇ21 ] , where vÃÇ1 is the leading eigenvector computed by machine 1.
Let us now consider the distribution of the leading eigenvector of the empirical covariance matrix XÃÇ(n) . Using a standard
formula for the leading eigenvector of a 2 √ó 2 matrix (eig), we have that this leading eigenvector vÃÇ1 is proportional to
Ô£´
Ô£∂
s
2
Œ¥
+
1
‚àí
z
Œ¥
+
1
‚àí
z
n
n
Ô£≠
+
+ yn2 , yn Ô£∏
(23)
2
2
Assume for now that zn ‚â§ 1 + cŒ¥ for some positive constant c to be fixed later (note this happens with arbitrarily high
probability as n ‚Üí ‚àû, as zn converges to 1 in probability). In that case, the sign of the first coordinate in the formula
above is positive, and has the same sign as the first coordinate of the leading eigenvector v1 = (1, 0). Moreover, we know
(1)
that vÃÇ1 must have unit norm, from which follows that


q

Œ¥+1‚àízn 2
Œ¥+1‚àízn
2 , y
+
+
y
n
n
2
2
(1)
sign(vÃÇ11 ) ¬∑ vÃÇ1 = s
(24)

2 .
q

2
Œ¥+1‚àíz
Œ¥+1‚àíz
n
n
yn2 +
+
+ yn2
2
2
In particular, letting rn = 1 ‚àí zn , we have that if rn ‚â• ‚àícŒ¥, then
sign(vÃÇ11 ) ¬∑ vÃÇ21 = s


yn2

+

= v
u
u
ty 2 +
n

Œ¥+rn
2

+

yn
q


Œ¥+rn 2
2

2
+

yn2

yn

Œ¥+rn 2
2

r
1+

1+



2yn
Œ¥+rn

2

!2 .

(25)

Towards using Eq. (22) to derive the lower bound, the main step is to bound the expectation of the RHS of Eq.(25) away
from zero. To get an intuition why this is possible, observe that when n ‚Üí ‚àû (in particular, when it is significantly larger
than 1/Œ¥ 2 ), it holds that
yn
RHS of (25) ‚âà p
,
2
yn + Œò(Œ¥ 2 )

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

since in this regime, with high probability, rn << Œ¥ and yn << 1. Now comes to play our choice of Œæ to be an asymmetric
random variable. If, just for sake of intuition, we set n = 1, it is easy to verify that despite the fact that E[yn ] = 0, it holds
that
"
#
#
"
Œæ
yn
=E p
< 0.
E p
yn2 + Œò(Œ¥ 2 )
Œæ 2 + Œò(Œ¥ 2 )
Note in particular that taking Œæ to be uniformly distributed on {‚àí1, +1}, as in Lemma 8, will still give zero expectation,
and hence will not work. We now formalize this intuition. We will use a Taylor expansion of the formula above, in order to

2
bound its expectation (over yn , rn ), from which a lower bound on E sign(vÃÇ11 ) ¬∑ vÃÇ21
would follow. To that end, define
the function
tyn
g(t) = v
!2 , t ‚àà [0, 1],
u
r


u
2

t(ty )2 + Œ¥+trn 2 1 + 1 + 2tyn
n
2
Œ¥+trn
and note that g(1) equals sign(vÃÇ11 ) ¬∑ vÃÇ21 as defined above. By a Taylor expansion, we have
1
s3
sign(vÃÇ11 ) ¬∑ vÃÇ21 = g(1) = g(0) + g 0 (0) + g 00 (0) + g 000 (s)
2
6
5
for some s ‚àà [0, 1]. A tedious calculation of g‚Äôs derivatives reveals that this implies


yn
rn yn
|yn |3 + |rn |3
1
1
sign(vÃÇ1 ) ¬∑ vÃÇ2 =
‚àí 2 ¬±O
,
Œ¥
Œ¥
Œ¥3

(26)

assuming max{|yn |, |rn |} ‚â§ cŒ¥ for some constant c (hence fixingc we used 
in our earlier assumptions on rn , zn ). To
3
3
n|
be the expression on the right-hand side
simplify notation, let qn = sign(vÃÇ11 ) ¬∑ vÃÇ21 , let bn = yŒ¥n ‚àí rnŒ¥y2 n ¬± O |yn | Œ¥+|r
3
of the equation above, and let A be the event that max{|yn |, |rn |} ‚â§ cŒ¥ indeed holds. Also, note that with probability 1,
|qn | ‚â§ 1 and |bn | = O(1/Œ¥ 3 ). Thus, by Eq. (26), we have that E[qn |A] = E[bn |A], and therefore
E[qn ] = Pr(¬¨A) ¬∑ E[qn |¬¨A] + Pr(A) ¬∑ E[qn |A]
= Pr(¬¨A) ¬∑ E[qn |¬¨A] + Pr(A) ¬∑ E[bn |A]
= Pr(¬¨A) ¬∑ E[qn |¬¨A] + E[bn ] ‚àí Pr(¬¨A) ¬∑ E[bn |¬¨A]

= E[bn ] ¬± O Pr(¬¨A)/Œ¥ 3 .
Plugging back the definitions of qn , bn , A, we get that







yn
rn yn
|yn |3 + |rn |3
1
E sign(vÃÇ11 ) ¬∑ vÃÇ21 = E
‚àí 2 ¬±O
¬±
O
Pr(max{|y
|,
|r
|}
>
cŒ¥)
.
n
n
Œ¥
Œ¥
Œ¥3
Œ¥3
‚àö
Pn
Pn
Recalling that yn = 1 + Œ¥ ¬∑ n1 i=1 Œæi and rn = 1 ‚àí zn = 1 ‚àí n1 i=1 Œæi2 , where Œæi are i.i.d. copies of a zero-mean,
‚àö
bounded random variable satisfying E[Œæ 3 ] = 1/ 2, and using Hoeffding‚Äôs inequality, it is easily verified that the above
equals




‚àö
1
1
1
2
0 + 1 + Œ¥‚àö
¬±O
¬±O
exp(‚àí‚Ñ¶(nŒ¥ )) ,
Œ¥3
(Œ¥ 2 n)3/2
2Œ¥ 2 n


2

which is ‚Ñ¶ Œ¥21n assuming n is sufficiently larger than 1/Œ¥ 2 . As a result, we get that E sign(vÃÇ11 ) ¬∑ vÃÇ21
= ‚Ñ¶ Œ¥41n2 as
required.

B. Proofs Omitted from Section 4
B.1. Proof of Lemma 4
Proof. Let ‚Ä¢ denote the standard inner product for matrices, i.e., A ‚Ä¢ B = Tr(AB> ). It holds that
(w> v1 )2

5

ww> ‚Ä¢ v1 v1> ‚â• vÃÇ1 vÃÇ1> ‚Ä¢ v1 v1> ‚àí kww> ‚àí vÃÇ1 vÃÇ1> kF ¬∑ kv1 v1> k
q
‚àö
= (w> v1 )2 ‚àí 2(1 ‚àí 1(w> vÃÇ1 )2 ) ‚â• (w> v1 )2 ‚àí 2.

=

Using MATLAB‚Äôs symbolic math toolbox together with some straightforward manual calculations

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

B.2. Proof of Lemma 6
Proof. Observe that C = M + (XÃÇ ‚àí XÃÇ1 ) + ¬µI. Thus, by our assumption on ¬µ it follows that
M + 2¬µI  C  M.

(27)

Since FÃÉŒª,w (y) is twice differentiable, in order to bound its smoothness and strong-convexity parameters, it suffices to
upper bound the largest eigenvalue and lower bound the smallest eigenvalue of its Hessian, respectively.
The Hessian of FÃÉŒª,w (y) is given by ‚àá2 FÃÉŒª,w (y) = C‚àí1/2 MC‚àí1/2 .
From Eq. (27) it follows that we can write M = C ‚àí ‚àÜ where ‚àÜ  0.
Thus we have that
Œª1 (C‚àí1/2 MC‚àí1/2 ) = Œª1 (C‚àí1/2 (C ‚àí ‚àÜ)C‚àí1/2 ) ‚â§ Œª1 (I) = 1,

(28)

where the inequality follows since C‚àí1/2 ‚àÜC‚àí1/2 is positive semidefinite.
Since M, C are invertible and positive definite, Eq. (27) implies that
M‚àí1  C‚àí1  (M + 2¬µI)‚àí1 .

(29)

Thus we have that
Œªd (C‚àí1/2 MC‚àí1/2 )

= Œªd (M1/2 C‚àí1/2 C‚àí1/2 MC‚àí1/2 C1/2 M‚àí1/2 ) = Œªd (M1/2 C‚àí1 M1/2 )
Œªi (M)
}
‚â• Œªd (M1/2 (M + 2¬µI)‚àí1 M1/2 ) = min{
i‚àà[d] Œªi (M) + 2¬µ
=

Œªd (M)
Œª ‚àí ŒªÃÇ1
,
=
Œªd (M) + 2¬µ
(Œª ‚àí ŒªÃÇ1 ) + 2¬µ

(30)

where the first equality follows from matrix similarity and the fact that M, C are invertible, and the first inequality follows
from Eq. (29).
To prove the second part of the lemma we observe that
kzÃÉ ‚àí M‚àí1 wk

= kC‚àí1/2 yÃÉ ‚àí C‚àí1/2 C1/2 M‚àí1 wk ‚â§ kC‚àí1/2 k ¬∑ kyÃÉ ‚àí C1/2 M‚àí1 wk
1
‚â§ q
kyÃÉ ‚àí C1/2 M‚àí1 wk,
Œª ‚àí Œª1 (XÃÇ)

where the second inequality follows from Eq. (29).
Finally, the last part of the lemma follows from a direct application of Theorem 1 to upper bound kX ‚àí XÃÇ1 k.
B.3. Proof of Lemma 7
Proof. Let z‚àó := (ŒªI ‚àí XÃÇ)‚àí1 w, y‚àó := C1/2 (ŒªI ‚àí XÃÇ)‚àí1 w, and recall that z‚àó and y‚àó are the global minimizers of FŒª,w (z)
and FÃÉŒª,w (y), respectively. Using the results of Lemma 6 we have that
‚àó

‚àí1/2

kzÃÉ ‚àí z k ‚â§ (Œª ‚àí ŒªÃÇ1 )

‚àó

kyÃÉ ‚àí y k ‚â§ (Œª ‚àí ŒªÃÇ1 )

‚àí1/2

s 
2 1+

2¬µ
Œª ‚àí ŒªÃÇ1



0 ,

where the second inequality follows from the strong-convexity of FÃÉŒª,w (y). Thus, it suffices to set 0 as stated in the lemma
in order to obtain the approximation guarantee for zÃÉ.

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

To upper-bound the total number of communication rounds required to obtain yÃÉ with the guarantee prescribed in the lemma,
we note that both the conjugate gradient method and Nesterov‚Äôs accelerated gradient method require
!
r
Œ≤
‚àó
0
O
ln (ky k/ )
(31)
Œ±
calls to the first-order oracle of FÃÉŒª,w (y) to obtain yÃÉ satisfying FÃÉŒª,w (yÃÉ) ‚àí miny‚ààRd FÃÉŒª,w (y) ‚â§ 0 , where Œ± and Œ≤ are
the strong-convexity and smoothness parameters of FÃÉŒª,w , respectively, and assuming w.l.o.g. that the initial iterate is
y0 = ~0. Thus, by our construction of a distributed first-order oracle given in Algorithm 2, we have that the total number
of communication rounds is upper bounded by (31). The lemma now follows from noticing that by Lemma 6 we have that
2¬µ
and that
Œ≤/Œ± = 1 + Œª‚àí
ŒªÃÇ1


ky‚àó k = kC1/2 (ŒªI ‚àí XÃÇ1 )wk ‚â§ Œª1 (C1/2 )(Œª ‚àí ŒªÃÇ1 )‚àí1 kwk = O kwk/(Œª ‚àí ŒªÃÇ1 ) .

B.4. Proof of Theorem 6
Proof. Under our assumption that mn = ‚Ñ¶(Œ¥ ‚àí2 ln(d/p)), the following three events all hold with probability at least 1 ‚àí p
(each of which holds w.p. at least 1 ‚àí p/3):
1. the output wf satisfies (wf> vÃÇ1 )2 ‚â• 1 ‚àí  (holds w.p. 1 ‚àí p/3 by applying Lemma 5 with our choice of parameters)
2. Œ¥ÃÇ = Œò(Œ¥) (by applying Theorem 1)
3. kXÃÇ ‚àí XÃÇ1 k ‚â§ ¬µ, where ¬µ is as prescribed in the Theorem (by applying Theorem 1)
The approximation guarantee of wf follows directly from Lemma 5. It thus remains to upper-bound the number of matrixvector products. Thus, combining Lemmas 5 and 7 we have that when using either the conjugate gradient method or
Nesterov‚Äôs accelerated method to approximately solve the linear systems in Algorithm 1, as prescribed in Lemma 7, the
total number of distributed matrix-vector products with XÃÇ is:
r

 

!!
 
d
(1 + 2¬µ/Œ¥)
2¬µ
d
‚àí1
¬∑
ln Œ¥ ln
+ ln
=
1+
O ln
p
Œ¥
p
Œ¥Àú

r

 
  

 !
2¬µ
d
d
(1 + 2¬µ/Œ¥)
1
‚àí1 2
O
1+
ln Œ¥ ln
+ ln
ln
+ ln
=
Œ¥
p
p
Œ¥
Àú
r

  

 
   !
2¬µ
d
(1
+
2¬µ/Œ¥)
1
d
d
O
1+
ln Œ¥ ‚àí1 ln2
+ ln
ln
+ ln2
ln
,
Œ¥
p
p
Œ¥
p
Œ¥
where the first term in the O(¬∑) in the first row accounts for the total number of instances of FŒª,w (z) needs to be solved,
given by the bound in Lemma 5, and the second term in the first row accounts for the communication-complexity of solving
each such instance according to Lemma 7. Additionally, we have used Lemma 5 to lower bound Œª ‚àí ŒªÃÇ1 = ‚Ñ¶(Œ¥ÃÇ), and Àú()
is as prescribed in Algorithm 1. Finally, we have upper-bounded ln(kwk), in all instances of FŒª,w (z) solved throughout
the run of the algorithm, by noticing that in all of them it holds that

 
 

d
,
ln(kwk) = O ln Œª(s) ‚àí ŒªÃÇ1 )‚àí max{m1 ,m2 }
= O ln Œ¥ ‚àí1 ln
p
where m1 , m2 are as prescribed in Algorithm 1, and we have used Lemma 5 again to lower bound Œª(s) ‚àí ŒªÃÇ1 = ‚Ñ¶(Œ¥).
‚àö
4 ln(3d/p)
‚àö
Finally, using Lemma 6, we can set ¬µ =
. Thus, the overall number of communication rounds is upper-bound
n
by
Ô£´s p
Ô£∂
!
p



  !
ln(d/p)
ln(d/p)
d
d
1
Ô£∏.
‚àö
‚àö
OÔ£≠
ln
ln
+ ln2
ln
p2
p2
Œ¥
Œ¥ n
Œ¥2 n

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

C. Additional Experimental Results
0.9

centralized ERM
avg. of ERMs
sign-fix avg. of ERMs.
projection avg.
avg. machine loss

0.8

0.7

avg. error

0.6

0.5

0.4

0.3

0.2

0.1

0
0

100

200

300

400

500

600

n

Figure 2. Estimation error vs. the per-machine sample size n for uniform sampling-based distribution.

D. Proof of the Davis-Kahan sinŒ∏ Theorem
We prove Theorem 2 in greater generality. In particular, Theorem 2 follows from setting k = 1 in the next theorem.
Theorem 7 (Davis-Kahan sinŒ∏ theorem). Let X, Y be symmetric real d √ó d matrices and fix k ‚àà [d]. Let VX and VY
denote d√ók matrix whose columns are the top k eigenvectors of X and the matrix whose columns are the top k eigenvectors
of Y, respectively. Also, suppose that Œ¥k (X) := Œªk (X) ‚àí Œªk+1 (X) > 0. Then it holds that
>
>
kVX VX
‚àí VY VY
kF ‚â§ 2

kX ‚àí Yk
.
Œ¥k (X)

Proof. Throughout the proof we denote the projection matrices:
>
>
>
‚ä•
>
PX := VX VX
, P‚ä•
X := I ‚àí VX VX , PY := VY VY , PY := I ‚àí VY VY ,

i.e., PX is the projection matrix onto the top k eigenvectors of X and P‚ä•
X is the projection matrix onto the lower d ‚àí k
eigenvectors, and same goes for PY , P‚ä•
Y . We also let A ‚Ä¢ B denote the standard inner products between matrices A, B.
We can write PY as
PY

‚ä•
‚ä•
‚ä•
= PX PY PX + P‚ä•
X PY PX + PX PY PX + PX PY PX .

(32)

Observe that


‚ä•
‚ä•
PX PY P‚ä•
X ‚Ä¢ X = Tr PX PY PX X = Tr PY PX XPX = 0,

(33)
P‚ä•
X XPX

where the second equality follows from the cyclic property of the trace, and the last equality follows since
0d√ód . Using Eq. (32) and (33) we have that

‚ä•
‚ä•
‚ä•
PY ‚Ä¢ X = PX PY PX ‚Ä¢ X + P‚ä•
X PY PX ‚Ä¢ X = Tr (PX PY PX X) + Tr PX PY PX X


‚ä• ‚ä•
‚ä•
‚ä•
‚ä•
= Tr (PY PX X) + Tr P‚ä•
X PY PX PX X ‚â§ Tr (PY PX X) + Tr PX PY PX ¬∑ Œª1 (PX X)

= Tr (PY PX X) + Œªk+1 (X) ¬∑ Tr P‚ä•
X PY ,

=

(34)

Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis

where the inequality follows since for any two positive semidefinite matrices A, B it holds that Tr(AB) ‚â§ Tr(A) ¬∑ Œª1 (B)
‚ä•
and the fact that P‚ä•
X X is positive semidefinite. The last equality follows since Œª1 (PX X) = Œªk+1 (X). It further holds that
PY ‚Ä¢ Y

‚â• PX ‚Ä¢ Y = Tr(PX X) + PX ‚Ä¢ (Y ‚àí X).

(35)

Subtracting Eq. (35) from Eq. (34) we have that

Tr (PY PX X) + Œªk+1 (X) ¬∑ Tr P‚ä•
X PY ‚àí Tr(PX X) ‚àí PX ‚Ä¢ (Y ‚àí X) ‚â• PY ‚Ä¢ X ‚àí PY ‚Ä¢ Y.
Rearranging we have that
Tr ((I ‚àí PY )PX X) ‚àí Œªk+1 (X) ¬∑ Tr P‚ä•
X PY



‚â§

(Y ‚àí X) ‚Ä¢ (PY ‚àí PX )

‚â§

kX ‚àí Yk ¬∑ kPX ‚àí PY kF .

(36)

It holds that
Tr ((I ‚àí PY )PX X)

= Tr (PX (I ‚àí PY )PX PX X)
‚â• Tr (PX (I ‚àí PY )PX ) ¬∑ Œªk (PX X)
= Tr (PX ‚àí PY PX )) ¬∑ Œªk (X)
(k ‚àí PX ‚Ä¢ PY ) ¬∑ Œªk (X)
Œªk (X)
kPX ‚àí PY k2F .
=
2

=

(37)

Furthermore, it holds that

1
2
Tr P‚ä•
X PY = Tr ((I ‚àí PX )PY ) = k ‚àí PX ‚Ä¢ PY = kPX ‚àí PY kF .
2

(38)

Plugging Eq. (37) and (38) into Eq. (36), we have that
1
kPX ‚àí PY k2F ¬∑ (Œªk (X) ‚àí Œªk+1 (X)) ‚â§ kX ‚àí Yk ¬∑ kPX ‚àí PY kF ,
2
which completes the proof.

(39)

