Bayesian Optimisation with Continuous Approximations

Appendix
A. Some Ancillary Material
A.1. Review of GP-UCB
We present a review of the GP-UCB algorithm of Srinivas et al. (2010) which we build on in this work. Here we will assume
f ∼ GP(0, κ) where κ : X 2 → R is a radial kernel defined on the domain X . The algorithm is given below.
Algorithm 2 GP-UCB
Input: kernel κ.
• D0 ← ∅, (µ0 , σ0 ) ← (0, κ1/2 ).

(Srinivas et al., 2010)

• for t = 1, 2, . . .
1/2
1. xt ← argmaxx∈X µt−1 (x) + βt σt−1 (x)
2. yt ← Query f at xt .
3. Perform Bayesian posterior updates to obtain µt , σt

See (1).

To present the theoretical results for GP-UCB, we begin by defining the Maximum Information Gain (MIG) which
characterises the statistical difficulty of GP bandits.
Definition 2. (Maximum Information Gain (Srinivas et al., 2010)) Let f ∼ GP(0, φX ). Consider any A ⊂ Rd and let
A0 = {x1 , . . . , xn } ⊂ A be a finite subset. Let fA0 , A0 ∈ Rn such that (fA0 )i = f (xi ) and (A0 )i ∼ N (0, η 2 ). Let
yA0 = fA0 + A0 . Denote the Shannon Mutual Information by I. The Maximum Information Gain of A is
Ψn (A) =

max

A0 ⊂A,|A0 |=n

I(yA0 ; fA0 ).

Next, we will need the following regularity conditions on the kernel. It is satisfied for four times differentiable kernels such
as the SE kernel and Matérn kernel when ν > 2 (Ghosal & Roy, 2006).
Assumption 3. Let f ∼ GP(0, κ), where κ : X 2 → R is a stationary kernel. The partial derivatives of f satisfies the
following condition. There exist constants a, b > 0 such that,


 ∂f (x) 
2


for all J > 0, and for all i ∈ {1, . . . , d}, P sup 
 > J ≤ ae−(J/b) .
∂xi
x
The following theorem is a bound on the simple regret Sn (2) for GP-UCB.
Theorem 4. ((Srinivas et al., 2010)) Let f ∼ GP(0, κ), where X = [0, 1]d , f : X → R and the kernel κ satisfies
Assumption 3). At each query, we have noisy observations y = f (x) +  where  ∼ N (0, η 2 ).Denote C1 =8/ log(1 + η −2 ).
q
 2 2
Pick a failure probability δ ∈ (0, 1) and run GP-UCB with βt = 2 log 2π3δt + 2d log t2 bdr 4ad
. The following
δ
holds with probability > 1 − δ,
r
for all n ≥ 1,

Sn ≤

C1 βn Ψn (X )
π2
+
.
n
6

A.2. Some Technical Results
Here we present some technical lemmas we will need for our analysis.
Lemma 5 (Gaussian Concentration). Let Z ∼ N (0, 1). Then P(Z > ) ≤ 21 exp(−2 /2).
Lemma 6 (Mutual Information in GP, (Srinivas et al., 2010) Lemma 5.3). Let f ∼ GP(0, κ), f : X → R and we observe
y = f (x) +  where  ∼ N (0, η 2 ). Let A be a finite subset of X and fA , yA be the function values and observations on this
set respectively. Then the Shannon Mutual Information I(yA ; fA ) is,
n

I(yA ; fA ) =

1X
2
log(1 + η −2 σt−1
(xt )).
2 t=1

Bayesian Optimisation with Continuous Approximations
2
where σt−1
is the posterior GP variance after observing the first t − 1 points.

Our next result is a technical lemma taken from Kandasamy et al. (2016a). It will be used in controlling the posterior
variance of our f and g GPs.
Lemma 7 (Posterior Variance Bound (Kandasamy et al., 2016a)). Let f ∼ (0, κ), f : U → R where κ(u, u0 ) =
κ0 φ(ku − u0 k) and φ is a radial kernel. Upon evaluating f at u we observe y = f (u) +  where  ∼ N (0, η 2 ). Let u1 ∈ U
and suppose we have s observations at u1 and no observations elsewhere. Then the posterior variance κ0 (see (1)) at all
u ∈ U satisfies,
η 2 /s
κ0 (u, u) ≤ κ0 (1 − φ2 (ku − u1 k)) +
2 .
1 + κη0 s
Proof: The proof is in Section C.0.1 of Kandasamy et al. (2016a) who prove this result as part of a larger proof.

B. Analysis
We will first state a formal version of Theorem 1. Recall from the main text where we stated that most evaluations at z• are
inside the following set Xρ .
√
Xρ = {x ∈ X : f? − f (x) ≤ 2ρ κ0 kξk∞ }.
This is not entirely accurate as it hides a dilation that arises due to a covering argument in our proofs. Precisely, we will
show that after n queries at any fidelity, BOCA will use most of the z• evaluations in Xρ,n defined below using Xρ .
Xρ,n =



x ∈ X : B2 x,

√


	
d/nα/2d ∩ Xρ,n 6= ∅

(10)

√
Here B2 (x, ) is an L2 ball of radius  centred at x. Xρ,n is a dilation of Xρ by d/nα/2d . Notice that for all α > 0, as
n → ∞, Xρ,n approaches Xρ at a polynomial rate. We now state our main theorem below.
Theorem 8. Let Z = [0, 1]p and X = [0, 1]d . Let g ∼ GP(0, κ) where κ is of the form (3). Let φX satisfy Assumption 3
with some constants a, b > 0. Pick δ ∈ (0, 1) and run BOCA with
 2 2




π t
6ad
βt = 2 log
+ 4d log(t) + max 0 , 2d log brd log
.
2δ
δ
Then, for all α ∈ (0, 1) there exists ρ, Λ0 such that with probability at least 1 − δ we have for all Λ ≥ Λ0 ,
s
s
2C1 β2nΛ Ψ2nαΛ (X )
π2
2C1 β2nΛ Ψ2nΛ (Xρ,n )
S(Λ) ≤
+
+
.
nΛ
6nΛ
n2−α
Λ
Here C1 = 8/ log(1 + η 2 ) is a constant and nΛ = bΛ/λ(z• )c. ρ satisfies ρ > ρ0 = max{2, 1 +

p
(1 + 2/α)/(1 + d)}.

In addition to the dilation, Theorem 1 in the main text also suppresses the constants and polylog terms. The next three
subsections are devoted to proving the above theorem. In Section B.1 we describe some discretisations for Z and X which
we will use in our proofs. Section B.2 gives some lemmas we will need and Section B.3 gives the proof.
B.1. Set Up & Notation
Notation: Let U ⊂ Z × X . Tn (U ) will denote the number of queries by BOCA at points (z, x) ∈ U within n time steps.
When A ⊂ Z and B ⊂ X , we will overload notation to denote Tn (A, B) = Tn (A × B). For z ∈ Z, [> z] will denote the
fidelities which are more expensive than z, i.e. [> z] = {z 0 ∈ Z : λ(z 0 ) > λ(z)}.
We will require a fairly delicate set up before we can prove Theorem 8. Let α > 0. All sets described in the rest of this
subsection are defined with respect to α. First define
√
H̃n = {(z, x) ∈ Z × X : f? − f (x) < 2ρβn1/2 κ0 ξ(z)},

Bayesian Optimisation with Continuous Approximations

where recall from (4), ξ(z) =
of H̃n in the X space, i.e.

p

1 − φ2Z (kz − z• k) is the information gap function. We next define Hn0 to be an L2 dilation

Hn0 = {(z, x) ∈ Z × X : B2 x,

√


d/nα/2d ∪ H̃n 6= ∅}.

Finally, we define Hn to be the intersection of Hn0 with all fidelities satisfying the third condition in (7). That is,
o
n
Hn = Hn0 ∩ (z, x) ∈ Z × X : ξ(z) > kξk∞ /βn1/2 .

(11)

In our proof we will use the second condition in (7) to control the number of queries in Hn .
√

To control the number of queries outside Hn we first introduce a

d
α
2n 2d

-covering of the space X of size nα/2 . If X = [0, 1]d ,
α

α

2
a sufficient covering would be an equally spaced grid having n 2d points per side. Let {ai,n }ni=1
be the points in the covering.
α
n2
Ai,n ⊂ X to be the points in X which are closest to ai,n in X . Therefore Fn = {Ai,n }i=1 is a partition of X .

Now define Qt : 2X → 2Z to be the following function which maps subsets of X to subsets of Z.
n
o
1/2 √
Qt (A) = z ∈ Z : ∀ x ∈ A, f? − f (x) ≥ 2ρβt
κ0 ξ(z) .

(12)
1/2

That is, Qt maps A ⊂ X to fidelities where the information gap ξ is smaller than (f? − f (x))/(2ρβt
we define θt : 2X → Z, to be the cheapest fidelity in Qt (A) for a subset A ∈ X .

) for all x ∈ A. Next

θt (A) = arginf λ(z).

(13)

z∈Qt (A)

We will see that BOCA will not query inside an Ai,n ∈ Fn at fidelities larger than θt (Ai,n ) too many times (see Lemma 12).
That is, Tn ([> θn (Ai,n )], Ai,n ) will be small. We now define Fn as follows,
[
Fn =
[> θn (Ai,n )] × Ai,n .
(14)
Ai,n ⊂X \Xρ,n

That is, we first choose Ai,n ’s that are completely outside Xρ,n and take their cross product with fidelities more expensive
than θt (Ai,n ). By design of the above sets, and using the third condition in (7) we can bound the total number of queries as
follows,
n = Tn (Z, X ) ≤ Tn ({z• }, Xρ,n ) + Tn (Fn ) + Tn (Hn )
We will show that the last two terms on the right hand side are small for BOCA and consequently, the first term will be large.
But first, we establish a series of technical results which will be useful in proving theorem 8.
B.2. Some Technical Lemmas
The first lemma proves that the UCB ϕt in (6) upper bounds f (xt ) on all the domain points {xt }t≥1 chosen for evaluation.
Lemma 9. Let βt > 2 log(π 2 t2 /2δ). Then, with probability > 1 − δ/3, we have
∀ t ≥ 1,

1/2

|f (xt ) − µt−1 (xt )| ≤ βt

σt−1 (xt ).

Proof: This is a straightforward argument using Lemma 5 and the union bound. At t ≥ 1,



h h
ii

1/2
1/2
P |f (x) − µt−1 (x)| > βt σt−1 (x) = E E |f (x) − µt−1 (x)| > βt σt−1 (x)  Dt−1
h

i
 −β 
2δ
t
1/2
= E PZ∼N (0,1) |Z| > βt
≤ exp
= 2 2.
2
π t
In the first step we have conditioned w.r.t Dt−1 = {(zi , xi , yi )}t−1
5 as f (x)|Dt−1 ∼
i=1 which allows us to use Lemma
P
2
N (µt−1 (x), σt−1
(x)). The statement follows via a union bound over all t ≥ 0 and the fact that t t−2 = π 2 /6.
Next we show that the GP sample paths are well behaved and that ϕt (x) upper bounds f (x) on a sufficiently dense subset at
each time step. For this we use the following lemma.

Bayesian Optimisation with Continuous Approximations

p
Lemma 10. Let βt be as given in Theorem 8. Then for all t, there exists a discretisation Gt of X of size (t2 brd 6ad/δ)d
such that the following hold.
• Let [x] be the closest point to x ∈ X in the discretisation. With probability > 1 − δ/6, we have
∀ t ≥ 1,

∀ x ∈ X,

|f (x) − f ([x]t )| ≤ 1/t2 .
1/2

• With probability > 1 − δ/3, for all t ≥ 1 and for all a ∈ Gt , |f (a) − µt−1 (a)| ≤ βt

σt−1 (a).

Proof: The first part of the proof, which we skip here, uses the regularity condition for φX in Assumption 3 and mimics the
argument in Lemmas 5.6, 5.7 of Srinivas et al. (2010). The second part mimics the proof of Lemma 9 and uses the fact that
βt > 2 log(|Gt |π 2 t2 /2δ).
The discretisation in the above lemma is different to the coverings introduced in Section B.1. The next lemma is about the
information gap function in (4).
Lemma 11. Let g ∼ GP(0, κ), g : Z × X → R and κ is of the form (3). Suppose we have s observations from g. Let
√
z ∈ Z and x ∈ X . Then τt−1 (z, x) < α implies σt−1 (x) < α + κ0 ξ(z).
Proof: The proof uses the observation that for radial kernels, the maximum difference between the variances at two points u1
and u2 occurs when all s observations are at u2 or vice versa. Now we use u1 = (z, x) and u2 = (z• , x) and apply Lemma 7
2
2
2
2
to obtain τt−1
(z• , x) ≤ κ0 (1 − φZ (kz• − zk))2 + η /s
. However, As τt−1
(z, x) = η /s
when all observations are at
η2
η2
1+ sκ

0

1+ sκ

0

2
2
2
2
(z, x) and noting that σt−1
(x) = τt−1
(z• , x), we have σt−1
(x) ≤ κ0 (1 − φZ (kz• − zk))2 + τt−1
(z, x). Since the above
2
2
situation characterised the maximum difference between σt−1
(x) and τt−1
(z, x), this inequality is valid for any general
observation set. The proof is completed using the elementary inequality a2 + b2 ≤ (a + b)2 for a, b > 0.

We are now ready to prove Theorem 8. The plan of attack is as follows. We will analyse BOCA after n time steps and
bound the number of plays at fidelities z 6= z• and outside Xρ,n at z• . Then we will show that for sufficiently large Λ, the
number of random plays N is bounded by 2nΛ with high probability. Finally we use techniques from Srinivas et al. (2010),
specifically the maximum information gain, to control the simple regret. However, unlike them we will obtain a tighter
bound as we can control the regret due to the sets Xρ,n and X \ Xρ,n separately.
B.3. Proof of Theorem 8
Let α > 0 be given. We invoke the sets Xρ,n , Hn , Fn in equations (10), (11), (14) for the given α. The following lemma
establishes that for any A ⊂ X , we will not query inside A at fidelities larger than θt (A) (13) too many times. The proof is
given in Section B.3.1.
Lemma 12. Let A ⊂ X which does not contain the optimum. Let ρ, βt be as given in Theorem 8. Then for all u >
max{3, (2(ρ − ρ0 )η)−2/3 }, we have


δ
1
P Tn ([> θt (A)], A) > u ≤ 2 1+4/α
π u
To bound T (Fn ), we will apply Lemma 12 with u = nα/2 on all Ai,n ∈ Fn satisfying Ai,n ⊂ X \ Xρ,n . Since Xρ ⊂ Xρ,n ,
Ai,n does not contain the optimum. As Fn is the union of such sets (14), we have for all n (larger than a constant),


P(T (Fn ) > nα ) ≤ P ∃Ai,n ⊂ X \ Xρ,n , Tn ([> θt (Ai,n )], Ai,n ) > nα/2


X
δ
1
δ 1
≤
P Tn ([> θt (Ai,n )], Ai,n ) > nα/2 ≤ |Fn | 2 α/2+2 ≤ 2 2
π n
π n
Ai,n ∈Fn
Ai,n ⊂X \Xρ,n

Now applying the union bound over all n, we get P(∀ n ≥ 1, T (Fn ) > nα ) ≤ δ/6.
Now we will bound the number of plays in Hn using the second condition in (7). We begin with the following Lemma. The
proof mimics the argument in Lemma 11 of Kandasamy et al. (2016a) who prove a similar result for GPs defined on just the
domain, i.e. f ∼ GP(0, κ) where f : X → R.

Bayesian Optimisation with Continuous Approximations

Lemma 13. Let A ⊂ Z × X and the L2 diameter of A in X be DX and that in Z be DZ . Suppose we have n evaluations
of g of which s are in A. Then for any (z, x) ∈ A, the posterior variance τ 02 satisfies,
τ 02 (z, x) ≤ κ0 (1 − φ2Z (DZ )φ2X (DX )) +

η2
.
s

Let λr = λmin /λ(z• ) where λmin = minz∈Z λ(z). If the maximum posterior variance in a certain region is smaller than
γ(z), then we will not query within that region by the second condition in (7). Further by the third condition, since we
1/2
will only query at fidelities satisfying ξ(z) > kξk∞ /βn , it is sufficient to show that the posterior variance is bounded by
κ0 kξk2∞ λ2q
r /βn at time n to prove that we will not query again in that region. For this we can construct a covering of Hn such
that 1 − φ2Z (DZ )φ2X (DX ) < 12 kξk2∞ λ2q
r /βn . For any A ⊂ Z × X , the covering number, which we denote Ωn (A) of this
construction will typically be poly-logarithmic in n (See Remark 15 below). Now if there are

2βn η 2
2
λ2q
r kξk∞ κ0

+ 1 queries inside a

κ0 kξk2∞ λ2q
r /βn .

ball in this covering, the posterior variance, by Lemma 13 will be smaller than
Therefore, we will not query
n
any further inside this ball. Hence, the total number of queries in Hn is Tn (Hn ) ≤ C2 Ωn (Hn ) λβ2q
≤ C3 vol(Hn ) polylog(n)
poly(λr )
r
for appropriate constants C2 , C3 . (Also see Remark 16).
Next, we will argue that the number of queries for sufficiently large Λ, is bounded by nΛ /2 where, recall nΛ = bΛ/λ(z• )c.
This simply follows from the bounds we have for Tn (Fn ) and Tn (Hn ).
Tn (Z \ {z• }, X ) ≤ Tn (Fn ) + Tn (Hn ) ≤ nα + O(polylog(n)).
Since the right hand side is sub-linear in n, we can find n0 such that for all n0 , n/2 is larger than the right hand side.
Therefore for all n ≥ n0 , Tn ({z• }, X ) > n/2. Since our bounds hold with probability > 1 − δ for all n we can invert the
above inequality to bound N , the random number of queries after capital Λ. We have N ≤ 2Λ/λ(z• ). We only need to
make sure that N ≥ n0 which can be guaranteed if Λ > Λ0 = n0 λ(z• ).
The final step of the proof is to bound the simple regret after n time steps in BOCA. This uses techniques that are now
standard in GP bandit optimisation, so we only provide an outline. We will need the following Lemma, whose proof is given
in Section B.3.2.
points are in {z• } × A for any A ⊂ X . Let
Lemma 14. Assume that we have queried g at n points, (zt , xt )nt=1 of which sP
2
2
(xt ) ≤ log(1+η
σt−1 denote the posterior variance of f at time t, i.e. after t − 1 queries. Then, xt ∈A,zt =z• σt−1
−2 ) Ψs (A).
Here Ψs (A) is the MIG of φX after s queries to A as given in Definition 2.
We now define the quantity Rn below. Readers familiar with the GP bandit literature might see that it is similar to the notion
of cumulative regret, but we only consider queries at z• .
Rn =

n
X
t=1
zt =z•

f? − f (xt ) =

X
zt =z•
xt ∈Xρ,n

f? − f (xt )

+

X

f? − f (xt ).

For any A ⊂ X we can use Lemmas 9, 10, and 14 and the Cauchy Schwartz inequality to obtain,
q
X
X 1
f? − f (xt ) ≤ C1 Tn (z• , A)βn ΨTn (z• ,A) (A) +
.
2
zt =z• t
zt =z•
xt ∈A

(15)

zt =z•
xt ∈X
/ ρ,n

(16)

xt ∈A

For the first term in (15), we set A = Xρ,n in (16) and use the trivial bound Tn (z• , Xρ,n ) ≤ n. For the second term we note
that {zp
and hence, Tn (z• , X \ Xρ,n ) ≤ Tn (Fn ) ≤ nα . As A ⊂ B =⇒ Ψn (A) ≤ Ψn (B), we have
• } × (X \ Xρ,n ) ⊂ Fnp
Rn ≤ C1 nβn Ψn (Xρ,n ) + C1 nα βn Ψnα (X ) + π 2 /6. Now, using the fact that N ≤ 2nΛ for large enough N we have,
RN ≤

q
q
π2
α
2C1 nΛ β2nΛ Ψ2nΛ (Xρ,n ) + 2α C1 nα
.
Λ β2nΛ Ψ2nΛ (X ) +
6

The theorem now follows from the fact that S(Λ) ≤ N1 RN by definition and that N ≥ nΛ . The failure instances arise out of
Lemmas 9, 10 and the bound on Tn (Fn ), the summation of whose probabilities are bounded by δ.

Bayesian Optimisation with Continuous Approximations

Remark 15 (Construction of covering for the SE kernel). We demonstrate that such a construction is always possible using
the SE kernel. Using the inequality e−x ≥ 1 − x for x > 0 we have,
1 − φ2X (DX )φ2Z (DZ ) <

2
2
DX
DZ
+
h2X
h2Z

where DZ , DX will be the L2 diameters of the balls in the covering. Now let h = min{hZ , hX } and choose
DX = DZ =

h kξk∞ q
λ ,
2 βn1/2 r

√
via which we have 1 − φ2Z (z)φ2X (x) < 12 ξ( p)2 λ2q
r /βn as stated in the proof. Noting that βn  log(n), using standard
d+p
q(d+p)
results on covering numbers, we can show that the size of this covering will be log(n) 2 /λr
. A similar argument is
possible for Matérn kernels, but the exponent on log(n) will be worse.
Remark 16 (Choice of q for SE kernel). From the arguments in our proof and Remark 15, we have that the number

q(p+d+2)
d+p+2
λ(z• )
of plays in a set S ⊂ (Z × X ) is T (S) ≤ vol(S) log(n) 2
. However, we chose to work work
λmin
λmin mostly to simplify the proof. It is not hard to see that for A ⊂ X and B ⊂ Z if λ(z) ≈ λ0 for all z ∈ B, then

q(p+d+2)
d+p+2
λ(z• )
Tn (B, A) ≈ vol(B × A) log(n) 2
. As the capital spent in this region is λ0 Tn (A, B), by picking
0
λ
q = 1/(p + d + 2) we ensure that the capital expended for a certain A ⊂ X at all fidelities is roughly the same, i.e. for any
A, the capital density in fidelities z such that λ(z) < λ(θt (A)) will be roughly the same. Kandasamy et al. (2016c) showed
that doing so achieved a nearly minimax optimal strategy for cumulative regret in K-armed bandits. While it is not clear
that this is the best strategy for optimisation under GP assumptions, it did reasonably well in our experiments. We leave it to
future work to resolve this.
B.3.1. P ROOF OF L EMMA 12
For brevity, we will denote θ = θt (A). We will invoke the discretisation Gt used in Lemma 10 via which we have
ϕt ([x? ]t ) ≥ f? − 1/t2 for all t ≥ 1. Let b = argmaxx∈A ϕt (x) be the maximiser of the upper confidence bound ϕt in A at
time t. Now note that, xt ∈ A =⇒ ϕt (b) > ϕt ([x? ]t ) =⇒ ϕt (b) > f? − 1/t2 . We therefore have,

P Tn ([> θ], A) > u ≤ P ∃t : u + 1 ≤ t ≤ n, ϕt (b) > f? − 1/t2 ∧ τt−1 (θ, b) < γ(θ))
n
X

1/2
≤
P µt−1 (b) − f (b) > f? − f (b) − βt σt−1 (b) − 1/t2 ∧ τt−1 (θ, b) < γ(θ) (17)
t=u+1

We now note that
τt−1 (θ, b) < γ(θ) =⇒ σt−1 (b) < γ(θ) +

√

√
κ0 ξ(θ) ≤ 2 κ0 ξ(θ) ≤

1
1/2
βt ρ

(f? − f (b)).

√
√
The first step uses Lemma 11. The second step uses the fact that γ(θ) = κ0 ξ(θ)(λ(z)/λ(z• ))1/(p+d+2) ≤ κ0 ξ(θ) and
√
1/2
the last step uses the definition of Qt (A) in (12) whereby we have f? − f (x) ≥ 2ρβt
κ0 ξ(θ). Now plugging this back
into (17), we can bound each term in the summation by,



1/2
1/2
P µt−1 (b) − f (b) > (ρ − 1)βt σt−1 (b) − 1/t2 ≤ PZ∼N (0,1) Z > (ρ0 − 1)βt


 (ρ0 −1)2
2
2
1
(ρ0 − 1)2
1 2δ
δ
≤ exp
βt ≤
t−(ρ0 −1) (2+2d) ≤ 2 t−(ρ0 −1) (2+2d) .
(18)
2
2
2
2 π
π
√
In the first step we have used the following facts, t > u ≥ max{3, (2(ρ − ρ0 )η)−2/3 }, π 2 /2δ > 1 and σt−1 (b) > η/ t to
conclude,
s
p
 2 2
η 4 log(t)
1
π t
η
1
1
1/2
√
(ρ − ρ0 )
> 2 =⇒ (ρ − ρ0 ) · 2 log
· √ > 2 =⇒ (ρ − ρ0 )βt σt−1 (b) > 2 .
t
2δ
t
t
t
t
1/2

The second step of (18) uses Lemma 5, the third step uses the conditions on βt as given in theorem 8 and the last step
uses the fact that π 2 /2δ > 1. Now
p plug (18) back into (17). The result follows by bounding the sum by an integral and
noting that ρ0 > 2 and ρ0 ≥ 1 + (1 + 2/α)/(1 + d).

Bayesian Optimisation with Continuous Approximations

B.3.2. P ROOF OF L EMMA 14
Let As = {u1 , u2 , . . . , us } be the queries in {z• } × A in the order they were queried. Now, assuming that we have queried
g only inside {z• } × A, denote by σ̃t−1 (·), the posterior standard deviation after t − 1 such queries. Then,
X
t:xt ∈A,zt =z•

2
σt−1
(xt ) ≤

s
X

2
σ̃t−1
(ut ) ≤

t=1

s
X
t=1

η2

s
2
2
X
(ut ))
σ̃t−1
(ut )
log(1 + η −2 σ̃t−1
2
≤
≤
I(yAs ; fAs ).
2
−2 )
η
log(1
+
η
log(1
+
η −2 )
t=1

Queries outside {z• } × A will only decrease the variance of the GP so we can upper bound the first sum by the posterior
variances of the GP with only the queries in {z• } × A. The third step uses the inequality u2 /v 2 ≤ log(1 + u2 )/ log(1 + v 2 ).
The result follows from the fact that Ψs (A) maximises the mutual information among all subsets of size s.

C. Addendum to Experiments
C.1. Implementation Details
We describe some of our implementation details below.
Domain and Fidelity space: Given a problem with arbitrary domain X and Z, we mapped them to [0, 1]d and [0, 1]p by
appropriately linear transforming the coordinates.
Initialisation: Following recommendations in Brochu et al. (2010) all GP methods were initialised with uniform random
queries with Λ/10 capital, where Λ is the total capital used in the experiment. For GP-UCB and GP-EI all queries were
initialised at z• whereas for the multi-fidelity methods, the fidelities were picked at random from the available fidelities.
GP Hyper-parameters: Except in the first two experiments of Fig. 3, the GP hyper-parameters were learned after
initialisation by maximising the GP marginal likelihood (Rasmussen & Williams, 2006) and then updated every 25 iterations.
We use an SE kernel for both φX and φZ and instead of using one bandwidth for the entire fidelity space and domain, we
learn a bandwidth for each dimension separately. We learn the kernel scale, bandwidths and noise variance using marginal
likelihood. The mean of the GP is set to be the median of the observations.
Choice of βt : βt , as specified in Theorem 8 has unknown constants and tends to be too conservative in practice (Srinivas
et al., 2010). Following the recommendations in Kandasamy et al. (2015) we set it to be of the correct “order”; precisely,
βt = 0.5d log(2`t + 1). Here, ` is the effective L1 diameter of X and is computed by scaling each dimension by the inverse
of the bandwidth of the SE kernel for that dimension.
Maximising ϕt : We used the DiRect algorithm (Jones et al., 1993).
Fidelity selection: Since we only worked in low dimensional fidelity spaces, the set Zt was constructed in practice by
obtaining a finely sampled grid of Z and then filtering out those which satisfied the 3 conditions in (7). In the second
condition of (7), the threshold γ(z) can be multiplied up to a constant factor, i.e cγ(z) without affecting our theoretical
results. In practice, we started with c = 1 but we updated it every 20 iterations via the following rule: if the algorithm has
queried z• more than 75% of the time in the last 20 iterations, we decrease it to c/2 and if it queried less than 25% of the
time we increase it to 2c. But the c value is always clipped inbetween 0.1 and 20. In practice we observed that the value for
c usually stabilised around 1 and 8 although in some experiments it shot up to 20. Changing c this way resulted in slightly
better performance in practice.
C.2. Description of Synthetic Functions
The following are the synthetic functions used in the paper.
GP Samples: For the GP samples in the first two experiments of Figure 3 we used an SE kernel with bandwidth 0.1 for φX .
For φZ we used bandwidths 1 and 0.01 for the first and second experiments respectively. The function was constructed by
obtaining the GP function values on a 50 × 50 grid in the two dimensional Z × X space and then interpolating for evaluations
in between via bivariate splines. For both experiments we used η 2 = 0.05 and the cost function λ(z) = 0.2 + 6z 2 .

Bayesian Optimisation with Continuous Approximations

Currin exponential function: The domain is the two dimensional unit cube X = [0, 1]2 and the fidelity was Z = [0, 1]
with z• = 1. We used λ(z) = 0.1 + z 2 , η 2 = 0.5 and,



 
−1
2300x31 + 1900x21 + 2092x1 + 60
.
g(z, x) = 1 − 0.1(1 − z) exp
2x2
100x31 + 500x21 + 4x1 + 20

P4
P3
Hartmann functions: We used g(z, x) = i=1 (αi − αi0 (z)) exp − j=1 Aij (xj − Pij )2 . Here A, P are given below
for the 3 and 6 dimensional cases and α = [1.0, 1.2, 3.0, 3.2]. Then αi0 was set as αi0 (z) = 0.1(1 − zi ) if i ≤ p for
i = 1, 2, 3, 4. We constructed the p = 4 and p = 2 Hartmann functions for the 3 and 6 dimensional cases respectively
this way. When z = z• = 1p , this reduces to the usual Hartmann function commonly used as a benchmark in global
optimisation.
For the 3 dimensional case we used λ(z) = 0.05 + (1 − 0.05)z13 z22 , η 2 = 0.01 and,



3 10 30
3689 1170
0.1 10 35
4699 4387
−4


A=
 3 10 30 , P = 10 × 1091 8732
0.1 10 35
381 5743


2673
7470
.
5547
8828

For the 3 dimensional case we used λ(z) = 0.05 + (1 − 0.05)z13 z22 z31.5 z41 , η 2 = 0.05 and,



10
3
17 3.5 1.7 8
1312 1696 5569
0.05 10

2329 4135 8307
17
0.1
8
14
−4
 , P = 10 × 
A=
 3
2348 1451 3522
3.5 1.7 10 17 8 
17
8 0.05 10 0.1 14
4047 8828 8732

124
3736
2883
5743


8283 5886
1004 9991
.
3047 6650
1091 381

Borehole function: This function was taken from (Xiong et al., 2013). We first let,
f2 (x) =

2πx3 (x4 − x6 )

7 x3
log(x2 /x1 ) 1 + log(x2x
+
2
2 /x1 )x x8
1

f1 (x) =

x3
x5

,

5x3 (x4 − x6 )

log(x2 /x1 ) 1.5 +

2x7 x3
log(x2 /x1 )x21 x8

+

x3
x5

.

Then we define g(z, x)
=
zf2 (x) + (1 − z)f1 (x).
The domain of the function is X
=
[0.05, 0.15; 100, 50K; 63.07K, 115.6K; 990, 1110; 63.1, 116; 700, 820; 1120, 1680; 9855, 12045] and Z = [0, 1]
with z• = 1. We used λ(z) = 0.1 + z 1.5 for the cost function and η 2 = 5 for the noise variance.
Branin function: We use the following function where X = [[−5, 10], [0, 15]]2 and Z = [0, 1]3 .
g(z, x) = a(x2 − b(z1 )x21 + c(z2 )x1 − r)2 + s(1 − t(z)) cos(x1 ) + s,
where a = 1, b(z1 ) = 5.1/(4π 2 )−0.01(1−z1 ) c(z2 ) = 5/π−0.1(1−z2 ), r = 6, s = 10 and t(z3 ) = 1/(8π)+0.05(1−z3 ).
At z = z• = 1p , this becomes the standard Branin function used as a benchmark in global optimisation. We used
λ(z) = 0.05 + z13 z22 z31.5 for the cost function and η 2 = 0.05 for the noise variance.

