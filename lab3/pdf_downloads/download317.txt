Bayesian Optimisation with Continuous Approximations

Appendix
A. Some Ancillary Material
A.1. Review of GP-UCB
We present a review of the GP-UCB algorithm of Srinivas et al. (2010) which we build on in this work. Here we will assume
f âˆ¼ GP(0, Îº) where Îº : X 2 â†’ R is a radial kernel defined on the domain X . The algorithm is given below.
Algorithm 2 GP-UCB
Input: kernel Îº.
â€¢ D0 â† âˆ…, (Âµ0 , Ïƒ0 ) â† (0, Îº1/2 ).

(Srinivas et al., 2010)

â€¢ for t = 1, 2, . . .
1/2
1. xt â† argmaxxâˆˆX Âµtâˆ’1 (x) + Î²t Ïƒtâˆ’1 (x)
2. yt â† Query f at xt .
3. Perform Bayesian posterior updates to obtain Âµt , Ïƒt

See (1).

To present the theoretical results for GP-UCB, we begin by defining the Maximum Information Gain (MIG) which
characterises the statistical difficulty of GP bandits.
Definition 2. (Maximum Information Gain (Srinivas et al., 2010)) Let f âˆ¼ GP(0, Ï†X ). Consider any A âŠ‚ Rd and let
A0 = {x1 , . . . , xn } âŠ‚ A be a finite subset. Let fA0 , A0 âˆˆ Rn such that (fA0 )i = f (xi ) and (A0 )i âˆ¼ N (0, Î· 2 ). Let
yA0 = fA0 + A0 . Denote the Shannon Mutual Information by I. The Maximum Information Gain of A is
Î¨n (A) =

max

A0 âŠ‚A,|A0 |=n

I(yA0 ; fA0 ).

Next, we will need the following regularity conditions on the kernel. It is satisfied for four times differentiable kernels such
as the SE kernel and MateÌrn kernel when Î½ > 2 (Ghosal & Roy, 2006).
Assumption 3. Let f âˆ¼ GP(0, Îº), where Îº : X 2 â†’ R is a stationary kernel. The partial derivatives of f satisfies the
following condition. There exist constants a, b > 0 such that,


 âˆ‚f (x) 
2


for all J > 0, and for all i âˆˆ {1, . . . , d}, P sup 
 > J â‰¤ aeâˆ’(J/b) .
âˆ‚xi
x
The following theorem is a bound on the simple regret Sn (2) for GP-UCB.
Theorem 4. ((Srinivas et al., 2010)) Let f âˆ¼ GP(0, Îº), where X = [0, 1]d , f : X â†’ R and the kernel Îº satisfies
Assumption 3). At each query, we have noisy observations y = f (x) +  where  âˆ¼ N (0, Î· 2 ).Denote C1 =8/ log(1 + Î· âˆ’2 ).
q
 2 2
Pick a failure probability Î´ âˆˆ (0, 1) and run GP-UCB with Î²t = 2 log 2Ï€3Î´t + 2d log t2 bdr 4ad
. The following
Î´
holds with probability > 1 âˆ’ Î´,
r
for all n â‰¥ 1,

Sn â‰¤

C1 Î²n Î¨n (X )
Ï€2
+
.
n
6

A.2. Some Technical Results
Here we present some technical lemmas we will need for our analysis.
Lemma 5 (Gaussian Concentration). Let Z âˆ¼ N (0, 1). Then P(Z > ) â‰¤ 21 exp(âˆ’2 /2).
Lemma 6 (Mutual Information in GP, (Srinivas et al., 2010) Lemma 5.3). Let f âˆ¼ GP(0, Îº), f : X â†’ R and we observe
y = f (x) +  where  âˆ¼ N (0, Î· 2 ). Let A be a finite subset of X and fA , yA be the function values and observations on this
set respectively. Then the Shannon Mutual Information I(yA ; fA ) is,
n

I(yA ; fA ) =

1X
2
log(1 + Î· âˆ’2 Ïƒtâˆ’1
(xt )).
2 t=1

Bayesian Optimisation with Continuous Approximations
2
where Ïƒtâˆ’1
is the posterior GP variance after observing the first t âˆ’ 1 points.

Our next result is a technical lemma taken from Kandasamy et al. (2016a). It will be used in controlling the posterior
variance of our f and g GPs.
Lemma 7 (Posterior Variance Bound (Kandasamy et al., 2016a)). Let f âˆ¼ (0, Îº), f : U â†’ R where Îº(u, u0 ) =
Îº0 Ï†(ku âˆ’ u0 k) and Ï† is a radial kernel. Upon evaluating f at u we observe y = f (u) +  where  âˆ¼ N (0, Î· 2 ). Let u1 âˆˆ U
and suppose we have s observations at u1 and no observations elsewhere. Then the posterior variance Îº0 (see (1)) at all
u âˆˆ U satisfies,
Î· 2 /s
Îº0 (u, u) â‰¤ Îº0 (1 âˆ’ Ï†2 (ku âˆ’ u1 k)) +
2 .
1 + ÎºÎ·0 s
Proof: The proof is in Section C.0.1 of Kandasamy et al. (2016a) who prove this result as part of a larger proof.

B. Analysis
We will first state a formal version of Theorem 1. Recall from the main text where we stated that most evaluations at zâ€¢ are
inside the following set XÏ .
âˆš
XÏ = {x âˆˆ X : f? âˆ’ f (x) â‰¤ 2Ï Îº0 kÎ¾kâˆ }.
This is not entirely accurate as it hides a dilation that arises due to a covering argument in our proofs. Precisely, we will
show that after n queries at any fidelity, BOCA will use most of the zâ€¢ evaluations in XÏ,n defined below using XÏ .
XÏ,n =



x âˆˆ X : B2 x,

âˆš


	
d/nÎ±/2d âˆ© XÏ,n 6= âˆ…

(10)

âˆš
Here B2 (x, ) is an L2 ball of radius  centred at x. XÏ,n is a dilation of XÏ by d/nÎ±/2d . Notice that for all Î± > 0, as
n â†’ âˆ, XÏ,n approaches XÏ at a polynomial rate. We now state our main theorem below.
Theorem 8. Let Z = [0, 1]p and X = [0, 1]d . Let g âˆ¼ GP(0, Îº) where Îº is of the form (3). Let Ï†X satisfy Assumption 3
with some constants a, b > 0. Pick Î´ âˆˆ (0, 1) and run BOCA with
 2 2




Ï€ t
6ad
Î²t = 2 log
+ 4d log(t) + max 0 , 2d log brd log
.
2Î´
Î´
Then, for all Î± âˆˆ (0, 1) there exists Ï, Î›0 such that with probability at least 1 âˆ’ Î´ we have for all Î› â‰¥ Î›0 ,
s
s
2C1 Î²2nÎ› Î¨2nÎ±Î› (X )
Ï€2
2C1 Î²2nÎ› Î¨2nÎ› (XÏ,n )
S(Î›) â‰¤
+
+
.
nÎ›
6nÎ›
n2âˆ’Î±
Î›
Here C1 = 8/ log(1 + Î· 2 ) is a constant and nÎ› = bÎ›/Î»(zâ€¢ )c. Ï satisfies Ï > Ï0 = max{2, 1 +

p
(1 + 2/Î±)/(1 + d)}.

In addition to the dilation, Theorem 1 in the main text also suppresses the constants and polylog terms. The next three
subsections are devoted to proving the above theorem. In Section B.1 we describe some discretisations for Z and X which
we will use in our proofs. Section B.2 gives some lemmas we will need and Section B.3 gives the proof.
B.1. Set Up & Notation
Notation: Let U âŠ‚ Z Ã— X . Tn (U ) will denote the number of queries by BOCA at points (z, x) âˆˆ U within n time steps.
When A âŠ‚ Z and B âŠ‚ X , we will overload notation to denote Tn (A, B) = Tn (A Ã— B). For z âˆˆ Z, [> z] will denote the
fidelities which are more expensive than z, i.e. [> z] = {z 0 âˆˆ Z : Î»(z 0 ) > Î»(z)}.
We will require a fairly delicate set up before we can prove Theorem 8. Let Î± > 0. All sets described in the rest of this
subsection are defined with respect to Î±. First define
âˆš
HÌƒn = {(z, x) âˆˆ Z Ã— X : f? âˆ’ f (x) < 2ÏÎ²n1/2 Îº0 Î¾(z)},

Bayesian Optimisation with Continuous Approximations

where recall from (4), Î¾(z) =
of HÌƒn in the X space, i.e.

p

1 âˆ’ Ï†2Z (kz âˆ’ zâ€¢ k) is the information gap function. We next define Hn0 to be an L2 dilation

Hn0 = {(z, x) âˆˆ Z Ã— X : B2 x,

âˆš


d/nÎ±/2d âˆª HÌƒn 6= âˆ…}.

Finally, we define Hn to be the intersection of Hn0 with all fidelities satisfying the third condition in (7). That is,
o
n
Hn = Hn0 âˆ© (z, x) âˆˆ Z Ã— X : Î¾(z) > kÎ¾kâˆ /Î²n1/2 .

(11)

In our proof we will use the second condition in (7) to control the number of queries in Hn .
âˆš

To control the number of queries outside Hn we first introduce a

d
Î±
2n 2d

-covering of the space X of size nÎ±/2 . If X = [0, 1]d ,
Î±

Î±

2
a sufficient covering would be an equally spaced grid having n 2d points per side. Let {ai,n }ni=1
be the points in the covering.
Î±
n2
Ai,n âŠ‚ X to be the points in X which are closest to ai,n in X . Therefore Fn = {Ai,n }i=1 is a partition of X .

Now define Qt : 2X â†’ 2Z to be the following function which maps subsets of X to subsets of Z.
n
o
1/2 âˆš
Qt (A) = z âˆˆ Z : âˆ€ x âˆˆ A, f? âˆ’ f (x) â‰¥ 2ÏÎ²t
Îº0 Î¾(z) .

(12)
1/2

That is, Qt maps A âŠ‚ X to fidelities where the information gap Î¾ is smaller than (f? âˆ’ f (x))/(2ÏÎ²t
we define Î¸t : 2X â†’ Z, to be the cheapest fidelity in Qt (A) for a subset A âˆˆ X .

) for all x âˆˆ A. Next

Î¸t (A) = arginf Î»(z).

(13)

zâˆˆQt (A)

We will see that BOCA will not query inside an Ai,n âˆˆ Fn at fidelities larger than Î¸t (Ai,n ) too many times (see Lemma 12).
That is, Tn ([> Î¸n (Ai,n )], Ai,n ) will be small. We now define Fn as follows,
[
Fn =
[> Î¸n (Ai,n )] Ã— Ai,n .
(14)
Ai,n âŠ‚X \XÏ,n

That is, we first choose Ai,n â€™s that are completely outside XÏ,n and take their cross product with fidelities more expensive
than Î¸t (Ai,n ). By design of the above sets, and using the third condition in (7) we can bound the total number of queries as
follows,
n = Tn (Z, X ) â‰¤ Tn ({zâ€¢ }, XÏ,n ) + Tn (Fn ) + Tn (Hn )
We will show that the last two terms on the right hand side are small for BOCA and consequently, the first term will be large.
But first, we establish a series of technical results which will be useful in proving theorem 8.
B.2. Some Technical Lemmas
The first lemma proves that the UCB Ï•t in (6) upper bounds f (xt ) on all the domain points {xt }tâ‰¥1 chosen for evaluation.
Lemma 9. Let Î²t > 2 log(Ï€ 2 t2 /2Î´). Then, with probability > 1 âˆ’ Î´/3, we have
âˆ€ t â‰¥ 1,

1/2

|f (xt ) âˆ’ Âµtâˆ’1 (xt )| â‰¤ Î²t

Ïƒtâˆ’1 (xt ).

Proof: This is a straightforward argument using Lemma 5 and the union bound. At t â‰¥ 1,



h h
ii

1/2
1/2
P |f (x) âˆ’ Âµtâˆ’1 (x)| > Î²t Ïƒtâˆ’1 (x) = E E |f (x) âˆ’ Âµtâˆ’1 (x)| > Î²t Ïƒtâˆ’1 (x)  Dtâˆ’1
h

i
 âˆ’Î² 
2Î´
t
1/2
= E PZâˆ¼N (0,1) |Z| > Î²t
â‰¤ exp
= 2 2.
2
Ï€ t
In the first step we have conditioned w.r.t Dtâˆ’1 = {(zi , xi , yi )}tâˆ’1
5 as f (x)|Dtâˆ’1 âˆ¼
i=1 which allows us to use Lemma
P
2
N (Âµtâˆ’1 (x), Ïƒtâˆ’1
(x)). The statement follows via a union bound over all t â‰¥ 0 and the fact that t tâˆ’2 = Ï€ 2 /6.
Next we show that the GP sample paths are well behaved and that Ï•t (x) upper bounds f (x) on a sufficiently dense subset at
each time step. For this we use the following lemma.

Bayesian Optimisation with Continuous Approximations

p
Lemma 10. Let Î²t be as given in Theorem 8. Then for all t, there exists a discretisation Gt of X of size (t2 brd 6ad/Î´)d
such that the following hold.
â€¢ Let [x] be the closest point to x âˆˆ X in the discretisation. With probability > 1 âˆ’ Î´/6, we have
âˆ€ t â‰¥ 1,

âˆ€ x âˆˆ X,

|f (x) âˆ’ f ([x]t )| â‰¤ 1/t2 .
1/2

â€¢ With probability > 1 âˆ’ Î´/3, for all t â‰¥ 1 and for all a âˆˆ Gt , |f (a) âˆ’ Âµtâˆ’1 (a)| â‰¤ Î²t

Ïƒtâˆ’1 (a).

Proof: The first part of the proof, which we skip here, uses the regularity condition for Ï†X in Assumption 3 and mimics the
argument in Lemmas 5.6, 5.7 of Srinivas et al. (2010). The second part mimics the proof of Lemma 9 and uses the fact that
Î²t > 2 log(|Gt |Ï€ 2 t2 /2Î´).
The discretisation in the above lemma is different to the coverings introduced in Section B.1. The next lemma is about the
information gap function in (4).
Lemma 11. Let g âˆ¼ GP(0, Îº), g : Z Ã— X â†’ R and Îº is of the form (3). Suppose we have s observations from g. Let
âˆš
z âˆˆ Z and x âˆˆ X . Then Ï„tâˆ’1 (z, x) < Î± implies Ïƒtâˆ’1 (x) < Î± + Îº0 Î¾(z).
Proof: The proof uses the observation that for radial kernels, the maximum difference between the variances at two points u1
and u2 occurs when all s observations are at u2 or vice versa. Now we use u1 = (z, x) and u2 = (zâ€¢ , x) and apply Lemma 7
2
2
2
2
to obtain Ï„tâˆ’1
(zâ€¢ , x) â‰¤ Îº0 (1 âˆ’ Ï†Z (kzâ€¢ âˆ’ zk))2 + Î· /s
. However, As Ï„tâˆ’1
(z, x) = Î· /s
when all observations are at
Î·2
Î·2
1+ sÎº

0

1+ sÎº

0

2
2
2
2
(z, x) and noting that Ïƒtâˆ’1
(x) = Ï„tâˆ’1
(zâ€¢ , x), we have Ïƒtâˆ’1
(x) â‰¤ Îº0 (1 âˆ’ Ï†Z (kzâ€¢ âˆ’ zk))2 + Ï„tâˆ’1
(z, x). Since the above
2
2
situation characterised the maximum difference between Ïƒtâˆ’1
(x) and Ï„tâˆ’1
(z, x), this inequality is valid for any general
observation set. The proof is completed using the elementary inequality a2 + b2 â‰¤ (a + b)2 for a, b > 0.

We are now ready to prove Theorem 8. The plan of attack is as follows. We will analyse BOCA after n time steps and
bound the number of plays at fidelities z 6= zâ€¢ and outside XÏ,n at zâ€¢ . Then we will show that for sufficiently large Î›, the
number of random plays N is bounded by 2nÎ› with high probability. Finally we use techniques from Srinivas et al. (2010),
specifically the maximum information gain, to control the simple regret. However, unlike them we will obtain a tighter
bound as we can control the regret due to the sets XÏ,n and X \ XÏ,n separately.
B.3. Proof of Theorem 8
Let Î± > 0 be given. We invoke the sets XÏ,n , Hn , Fn in equations (10), (11), (14) for the given Î±. The following lemma
establishes that for any A âŠ‚ X , we will not query inside A at fidelities larger than Î¸t (A) (13) too many times. The proof is
given in Section B.3.1.
Lemma 12. Let A âŠ‚ X which does not contain the optimum. Let Ï, Î²t be as given in Theorem 8. Then for all u >
max{3, (2(Ï âˆ’ Ï0 )Î·)âˆ’2/3 }, we have


Î´
1
P Tn ([> Î¸t (A)], A) > u â‰¤ 2 1+4/Î±
Ï€ u
To bound T (Fn ), we will apply Lemma 12 with u = nÎ±/2 on all Ai,n âˆˆ Fn satisfying Ai,n âŠ‚ X \ XÏ,n . Since XÏ âŠ‚ XÏ,n ,
Ai,n does not contain the optimum. As Fn is the union of such sets (14), we have for all n (larger than a constant),


P(T (Fn ) > nÎ± ) â‰¤ P âˆƒAi,n âŠ‚ X \ XÏ,n , Tn ([> Î¸t (Ai,n )], Ai,n ) > nÎ±/2


X
Î´
1
Î´ 1
â‰¤
P Tn ([> Î¸t (Ai,n )], Ai,n ) > nÎ±/2 â‰¤ |Fn | 2 Î±/2+2 â‰¤ 2 2
Ï€ n
Ï€ n
Ai,n âˆˆFn
Ai,n âŠ‚X \XÏ,n

Now applying the union bound over all n, we get P(âˆ€ n â‰¥ 1, T (Fn ) > nÎ± ) â‰¤ Î´/6.
Now we will bound the number of plays in Hn using the second condition in (7). We begin with the following Lemma. The
proof mimics the argument in Lemma 11 of Kandasamy et al. (2016a) who prove a similar result for GPs defined on just the
domain, i.e. f âˆ¼ GP(0, Îº) where f : X â†’ R.

Bayesian Optimisation with Continuous Approximations

Lemma 13. Let A âŠ‚ Z Ã— X and the L2 diameter of A in X be DX and that in Z be DZ . Suppose we have n evaluations
of g of which s are in A. Then for any (z, x) âˆˆ A, the posterior variance Ï„ 02 satisfies,
Ï„ 02 (z, x) â‰¤ Îº0 (1 âˆ’ Ï†2Z (DZ )Ï†2X (DX )) +

Î·2
.
s

Let Î»r = Î»min /Î»(zâ€¢ ) where Î»min = minzâˆˆZ Î»(z). If the maximum posterior variance in a certain region is smaller than
Î³(z), then we will not query within that region by the second condition in (7). Further by the third condition, since we
1/2
will only query at fidelities satisfying Î¾(z) > kÎ¾kâˆ /Î²n , it is sufficient to show that the posterior variance is bounded by
Îº0 kÎ¾k2âˆ Î»2q
r /Î²n at time n to prove that we will not query again in that region. For this we can construct a covering of Hn such
that 1 âˆ’ Ï†2Z (DZ )Ï†2X (DX ) < 12 kÎ¾k2âˆ Î»2q
r /Î²n . For any A âŠ‚ Z Ã— X , the covering number, which we denote â„¦n (A) of this
construction will typically be poly-logarithmic in n (See Remark 15 below). Now if there are

2Î²n Î· 2
2
Î»2q
r kÎ¾kâˆ Îº0

+ 1 queries inside a

Îº0 kÎ¾k2âˆ Î»2q
r /Î²n .

ball in this covering, the posterior variance, by Lemma 13 will be smaller than
Therefore, we will not query
n
any further inside this ball. Hence, the total number of queries in Hn is Tn (Hn ) â‰¤ C2 â„¦n (Hn ) Î»Î²2q
â‰¤ C3 vol(Hn ) polylog(n)
poly(Î»r )
r
for appropriate constants C2 , C3 . (Also see Remark 16).
Next, we will argue that the number of queries for sufficiently large Î›, is bounded by nÎ› /2 where, recall nÎ› = bÎ›/Î»(zâ€¢ )c.
This simply follows from the bounds we have for Tn (Fn ) and Tn (Hn ).
Tn (Z \ {zâ€¢ }, X ) â‰¤ Tn (Fn ) + Tn (Hn ) â‰¤ nÎ± + O(polylog(n)).
Since the right hand side is sub-linear in n, we can find n0 such that for all n0 , n/2 is larger than the right hand side.
Therefore for all n â‰¥ n0 , Tn ({zâ€¢ }, X ) > n/2. Since our bounds hold with probability > 1 âˆ’ Î´ for all n we can invert the
above inequality to bound N , the random number of queries after capital Î›. We have N â‰¤ 2Î›/Î»(zâ€¢ ). We only need to
make sure that N â‰¥ n0 which can be guaranteed if Î› > Î›0 = n0 Î»(zâ€¢ ).
The final step of the proof is to bound the simple regret after n time steps in BOCA. This uses techniques that are now
standard in GP bandit optimisation, so we only provide an outline. We will need the following Lemma, whose proof is given
in Section B.3.2.
points are in {zâ€¢ } Ã— A for any A âŠ‚ X . Let
Lemma 14. Assume that we have queried g at n points, (zt , xt )nt=1 of which sP
2
2
(xt ) â‰¤ log(1+Î·
Ïƒtâˆ’1 denote the posterior variance of f at time t, i.e. after t âˆ’ 1 queries. Then, xt âˆˆA,zt =zâ€¢ Ïƒtâˆ’1
âˆ’2 ) Î¨s (A).
Here Î¨s (A) is the MIG of Ï†X after s queries to A as given in Definition 2.
We now define the quantity Rn below. Readers familiar with the GP bandit literature might see that it is similar to the notion
of cumulative regret, but we only consider queries at zâ€¢ .
Rn =

n
X
t=1
zt =zâ€¢

f? âˆ’ f (xt ) =

X
zt =zâ€¢
xt âˆˆXÏ,n

f? âˆ’ f (xt )

+

X

f? âˆ’ f (xt ).

For any A âŠ‚ X we can use Lemmas 9, 10, and 14 and the Cauchy Schwartz inequality to obtain,
q
X
X 1
f? âˆ’ f (xt ) â‰¤ C1 Tn (zâ€¢ , A)Î²n Î¨Tn (zâ€¢ ,A) (A) +
.
2
zt =zâ€¢ t
zt =zâ€¢
xt âˆˆA

(15)

zt =zâ€¢
xt âˆˆX
/ Ï,n

(16)

xt âˆˆA

For the first term in (15), we set A = XÏ,n in (16) and use the trivial bound Tn (zâ€¢ , XÏ,n ) â‰¤ n. For the second term we note
that {zp
and hence, Tn (zâ€¢ , X \ XÏ,n ) â‰¤ Tn (Fn ) â‰¤ nÎ± . As A âŠ‚ B =â‡’ Î¨n (A) â‰¤ Î¨n (B), we have
â€¢ } Ã— (X \ XÏ,n ) âŠ‚ Fnp
Rn â‰¤ C1 nÎ²n Î¨n (XÏ,n ) + C1 nÎ± Î²n Î¨nÎ± (X ) + Ï€ 2 /6. Now, using the fact that N â‰¤ 2nÎ› for large enough N we have,
RN â‰¤

q
q
Ï€2
Î±
2C1 nÎ› Î²2nÎ› Î¨2nÎ› (XÏ,n ) + 2Î± C1 nÎ±
.
Î› Î²2nÎ› Î¨2nÎ› (X ) +
6

The theorem now follows from the fact that S(Î›) â‰¤ N1 RN by definition and that N â‰¥ nÎ› . The failure instances arise out of
Lemmas 9, 10 and the bound on Tn (Fn ), the summation of whose probabilities are bounded by Î´.

Bayesian Optimisation with Continuous Approximations

Remark 15 (Construction of covering for the SE kernel). We demonstrate that such a construction is always possible using
the SE kernel. Using the inequality eâˆ’x â‰¥ 1 âˆ’ x for x > 0 we have,
1 âˆ’ Ï†2X (DX )Ï†2Z (DZ ) <

2
2
DX
DZ
+
h2X
h2Z

where DZ , DX will be the L2 diameters of the balls in the covering. Now let h = min{hZ , hX } and choose
DX = DZ =

h kÎ¾kâˆ q
Î» ,
2 Î²n1/2 r

âˆš
via which we have 1 âˆ’ Ï†2Z (z)Ï†2X (x) < 12 Î¾( p)2 Î»2q
r /Î²n as stated in the proof. Noting that Î²n  log(n), using standard
d+p
q(d+p)
results on covering numbers, we can show that the size of this covering will be log(n) 2 /Î»r
. A similar argument is
possible for MateÌrn kernels, but the exponent on log(n) will be worse.
Remark 16 (Choice of q for SE kernel). From the arguments in our proof and Remark 15, we have that the number

q(p+d+2)
d+p+2
Î»(zâ€¢ )
of plays in a set S âŠ‚ (Z Ã— X ) is T (S) â‰¤ vol(S) log(n) 2
. However, we chose to work work
Î»min
Î»min mostly to simplify the proof. It is not hard to see that for A âŠ‚ X and B âŠ‚ Z if Î»(z) â‰ˆ Î»0 for all z âˆˆ B, then

q(p+d+2)
d+p+2
Î»(zâ€¢ )
Tn (B, A) â‰ˆ vol(B Ã— A) log(n) 2
. As the capital spent in this region is Î»0 Tn (A, B), by picking
0
Î»
q = 1/(p + d + 2) we ensure that the capital expended for a certain A âŠ‚ X at all fidelities is roughly the same, i.e. for any
A, the capital density in fidelities z such that Î»(z) < Î»(Î¸t (A)) will be roughly the same. Kandasamy et al. (2016c) showed
that doing so achieved a nearly minimax optimal strategy for cumulative regret in K-armed bandits. While it is not clear
that this is the best strategy for optimisation under GP assumptions, it did reasonably well in our experiments. We leave it to
future work to resolve this.
B.3.1. P ROOF OF L EMMA 12
For brevity, we will denote Î¸ = Î¸t (A). We will invoke the discretisation Gt used in Lemma 10 via which we have
Ï•t ([x? ]t ) â‰¥ f? âˆ’ 1/t2 for all t â‰¥ 1. Let b = argmaxxâˆˆA Ï•t (x) be the maximiser of the upper confidence bound Ï•t in A at
time t. Now note that, xt âˆˆ A =â‡’ Ï•t (b) > Ï•t ([x? ]t ) =â‡’ Ï•t (b) > f? âˆ’ 1/t2 . We therefore have,

P Tn ([> Î¸], A) > u â‰¤ P âˆƒt : u + 1 â‰¤ t â‰¤ n, Ï•t (b) > f? âˆ’ 1/t2 âˆ§ Ï„tâˆ’1 (Î¸, b) < Î³(Î¸))
n
X

1/2
â‰¤
P Âµtâˆ’1 (b) âˆ’ f (b) > f? âˆ’ f (b) âˆ’ Î²t Ïƒtâˆ’1 (b) âˆ’ 1/t2 âˆ§ Ï„tâˆ’1 (Î¸, b) < Î³(Î¸) (17)
t=u+1

We now note that
Ï„tâˆ’1 (Î¸, b) < Î³(Î¸) =â‡’ Ïƒtâˆ’1 (b) < Î³(Î¸) +

âˆš

âˆš
Îº0 Î¾(Î¸) â‰¤ 2 Îº0 Î¾(Î¸) â‰¤

1
1/2
Î²t Ï

(f? âˆ’ f (b)).

âˆš
âˆš
The first step uses Lemma 11. The second step uses the fact that Î³(Î¸) = Îº0 Î¾(Î¸)(Î»(z)/Î»(zâ€¢ ))1/(p+d+2) â‰¤ Îº0 Î¾(Î¸) and
âˆš
1/2
the last step uses the definition of Qt (A) in (12) whereby we have f? âˆ’ f (x) â‰¥ 2ÏÎ²t
Îº0 Î¾(Î¸). Now plugging this back
into (17), we can bound each term in the summation by,



1/2
1/2
P Âµtâˆ’1 (b) âˆ’ f (b) > (Ï âˆ’ 1)Î²t Ïƒtâˆ’1 (b) âˆ’ 1/t2 â‰¤ PZâˆ¼N (0,1) Z > (Ï0 âˆ’ 1)Î²t


 (Ï0 âˆ’1)2
2
2
1
(Ï0 âˆ’ 1)2
1 2Î´
Î´
â‰¤ exp
Î²t â‰¤
tâˆ’(Ï0 âˆ’1) (2+2d) â‰¤ 2 tâˆ’(Ï0 âˆ’1) (2+2d) .
(18)
2
2
2
2 Ï€
Ï€
âˆš
In the first step we have used the following facts, t > u â‰¥ max{3, (2(Ï âˆ’ Ï0 )Î·)âˆ’2/3 }, Ï€ 2 /2Î´ > 1 and Ïƒtâˆ’1 (b) > Î·/ t to
conclude,
s
p
 2 2
Î· 4 log(t)
1
Ï€ t
Î·
1
1
1/2
âˆš
(Ï âˆ’ Ï0 )
> 2 =â‡’ (Ï âˆ’ Ï0 ) Â· 2 log
Â· âˆš > 2 =â‡’ (Ï âˆ’ Ï0 )Î²t Ïƒtâˆ’1 (b) > 2 .
t
2Î´
t
t
t
t
1/2

The second step of (18) uses Lemma 5, the third step uses the conditions on Î²t as given in theorem 8 and the last step
uses the fact that Ï€ 2 /2Î´ > 1. Now
p plug (18) back into (17). The result follows by bounding the sum by an integral and
noting that Ï0 > 2 and Ï0 â‰¥ 1 + (1 + 2/Î±)/(1 + d).

Bayesian Optimisation with Continuous Approximations

B.3.2. P ROOF OF L EMMA 14
Let As = {u1 , u2 , . . . , us } be the queries in {zâ€¢ } Ã— A in the order they were queried. Now, assuming that we have queried
g only inside {zâ€¢ } Ã— A, denote by ÏƒÌƒtâˆ’1 (Â·), the posterior standard deviation after t âˆ’ 1 such queries. Then,
X
t:xt âˆˆA,zt =zâ€¢

2
Ïƒtâˆ’1
(xt ) â‰¤

s
X

2
ÏƒÌƒtâˆ’1
(ut ) â‰¤

t=1

s
X
t=1

Î·2

s
2
2
X
(ut ))
ÏƒÌƒtâˆ’1
(ut )
log(1 + Î· âˆ’2 ÏƒÌƒtâˆ’1
2
â‰¤
â‰¤
I(yAs ; fAs ).
2
âˆ’2 )
Î·
log(1
+
Î·
log(1
+
Î· âˆ’2 )
t=1

Queries outside {zâ€¢ } Ã— A will only decrease the variance of the GP so we can upper bound the first sum by the posterior
variances of the GP with only the queries in {zâ€¢ } Ã— A. The third step uses the inequality u2 /v 2 â‰¤ log(1 + u2 )/ log(1 + v 2 ).
The result follows from the fact that Î¨s (A) maximises the mutual information among all subsets of size s.

C. Addendum to Experiments
C.1. Implementation Details
We describe some of our implementation details below.
Domain and Fidelity space: Given a problem with arbitrary domain X and Z, we mapped them to [0, 1]d and [0, 1]p by
appropriately linear transforming the coordinates.
Initialisation: Following recommendations in Brochu et al. (2010) all GP methods were initialised with uniform random
queries with Î›/10 capital, where Î› is the total capital used in the experiment. For GP-UCB and GP-EI all queries were
initialised at zâ€¢ whereas for the multi-fidelity methods, the fidelities were picked at random from the available fidelities.
GP Hyper-parameters: Except in the first two experiments of Fig. 3, the GP hyper-parameters were learned after
initialisation by maximising the GP marginal likelihood (Rasmussen & Williams, 2006) and then updated every 25 iterations.
We use an SE kernel for both Ï†X and Ï†Z and instead of using one bandwidth for the entire fidelity space and domain, we
learn a bandwidth for each dimension separately. We learn the kernel scale, bandwidths and noise variance using marginal
likelihood. The mean of the GP is set to be the median of the observations.
Choice of Î²t : Î²t , as specified in Theorem 8 has unknown constants and tends to be too conservative in practice (Srinivas
et al., 2010). Following the recommendations in Kandasamy et al. (2015) we set it to be of the correct â€œorderâ€; precisely,
Î²t = 0.5d log(2`t + 1). Here, ` is the effective L1 diameter of X and is computed by scaling each dimension by the inverse
of the bandwidth of the SE kernel for that dimension.
Maximising Ï•t : We used the DiRect algorithm (Jones et al., 1993).
Fidelity selection: Since we only worked in low dimensional fidelity spaces, the set Zt was constructed in practice by
obtaining a finely sampled grid of Z and then filtering out those which satisfied the 3 conditions in (7). In the second
condition of (7), the threshold Î³(z) can be multiplied up to a constant factor, i.e cÎ³(z) without affecting our theoretical
results. In practice, we started with c = 1 but we updated it every 20 iterations via the following rule: if the algorithm has
queried zâ€¢ more than 75% of the time in the last 20 iterations, we decrease it to c/2 and if it queried less than 25% of the
time we increase it to 2c. But the c value is always clipped inbetween 0.1 and 20. In practice we observed that the value for
c usually stabilised around 1 and 8 although in some experiments it shot up to 20. Changing c this way resulted in slightly
better performance in practice.
C.2. Description of Synthetic Functions
The following are the synthetic functions used in the paper.
GP Samples: For the GP samples in the first two experiments of Figure 3 we used an SE kernel with bandwidth 0.1 for Ï†X .
For Ï†Z we used bandwidths 1 and 0.01 for the first and second experiments respectively. The function was constructed by
obtaining the GP function values on a 50 Ã— 50 grid in the two dimensional Z Ã— X space and then interpolating for evaluations
in between via bivariate splines. For both experiments we used Î· 2 = 0.05 and the cost function Î»(z) = 0.2 + 6z 2 .

Bayesian Optimisation with Continuous Approximations

Currin exponential function: The domain is the two dimensional unit cube X = [0, 1]2 and the fidelity was Z = [0, 1]
with zâ€¢ = 1. We used Î»(z) = 0.1 + z 2 , Î· 2 = 0.5 and,



 
âˆ’1
2300x31 + 1900x21 + 2092x1 + 60
.
g(z, x) = 1 âˆ’ 0.1(1 âˆ’ z) exp
2x2
100x31 + 500x21 + 4x1 + 20

P4
P3
Hartmann functions: We used g(z, x) = i=1 (Î±i âˆ’ Î±i0 (z)) exp âˆ’ j=1 Aij (xj âˆ’ Pij )2 . Here A, P are given below
for the 3 and 6 dimensional cases and Î± = [1.0, 1.2, 3.0, 3.2]. Then Î±i0 was set as Î±i0 (z) = 0.1(1 âˆ’ zi ) if i â‰¤ p for
i = 1, 2, 3, 4. We constructed the p = 4 and p = 2 Hartmann functions for the 3 and 6 dimensional cases respectively
this way. When z = zâ€¢ = 1p , this reduces to the usual Hartmann function commonly used as a benchmark in global
optimisation.
For the 3 dimensional case we used Î»(z) = 0.05 + (1 âˆ’ 0.05)z13 z22 , Î· 2 = 0.01 and,
ï£®
ï£¹
ï£®
3 10 30
3689 1170
ï£¯0.1 10 35ï£º
ï£¯4699 4387
âˆ’4
ï£º
ï£¯
A=ï£¯
ï£° 3 10 30ï£» , P = 10 Ã— ï£°1091 8732
0.1 10 35
381 5743

ï£¹
2673
7470ï£º
ï£º.
5547ï£»
8828

For the 3 dimensional case we used Î»(z) = 0.05 + (1 âˆ’ 0.05)z13 z22 z31.5 z41 , Î· 2 = 0.05 and,
ï£®
ï£¹
ï£®
10
3
17 3.5 1.7 8
1312 1696 5569
ï£¯0.05 10
ï£º
ï£¯2329 4135 8307
17
0.1
8
14
âˆ’4
ï£º , P = 10 Ã— ï£¯
A=ï£¯
ï£° 3
ï£°2348 1451 3522
3.5 1.7 10 17 8 ï£»
17
8 0.05 10 0.1 14
4047 8828 8732

124
3736
2883
5743

ï£¹
8283 5886
1004 9991ï£º
ï£º.
3047 6650ï£»
1091 381

Borehole function: This function was taken from (Xiong et al., 2013). We first let,
f2 (x) =

2Ï€x3 (x4 âˆ’ x6 )

7 x3
log(x2 /x1 ) 1 + log(x2x
+
2
2 /x1 )x x8
1

f1 (x) =

x3
x5

,

5x3 (x4 âˆ’ x6 )

log(x2 /x1 ) 1.5 +

2x7 x3
log(x2 /x1 )x21 x8

+

x3
x5

.

Then we define g(z, x)
=
zf2 (x) + (1 âˆ’ z)f1 (x).
The domain of the function is X
=
[0.05, 0.15; 100, 50K; 63.07K, 115.6K; 990, 1110; 63.1, 116; 700, 820; 1120, 1680; 9855, 12045] and Z = [0, 1]
with zâ€¢ = 1. We used Î»(z) = 0.1 + z 1.5 for the cost function and Î· 2 = 5 for the noise variance.
Branin function: We use the following function where X = [[âˆ’5, 10], [0, 15]]2 and Z = [0, 1]3 .
g(z, x) = a(x2 âˆ’ b(z1 )x21 + c(z2 )x1 âˆ’ r)2 + s(1 âˆ’ t(z)) cos(x1 ) + s,
where a = 1, b(z1 ) = 5.1/(4Ï€ 2 )âˆ’0.01(1âˆ’z1 ) c(z2 ) = 5/Ï€âˆ’0.1(1âˆ’z2 ), r = 6, s = 10 and t(z3 ) = 1/(8Ï€)+0.05(1âˆ’z3 ).
At z = zâ€¢ = 1p , this becomes the standard Branin function used as a benchmark in global optimisation. We used
Î»(z) = 0.05 + z13 z22 z31.5 for the cost function and Î· 2 = 0.05 for the noise variance.

