Tensor Decomposition via Simultaneous Power Iteration
(Supplementary Material)

Po-An Wang 1 Chi-Jen Lu 1

A. Technical Lemmas
For a matrix A, let σmax (A) and σmin (A) denote its largest
and smallest singular values, respectively. Then we will
need the following lemma relating such singular values of
a matrix and its sub-matrix.
Lemma A.1. (Corollary 3.1.3 in (Hom & Johnson, 1991))
Let A and B be matrices such that B is derived from A by
deleting some of its rows and/or columns. Then σmax (A) ≥
σmax (B) and σmin (A) ≤ σmin (B).
For a matrix Z, let Z 2 = Z  Z denote the Hadamard
(entry-wise) product of Z with itself. Then we will need the
following lemma relating the singular values of matrices Z
and Z 2 .
Lemma A.2. For any matrix Z, σmin (Z 2 ) ≥ (σmin (Z))2
and σmax (Z 2 ) ≤ (σmax (Z))2 .
Proof. One can relate the singular values of the Hadamard
product Z 2 to those of the Kronecker product Z ⊗ Z.
In particular, as Z  Z can be obtain from Z ⊗ Z by
deleting some rows and columns, Lemma A.1 tells us that
σmin (Z  Z) ≥ σmin (Z ⊗ Z) and σmax (Z  Z) ≤
σmax (Z ⊗ Z). Then the lemma follows as the Kronecker
product Z ⊗Z is known to have the property that σmin (Z ⊗
Z) = (σmin (Z))2 and σmax (Z ⊗ Z) = (σmax (Z))2 .1
We will need the following two tail bounds. The first is for
the sum of the squares of independent standard normal random variables, known as the χ-square distribution, which
follows from the bound in (Laurent & Massart, 2000).
Lemma A.3. Let z1 , . . . , zL be a sequence of i.i.d. random
variables, each from the distribution N (0, 1). Then for any
1

Academia Sinica, Taiwan. Correspondence to: Po-An Wang
<poanwang@iis.sinica.edu.tw>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
1
See e.g. Theorem 4.2.12 in (Hom & Johnson, 1991) for the
case of square matrices; the extension to rectangular matrices is
straightforward.

δ ∈ (0, 1), we have





1 X 2

Pr 
zi − 1 ≥ δ  ≤ 2−Ω
L


δ2 L



.

i∈[L]

The second is the following matrix version of the Bernstein
inequality (see e.g. Theorem 1.6 in (Tropp, 2012)).
Lemma A.4. Consider a finite sequence Z1 , . . . , Zn of independent, random, matrices in Rd×k . Assume that each
random matrix satisfies E[Zi ] = 0 and kZi k ≤ R almost
surely. Define the variance parameter
 
)
( n
n
X
 X



2
>  
>
σ = max 
E[Zi Zi ] , 
E[Zi Zi ] .

 

i=1

i=1

Then, for all t ≥ 0,

" n
#
X 
−t2


Pr 
Zi  ≥ t ≤ (d + k) · 2 σ2 +Rt/3 .


i=1

We will also need the following two matrix perturbation
bounds.
Lemma A.5. (Theorem 2.5 in (Stewart & Sun, 1990))Let
A, E ∈ Rk×k be given. If A is invertible, and A−1 E  <
1 then Ā := A + E is invertible, and


 −1
 kEk A−1 2
−1
Ā − A  ≤
.
1 − kA−1 Ek
Lemma A.6. (Lemma 2.2 in (Schmitt, 1992)) Given any
A, Ā ∈ Rk×k with smallest singular values σ > 0 and
σ̄ > 0, respectively, we have

 1
 
Ā − A
1
 2
Ā − A 2  ≤ 1
1 .
σ̄ 2 + σ 2

B. Proofs in Section 3
B.1. Proof of Lemma 1
Recall that Q(t) is derived from Y (t) by the QR decomposition Y (t) = Q(t) · R(t) via the Gram-Schmidt process,

Tensor Decomposition via Simultaneous Power Iteration

which has the same effect as performing k copies of the QR
(t)
decomposition on the k sub-matrices Y[m] , for m ∈ [k], to

while using the assumption kΦ̂k ≤ 4 cos2 (Q), we can
bound the enumerator by

(t)

σmin (U > Y ) ≥ λm cos2 (Q) − 4 cos2 (Q).

obtain the k sub-matrices Q[m] , for m ∈ [k].
Let us fix any m ∈ [k] and t ≥ 0. To simply our notation, we will drop the indices of m and t in the fol(t−1)
(t)
lowing. We will write Q for Q[m] , Q0 for Q[m] , Y for
(t)

tan(Q0 ) ≤

(t)

Y[m] , and Φ̂ for Φ̂[m] . We will write U for U[m] , with
the vector u1 , . . . , um as its columns, while we will use
V to denote the d × (d − m) matrix having the vectors
um+1 , . . . , ud as its columns. We will write tan, cos, sin
for tanm , cosm , sinm , respectively. Furthermore, for a matrix A, let σmin (A) and σmax (A) denote its smallest and
largest singular values, respectively.
Recall that our goal is to bound tan(Q0 ) in terms of
tan(Q). As discussed before, Q0 is derived from Y by a
QR decomposition, with Y = Q0 R for some matrix R. To
achieve our goal, we will first show that R is invertible so
that Q0 = Y R−1 , and then relate tan(Q0 ) to the singular
values σmax (V > Y ) and σmin (U > Y ), followed by bounding these two singular values.
First, from the condition (3), we have cos Q > 0 which
implies that Q has full rank and consists of orthonormal
columns. Our key lemma is the following, which we will
prove later in Subsection B.1.1.
Lemma B.1. The following two bounds hold:

• σmin U > Y ≥ λm cos2 (Q) − kΦ̂k, and

• σmax V > Y ≤ λm+1 sin2 (Q) + kΦ̂k.
Using this lemma and the assumption kΦ̂k ≤ 4 cos2 (Q)
in (3), we have

σmin U > Y ≥ λm cos2 (Q) − 4 cos2 (Q) > 0,
as ∆ ≤ λ2m . This implies that Y has full rank, R−1 exists,
Q0 = Y R−1 has orthonormal columns, and (U > Q0 )−1 exists. Then from standard properties of principal angles (see
e.g. (Zhu & Knyazev, 2012)), we know that
tan(Q0 ) =

Combining these bounds together, we obtain


σmax (V > Q0 ) 
= (V > Q0 )(U > Q0 )−1  ,
>
0
σmin (U Q )

which equals
 >

(V Y R−1 )(U > Y R−1 )−1  =
≤

 >

(V Y )(U > Y )−1 
σmax (V > Y )
.
σmin (U > Y )

Then by Lemma B.1, together with the assumption from
(3) that kΦ̂k ≤ 4β = 4β sin2 (Q) + 4β cos2 (Q), we can
bound the denominator by
σmax (V > Y ) ≤ (λm+1 + 4β) sin2 (Q) + 4β cos2 (Q),

λm+1 + 4β
4β
tan2 (Q) +
.
λm − 4
λm − 4

Then the rest of the analysis is identical to that of Hardt &
Price (2014) (for the proof of their Lemma 2.2). Specifically, we can rewrite the righthand side above as the
weighted average of two terms
(1 − α) ·
with α =

λm+1 + 4β
tan2 (Q) + α · β,
λm+1 + 24

4
λm+1 +34 ,

which can be upper-bounded by


λm+1 + 4β
2
max
tan (Q), β ,
λm+1 + 24

and similarly, we can also have


λm+1 + 4β
λm+1
≤ max
,β .
λm+1 + 24
λm+1 + 4
λm+1
λm+1
1/4
Since λm+1
= ( λλm+1
)1/4 = ρ, we
+4 ≤ ( λm+1 +44 )
m
thus have the desired bound

	
tan(Q0 ) ≤ max max {ρ, β} tan2 (Q), β .

To finish the proof, it remains to prove Lemma B.1, which
we do next.
B.1.1. P ROOF OF L EMMA B.1
Recall from (2) that for any column Yj of Y and for any
target vector ui ,
2
>
u>
+ u>
i Yj = λi ui Qj
i Φ̂j .
These equations can be summarized as
2
U >Y = Λ U >Q
+ U > Φ̂, and

2
V > Y = Λ̄ V > Q
+ V > Φ̂,
using the notation Λ for the m × m diagonal matrix with
λ1 , . . . , λm at its diagonal, Λ̄ for the (d − m) × (d − m)
diagonal matrix with λm+1 , . . . , λd at its diagonal, and
A2 = A  A for the Hadamard (entry-wise) product of
matrix A with itself. From this, we have



2
σmin U > Y
= σmin Λ U > Q
+ U > Φ̂


2  


≥ σmin Λ U > Q
− U > Φ̂


2  
 
≥ σmin (Λ) σmin U > Q
− Φ̂ ,

Tensor Decomposition via Simultaneous Power Iteration
(t)

as well as
σmax V > Y




2
= σmax Λ̄ V > Q
+ V > Φ̂


2  


≤ σmax Λ̄ V > Q
+ V > Φ̂



2  
 
≤ σmax Λ̄ σmax V > Q
+ Φ̂ .


From Lemma A.2, we have

2 
2
σmin U > Q
≥ σmin U > Q
= cos2 (Q),
since Q has orthonormal columns, and moreover

2 
2
σmax V > Q
≤ σmax V > Q
= sin2 (Q).

As σmin Λ̄ = λm and σmax (Λ) = λm+1 , Lemma B.1
follows.
B.2. Proof of Theorem 2
Suppose we have Q(0) such that for every m ∈ [k],
tanm (Q(0) ) ≤ 1 and hence cosm (Q(0) ) ≥ √12 . We would
like to apply Lemma 1 repeatedly with β = 2ε to achieve
tanm (Q(t) ) ≤ 2ε for every m. To be able to do this, we
need to verify that for every t, the condition (3) in Lemma 1
(t)
is satisfied. For this, we first claim that kΦ̂[m] k ≤ ∆ε
2 . This
holds since for any vector x = (x1 , . . . , xm ) of unit length,



X 
X
 (t) 

(t)
(t) 
|xj | · kΦk
Φ̂[m] x ≤
xj Φ(Id , Qj , Qj ) ≤
j∈[m]

j∈[m]

Proposition
B.1. For any t ≥ N , we have u>
i Qi
q
2
ε
1 − 2 for every i ∈ [k].

Proof. Let us fix any i ∈ [k]. In the following, we first
(t) 2
ε2
show that (u>
i Qi ) ≥ 1 − 2 for any t ≥ N − 1, and then
(t)
we show that u>
≥ 0 for any t ≥ N , which together
i Qi
prove the proposition.
First, consider any t ≥ N − 1, which from the discus(t) 2
sion above has tani (Q(t) ) ≤ 2ε . Note that (u>
i Qi ) ≥
cos2i (Q(t) ) − sin2i−1 (Q(t) ), because

2

(t) 
cos2i (Q(t) ) ≤ u>
i Q[i] 

2 
2

(t) 
> (t)
= u>
i Q[i−1]  + ui Qi

2
(t)
≤ sin2i−1 (Q(t) ) + u>
.
i Qi
From this and the fact that cos2i (Q(t) ) =
sin2i−1 (Q(t) )


≤

tan2i−1 (Q(t) ),
(t)

u>
i Qi

mkxk · kΦk =

√

m · kΦk ≤

t

(t)

Yi

1
1−γ )

1
ε2
ε2
−
≥
1
−
.
2
4
2
1 + ε4

=

X


2
(t−1)
(t−1)
λj u>
· uj + Φ̂i
j Qi

j

length. Thus, the sign of u>
i Qi is the same as that of


 
(t)
(t)
u>
Yi − z > Yi
z
i



(t)
> (t)
= u>
Y
−
z
Y
u>
i i
i z
i


2 
 (t−1) 
(t−1)
≥ λi u>
Q
−
Φ̂

 − sini−1 (Q(t) )
i
i
i


ε2
5ε
≥ λi 1 −
− ,
2
8
(t−1)

≥ Ω(γ).
(t)

Next, we show that for any t ≥ N , each Qi
enough to ui . For this, we rely on the following.

is close

2

(t−1)

since (u>
)2 ≥ 1− ε2 for t−1 ≥ N −1, kΦ̂i
k≤
i Qi
∆ε
ε
ε
(t)
(t)
≤
,
and
sin
(Q
)
≤
tan
(Q
)
≤
for
t ≥
i−1
i−1
2
8
2
N . Finally, as λi ≥ λk ≥ 2ε, the last line above is positive,
(t)
which implies that u>
i Qi > 0.
(t)

≥ Ω(log

we get

(t)

for every m. Thus, we have ρ2 −1 ≤ 2ε and tanm (Q(t) ) ≤
ε
2 whenever t ≥ N − 1, for some
!
 

log 1ε
1
1
N = O log
=
O
log
log
,
γ
ε
log ρ1
1
ρ

≥

and

by subtracting from it its projection to some unit vector z
(t)
in the column space of Q[i−1] and then scaling it to unit

∆ε
.
2

Moreover, we can assume without loss of generality that
max{ 2ε , ρ} = ρ because otherwise, we immediately have
tanm (Q(1) ) ≤ 2ε for every m, as the condition (3) is satisfied for t = 1. Then a simple induction shows that for
every t ≥ 1, the condition (3) in Lemma 1 holds and
nε
o
tanm (Q(t) ) ≤ max
, ρ tan2m (Q(t−1) )
n 2ε t o
≤ max
, ρ2 −1
2

by noting that log

2

1
1+tan2i (Q(t) )

Next, consider any t ≥ N and our goal is to show that
(t)
(t)
u>
i Qi > 0. Recall that Qi is derived from

which by the Cauchy-Schwarz inequality is at most
√

≥

As kQi k = kui k = 1, this proposition immediately implies that for any t ≥ N and i ∈ [k],

 q
 (t)

(t)
Qi − ui  = 2 − 2u>
i Qi ≤ ε,

Tensor Decomposition via Simultaneous Power Iteration
(t)

(t)

>
2
as u>
i Qi ≥ (ui Qi ) ≥ 1 −

ε2
2 .

Finally, let us show that for any t ≥ N , each λi can be
(t)
(t)
(t)
approximated well by λ̂i = T̄ (Qi , Qi , Qi ). Fix any
t ≥ N and i ∈ [k]. Note that |λi − λ̂i | is at most


  



(t)
(t)
(t) 
(t)
(t)
(t) 
λi − T Qi , Qi , Qi  + Φ Qi , Qi , Qi  .
(t)

ε
4,

The second term above is at most kΦkkQi k3 ≤
the first term above is at most


3  X  
3 

(t)
> (t)
λi − λi u>



Q
+
λ
u
Q
i
i


 j j i


and

that the matrix Φ(ui , Id , Id ) has norm
PkΦ(ui , Id , Id )k ≤
∆
kΦk ≤ 3d
and can be decomposed as r∈[d] λ̃r · ũr ⊗ ũr ,
for some orthonormal vectors ũr ’s as well as some values
∆
λ̃r ’s, each with |λ̃r | ≤ 3d
. Then by a similar analysis as
above, together with a union bound, we can have with prob1
1
ability at least 1 − d · 200d
2 = 1 − 200d that




1 X


Φ(ui , wj , wj )
L
 j∈[L]


≤

r∈[d]

≤



(t)

≤ λi 1 − u>
i Qi

3 

+

X





(t)

λ j 1 − u>
i Qi

2 

≤

j6=i
2

≤ λi

3ε
+
4

X
j6=i

2

λj

ε
,
2

where the first inequality uses the fact that for j 6= i,
(t) 3
(t) 2
(t)
(t) 2
(u>
≤ (u>
≤ kQi k2 − (u>
=
j Qi )
j Qi )
i Qi )
(t)

2
1 − (u>
i Qi ) , while the second inequality uses the bound
2
2
> (t) 3
(ui Qi ) ≥ (1 − ε2 )3/2 ≥ 1 − 3ε4 . As a result, we have

B.4. Proof of Lemma 3
Consider any w̄ satisfying the condition (5) in Lemma 2.
By definition, M̄ = T̄ (Id , Id , w̄) can be decomposed as
T (Id , Id , w̄) + Φ(Id , Id , w̄).

P
using the assumption j λj ≤ 1 from (1). This completes
the proof of the theorem.

The first matrix can be expressed as
X

T (Id , Id , w̄) =
λi u>
i w̄ · ui ⊗ ui ,
i∈[d]

B.3. Proof of Lemma 2

with λ̄i = λi (u>
i w̄) and ui as its i’th eigenvalue and eigeni+1
vector, respectively. Note that as ∆ ≤ λi −λ
, we have
4

From the definition of w̄, we can express u>
i w̄ as
1 X
1 X
T (ui , wj , wj ) +
Φ(ui , wj , wj ).
L
L

(1)

j∈[L]

λ̄i

The first term above equals
2
1 X
1 X > 2
λ i u>
= λi ·
ui w j ,
i wj
L
L
j∈[L]

j∈[L]

and note that the sum has a χ-square distribution because each u>
i wj is an independent random variable with
the standard normal distribution N (0, 1).2 Then from
Lemma A.3, we know that for δ = γ4 , there exists some
L ≤ O( γ12 log d) such that the first term in (1) differs from
λi by at most

λi γ
4

with probability at least 1 −

1
200d2 .

The second term in (1) can be bounded in a similar way
as follows. Since kui k = 1 and Φ is symmetric, we know
2

∆
.
2

By combining the two bounds above, we can conclude that
for any i ∈ [d], the sum in (1) differs from λi by at most
1
1
4 (λi γ + 2∆) with probability at least 1 − 100d . Then the
lemma immediately follows by a union bound.


 X 3ε2
ε


λj
+ ≤ ε,
λi − λ̂i  ≤
4
4
j

j∈[L]

j∈[L]

X ∆ 
γ
· 1+
3d
4

r∈[d]

j6=i



X   1 X
2
ũ>
λ̃r  ·
r wj
L

This is because each component of wj has P
the distribution
N (0, 1), and the distribution of u>
i wj has mean
r ui,r · 0 = 0
P
and variance r u2i,r · 1 = 1, where ui,r denotes the r’th component of ui .


1 2
λ γ + 2λi ∆
4 i
λ2 − λ2i+1
λ2 − λi λi+1
≥ λ2i − i
− i
4
8

2
2
3
λ
−
λ
i
i+1
≥ λ2i −
,
8
≥ λ2i −

as well as
λ̄i+1


1 2
λi+1 γ + 2λi+1 ∆
4
λ2 − λ2i+1
λi+1 λi − λ2i+1
≤ λ2i+1 + λ2i+1 i
+
4λ2i
8

2
2
3
λ
−
λ
i
i+1
≤ λ2i+1 +
,
8
≤ λ2i+1 +

which together imply that
λ̄i − λ̄i+1 ≥

λ2i − λ2i+1
≥ ∆2 .
4

Tensor Decomposition via Simultaneous Power Iteration

It remains to show that we can have an initial Z (0) , such
(0)
that for each m ∈ [k], Z[m] satisfies the condition required
by Lemma B.2. For this, we need the following bound from
(Mitliagkas et al., 2013).
Proposition B.2. For any δ, we have


δ
Pr cosk (Z (0) ) ≤ √
≤ O(δ) + 2−Ω(d) .
dk

It remains to bound the norm of Φ(Id , Id , w̄), which is
kΦ(Id , Id , w̄)k ≤ kΦk · kw̄k,
where kw̄k2 =
X
i∈[d]

P

i∈[d]

u>
i w̄

2

1
λi + (λi γ + 2∆)
4

is at most

2
≤

X

2

(2λi ) ≤ 4.

i∈[d]

This implies that kΦ(Id , Id , w̄)k ≤ 2kΦk, which completes
the proof of the lemma.
B.5. Proof of Lemma 4

σmin (U > Z (0) ) = cosk (Z (0) ). Thus, with high probabil√ 0 for every m ∈ [k].
ity we in fact have cosm (Z (0) ) ≥ 10α
dk

Suppose we have a matrix M̄ = M + Φ̄, where
X
M=
λ̄i · ui ⊗ ui
i∈[d]
2

∆
with λ̄i − λ̄i+1 ≥ ∆2 for every i ∈ [k] and kΦ̄k ≤ α√1dk
for a small enough constant α1 . The key observation is that
although we run one copy of the matrix power method of
Hardt & Price (2014) to update the whole d×k matrix Z (s) ,
we can actually see our algorithm as running k copies of
(s)
(s)
the matrix power method on k sub-matrices Z[1] , . . . , Z[k]
simultaneously. This allows us to apply their analysis immediately.

More precisely, although our QR decomposition at each
step s is applied to the whole d × k matrix Y (s) to obtain our d × k matrix Z (s) , the Gram-Schmidt process we
(s)
use has the effect that each d × m sub-matrix Z[m] can also
(s)

be seen as obtained from the d × m matrix Y[m] by a QR
decomposition. Thus, our algorithm can be seen as running
k copies of the algorithm of (Hardt & Price, 2014) simultaneously, and we can apply the following lemma of theirs3
simultaneously for every m ∈ [k] with X (s) being our d×k
(s)
matrix Z[m] .
Lemma B.2. Fix any m ∈ [k]. Suppose that the initial
(s)
X (0) and the noise Gm = Φ̄ · X (s) at each step s is such
that



 > (s) 
5 U[m]
Gm  ≤ λ̄m − λ̄m+1 cosm (X (0) )





5 G(s)
λ̄m − λ̄m+1 
m  ≤
for some 

< 12 .
O( γ1m

Then for γm = 1 −

some S =
log tanm (X

we have tanm (X (t) ) ≤ .
3

(0)

)

λ̄m+1
,
λ̄m

By applying this proposition, with δ = 10α0 for a small
√ 0
enough constant α0 , we can have cosk (Z (0) ) ≥ 10α
dk
with high probability. From Lemma A.1, we know that
(0)
>
for any m ∈ [k], cosm (Z (0) ) = σmin (U[m]
Z[m] ) ≥

there exists

) such that for any t ≥ S

It corresponds to Theorem 2.3 in (Hardt & Price, 2014). Although it is stated there for m = k, it in fact works for any value
of k and hence m.

Given such an initial Z (0) , we can have for every s and m
that
10α0 ∆2
√
5kG(s)
m k ≤ 5kΦ̄k ≤
dk
which satisfies the two conditions needed by Lemma B.2,
with  = 13 . Then we can repeatedly apply Lemma B.2,
simultaneously for every m ∈ [k], and a simple induction shows that for some S = O( γ1 log d), we have
tanm (Z (s) ) ≤  < 1 for any m ∈ [k] and s ≥ S. This
completes the proof of our Lemma 4

C. Proofs in Section 5
C.1. Proof of Lemma 5
First, we claim that tank (Q) < 1 with high probability.
To show this, note that by Proposition B.2, we have with
4α0
high probability that cosk (Z) > √
for a small enough
dk
constant α0 . In the following, let us assume that we indeed
√
have such a matrix Z, and note that it has tank (Z) < 4αdk0 .
Then we need the following.
Lemma C.1. (Lemma 2.2 in (Hardt & Price, 2014)) Let
Z, G ∈ Rd×k satisfy


4 U > G ≤ (λk − λk+1 ) cosk (Z)
4 kGk

≤ (λk − λk+1 ) β

1/4
for some β < 1. Then for ρ = λλk+1
, we have
k
tank (M Z + G) ≤ max{β, max{β, ρ} tank (Z)}.
Recall that we assume λk+1 = 0, and to apply the lemma,
4α0
let β = √
and G = Φ̄Z. Note that
dk
  λk β
kGk ≤ Φ̄ ≤
,
4
which satisfies both requirements of the lemma, and thus
with Ȳ = M Z + G, we have
tank (Q) = tank (Ȳ ) ≤ β tank (Z) < 1.

Tensor Decomposition via Simultaneous Power Iteration

Next, let us bound σmin (P ) and σmax (P ). Recall that
P = Q> M Q = Q> U ΛU > Q,

1

σmin (P ) ≥ σmin
σmax (P ) ≤ σmax

>




Q U σmin (Λ) σmin U Q and


Q> U σmax (Λ) σmax U > Q .

Since the matrix Q has orthonormal columns, we have

(σmin U > Q )2 = (cosk (Q))2 =

λk
2 ,
1

and

• kP̄ − 2 − P − 2 k ≤

which implies that
>

• σmin (P ) ≥

1
1
≥ ,
2
2
1 + tank (Q)

as well as

σmax U > Q ≤ kU k kQk = 1.
Finally, as σmax (Λ) = λ1 and σmin (Λ) = λk , we have

λk ε
64 .

Assume from now on that the above three conditions hold.
Next, observe that



T̄ W̄ , W̄ , W̄ − T (W, W, W )



≤ T̄ W̄ , W̄ , W̄ − T W̄ , W̄ , W̄  +
(2)



T W̄ , W̄ , W̄ − T W, W̄ , W̄  +
(3)



T W, W̄ , W̄ − T W, W, W̄  +
(4)



T W, W, W̄ − T (W, W, W ) .
(5)
The term in (2) is at most

 

Φ W̄ , W̄ , W̄  ≤ kΦk W̄ 3 ≤ ε
4
3

λk
and σmax (P ) ≤ λ1 .
2

C.2. Proof of Lemma 6

as kΦk ≤ α0 λk2 ε for a small enough constant α0 and

 

 
1
1

W̄  ≤ kQk 
P̄ − 2  ≤ P̄ − 2  ,

First, from the definition, we have

 

 
P̄ − P  = Q> M̄ Q − Q> M Q ≤ kQk2 Φ̄ ≤ 

which can be upper-bounded by

 

1
−1
 − 12   − 12
− P − 2  ≤ 4λk 2 .
P  + P̄

as Q has orthonormal columns so that kQk2 ≤ 1. Therefore, given the assumption that 0 <  ≤ σmin2(P ) , we have


σmin (P̄ ) ≥ σmin (P ) − P̄ − P  > 0,

The term in (3) is at most


T W̄ − W, W̄ , W̄ 

σmin (P ) ≥

which implies that P̄ is invertible.
Then according to Lemma A.5, we have



P̄ − P  P −1 2
 −1

P̄ − P −1  ≤


1 − P̄ − P  kP −1 k

Similarly, the term in (4) can be upper-bounded by


  ε
kT k W̄ − W  kW k W̄  ≤
4

≤ 2(σmin (P ))−2 ,


as P −1  = (σmin (P ))−1 and kP̄ − P k ≤  ≤ σmin2(P ) .
Combining this with Lemma A.6, we obtain
 −1



P̄ − P −1 
 − 12
− 12 
−P  ≤
P̄
1
1
(σmin (P̄ −1 )) 2 + (σmin (P −1 )) 2
1

≤ 2(σmin (P ))−2 (σmax (P )) 2 ,
since σmin (P̄ −1 ) ≥ 0 and σmin (P −1 ) = (σmax (P ))−1 .

and the term in (5) can be upper-bounded by


ε
kT k W̄ − W  kW k2 ≤ .
4
As a result, we can conclude that



T̄ W̄ , W̄ , W̄ − T (W, W, W ) ≤ ε
with high probability, which proves the theorem.

D. Proofs in Section 6

C.3. Proof of Theorem 3
λ3

k
First, given ε ∈ (0, 12 ) and kΦ̄k ≤ α0 ε min{ √λdk
, √λk },
1
for a small enough constant α0 , we know from Lemma 5
and Lemma 6 that with high probability,

• σmax (P ) ≤ λ1 ,


  2
kT k W̄ − W  W̄ 


1
1

≤ P̄ − 2 − P − 2  16λ−1
k
ε
≤
.
4
≤

Our streaming algorithm for orthogonal tensors with g of
the form g(x) = x ⊗ x ⊗ x is summarized in Algorithm 2.
We will use the parameters


c0 log k
c0 log k
1
1
L=
,
S
=
,
N
=
c
log
log
0
∆2
γ
γ
ε

Tensor Decomposition via Simultaneous Power Iteration

Algorithm 2 Streaming robust tensor power method
Input: a stream of data {x1 , x2 , . . . , }, parameters
S
N
L, S, N , index sets {Bs }s=1 , {Jt }t=1 .
Initialization Phase
Let w̄ = 0 ∈ Rd .
for τ = 1 to L do
Update w̄ = w̄ + L1 xτ .
end for
(0)
(0)
Sample Y1 , . . . , Yk ∼ N d (0, 1).
Factorize Y (0) as Z (0) R(0) by QR decomposition.
for s = 1 to S do
for τ ∈ Bs do

> (s−1)
Update Y (s) = Y (s) + |B1s | x>
.
τ w̄ xτ xτ Z
end for
Factorize Y (s) as Z (s) R(s) by QR decomposition.
end for
Tensor power phase
Let Q(0) = Z (S) .
for t = 1 to N do
for τ ∈ Jt do


(t)

Update Yi

(t)

(t)

(t)

2

+ |J1t | xτ x>
, ∀i ∈ [k].
τ Qi

3
(t)
(t)
= λi + |J1t | x>
, ∀i ∈ [k].
τ Qi
= Yi

Update λi
end for
Factorize Y (t) as Q(t) R(t) by QR decomposition.
end for
(N )
(N )
Output: ûi = Qi and λ̂i = λi , ∀i ∈ [k]

for a large enough constant c0 . Moreover, we partition the
time steps into consecutive blocks: with the first block [L]
for finding the vector w̄, the next S blocks B1 , . . . , BS for
the matrix power method in the initialization phase, followed by N blocks J1 , . . . , JN for the tensor power phase,
with their sizes |Bs | and |Jt | given in (6) and (6) respectively. The proofs of related lemmas in Section 6 are given
next.
D.1. Proof of Lemma 7
First, from the assumption that T = Ex [x ⊗ x ⊗ x], where
Ex [·] denotes the expectation over the distribution of x, we
have the following.
P
Proposition D.1. Ex [kxk2 x] = i∈[d] λi ui .
Proof. Recall that if we sample w according to the distribution N d (0, 1), then for any u ∈ Rd , we have
Ew [(u> w)2 ] = kuk2 , where Ew [·] denotes the expectation
over w. Then we have
h
X
X
2 i
Ew [T (Id , w, w)] =
λi Ew u>
ui =
λi ui ,
i w
i∈[d]

i∈[d]

as kui k2 = 1. On the other hand, from the assumption that

T = Ex [x ⊗ x ⊗ x], we also have
h h
2 ii
Ew [T (Id , w, w)] = Ew Ex x> w x
h h
2 ii
= Ex Ew x> w x


= Ex kxk2 x .
The proposition follows by combining these two equalities.
PL
This suggests us to take w̄ = L1 τ =1 (kxτ k2 xτ ), for some
L to be determined next. This is because for any i ∈ [k],
2
the random variable zτ = u>
i (kxτ k xτ ) falls in [−1, 1] and
has expected value
X
>
λ i ui = λ i
E[zτ ] = ui
i∈[d]

for each τ , so that for δ =

1
4 (λi γ

+ 2∆),

"
#
L
1 X


 >



Pr ui w̄ − λi  > δ = Pr 
zτ − λi  > δ
L

τ =1

which by Hoeffding inequality is at most
2−Ω


δ2 L

≤

1
100k

for some L = O( δ12 ) = O( ∆12 log k). Then by a union
bound, with probability 0.99 we have w̄ satisfying |u>
i w̄ −
λi | ≤ δ for every i ∈ [k]. As w̄ can clearly be computed in
O(d) space, the lemma follows.
D.2. Proof of Lemma 8
The streaming algorithm for this lemma can be found in the
initialization phase of our Algorithm 2, which is based on
that of Li et al. (2016).
Recall that Li et al. (2016) considered the matrix case,
in which each vector xτ in the stream has the expectation E[xτ ⊗ xτ ] = M for some d × d matrix M to
be decomposed. To apply their result, let us make the
connection by seeing T (Id , Id , w̄) as their matrix M and
Mτ = g(xτ )(Id , Id , w̄) = (x>
τ w̄) · xτ ⊗ xτ as their estimator xτ ⊗ xτ , by noting that
E[Mτ ] = E[g(xτ )(Id , Id , w̄)] = E[g(xτ )](Id , Id , w̄) = M.
Since kw̄k ≤ 1, kMτ k ≤ kw̄kkxτ k3 ≤ 1, and kM k ≤
kT kkw̄k ≤ 1, we have
kMτ − M k ≤ kMτ k + kM k ≤ 2.
Thus, we have from the matrix Bernstein inequality
(Lemma A.4) that

"
#
 1 X

2


Pr 
Mτ − M  ≥ δ ≤ 2d2−Ω(δ |B|) ,
 |B|

τ ∈B

Tensor Decomposition via Simultaneous Power Iteration

for any block B of time steps, and this allows us to apply
the analysis of (Li et al., 2016).
Following (Li et al., 2016), we use the parameters
 q

εs = ε0 ρs and βs = min ρ/ 1 + ε2s−1 , ρεs−1 ,

P
as its j’th column, so that Φ̂(t) = |J1t | τ ∈Jt Aτ . Note that
we have E[Aτ ] = 0 for each τ , because
h
2 i
>
= E [(xτ ⊗ xτ ⊗ xτ ) (Id , qj , qj )]
E xτ qj xτ
(E [xτ ⊗ xτ ⊗ xτ ]) (Id , qj , qj )
= T (Id , qj , qj ) .
=

√

with ε0 = αdk
for a small enough constant α0 , and divide
0
the time steps into S = O( γ1 log d) blocks, with the s’th
block Bs having size
|Bs | ≤

c0 log(ds)
,
∆4 βs2

(6)

for a large enough constant c0 . Then according to the analysis in (Li et al., 2016) together with that in our proof of
Lemma 4, one can show that tanm (Z (s) ) ≤ εs for every
s ≤ S, so that we can have tanm (Z (S) ) ≤ 1, for every m ∈ [k]. Moreover, from the analysis in (Li et al.,
2016), we know that the number of samples needed can be
bounded by
 2



S
X
ε0 log(dS)
dk log(dS)
|Bs | ≤ O
=
O
.
∆4 γ
∆4 γ
s=1
Finally, note that for each update, the matrix product
> (s−1)
Mτ Z (s−1) equals (x>
, which can be comτ w̄)xτ xτ Z
puted in O(kd) space. Thus, the algorithm works in O(kd)
space, and the lemma follows.
D.3. Proof of Theorem 4
According to Lemma 7 and Lemma 8, let us assume that
we have obtained some Z ∈ Rd×k such that tanm (Z) < 1
for every m ∈ [k]. Now let us focus on the tensor power
phase.
Consider a fixed iteration t. We would like to show that
tanm (Q(t) ) ≤ βt with high probability, using Lemma 1.
For this, we need to show that the condition (3) there is
satisfied with high probability. For j ∈ [k], let qj denote
(t−1)
(t)
Qj
, and recall that Φ̂j = Φ(Id , qj , qj ), which now
equals
1 X > 2
xτ qj xτ − T (Id , qj , qj ) .
|Jt |
τ ∈Jt

(t)

Let Φ̂(t) be the d × k matrix with Φ̂j as its j’th column.
Then we have the following.
1
t
Lemma D.1. kΦ̂(t) k > ∆β
2 with probability at most 200t2 .
Proof. Let us see Φ̂(t) as the average of |Jt | i.i.d. random
matrices, so that we can apply the matrix Bernstein inequality (Lemma A.4). More precisely, for τ ∈ Jt , let Aτ denote
the d × k matrix with
2
x>
xτ − T (Id , qj , qj )
τ qj

Moreover, we claim that kAτ k ≤ 2. This is because for any
v = (v1 , . . . , vk ) ∈ Rk with kvk = 1, kAτ vk is at most

 

X
 X






2
>



vj x τ q j x τ  + 
vj T (Id , qj , qj )


j∈[k]
 j∈[k]

where the first term above is at most
X
X
2
2
|vj | x>
kxτ k ≤
x>
≤ kxτ k ≤ 1
τ qj
τ qj
j∈[k]

j∈[k]

as the qj ’s are orthonormal, while the second term above is


X

X



2
>

vj
λi ui qj ui 


j∈[k] i∈[d]

which can be upper-bounded by




X X
 X

2
>

λi 
v
u
q
u
λi ≤ 1
j
i ≤
i j

j∈[k]
 i∈[d]
i∈[d]
using
a similar argument and the assumption that
P
λ
i∈[d] i ≤ 1. Then we can apply Lemma A.4, and conclude that

"
#
 1 X
 ∆β
1


t
Pr 
Aτ  >
≤
 |Jt |

2
200t2
τ ∈Jt

for our choice of |Jt |.
(t)

Note that kΦ̂[m] k ≤ kΦ̂(t) k for any m ∈ [k], and recall that
we start with cos2m (Q(0) ) = tan2 (Q1 (0) )+1 ≥ 12 . Therem
fore, given this lemma, we can then apply Lemma 1 repeatedly and a simple induction shows that the probability
tanm (Q(t) ) > βt for some m and t is at most
P that
1
t 200t2 ≤ 0.01. Thus, with high probability we have
tanm (Q(t) ) ≤ βt for every m and t. Let N be the number
such that βt > 2ε for t ≤ N − 2 and β = 2ε for t ≥ N − 1.
Note that
!
 

log 1ε
1
1
N = O log
=
O
log
log
,
γ
ε
log ρ1
and recall from the proof of Theorem 2 that from Q(N ) we
can obtain the required ûi ’s and λ̂i ’s.

Tensor Decomposition via Simultaneous Power Iteration

It remains to bound the number of samples needed in this
phase, which is
N
X


|Jt | ≤ O

t=1

log(dN )
∆2

X
N
t=1

1
.
βt2

t

As βt = max{ρ2 −1 , 2ε }, we have βt = 2ε for t ≥ N − 1
t
N −2
t
N −2
and βt = βN −2 ρ2 −2
≥ 2ε ρ2 −2
for t ≤ N − 2.
Therefore,


N
N −2
X
1
8
4 X 2(2N −2 −2t )
1
≤ 2+ 2
ρ
≤O
,
β2
ε
ε t=1
ε2 (1 − ρ4 )
t=1 t
1
1−ρ4
λ2
maxi∈[k] λi+1
2
i

where

=

1+ρ4
1−ρ8

2
1−ρ8

≤

= mini∈[k]

λ2i −λ2i+1
λ2i

and 1 − ρ8 = 1 −
= γ. As a result, we

have
N
X
t=1


|Jt | ≤ O

log(dN )
∆2 γε2


.

Combining this with the number of samples for the initialization phase, including that for finding w̄, we have the
stated sample complexity bound of the theorem.

References
Hardt, Moritz and Price, Eric. The noisy power method: A
meta algorithm with applications. In Advances in Neural
Information Processing Systems, pp. 2861–2869, 2014.
Hom, Roger A and Johnson, Charles R. Topics in matrix
analysis. Cambridge University Press, New York, 1991.
Laurent, B. and Massart, P. Adaptive estimation of a
quadratic functional by model selection. Annals of
Statistics, 28(5):1302–1338, 2000.
Li, Chun-Liang, Lin, Hsuan-Tien, and Lu, Chi-Jen. Rivalry
of two families of algorithms for memory-restricted
streaming PCA. In Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 473–481, 2016.
Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Prateek. Memory limited, streaming PCA. In Advances in
Neural Information Processing Systems, pp. 2886–2894,
2013.
Schmitt, Bernhard A. Perturbation bounds for matrix
square roots and pythagorean sums. Linear algebra and
its applications, 174:215–227, 1992.
Stewart, Gilbert W. and Sun, Ji-Guang. Matrix Perturbation Theory. Academic Press, 1990.

Tropp, Joel A. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389–434, 2012.
Zhu, Peizhen. and Knyazev, Andrew V. Angles between subspaces and their tangents. Arxiv preprint at
arXiv:1209.0523, 2012.

