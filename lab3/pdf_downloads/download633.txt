Tensor Decomposition via Simultaneous Power Iteration
(Supplementary Material)

Po-An Wang 1 Chi-Jen Lu 1

A. Technical Lemmas
For a matrix A, let Ïƒmax (A) and Ïƒmin (A) denote its largest
and smallest singular values, respectively. Then we will
need the following lemma relating such singular values of
a matrix and its sub-matrix.
Lemma A.1. (Corollary 3.1.3 in (Hom & Johnson, 1991))
Let A and B be matrices such that B is derived from A by
deleting some of its rows and/or columns. Then Ïƒmax (A) â‰¥
Ïƒmax (B) and Ïƒmin (A) â‰¤ Ïƒmin (B).
For a matrix Z, let Z 2 = Z  Z denote the Hadamard
(entry-wise) product of Z with itself. Then we will need the
following lemma relating the singular values of matrices Z
and Z 2 .
Lemma A.2. For any matrix Z, Ïƒmin (Z 2 ) â‰¥ (Ïƒmin (Z))2
and Ïƒmax (Z 2 ) â‰¤ (Ïƒmax (Z))2 .
Proof. One can relate the singular values of the Hadamard
product Z 2 to those of the Kronecker product Z âŠ— Z.
In particular, as Z  Z can be obtain from Z âŠ— Z by
deleting some rows and columns, Lemma A.1 tells us that
Ïƒmin (Z  Z) â‰¥ Ïƒmin (Z âŠ— Z) and Ïƒmax (Z  Z) â‰¤
Ïƒmax (Z âŠ— Z). Then the lemma follows as the Kronecker
product Z âŠ—Z is known to have the property that Ïƒmin (Z âŠ—
Z) = (Ïƒmin (Z))2 and Ïƒmax (Z âŠ— Z) = (Ïƒmax (Z))2 .1
We will need the following two tail bounds. The first is for
the sum of the squares of independent standard normal random variables, known as the Ï‡-square distribution, which
follows from the bound in (Laurent & Massart, 2000).
Lemma A.3. Let z1 , . . . , zL be a sequence of i.i.d. random
variables, each from the distribution N (0, 1). Then for any
1

Academia Sinica, Taiwan. Correspondence to: Po-An Wang
<poanwang@iis.sinica.edu.tw>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
1
See e.g. Theorem 4.2.12 in (Hom & Johnson, 1991) for the
case of square matrices; the extension to rectangular matrices is
straightforward.

Î´ âˆˆ (0, 1), we have

ï£®
ï£¹


1 X 2

Pr ï£°
zi âˆ’ 1 â‰¥ Î´ ï£» â‰¤ 2âˆ’â„¦
L


Î´2 L



.

iâˆˆ[L]

The second is the following matrix version of the Bernstein
inequality (see e.g. Theorem 1.6 in (Tropp, 2012)).
Lemma A.4. Consider a finite sequence Z1 , . . . , Zn of independent, random, matrices in RdÃ—k . Assume that each
random matrix satisfies E[Zi ] = 0 and kZi k â‰¤ R almost
surely. Define the variance parameter
 
)
( n
n
X
 X



2
>  
>
Ïƒ = max 
E[Zi Zi ] , 
E[Zi Zi ] .

 

i=1

i=1

Then, for all t â‰¥ 0,

" n
#
X 
âˆ’t2


Pr 
Zi  â‰¥ t â‰¤ (d + k) Â· 2 Ïƒ2 +Rt/3 .


i=1

We will also need the following two matrix perturbation
bounds.
Lemma A.5. (Theorem 2.5 in (Stewart & Sun, 1990))Let
A, E âˆˆ RkÃ—k be given. If A is invertible, and Aâˆ’1 E  <
1 then AÌ„ := A + E is invertible, and


 âˆ’1
 kEk Aâˆ’1 2
âˆ’1
AÌ„ âˆ’ A  â‰¤
.
1 âˆ’ kAâˆ’1 Ek
Lemma A.6. (Lemma 2.2 in (Schmitt, 1992)) Given any
A, AÌ„ âˆˆ RkÃ—k with smallest singular values Ïƒ > 0 and
ÏƒÌ„ > 0, respectively, we have

 1
 
AÌ„ âˆ’ A
1
 2
AÌ„ âˆ’ A 2  â‰¤ 1
1 .
ÏƒÌ„ 2 + Ïƒ 2

B. Proofs in Section 3
B.1. Proof of Lemma 1
Recall that Q(t) is derived from Y (t) by the QR decomposition Y (t) = Q(t) Â· R(t) via the Gram-Schmidt process,

Tensor Decomposition via Simultaneous Power Iteration

which has the same effect as performing k copies of the QR
(t)
decomposition on the k sub-matrices Y[m] , for m âˆˆ [k], to

while using the assumption kÎ¦Ì‚k â‰¤ 4 cos2 (Q), we can
bound the enumerator by

(t)

Ïƒmin (U > Y ) â‰¥ Î»m cos2 (Q) âˆ’ 4 cos2 (Q).

obtain the k sub-matrices Q[m] , for m âˆˆ [k].
Let us fix any m âˆˆ [k] and t â‰¥ 0. To simply our notation, we will drop the indices of m and t in the fol(tâˆ’1)
(t)
lowing. We will write Q for Q[m] , Q0 for Q[m] , Y for
(t)

tan(Q0 ) â‰¤

(t)

Y[m] , and Î¦Ì‚ for Î¦Ì‚[m] . We will write U for U[m] , with
the vector u1 , . . . , um as its columns, while we will use
V to denote the d Ã— (d âˆ’ m) matrix having the vectors
um+1 , . . . , ud as its columns. We will write tan, cos, sin
for tanm , cosm , sinm , respectively. Furthermore, for a matrix A, let Ïƒmin (A) and Ïƒmax (A) denote its smallest and
largest singular values, respectively.
Recall that our goal is to bound tan(Q0 ) in terms of
tan(Q). As discussed before, Q0 is derived from Y by a
QR decomposition, with Y = Q0 R for some matrix R. To
achieve our goal, we will first show that R is invertible so
that Q0 = Y Râˆ’1 , and then relate tan(Q0 ) to the singular
values Ïƒmax (V > Y ) and Ïƒmin (U > Y ), followed by bounding these two singular values.
First, from the condition (3), we have cos Q > 0 which
implies that Q has full rank and consists of orthonormal
columns. Our key lemma is the following, which we will
prove later in Subsection B.1.1.
Lemma B.1. The following two bounds hold:

â€¢ Ïƒmin U > Y â‰¥ Î»m cos2 (Q) âˆ’ kÎ¦Ì‚k, and

â€¢ Ïƒmax V > Y â‰¤ Î»m+1 sin2 (Q) + kÎ¦Ì‚k.
Using this lemma and the assumption kÎ¦Ì‚k â‰¤ 4 cos2 (Q)
in (3), we have

Ïƒmin U > Y â‰¥ Î»m cos2 (Q) âˆ’ 4 cos2 (Q) > 0,
as âˆ† â‰¤ Î»2m . This implies that Y has full rank, Râˆ’1 exists,
Q0 = Y Râˆ’1 has orthonormal columns, and (U > Q0 )âˆ’1 exists. Then from standard properties of principal angles (see
e.g. (Zhu & Knyazev, 2012)), we know that
tan(Q0 ) =

Combining these bounds together, we obtain


Ïƒmax (V > Q0 ) 
= (V > Q0 )(U > Q0 )âˆ’1  ,
>
0
Ïƒmin (U Q )

which equals
 >

(V Y Râˆ’1 )(U > Y Râˆ’1 )âˆ’1  =
â‰¤

 >

(V Y )(U > Y )âˆ’1 
Ïƒmax (V > Y )
.
Ïƒmin (U > Y )

Then by Lemma B.1, together with the assumption from
(3) that kÎ¦Ì‚k â‰¤ 4Î² = 4Î² sin2 (Q) + 4Î² cos2 (Q), we can
bound the denominator by
Ïƒmax (V > Y ) â‰¤ (Î»m+1 + 4Î²) sin2 (Q) + 4Î² cos2 (Q),

Î»m+1 + 4Î²
4Î²
tan2 (Q) +
.
Î»m âˆ’ 4
Î»m âˆ’ 4

Then the rest of the analysis is identical to that of Hardt &
Price (2014) (for the proof of their Lemma 2.2). Specifically, we can rewrite the righthand side above as the
weighted average of two terms
(1 âˆ’ Î±) Â·
with Î± =

Î»m+1 + 4Î²
tan2 (Q) + Î± Â· Î²,
Î»m+1 + 24

4
Î»m+1 +34 ,

which can be upper-bounded by


Î»m+1 + 4Î²
2
max
tan (Q), Î² ,
Î»m+1 + 24

and similarly, we can also have


Î»m+1 + 4Î²
Î»m+1
â‰¤ max
,Î² .
Î»m+1 + 24
Î»m+1 + 4
Î»m+1
Î»m+1
1/4
Since Î»m+1
= ( Î»Î»m+1
)1/4 = Ï, we
+4 â‰¤ ( Î»m+1 +44 )
m
thus have the desired bound

	
tan(Q0 ) â‰¤ max max {Ï, Î²} tan2 (Q), Î² .

To finish the proof, it remains to prove Lemma B.1, which
we do next.
B.1.1. P ROOF OF L EMMA B.1
Recall from (2) that for any column Yj of Y and for any
target vector ui ,
2
>
u>
+ u>
i Yj = Î»i ui Qj
i Î¦Ì‚j .
These equations can be summarized as
2
U >Y = Î› U >Q
+ U > Î¦Ì‚, and

2
V > Y = Î›Ì„ V > Q
+ V > Î¦Ì‚,
using the notation Î› for the m Ã— m diagonal matrix with
Î»1 , . . . , Î»m at its diagonal, Î›Ì„ for the (d âˆ’ m) Ã— (d âˆ’ m)
diagonal matrix with Î»m+1 , . . . , Î»d at its diagonal, and
A2 = A  A for the Hadamard (entry-wise) product of
matrix A with itself. From this, we have



2
Ïƒmin U > Y
= Ïƒmin Î› U > Q
+ U > Î¦Ì‚


2  


â‰¥ Ïƒmin Î› U > Q
âˆ’ U > Î¦Ì‚


2  
 
â‰¥ Ïƒmin (Î›) Ïƒmin U > Q
âˆ’ Î¦Ì‚ ,

Tensor Decomposition via Simultaneous Power Iteration
(t)

as well as
Ïƒmax V > Y




2
= Ïƒmax Î›Ì„ V > Q
+ V > Î¦Ì‚


2  


â‰¤ Ïƒmax Î›Ì„ V > Q
+ V > Î¦Ì‚



2  
 
â‰¤ Ïƒmax Î›Ì„ Ïƒmax V > Q
+ Î¦Ì‚ .


From Lemma A.2, we have

2 
2
Ïƒmin U > Q
â‰¥ Ïƒmin U > Q
= cos2 (Q),
since Q has orthonormal columns, and moreover

2 
2
Ïƒmax V > Q
â‰¤ Ïƒmax V > Q
= sin2 (Q).

As Ïƒmin Î›Ì„ = Î»m and Ïƒmax (Î›) = Î»m+1 , Lemma B.1
follows.
B.2. Proof of Theorem 2
Suppose we have Q(0) such that for every m âˆˆ [k],
tanm (Q(0) ) â‰¤ 1 and hence cosm (Q(0) ) â‰¥ âˆš12 . We would
like to apply Lemma 1 repeatedly with Î² = 2Îµ to achieve
tanm (Q(t) ) â‰¤ 2Îµ for every m. To be able to do this, we
need to verify that for every t, the condition (3) in Lemma 1
(t)
is satisfied. For this, we first claim that kÎ¦Ì‚[m] k â‰¤ âˆ†Îµ
2 . This
holds since for any vector x = (x1 , . . . , xm ) of unit length,



X 
X
 (t) 

(t)
(t) 
|xj | Â· kÎ¦k
Î¦Ì‚[m] x â‰¤
xj Î¦(Id , Qj , Qj ) â‰¤
jâˆˆ[m]

jâˆˆ[m]

Proposition
B.1. For any t â‰¥ N , we have u>
i Qi
q
2
Îµ
1 âˆ’ 2 for every i âˆˆ [k].

Proof. Let us fix any i âˆˆ [k]. In the following, we first
(t) 2
Îµ2
show that (u>
i Qi ) â‰¥ 1 âˆ’ 2 for any t â‰¥ N âˆ’ 1, and then
(t)
we show that u>
â‰¥ 0 for any t â‰¥ N , which together
i Qi
prove the proposition.
First, consider any t â‰¥ N âˆ’ 1, which from the discus(t) 2
sion above has tani (Q(t) ) â‰¤ 2Îµ . Note that (u>
i Qi ) â‰¥
cos2i (Q(t) ) âˆ’ sin2iâˆ’1 (Q(t) ), because

2

(t) 
cos2i (Q(t) ) â‰¤ u>
i Q[i] 

2 
2

(t) 
> (t)
= u>
i Q[iâˆ’1]  + ui Qi

2
(t)
â‰¤ sin2iâˆ’1 (Q(t) ) + u>
.
i Qi
From this and the fact that cos2i (Q(t) ) =
sin2iâˆ’1 (Q(t) )


â‰¤

tan2iâˆ’1 (Q(t) ),
(t)

u>
i Qi

mkxk Â· kÎ¦k =

âˆš

m Â· kÎ¦k â‰¤

t

(t)

Yi

1
1âˆ’Î³ )

1
Îµ2
Îµ2
âˆ’
â‰¥
1
âˆ’
.
2
4
2
1 + Îµ4

=

X


2
(tâˆ’1)
(tâˆ’1)
Î»j u>
Â· uj + Î¦Ì‚i
j Qi

j

length. Thus, the sign of u>
i Qi is the same as that of


 
(t)
(t)
u>
Yi âˆ’ z > Yi
z
i



(t)
> (t)
= u>
Y
âˆ’
z
Y
u>
i i
i z
i


2 
 (tâˆ’1) 
(tâˆ’1)
â‰¥ Î»i u>
Q
âˆ’
Î¦Ì‚

 âˆ’ siniâˆ’1 (Q(t) )
i
i
i


Îµ2
5Îµ
â‰¥ Î»i 1 âˆ’
âˆ’ ,
2
8
(tâˆ’1)

â‰¥ â„¦(Î³).
(t)

Next, we show that for any t â‰¥ N , each Qi
enough to ui . For this, we rely on the following.

is close

2

(tâˆ’1)

since (u>
)2 â‰¥ 1âˆ’ Îµ2 for tâˆ’1 â‰¥ N âˆ’1, kÎ¦Ì‚i
kâ‰¤
i Qi
âˆ†Îµ
Îµ
Îµ
(t)
(t)
â‰¤
,
and
sin
(Q
)
â‰¤
tan
(Q
)
â‰¤
for
t â‰¥
iâˆ’1
iâˆ’1
2
8
2
N . Finally, as Î»i â‰¥ Î»k â‰¥ 2Îµ, the last line above is positive,
(t)
which implies that u>
i Qi > 0.
(t)

â‰¥ â„¦(log

we get

(t)

for every m. Thus, we have Ï2 âˆ’1 â‰¤ 2Îµ and tanm (Q(t) ) â‰¤
Îµ
2 whenever t â‰¥ N âˆ’ 1, for some
!
 

log 1Îµ
1
1
N = O log
=
O
log
log
,
Î³
Îµ
log Ï1
1
Ï

â‰¥

and

by subtracting from it its projection to some unit vector z
(t)
in the column space of Q[iâˆ’1] and then scaling it to unit

âˆ†Îµ
.
2

Moreover, we can assume without loss of generality that
max{ 2Îµ , Ï} = Ï because otherwise, we immediately have
tanm (Q(1) ) â‰¤ 2Îµ for every m, as the condition (3) is satisfied for t = 1. Then a simple induction shows that for
every t â‰¥ 1, the condition (3) in Lemma 1 holds and
nÎµ
o
tanm (Q(t) ) â‰¤ max
, Ï tan2m (Q(tâˆ’1) )
n 2Îµ t o
â‰¤ max
, Ï2 âˆ’1
2

by noting that log

2

1
1+tan2i (Q(t) )

Next, consider any t â‰¥ N and our goal is to show that
(t)
(t)
u>
i Qi > 0. Recall that Qi is derived from

which by the Cauchy-Schwarz inequality is at most
âˆš

â‰¥

As kQi k = kui k = 1, this proposition immediately implies that for any t â‰¥ N and i âˆˆ [k],

 q
 (t)

(t)
Qi âˆ’ ui  = 2 âˆ’ 2u>
i Qi â‰¤ Îµ,

Tensor Decomposition via Simultaneous Power Iteration
(t)

(t)

>
2
as u>
i Qi â‰¥ (ui Qi ) â‰¥ 1 âˆ’

Îµ2
2 .

Finally, let us show that for any t â‰¥ N , each Î»i can be
(t)
(t)
(t)
approximated well by Î»Ì‚i = TÌ„ (Qi , Qi , Qi ). Fix any
t â‰¥ N and i âˆˆ [k]. Note that |Î»i âˆ’ Î»Ì‚i | is at most


  



(t)
(t)
(t) 
(t)
(t)
(t) 
Î»i âˆ’ T Qi , Qi , Qi  + Î¦ Qi , Qi , Qi  .
(t)

Îµ
4,

The second term above is at most kÎ¦kkQi k3 â‰¤
the first term above is at most


3  X  
3 

(t)
> (t)
Î»i âˆ’ Î»i u>



Q
+
Î»
u
Q
i
i


 j j i


and

that the matrix Î¦(ui , Id , Id ) has norm
PkÎ¦(ui , Id , Id )k â‰¤
âˆ†
kÎ¦k â‰¤ 3d
and can be decomposed as râˆˆ[d] Î»Ìƒr Â· uÌƒr âŠ— uÌƒr ,
for some orthonormal vectors uÌƒr â€™s as well as some values
âˆ†
Î»Ìƒr â€™s, each with |Î»Ìƒr | â‰¤ 3d
. Then by a similar analysis as
above, together with a union bound, we can have with prob1
1
ability at least 1 âˆ’ d Â· 200d
2 = 1 âˆ’ 200d that




1 X


Î¦(ui , wj , wj )
L
 jâˆˆ[L]


â‰¤

râˆˆ[d]

â‰¤



(t)

â‰¤ Î»i 1 âˆ’ u>
i Qi

3 

+

X





(t)

Î» j 1 âˆ’ u>
i Qi

2 

â‰¤

j6=i
2

â‰¤ Î»i

3Îµ
+
4

X
j6=i

2

Î»j

Îµ
,
2

where the first inequality uses the fact that for j 6= i,
(t) 3
(t) 2
(t)
(t) 2
(u>
â‰¤ (u>
â‰¤ kQi k2 âˆ’ (u>
=
j Qi )
j Qi )
i Qi )
(t)

2
1 âˆ’ (u>
i Qi ) , while the second inequality uses the bound
2
2
> (t) 3
(ui Qi ) â‰¥ (1 âˆ’ Îµ2 )3/2 â‰¥ 1 âˆ’ 3Îµ4 . As a result, we have

B.4. Proof of Lemma 3
Consider any wÌ„ satisfying the condition (5) in Lemma 2.
By definition, MÌ„ = TÌ„ (Id , Id , wÌ„) can be decomposed as
T (Id , Id , wÌ„) + Î¦(Id , Id , wÌ„).

P
using the assumption j Î»j â‰¤ 1 from (1). This completes
the proof of the theorem.

The first matrix can be expressed as
X

T (Id , Id , wÌ„) =
Î»i u>
i wÌ„ Â· ui âŠ— ui ,
iâˆˆ[d]

B.3. Proof of Lemma 2

with Î»Ì„i = Î»i (u>
i wÌ„) and ui as its iâ€™th eigenvalue and eigeni+1
vector, respectively. Note that as âˆ† â‰¤ Î»i âˆ’Î»
, we have
4

From the definition of wÌ„, we can express u>
i wÌ„ as
1 X
1 X
T (ui , wj , wj ) +
Î¦(ui , wj , wj ).
L
L

(1)

jâˆˆ[L]

Î»Ì„i

The first term above equals
2
1 X
1 X > 2
Î» i u>
= Î»i Â·
ui w j ,
i wj
L
L
jâˆˆ[L]

jâˆˆ[L]

and note that the sum has a Ï‡-square distribution because each u>
i wj is an independent random variable with
the standard normal distribution N (0, 1).2 Then from
Lemma A.3, we know that for Î´ = Î³4 , there exists some
L â‰¤ O( Î³12 log d) such that the first term in (1) differs from
Î»i by at most

Î»i Î³
4

with probability at least 1 âˆ’

1
200d2 .

The second term in (1) can be bounded in a similar way
as follows. Since kui k = 1 and Î¦ is symmetric, we know
2

âˆ†
.
2

By combining the two bounds above, we can conclude that
for any i âˆˆ [d], the sum in (1) differs from Î»i by at most
1
1
4 (Î»i Î³ + 2âˆ†) with probability at least 1 âˆ’ 100d . Then the
lemma immediately follows by a union bound.


 X 3Îµ2
Îµ


Î»j
+ â‰¤ Îµ,
Î»i âˆ’ Î»Ì‚i  â‰¤
4
4
j

jâˆˆ[L]

jâˆˆ[L]

X âˆ† 
Î³
Â· 1+
3d
4

râˆˆ[d]

j6=i



X   1 X
2
uÌƒ>
Î»Ìƒr  Â·
r wj
L

This is because each component of wj has P
the distribution
N (0, 1), and the distribution of u>
i wj has mean
r ui,r Â· 0 = 0
P
and variance r u2i,r Â· 1 = 1, where ui,r denotes the râ€™th component of ui .


1 2
Î» Î³ + 2Î»i âˆ†
4 i
Î»2 âˆ’ Î»2i+1
Î»2 âˆ’ Î»i Î»i+1
â‰¥ Î»2i âˆ’ i
âˆ’ i
4
8

2
2
3
Î»
âˆ’
Î»
i
i+1
â‰¥ Î»2i âˆ’
,
8
â‰¥ Î»2i âˆ’

as well as
Î»Ì„i+1


1 2
Î»i+1 Î³ + 2Î»i+1 âˆ†
4
Î»2 âˆ’ Î»2i+1
Î»i+1 Î»i âˆ’ Î»2i+1
â‰¤ Î»2i+1 + Î»2i+1 i
+
4Î»2i
8

2
2
3
Î»
âˆ’
Î»
i
i+1
â‰¤ Î»2i+1 +
,
8
â‰¤ Î»2i+1 +

which together imply that
Î»Ì„i âˆ’ Î»Ì„i+1 â‰¥

Î»2i âˆ’ Î»2i+1
â‰¥ âˆ†2 .
4

Tensor Decomposition via Simultaneous Power Iteration

It remains to show that we can have an initial Z (0) , such
(0)
that for each m âˆˆ [k], Z[m] satisfies the condition required
by Lemma B.2. For this, we need the following bound from
(Mitliagkas et al., 2013).
Proposition B.2. For any Î´, we have


Î´
Pr cosk (Z (0) ) â‰¤ âˆš
â‰¤ O(Î´) + 2âˆ’â„¦(d) .
dk

It remains to bound the norm of Î¦(Id , Id , wÌ„), which is
kÎ¦(Id , Id , wÌ„)k â‰¤ kÎ¦k Â· kwÌ„k,
where kwÌ„k2 =
X
iâˆˆ[d]

P

iâˆˆ[d]

u>
i wÌ„

2

1
Î»i + (Î»i Î³ + 2âˆ†)
4

is at most

2
â‰¤

X

2

(2Î»i ) â‰¤ 4.

iâˆˆ[d]

This implies that kÎ¦(Id , Id , wÌ„)k â‰¤ 2kÎ¦k, which completes
the proof of the lemma.
B.5. Proof of Lemma 4

Ïƒmin (U > Z (0) ) = cosk (Z (0) ). Thus, with high probabilâˆš 0 for every m âˆˆ [k].
ity we in fact have cosm (Z (0) ) â‰¥ 10Î±
dk

Suppose we have a matrix MÌ„ = M + Î¦Ì„, where
X
M=
Î»Ì„i Â· ui âŠ— ui
iâˆˆ[d]
2

âˆ†
with Î»Ì„i âˆ’ Î»Ì„i+1 â‰¥ âˆ†2 for every i âˆˆ [k] and kÎ¦Ì„k â‰¤ Î±âˆš1dk
for a small enough constant Î±1 . The key observation is that
although we run one copy of the matrix power method of
Hardt & Price (2014) to update the whole dÃ—k matrix Z (s) ,
we can actually see our algorithm as running k copies of
(s)
(s)
the matrix power method on k sub-matrices Z[1] , . . . , Z[k]
simultaneously. This allows us to apply their analysis immediately.

More precisely, although our QR decomposition at each
step s is applied to the whole d Ã— k matrix Y (s) to obtain our d Ã— k matrix Z (s) , the Gram-Schmidt process we
(s)
use has the effect that each d Ã— m sub-matrix Z[m] can also
(s)

be seen as obtained from the d Ã— m matrix Y[m] by a QR
decomposition. Thus, our algorithm can be seen as running
k copies of the algorithm of (Hardt & Price, 2014) simultaneously, and we can apply the following lemma of theirs3
simultaneously for every m âˆˆ [k] with X (s) being our dÃ—k
(s)
matrix Z[m] .
Lemma B.2. Fix any m âˆˆ [k]. Suppose that the initial
(s)
X (0) and the noise Gm = Î¦Ì„ Â· X (s) at each step s is such
that



 > (s) 
5 U[m]
Gm  â‰¤ Î»Ì„m âˆ’ Î»Ì„m+1 cosm (X (0) )





5 G(s)
Î»Ì„m âˆ’ Î»Ì„m+1 
m  â‰¤
for some 

< 12 .
O( Î³1m

Then for Î³m = 1 âˆ’

some S =
log tanm (X

we have tanm (X (t) ) â‰¤ .
3

(0)

)

Î»Ì„m+1
,
Î»Ì„m

By applying this proposition, with Î´ = 10Î±0 for a small
âˆš 0
enough constant Î±0 , we can have cosk (Z (0) ) â‰¥ 10Î±
dk
with high probability. From Lemma A.1, we know that
(0)
>
for any m âˆˆ [k], cosm (Z (0) ) = Ïƒmin (U[m]
Z[m] ) â‰¥

there exists

) such that for any t â‰¥ S

It corresponds to Theorem 2.3 in (Hardt & Price, 2014). Although it is stated there for m = k, it in fact works for any value
of k and hence m.

Given such an initial Z (0) , we can have for every s and m
that
10Î±0 âˆ†2
âˆš
5kG(s)
m k â‰¤ 5kÎ¦Ì„k â‰¤
dk
which satisfies the two conditions needed by Lemma B.2,
with  = 13 . Then we can repeatedly apply Lemma B.2,
simultaneously for every m âˆˆ [k], and a simple induction shows that for some S = O( Î³1 log d), we have
tanm (Z (s) ) â‰¤  < 1 for any m âˆˆ [k] and s â‰¥ S. This
completes the proof of our Lemma 4

C. Proofs in Section 5
C.1. Proof of Lemma 5
First, we claim that tank (Q) < 1 with high probability.
To show this, note that by Proposition B.2, we have with
4Î±0
high probability that cosk (Z) > âˆš
for a small enough
dk
constant Î±0 . In the following, let us assume that we indeed
âˆš
have such a matrix Z, and note that it has tank (Z) < 4Î±dk0 .
Then we need the following.
Lemma C.1. (Lemma 2.2 in (Hardt & Price, 2014)) Let
Z, G âˆˆ RdÃ—k satisfy


4 U > G â‰¤ (Î»k âˆ’ Î»k+1 ) cosk (Z)
4 kGk

â‰¤ (Î»k âˆ’ Î»k+1 ) Î²

1/4
for some Î² < 1. Then for Ï = Î»Î»k+1
, we have
k
tank (M Z + G) â‰¤ max{Î², max{Î², Ï} tank (Z)}.
Recall that we assume Î»k+1 = 0, and to apply the lemma,
4Î±0
let Î² = âˆš
and G = Î¦Ì„Z. Note that
dk
  Î»k Î²
kGk â‰¤ Î¦Ì„ â‰¤
,
4
which satisfies both requirements of the lemma, and thus
with YÌ„ = M Z + G, we have
tank (Q) = tank (YÌ„ ) â‰¤ Î² tank (Z) < 1.

Tensor Decomposition via Simultaneous Power Iteration

Next, let us bound Ïƒmin (P ) and Ïƒmax (P ). Recall that
P = Q> M Q = Q> U Î›U > Q,

1

Ïƒmin (P ) â‰¥ Ïƒmin
Ïƒmax (P ) â‰¤ Ïƒmax

>




Q U Ïƒmin (Î›) Ïƒmin U Q and


Q> U Ïƒmax (Î›) Ïƒmax U > Q .

Since the matrix Q has orthonormal columns, we have

(Ïƒmin U > Q )2 = (cosk (Q))2 =

Î»k
2 ,
1

and

â€¢ kPÌ„ âˆ’ 2 âˆ’ P âˆ’ 2 k â‰¤

which implies that
>

â€¢ Ïƒmin (P ) â‰¥

1
1
â‰¥ ,
2
2
1 + tank (Q)

as well as

Ïƒmax U > Q â‰¤ kU k kQk = 1.
Finally, as Ïƒmax (Î›) = Î»1 and Ïƒmin (Î›) = Î»k , we have

Î»k Îµ
64 .

Assume from now on that the above three conditions hold.
Next, observe that



TÌ„ WÌ„ , WÌ„ , WÌ„ âˆ’ T (W, W, W )



â‰¤ TÌ„ WÌ„ , WÌ„ , WÌ„ âˆ’ T WÌ„ , WÌ„ , WÌ„  +
(2)



T WÌ„ , WÌ„ , WÌ„ âˆ’ T W, WÌ„ , WÌ„  +
(3)



T W, WÌ„ , WÌ„ âˆ’ T W, W, WÌ„  +
(4)



T W, W, WÌ„ âˆ’ T (W, W, W ) .
(5)
The term in (2) is at most

 

Î¦ WÌ„ , WÌ„ , WÌ„  â‰¤ kÎ¦k WÌ„ 3 â‰¤ Îµ
4
3

Î»k
and Ïƒmax (P ) â‰¤ Î»1 .
2

C.2. Proof of Lemma 6

as kÎ¦k â‰¤ Î±0 Î»k2 Îµ for a small enough constant Î±0 and

 

 
1
1

WÌ„  â‰¤ kQk 
PÌ„ âˆ’ 2  â‰¤ PÌ„ âˆ’ 2  ,

First, from the definition, we have

 

 
PÌ„ âˆ’ P  = Q> MÌ„ Q âˆ’ Q> M Q â‰¤ kQk2 Î¦Ì„ â‰¤ 

which can be upper-bounded by

 

1
âˆ’1
 âˆ’ 12   âˆ’ 12
âˆ’ P âˆ’ 2  â‰¤ 4Î»k 2 .
P  + PÌ„

as Q has orthonormal columns so that kQk2 â‰¤ 1. Therefore, given the assumption that 0 <  â‰¤ Ïƒmin2(P ) , we have


Ïƒmin (PÌ„ ) â‰¥ Ïƒmin (P ) âˆ’ PÌ„ âˆ’ P  > 0,

The term in (3) is at most


T WÌ„ âˆ’ W, WÌ„ , WÌ„ 

Ïƒmin (P ) â‰¥

which implies that PÌ„ is invertible.
Then according to Lemma A.5, we have



PÌ„ âˆ’ P  P âˆ’1 2
 âˆ’1

PÌ„ âˆ’ P âˆ’1  â‰¤


1 âˆ’ PÌ„ âˆ’ P  kP âˆ’1 k

Similarly, the term in (4) can be upper-bounded by


  Îµ
kT k WÌ„ âˆ’ W  kW k WÌ„  â‰¤
4

â‰¤ 2(Ïƒmin (P ))âˆ’2 ,


as P âˆ’1  = (Ïƒmin (P ))âˆ’1 and kPÌ„ âˆ’ P k â‰¤  â‰¤ Ïƒmin2(P ) .
Combining this with Lemma A.6, we obtain
 âˆ’1



PÌ„ âˆ’ P âˆ’1 
 âˆ’ 12
âˆ’ 12 
âˆ’P  â‰¤
PÌ„
1
1
(Ïƒmin (PÌ„ âˆ’1 )) 2 + (Ïƒmin (P âˆ’1 )) 2
1

â‰¤ 2(Ïƒmin (P ))âˆ’2 (Ïƒmax (P )) 2 ,
since Ïƒmin (PÌ„ âˆ’1 ) â‰¥ 0 and Ïƒmin (P âˆ’1 ) = (Ïƒmax (P ))âˆ’1 .

and the term in (5) can be upper-bounded by


Îµ
kT k WÌ„ âˆ’ W  kW k2 â‰¤ .
4
As a result, we can conclude that



TÌ„ WÌ„ , WÌ„ , WÌ„ âˆ’ T (W, W, W ) â‰¤ Îµ
with high probability, which proves the theorem.

D. Proofs in Section 6

C.3. Proof of Theorem 3
Î»3

k
First, given Îµ âˆˆ (0, 12 ) and kÎ¦Ì„k â‰¤ Î±0 Îµ min{ âˆšÎ»dk
, âˆšÎ»k },
1
for a small enough constant Î±0 , we know from Lemma 5
and Lemma 6 that with high probability,

â€¢ Ïƒmax (P ) â‰¤ Î»1 ,


  2
kT k WÌ„ âˆ’ W  WÌ„ 


1
1

â‰¤ PÌ„ âˆ’ 2 âˆ’ P âˆ’ 2  16Î»âˆ’1
k
Îµ
â‰¤
.
4
â‰¤

Our streaming algorithm for orthogonal tensors with g of
the form g(x) = x âŠ— x âŠ— x is summarized in Algorithm 2.
We will use the parameters


c0 log k
c0 log k
1
1
L=
,
S
=
,
N
=
c
log
log
0
âˆ†2
Î³
Î³
Îµ

Tensor Decomposition via Simultaneous Power Iteration

Algorithm 2 Streaming robust tensor power method
Input: a stream of data {x1 , x2 , . . . , }, parameters
S
N
L, S, N , index sets {Bs }s=1 , {Jt }t=1 .
Initialization Phase
Let wÌ„ = 0 âˆˆ Rd .
for Ï„ = 1 to L do
Update wÌ„ = wÌ„ + L1 xÏ„ .
end for
(0)
(0)
Sample Y1 , . . . , Yk âˆ¼ N d (0, 1).
Factorize Y (0) as Z (0) R(0) by QR decomposition.
for s = 1 to S do
for Ï„ âˆˆ Bs do

> (sâˆ’1)
Update Y (s) = Y (s) + |B1s | x>
.
Ï„ wÌ„ xÏ„ xÏ„ Z
end for
Factorize Y (s) as Z (s) R(s) by QR decomposition.
end for
Tensor power phase
Let Q(0) = Z (S) .
for t = 1 to N do
for Ï„ âˆˆ Jt do


(t)

Update Yi

(t)

(t)

(t)

2

+ |J1t | xÏ„ x>
, âˆ€i âˆˆ [k].
Ï„ Qi

3
(t)
(t)
= Î»i + |J1t | x>
, âˆ€i âˆˆ [k].
Ï„ Qi
= Yi

Update Î»i
end for
Factorize Y (t) as Q(t) R(t) by QR decomposition.
end for
(N )
(N )
Output: uÌ‚i = Qi and Î»Ì‚i = Î»i , âˆ€i âˆˆ [k]

for a large enough constant c0 . Moreover, we partition the
time steps into consecutive blocks: with the first block [L]
for finding the vector wÌ„, the next S blocks B1 , . . . , BS for
the matrix power method in the initialization phase, followed by N blocks J1 , . . . , JN for the tensor power phase,
with their sizes |Bs | and |Jt | given in (6) and (6) respectively. The proofs of related lemmas in Section 6 are given
next.
D.1. Proof of Lemma 7
First, from the assumption that T = Ex [x âŠ— x âŠ— x], where
Ex [Â·] denotes the expectation over the distribution of x, we
have the following.
P
Proposition D.1. Ex [kxk2 x] = iâˆˆ[d] Î»i ui .
Proof. Recall that if we sample w according to the distribution N d (0, 1), then for any u âˆˆ Rd , we have
Ew [(u> w)2 ] = kuk2 , where Ew [Â·] denotes the expectation
over w. Then we have
h
X
X
2 i
Ew [T (Id , w, w)] =
Î»i Ew u>
ui =
Î»i ui ,
i w
iâˆˆ[d]

iâˆˆ[d]

as kui k2 = 1. On the other hand, from the assumption that

T = Ex [x âŠ— x âŠ— x], we also have
h h
2 ii
Ew [T (Id , w, w)] = Ew Ex x> w x
h h
2 ii
= Ex Ew x> w x


= Ex kxk2 x .
The proposition follows by combining these two equalities.
PL
This suggests us to take wÌ„ = L1 Ï„ =1 (kxÏ„ k2 xÏ„ ), for some
L to be determined next. This is because for any i âˆˆ [k],
2
the random variable zÏ„ = u>
i (kxÏ„ k xÏ„ ) falls in [âˆ’1, 1] and
has expected value
X
>
Î» i ui = Î» i
E[zÏ„ ] = ui
iâˆˆ[d]

for each Ï„ , so that for Î´ =

1
4 (Î»i Î³

+ 2âˆ†),

"
#
L
1 X


 >



Pr ui wÌ„ âˆ’ Î»i  > Î´ = Pr 
zÏ„ âˆ’ Î»i  > Î´
L

Ï„ =1

which by Hoeffding inequality is at most
2âˆ’â„¦


Î´2 L

â‰¤

1
100k

for some L = O( Î´12 ) = O( âˆ†12 log k). Then by a union
bound, with probability 0.99 we have wÌ„ satisfying |u>
i wÌ„ âˆ’
Î»i | â‰¤ Î´ for every i âˆˆ [k]. As wÌ„ can clearly be computed in
O(d) space, the lemma follows.
D.2. Proof of Lemma 8
The streaming algorithm for this lemma can be found in the
initialization phase of our Algorithm 2, which is based on
that of Li et al. (2016).
Recall that Li et al. (2016) considered the matrix case,
in which each vector xÏ„ in the stream has the expectation E[xÏ„ âŠ— xÏ„ ] = M for some d Ã— d matrix M to
be decomposed. To apply their result, let us make the
connection by seeing T (Id , Id , wÌ„) as their matrix M and
MÏ„ = g(xÏ„ )(Id , Id , wÌ„) = (x>
Ï„ wÌ„) Â· xÏ„ âŠ— xÏ„ as their estimator xÏ„ âŠ— xÏ„ , by noting that
E[MÏ„ ] = E[g(xÏ„ )(Id , Id , wÌ„)] = E[g(xÏ„ )](Id , Id , wÌ„) = M.
Since kwÌ„k â‰¤ 1, kMÏ„ k â‰¤ kwÌ„kkxÏ„ k3 â‰¤ 1, and kM k â‰¤
kT kkwÌ„k â‰¤ 1, we have
kMÏ„ âˆ’ M k â‰¤ kMÏ„ k + kM k â‰¤ 2.
Thus, we have from the matrix Bernstein inequality
(Lemma A.4) that

"
#
 1 X

2


Pr 
MÏ„ âˆ’ M  â‰¥ Î´ â‰¤ 2d2âˆ’â„¦(Î´ |B|) ,
 |B|

Ï„ âˆˆB

Tensor Decomposition via Simultaneous Power Iteration

for any block B of time steps, and this allows us to apply
the analysis of (Li et al., 2016).
Following (Li et al., 2016), we use the parameters
 q

Îµs = Îµ0 Ïs and Î²s = min Ï/ 1 + Îµ2sâˆ’1 , ÏÎµsâˆ’1 ,

P
as its jâ€™th column, so that Î¦Ì‚(t) = |J1t | Ï„ âˆˆJt AÏ„ . Note that
we have E[AÏ„ ] = 0 for each Ï„ , because
h
2 i
>
= E [(xÏ„ âŠ— xÏ„ âŠ— xÏ„ ) (Id , qj , qj )]
E xÏ„ qj xÏ„
(E [xÏ„ âŠ— xÏ„ âŠ— xÏ„ ]) (Id , qj , qj )
= T (Id , qj , qj ) .
=

âˆš

with Îµ0 = Î±dk
for a small enough constant Î±0 , and divide
0
the time steps into S = O( Î³1 log d) blocks, with the sâ€™th
block Bs having size
|Bs | â‰¤

c0 log(ds)
,
âˆ†4 Î²s2

(6)

for a large enough constant c0 . Then according to the analysis in (Li et al., 2016) together with that in our proof of
Lemma 4, one can show that tanm (Z (s) ) â‰¤ Îµs for every
s â‰¤ S, so that we can have tanm (Z (S) ) â‰¤ 1, for every m âˆˆ [k]. Moreover, from the analysis in (Li et al.,
2016), we know that the number of samples needed can be
bounded by
 2



S
X
Îµ0 log(dS)
dk log(dS)
|Bs | â‰¤ O
=
O
.
âˆ†4 Î³
âˆ†4 Î³
s=1
Finally, note that for each update, the matrix product
> (sâˆ’1)
MÏ„ Z (sâˆ’1) equals (x>
, which can be comÏ„ wÌ„)xÏ„ xÏ„ Z
puted in O(kd) space. Thus, the algorithm works in O(kd)
space, and the lemma follows.
D.3. Proof of Theorem 4
According to Lemma 7 and Lemma 8, let us assume that
we have obtained some Z âˆˆ RdÃ—k such that tanm (Z) < 1
for every m âˆˆ [k]. Now let us focus on the tensor power
phase.
Consider a fixed iteration t. We would like to show that
tanm (Q(t) ) â‰¤ Î²t with high probability, using Lemma 1.
For this, we need to show that the condition (3) there is
satisfied with high probability. For j âˆˆ [k], let qj denote
(tâˆ’1)
(t)
Qj
, and recall that Î¦Ì‚j = Î¦(Id , qj , qj ), which now
equals
1 X > 2
xÏ„ qj xÏ„ âˆ’ T (Id , qj , qj ) .
|Jt |
Ï„ âˆˆJt

(t)

Let Î¦Ì‚(t) be the d Ã— k matrix with Î¦Ì‚j as its jâ€™th column.
Then we have the following.
1
t
Lemma D.1. kÎ¦Ì‚(t) k > âˆ†Î²
2 with probability at most 200t2 .
Proof. Let us see Î¦Ì‚(t) as the average of |Jt | i.i.d. random
matrices, so that we can apply the matrix Bernstein inequality (Lemma A.4). More precisely, for Ï„ âˆˆ Jt , let AÏ„ denote
the d Ã— k matrix with
2
x>
xÏ„ âˆ’ T (Id , qj , qj )
Ï„ qj

Moreover, we claim that kAÏ„ k â‰¤ 2. This is because for any
v = (v1 , . . . , vk ) âˆˆ Rk with kvk = 1, kAÏ„ vk is at most

 

X
 X






2
>



vj x Ï„ q j x Ï„  + 
vj T (Id , qj , qj )


jâˆˆ[k]
 jâˆˆ[k]

where the first term above is at most
X
X
2
2
|vj | x>
kxÏ„ k â‰¤
x>
â‰¤ kxÏ„ k â‰¤ 1
Ï„ qj
Ï„ qj
jâˆˆ[k]

jâˆˆ[k]

as the qj â€™s are orthonormal, while the second term above is


X

X



2
>

vj
Î»i ui qj ui 


jâˆˆ[k] iâˆˆ[d]

which can be upper-bounded by




X X
 X

2
>

Î»i 
v
u
q
u
Î»i â‰¤ 1
j
i â‰¤
i j

jâˆˆ[k]
 iâˆˆ[d]
iâˆˆ[d]
using
a similar argument and the assumption that
P
Î»
iâˆˆ[d] i â‰¤ 1. Then we can apply Lemma A.4, and conclude that

"
#
 1 X
 âˆ†Î²
1


t
Pr 
AÏ„  >
â‰¤
 |Jt |

2
200t2
Ï„ âˆˆJt

for our choice of |Jt |.
(t)

Note that kÎ¦Ì‚[m] k â‰¤ kÎ¦Ì‚(t) k for any m âˆˆ [k], and recall that
we start with cos2m (Q(0) ) = tan2 (Q1 (0) )+1 â‰¥ 12 . Therem
fore, given this lemma, we can then apply Lemma 1 repeatedly and a simple induction shows that the probability
tanm (Q(t) ) > Î²t for some m and t is at most
P that
1
t 200t2 â‰¤ 0.01. Thus, with high probability we have
tanm (Q(t) ) â‰¤ Î²t for every m and t. Let N be the number
such that Î²t > 2Îµ for t â‰¤ N âˆ’ 2 and Î² = 2Îµ for t â‰¥ N âˆ’ 1.
Note that
!
 

log 1Îµ
1
1
N = O log
=
O
log
log
,
Î³
Îµ
log Ï1
and recall from the proof of Theorem 2 that from Q(N ) we
can obtain the required uÌ‚i â€™s and Î»Ì‚i â€™s.

Tensor Decomposition via Simultaneous Power Iteration

It remains to bound the number of samples needed in this
phase, which is
N
X


|Jt | â‰¤ O

t=1

log(dN )
âˆ†2

X
N
t=1

1
.
Î²t2

t

As Î²t = max{Ï2 âˆ’1 , 2Îµ }, we have Î²t = 2Îµ for t â‰¥ N âˆ’ 1
t
N âˆ’2
t
N âˆ’2
and Î²t = Î²N âˆ’2 Ï2 âˆ’2
â‰¥ 2Îµ Ï2 âˆ’2
for t â‰¤ N âˆ’ 2.
Therefore,


N
N âˆ’2
X
1
8
4 X 2(2N âˆ’2 âˆ’2t )
1
â‰¤ 2+ 2
Ï
â‰¤O
,
Î²2
Îµ
Îµ t=1
Îµ2 (1 âˆ’ Ï4 )
t=1 t
1
1âˆ’Ï4
Î»2
maxiâˆˆ[k] Î»i+1
2
i

where

=

1+Ï4
1âˆ’Ï8

2
1âˆ’Ï8

â‰¤

= miniâˆˆ[k]

Î»2i âˆ’Î»2i+1
Î»2i

and 1 âˆ’ Ï8 = 1 âˆ’
= Î³. As a result, we

have
N
X
t=1


|Jt | â‰¤ O

log(dN )
âˆ†2 Î³Îµ2


.

Combining this with the number of samples for the initialization phase, including that for finding wÌ„, we have the
stated sample complexity bound of the theorem.

References
Hardt, Moritz and Price, Eric. The noisy power method: A
meta algorithm with applications. In Advances in Neural
Information Processing Systems, pp. 2861â€“2869, 2014.
Hom, Roger A and Johnson, Charles R. Topics in matrix
analysis. Cambridge University Press, New York, 1991.
Laurent, B. and Massart, P. Adaptive estimation of a
quadratic functional by model selection. Annals of
Statistics, 28(5):1302â€“1338, 2000.
Li, Chun-Liang, Lin, Hsuan-Tien, and Lu, Chi-Jen. Rivalry
of two families of algorithms for memory-restricted
streaming PCA. In Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 473â€“481, 2016.
Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Prateek. Memory limited, streaming PCA. In Advances in
Neural Information Processing Systems, pp. 2886â€“2894,
2013.
Schmitt, Bernhard A. Perturbation bounds for matrix
square roots and pythagorean sums. Linear algebra and
its applications, 174:215â€“227, 1992.
Stewart, Gilbert W. and Sun, Ji-Guang. Matrix Perturbation Theory. Academic Press, 1990.

Tropp, Joel A. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389â€“434, 2012.
Zhu, Peizhen. and Knyazev, Andrew V. Angles between subspaces and their tangents. Arxiv preprint at
arXiv:1209.0523, 2012.

