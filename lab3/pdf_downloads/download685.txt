Canopy — Fast Sampling with Cover Trees

A. Alias Sampler
A key component is the alias sampler of (Walker, 1977). Given an arbitrary discrete probability distribution on n outcomes, it
allows for O(1) sampling once an O(n) preprocessing step has been performed. Hence, drawing n observations a distribution
over n outcomes costs an amortized O(1) per sample. Given probabilities πi with π ∈ Pn the algorithm proceeds as follows:
• Decompose {1, . . . n} into sets L, H with i ∈ L if πi < n−1 and i ∈ H otherwise.
• For each i ∈ L pick some j ∈ H.
– Append the triple (i, j, πi ) to an array A
– Set residual πj0 := πj + πi − n−1
– If πj0 > n−1 return πj0 to H, otherwise to L.
Preprocessing takes O(n) computation and memory since we remove one element at a time from L.
• To sample from the array pick u ∼ U (0, 1) uniformly at random.
• Choose the tuple (i, j, πi ) at position bunc.
• If u − n−1 bunc < πi return i, else return j.
This step costs O(1) operations and it follows by construction that i is returned with probability πi . Now we need a data
structure that will allow us to sample many objects in bulk without the need to inspect each item individually. Cover trees
satisfy this requirement.

B. Rejection Sampling
B.1. Flat Clusters
The proof for the proposed rejection sampler in case of sampling a cluster for a single observation x is as follows. If we
approximate p(x|θz ) by some qz such that
e− p(x|θz ) ≤ qz ≤ e p(x|θz )

(13)

then it follows that a sampler drawing z from
qz p(z)
0
0
z 0 qz p(z )

z∼P

(14)

and then accepting with probability e− qz−1 p(x|θz ) will draw from p(z|x). To prove this, we begin by computing the
probability of this sampler r(z) to return a particular value z. The sampler returnsP
z when it (a) samples and
Paccepts z, or (b)
samples any value, rejects it to proceed to next iteration of sampling. Using γ = z0 qz0 p(z 0 ) and γT = z0 p(x|θz0 )p(z 0 )
to denote normalization for proposal and true posterior respectively, we have:
r(z) =

X
qz p(z) − −1
qz0 p(z 0 )
e qz p(x|θz ) +
(1 − e− qz−1
r(z)
0 p(x|θz 0 ))
γ
γ
0
z

e−
r(z) X
e− X
=
p(z)p(x|θz ) +
qz0 p(z 0 ) − r(z)
p(x|θz0 )p(z 0 )
γ
γ
γ
0
0
z

z

(15)

e−
e−
p(z)p(x|θz ) + r(z) − r(z)
γT
=
γ
γ
p(z)p(x|θz )
r(z) =
γT
Hence the procedure will draw from the true posterior p(z|x).
B.2. Clusters Arranged in Cover Tree
We now extend the above proof strategy when the clusters are arranged in a cover tree, thereby proving the correctness of
our rejection sampler in Sec. 3.2.2.

Canopy — Fast Sampling with Cover Trees

Similar to previous case, we approximate p(x|θz ) for z in level i by some qz such that
e−i p(x|θz ) ≤ qz ≤ ei p(x|θz ).

(16)

Note that approximation error i now depends on the location of the cluster in the cover tree. To be specific, if the cluster z
is located at level i, then i = 2i kφ̄(x)k. Also, we assume the path to reach the node z starting from its (grand) parent at
level ı̂ is given by T = [T (ı̂), T (ı̂ − 1), ..., T (i)], with T (i) = z.
To prove the correctness of our rejection sampler in Sec. 3.2.1, we simply show that probability of this sampler to return a
particular value z is equal to the true posterior. The sampler returns z when it (a) reaches the corresponding node in the
cover tree and accepts it, or (b) rejects or exits to proceed to next iteration of sampling. So, the probability of this sampler to
return z is given by:
+
r(z)
E
(17)
r(z) =
A(z)
|{z}
| {z }
Probability of the
sampler rejecting or exiting

Probability of the
sampler accepting z

We calculate
these individual terms beginning with the probability of sampler accepting z. Using γ as defined in (10) and
P
γT = z0 p(x|θz0 )p(z 0 ), we have:







 ı̂ 



i0 −1
0
0
0
Y
e β(ı̂, T (ı̂))p(x|θT (ı̂) ) 
e
β(i − 1, T (i − 1))p(x|θT (i0 −1) )
p(T (i ))p(x|θT (i0 ) )

A(z) =
1− 0


0
γ
e i β(i0 , T (i0 ))p(x|θT (i0 ) ) ei0 β(i0 , T (i0 ))p(x|θT (i0 ) ) − p(T (i0 ))p(x|θT (i0 ) ) 

|
{z
} |i =i+1
{z
}
{z
}
|
|

 {z }
Selecting the first parent (Step 4)
ı̂

The loop
(Step 5)

Rejecting the nodes (Step 5iii)

Selecting the next node (Step 5ii)

×

p(z)p(x|θz )
ei β(i, T (i))p(x|θT (i) )
|
{z
}
Accepting node z

#
ı̂
e β(ı̂, T (ı̂))p(x|θT (ı̂) ) Y ei0 −1 β(i0 − 1, T (i0 − 1))p(x|θT (i0 −1) )
p(z)p(x|θz )
=

0
0

0
i
i
0
γ
e β(i , T (i ))p(x|θT (i ) )
e β(i, T (i))p(x|θT (i) )
i0 =i+1
 i

ı̂
e β(ı̂, T (ı̂))p(x|θT (ı̂) ) e β(i, T (i))p(x|θT (i) )
p(z)p(x|θz )
=


i
ı̂
γ
e β(ı̂, T (ı̂))p(x|θT (ı̂) ) e β(i, T (i))p(x|θT (i) )
ı̂

=

p(z)p(x|θz )
γ

"

(by telescoping)
(18)

Next, the probability of rejecting or exiting from the sampler is one minus probability of accepting any node z, i.e.
X
E =1−
A(z 0 )
z 0 ∈Z

X p(z 0 )p(x|θz0 )
γ
z 0 ∈Z
γT
=1−
γ
=1−

(19)

Plugging back the acceptance and exit probabilities into (17):
r(z) = A(z) + r(z)E


p(z)p(x|θz )
γT
=
+ r(z) 1 −
γ
γ
p(z)p(x|θz )
γT
=
+ r(z) − r(z)
γ
γ
p(z)p(x|θz )
r(z) =
γT

(20)

Canopy — Fast Sampling with Cover Trees

(a) Expansion rate

(b) Separation property

(c) Covering property

Figure 6. Illustration of various properties of covering tree.

Hence the procedure will draw from the true posterior p(z|x).
The above describes a rejection sampler that keeps on upper-bounding the probability of accepting a particular parameter or
any of its children. It is as aggressive as possible at retaining tight lower bounds on the acceptance probability such that
not too much effort is wasted in traversing the cover tree to he bottom. In other words, we attempt to reject as quickly as
possible. Some computational considerations are in order:
D
E
1. The computationally most expensive part is to compute the inner products φ̃(x), θ̃z .
2. As soon as we compute this value for a particular θ̃z we cache it at the corresponding vertex of the cover tree.
3. To avoid expensive bookkeeping we attach to each vertex two variables: the value of the last compute inner product
and the observation ID of x that it is associated with. +

C. Cover Trees
Cover Trees (Beygelzimer et al., 2006) and their improved version (Izbicki & Shelton, 2015) form a hierarchical data
structure that allows fast retrieval in logarithmic time. The key properties for the purpose of this paper are that it allows for
O(n log n) construction time, O(log n) retrieval, and that it only depends polynomially on the expansion rate (Karger &
Ruhl, 2002) of the underlying space, which we refer to as c. Moreover, the degree of all internal nodes is well controlled,
thus giving guarantees for retrieval (as exploited in (Beygelzimer et al., 2006)), and for sampling (as we will be using in this
paper).
The expansion rate of a set, due to (Karger & Ruhl, 2002) captures several key properties.
Definition 2 (Expansion Rate) Denote by Bρ (r) a ball of radius of r centered at ρ. Then a set S has a (l, c) expansion
rate iff all r > 0 and ρ ∈ S satisfy
|Bρ (r) ∩ S| ≥ l =⇒ |Bρ (2r) ∩ S| ≤ c |Bρ (r) ∩ S| .

(21)

In the following we set l = O(log |S|), thus referring to c simply as the expansion rate of S.
Cover trees are defined as an infinite succession of levels Si with i ∈ Z. Each level i contains (a nested subset of) the data
with the following properties:
•
•
•
•

Nesting property: Si ⊆ Si−1 .
Separation property: All x, x0 ∈ Si satisfy kx − x0 k ≥ 2i .
All x ∈ Si−1 have a parent in x0 ∈ Si , possibly with x = x0 , with kx − x0 k ≤ 2i .
As a consequence, the subtree for any x ∈ Si has distance at most 2i+1 from x.

Clearly we need to reperesent each x only once, namely in terms of Si with the largest i for which x ∈ Si holds. This data
structure has a number of highly desirable properties, as proved in (Beygelzimer et al., 2006). We list the most relevant ones
below:
• The depth of the tree in terms of its explicit representation is at most O(c2 log n).

Canopy — Fast Sampling with Cover Trees

• The maximum degree of any node is O(c4 ).
• Insertion & removal take at most O(c6 log n) time.
• Retrieval of the nearest neighbor takes at most O(c12 log n) time.
• The time to construct the tree is O(c6 n log n).
The fast lookup of cover tree is built upon the implicit assumption in terms of the distinguishability of parameters θz , which
we also borrow in Canopy. This is related to the issue that if we had many choices of θz that, a-priori, all looked quite
relevant yet distinct, we would have no efficient means of evaluating them short of testing all by brute force. Note that this
could be achieved, e.g. by using the fast hash approximation of a sampler in (Ahmed et al., 2012). This is complementary to
the present paper.

D. Theoretical Analysis
Some more conclusions we can make about the algorithm Canopy I:
Remark 3 (Rejection Sampler) The same reasoning yields a rejection sampler since
̄+1
p(z|x̄)
≥ e−kφ(x)−φ(x̄)kkθz k ≥ e−2 L .
p(z|x)

(22)

Here we may bound each term (and the normalization) in computing p(z|x) appropriately.
Remark 4 The efficiency of the sampler increases as the sample size m increases. In particular,
an increase of m by O(c4 )
√
is guaranteed to decrease ̄ by 1, thus increasing the acceptance probability π from π to π. This follows from the fact that
each node in the cover tree has at most O(c4 ) children.
Remark 5 There is no need to build a cover tree to a level beyond ̄ since we do not exploit the improvement. This could be
used to remove the logarithmic dependence O(n log n) in constructing the cover tree and reduce it to O(n̄).

E. Feature Extraction
E.1. Denoising Autoencoder for MNIST
The autoencoder consists of an encoder with fully connected layers of size (28x28)-1000-500-250-30 and a symmetric
decoder. The thirty units in the code layer were linear and all the other units were logistic. The network was trained on the 8
million images using mean square error loss.
E.2. Denoising Autoencoder for CIFAR100
The autoencoder consists of an encoder with convolutional layers of size (3x32x32)-(64, 5, 5)-(32, 5, 5)-(16, 4, 4) and having
a 2x2 max pooling after each convolutional layer. The decoder is symmetric with max pooling replaced by upsampling. The
256 units in the code layer were linear and all the other internal units were RelU while the final layer was sigmoid. The
network was trained on the 50 thousand images using mean square error loss.
E.3. ResNet for ImageNet
We use the state of the art deep convolutional neural network (DCNN), based on the ResNet (”Residual Network”)
architecture (He et al., 2015; 2016). ResNet consists of small building blocks of layers which learn the residual functions
with reference to the input. It is demonstrated that ResNet is able to train networks that are substantially deeper without the
problem of noisy backpropagation gradient. For feature extraction We use a 200 layer ResNet that is trained on a task of
classification on ImageNet. In the process, the network learned which high-level visual features (and combinations of those
features) are important. After training the model, we remove the final classification layer of the network and extract from the
next-to-last layer of the DCNN, as the representation of the input image which is of dimension 2048.

Canopy — Fast Sampling with Cover Trees

F. Further Experimental Results

Clusters
10

100

Clusters
10

100

Clusters
100

500

Clusters
100

500

Method
EM
SEM
Canopy I
Canopy II

s/iter
39.588 ± 1.801
7.124 ± 0.241
7.453 ± 0.255
7.534 ± 0.320

EM
SEM
Canopy I
Canopy II

512.185 ± 13.295
10.085 ± 0.162
6.882 ± 0.174
6.483 ± 0.298

Method
EM
SEM
Canopy I
Canopy II

s/iter
6.595 ± 0.230
0.943 ± 0.037
0.932 ± 0.027
1.008 ± 0.053

EM
SEM
Canopy I
Canopy II

56.640 ± 1.060
4.006 ± 0.050
1.220 ± 0.025
1.015 ± 0.029

Method
EM
SEM
Canopy I
Canopy II

MNIST8m - Direct
Random I
Random II
LLH
Purity
LLH
Purity
3.04 ×107
32.39%
3.05 ×107
30.76%
3.04 ×107
32.33%
3.03 ×107
30.65%
1.49 ×107
42.12%
1.49 ×107
40.51%
1.49 ×107
42.85%
1.49 ×107
40.69%

KMeans++
LLH
Purity
3.04 ×107
30.81%
3.04 ×107
30.61%
1.49 ×107
40.41%
1.49 ×107
40.95%

CoverTree
LLH
Purity
3.05 ×107
30.50%
3.04 ×107
31.69%
1.50 ×107
42.84%
1.50 ×107
42.59%

3.27 ×107
3.34 ×107
2.02 ×107
1.91 ×107

3.28 ×107
3.34 ×107
2.01 ×107
1.90 ×107

3.32 ×107
3.33 ×107
2.02 ×107
1.90 ×107

53.20%
3.26 ×107
53.24%
53.19%
3.34 ×107
53.21%
53.39%
2.04 ×107
53.53%
60.19%
1.90 ×107
61.09%
MNIST8m - Embedding
Random I
Random II
LLH (×107 ) Purity LLH (×107 ) Purity
-4.35 ×105
58.43%
-4.36 ×105
63.14%
-4.35 ×105
58.43%
-4.35 ×105
62.05%
-4.35 ×105
58.78%
-4.36 ×105
61.61%
-4.35 ×105
58.78%
-4.35 ×105
62.30%
-3.93 ×105
-3.93 ×105
-3.96 ×105
-3.97 ×105

52.45%
52.42%
53.88%
60.61%

KMeans++
LLH (×107 ) Purity
-4.35 ×105
63.19%
-4.36 ×105
61.44%
-4.35 ×105
64.46%
-4.36 ×105
61.69%

53.10%
53.52%
52.69%
60.29%

CoverTree
LLH (×107 ) Purity
-4.34 ×105
63.22%
-4.35 ×105
60.58%
-4.35 ×105
58.78%
-4.35 ×105
58.78%

-3.94 ×105
-3.94 ×105
-3.97 ×105
-3.97 ×105

s/iter
78.019 ± 10.702
1.055 ± 0.095
1.027 ± 0.095
1.190 ± 0.099

83.95%
-3.94 ×105
82.33%
83.99%
-3.93 ×105
83.37%
83.44%
-3.96 ×105
83.20%
82.77%
-3.97 ×105
83.21%
CIFAR100 - Direct
Random I
Random II
LLH
Purity
LLH
Purity
2.86 ×106
14.27%
3.03 ×106
13.31%
2.93 ×106
14.08%
2.93 ×106
14.12%
3.20 ×106
12.98%
3.36 ×106
12.43%
2.99 ×106
12.87%
3.08 ×106
13.23%

KMeans++
LLH
Purity
3.09 ×106
13.84%
2.86 ×106
14.75%
3.21 ×106
13.55%
3.28 ×106
13.72%

CoverTree
LLH
Purity
3.09 ×106
14.19%
3.00 ×106
14.90%
3.25 ×106
12.91%
3.08 ×106
12.87%

EM
SEM
Canopy I
Canopy II

407.764 ± 18.160
6.486 ± 0.613
2.745 ± 0.225
1.908 ± 0.152

3.37 ×106
3.39 ×106
3.38 ×106
3.17 ×106

3.27 ×106
3.36 ×106
3.45 ×106
3.19 ×106

3.31 ×106
3.22 ×106
3.44 ×106
3.19 ×106

Method
EM
SEM
Canopy I
Canopy II

s/iter
12.589 ± 0.255
0.491 ± 0.022
0.315 ± 0.014
0.313 ± 0.124

25.19%
3.31 ×106
24.70%
25.14%
3.30 ×106
24.33%
22.35%
3.50 ×106
22.14%
22.68%
3.18 ×106
22.83%
CIFAR100 - Embedding
Random I
Random II
LLH
Purity
LLH
Purity
5.45 ×105
12.38%
5.50 ×105
12.14%
5.46 ×105
12.21%
5.53 ×105
11.57%
5.01 ×105
12.34%
5.04 ×105
11.96%
5.00 ×105
12.50%
5.02 ×105
11.97%

KMeans++
LLH
Purity
5.50 ×105
12.25%
5.45 ×105
12.72%
4.99 ×105
13.16%
4.99 ×105
13.01%

CoverTree
LLH
Purity
5.46 ×105
12.59%
5.47 ×105
12.68%
5.06 ×105
12.30%
5.02 ×105
12.29%

EM
SEM
Canopy I
Canopy II

62.520 ± 1.135
2.276 ± 0.112
0.963 ± 0.061
0.333 ± 0.101

6.94 ×105
6.92 ×105
6.25 ×105
6.20 ×105

6.86 ×105
6.85 ×105
6.14 ×105
6.12 ×105

6.86 ×105
6.85 ×105
6.24 ×105
6.18 ×105

19.17%
18.97%
20.07%
22.26%

6.96 ×105
6.93 ×105
6.21 ×105
6.16 ×105

18.93%
18.64%
19.19%
21.61%

83.44%
83.05%
83.48%
82.66%

26.03%
26.16%
24.03%
24.91%

21.13%
21.16%
21.57%
23.18%

-3.94 ×105
-3.95 ×105
-3.96 ×105
-3.97 ×105

82.77%
83.44%
83.22%
82.66%

25.59%
25.39%
22.31%
22.71%

21.05%
21.20%
20.04%
22.25%

Table 1. Comparison of ESCA, Canopy I and Canopy II on cluster purity and loglikelihood on real, benchmark datasets–MNIST8m and
CIFAR-100. Additionally, standard deviations are shown for 5 runs.

F.1. Image Clustering
We sample images from varied sized clusters, as described below, to study the semantic concept they usually represent: (a)
> 10k members: As our dataset is extracted from Flickr, a photo sharing platform, it is heavily biased towards everyday
objects like humans, flowers, birds, etc. We found several consistent clusters containing people (sitting, standing, crowd). (b)
> 5 but < 10k members: These contains less common semantic groups like swings, transmission lines, etc, out of which
some are absent as explicit concepts in underlying Resnet model. (c) < 5 members: We found around 15% small sized
clusters which are typically outliers containing less than 5 images. Fig. 7 contains more examples.

Canopy — Fast Sampling with Cover Trees

Figure 7. Illustration of concepts captured by clustering images in the feature space extracted by ResNet (He et al., 2015; 2016). Figure
shows four closest images of seven more randomly selected clusters (one in each row) possibly denoting the semantic concepts of ‘electrical
transmission lines’, ‘image with text’, ‘lego toys’, ‘lightening’, ‘Aurora’, ‘buggy’ and ‘eyes’. Few of the concepts are discovered by
clustering as Resnet received supervision only for 1000 categories (for example does not include label ‘lightening’, ‘thunder’, or ‘storm’).
Full set of 1000 imagenet label can be seen at http://image-net.org/challenges/LSVRC/2014/browse-synsets.

Canopy — Fast Sampling with Cover Trees

G. Graphical Explanation
We now present insights about our approach graphically.
2/37

3/37

Motivation

Inference Strategy



They are used in diverse fields ranging from text, images,
to user modelling and content recommendations.



Inference is often slow
8.19E+03

-10000

2.05E+03

-12500

5.12E+02

-15000

1.28E+02

-17500

3.20E+01

EM

ESCA

Canopy I

Canopy II



Inference using Gibbs sampling, stochastic EM, or
stochastic variational methods requires drawing from

Log-likelihood

Latent variable models (LVM), such as Mixture Models,
Latent Dirichlet Allocation, are popular tools in statistical
data analysis.

Time (s/iter)



z
Global

-20000

4/37

5/37

Inference Strategy

Insights



Inference using Gibbs sampling, stochastic EM, or
stochastic variational methods requires drawing from



Assume exponential family, i.e.



Local

For example assume we have following data:

z
Global

Local

6/37

7/37

Insights

Insights



For example assume we have following data:



For example assume we have following data:



Two key observations



Two key observations




Points close by will have similar posteriors
No need to consider clusters far away






Points close by will have similar posteriors
No need to consider clusters far away

Two tools to exploit the observations



Cover trees
Metropolis Hasting sampling

Canopy — Fast Sampling with Cover Trees

8/37

9/37

Cover Tree

Cover Tree



Cover tree is a hierarchical data structure



Cover tree is a hierarchical data structure



Covering property:

10/37

11/37

Cover Tree

Computational Cost of Cover Trees



Cover tree is a hierarchical data structure



Covering property:



Separating property:



Does not depend on the dimension of the data



c: Expansion rate of data or Hausdorff dimension
(special case of fractal dimension)

12/37

13/37

Insights

Metropolis Hasting Sampling



For example assume we have following data:



Two key observations







Enables us to construct sound sampler that incorporates
our intuitions

Points close by will have similar posteriors
No need to consider clusters far away

Accept/
Reject

Two tools to exploit the observations



Cover trees
Metropolis Hasting sampling

An easy to draw
distribution

Acceptance
probability

Sample
from p
Only need to
look at a few
probabilities!

Canopy — Fast Sampling with Cover Trees

14/37

15/37

How to Design a Good Proposal?

How to Design a Good Proposal?



For example assume we have following data:



For example assume we have following data:



Suppose for each point x we can find surrogates

16/37

17/37

How to Design a Good Proposal?

Outline



For example assume we have following data:



Background







Suppose for each point x we can find surrogates



Then







Canopy: Proposed Method


becomes a good proposal for

Moderate number of clusters
Large number of clusters

Experimental Results


Compute alias table and re-use for many points
Cost for sampling from proposal given alias table is O(1)

Latent Variable Models
Cover tree
Metropolis Hastings



Synthetic data
Images

18/37

19/37

Canopy I – Method 1

Canopy I – Method 2



Data:

Build a cover tree on data points – Cost

Cover Tree:



Build a cover tree on data points – Cost



Pick an accuracy level having

Data:

elements

Cover Tree:

1

1

2
3

4

6
5

7

8

2
9

3

4

6
5

7

8

9

Canopy — Fast Sampling with Cover Trees

20/37

21/37

Canopy I – Method 3

Canopy I – Method 4



Build a cover tree on data points – Cost



Pick an accuracy level having

Data:

elements

Cover Tree:



Build a cover tree on data points – Cost



Pick an accuracy level having

Data:

elements

Cover Tree:

1

1

2
3

4

6
5

7

8

2
9

Surrogates:

22/37

23/37

Canopy I – Method 5

Canopy I – Method 6



Build a cover tree on data points – Cost



Pick an accuracy level



Build alias tables for

Data:

having



3

4

6
5

7

8

For each observation x perform Metropolis-Hastings

elements

– Cost
Cover Tree:

Data:

Sample from

1
2
Surrogates:

3

4

6
5

7

8

9

Surrogates:

24/37

25/37

Canopy I – Method 7

Canopy I – Method 8



For each observation x perform Metropolis-Hastings

Data:

Sample from

Surrogates:



For each observation x perform Metropolis-Hastings

Data:

Surrogates:

Propose in O(1)

Sample from

Accept/Reject

Propose in O(1)

9

Canopy — Fast Sampling with Cover Trees

26/37

27/37

Canopy I – Method 9

Canopy I – Method 10



For each observation x perform Metropolis-Hastings



For each observation x perform Metropolis-Hastings


Data:

Data:

Sample from

Accept/Reject

Surrogates:

For exponential families:

Sample from

Accept/Reject

Surrogates:

Propose in O(1)

Propose in O(1)

28/37

29/5

Large Number of Clusters

Canopy II – Method 1



Using first insight, cost reduced



Build a cover tree on cluster parameters – Cost



When moderate number of clusters, e.g.



Pick an accuracy level





Choose
Then alias table will be used at least K times – full amortization!
Total cost
Cluster Parameters:









Cover Tree:

When there many clusters, e.g.

1

Either high overhead of memory and computation,
Or granularity in x that is less precise than desired

2

3

4

Use second insight: not all clusters are relevant

8

Apply cover trees not only to observations but also to the
clusters themselves!

30/5

5
9

6
B

A

7

C

E

D

F

31/5

Canopy II – Method 2


The nodes at the selected
accuracy level act as coarse
approximation to the posterior

2

(Sec 3.2.2 of paper)

3

4
8



Canopy II – Method 3

1

5
9

A

6
B

C


7

D

E

F

1

The nodes at the selected
accuracy level act as coarse
approximation to the posterior



Treat this as a proposal for a
rejection sampler



(Sec 3.2.2 of paper)

2

3

4
8

5
9

A

6
B

C

7
D

E

F

Canopy — Fast Sampling with Cover Trees

32/5

33/5

Canopy II – Method 4


Canopy II – Method 5

1

The nodes at the selected
accuracy level act as coarse
approximation to the posterior



Treat this as a proposal for a
rejection sampler



Sample from the proposal



(Sec 3.2.2 of paper)

2

3

4
8

5
9

6
B

A

C



If the proposed sample is
accepted, exit



Else descend down the tree,
and obtain a finer proposal
around the region of interest



(Sec 3.2.2 of paper)

7
D

E

F

34/5

1
2

3

4
8

5
9

6
B

A

C

7
D

E

F

35/5

Canopy II – Method 6






Canopy II – Method 7

1

If the proposed sample is
accepted, exit

2

3

4

Else descend down the tree,
and obtain a finer proposal
around the region of interest

8

5
9

A



6
B

C

7
D

E

F

(Sec 3.2.2 of paper)

36/5

Sampler is as aggressive as
possible in rejecting early on
such that not much effort is
wasted in traversing the tree



The deeper we descend into the
tree, the less likely we reject



In worst case the cost is

1
2

3

4
8

5
9

A

6
B

C

7
D

∞

37/5

Canopy II – Full Picture


Canopy II – Descending both Trees
Clusters

Using both the trees allows to
deal with an aggregate of
clusters and data

This leads to a much smaller
observation group



Employ a MH
scheme as before



We propose from a
distribution where
both observations
and clusters are
grouped

8
4
3

2

5

1

7

6

Data

9





1



2
3



6
5

Recursively descend in both the trees while sampling

7

9

Until number of observations for a given cluster is too small
Then use the rejection sampler as describer earlier



Finally perform a MH accept/reject step



This reduces total cost



Acceptance probabilities are as high as in previous case

E

F

