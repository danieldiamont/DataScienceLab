The Price of Differential Privacy for Online Learning
(with Supplementary Material)

Naman Agarwal 1 Karan Singh 1

Abstract
We design differentially private algorithms for
the problem of online linear optimization in the
full√information and bandit settings with optimal
Õ( T )1 regret bounds. In the full-information
setting, our results demonstrate that ε-differential
privacy may be ensured for free
√ – in particular,

the regret bounds scale as O( T ) + Õ 1ε . For
bandit linear optimization, and as a special case,
for non-stochastic multi-armed bandits,the pro√ 
posed algorithm achieves a regret of Õ 1ε T ,
while the
known best regret bound
 previously

2
was Õ 1ε T 3 .

1. Introduction
In the paradigm of online learning, a learning algorithm
makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past
queries. In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees.
Consequently, online learning algorithms are well suited
to dynamic and adversarial environments, where real-time
learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements. In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms. A natural notion of privacy in
such settings is differential privacy (Dwork et al., 2006)
which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as
opposed to when it is absent in a dataset.
In this paper, we design differentially private algorithms for
*

Equal contribution 1 Computer Science, Princeton University, Princeton, NJ, USA. Correspondence to:
Naman Agarwal <namana@cs.princeton.edu>, Karan Singh
<karans@cs.princeton.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
1
Here the Õ(·) notation hides polylog(T ) factors.

online linear optimization with near-optimal regret, both in
the full information and partial information (bandit) settings. This result improves the known best regret bounds
for a number of important online learning problems – including prediction from expert advice and non-stochastic
multi-armed bandits.
1.1. Full-Information Setting: Privacy for Free
For the full-information setting where the algorithm gets
to see the complete loss vector every round, we design
ε-differentially
algorithms with regret bounds that

√ private

T + Õ 1ε (Theorem 3.1), partially resolvscale as O
ing an open question
 √  to improve the previously best known
bound of O 1ε T posed in (Smith & Thakurta, 2013).
A decomposition of the bound on the regret bound of this
form implies that when ε ≥ √1T , the regret incurred by
the differentially private algorithm matches the optimal regret in the non-private setting, i.e. differential privacy is
free. Moreover even when ε ≤ √1T , our results guarantee
a sub-constant regret per round in contrast to the vacuous
constant regret per round guaranteed by existing results.
Concretely, consider the case of online linear optimization
over the cube, with unit l∞ -norm-bounded loss vectors. In
this setting, (Smith
& Thakurta, 2013) achieves a regret
√
bound of O( 1ε N T ), which is meaningful only if T ≥ εN2 .
√
Our theorems imply a regret bound of Õ( N T + Nε ). This
is an improvement on the previous bound regardless of the
value of ε. Furthermore, when T is between Nε and εN2 , the
previous bounds are vacuous whereas our results are still
meaningful. Note that the above arguments show an improvement over existing results even for moderate value of
ε. Indeed, when ε is very small, the magnitude of improvements are more pronounced.
Beyond the separation between T and ε, the key point of
our analysis is that we obtain bounds for general regularization based algorithms which adapt to the geometry of
the underlying problem optimally, unlike the previous algorithms (Smith & Thakurta, 2013) which utilizes euclidean
regularization. This allows our results
√ to get rid of a polynomial dependence on N (in the T term) in some cases.

The Price of Differential Privacy for Online Learning

Online linear optimization over the sphere and prediction
with expert advice are notable examples.
We summarize our results in Table 1.1.
1.2. Bandits: Reduction to the Non-private Setting
In the partial-information (bandit) setting, the online learning algorithm only gets to observe the loss of the prediction it prescribed. We outline a reduction technique that
translates a non-private bandit algorithm to a differentially
√
private bandit algorithm, while retaining the Õ( T ) dependency of the regret bound on the number of rounds of
play (Theorem 4.5). This allows us to derive the first εdifferentially private
√ algorithm for bandit linear optimization achieving Õ( T ) regret, using the algorithm for the
non-private setting from (Abernethy et al., 2012). This answers
√ a question from (Smith & Thakurta, 2013) asking if
Õ( T ) regret is attainable for differentially private linear
bandits .
An important case of the general bandit linear optimization
framework is the non-stochastic multi-armed bandits problem(Bubeck et al., 2012b), with applications for website
optimization, personalized medicine, advertisement placement and recommendation systems. Here, we propose
an ε-differentially
private algorithm which enjoys a re√
gret of Õ( 1ε N T log N ) (Theorem 4.1), improving on the
2
previously best attainable regret of Õ( 1ε N T 3 )(Smith &
Thakurta, 2013).
We summarize our results in Table 1.2.
1.3. Related Work
The problem of differentially private online learning was
first considered in (Dwork et al., 2010), albeit guaranteeing privacy in a weaker setting – ensuring the privacy of
the individual entries of the loss vectors. (Dwork et al.,
2010) also introduced the tree-based aggregation scheme
for releasing the cumulative sums of vectors in a differentially private manner, while ensuring that the total amount
of noise added for each cumulative sum is only polylogarithmically dependent on the number of vectors. The
stronger notion of privacy protecting entire loss vectors was
first studied in (Jain et al., 2012), where gradient-based algorithms were proposed that achieve
pri √ (ε, δ)-differntial

1
1
vacy and regret bounds of Õ ε T log δ . (Smith &
Thakurta, 2013) proposed a modification of Follow-the-
Approximate-Leader template to achieve Õ 1ε log2.5 T
regret for strongly
loss functions, implying a regret
 √ convex

bound of Õ 1ε T for general convex functions. In addition, they also demonstrated that under bandit feedback,
 2 it
is possible to obtain regret bounds that scale as Õ 1ε T 3 .
(Dwork et al., 2014a; Jain & Thakurta, 2014) proved that

in the special case of prediction with expert
√ advice setting,
it is possible to achieve a regret of O 1ε T log N . While
most algorithms for differentially private online learning
are based on the regularization template, (Dwork et al.,
2014b) used a perturbation-based algorithm to guarantee
(ε, δ)-differential privacy for the problem of online PCA.
(Tossou & Dimitrakakis, 2016) showed that it is possible
to design ε-differentially private algorithms for the stochastic multi-armed bandit problem with a separation of ε, T
for the regret bound. Recently, an independent work due
to (Tossou & Dimitrakakis, 2017), which we were made
aware
 √of after the first manuscript, also demonstrated a
Õ 1ε T regret bound in the non-stochastic multi-armed
bandits setting. We match their results (Theorem 4.1), as
well as provide a generalization to arbitrary convex sets
(Theorem 4.5).
1.4. Overview of Our Techniques
Full Information Setting:
We consider the two
well known paradigms for online learning, Folllow-theRegularized-Leader (FTRL) and Folllow-the-PerturbedLeader (FTPL). In both cases, we ensure differential privacy by restricting the mode of access to the inputs (the
loss vectors). In particular, the algorithm can only retrieve
estimates of the loss vectors released by a tree based aggregation protocol (Algorithm 2) which is a slight modification of the protocol used in (Jain et al., 2012; Smith &
Thakurta, 2013). We outline a tighter analysis of the regret
minimization framework by crucially observing that in case
of linear losses, the expected regret of an algorithm that
injects identically (though not necessarily independently)
distributed noise per step is the same as one that injects a
single copy of the noise at the very start of the algorithm.
The regret analysis of Follow-the-Leader based algorithm
involves two components, a bias term due to the regularization and a stability term which bounds the change in the
output of the algorithm per step. In the analysis due to
(Smith & Thakurta, 2013), the stability term is affected by
the variance of the noise as it changes from step to step.
However in our analysis, since we treat the noise to have
been sampled just once, the stability analysis does not factor in the variance and the magnitude of the noise essentially appears as an additive term in the bias.
Bandit Feedback: In the bandit feedback setting, we show
a general reduction that takes a non-private algorithm and
outputs a private algorithm (Algorithm 4). Our key observation here (presented as Lemma 4.3) is that on linear
functions, in expectation the regret of an algorithm on a
noisy sequence of loss vectors is the same as its regret on
the original loss sequence as long as noise is zero mean.
We now bound the regret on the noisy sequence by conditioning out the case when the noise can be large and us-

The Price of Differential Privacy for Online Learning

F UNCTION C LASS (N D IMENSIONS )

P REDICTION WITH E XPERT A DVICE

Õ

√

T log N
ε

O NLINE LINEAR OPTIMIZATION OVER
THE S PHERE

Õ

O NLINE LINEAR OPTIMIZATION OVER
THE C UBE

Õ

√
√

Õ

O NLINE L INEAR O PTIMIZATION

NT
ε



NT
ε





B EST

O UR R EGRET B OUND

P REVIOUS
B EST
K NOWN R EGRET

N ON -

PRIVATE
R EGRET

O

√

O
O

√ 
T
ε

N log N log2 T
ε

T log N +
√

T+

N log2 T
ε

√
NT +
O

√
T+



N log2 T
ε
log2 T
ε







√
O( T log N )
√
O( T )
√
O( N T )
√
O( T )

Table 1. Summary of our results in the full-information setting. In the last row we suppress the constants depending upon X , Y.
F UNCTION C LASS (N D IMENSIONS )

P REVIOUS
B EST
K NOWN R EGRET


BANDIT L INEAR O PTIMIZATION

Õ


N ON - STOCHASTIC
BANDITS

M ULT- ARMED

Õ

2

T3
ε

O UR R EGRET B OUND



2

NT 3
ε

Õ

Õ

T
ε

T N log N
ε

N ON -

√
O( T )

√ 

√

B EST
PRIVATE
R EGRET



√
O( N T )

Table 2. Summary of our results in the bandit setting. In the first row we suppress the specific constants depending upon X , Y.

ing exploration techniques from (Bubeck et al., 2012a) and
(Abernethy et al., 2008).

properties of the specific decision set X and loss set Y.
(See (Hazan et al., 2016) for a survey of results.)

2. Model and Preliminaries

Following are three important instantiations of the above
framework.

This section introduces the model of online (linear) learning, the distinction between full and partial feedback scenarios, and the notion of differential privacy in this model.
Full-Information Setting: Online linear optimization
(Hazan et al., 2016; Shalev-Shwartz, 2011) involves repeated decision making over T rounds of play. At the beginning of every round (say round t), the algorithm chooses
a point in xt ∈ X , where X ⊆ RN is a (compact) convex
set. Subsequently, it observes the loss lt ∈ Y ⊆ RN and
suffers a loss of hlt , xt i. The success of such an algorithm,
across T rounds of play, is measured though regret, which
is defined as
X

T
T
X
Regret = E
hlt , xt i − min
hlt , xi
t=1

x∈K

t=1

where the expectation is over the randomness of the algorithm. In particular, achieving a sub-linear regret (o(T ))
corresponds to doing almost as good (averaging across T
rounds) as the fixed decision with the least loss in hindsight. In the non-private setting, a√number of algorithms
have been devised to achieve O( T ) regret, with additional dependencies on other parameters dependent on the

• Prediction with Expert Advice: Here the underlying
decision P
set is the simplex X = ∆N = {x ∈ Rn :
n
xi ≥ 0, i=1 xi = 1} and the loss vectors are constrained to the unit cube Y = {lt ∈ RN : klt k∞ ≤ 1}.
• OLO over the Sphere: Here the underlying decision is
the euclidean ball X = {x ∈ Rn : kxk2 ≤ 1} and the
loss vectors are constrained to the unit euclidean ball
Y = {lt ∈ RN : klt k2 ≤ 1}.
• OLO over the Cube: The decision is the unit cube
X = {x ∈ Rn : kxk∞ ≤ 1}, while the loss vectors
are constrained to the set Y = {lt ∈ RN : klt k1 ≤ 1}.
Partial-Information Setting: In the setting of bandit feedback, the critical difference is that the algorithm only gets
to observe the value hlt , xt i, in contrast to the complete
loss vector lt ∈ RN as in the full information scenario.
Therefore, the only feedback the algorithm receives is the
value of the loss it incurs for the decision it takes. This
makes designing algorithms for this feedback model challenging. Nevertheless for the general problem of bandit
linear optimization, (Abernethy et al., 2008) introduced a

The Price of Differential Privacy for Online Learning

computationally efficient algorithm that achieves
√ an optimal dependence of the incurred regret of O( T ) on the
number of rounds of play. The non-stochastic multi-armed
bandit (Auer et al., 2002) problem is the bandit version of
the prediction with expert advice framework.

bounded by

Differential Privacy: Differential Privacy (Dwork et al.,
2006) is a rigorous framework for establishing guarantees
on privacy loss, that admits a number of desirable properties such as graceful degradation of guarantees under
composition and robustness to linkage acts (Dwork et al.,
2014a).

where

Definition 2.1 ((ε, δ)-Differential Privacy). A randomized
online learning algorithm A on the action set X and the
loss set Y is (ε, δ)-differentially private if for any two sequence of loss vectors L = (l1 , . . . lT ) ⊆ Y T and L0 =
(l10 , . . . lT0 ) ⊆ Y T differing in at most one vector – that is to
say ∃t0 ∈ [T ], ∀t ∈ [T ] − {t0 }, lt = lt0 – for all S ⊆ X T ,
it holds that
P(A(L) ∈ S) ≤ eε P(A(L0 ) ∈ S) + δ
Remark 2.2. The above definition of Differential Privacy
is specific to the online learning scenario in the sense that
it assumes the change of a complete loss vector. This has
been the standard notion considered earlier in (Jain et al.,
2012; Smith & Thakurta, 2013). Note that the definition
entails that the entire sequence of predictions produced by
the algorithm is differentially private.
Notation: We define kYkp = max{klt kp : lt ∈
Y}, kX kp = max{kxkp : x ∈ X }, and M =
maxl∈Y,x∈X |hl, xi|, where k · kp is the lp norm. By
Holder’s inequality, it is easy to see that M ≤ kYkp kX kq
for all p, q ≥ 1 with p1 + 1q = 1. We define the distribution LapN (λ) to be the distribution over RN such that
each coordinate is drawn independently from the Laplace
distribution with parameter λ.

3. Full-Information Setting: Privacy for Free
In this section, we describe an algorithmic template (Algorithm 1) for differentially private online linear optimization, based on Follow-the-Regularized-Leader scheme.
Subsequently, we outline the noise injection scheme (Algorithm 2), based on the Tree-based Aggregation Protocol
(Dwork et al., 2010), used as a subroutine by Algorithm 1
to ensure input differential privacy. The following is our
main theorem in this setting.
Theorem 3.1. Algorithm 1 when run with D = LapN (λ)
where λ = kYk1εlog T , regularization R(x), decision set
X and loss vectors l1 , . . . lt , the regret of Algorithm 1 is

v
u
T
u
X
Regret ≤ tDR
max(klt k∗∇2 R(x) )2 + DLap
t=1

DLap

x∈X



= EZ∼D0 max hZ, xi − min hZ, xi
x∈X

x∈X

DR = max R(x) − min R(x)
x∈X

x∈X

and D0 is the distribution induced by the sum of dlog T e
independent samples from D, k · k∗∇2 R(x) represents the
dual of the norm with respect to the hessian of R. Moreover,
the algorithm is ε-differentially private, i.e. the sequence
of predictions produced (xt : t ∈ [T ]) is ε-differentially
private.
Algorithm 1 FTRL Template for OLO
input Noise distribution D, Regularization R(x)
1: Initialize an empty binaryP
tree B to compute different
tially private estimates of s=1 ls .
dlog
T
e
2: Sample n10 , . . . n0
independently from D.
Pdlog T e i
3: L̃0 ← i=1
n0 .
4: for t = 1 to T do


5:
Choose xt = argminx∈X ηhx, L̃t−1 i + R(x) .
6:
Observe lt ∈ Y, and suffer a loss of hlt , xt i.
7:
(L̃t , B) ← TreeBasedAgg(lt , B, t, D, T ).
8: end for

The above theorem leads to following corollary where we
show the bounds obtained in specific instantiations of online linear optimization.
Corollary 3.2. Substituting the choices of λ, R(x) listed
below, we specify the regret bounds in each case.
1. Prediction with Expert
Choosing λ =
PN Advice:
N log T
and R(x) = i=1 xi log(xi ),
ε
!
p
N log2 T log N
T log N +
Regret ≤ O
ε
√

2. OLO over the Sphere Choosing λ =
R(x) = kxk22
Regret ≤ O

√

N log2 T
T+
ε
log T
ε

N log T
ε

and

!

and R(x) = kxk22
!
√
N log2 T
Regret ≤ O
NT +
ε

3. OLO over the Cube With λ =

The Price of Differential Privacy for Online Learning

Algorithm 2 TreeBasedAgg(lt , B, t, D, T )
input Loss vector lt , Binary tree B, Round t, Noise distribution D, Time horizon T
1: (L̃0 t , B) ← PrivateSum(lt , B, t, D, T ) – Algorithm 5 ((Jain et al., 2012)) with the noise added at
each node – be it internal or leaf – sampled independently from the distribution D.
2: st ← the binary representation of t as a string.
3: Find the minimum set
PSt of already populated nodes in
B that can compute s=1 ls .
4: Define Q = |S| ≤ dlog T e. Define rt = dlog T e − Q.
5: Sample n1t , . P
. . nrt t independently from D.
rt
0
nit .
6: L̃t ← L̃ t + i=1
output (L̃t , B).

3.1. Proof of Theorem 3.1
We first prove the privacy guarantee, and then prove the
claimed bound on the regret. For the analysis, we define
the random variable Zt to be the net amount of noise injected by the TreeBasedAggregation (Algorithm 2) on the
true partial sums. Formally, Zt is the difference between
cumulative sum of loss vectors and its differentially private
estimate used as input to the arg-min oracle.
Zt = L̃t −

t
X

each ni is independently sampled from LapN (λ).
Proof. By Theorem 9 ((Jain et al., 2012)), we have that the
sequence (L̃0 t : t ∈ [T ]) is ε-differentially private. Now the
sequence (L̃t : t ∈ [T ]) is ε-differentially private because
differential privacy is immune to post-processing(Dwork
et al., 2014a).
Note that the PrivateSum algorithm addsPexactly |S| int
dependent draws from the distribution D to s=1 ls , where
S is the minimum set of already populated nodes in the tree
that can compute the required prefix sum. Due to Line 6 in
Algorithm 2, it is made certain that every prefix sum released is a sum of the true prefix sum and dlog T e independent draws from D.
Regret Analysis: In this section, we show that for linear loss functions any instantiation of the Follow-theRegularized-Leader algorithm can be made differentially
private with an additive loss in regret.
Theorem 3.4. For any noise distribution D, regularization
R(x), decision set X and loss vectors l1 , . . . lt , the regret
of Algorithm 1 is bounded by
v
u
T
u
X
Regret ≤ tDR
max(klt k∗∇2 R(x) )2 + DD0
t=1

li

i=1

Further, let D0 be the distribution induced by summing of
dlog T e independent samples from D.
Privacy : To make formal claims about the quality of
privacy, we ensure input differential privacy for the algorithm – that is, we ensure that the
Ptentire sequence of partial sums of the loss vectors ( s=1 ls : t ∈ [T ]) is εdifferentially private. Since the outputs of Algorithm 1 are
strictly determined by the prefix sum estimates produced
by TreeBasedAgg, by the post-processing theorem, this
certifies that the entire sequence of choices made by the
algorithm (across all T rounds of play) (xt : t ∈ [T ]) is εdifferentially private. We modify the standard Tree-based
Aggregation protocol to make sure that the noise on each
output (partial sum) is distributed identically (though not
necessarily independently) across time. While this modification is not essential for ensuring privacy, it simplifies the
regret analysis.
Lemma 3.3 (Privacy Guarantees with Laplacian Noise).
Choose any λ ≥ kYk1εlog T . When Algorithm 2 A(D, T )
is run with D = LapN (λ), the following claims hold true:
• Privacy: The sequence (L̃t : t ∈ [T ]) is εdifferentially private.
Pdlog T e
• Distribution: ∀t ∈ [T ], Zt ∼
ni , where
i=1

x∈X

where DD0 = EZ∼D0 [maxx∈X hZ, xi − minx∈X hZ, xi],
DR = maxx∈X R(x) − minx∈X R(x), and k · k∗∇2 R(x)
represents the dual of the norm with respect to the hessian
of R.
Proof. To analyze the regret suffered by Algorithm 1, we
consider an alternative algorithm that performs a one-shot
noise injection – this alternate algorithm may not be differentially private. The observation here is that the alternate algorithm and Algorithm 1 suffer the same loss in expectation and therefore the same expected regret which we
bound in the analysis below.
Consider the following alternate algorithm which instead
of sampling noise Zt at each step instead samples noise at
the beginning of the algorithm and plays with respect to
that. Formally consider the sequence of iterates x̂t defined
as follows. Let Z ∼ D.
X
x̂1 , x1 , x̂t , argminx∈X ηhx, Z +
li i + R(x)
i

We have that
EZ1 ...ZT ∼D

" T
X
t=1

#
hlt , xt i = EZ∼D

" T
X

#
hlt , x̂t i

(1)

t=1

To see the above equation note that EZt ∼D [hlt , x̂t i] =
EZ∼D [hlt , xt i] since x, x̂t have the same distribution.

The Price of Differential Privacy for Online Learning

Therefore it is sufficient to bound the regret of the sequence
x̂1 . . . x̂t . The key idea now is to notice that the addition
of one shot noise does not affect the stability term of the
FTRL analysis and therefore the effect of the noise need
not be paid at every time step. Our proof will follow the
standard template of using the FTL-BTL (Kalai & Vempala, 2005) lemma and then bounding the stability term in
the standard way. Formally define the augmented series of
loss functions by defining
l0 (x) =

1
R(x) + hZ, xi
η

where Z ∼ D is a sample. Now invoking the Follow the
Leader, Be the Leader Lemma (Lemma 5.3, (Hazan et al.,
2016)) we get that for any fixed u ∈ X
T
X

lt (u) ≥

T
X

t=0

lt (x̂t+1 )

t=0

Therefore we can conclude that
T
X

[lt (x̂t ) − lt (u)]

(2)

(Abernethy et al., 2014), for the full-information setting,
we√establish that the regret guarantees obtained scale as
O( T )+Õ( 1ε log 1δ ). While Theorem 3.5 is valid for all in√
stances of online linear optimization and achieves O( T )
regret, it yields sub-optimal dependence on the dimension
of the problem. The advantage of FTPL-based approaches
over FTRL is that FTPL performs linear optimization over
the decision set every round, which is possibly computationally less expensive than solving a convex program every round, as FTRL requires.
Algorithm 3 FTPL Template for OLO – A(D, T ) on the
action set X , the loss set Y.
1: Initialize an empty binaryP
tree B to compute different
tially private estimates of s=1 ls .
dlog
T
e
2: Sample n10 , . . . n0
independently from D.
Pdlog T e i
3: L̃0 ← i=1
n0 .
4: for t = 1 to T do
5:
Choose xt = argminx∈X hx, L̃t−1 i.
6:
Observe the loss vector lt ∈ Y, and suffer hlt , xt i.
7:
(L̃t , B) ← TreeBasedAgg(lt , B, t, D, T ).
8: end for

t=1

≤

T
X

Theorem 3.5 (FTPL: Online Linear Optimization). Let
kX k2 = supx∈X kxk2 and kYk2 = suplt ∈Y klt k2 . Choosq
√
ing σ = max{kYk2 √N Tlog T , εN log T log logδ T } and

[lt (x̂t ) − lt (x̂t+1 )] + l0 (u) − l0 (x̂1 )

t=1

≤

T
X

1
[lt (x̂t ) − lt (x̂t+1 )] + DR + DZ
η
t=1

(3)

where DZ , maxx∈X (hZ, xi) − minx∈X (hZ, xi) Therefore we now need to bound the stability term lt (x̂t ) −
lt (x̂t+1 ). Now, the regret bound follows from the standard
analysis for the stability term in the FTRL scheme (see for
instance (Hazan et al., 2016)). Notice that the bound only
depends
P on the change in the cumulative loss per step i.e.
η ( t lt + Z), for which the change is the loss vector ηlt+1
across time steps. Therefore we get that
lt (x̂t ) − lt (x̂t+1 ) ≤ max kηlt k2η∇−2 R(x)
x∈X

(4)

Combining Equations (1), (3), (4) we get the regret bound
in Theorem 3.4.

3.2. Regret Bounds for FTPL
In this section, we outline algorithms based on the Followthe-Perturbed-Leader template(Kalai & Vempala, 2005).
FTPL-based algorithms ensure low-regret by perturbing the
cumulative sum of loss vectors with noise from a suitably chosen distribution. We show that the noise added in
the process of FTPL is sufficient to ensure differential privacy. More concretely, using the regret guarantees due to

D = N (0, σ 2 IN ), we have that RegretA(D,T ) (T ) is
1
4

O N kX k2 kYk2

√

N kX k2
log T
T+
log1.5 T log
ε
δ

!

Moreover the algorithm is ε-differentially private.
The proof of the theorem is deferred to the appendix.

4. Differentially Private Multi-Armed Bandits
In this section, we state our main results regarding bandit
linear optimization, the algorithms that achieve it and prove
the associated regret bounds. The following is our main
theorem concerning non-stochastic multi-armed bandits.
Theorem 4.1 (Differentially Private Multi-Armed Bandits). Fix loss vectors (l1 . . . lT ) such that klt k∞ ≤ 1.
When Algorithm 4 is run with parameters D = LapN (λ)
where λ = 1ε and algorithm A = Algorithm 5 with
q
log N
the following parameters: η =
2N T (1+2λ2 log N T ) ,
p
γ = ηN 1 + 2λ2 log N T and the exploration distribution µ(i) = N1 . The regret of the Algorithm 4 is
√

N T log T log N
O
ε
Moreover, Algorithm 4 is ε-differentially private

The Price of Differential Privacy for Online Learning

Bandit Feedback: Reduction to the Non-private Setting
We begin by describing an algorithmic reduction that takes
as input a non-private bandit algorithm and translates it into
an ε-differentially private bandit algorithm. The reduction
works in a straight-forward manner by adding the requisite
magnitude of Laplace noise to ensure differential privacy.
For the rest of this section, for ease of exposition we will
assume that both T and N are sufficiently large.

input ˜lt (respectively h˜lt , x̃t i) at time t. Let x∗ ∈ K be a
fixed point in the convex set. Then we have that
"
" T
##
X
E{Zt } EA
(hlt , x̃t i − hlt , x∗ i)
t=1

"
= E{Zt } EA

" T
X


h˜lt .x̃t i − h˜lt , x i

##

∗

t=1
0

Algorithm 4 A (A, D) – Reduction to the Non-private Setting for Bandit Feedback
Input: Online Algorithm A, Noise Distribution D.
1: for t = 0 to T do
2:
Receive x˜t ∈ X from A and output x̃t .
3:
Receive a loss value hlt , x˜t i from the adversary.
4:
Sample Zt ∼ D.
5:
Forward hlt x˜t i + hZt , x˜t i as input to A.
6: end for
Algorithm 5 EXP2 with exploration µ
Input: learning rate η; mixing coefficient γ; distribution µ
1
1: q1 = N
. . . N1 ∈ RN .
2: for t = 1,2 . . . T do
3:
Let pt = (1 − γ)qt + γµ and play it ∼ pt
4:
Estimate loss vector lt by l˜t = Pt+ eit eTit lt , with


Pt = Ei∼pt ei eTi
5:
Update the exponential weights,
qt+1 (i) = P

e−ηhei ,l̃t i qt (i)

i0

e−ηhei0 ,l̃t i qt (i0 )

6: end for

The following Lemma characterizes the conditions under
which Algorithm 4 is ε differentially private
Lemma 4.2 (Privacy Guarantees). Assume that each
loss vector lt is in the set Y ⊆ RN , such that
hl,x̃t i
maxt,l∈Y | kx̃
| ≤ B. For D = LapN (λ) where λ = Bε ,
t k∞
the sequence of outputs (x˜t : t ∈ [T ]) produced by the
Algorithm A0 (A, D) is ε-differentially private.
The following lemma charaterizes the regret of Algorithm
4. In particular we show that the regret of Algorithm 4
is, in expectation, same as that of the regret of the input
algorithm A on a perturbed version of loss vectors.
Lemma 4.3 (Noisy Online Optimization). Consider a loss
sequence (l1 . . . lT ) and a convex set X . Define a perturbed
version of the sequence as random vectors (˜lt : t ∈ [T ])
as ˜lt = lt + Zt where Zt is a random vector such that
{Z1 , . . . Zt } are independent and E[Zt ] = 0 for all t ∈ [T ].
Let A be a full information (or bandit) online algorithm
which outputs a sequence (x̃t ∈ X : t ∈ [T ]) and takes as

We provide the proof of Lemma 4.2 and defer the proof of
Lemma 4.3 to the Appendix Section B.
Proof of Lemma 4.2. Consider a pair of sequence of loss
vectors that differ at exactly one time step – say L =
(l1 , . . . lt0 . . . , lT ) and L0 = (l1 , . . . , lt0 0 , . . . lT ). Since
the prediction of produced by the algorithm at time step
any time t can only depend on the loss vectors in the past
(l1 , . . . lt−1 ), it is clear that the distribution of the output
of the algorithm for the first t0 rounds (x̃1 , . . . x̃t0 ) is unaltered. We claim that ∀I ⊆ R, it holds that
P(hlt0 + Zt0 , x̃t0 i ∈ I) ≤ eε P(hlt0 0 + Zt0 , x̃t0 i ∈ I)
Before we justify the claim, let us see how this implies that
desired statement. To see this, note that conditioned on the
value fed to the inner algorithm A at time t0 , the distribution of all outputs produced by the algorithm are completely determined since the feedback to the algorithm at
other time steps (discounting t0 ) stays the same (in distribution). By the above discussion, it is sufficient to demonstrate ε-differential privacy for each input fed (as feedback)
to the algorithm A.
For the sake of analysis, define ltF ict as follows. If x̃t = 0,
define ltF ict = 0 ∈ RN . Else, define ltF ict ∈ RN to be such
that (ltF ict )i = hltx̃,xi˜t i if and only if i = argmaxi∈[d] |x̃i |
and 0 otherwise, where argmax breaks ties arbitrarily. Define ˜ltF ict = ltF ict + Zt . Now note that h˜ltF ict , x̃t i =
hlt x˜t i + hZt , x˜t i.
It suffices to establish that each ˜ltF ict is ε-differentially
private. To argue for this, note that Laplace mechanism
(Dwork et al., 2014a) ensures the same, since the l1 norm
of ˜ltF ict is bounded by B.
4.1. Proof of Theorem 4.1
hl,x̃t i
Privacy: Note that since maxt,l∈Y | kx̃
| ≤ kYk∞ ≤ 1
t k∞
as x̃t ∈ {ei : i ∈ [N ]}. Therefore by Lemma 4.2, setting
λ = 1ε is sufficient to ensure ε-differential privacy.

Regret Analysis: For the purpose of analysis we define the
following pseudo loss vectors.
˜lt = lt + Zt

The Price of Differential Privacy for Online Learning

where by definition Zt ∼ LapN (λ). The following follows
from Fact C.1 proved in the appendix.
P(kZt k2∞ ≥ 10λ2 log2 N T ) ≤

which holds by the choice of these parameters. Finally
E[Regret|F̄ ]
"
"

1
T2

= E{Zt } EA

Taking a union bound, we have
P(∃t

kZt k2∞


h˜lt , x̃t i − h˜lt , x i
∗

t=1

1
≥ 10λ log N T ) ≤
T
2

T 
X

2

(5)

To bound the norm of the loss we define the event F ,
{∃t : kZt k2∞ ≥ 10λ2 log2 N T }. We have from (5) that
P(F ) ≤ T1 . We now have that
E[Regret] ≤ E[Regret|F̄ ] + P(F )E[Regret|F ]
Since the regret is always bounded by T we get that the
second term above is at most 1. Therefore we will concern
ourselves with bounding the first term above. Note that
Zt remains independent and symmetric even when conditioned on the event F̄ . Moreover the following statements
also hold.
∀t E[Zt |F̄ ] = 0
(6)
∀t E[kZt k2∞ |F̄ ] ≤ 10λ2 log2 N T

(7)

Equation (6) follows by noting that Zt remains symmetric around the origin even after conditioning. It can now
be seen that Lemma 4.3 still applies even when the noise
is sampled from LapN (λ) conditioned under the event F̄
(due to Equation 6). Therefore we have that
"
" T
# #
 
X
∗
E[Regret|F̄ ] = E{Zt } EA
h˜lt , x̃t i − h˜lt , x i F̄
t=1

(8)
To bound the above quantity we make use of the following
lemma which is a specialization of Theorem 1 in (Bubeck
et al., 2012a) to the case of multi-armed bandits.
Lemma 4.4 (Regret Guarantee for Algorithm 5). If η is
such that η|hei , ˜lt i| ≤ 1, we have that the regret of Algorithm 5 is bounded by
XX

log N
Regret ≤ 2γT +
+ ηE
pt (i)hei , ˜lt i2
η
t
i
Now note that due to the conditioning kZt k2∞
10λ2 log2 N T and therefore we have that

≤

maxt,x∈∆N |hZt , xi| ≤ 4λ log N T.
It can be seen that the condition η|hei , ˜lt i| ≤ 1 in Theorem
4.4 is satisfied for exploration µ(i) = N1 and under the
condition F̄ as long as
ηN (1 + 4λ log N T ) ≤ γ

"

T
X
log N
+η
N k˜lt k2∞ + 2T γ
η
t=1

"

T
X
log N
+ 2η
N (klt k2∞ + kZt k2∞ ) + 2T γ
η
t=1

≤ E{Zt }
≤ E{Zt }

# #

F̄

 #

F̄

 #

F̄


log N
+ 2ηT N (1 + λ2 log2 N T ) + 2T γ
η
q

≤O
T N log N (1 + λ2 log2 N T )
√

N T log T log N
≤O
ε

≤

4.2. Differentially Private Bandit Linear Optimization
In this section we prove a general result about bandit linear
optimization over general convex sets, the proof of which
is deferred to the appendix.
Theorem 4.5 (Bandit Linear Optimization). Let X ⊆ RN
be a convex set. Fix loss vectors (l1 , . . . lT ) such that
maxt,x∈X |hlt , xi| ≤ M . We have that Algorithm 4 when
1
run with parameters D = LapN (λ) (with λ = kYk
ε )
and algorithm A = SCRiBLe(Abernethy
et al., 2012)
q
ν log T
we have
with step parameter η =
2N 2 T (M 2 +λ2 N kX k22 )
the following guarantees that the regret of the algorithm is
bounded by
s
O

p

T log T

N kX k22 kYk21
N 2ν M 2 +
ε2


!

where ν is the self-concordance parameter of the convex
body X . Moreover the algorithm is ε-differentially private.

5. Conclusion
In this work, we demonstrate that ensuring differential privacy leads to only a constant additive increase in the incurred regret for online linear optimization in the full feedback setting. We also show nearly optimal bounds (in terms
of T) in the bandit feedback setting. Multiple avenues for
future research arise, including extending our bandit results to other challenging partial-information models such
as semi-bandit, combinatorial bandit and contextual bandits. Another important unresolved question is whether it
is possible to achieve an additive separation in ε, T in the
adversarial bandit setting.

The Price of Differential Privacy for Online Learning

References
Abernethy, Jacob, Hazan, Elad, and Rakhlin, Alexander.
Competing in the dark: An efficient algorithm for bandit
linear optimization. In COLT, pp. 263–274, 2008.
Abernethy, Jacob, Lee, Chansoo, Sinha, Abhinav, and
Tewari, Ambuj. Online linear optimization via smoothing. In COLT, pp. 807–823, 2014.
Abernethy, Jacob D, Hazan, Elad, and Rakhlin, Alexander. Interior-point methods for full-information and bandit online learning. IEEE Transactions on Information
Theory, 58(7):4164–4175, 2012.
Auer, Peter, Cesa-Bianchi, Nicolo, Freund, Yoav, and
Schapire, Robert E. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77,
2002.
Bubeck, Sébastien, Cesa-Bianchi, Nicolo, Kakade,
Sham M, Mannor, Shie, Srebro, Nathan, and
Williamson, Robert C. Towards minimax policies
for online linear optimization with bandit feedback. In
COLT, volume 23, 2012a.
Bubeck, Sébastien, Cesa-Bianchi, Nicolo, et al. Regret
analysis of stochastic and nonstochastic multi-armed
R in Machine
bandit problems. Foundations and Trends
Learning, 5(1):1–122, 2012b.
Dwork, Cynthia, McSherry, Frank, Nissim, Kobbi, and
Smith, Adam. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography Conference,
pp. 265–284. Springer, 2006.
Dwork, Cynthia, Naor, Moni, Pitassi, Toniann, and Rothblum, Guy N. Differential privacy under continual observation. In Proceedings of the forty-second ACM symposium on Theory of computing, pp. 715–724. ACM, 2010.
Dwork, Cynthia, Roth, Aaron, et al. The algorithmic
foundations of differential privacy. Foundations and
R in Theoretical Computer Science, 9(3–4):211–
Trends
407, 2014a.
Dwork, Cynthia, Talwar, Kunal, Thakurta, Abhradeep, and
Zhang, Li. Analyze gauss: optimal bounds for privacypreserving principal component analysis. In Proceedings
of the 46th Annual ACM Symposium on Theory of Computing, pp. 11–20. ACM, 2014b.
Hazan, Elad et al. Introduction to online convex optimizaR in Optimization, 2(3-4):
tion. Foundations and Trends
157–325, 2016.
Jain, Prateek and Thakurta, Abhradeep G. (near) dimension
independent risk bounds for differentially private learning. In Proceedings of the 31st International Conference
on Machine Learning (ICML-14), pp. 476–484, 2014.

Jain, Prateek, Kothari, Pravesh, and Thakurta, Abhradeep.
Differentially private online learning. In COLT, volume 23, pp. 24–1, 2012.
Kalai, Adam and Vempala, Santosh. Efficient algorithms
for online decision problems. Journal of Computer and
System Sciences, 71(3):291–307, 2005.
Shalev-Shwartz, Shai. Online learning and online convex optimization. Foundations and Trends in Machine
Learning, 4(2):107–194, 2011.
Smith, Adam and Thakurta, Abhradeep Guha. (nearly)
optimal algorithms for private online learning in fullinformation and bandit settings. In Advances in Neural
Information Processing Systems, pp. 2733–2741, 2013.
Tossou, Aristide and Dimitrakakis, Christos. Algorithms
for differentially private multi-armed bandits. In AAAI
2016, 2016.
Tossou, Aristide C. Y. and Dimitrakakis, Christos. Achieving privacy in the adversarial multi-armed bandit. In
14th International Conference on Artificial Intelligence
(AAAI 2017), 2017.

The Price of Differential Privacy for Online Learning

A. Proofs for FTPL (Theorem 3.5)

Now we have that

Theorem A.1 (Privacy Guarantees with Gaussian Noise).
kYk2
Choose any σ 2 ≥ ε2 2 log2 T log2 logδ T . When Algorithm
2 A(D, T ) is run with D = N (0, σ 2 IN ), the following
claims hold true:

"
E{Zt } EA

" T
X

##
˜
hlt − lt , x̃t i

t=1

"

"

− E{Zt } EA

T
X
hlt − ˜lt , x∗ i

##

t=1

• Privacy: The sequence (L̃t : t ∈ [T ]) is (ε, δ)differentially private.

=EA

" T
X

h
E{Zt }

i
hlT − ˜lt , x̃t − x∗ i

#

t=1

• Distribution:
It holds that ∀t
Pt
N ( s=1 ls , dlog T eσ 2 IN ).

∈

[T ], L̃t

∼
=EA

" T
X

#
∗

hE [Zt ] , E[x̃t − x ]i

t=1

=0
Proof. By Theorem 9 ((Jain et al., 2012)), we have that
the sequence (L̃0 t : t ∈ [T ]) is (ε, δ)-differentially private.
Now the sequence (L̃t : t ∈ [T ]) is (ε, δ)-differentially
private because differential privacy is immune to postprocessing (Dwork et al., 2014a).
Note that the PrivateSum algorithm addsPexactly |S| int
dependent draws from the distribution D to s=1 ls , where
S is the minimum set of already populated nodes in the tree
that can compute the required prefix sum. Due to Line 6, it
is made certain that every prefix sum released is a sum of
the prefix sum and dlog T e independent draws from D.

Proof of Theorem 3.5. As a consequence of Corollary 4
((Abernethy et al., 2014)) and Lemma 13 ((Abernethy
et al., 2014)), it is true that

The first inequality follows because the randomness of the
algorithm does not depend on Zt . The second equality follows because xt , being a function of the loss vectors of the
previous round, is independent of the noise Zt and the third
equality follows from the fact that E[Zt ] = 0.

C. Facts about Norms of Laplace Vectors
The following simple fact follows for Laplacian distributions. The general versions follow immediately.
Fact C.1. Let Z ∼ LapN (λ), then we have that
P(kZk2∞ ≥ 10λ2 log2 T N ) ≤

1
T2

Proof. We will show that for a fixed i ∈ [N ]
RegretA(D,T ) (T ) ≤ σ

p

T kX k2 kYk22
√
N log T kX k2 +
2σ log T

Substituting the proposed value of σ, we obtain the stated
claim.

P(Z(i)2 ≥ 10λ2 log2 T N ) ≤

1
NT2

We now give the proof of Lemma 4.3.

The proof then follows by a simple union bound. Note
that
√ it is enough to bound the probability that P (Y ≥
10λ log T N ) where Y is distributed as an exponential
random variable with parameter λ. This is bounded by
1
T 2 N . A simple union bound finishes the proof now.

Proof. The proof of the lemma is a straightforward calculation. First note that

D. Proof for Bandit Linear Optimization

B. Noisy OCO Theorem Proof

"
E{Zt } EA

"

T
X

Proof of Theorem 4.5. We prove the privacy guarantees
first, followed by the regret bound.

##
(hlt , x̃t i − hlt , x∗ i)

t=1

"

"

T 
X

##

hl,x̃t i
Privacy: Note that maxt,l∈Y | kx̃
| ≤ kYk1 by Holder’s
t k∞

1
N
inequality. Therefore, with λ = kYk
ε , D = Lap (λ) ensures ε-differential privacy due to Lemma 4.2.
t=1
"
" T
##
"
" T
##
Regret: For the purpose of analysis we define the followX
X
∗
˜
˜
+E{Zt } EA
hlt − lt , x̃t i −E{Zt } EA
hlt − lt , x iing pseudo loss vectors ˜lt = lt + Zt , where by definition
t=1
t=1
Z ∼ LapN (λ). Further note that the following (which is

= E{Zt } EA


h˜lt , x̃t i − h˜lt , x∗ i

The Price of Differential Privacy for Online Learning

a direct consequence of Fact C.1 proved in the appendix)
holds for Zt ∼ LapN (λ)
P(kZt k22 ≥ 10λ2 N log2 N T ) ≤

1
T2

E[Regret|F̄ ]
"
"
= E{Zt } EA

Therefore taking a union bound we get that
(9)

We condition on the following event F = {∃t : kZt k22 ≥
10λ2 N log2 N T }. We have from (9) that P(F ) ≤ T1 . We
now have that
E[Regret] ≤ E[Regret|F̄ ] + P(F )E[Regret|F ]
Since the regret is always bounded by T we get that the
second term above is at most 1. Therefore we will concern
ourselves with bounding the first term above. For the rest of
the section we will assume the conditioning on the event F̄ .
First note that since the noise vectors Zt were independent
to begin with they still remain conditionally independent
even when conditioned on the event F̄ . The following two
statements can be seen to follow easily.
∀t E[Zt |F ] = 0

(10)

∀t E[kZt k22 |F̄ ] ≤ 10λ2 N log2 N T

(11)

It follows from Equation 10 that Lemma 4.3 applies even
when the noise is sampled from LapN (λ) conditioned on
the event F̄ . Therefore we have that
"
" T
# #
 
X
∗
E[Regret|F̄ ] = E{Zt } EA
h˜lt , x̃t i − h˜lt , x i F̄
t=1

(12)
Now note that since due to the conditioning kZt k2 ≤
10λ2 N log2 N T and therefore we have that
q
N log2 N T .

We now use the following theorem which is a simple restatement of Theorem 5.1 in (Abernethy et al., 2012).
Theorem D.1 (Theorem 5.1 in (Abernethy et al., 2012)).
Fix a loss sequence l1 , . . . lT . Let X be a compact convex
set and R be a ν-self concordant barrier on X . Assume
maxt,x∈X |hlt , xi| ≤ M . Setting η ≤ 4N1M then the regret
of the SCRiBLe algorithm is bounded by
RegretSCRiBLe ≤ 2η

X

T 
X


h˜lt , x̃t i − h˜lt , x∗ i

t=1

h

1
P(∃t : kZt k22 ≥ 10λ2 N log2 N T ) ≤
T

L , maxt,x∈X |hZt , xi| ≤ 4λkX k2

1
It can now be verified that η ≤ 4N (M
+L) . Therefore we
can apply Theorem D.1 obtain that

N hlt , xt i2 + νη −1 log T + 2M

where (xt ∈ X : t ∈ [T ]) is the sequence of points played
by the algorithm.

≤ E{Zt } 2η

X

N 2 |h˜lt , x̃t i|2 |F̄

# #

F̄


i

 


+ E{Zt } νη −1 log T + 2(M + L) F̄
h X
i
≤ E{Zt } 2η
N 2 (|hlt , x̃t i|2 + kZt k22 kx̃t k22 )|F̄
 


−1
+ E{Zt } νη log T + 2(M + L) F̄
≤ 2ηT N 2 (M 2 + λ2 N kX k22 ) + νη −1 log T
q
+ 8λkX k2 N log2 N T + 2M


q
p
T log T N 2 ν(M 2 + λ2 N kX k22 )
≤O

