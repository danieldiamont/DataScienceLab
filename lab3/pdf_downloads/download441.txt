Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Mahesh Chandra Mukkamala 1 2 Matthias Hein 1

Abstract
Adaptive gradient methods have become recently
very popular, in particular as they have been
shown to be useful in the training of deep neural networks. In this paper we have analyzed
RMSProp, originally proposed for the training
of deep neural networks, in the context
of onâˆš
line convex optimization and show T -type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which
we show logarithmic regret bounds for strongly
convex functions. Finally, we demonstrate in the
experiments that these new variants outperform
other adaptive gradient techniques or stochastic
gradient descent in the optimization of strongly
convex functions as well as in training of deep
neural networks.

1. Introduction
There has recently been a lot of work on adaptive gradient
algorithms such as Adagrad (Duchi et al., 2011), RMSProp
(Hinton et al., 2012), ADADELTA (Zeiler, 2012), and
Adam (Kingma & Bai, 2015). The original idea of Adagrad to have a parameter specific learning rate by analyzing the gradients observed during the optimization turned
out to be useful not only in online convex optimization
but also for training deep neural networks. The original
analysis of Adagrad (Duchi et al., 2011) was limited to the
case of all convex functions for which
âˆš it obtained a datadependent regret bound of order O( T ) which is known
to be optimal (Hazan, 2016) for this class. However, a lot
of learning problems have more structure in the sense that
one optimizes over the restricted class of strongly convex
functions. It has been shown in (Hazan et al., 2007) that
one can achieve much better logarithmic regret bounds for
the class of strongly convex functions.

The goal of this paper is twofold. First, we propose SCAdagrad which is a variant of Adagrad adapted to the
strongly convex case. We show that SC-Adagrad achieves
a logarithmic regret bound for the case of strongly convex
functions, which is data-dependent. It is known that such
bounds can be much better in practice than data independent bounds (Hazan et al., 2007),(McMahan, 2014). Second, we analyze RMSProp which has become one of the
standard methods to train neural networks beyond stochastic gradient descent. We show that under some conditions on the weighting schemeâˆšof RMSProp, this algorithm
achieves a data-dependent O( T ) regret bound. In fact, it
turns out that RMSProp contains Adagrad as a special case
for a particular choice of the weighting scheme. Up to our
knowledge this is the first theoretical result justifying the
usage of RMSProp in online convex optimization and thus
can at least be seen as theoretical support for its usage in
deep learning. Similarly, we then propose the variant SCRMSProp for which we also show a data-dependent logarithmic regret bound similar to SC-Adagrad for the class of
strongly convex functions. Interestingly, SC-Adagrad has
been discussed in (Ruder, 2016), where it is said that â€œit
does not to workâ€. The reason for this is that SC-Adagrad
comes along with a damping factor which prevents potentially large steps in the beginning of the iterations. However, as our analysis shows this damping factor has to be
rather large initially to prevent large steps and should be
then monotonically decreasing as a function of the iterations in order to stay adaptive. Finally, we show in experiments on three datasets that the new methods are competitive or outperform other adaptive gradient techniques as
well as stochastic gradient descent for strongly convex optimization problem in terms of regret and training objective
but also perform very well in the training of deep neural
networks, where we show results for different networks and
datasets.

2. Problem Statement

1

Department of Mathematics and Computer Science, Saarland
University, Germany 2 IMPRS-CS, Max Planck Institute for Informatics, SaarbruÌˆcken, Germany . Correspondence to: Mahesh
Chandra Mukkamala <mmahesh.chandra873@gmail.com>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

We first need some technical statements and notation and
then introduce the online convex optimization problem.

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

2.1. Notation and Technical Statements
We denote by [T ] the set {1, . . . , T }. Let A âˆˆ RdÃ—d be a
symmetric, positive definite matrix. We denote as
hx, yiA = hx, Ayi =

d
X

Aij xi yj ,

kxkA

=

q
hx, xiA

i,j=1

Note that P
the standard Euclidean inner product becomes
hx, yi =
i xi yi = hx, yiI While we use here the general notation for matrices for comparison to the literature.
All positive definite matrices A in this paper will be diagonal matrices, so that the computational effort for computing
inner products and norms is still linear in d. The CauchySchwarz inequality becomes, hx, yiA â‰¤ kxkA kykA . We
further introduce the element-wise product ab of two vectors. Let a, b âˆˆ Rd , then (a  b)i = ai bi for i = 1, . . . , d.
Let A âˆˆ RdÃ—d be a symmetric, positive definite matrix,
z âˆˆ Rd and C âŠ‚ Rd a convex set. Then we define the
weighted projection PCA (z) of z onto the set C as
2

PCA (z) = arg min kx âˆ’ zkA .

hx, Axi â‰¤ Î»max (A) hx, xi â‰¤ tr(A) hx, xi

2.2. Problem Statement
In this paper we analyze the online convex optimization
setting, that is we have a convex set C and at each round we
get access to a (sub)-gradient of some continuous convex
function ft : C â†’ R. At the t-th iterate we predict Î¸t âˆˆ C
and suffer a loss ft (Î¸t ). The goal is to perform well with
respect to the optimal decision in hindsight defined as
âˆ—

Î¸ = arg min
Î¸âˆˆC

Lemma 2.1 Let A âˆˆ RdÃ—d be a symmetric, positive definite matrix and C âŠ‚ Rd be a convex set. Then
 A

PC (z) âˆ’ PCA (y) â‰¤ kz âˆ’ yk .
A
A
Proof:
The first order optimality condition for the
weighted projection in (1) is given as
A(x âˆ’ z) âˆˆ NC (x),
where NC (x) denotes the normal cone of C at x. This can
be rewritten as
âˆ€y âˆˆ C.

T
X

ft (Î¸).

t=1

The adversarial regret at time T âˆˆ N is then given as
R(T ) =

It is well-known that the weighted projection is unique and
non-expansive.

(2)

where Î»max (A) is the maximum eigenvalue of matrix A
and tr(A) denotes the trace of matrix A .

(1)

xâˆˆC

hz âˆ’ x, y âˆ’ xiA â‰¤ 0

Lemma 2.2 For any symmetric, positive semi-definite matrix A âˆˆ RdÃ—d we have

T
X

(ft (Î¸t ) âˆ’ ft (Î¸âˆ— )).

t=1

We assume that the adversarial can choose from the class
of convex functions on C, for some parts we will specialize
this to the set of strongly convex functions.
Definition 2.1 Let C be a convex set. We say that a function f : C â†’ R is Âµ-strongly convex, if there exists Âµ âˆˆ Rd
with Âµi > 0 for i = 1, . . . , d such that for all x, y âˆˆ C,
2

f (y) â‰¥ f (x) + hâˆ‡f (x), y âˆ’ xi + ky âˆ’ xkdiag(Âµ)
= f (x) + hâˆ‡f (x), y âˆ’ xi +

d
X

Âµi (yi âˆ’ xi )2 .

i=1

Let Î¶ = mini=1,...,d Âµi , then this function is Î¶-strongly convex (in the usual sense), that is
2

This yields



z âˆ’ PCA (z), PCA (y) âˆ’ PCA (z) A â‰¤ 0,



y âˆ’ PCA (y), PCA (z) âˆ’ PCA (y) A â‰¤ 0.
Adding these two inequalities yields



z âˆ’ PCA (z) âˆ’ y + PCA (y), PCA (y) âˆ’ PCA (z) A â‰¤ 0

2



=â‡’ PCA (y) âˆ’ PCA (z)A â‰¤ z âˆ’ y, PCA (y) âˆ’ PCA (z) A .
The result follows from the application of the weighted
Cauchy-Schwarz inequality.


f (y) â‰¥ f (x) + hâˆ‡f (x), y âˆ’ xi + Î¶ kx âˆ’ yk .
Note that the difference between our notion of componentwise strong convexity and the usual definition of strong
convexity is indicated by the bold font versus normal font.
We have two assumptions:
â€¢ A1: It holds suptâ‰¥1 kgt k2 â‰¤ G which implies the
existence of a constant Gâˆ such that suptâ‰¥1 kgt kâˆ â‰¤
Gâˆ .
â€¢ A2: It holds suptâ‰¥1 kÎ¸t âˆ’ Î¸âˆ— k2 â‰¤ D which implies the existence of a constant Dâˆ such that
suptâ‰¥1 kÎ¸t âˆ’ Î¸âˆ— kâˆ â‰¤ Dâˆ .

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Î±
1
=âˆš q P
T
1
2
T
t=1 gt,i +
T

Algorithm 1 Adagrad
Input: Î¸1 âˆˆ C , Î´ > 0, v0 = 0 âˆˆ Rd
for t = 1 to T do
gt âˆˆ âˆ‚ft (Î¸t )
vt = vtâˆ’1 + (gt  gt )
âˆš
At = diag( vt ) + Î´I

Î¸t+1 = PCAt Î¸t âˆ’ Î±Aâˆ’1
t gt
end for
One of the first
âˆš methods which achieves the optimal regret
bound of O( T ) for convex problems is online projected
gradient descent (Zinkevich, 2003), defined as
Î¸t+1 = PC (Î¸t âˆ’ Î±t gt )

(3)

where Î±t = âˆšÎ±t is the step-size scheme and gt is a (sub)gradient of ft at Î¸t . With Î±t = Î±t , online projected gradient descent method achieves the optimal O(log(T )) regret
bound for strongly-convex problems (Hazan et al., 2007).
We consider Adagrad in the next subsection which is one of
the popular adaptive alternative to online projected gradient
descent.
2.3. Adagrad for convex problems
In this section we briefly recall the main result for the Adagrad. The algorithm for Adagrad is given in Algorithm 1. If
the adversarial is allowed to choose from the set of all possible convex functions on C âˆš
âŠ‚ Rd , then Adagrad achieves
the regret bound of order O( T ) as shown in (Duchi et al.,
2011). This regret bound is known to be optimal for this
class, see e.g. (Hazan, 2016). For better comparison to
our results for RMSProp, we recall the result from (Duchi
et al., 2011) in our notation. For this purpose, we introduce
the notation, g1:T,i = (g1,i , g2,i , .., gT,i )T , where gt,i is the
i-th component of the gradient gt âˆˆ Rd of the function ft
evaluated at Î¸t .
Theorem 2.1 (Duchi et al., 2011) Let Assumptions A1, A2
hold and let Î¸t be the sequence generated by Adagrad in
Algorithm 1, where gt âˆˆ âˆ‚ft (Î¸t ) and ft : C â†’ R is an
arbitrary convex function, then for stepsize Î± > 0 the regret
is upper bounded as
R(T ) â‰¤

d
d
2 X
X
Dâˆ
kg1:T,i k2 + Î±
kg1:T,i k2 .
2Î± i=1
i=1

The effective step-length of Adagrad is on the order of âˆšÎ±t .
PT
2
This can be seen as follows; first note that vT,i = t=1 gt,i
1
âˆ’1
and thus (At ) is a diagonal matrix with entries âˆšvt,i +Î´ .
Then one has
Î±
Î±(Aâˆ’1
T )ii = qP
T
2
t=1 gt,i + Î´

(4)
âˆšÎ´
T

Thus an alternative point of view of Adagrad, is that it has a
decaying stepsize âˆšÎ±t but now the correction term becomes
the running average of the squared derivatives plus a vanishing damping term. However, the effective stepsize has
to decay faster to get a logarithmic regret bound for the
strongly convex case. This is what we analyze in the next
section, where we propose SC-Adagrad for strongly convex
functions.

3. Strongly convex Adagrad (SC-Adagrad)
The modification SC-Adagrad of Adagrad which we propose in the following can be motivated by the observation
that the online projected gradient descent (Hazan et al.,
2007) uses stepsizes of order Î± = O( T1 ) in order to achieve
the logarithmic regret bound for strongly convex functions.
In analogy with the
in the previous section, we
Pderivation
T
2
still have vT,i = t=1 gt,i
. But now we modify (At )âˆ’1
and set it as a diagonal matrix with entries vt,i1+Î´t . Then
one has
Î±(Aâˆ’1
T )ii = PT

Î±

2
t=1 gt,i

+ Î´t

=

Î±
T

1
1
T

PT

2
t=1 gt,i

+

Î´T
T

.
(5)

Again, we have in the denominator a running average of
the observed gradients and a decaying damping factor. In
this way, we get an effective stepsize of order O( T1 ) in SCAdagrad. The formal method is presented in Algorithm
2. As just derived the only difference of Adagrad and SCAdagrad is the definition of the diagonal matrix At . Note
Algorithm 2 SC-Adagrad
Input: Î¸1 âˆˆ C , Î´0 > 0, v0 = 0 âˆˆ Rd
for t = 1 to T do
gt âˆˆ âˆ‚ft (Î¸t )
vt = vtâˆ’1 + (gt  gt )
Choose 0 < Î´t â‰¤ Î´tâˆ’1 element wise
At = diag(vt ) + diag(Î´t ) 
Î¸t+1 = PCAt Î¸t âˆ’ Î±Aâˆ’1
t gt
end for
also that we have defined the damping factor Î´t as a function of t which is also different from standard Adagrad.
The constant Î´ in Adagrad is mainly introduced due to numerical reasons in order to avoid problems when gt,i is
very small for some components in the first iterations and
is typically chosen quite small e.g. Î´ = 10âˆ’8 . For SCAdagrad the situation is different. If the first components
g1,i , g2,i , . . . are very small, say of order , then the update

is 2 +Î´
which can become extremely large if Î´t is chosen
t

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

to be small. This would make the method very unstable
and would lead to huge constants in the bounds. This is
probably why in (Ruder, 2016), the modification of Adagrad where one â€œdrops the square-rootâ€ did not work. A
good choice of Î´t should be initially
on the order
PTroughly
2
of 1 and it should decay as vt,i = t=1 gt,i
starts to grow.
This is why we propose to use
Î´t,i = Î¾2 eâˆ’Î¾1 vt,i ,

i = 1, . . . , d,

for Î¾1 > 0, Î¾2 > 0 as a potential decay scheme as it satisfies both properties for sufficiently large Î¾1 and Î¾2 chosen
on the order of 1. Also, one can achieve a constant decay scheme for Î¾1 = 0 , Î¾2 > 0. We will come back to
this choice after the proof. In the following we provide the
regret analysis of SC-Adagrad and show that the optimal
logarithmic regret bound can be achieved. However, as it is
data-dependent it is typically significantly better in practice
than data-independent bounds.
3.1. Analysis
For any two matrices A, B âˆˆ RdÃ—d , we use
P the
P notation
â€¢ to denote the inner product i.e A â€¢ B = i j Aij Bij .
Note that A â€¢ B = tr(AT B).
Lemma 3.1 [Lemma 12 (Hazan et al., 2007)] Let A, B be
positive definite matrices, let A  B  0 then
Aâˆ’1 â€¢ (A âˆ’ B) â‰¤ log

 |A| 
|B|

(6)

where |A| denotes the determinant of the matrix A
Lemma 3.2 Let Assumptions A1, A2 hold, then for T â‰¥
1 and At , Î´t as defined in the SC-Adagrad algorithm we
have,
!
T
d
X


 X
kg1:T,i k2 + Î´T,i
âˆ’1
gt , At gt â‰¤
log
Î´1,i
t=1
i=1
âˆ’

d X
T
X
Î´t,i âˆ’ Î´tâˆ’1,i
2
kg
1:t,i k + Î´t,i
i=1 t=2

âˆ’

T
X

Aâˆ’1
t â€¢ (diag(Î´t ) âˆ’ diag(Î´tâˆ’1 ))

t=2
T
|AT |  X âˆ’1
âˆ’
At â€¢ (diag(Î´t ) âˆ’ diag(Î´tâˆ’1 ))
= log
|diag(Î´1 )|
t=2
!
d
X
kg1:T,i k2 + Î´T,i
â‰¤
log
Î´1,i
i=1



âˆ’

T
X

Aâˆ’1
t â€¢ (diag(Î´t ) âˆ’ diag(Î´tâˆ’1 ))

t=2

â‰¤

d
X

log

i=1

kg1:T,i k2 + Î´T,i
Î´1,i

!
âˆ’

d X
T
X
Î´t,i âˆ’ Î´tâˆ’1,i
2
kg
1:t,i k + Î´t,i
i=1 t=2

In the first step we use hx, Axi = A â€¢ diag(xxT ) where
A is a diagonal matrix and subsequently we use âˆ€t > 1 ,
diag(gt gtT ) = At âˆ’ Atâˆ’1 âˆ’ diag(Î´t ) + diag(Î´tâˆ’1 ), and
for t = 1 we have diag(g1 g1T ) = A1 âˆ’ diag(Î´1 ). In the
first inequality we use Lemma 3.1 also see for Lemma 12
of (Hazan et al., 2007). Note that for T = 1, the upper
bound results in 0.


Theorem 3.1 Let Assumptions A1, A2 hold and let Î¸t be
the sequence generated by the SC-Adagrad in Algorithm
2, where gt âˆˆ âˆ‚ft (Î¸t ) and ft : C â†’ R is an arbitrary
Âµ-strongly convex function (Âµ âˆˆ Rd+ ) where the stepsize
G2

fulfills Î± â‰¥ maxi=1,...,d 2Âµâˆi . Furthermore, let Î´t > 0 and
Î´t,i â‰¤ Î´tâˆ’1,i âˆ€t âˆˆ [T ], âˆ€i âˆˆ [d], then the regret of SCAdagrad can be upper bounded for T â‰¥ 1 as
d

R(T ) â‰¤

 kg

2
2
tr(diag(Î´1 )) Î± X
Dâˆ
1:T,i k + Î´T,i
+
log
2Î±
2 i=1
Î´1,i

d

+

 (Î¸ âˆ’ Î¸âˆ— )2

1X
Î±
t,i
i
inf
âˆ’
(Î´T,i âˆ’ Î´1,i )
2 i=1 tâˆˆ[T ]
Î±
kg1:t,i k2 + Î´t,i

For constant Î´t i.e Î´t,i = Î´ > 0 âˆ€t âˆˆ [T ] and âˆ€i âˆˆ [d] then
the regret of SC-Adagrad is upper bounded as
d

Proof: Consider the following summation,
T
T
X


 X
T
gt , Aâˆ’1
Aâˆ’1
t gt â‰¤
t â€¢ diag(gt gt )
t=1

=
+

t=1

Aâˆ’1
1
T
X

â€¢ (A1 âˆ’ diag(Î´1 ))

Aâˆ’1
t â€¢ (At âˆ’ Atâˆ’1 âˆ’ diag(Î´t ) + diag(Î´tâˆ’1 ))

R(T ) â‰¤

 kg

2
2
Î±X
Dâˆ
dÎ´
1:T,i k + Î´
+
log
2Î±
2 i=1
Î´

For Î¶-strongly convex function choosing Î± â‰¥
tain the above mentioned regret bounds.



T
 |A | 
|A1 |  X
t
+
log
|diag(Î´1 )|
|A
|
tâˆ’1
t=2

we ob-

Proof: We rewrite the regret bound with the definition of
Âµ-strongly convex functions as

t=2

â‰¤ log

G2âˆ
2Î¶

(7)

R(T ) =

T
X
t=1

(ft (Î¸t ) âˆ’ ft (Î¸âˆ— ))

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

â‰¤

T
X

hgt , Î¸t âˆ’ Î¸âˆ— i âˆ’

t=1

T
X

2

kÎ¸t âˆ’ Î¸âˆ— kdiag(Âµ)

â‰¤

+

2
Î¸âˆ— kAt

kÎ¸t+1 âˆ’

2


âˆ—
= PCAt Î¸t âˆ’ Î±Aâˆ’1
t gt âˆ’ Î¸ 
At

2
âˆ’1
âˆ—
â‰¤ Î¸t âˆ’ Î±At gt âˆ’ Î¸ At
Î¸âˆ— k2At

=

âˆ’ 2Î± hgt , Î¸t âˆ’ Î¸ i + Î±

2




gt , Atâˆ’1 gt



hgt , Î¸t âˆ’ Î¸ i

â‰¤

T
X
kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ kÎ¸t+1 âˆ’ Î¸âˆ— k2At
2Î±
t=1

+

T
T
 X
Î± X

2
gt , Aâˆ’1
kÎ¸t âˆ’ Î¸âˆ— kdiag(Âµ)
t gt âˆ’
2 t=1
t=1

kÎ¸1 âˆ’ Î¸âˆ— k2diag(Î´1 )
2Î±

+

T X
d
X
(Î¸t,i âˆ’ Î¸âˆ— )2 (Î´t,i âˆ’ Î´tâˆ’1,i )
i

2Î±

t=2 i=1

T

kÎ¸T +1 âˆ’ Î¸âˆ— k2AT
Î± X

+
gt , Aâˆ’1
t gt
2Î±
2 t=1
2

t=1

kÎ¸1 âˆ’ Î¸âˆ— k2A1 âˆ’2Î±diag(Âµ)

2Î±

i

2Î±

t=2 i=1

d
2
Dâˆ
tr(diag(Î´1 )) Î± X  kg1:T,i k2 + Î´T,i 
log
+
2Î±
2 i=1
Î´1,i
d

1 XX
+
2 t=2 i=1

(Î¸t,i âˆ’ Î¸iâˆ— )2 (Î´t,i âˆ’ Î´tâˆ’1,i ) Î±(Î´t,i âˆ’ Î´tâˆ’1,i )
âˆ’
Î±
kg1:t,i k2 + Î´t,i

â‰¤

d
2
Dâˆ
tr(diag(Î´1 )) Î± X  kg1:T,i k2 + Î´T,i 
+
log
2Î±
2 i=1
Î´1,i

+

 (Î¸ âˆ’ Î¸âˆ— )2

1X
Î±
t,i
i
inf
âˆ’
(Î´T,i âˆ’ Î´1,i )
2
2 i=1 tâˆˆ[T ]
Î±
kg1:t,i k + Î´t,i

d

In the second inequality we bounded kÎ¸1 âˆ’ Î¸âˆ— k2diag(Î´1 ) â‰¤
2
tr(diag(Î´1 )). In the second last step we use the
Dâˆ
Lemma 3.2. So under a constant Î´t i.e Î´t,i = Î´ > 0, âˆ€t âˆˆ
[T ], âˆ€i âˆˆ [d] we have tr(diag(Î´1 )) = dÎ´ hence proving the
result (7). For Î¶-strongly convex functions choosing Î± â‰¥

kÎ¸t âˆ’ Î¸âˆ— kdiag(Âµ)

2Î±
T
X
kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’Atâˆ’1 âˆ’2Î± diag(Âµ)

T X
d
X
(Î¸t,i âˆ’ Î¸âˆ— )2 (Î´t,i âˆ’ Î´tâˆ’1,i )

T

T 
kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ kÎ¸t âˆ’ Î¸âˆ— k2Atâˆ’1 
kÎ¸1 âˆ’ Î¸âˆ— k2A1 X
â‰¤
+
2Î±
2Î±
t=2

G2âˆ,i
2Î¶

T

+


Î± X

gt , Aâˆ’1
t gt
2 t=1

In the last step we use the equality âˆ€x âˆˆ Rn kxk2A âˆ’
kxk2B = kxk2Aâˆ’B where A, B âˆˆ Rn x n and both are
diagonal matrices. Now, we choose Î± such that At âˆ’
Atâˆ’1 âˆ’ 2Î± diag(Âµ) 4 diag(Î´t ) âˆ’ diag(Î´tâˆ’1 ) âˆ€t â‰¥ 2
and A1 âˆ’ 2Î± diag(Âµ) 4 diag(Î´1 ) Since At âˆ’ Atâˆ’1 4
G2âˆ I + diag(Î´t ) âˆ’ diag(Î´tâˆ’1 ) and A1 4 G2âˆ I + diag(Î´1 )
because at any round the difference between subsequent
squares of sub-gradients is bounded by G2âˆ . Also by
Algorithm 2, Î´t,i â‰¤ Î´tâˆ’1,i âˆ€t > 1, âˆ€i âˆˆ [d] hence
diag(Î´t ) âˆ’ diag(Î´tâˆ’1 )  0. Hence by choosing Î± â‰¥
G2
maxi=1,...,d 2Âµâˆi we have At âˆ’ Atâˆ’1 âˆ’ 2Î± diag(Âµ) 4
diag(Î´t ) âˆ’ diag(Î´tâˆ’1 ) âˆ€t â‰¥ 2 and A1 âˆ’ 2Î±diag(Âµ) 4
diag(Î´1 ) which yields
R(T )

t=1

2

Dâˆ
tr(diag(Î´1 )) Î± X 

+
gt , Aâˆ’1
t gt
2Î±
2 t=1

â‰¤

t=2



â‰¤

R(T )

+

gt , Aâˆ’1
t gt


Î± X

gt , Aâˆ’1
t gt
2 t=1

+

â‰¤

2Î±

t=2

+


kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ kÎ¸t+1 âˆ’ Î¸âˆ— k2At
Î±

+
gt , Aâˆ’1
t gt
2Î±
2
Hence we can upper bound the regret as follows
â‰¤

T
X




T
X
kÎ¸t âˆ’ Î¸âˆ— kdiag(Î´t )âˆ’diag(Î´tâˆ’1 )

T

âˆ—

âˆ’

Î±
2

T
X

+

T

âˆ—

This yields

âˆ’

2Î±

t=1

Using the non-expansiveness we have

â‰¤ kÎ¸t âˆ’

kÎ¸1 âˆ’ Î¸âˆ— k2diag(Î´1 )

we obtain the the same results as Âµ-strongly convex
functions. This can be seen by setting Âµi = Î¶, âˆ€i âˆˆ [d]. 
Note that the first and the last term in the regret bound
can be upper bounded by constants. Only the second term
2
depends on T . Note that kg1:T,i k â‰¤ T G2 and as Î´t is
monotonically decreasing, the second term is on the order
of O(log(T )) and thus we have a logarithmic regret bound.
As the bound is data-dependent, in the sense that it depends
on the observed sequence of gradients, it is much tighter
than a data-independent bound.
The bound includes also the case of a non-decaying damping factor Î´t = Î´ = Î¾2 (Î¾1 = 0). While a rather large
constant damping factor can work well, we have noticed
that the best results are obtained with the decay scheme
Î´t,i = Î¾2 eâˆ’Î¾1 vt,i ,

i = 1, . . . , d.

where Î¾1 > 0 , Î¾2 > 0 , which is what we use in the experiments. Note that this decay scheme for Î¾1 , Î¾2 > 0 is

!

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

adaptive to the specific dimension and thus increases the
adaptivity of the overall algorithm. For completeness we
also give the bound specialized for this decay scheme.
Corollary 3.1 In the setting of Theorem 3.1 choose Î´t,i =
Î¾2 eâˆ’Î¾1 vt,i for i = 1, . . . , d for some Î¾1 > 0, Î¾2 > 0 . Then
the regret of SC-Adagrad can be upper bounded for T â‰¥ 1
as
2
2
Î±
dDâˆ
Î¾2
âˆ’ log(Î¾2 eâˆ’Î¾1 Gâˆ )
2Î±
2
d


X
2
Î±
+
log kg1:T,i k2 + Î¾2 eâˆ’Î¾1 kg1:T ,i k
2 i=1

Algorithm 3 RMSProp

R(T ) â‰¤

d

+


X
2
Î±Î¾1 Î¾2

1 âˆ’ eâˆ’Î¾1 kg1:T ,i k
2 log(Î¾2 Î¾1 ) + 1 i=1
2

Proof: Note that Î´T,i = Î¾2 eâˆ’Î¾1 vT ,i = Î¾2 eâˆ’Î¾1 kg1:T ,i k .
Plugging this into Theorem 3.1 for Î¾1 , Î¾2 > 0 yields the
results for the first three terms. Using (Î¸t,i âˆ’ Î¸iâˆ— )2 â‰¥ 0 we
have

Î±
âˆ’ Î¸iâˆ— )2
âˆ’
Î±
kg1:t,i k2 + Î´t,i
tâˆˆ[T ]
âˆ’Î±
â‰¥
inf jâˆˆ[1:T ] kg1:j,i k2 + Î´j,i
inf

 (Î¸

are approximately Gaussian distributed, then the matrix At
can be seen as a preconditioner which approximates the diagonal of the Hessian (Daniel et al., 2016). However, it is
fair to say that despite its huge empirical success in practice and some first analysis in the literature, there is so far
no rigorous theoretical analysis of RMSProp. We will analyze RMSProp given in Algorithm 3 in the framework of
of online convex optimization.

t,i

Input: Î¸1 âˆˆ C , Î´ > 0, Î± > 0, v0 = 0 âˆˆ Rd
for t = 1 to T do
gt âˆˆ âˆ‚ft (Î¸t )
vt = Î²t vtâˆ’1 + (1 âˆ’ Î²t )(gt  gt )
Set t = âˆšÎ´t and Î±t = âˆšÎ±t
âˆš
At = diag( vt ) + t I

Î¸t+1 = PCAt Î¸t âˆ’ Î±t Aâˆ’1
t gt
end for
First, we will show that RMSProp reduces to Adagrad for
a certain choice of its parameters. Second, we will
âˆš prove
for the general convex case a regret bound of O( T ) similar to the bound given in Theorem 2.1. It turns out that
the convergence analysis requires that in the update of the
weighted cumulative squared gradients (vt ) , it has to hold
1âˆ’

2

Note that kg1:j,i k + Î´j,i = vj,i + Î¾2 eâˆ’Î¾1 vj,i , in order to
find the minimum of this term we thus analyze the function f : R+ â†’ R, f (x) = x + Î¾2 eâˆ’Î¾1 x . and a straightforward calculation shows that the minimum is attained at
xâˆ— = Î¾11 log(Î¾1 Î¾2 ) and f (xâˆ— ) = Î¾11 (log(Î¾1 Î¾2 ) + 1). This
yields the fourth term.

Unfortunately, it is not obvious that the regret bound for our
decaying damping factor is better than the one of a constant
damping factor. Note, however that the third term in the regret bound of Theorem 3.1 can be negative. It thus remains
an interesting question for future work, if there exists an
optimal decay scheme which provably works better than
any constant one.

for some 0 < Î³ â‰¤ 1. This is in contrast to the original
suggestion of (Hinton et al., 2012) to choose Î²t = 0.9.
It will turn out later in the experiments that the constant
choice of Î²t leads sometimes to divergence of the sequence,
whereas the choice derived from our theoretical analysis
always leads to a convergent scheme even when applied to
deep neural networks. Thus we think that the analysis in
the following is not only interesting for the convex case but
can give valuable hints how the parameters of RMSProp
should be chosen in deep learning.
Before we start the regret analysis we want to discuss the
sequence vt in more detail. Using the recursive definition
of vt , we get the closed form expression

4. RMSProp and SC-RMSProp
RMSProp is one of the most popular adaptive gradient
algorithms used for the training of deep neural networks
(Schaul et al., 2014; Dauphin et al., 2015; Daniel et al.,
2016; Schmidhuber, 2015). It has been used frequently in
computer vision (Karpathy & Fei-Fei, 2016) e.g. to train
the latest InceptionV4 network (Szegedy et al., 2016a;b).
Note that RMSProp outperformed other adaptive methods
like Adagrad order Adadelta as well as SGD with momentum in a large number of tests in (Schaul et al., 2014). It
has been argued that if the changes in the parameter update

1
Î³
â‰¤ Î²t â‰¤ 1 âˆ’ ,
t
t

vt,i =

t
X
j=1

(1 âˆ’ Î²j )

t
Y

2
Î²k gj,i
.

k=j+1

Pt
Qt
2
With Î²t = 1âˆ’ 1t one gets, vt,i = j=1 1j k=j+1 kâˆ’1
k gj,i ,
and
the telescoping product ones gets
Qt using
j
kâˆ’1
k=j+1 k = t and thus
Pt
2
vt,i = 1t j=1 gj,i
.
If one uses additionally the stepsize scheme Î±t =
t =

âˆšÎ´
T

Î±
âˆš
t

and

, then we recover the update scheme of Adagrad,

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

see (4), as a particular case of RMSProp. We are not aware
of that this correspondence of Adagrad and RMSProp has
been observed before.
The proof of the regret bound for RMSProp relies on the
following lemma.
Lemma 4.1 Let Assumptions A1 and A2 and suppose that
1 âˆ’ 1t â‰¤ Î²t â‰¤ 1 âˆ’ Î³t for some 0 < Î³ â‰¤ 1, and t â‰¥ 1. Also
p
âˆš
for t > 1 suppose (t âˆ’ 1)tâˆ’1 â‰¤ tt , then
T
X

âˆš

t=1

2
gt,i

t vt,i +

âˆš

tt

â‰¤


âˆš
2(2 âˆ’ Î³) p
T vT,i + T T .
Î³

Proof: The lemma is proven via induction. For T = 1 we
2
have v0 = 0 and thus v1,i = (1 âˆ’ Î²1 )g1,i
and thus
2
2
(1 âˆ’ Î²1 )gt,i
g1,i
q

=
âˆš
( v1,i + 1 )
2 +
(1 âˆ’ Î²1 )
(1 âˆ’ Î²1 )g1,i
1
q
2 +
âˆš
(1 âˆ’ Î²1 )g1,i
1
( v1,i + 1 )
.
â‰¤
â‰¤
1 âˆ’ Î²1
Î³

Note that Î³1 â‰¤ 2(2âˆ’Î³)
since 2(2 âˆ’ Î³) > 1 for Î³ â‰¤ 1 hence
Î³
the bound holds for T = 1. For T > 1 we suppose that the
bound is true for T âˆ’ 1 and get
T
âˆ’1
X

2
gt,i

âˆš
t vt,i + tt
q

p
2(2 âˆ’ Î³) 
â‰¤
(T âˆ’ 1) vT âˆ’1,i + (T âˆ’ 1)T âˆ’1 .
Î³
âˆš

t=1

2
as vT âˆ’1,i =
We rewrite vT,i = Î²T vT âˆ’1,i + (1 âˆ’ Î²T )gT,i
p
âˆš
1âˆ’Î²T 2
1
and
with
v
âˆ’
g
(t
âˆ’
1)
â‰¤
tt we get
tâˆ’1
T,i
Î²T T,i
Î²T
q
p
(T âˆ’ 1)vT âˆ’1,i + (T âˆ’ 1)T âˆ’1
s
âˆš
T âˆ’1
(T âˆ’ 1)(1 âˆ’ Î²T ) 2
vT,i âˆ’
gT,i + T T
â‰¤
Î²T
Î²T
s
âˆš
T âˆ’1
(T âˆ’ 1)(1 âˆ’ Î²T ) 2
T vT,i âˆ’
gT,i + T T
=
T Î²T
Î²T
s
âˆš
(T âˆ’ 1)(1 âˆ’ Î²T ) 2
â‰¤ T vT,i âˆ’
gT,i + T T
Î²T

â‰¤

p

âˆš
T vT,i +

T T âˆ’

(T âˆ’ 1)(1 âˆ’ Î²T )
2
âˆš
(8)
 gT,i
p
2Î²T
T vT,i + T T

Using the above bound we have the following
T
X
t=1

âˆš

2
gt,i

t vt,i +

T
âˆ’1
X

âˆš

tt

2
gT,i
âˆš
+p
t vt,i + tt
T vT,i + T T
t=1
q

p
2(2 âˆ’ Î³) 
â‰¤
(T âˆ’ 1) vT âˆ’1,i + (T âˆ’ 1)T âˆ’1
Î³
2
gT,i
âˆš
+p
T vT,i + T t

âˆš
2(2 âˆ’ Î³) p
â‰¤
T vT,i + T T
Î³
2

gT,i
2(2 âˆ’ Î³) (T âˆ’ 1)(1 âˆ’ Î²T ) 
âˆš
+ 1âˆ’
p
Î³
2Î²T
T vT,i + T T

=

âˆš

2
gt,i

âˆš

In
 the last step we use (8)
 and since for T > 1 the term
(T âˆ’1)(1âˆ’Î²T )
1 âˆ’ 2(2âˆ’Î³)
â‰¤ 0 for 1 âˆ’ 1t â‰¤ Î²t â‰¤ 1 âˆ’ Î³t 
Î³
2Î²T

Corollary 4.1 Let Assumptions A1, A2 hold and suppose
that 1 âˆ’ 1t â‰¤ Î²t â‰¤ 1 âˆ’ Î³t for some 0 < Î³ â‰¤ 1, and
p
âˆš
t â‰¥ 1. Also for t > 1 suppose (t âˆ’ 1)tâˆ’1 â‰¤ tt , and
set Î±t = âˆšÎ±t , then
T
d 

X
âˆš
p
 Î±(2 âˆ’ Î³) X
Î±t 

gt , Aâˆ’1
g
â‰¤
T
v
+
T

t
T,i
T
t
2
Î³
t=1
i=1

âˆš
Proof: Using the definition of At = diag( vt ) + t I,
Î±
Î±t = âˆšt along with Lemma 4.1 we get
T
T
d
2
X
 X
gt,i
Î±t 

Î±t X
âˆ’1
gt , At gt =
âˆš
2
2 i=1 vt,i + t
t=1
t=1

=

T
d
2
gt,i
Î± XX
âˆš
âˆš
2 t=1 i=1 t vt,i + tt

â‰¤


âˆš
Î± X 2(2 âˆ’ Î³) p
T vT,i + T T
2 i=1
Î³

d


Note that in the last step we have used that TT âˆ’1
Î²T â‰¤ 1 and
âˆš
âˆš
âˆš
the fact that x is concave and thus x âˆ’ c â‰¤ x âˆ’ 2âˆšc x
given that x âˆ’ c â‰¥ 0, along with âˆš âˆ’1 â‰¤ âˆš âˆ’1 âˆš
T vT ,i

for vt,i 6= 0 we have
q
p
(T âˆ’ 1)vT âˆ’1,i + (T âˆ’ 1)T âˆ’1

T vT ,i + T t

With the help of Lemma 4.1 and Corollary 4.1 we can now
state the regret bound for RMSProp.
Theorem 4.1 Let Assumptions A1, A2 hold and let Î¸t be
the sequence generated by RMSProp in Algorithm 3, where
gt âˆˆ âˆ‚ft (Î¸t ) and ft : C â†’ R is an arbitrary convex function and Î±t = âˆšÎ±t for some Î± > 0 and 1âˆ’ 1t â‰¤ Î²t â‰¤ 1âˆ’ Î³t

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

p
for some 0 < Î³ â‰¤ 1. Also for t > 1 let (t âˆ’ 1)tâˆ’1 â‰¤
âˆš
tt , then the regret of RMSProp can be upper bounded
for T â‰¥ 1 as
R(T ) â‰¤

d


âˆš
Î±(2 âˆ’ Î³)  X p
T vT,i + T T
+
2Î±
Î³
i=1

 D2

âˆ

Proof: Note that for every convex function ft : C â†’ R it
holds for all x, y âˆˆ C and gt âˆˆ âˆ‚ft (x),
ft (y) â‰¥ ft (x) + hgt , y âˆ’ xi .

R(T ) =

âˆ—

(ft (Î¸t ) âˆ’ ft (Î¸ )) â‰¤

t=1

T
X

2
and vt,i = Î²t vtâˆ’1,i + (1 âˆ’ Î²t )gt,i
as well as Î²t â‰¥ 1 âˆ’
which implies tÎ²t â‰¥ t âˆ’ 1. We get
âˆš
âˆš
p
t(At )ii = tvt,i + tt
q
âˆš
2 +
= tÎ²t vtâˆ’1,i + t(1 âˆ’ Î²t )gt,i
tt
q
âˆš
â‰¥ (t âˆ’ 1)vtâˆ’1,i + t âˆ’ 1tâˆ’1 ,

where
we used in the last inequality that
âˆš
t âˆ’ 1tâˆ’1 . Note that

We use this to upper bound the regret as
T
X

Note that At are diagonal matrices for all t â‰¥ 1. We note
that
âˆš
(At )ii = vt,i + t ,

hgt , Î¸t âˆ’ Î¸âˆ— i

T
X

t=1

Using the non-expansiveness of the weighted projection,
we have

=

â‰¤




âˆ’ 2Î±t hgt , Î¸t âˆ’ Î¸âˆ— i + Î±t2 gt , Aâˆ’1
t gt

=

q

(t âˆ’ 1)vtâˆ’1,i +

âˆš

t âˆ’ 1tâˆ’1



2
Dâˆ

tvt,i +

âˆš

q

tt âˆ’

(t âˆ’ 1)vtâˆ’1,i âˆ’

âˆš

t âˆ’ 1tâˆ’1



t=2

p

âˆš
T vT,i +

T T âˆ’

âˆš

v1,i âˆ’ 1



d

kÎ¸1 âˆ’ Î¸âˆ— k2A1
D2 X âˆš
â‰¤ âˆ
v1,i + 1 .
2Î±
2Î± i=1

T
X

Î±t 

+
gt , Aâˆ’1
t gt
2
t=1

+

2
Dâˆ

where the inequality could be done as we showed before
that the difference of the terms in vt,i is non-negative for
âˆš
all i âˆˆ [d] and t â‰¥ 1. As (A1 )ii = v1,i + 1 we get

T
X
kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ kÎ¸t+1 âˆ’ Î¸âˆ— k2At
2Î±t
t=1

T 
X

d
X
i=1

R(T )

âˆ’

tt âˆ’

t=2

Hence we can upper bound the regret as follows

kÎ¸1 âˆ’
2Î±1

d
X

âˆš

i=1


kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ kÎ¸t+1 âˆ’ Î¸âˆ— k2At
Î±t 

+
gt , Aâˆ’1
â‰¤
t gt
2Î±t
2

â‰¤

tvt,i +

T 
X
p

hgt , Î¸t âˆ’ Î¸âˆ— i

Î¸âˆ— k2A1

(Î¸t,i âˆ’ Î¸iâˆ— )2

t=2 i=1

This yields

â‰¤

â‰¥

kÎ¸t âˆ’ Î¸âˆ— k2âˆštAt âˆ’âˆštâˆ’1Atâˆ’1

T X
d
X

p

t

â‰¤ kÎ¸t âˆ’ Î¸âˆ— k2At

tt

t=2

2
Î¸âˆ— kAt

kÎ¸t+1 âˆ’

2


âˆ—
= PCAt Î¸t âˆ’ Î±t Aâˆ’1
t gt âˆ’ Î¸ 
At


âˆ’1
âˆ— 2

â‰¤ Î¸t âˆ’ Î±t At gt âˆ’ Î¸ A

âˆš

1
t

Thus in total we have
Î¸âˆ— k2At

kÎ¸t âˆ’
2Î±t

âˆ’

kÎ¸t âˆ’

Î¸âˆ— k2Atâˆ’1



2Î±tâˆ’1

T
X

kÎ¸T +1 âˆ’ Î¸âˆ— k2AT
Î±t 

+
gt , Aâˆ’1
t gt
2Î±t
2
t=1

kÎ¸1 âˆ’ Î¸âˆ— k2A1
â‰¤
2Î±
T kÎ¸ âˆ’ Î¸ âˆ— k2
T
âˆš
âˆš
X
X
t

Î±t 

tAt âˆ’ tâˆ’1Atâˆ’1
gt , Aâˆ’1
+
+
t gt
2Î±
2
t=2
t=1
In the last step we used Î±t = âˆšÎ±t . We show that
âˆš
âˆš
tAt âˆ’ t âˆ’ 1Atâˆ’1  0 âˆ€t âˆˆ [T ]

(9)

R(T ) â‰¤

2
Dâˆ

Pd

p

i=1

T vT,i +

âˆš

T T



2Î±
T
X



Î±t
+
gt , Aâˆ’1
t gt .
2
t=1

Finally, with Corollary 4.1 we get the result.



Note that for Î²t = 1 âˆ’ 1t , that is Î³ = 1, and t = TÎ´ where
RMSProp corresponds to Adagrad we recover the regret
bound of Adagrad in the convex case, see Theorem 2.1, up
to the damping factor. Note that in this case
v
u T
uX
p
2 = kg
T vT,i = t
gj,i
1:T,i k2 .
j=1

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Algorithm 4 SC-RMSProp
Input: Î¸1 âˆˆ C , Î´0 = 1 , v0 = 0 âˆˆ Rd
for t = 1 to T do
gt âˆˆ âˆ‚ft (Î¸t )
vt = Î²t vtâˆ’1 + (1 âˆ’ Î²t )(gt  gt )
Set t = Î´tt where Î´t,i â‰¤ Î´tâˆ’1,i for i âˆˆ [d] and Î±t =
At = diag(vt + t )

Î¸t+1 = PCAt Î¸t âˆ’ Î±t Aâˆ’1
t gt
end for

+

t=2

âˆ’
Î±
t

4.1. SC-RMSProp
Similar to the extension of Adagrad to SC-Adagrad, we
present in this section SC-RMSProp which achieves a logarithmic regret bound.
Note that again there exist choices for the parameters of
SC-RMSProp such that it reduces to SC-Adagrad. The correspondence is given by the choice
Î±
Î´t
, t = ,
t
t
P
t
2
for which again it follows vt,i = 1t j=1 gj,i
with the same
argument as for RMSProp. Please see Equation (5) for the
correspondence. Moreover, with the same argument as for
SC-Adagrad we use a decay scheme for the damping factor
t,i = Î¾2

eâˆ’Î¾1 t vt,i
,
t

Î±t =

i = 1, . . . , d. for Î¾1 â‰¥ 0 , Î¾2 > 0

The analysis of SC-RMSProp is along the lines of SCAdagrad with some overhead due to the structure of vt .

T
X
t=2

t=1


Î±
Î±t 

gt , Aâˆ’1
t gt â‰¤
2
2Î³

Î±
+
2Î³

T X
d
X
t=2 i=1

i=1

log

T v + 
T,i
T,i
1,i

Î² A

Î±
t tâˆ’1 âˆ’ Î²t diag(tâˆ’1 )
(tAt )âˆ’1 â€¢
2
(1 âˆ’ Î²t )

T
X
Î±
t=2

2

(tAt )âˆ’1 â€¢

+

T
X
Î±
t=2

â‰¤

+

2

(tAt )âˆ’1 â€¢

T
X
Î±
t=2

2

(tAt )âˆ’1 â€¢



+

T
d
Î± X X âˆ’tt,i + (t âˆ’ 1)tâˆ’1,i
2Î³ t=2 i=1
tvt,i + tt,i

+

d
Î± X (1 âˆ’ Î³)(1 + log T )
2Î³ i=1 inf jâˆˆ[1,T ] jvj,i + jj,i

We note that
log(|T AT |) = log

d
Y

(T (vT,i + T,i ))



i=1

=

d
X

log(T (vT,i + T,i )),

i=1
2
gt,i

Proof: Using hx, Axi = A â€¢ diag(xx ) and with
=
vt,i âˆ’Î²t vtâˆ’1 ,i
and
using
that
A
is
diagonal
with
(A
)
=
t
t ii
1âˆ’Î²t
vt,i + t,i , we get

2

 âˆ’diag( ) + Î² diag( ) 
t
t
tâˆ’1
(1 âˆ’ Î²t )

1
â‰¤ Î³t . In the last step we
In the first inequality we use 1âˆ’Î²
t
1
1
use, that âˆ€t > 1, tÎ²t â‰¤ tâˆ’1 . Finally, by upper bounding
the last term with Lemma 4.3
T
 |T A | 
X

Î±t 

Î±
T
gt , Aâˆ’1
log
t gt â‰¤
2
2Î³
|diag(
1 )|
t=1

âˆ’tt,i + (t âˆ’ 1)tâˆ’1,i
tvt,i + tt,i

T

t=1

 âˆ’diag( ) + Î² diag( ) 
t
t
tâˆ’1
(1 âˆ’ Î²t )

T

 |A | 

Î±
|tAt |
Î± X
1
log
log
+
2Î³
|diag(1 )|
2Î³ t=2
|(t âˆ’ 1)Atâˆ’1 |

d
Î± X (1 âˆ’ Î³)(1 + log T )
+
2Î³ i=1 inf jâˆˆ[1,T ] jvj,i + jj,i

T
X
Î±

 âˆ’diag( ) + Î² diag( ) 
t
t
tâˆ’1
(1 âˆ’ Î²t )

T
 |A | 
 |tA | 
Î± X
Î±
1
t
+
â‰¤
log
log
2Î³
|diag(1 )|
2Î³ t=2
|tÎ²t Atâˆ’1 |

Lemma 4.2 Let Î±t = Î±t , 1 âˆ’ 1t â‰¤ Î²t â‰¤ 1 âˆ’ and At as
defined in SC-RMSProp, then it holds for all T â‰¥ 1,
d
X

 A âˆ’ diag( ) 
t
t
(1 âˆ’ Î²t )

Î±
((A1 )âˆ’1 â€¢ (A1 âˆ’ diag(1 )))
2Î³
T
 tA âˆ’ tÎ² A 
X
Î±
t
t tâˆ’1
(tAt )âˆ’1 â€¢
+
2
Î³
t=2

Î³
t

T
X

2

(tAt )âˆ’1 â€¢

â‰¤

+

1
Î²t = 1 âˆ’ ,
t

T
X
Î±

((tAt )âˆ’1 â€¢ diag(gt gtT ))

Î±
((A1 )âˆ’1 â€¢ (A1 âˆ’ diag(1 )))
=
2(1 âˆ’ Î²1 )

and similar, log(|diag(1 )|) =

Pd

i=1

log(1,i ).

Note that for Î³ = 1 and the choice t =
the result of Lemma 3.2.
Lemma 4.3 Let t â‰¤ 1t and 1 âˆ’
1 â‰¥ Î³ > 0. Then it holds,
T
X
Î±
t=2

2

(tAt )âˆ’1 â€¢

1
t

Î´t
t



this reduces to

â‰¤ Î²t â‰¤ 1 âˆ’

Î³
t

for some

 âˆ’diag( ) + Î² diag( ) 
t
t
tâˆ’1
(1 âˆ’ Î²t )

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

â‰¤

T
d
Î± X X âˆ’tt,i + (t âˆ’ 1)tâˆ’1,i
2Î³ t=2 i=1
tvt,i + tt,i

d

d
Î± X (1 âˆ’ Î³)(1 + log T )
+
2Î³ i=1 inf jâˆˆ[1,T ] jvj,i + jj,i

Proof: Using
T
X
Î±
t=2

2

Î³
t

d
Î± X (1 âˆ’ Î³)(1 + log T )
+
2Î³ i=1 inf jâˆˆ[1,T ] jvj,i + jj,i

Proof: We rewrite the regret bound with the definition of
Âµ-strongly convex functions as

â‰¤ 1 âˆ’ Î²t â‰¤ 1t , we get

(tAt )âˆ’1 â€¢


 (Î¸ âˆ’ Î¸âˆ— )2
1X
Î±
t,i
i
(Î´T,i âˆ’ Î´1,i )
inf
âˆ’
2 i=1 tâˆˆ[T ]
Î±
Î³(tvt,i + tt,i )

+

 âˆ’diag( ) + Î² diag( ) 
t
t
tâˆ’1
(1 âˆ’ Î²t )

R(T ) =

â‰¤

T
d
Î± X X âˆ’tt,i + (t âˆ’ 1)tâˆ’1,i + (tÎ²t âˆ’ (t âˆ’ 1))tâˆ’1,i
2Î³ t=2 i=1
tvt,i + tt,i

â‰¤

d
T
Î± X X âˆ’tt,i + (t âˆ’ 1)tâˆ’1,i
2Î³ t=2 i=1
tvt,i + tt,i

Î±
2Î³

t=2 i=1

â‰¤

Î±
2Î³

Î±
2Î³
+

i=1

T X
d
X
t=2 i=1

Î±
2Î³

d
X
i=1

t=1

2

This yields

T
X
t=2

hgt , Î¸t âˆ’ Î¸âˆ— i
â‰¤

1
t


kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ kÎ¸t+1 âˆ’ Î¸âˆ— k2At
Î±t 

gt , Aâˆ’1
+
t gt
2Î±t
2

Hence we can upper bound the regret as follows

âˆ’tt,i + (t âˆ’ 1)tâˆ’1,i
tvt,i + tt,i

R(T )

1âˆ’Î³
(1 + log T )
inf jâˆˆ[2,T ] jvj,i + jj,i


Theorem 4.2 Let Assumptions A1, A2 hold and let Î¸t be
the sequence generated by SC-RMSProp in Algorithm 4,
where gt âˆˆ âˆ‚ft (Î¸t ) and ft : C â†’ R is an arbitrary Âµstrongly convex function (Âµ âˆˆ Rd+ ) with Î±t = Î±t for some
(2âˆ’Î³)G2

Î± â‰¥ 2 mini Âµâˆi and 1 âˆ’ 1t â‰¤ Î²t â‰¤ 1 âˆ’ Î³t for some 0 <
Î³ â‰¤ 1. Furthermore, set t = Î´tt and assume 1 â‰¥ Î´t,i > 0
and Î´t,i â‰¤ Î´tâˆ’1,i âˆ€t âˆˆ [T ], âˆ€i âˆˆ [d], then the regret of SCRMSProp can be upper bounded for T â‰¥ 1 as
R(T ) â‰¤

2

kÎ¸t âˆ’ Î¸âˆ— kdiag(Âµ)

t



â‰¤ kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ 2Î±t hgt , Î¸t âˆ’ Î¸âˆ— i + Î±t2 gt , Aâˆ’1
t gt

(1 âˆ’ Î³)tâˆ’1,i
tvt,i + tt,i

1âˆ’Î³
inf jâˆˆ[2,T ] jvj,i + jj,i

T
X

kÎ¸t+1 âˆ’ Î¸âˆ— kAt

2


âˆ—
= PCAt Î¸t âˆ’ Î±t Aâˆ’1
g
âˆ’
Î¸

t
t
At


âˆ’1
âˆ— 2

â‰¤ Î¸t âˆ’ Î±t At gt âˆ’ Î¸ A

d
T
Î± X X âˆ’tt,i + (t âˆ’ 1)tâˆ’1,i
â‰¤
2Î³ t=2 i=1
tvt,i + tt,i
d
X

hgt , Î¸t âˆ’ Î¸âˆ— i âˆ’

Using the non-expansiveness of the weighted projection,
we get

=

T X
d
X

T
X
t=1

T
d
Î± X X âˆ’tt,i + tÎ²t tâˆ’1,i
=
2Î³ t=2 i=1
tvt,i + tt,i

+

(ft (Î¸t ) âˆ’ ft (Î¸âˆ— ))

t=1
T

 âˆ’tdiag( ) + tÎ² diag( ) 
Î± X
t
t
tâˆ’1
â‰¤
(tAt )âˆ’1 â€¢
2 t=2
Î³

+

T
X

d
Tv + Î´ 
2
Dâˆ
tr(diag(Î´1 ))
Î± X
T,i
T,i
+
log
2Î±
2Î³ i=1
Î´1,i

â‰¤

T
X
kÎ¸t âˆ’ Î¸âˆ— k2At âˆ’ kÎ¸t+1 âˆ’ Î¸âˆ— k2At
2Î±t
t=1

+

T
T
X
 X
Î±t 

2
kÎ¸t âˆ’ Î¸âˆ— kdiag(Âµ)
gt , Aâˆ’1
t gt âˆ’
2
t=1
t=1

T 
kÎ¸t âˆ’ Î¸âˆ— k2Atâˆ’1 
kÎ¸1 âˆ’ Î¸âˆ— k2A1 X
kÎ¸t âˆ’ Î¸âˆ— k2At
â‰¤
+
âˆ’
2Î±1
2Î±t
2Î±tâˆ’1
t=2

âˆ’

âˆ’

T
X

kÎ¸T +1 âˆ’ Î¸âˆ— k2AT
Î±t 

+
gt , Aâˆ’1
t gt
2Î±t
2
t=1
T
X

2

kÎ¸t âˆ’ Î¸âˆ— kdiag(Âµ)

t=1

â‰¤

+

T
kÎ¸t âˆ’ Î¸âˆ— k2tAt âˆ’(tâˆ’1)Atâˆ’1
kÎ¸1 âˆ’ Î¸âˆ— k2A1 X
+
2Î±1
2Î±
t=2
T
T
X
 X
Î±t 

2
gt , Aâˆ’1
g
âˆ’
kÎ¸t âˆ’ Î¸âˆ— kdiag(Âµ)
t
t
2
t=1
t=1

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Now on imposing the following condition

+

tAt âˆ’ (t âˆ’ 1)Atâˆ’1 âˆ’ 2Î± diag(Âµ)

(10)

 diag(Î´t ) âˆ’ diag(Î´tâˆ’1 ) âˆ€t â‰¥ 2
A1 âˆ’ 2Î±diag(Âµ)  diag(Î´1 )

(11)

T
2

Dâˆ
tr(diag(Î´1 )) X Î±t 

gt , Aâˆ’1
+
â‰¤
t gt
2Î±
2
t=1

Î´

Note that with t,i = t,i
t and Î´t,i â‰¤ Î´tâˆ’1,i for all t â‰¥ 1 and
i âˆˆ [d], it holds tt,i â‰¤ (t âˆ’ 1)tâˆ’1,i . We show regarding
the inequality in (10)

+

tvt,i + tt,i âˆ’ (t âˆ’ 1)vtâˆ’1,i âˆ’ (t âˆ’ 1)tâˆ’1,i âˆ’ 2Î±Âµi
2
= tÎ²t,i vtâˆ’1,i + tt,i + t(1 âˆ’ Î²t )gt,i
âˆ’ (t âˆ’ 1)tâˆ’1,i

2
â‰¤ (1 âˆ’ Î³)vtâˆ’1,i + gt,i
âˆ’ 2Î±Âµi + tt,i âˆ’ (t âˆ’ 1)tâˆ’1,i

= (2 âˆ’

âˆ’ 2Î±Âµi + tt,i âˆ’ (t âˆ’ 1)tâˆ’1,i

âˆ’ 2Î±Âµi + tt,i âˆ’ (t âˆ’ 1)tâˆ’1,i

â‰¤ tt,i âˆ’ (t âˆ’ 1)tâˆ’1,i ,
where the last inequality follows by choosing Î± â‰¥
G2âˆ
2minÂµi (2 âˆ’ Î³) and we have used that
i

vt,i =

t
t
Y
X
2
Î²k gj,i
(1 âˆ’ Î²j )
j=1

â‰¤ G2âˆ

j=1

Î²k âˆ’

k=j+1

t
Y

Î²k

k=j

k=1

The second inequality (11) holds easily with the given
choice of Î±. Choosing some Î²t = 1 âˆ’ Î³t

2
â‰¥ (t âˆ’ 1)vtâˆ’1,i + t(1 âˆ’ Î²t )gt,i
â‰¥ (t âˆ’ 1)vtâˆ’1,i

Hence Î´t,i â‰¤ Î´tâˆ’1,i where Î´t,i = eâˆ’ t vt,i for  > 0. With
Î´
t,i = t,i
t we have tt â‰¤ (t âˆ’ 1)tâˆ’1 .
R(T )
2Î±
+

T
X
t=1

=

+

T
X
kÎ¸t âˆ’ Î¸âˆ— kdiag(Î´t )âˆ’diag(Î´tâˆ’1 )
t=2

2Î±


Î±t 

gt , Aâˆ’1
t gt
2

kÎ¸1 âˆ’ Î¸âˆ— k2diag(Î´1 )
2Î±

+

T X
d
X
(Î¸t,i âˆ’ Î¸âˆ— )2 (Î´t,i âˆ’ Î´tâˆ’1,i )
i

t=2 i=1

d
Tv + Î´ 
2
tr(diag(Î´1 ))
Î± X
Dâˆ
T,i
T,i
+
log
2Î±
2Î³ i=1
Î´1,i
d

+

 (Î¸ âˆ’ Î¸âˆ— )2

Î±
1X
t,i
i
inf
âˆ’
(Î´T,i âˆ’ Î´1,i )
2 i=1 tâˆˆ[T ]
Î±
Î³(tvt,i + tt,i )

+

d
Î± X (1 âˆ’ Î³)(1 + log T )
2Î³ i=1 inf jâˆˆ[1,T ] jvj,i + jj,i

Note that the regret bound reduces for Î³ = 1 to that of SCAdagrad. For 0 < Î³ < 1 a comparison between the bounds
is not straightforward as the vt,i terms cannot be compared.
It is an interesting future research question whether it is
possible to show that one scheme is better than the other
one potentially dependent on the problem characteristics.

5. Experiments

2
tvt,i = tÎ²t,i vtâˆ’1,i + t(1 âˆ’ Î²t )gt,i

kÎ¸1 âˆ’ Î¸âˆ— k2diag(Î´1 )

â‰¤



t


Y
â‰¤ G2âˆ 1 âˆ’
Î²k â‰¤ G2âˆ

â‰¤

(Î¸t,i âˆ’ Î¸iâˆ— )2 (Î´t,i âˆ’ Î´tâˆ’1,i ) Î±(Î´t,i âˆ’ Î´tâˆ’1,i )
âˆ’
Î±
Î³(tvt,i + tt,i )

where we have used Lemma 4.2 in the last inequality and
Î´

t,i = t,i
t for i âˆˆ [d] and t â‰¥ 1.

k=j+1
t  Y
t
X

d

d
Î± X (1 âˆ’ Î³)(1 + log T )
+
2Î³ i=1 inf jâˆˆ[1,T ] jvj,i + jj,i

+ tt,i âˆ’ (t âˆ’ 1)tâˆ’1,i
+

2Î±

d
Tv + Î´ 
2
Î± X
Dâˆ
tr(diag(Î´1 ))
T,i
T,i
+
log
2Î±
2Î³ i=1
Î´1,i
T

2
= (tÎ²t,i âˆ’ (t âˆ’ 1))vtâˆ’1,i + t(1 âˆ’ Î²t )gt,i
âˆ’ 2Î±Âµi

â‰¤ (1 âˆ’

i

1 XX
+
2 t=2 i=1

âˆ’ (t âˆ’ 1)vtâˆ’1,i âˆ’ 2Î±Âµi

G2âˆ

T X
d
X
(Î¸t,i âˆ’ Î¸âˆ— )2 (Î´t,i âˆ’ Î´tâˆ’1,i )
t=2 i=1

â‰¤

Î³)G2âˆ
Î³)G2âˆ

T
X

Î±t 

gt , Aâˆ’1
t gt
2
t=1

2Î±

The idea of the experiments is to show that the proposed
algorithms are useful for standard learning problems in
both online and batch settings. We are aware of the fact
that in the strongly convex case online to batch conversion is not tight (Hazan & Kale, 2014), however that does
not necessarily imply that the algorithms behave generally suboptimal. We compare all algorithms for a strongly
convex
 problem
and present relative suboptimality plots,
f (xt )âˆ’pâˆ—
, where pâˆ— is the global optimum, as well
log10
pâˆ—
as separate regret plots, where we compare to the best optimal parameter in hindsight for the fraction of training
points seen so far. On the other hand RMSProp was originally developed by (Hinton et al., 2012) for usage in deep
learning. As discussed before the fixed choice of Î²t is

!

100

10âˆ’1

1

50

100
Epoch

150

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

100

10âˆ’1

200

1

50

(a) CIFAR10

100
Epoch

150

Relative Sub-optimality (log scale)

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

Relative Sub-optimality (log scale)

Relative Sub-optimality (log scale)

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

100

10âˆ’1

10âˆ’2

200

1

50

(b) CIFAR100

100
Epoch

150

200

(c) MNIST

Figure 1. Relative Suboptimality vs Number of Epoch for L2-Regularized Softmax Regression

104

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

5

10

Regret (log scale)

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

Regret (log scale)

Regret (log scale)

105

104
0.1

0.2

0.3

0.4
0.5
0.6
0.7
Dataset proportion

(a) CIFAR10

0.8

0.9

1.0

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

104

103
0.1

0.2

0.3

0.4
0.5
0.6
0.7
Dataset proportion

(b) CIFAR100

0.8

0.9

1.0

0.1

0.2

0.3

0.4
0.5
0.6
0.7
Dataset proportion

0.8

0.9

1.0

(c) MNIST

Figure 2. Regret (log scale) vs Dataset Proportion for Online L2-Regularized Softmax Regression

âˆš
not allowed if one wants to get the optimal O( T ) regret
bound in the convex case. Thus we think it is of interest to
the deep learning community, if the insights from the convex optimization case transfer to deep learning. Moreover,
Adagrad and RMSProp are heavily used in deep learning
and thus it is interesting to compare their counterparts SCAdagrad and SC-RMSProp developed for the strongly convex case also in deep learning. For the deep learning experiments we optimize the learning rate once for smallest
training objective as well as for best test performance after
a fixed number of epochs (typically 200 epochs).
Datasets: We use three datasets where it is easy, difficult and very difficult to achieve good test performance,
just in order to see if this influences the performance. For
this purpose we use MNIST (60000 training samples, 10
classes), CIFAR10 (50000 training samples, 10 classes)
and CIFAR100 (50000 training samples, 100 classes). We
refer to (Krizhevsky, 2009) for more details on the CIFAR
datasets.
Algorithms: We compare 1) Stochastic Gradient Descent
(SGD) (Bottou, 2010) with O(1/t) decaying step-size for
the strongly convex problems and for non-convex problems
we use a constant learning rate, 2) Adam (Kingma & Bai,
2015) , is used with step size decay of Î±t = âˆšÎ±t for strongly

convex problems and for non-convex problems we use a
constant step-size. 3) Adagrad, see Algorithm 1, remains
the same for strongly convex problems and non-convex
problems. 4) RMSProp as proposed in (Hinton et al., 2012)
is used for both strongly convex problems and non-convex
problems with Î²t = 0.9 âˆ€t â‰¥ 1. 5) RMSProp (Ours) is
used with step-size decay of Î±t = âˆšÎ±t and Î²t = 1 âˆ’ Î³t .
In order that the parameter range is similar to the original
RMSProp ((Hinton et al., 2012)) we fix as Î³ = 0.9 for all
experiment (note that for Î³ = 1 RMSProp (Ours) is equivalent to Adagrad), 6) SC-RMSProp is used with stepsize
Î±t = Î±t and Î³ = 0.9 as RMSProp (Ours) 7) SC-Adagrad
is used with a constant stepsize Î±. The decaying damping factor for both SC-Adagrad and SC-RMSProp is used
with Î¾1 = 0.1, Î¾2 = 1 for convex problems and we use
Î¾1 = 0.1, Î¾2 = 0.1 for non-convex deep learning problems. Finally, the numerical stability parameter Î´ used in
Adagrad, Adam, RMSProp is set to 10âˆ’8 as it is typically
recommended for these algorithms.
Setup: Note that all methods have only one varying parameter: the stepsize Î± which we choose from the set of
{1, 0.1, 0.01, 0.001, 0.0001} for all experiments. By this
setup no method has an advantage just because it has more
hyperparameters over which it can optimize. The optimal

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds
0.40
0.994

0.72
0.38

0.992
0.36

0.66

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.64
0.62
0.60

1

50

100
Epoch

150

0.34
0.32

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.30
0.28
0.26

200

1

50

(a) CIFAR10

100
Epoch

150

0.990

Test Accuracy

0.68

Test Accuracy

Test Accuracy

0.70

0.988
SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.986
0.984
0.982
0.980

200

1

50

(b) CIFAR100

100
Epoch

150

200

(c) MNIST

Figure 3. Test Accuracy vs Number of Epochs for 4-layer CNN
0.6

0.86

Training Objective

0.5

0.4

0.3

0.84
0.82
Test Accuracy

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.80
0.78
SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.76
0.74

0.2
0.72
0.1

1

100

200
Epoch

300

400

(a) Training Objective

0.70

1

100

200
Epoch

300

400

(b) Test Accuracy

Figure 4. Plots of ResNet-18 on CIFAR10 dataset

rate is always chosen for each algorithm separately so that
one achieves either best training objective or best test performance after a fixed number of epochs.
Strongly Convex Case - Softmax Regression: Given the
training data (xi , yi )iâˆˆ[m] and let yi âˆˆ [K]. we fit a linear model with cross entropy loss and use as regularization
the squared Euclidean norm of the weight parameters. The
objective is then given as
!
T
m
K
X
1 X
eÎ¸yi xi +byi
J(Î¸) = âˆ’
log PK Î¸T x +b
+Î»
kÎ¸k k2
i
j
m i=1
j
e
j=1

k=1

All methods are initialized with zero weights. The regularization parameter was chosen so that one achieves the
best prediction performance on the test set. The results are
shown in Figure 1. We also conduct experiments in an online setting, where we restrict the number of iterations to
the number of training samples. Here for all the algorithms,
we choose the stepsize resulting in best regret value at the
end. We plot the Regret ( in log scale ) vs dataset proportion seen, and as expected SC-Adagrad and SC-RMSProp
outperform all the other methods across all the considered
datasets. Also, RMSProp (Ours) has a lower regret values
than the original RMSProp as shown in Figure 2.

Convolutional Neural Networks: Here we test a 4-layer
CNN with two convolutional (32 filters of size 3 Ã— 3) and
one fully connected layer (128 hidden units followed by 0.5
dropout). The activation function is ReLU and after the last
convolutional layer we use max-pooling over a 2 Ã— 2 window and 0.25 dropout. The final layer is a softmax layer
and the final objective is cross-entropy loss. This is a pretty
simple standard architecture and we use it for all datasets.
The results are shown in Figures 3, 6. SC-RMSProp is
competitive in terms of training objective on all datasets
though SGD achieves the best performance. SC-Adagrad is
not very competitive and the reason seems to be that the numerical stability parameter is too small. RMSProp diverges
on CIFAR10 dataset whereas RMSProp (Ours) converges
on all datasets and has similar performance as Adagrad in
terms of training objective. Both RMSProp (Ours) and SCAdagrad perform better than all the other methods in terms
of test accuracy for CIFAR10 dataset. On both CIFAR100
and MNIST datasets SC-RMSProp is very competitive.
Multi-Layer Perceptron: We also conduct experiments
for a 3-layer Multi-Layer perceptron with 2 fully connected
hidden layers and a softmax layer according to the number
of classes in each dataset. For the first two hidden layers we

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds
0.19
0.41

0.18

0.926

0.17

0.40

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.37
0.36
0.35

1

50

100
Epoch

150

0.16
0.15
0.14

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.13
0.12
0.11
0.10

200

1

(a) CIFAR10

50

100
Epoch

150

Test Accuracy

0.38

Test Accuracy

Test Accuracy

0.924
0.39

0.922
SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.920

0.918

0.916

200

1

(b) CIFAR100

50

100
Epoch

150

200

(c) MNIST

Figure 5. Test Accuracy vs Number of Epochs for L2-Regularized Softmax Regression

1.50
1.25
1.00
0.75
0.50

3.5
3.0
2.5
2.0
1.5

0.14
SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.12

Training Objective

Training Objective

1.75

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

4.0

Training Objective

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

2.00

0.10
0.08
0.06
0.04
0.02

0.25
1

50

100
Epoch

(a) CIFAR10

150

200

1.0

1

50

100
Epoch

150

200

0.00

1

(b) CIFAR100

50

100
Epoch

150

200

(c) MNIST

Figure 6. Training Objective vs Number of Epoch for 4-layer CNN

have 512 units in each layer with ReLU activation function
and 0.2 dropout. The final layer is a softmax layer. We
report the results in Figures 7, 8. On all the datasets, SCAdagrad and SC-RMSProp perform better in terms of Test
accuracy and also have the best training objective performance on CIFAR10 dataset. On MNIST dataset, Adagrad
and RMSProp(Ours) achieves best training objective performance however SC-Adagrad and SC-RMSProp eventually performs as good as Adagrad. Here, the performance is
not as competitive as Adagrad, because the numerical stability decay parameter of SC-Adagrad and SC-RMSProp
are too prohibitive.
Residual Network: We also conduct experiments for
ResNet-18 network proposed in (He et al., 2016a) where
the residual blocks are used with modifications proposed
in (He et al., 2016b) on CIFAR10 dataset. We report the
results in Figures 4. SC-Adagrad, SC-RMSProp and RMSProp (Ours) have the best performance in terms of test
Accuracy and RMSProp (Ours) has the best performance
in terms of training objective along with Adagrad.
Given these experiments, we think that SC-Adagrad, SCRMSProp and RMSProp (Ours) are valuable new adaptive
gradient techniques for deep learning.

6. Conclusion
We have analyzed RMSProp originally proposed in the
deep learning community in the framework of online convex optimization. We show that the conditions for convergence of RMSProp for the convex case are different than
what is used by (Hinton et al., 2012) and that this leads
to better performance in practice. We also propose variants SC-Adagrad and SC-RMSProp which achieve logarithmic regret bounds for the strongly convex case. Moreover, they perform very well for different network models
and datasets and thus they are an interesting alternative to
existing adaptive gradient schemes. In the future we want
to explore why these algorithms perform so well in deep
learning tasks even though they have been designed for the
strongly convex case.

Acknowledgements
We would like to thank Shweta Mahajan and all the reviewers for their insightful comments.

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds
4.0

Training Objective

1.6
1.4
1.2
1.0
0.8

0.05
SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

3.5
Training Objective

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

1.8

0.6

3.0

2.5

2.0

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.04
Training Objective

2.0

0.03

0.02

0.01

1.5

0.4
1

50

100
Epoch

150

1.0

200

1

(a) CIFAR10

50

100
Epoch

150

0.00

200

1

(b) CIFAR100

50

100
Epoch

150

200

(c) MNIST

Figure 7. Training Objective vs Number of Epoch for 3-layer MLP
0.58

0.988
0.30
0.986
0.28

0.52

0.26

0.50
0.48

SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.46
0.44
0.42
0.40

1

50

100
Epoch

(a) CIFAR10

150

200

0.984

0.24
SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.22
0.20
0.18
0.16
1

50

100
Epoch

150

200

Test Accuracy

0.54
Test Accuracy

Test Accuracy

0.56

0.982
SGD
Adam
Adagrad
RMSProp
RMSProp (Ours)
SC-RMSProp
SC-Adagrad

0.980
0.978
0.976
1

(b) CIFAR100

50

100
Epoch

150

200

(c) MNIST

Figure 8. Test Accuracy vs Number of Epochs for 3-layer MLP

References
Bottou, L. Large-scale machine learning with stochastic
gradient descent. In Proceedings of COMPSTATâ€™2010,
pp. 177â€“186. Springer, 2010.
Daniel, C., Taylor, J., and Nowozin, S. Learning step size
controllers for robust neural network training. In AAAI,
2016.
Dauphin, Y., de Vries, H., and Bengio, Y. Equilibrated
adaptive learning rates for non-convex optimization. In
NIPS, 2015.
Duchi, J., Hazan, E., and Singer, Y. Adaptive subgradient
methods for online learning and stochastic optimization.
Journal of Machine Learning Research, 12:2121â€“2159,
2011.
Hazan, E. Introduction to online convex optimization.
Foundations and Trends in Optimization, 2:157â€“325,
2016.
Hazan, E. and Kale, S. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex
optimization. Journal of Machine Learning Research,
15(1):2489â€“2512, 2014.

Hazan, E., Agarwal, A., and Kale, S. Logarithmic regret algorithms for online convex optimization. Machine
Learning, 69(2-3):169â€“192, 2007.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770â€“778, 2016a.
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,
Jian. Identity mappings in deep residual networks. In
European Conference on Computer Vision, pp. 630â€“645.
Springer, 2016b.
Hinton, G., Srivastava, N., and Swersky, K. Lecture 6d
- a separate, adaptive learning rate for each connection.
Slides of Lecture Neural Networks for Machine Learning, 2012.
Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In CVPR,
2016.
Kingma, D. P. and Bai, J. L. Adam: a method for stochastic
optimization. ICLR, 2015.
Koushik, Jayanth and Hayashi, Hiroaki.
Improving
stochastic gradient descent with feedback.
arXiv
preprint arXiv:1611.01505, 2016.

Variants of RMSProp and Adagrad with Logarithmic Regret Bounds

Krizhevsky, A. Learning multiple layers of features from
tiny images. Technical report, University of Toronto,
2009.
McMahan, H Brendan. A survey of algorithms and
analysis for adaptive online learning. arXiv preprint
arXiv:1403.3465, 2014.
Ruder, S. An overview of gradient descent optimization
algorithms. preprint, arXiv:1609.04747v1, 2016.
Schaul, T., Antonoglou, I., and Silver, D. Unit tests for
stochastic optimization. In ICLR, 2014.
Schmidhuber, J. Deep learning in neural networks: An
overview. Neural Networks, 61:85 â€“ 117, 2015.
Srivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex,
Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:
a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):
1929â€“1958, 2014.
Szegedy, C., Ioffe, S., and Vanhoucke, V. Inception-v4,
inception-resnet and the impact of residual connections
on learning. In ICLR Workshop, 2016a.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In CVPR, 2016b.
Zeiler, M. D. ADADELTA: An adaptive learning rate
method. preprint, arXiv:1212.5701v1, 2012.
Zinkevich, M. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003.

