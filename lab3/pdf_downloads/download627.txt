Efficient Distributed Learning with Sparsity

A. Illustrative Examples of General Sparse Learning Problems
In this section we discuss additional examples of high-dimensional statistical learning problems for which Theorem 6 is
applicable.
A.1. Sparse Logistic Regression
For logistic model, performing maximum likelihood estimation (MLE) on (12) leads to the logistic loss function
`pyji , xβ, xji yq  logp1 exppyji xβ, xji yqq. For high-dimensional problems, when we add a `1 regularization, we
obtain the `1 regularized logistic regression model (Zhu & Hastie, 2004, Wu et al., 2009):
βbcentralize

1
 arg min
β mn

¸ ¸

j

Prms iPrns

logp1

exppyji xβ, xji yqq

λ||β ||1 .

1
1
The
°logistic loss is 4 -smooth, and we also know M  4 because of self-concordance (Zhang & Xiao, 2015). Let Lj pβq 
1
exppyji xβ, xji yqq, (Negahban et al., 2012) showed that if xji are drawn from mean zero distribution
iPrns logp1
n
with sub-Gaussian
the restricted strong condition (5). Moreover, we have the following control
 tails, then L1 pβq satisfies


 °

1
on the quantity  m

j

 
Prms ∇Lj pβ q .
8

Lemma 10. Then we have the following upper bound holds in probability at least 1  δ:

c
 ¸

 1

2 logpp{δ q


∇Lj pβ q À ||xji ||8
.
 m
mn
8
j Prms

The following `1 error bound states the estimation error for logistic regression with `1 regularization, which was established, for example, in (van de Geer, 2008, Negahban et al., 2012).
Lemma 11. Under the model (12), when n ¥ p64{κqs log p, we have the following estimation error bound for βb0 holds
with probability at least 1  δ:
c
sσX 2 logpnp{δ q

b
||β0  β ||1 À κ
.
n

With above analysis for sparse logistic regression model with random design, we are ready to present the results for the
estimation error bound which established local exponential convergence.
Corollary 12. Under sparse logistic regression model with random design, and set λt 1 as (9). If the following condition
holds for some T ¥ 0:

||βbT

c

 β ||1 ¤ 4 logp2p{δq .

n
Then with probability at least 1  2δ, we have the following estimation error bound for all t ¥ T :

||βbt

1  antT

1  β ||1 ¤

1

1  an



||βbt 1  β ||2 ¤ 1 1 ana

t T

1

n

where

an



24sσσX
κ

c

96sσσX
κ

?

c

4 sσσX
κ

logp2p{δ q
n

logpp{δ q
mn

c

logpp{δ q
mn

and bn



c



4ant T

1

4atnT bn

?sσσ

X

κ

c

logp2p{δ q
,
n

c

logp2p{δ q
,
n

(13)

(14)
(15)

logpnp{δ q
.
n

A.2. High-dimensional Generalized Linear Models
The results are readily extendable to other high-dimensional generalized linear models (McCullagh & Nelder, 1989, van de
Geer, 2008), where the response variable yji P Y is drawn from the distribution
Ppyji |xji q9 exp



yji xxji , β  y  Φpxxji , β  yq
Ap σ q




,

Efficient Distributed Learning with Sparsity

where Φpq is a link function and Apσ q is a scale parameter. Under the random subgaussian design, as long as the loss
function has Lipschitz gradient, then the algorithm and corresponding estimation error bound and be applied.
A.3. High-dimensional Graphical Models
The results can also be used for the distributed unsupervised learning setting where the task is to learn a sparse graphical
structure that represents the conditional independence between variables. Widely studied graphical models are Gaussian
graphical models (Meinshausen & Bühlmann, 2006, Yuan & Lin, 2007) for continuous data and Ising graphical models
(Ravikumar et al., 2010) for binary observations. As shown in (Meinshausen & Bühlmann, 2006, Ravikumar et al., 2010),
these model selection problems can be reduced to solving parallel `1 regularized linear regression and logistic regression
problems, respectively. Thus the approach presented in this paper can be readily applicable for these tasks.

B. Proofs
The section contains proofs of some theorems and lemmas stated in the main paper.
B.1. Proof of Lemma 8
Proof. Recall the definition of Le1 from (11). We have

1 ¸
∇Lj pβbt q  ∇L1 pβbt q
m

∇Le1 pβ  , βbt q  ∇L1 pβ  q



j

Prms

1 ¸
∇Lj pβ  q
m
j

Prms





1 ¸
1 ¸
∇L1 pβ  q  ∇L1 pβbt q  
∇Lj pβ  q 
∇Lj pβbt q.
m
m
j

Prms

j

Prms

Using the triangle inequality





∇Le1 pβ , βbt q


8
 ¸



¤  m1
∇Lj pβ  q
8
j Prms


 

¸
¸


1
∇L1 pβ q  ∇L1 pβbt q   1
∇Lj pβ  q 
∇Lj pβbt q .

m
m
8
j Prms
j Prms

We focus on bounding the second term in the right-hand-side inequality above. Let τji
vji pβbt q P Rp :

 `1 pyji , xβ , xji yq and define

vji pβbt q  xji p`1 pyji , xβ  , xji yq  `1 pyji , xβbt , xji yqq

 τji xji xTji



βbt  β 

	

xji

`3 pyji , uji q b
pxβt  β , xji yq2
2

where uji is a number between xβbt , xji y and xβ  , xji y. With this notation


 

¸
¸


1
1
∇L1 pβ q  ∇L1 pβbt q  
∇Lj pβ  q 
∇Lj pβbt q

m
m
8
j Prms
j Prms
 ¸


¸
¸


1
¤  n1 v1i pβbt q  mn
vji pβbt q
8
j i
iPrns
 ¸




 1

1 ¸¸
T b

T b

3




¤  n τ1i x1i x1i pβt  β q  mn
τji xji xji pβt  β q
M  max ||xji ||8  ||βbt  β  ||21 .
j,i
8
i
j i

Efficient Distributed Learning with Sparsity

The first term above can be further upper bounded by

 ¸

 1

1 ¸¸
T b

T b


τji xji xji pβt  β q
 n τ1i x1i x1i pβt  β q  mn
8
j
j i
 ¸

¸
¸


1
¤  n1 τ1i x1i xT1i  mn
τji xji xTji   ||βbt  β  ||1 .
8
j
j i



 
¸
¸


 1 ¸







1
¤  n τ1i x1i xT1i  E τji xji xTji   mn
τji xji xTji  E τji xji xTji   ||βbt  β  ||1 .
8
8
j i
iPrns

Using Hoeffding’s inequality together with a union bound, we have with probability at least 1  δ,

 ¸



c
 1



2 logp2p{δ q
2
T
T

τ1i x1i x1i  E τji xji xji  ¤ L max ||xji ||8
,
 n
j,i
n
8
iPrns

and





c
 1 ¸ ¸



2 logp2p{δ q
2
T
T

τji xji xji  E τji xji xji  ¤ L max ||xji ||8
.
 mn
j,i
mn
8
j i

Combining the bounds, the proof of the lemma is complete.
B.2. Proof of Lemma 9
Proof. The proof uses ideas presented in (Negahban et al., 2012). By triangle inequality we have

||βbt 1 ||1  ||β ||1 ||β pβbt 1  β qS pβbt 1  β qS ||1  ||β ||1
¥||β pβbt 1  β qS ||1  ||pβbt 1  β qS ||1  ||β ||1
||pβbt 1  β qS ||1  ||pβbt 1  β qS ||1 .
c

c

c

By the optimality of βbt
Thus

1

Le1 pβbt

for (4), we have
Le1 pβbt

bq

1 , βt

λt

1

b q  Le1 pβ  , βbt q

1 , βt

||βbt 1 ||1  Le1 pβ , βbt q  λt 1 ||β ||1 ¤ 0.
λt

1

By the convexity of Le1 p, βbt q, we further have
Le1 pβbt

Thus by Hölder’s inequality

p||pβbt 1  β qS ||1  ||pβbt 1  β qS ||1 q ¤ 0.
c

b q  Le1 pβ  , βbt q ¥ x∇Le1 pβ  , βbt q, βbt

1 , βt

0 ¥x∇Le1 pβ  , βbt q, βbt 1  β  y λt 1 p||pβbt
¥  ||∇Le1 pβ , βbt q||8 ||βbt 1  β ||1 λt

Under the assumption on λt

0¥

1

we further have

λt 1 b
||βt
2

1

 β ||1

λt

1

which completes the proof.

 β y.

 β qS ||1  ||pβbt 1  β qS ||1 q


b
b
1 p||pβt 1  β qS ||1  ||pβt 1  β qS ||1 q.
1

c

c

p||pβbt 1  β qS ||1  ||pβbt 1  β qS ||1 q
c

 λt2 1 ||pβbt 1  β qS ||1  3λ2t 1 ||pβbt 1  β qS ||1 ,
c

1

Efficient Distributed Learning with Sparsity

B.3. Proof of Theorem 6
Proof. For the term Le1 pβbt
Le1 pβbt

b q  Le1 pβ  , βbt q we have

1 , βt

b
b
e  b
1 , βt q  L1 pβ , βt q L1 pβt

1

q

C

1 ¸
∇Lj pβbt q  ∇L1 pβbt q, βbt
m

Prms
C
1 ¸
m

¥x∇L1 pβ q, βbt
C
1 ¸



C

Prms
C
1 ¸
m

1

j

 L1 pβ q 

m

G

j

j

Prms

∇L1 pβ  q

∇Lj pβbt q  ∇L1 pβbt q, β 

Prms
 κ||βbt
1β y
j

1

 β ||22

∇Lj pβbt q  ∇L1 pβbt q, βbt

∇Lj pβbt q  ∇L1 pβbt q, β 

G

1

G

1 ¸
∇Lj pβbt q  ∇L1 pβbt q, βbt
m
j

Prms

κ||βbt 1  β  ||22
x∇Le1 pβ , βbt q, βbt 1  β y

G

κ||βbt

1

1

 β

G

 β ||22 ,

where the first inequality we use the restricted strong convexity condition (5). Also by the optimality of βbt
have
Le1 pβbt

b q  Le1 pβ  , βbt q

1 , βt

λt

1

||βbt 1 ||1  λt 1 ||β ||1 ¤ 0.

Combining above two inequalities we obtain with probability at least 1  δ:
λt

1

||β ||1  λt 1 ||βbt 1 ||1 ¥x∇Le1 pβ , βbt q, βbt 1  β y κ||βbt 1  β ||22
¥  ||∇Le1 pβ , βbt q||8 ||βbt 1  β ||1 κ||βbt 1  β ||22
¥  λt2 1 ||βbt 1  β ||1 κ||βbt 1  β ||22 .

By triangle inequality that λt

1

κ||βbt

We get

||βbt 1  β ||1 ¥ λt 1 ||β ||1  λt 1 ||βbt 1 ||1 , we have
1

 β ||22 ¤ 3λ2t 1 ||βbt 1  β ||1

 3λ2t 1 p||pβbt 1  β qS ||1 ||pβbt 1  β qS ||1 q
c

¤ 3λ2t 1 p||pβbt 1  β qS ||1 3||pβbt 1  β qS ||1 q
6λt 1 ||pβbt 1  β qS ||1
¤6?sλt 1 ||pβbt 1  β qS ||2
¤6?sλt 1 ||βbt 1  β ||2 .
||βbt

?

6 sλt 1

.
1  β ||2 ¤
κ

1

for (4), we

Efficient Distributed Learning with Sparsity

Substitute λt

1

 β ||1 , we know


b
||pβbt 1  β qS ||1
1  β ||1 ¤||pβt 1  β qS ||1
¤4||pβbt 1  β qS ||1 ¤ 4?s||pβbt 1  β qS ||2
¤4?s||βbt 1  β ||2 ¤ 24sλκ t 1 ,

in (9) concludes the proof for `2 estimation error bound. For ||βbt

||βbt

which obtains the desired bound.

1

c

B.4. Proof of Theorem 3
Proof. Theorem 3 follows from Theorem 6 after we verify some conditions. First, it is easy to see that the quadratic loss
L  1, M  0. Under conditions of Theorem, with probability 1  δ,

c

 ¸

 1
logpp{δ q




∇Lj pβ q À σσX
.
 m
mn
8
j Prms

This follows from Corollary 5.17 of Vershynin (2012). Furthermore, with probability at least 1  δ, we have

||xji ||8 À σX
j Prms,iPrns
max

Finally,

a

logpmnp{δ q.

c

{δ q ,
À sσσκ X logpnp
n
with probability at least 1  δ (Wainwright, 2009, Meinshausen & Yu, 2009, Bickel et al., 2009). Plugging these bounds
into Theorem 6 completes the proof.
||βb0  β ||1

B.5. Proof of Corollary 7
Proof. The proof proceeds by recursively applying Theorem 6 and sum a geometric sequence. For notation simplicity let
a
b
c




48s  1 ¸

 ,
∇L
p
β
q
j



κ m
8


48sL
κ

48sM
κ

j Prms




max ||xji ||8


c

2

j,i




4 logp2p{δ q
n


,

max ||xji ||38 .
j,i

By Theorem 6 we have

||βbt 1  β ||1 ¤a
¤a
¤a
¤a

b||βbt  β  ||1 c||βbt  β  ||21
2b||βbt  β  ||1
2bpa

ţ



2b||βbt1  β  ||1 q ¤ . . .

p2bqk p2bqt 1 ||βb0  β ||1 .

k 0

t 1
 ap1 1 p2b2bq q p2bqt 1 ||βb0  β ||1 ,

 β ||2 , we first use (16) to obtain
2bqt q
t b

||βbt  β ||1 ¤ ap11  pp2b
q p2bq ||β0  β ||1 .

which completes the `1 estimation error bound. For ||βbt

1

(16)

Efficient Distributed Learning with Sparsity

Then apply Theorem 6 to obtain that

||βbt

a

1  β ||2 ¤ ?

4 s

 4?1 s

which concludes the proof.



p2b
?q ||βbt  β ||1 ¤ 4?a s
4 s

a

app2bq  p2bqt
1  p2bq

1

q




ap1  p2bqt q
?
4 s
1  p2bq
p2bqt 1 ||βb0  β ||1
b

?

t 1
t 1 b

 a4p?1sp1p2bqp2bqqq p2bq ||4β?0s β ||1 ,

p2bq ||βb0  β ||1




t

4 s

B.6. Proof of Lemma 10
Proof. By the definition of Lj pβ q, we have


1 ¸
1 ¸ ¸
∇Lj pβ  q 
xji yji 
m
mn
1
j

Prms

j

It is easy to check that



E yji 
and thus

1

yji
exppyji xβ, xji yq





Prms iPrns



 0,

and



yji 

1

yji
exppyji xβ, xji yq



.




yji
¤1
expp  yji xβ, xji yq 




yji
 0,
E xji yji 
1 exppyji xβ, xji yq
 





yji
xji yji 
 ¤ max p||xji ||8 q .

ji
1 expp  yji xβ, xji yq 8

Appling Azuma-Hoeffding inequality (Hoeffding, 1963) and the union bound over rps leads to the desired bound.

C. Full Experimental Results
We run the algorithms for both distributed regression and classification problems. The algorithms to be compared are:
• Local: the first machine just solves a related `1 regularized problem (lasso or `1 regularized logistic regression) with
the optimal λ, and outputs the solution. Obviously this approach is communication free.
• Centralize: the master gathers all data from different machines together, and solves a centralized `1 regularized loss
minimization problem with the optimal λ, and outputs the solution. This approach is communication expensive as all
data needs to be communicated, but it usually gives us the best estimation and prediction performance.
• Prox GD: the distributed proximal gradient descent is ran on the `1 regularized objective, where we initialized the
starting point with the first machine’s solution.
• Avg-Debias: the method proposed in Lee et al. (2015b), with fine tuned regularization and hard thresholding parameters. This approach only requires one round of communication, where each machine sends a p-dimensional vector.
However, Avg-Debias is computationally prohibitive because of the debiasing operation.
• EDSL: the proposed efficient distributed sparse learning approach, where the regularization level at each iteration is
fine tuned on a held out test data set.
C.1. Simulations
The full experimental results plotted in Figure 3 and Figure 4, with various settings of pn, p, m, sq, and condition numbers
1{κ. We have the following observations:

Efficient Distributed Learning with Sparsity
Table 2. List of real-world datasets used in the experiments.

Name
a9a
connect-4
dna
mitface
mnist 1 vs 2
mnist
mushrooms
protein
spambase
usps
w8a
year

#Instances
48,842
67,557
2,000
6,977
14,867
60,000
8,124
17,766
4,601
7,291
64,700
51,630

#Features
123
127
181
362
785
785
113
358
57
257
301
91

Task
Classification
Regression
Regression
Classification
Classification
Regression
Classification
Regression
Classification
Regression
Classification
Regression

• The Avg-Debias approach obtained much better estimation error compared to Local after one round of communication
and sometimes performed quite close to Centralize. However, in most cases, there is still a gap compared with
Centralize, especially when the problem is not well-conditioned or the number of machines m is large.
• When the problem is well conditioned (Σij  0.5|ij | case), Prox GD converges reasonably fast. However, it
becomes very slow when the condition number becomes bad (Σij  0.5|ij |{5 case). We expect to observe a similar phenomenon for other first-order distributed optimization algorithms, such as accelerated proximal gradient or
ADMM.
• As theory suggests, EDSL obtained a solution that is competitive with Avg-Debias after one round of communication.
The estimation error decreases to match performance of Centralize within few rounds of communications; typically
less than 5, even though the theory suggests EDSL will match the performance of centralize within Oplog mq rounds
of communication.
C.2. Real-world Data Evaluation
In real world data evaluation presented in Section 5.2, the datasets are publicly available from the LIBSVM website7 and
UCI Machine Learning Repository8 . The statistics of these datasets are summarized in Table 2, where some of the multiclass classification datasets are adopted under the regression setting with squared losses. The results are plotted in Figure
5 where for some datasets the performance of Avg-Debias is significantly worse than others (mostly because the debiasing
step fails), thus we omit these plots. The plots are shown in Figure 5 We have the following observations
• Since there is no well-specified model on these datasets, the curves behave quite differently on different data sets.
However, a large gap between the local and centralized procedure is consistent as the later uses 10 times more data.
• Avg-Debias often fails on these real datasets and performs much worse than in simulations. The main reason might
be that the assumptions, such as well-specified model or generalized coherence condition, fail, then Avg-Debias can
totally fail and produce solution even much worse than the local.
• Prox GD approach still converges slowly in most of the cases.
• The proposed EDSL is quite robust on real world data sets, and can output a solution which is highly competitive with
the centralized model within a few rounds of communications.
• There exits a slight “zig-zag” behavior for EDSL approach on some data sets. For example, on the mushrooms data
set, the predictive performance of EDSL is not stable.

7
8

https://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
http://archive.ics.uci.edu/ml/

Efficient Distributed Learning with Sparsity

m5

m  10

0.5

0.6

0.29

0.22

1

2

3

4

5

6

Local
Prox-GD

0.48

0.36

0

0.77

EDSL

Estimation Error

Estimation Error

Centralize
Avg-Debias

7

8

EDSL

0.49

0.24

0

1

Rounds of Communications

2

3

4

5

6

7

8

0.63

EDSL

0.39

0.31

0

1

2

3

4

5

6

7

8

Centralize
Avg-Debias

EDSL

0.42

0.31

0

1

Rounds of Communications

2

3

4

5

6

7

8

0.4

EDSL

0.22

0.16

0.1

0

1

2

3

4

5

6

7

8

Centralize
Avg-Debias

0.14

0

1

EDSL

2

3

4

5

6

7

8

0.31

0.23

0.15

0

1

2

3

4

5

6

7

Rounds of Communications

8

9

EDSL

2

3

4

5

6

7

8

9

Local
Prox-GD

Centralize
Avg-Debias

EDSL

0.12

0

1

2

3

4

5

6

7

8

9

Rounds of Communications

Centralize
Avg-Debias

 0.5|ij| .

EDSL

0.28

0.19

0

1

2

3

4

5

6

7

8

Local
Prox-GD

0.47

0.37

0.1

Centralize
Avg-Debias

0.21

0.03

9

Estimation Error

0.39

9

0.57

Local
Prox-GD

0.46

Estimation Error

Estimation Error

Centralize
Avg-Debias

1

Rounds of Communications

0.55

Local
Prox-GD

0

0.3

n  500, p  3000, s  10, X  N p0, Σq, Σij
0.47

8

0.28

0.39

0.21

Rounds of Communications

0.55

7

 0.5|ij|{5 .

EDSL

0.28

0.07

9

6

Rounds of Communications

Estimation Error

0.28

5

0.48

Local
Prox-GD

0.35

Estimation Error

Estimation Error

Centralize
Avg-Debias

4

0.46

0.1

9

0.42

Local
Prox-GD

0.64

Rounds of Communications

n  200, p  1000, s  10, X  N p0, Σq, Σij
0.34

3

Local
Prox-GD

0.82

0.53

0.2

9

2

 0.5|ij| .
Estimation Error

0.47

0.23

1

1.0

Local
Prox-GD

0.64

Estimation Error

Estimation Error

Centralize
Avg-Debias

0

Rounds of Communications

0.75

Local
Prox-GD

EDSL

0.21

0.07

9

Centralize
Avg-Debias

0.35

Rounds of Communications

n  200, p  1000, s  10, X  N p0, Σq, Σij
0.55

Local
Prox-GD

0.63

0.36

0.12

9

Centralize
Avg-Debias

Estimation Error

Local
Prox-GD

0.43

0.15

m  20

9

0.37

EDSL

0.27

0.17

0.07

0

1

Rounds of Communications

n  500, p  3000, s  10, X  N p0, Σq, Σij

Centralize
Avg-Debias

2

3

4

5

6

7

8

9

Rounds of Communications

 0.5|ij|{5 .

Figure 3. Comparison of various algorithms for distributed sparse regression, 1st and 3rd row: well-conditioned cases, 2nd and 4th row:
ill-conditioned cases.

Efficient Distributed Learning with Sparsity

m5

m  10

1.3

EDSL

0.98

0.82

0.66

0

1

2

3

4

5

6

7

8

Local
Prox-GD

1.28

Estimation Error

Estimation Error

Centralize
Avg-Debias

1.7

EDSL

0.84

0.62

0

1

Rounds of Communications

2

3

4

5

6

7

8

1.5

EDSL

0.93

0.74

0

1

2

3

4

5

6

7

8

EDSL

0.7

0

1

2

3

4

5

6

7

8

0.71

0.58

0.45

0

1

2

3

4

5

6

7

8

Centralize
Avg-Debias

EDSL

0.69

0.52

0

1

Rounds of Communications

1.3

2

3

4

5

6

7

8

0.74

0

1

2

3

4

5

6

7

Rounds of Communications

8

9

Centralize
Avg-Debias

1

2

3

4

5

6

7

8

9

Local
Prox-GD

Centralize
Avg-Debias

EDSL

0.71

0.48

0

1

2

3

4

5

6

7

8

9

 0.5|ij| .

EDSL

0.78

0.64

0

1

2

3

4

5

6

7

8

Local
Prox-GD

1.42

0.92

0.5

EDSL

Rounds of Communications

Estimation Error

0.88

0.6

Local
Prox-GD

1.06

1.02

Centralize
Avg-Debias

1.7

EDSL

Estimation Error

Estimation Error

Centralize
Avg-Debias

9

0.94

0.25

9

1.2

Local
Prox-GD

0

Rounds of Communications

n  1000, p  3000, s  10, X  N p0, Σq, Σij
1.16

8

0.66

1.17

0.86

0.35

9

7

 0.5|ij|{5 .
Estimation Error

0.84

6

1.4

Local
Prox-GD

1.03

Estimation Error

Estimation Error

EDSL

5

Rounds of Communications

1.2

Centralize
Avg-Debias

4

0.97

0.35

9

n  500, p  1000, s  10, X  N p0, Σq, Σij
Local
Prox-GD

3

1.28

Rounds of Communications

1.1

2

Local
Prox-GD

1.59

1.2

0.45

9

Centralize
Avg-Debias

0.95

Rounds of Communications

0.97

1

 0.5|ij| .
Estimation Error

1.12

0.55

0

1.9

Local
Prox-GD

1.45

Estimation Error

Estimation Error

Centralize
Avg-Debias

0.58

Rounds of Communications

1.7

Local
Prox-GD

EDSL

0.86

0.3

9

Centralize
Avg-Debias

1.14

Rounds of Communications

n  500, p  1000, s  10, X  N p0, Σq, Σij
1.31

Local
Prox-GD

1.42

1.06

0.4

9

Centralize
Avg-Debias

Estimation Error

Local
Prox-GD

1.14

0.5

m  20

1.5

9

Rounds of Communications

n  1000, p  3000, s  10, X  N p0, Σq, Σij

Centralize
Avg-Debias

EDSL

1.14
0.86

0.58

0.3

0

1

2

3

4

5

6

7

8

9

Rounds of Communications

 0.5|ij|{5 .

Figure 4. Comparison of various algorithms for distributed sparse classification (logistic regression), 1st and 3rd row: well-conditioned
cases, 2nd and 4th row: ill-conditioned cases.

Efficient Distributed Learning with Sparsity

2.7

16.0

Centralize
Avg-Debias

EDSL

15.9

15.8

15.7

0

1

2

3

4

5

6

7

8

2.56

2.28

2.14

0

1

Rounds of Communications

2

3

EDSL

7

8

0.039

0.036

0.033

0

1

2

3

4

5

6

7

8

9

0.014

Centralize
Avg-Debias

0.002

Centralize
Avg-Debias

0

1

2

3

4

5

6

7

8

4

5

6

7

8

9

0.1

0

1

EDSL

2

3

0.74

0.72

0

1

2

3

4

0.44

5

6

7

8

9

5

6

7

8

9

Centralize

EDSL

0.42

0.41

0.4

0.39

0

1

2

3

4

5

6

7

8

9

Rounds of Communications

usps

Centralize
Avg-Debias

EDSL

Normalized MSE

Normalized MSE

0.48

EDSL

0.942

Local
Prox-GD

0.47

0.52

9

0.43

Rounds of Communications

EDSL

4

Local
Prox-GD

0.44

0.48

0.56

Centralize
Avg-Debias

protein

Centralize
Avg-Debias

8

spambase

Centralize
Avg-Debias

connect4
Local
Prox-GD

7

Rounds of Communications

0.76

Rounds of Communications

0.6

6

0.108

9

0.78

0.7

5

0.116

Normalized MSE

Normalized MSE
3

Local
Prox-GD

0.8

0.6795

4

0.45

EDSL

0.682

3

0.124

mushrooms

0.6845

2

2

Local
Prox-GD

0.132

Rounds of Communications

0.687

1

1

EDSL

0.82

0

0

mnist 1 vs 2

0.006

0.692

Local
Prox-GD

1.28

Rounds of Communications

0.01

mitface

0.6895

1.46

1.1

9

EDSL

0.14

Local
Prox-GD

0.018

Classification Error

Classification Error

Centralize
Avg-Debias

Rounds of Communications

Normalized MSE

6

0.022

Local
Prox-GD

0.042

Normalized MSE

5

Centralize
Avg-Debias

1.64

w8a

0.045

0.677

4

Local
Prox-GD

1.82

Rounds of Communications

a9a

0.03

EDSL

2.42

2.0

9

Centralize
Avg-Debias

Classification Error

15.6

2.0

Local
Prox-GD

Classification Error (%)

Local
Prox-GD

Classification Error (%)

Classification Error (%)

16.1

0.46
0.45
0.44
0.43
0.42

Local
Prox-GD

0.941

Centralize

EDSL

0.94

0.939

0.938

0.41
0.4

0

1

2

3

4

5

6

7

Rounds of Communications

dna

8

9

0.4

0

1

2

3

4

5

6

7

Rounds of Communications

mnist

8

9

0.937

0

1

2

3

4

5

6

7

8

9

Rounds of Communications

year

Figure 5. Comparison of various approaches for distributed sparse regression and classification on real world datasets. (Avg-Debias is
omitted when it is significantly worse than others.)

