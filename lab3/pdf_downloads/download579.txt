Selective Inference for Sparse High-Order Interaction Models

A. Proofs

In the following, let us denote (S, z) by S for shorthand.
The remaining is to show that truncation points in Eqs.(13)
are equivalent to

A.1. Proof of Lamma 1
Proof. In the following, we abbreviate j in Lemma 1 for
the simplicity of the notation unless there is no confusion,
and prove the lemma in slightly general case of V[y] = Î£.
To prove the lemma, we first state the polyhedral lemma in
Lee et al. (2016) as follows:
Lemma 7 (Polyhedral Lemma; (Lee et al., 2016)). Suppose y âˆ¼ N(Âµ, Î£). Let c = Î£Î·(Î· > Î£Î·)âˆ’1 for any
Î· âˆˆ Rn , and let z = (In âˆ’ cÎ· > )y. Then we have
n

Pol(S) = {y âˆˆ R | Ay â‰¤ b}



 L(S, z) â‰¤ Î· > y â‰¤ U (S, z),
= y âˆˆ Rn 
,
N (S, z) â‰¥ 0
where

L(S) = Î· > y + Î¸L Î· > Î£Î·

(15a)

where Î¸L = min Î¸ s.t. y + Î¸Î£Î· âˆˆ Pol(S)
Î¸âˆˆR

and
U (S) = Î· > y + Î¸U Î· > Î£Î·

(15b)

where Î¸U = max Î¸ s.t. y + Î¸Î£Î· âˆˆ Pol(S),
Î¸âˆˆR

respectively. Simple calculation shows that, for any Î¸ âˆˆ R,
we have
y + Î¸Î£Î· âˆˆ Pol(S)
â‡” A(y + Î¸Î£Î·) â‰¤ b

bj âˆ’ (Az)j
,
(Ac)j
j:(Ac)j <0
bj âˆ’ (Az)j
U (S, z) = min
(Ac)j
j:(Ac)j >0
L(S, z) =

max

(13a)
(13b)

and N (S, z) = maxj:(Ac)j =0 bj âˆ’ (Az)j . In addition,
(L(S, z), U (S, z), N (S, z)) is independent of Î· > y.
The polyhedral lemma allows us to construct a pivotal
quantity as a truncated normal distribution, that is, for any
z, we have
[L(S,z),U (S,z)]

[F0,Î·> Î£Î·

â‡” Î¸ Â· AÎ£Î· â‰¤ b âˆ’ Ay.
ï£±
ï£² Î¸ â‰¤ (b âˆ’ Ay)j /(AÎ£Î·)j , (AÎ£Î·)j > 0
Î¸ â‰¥ (b âˆ’ Ay)j /(AÎ£Î·)j , (AÎ£Î·)j < 0 .
â‡”
ï£³
0 â‰¤ (b âˆ’ Ay)j ,
(AÎ£Î·)j = 0
On the other hand, by the definition of c and z in Lemma
7, it is easy to see that
L(S) = Î· > y + Î· > Î£Î·

max

j:(AÎ£Î·)j <0

>

[Î· y | y âˆˆ Pol(S), z = z0 ]
d

âˆ¼ TN(Î· > Âµ, Î· > Î£Î·, L(S, z0 ), U (S, z0 )),
d

where = denotes the equality of random variables in distribution. Therefore, probability integral transformation implies
[L(S,z),U (S,z)]

[FÎ·> Âµ,Î·> Î£Î·

(Î· > y) | y âˆˆ Pol(S), z = z0 ]

has a uniform distribution Unif(0, 1) for any z0 . By integrating out z0 , the pivotal quantity Eq.(14) holds. In addition, an lower Î±-percentile of the distribution can be obtained as
[L(S,z),U (S,z)] âˆ’1

qÎ± = (FÎ·> Âµ,Î·> Î£Î·

)

(Î±).

(b âˆ’ Ay)j
â‰¤Î¸
(AÎ£Î·)j

and thus the minimum possible feasible Î¸ would be
Î¸L = min{Î¸ âˆˆ R | y + Î¸Î£Î· âˆˆ Pol(S)}
=

= [Î· > y | L(S, z0 ) â‰¤ Î· > y â‰¤ U (S, z0 )]

(b âˆ’ Ay)j
(AÎ£Î·)j

Therefore, for each j such that (AÎ£Î·)j < 0, we have

(Î· > y)|y âˆˆ Pol(S)] âˆ¼ Unif(0, 1), (14)

where Unif(0, 1) denotes the standard (continuous) uniform distribution. In fact, by letting z0 be an arbitrary realization of z, one can see that

max

j:(AÎ£Î·)j <0

(b âˆ’ Ay)j
.
j:(AÎ£Î·)j <0 (AÎ£Î·)j
max

Similarly, we see that the equivalency of U (S).
To complete the proof, let us consider a Gaussian random
variable y with mean XÎ² âˆ— and covariance matrix Ïƒ 2 In
with some constant Ïƒ 2 . We can choose Î· = (XS+ )> ej for
âˆ—
testing the null hypothesis H0,j : Î²S,j
= 0 for each j âˆˆ S,
>
since Î· y reduces to the j-th element of an ordinary least
square estimator for the selected model, and in this case,
Î· > Î£Î· reduces to
ÏƒS2 = Ïƒ 2 kÎ·k2 = Ïƒ 2 (XS> XS )âˆ’1
jj .
Then the critical values are computed as
[L(S),U (S)] âˆ’1

`SÎ±/2 = qÎ±/2 = (F0,Ïƒ2

S

)

(Î±/2)

Selective Inference for Sparse High-Order Interaction Models

A.3. Proof of Lemma 4

and
[L(S),U (S)] âˆ’1

uSÎ±/2 = q1âˆ’Î±/2 = (F0,Ïƒ2

)

Proof. In MS, from Eq.(9), the constraint y +Î¸Î· âˆˆ Pol(S)
is written as

(1 âˆ’ Î±/2),

S

respectively. From the above argument, there are no matter
to compute the truncation points in Eqs.(15) based on the
observations. In this case, Eqs.(15) can be written as
L(S) = Î· > y + Î¸L Ïƒ 2 (XS> XS )âˆ’1
jj

(âˆ’sj xÂ·j âˆ’ xj 0 )> (y + Î¸Î·) â‰¤ 0
â‡”

and

where Î¸L = min Î¸ s.t. y + Î¸Ïƒ 2 (XS+ )> ej âˆˆ Pol(S)
Î¸âˆˆR

âˆ’(sj xÂ·j + xÂ·j 0 )> y
â‰¤ Î¸ if (sj xÂ·j + xÂ·j 0 )> Î· > 0,
(sj xÂ·j + xÂ·j 0 )> Î·
(16a)
âˆ’(sj xÂ·j + xÂ·j 0 )> y
â‰¥ Î¸ if (sj xÂ·j + xÂ·j 0 )> Î· < 0.
(sj xÂ·j + xÂ·j 0 )> Î·
(16b)

(âˆ’sj xÂ·j + xj 0 )> (y + Î¸Î·) â‰¤ 0
and

â‡”

U (S) = Î· > y + Î¸U Ïƒ 2 (XS> XS )âˆ’1
jj
where Î¸U = max Î¸ s.t. y + Î¸Ïƒ 2 (XS+ )> ej âˆˆ Pol(S),
Î¸âˆˆR

and

â‡”

âˆ’sj x>
Â·j y

and

âˆ’sj x>
Â·j y

min{Î¸ âˆˆ Rn | y + Î¸(XS+ )> >ej âˆˆ Pol(S)}
n

= min{Ïƒ Î¸ âˆˆ R | y + Î¸Ïƒ

2

(XS+ )> ej

âˆˆ Pol(S)}

max{Î¸ âˆˆ Rn | y + Î¸(XS+ )> ej âˆˆ Pol(S)}
= max{Ïƒ 2 Î¸ âˆˆ Rn | y + Î¸Ïƒ 2 (XS+ )> ej âˆˆ Pol(S)}.

â‰¤ max

xijÌƒ yi +

â‰¤ max

ï£³

X

i:yi >0

(16f)

X

xijÌƒ yi , âˆ’

xijÌƒ yi

i:yi <0

xij yi , âˆ’

(b)

(c)

Similarly, the conditions in Eqs.(16b), (16d), and (16f) imply that
(b)

(c)

Î¸L = âˆ’ max{Î¸U , Î¸U , Î¸U }.

A.4. Proof of Lemma 5

xijÌƒ yi |

Proof. First, note that 0 â‰¤ xijÌƒ 0 â‰¤ xij 0 â‰¤ 1 for any
(j, j 0 , jÌƒ 0 ) âˆˆ S Ã— SÌ„ Ã— Desj (j 0 ). We first prove Eq.(12a).

i:yi <0

ï£³
i:yi >0
ï£±
ï£² X

â‰¥ Î¸ if sj x>
Â·j Î· < 0

(a)

Proof. Since xij âˆˆ [0, 1], for any pair (j, jÌƒ) such that jÌƒ âˆˆ
Des(j), xj â‰¥ xjÌƒ holds. Then,

ï£±
ï£² X

(16e)

(a)

A.2. Proof of Lamma 3

i:yi >0

sj x>
Â·j Î·

â‰¤ Î¸ if sj x>
Â·j Î· > 0

Î¸L = âˆ’ min{Î¸L , Î¸L , Î¸L }.


X

sj x>
Â·j Î·

for all (j, j 0 ) âˆˆ S Ã— SÌ„. The conditions in Eqs.(16a), (16c),
and (16e) suggests that âˆ’Î¸L must be at least smaller than
(a)
(b)
(c)
Î¸L in Eq.(11a), Î¸L in Eq.(11c), and Î¸L in the second
last inequality in Eq.(11), respectively. Therefore, we have

and

|x>
y| = |
Â·jÌƒ

âˆ’(sj xÂ·j âˆ’ xÂ·j 0 )> y
â‰¥ Î¸ if (sj xÂ·j âˆ’ xÂ·j 0 )> Î· < 0.
(sj xÂ·j âˆ’ xÂ·j 0 )> Î·
(16d)
âˆ’ sj x>
Â·j (y + Î¸Î·) â‰¤ 0

respectively, but we can ignore the scaling factor Ïƒ 2 because

2

âˆ’(sj xÂ·j âˆ’ xÂ·j 0 )> y
â‰¤ Î¸ if (sj xÂ·j âˆ’ xÂ·j 0 )> Î· > 0
(sj xÂ·j âˆ’ xÂ·j 0 )> Î·
(16c)

X
i:yi <0

ï£¼
ï£½
ï£¾
ï£¼
ï£½

xij yi

(sj xÂ·j + xÂ·jÌƒ 0 )> y = sj x>
Â·j y +
â‰¥ sj x>
Â·j y +

.

ï£¾


â‰¥ sj x>
Â·j y +

X

xijÌƒ 0 yi +

i:yi >0

X

X
i:yi <0

xijÌƒ 0 yi .

i:yi <0

X
i:yi <0

(a)

xij 0 yi = LE ,

xijÌƒ 0 yi

Selective Inference for Sparse High-Order Interaction Models

which proves the first line. Next, we prove Eq.(12b).
X
X
(sj xÂ·j + xÂ·jÌƒ 0 )> y = sj x>
xijÌƒ 0 yi +
xijÌƒ 0 yi
Â·j y +
i:yi >0

â‰¤ sj x>
Â·j y +
â‰¤ sj x>
Â·j y +

X

i:yi <0

B. Selectivxe inference for OMP
Lemma 8. Let Î· := (X + )> ej . The solutions of the optimization problems in (7) are respectively written as

xijÌƒ 0 yi .

i:yi >0
(a)

X

xij 0 yi = UE ,

i:yi >0

which proves the second line. Eqs. (12c) to (12h) are
proved similarly.


(a)

(b)

(c)

(a)

(b)

(c)

Î¸L = âˆ’ min{Î¸L , Î¸L , Î¸L },
Î¸U = âˆ’ max{Î¸U , Î¸U , Î¸U },

A.5. Proof of Theorem 6
Proof. First, we prove (i). For any (j, j 0 , jÌƒ 0 ) âˆˆ S Ã— SÌ„ Ã—
Desj (j 0 ), by using Lemma 5 directly, a lower and an upper
>
bound of sj x>
Â·j y + xÂ·jÌƒ 0 y can be obtained as
(a)

(a)

>
LE â‰¤ sj x>
Â·j y + xÂ·jÌƒ 0 y â‰¤ UE

(17)

where

>
Similarly, a lower and an upper bound of sj x>
Â·j Î· + xÂ·jÌƒ 0 Î·
can be also obtained as
(a)

(a)

>
LD â‰¤ sj x>
Â·j Î· + xÂ·jÌƒ 0 Î· â‰¤ UD

(18)
(a)

Î¸L :=

From Eq.(18), we have
(a)

UD < 0 â‡’ (sj xÂ·j + xÂ·jÌƒ 0 )> Î· < 0
Î¸L :=

(sj xÂ·j + xÂ·j 0 )> y
(sj xÂ·j + xÂ·j 0 )> Î·

Î¸L :=

(a)

(a)

(a)

LD > 0, LE > 0 and

(b)

(c)

(a)

(a)

(a)
LE
(a)
UD

(a)

Î¸U :=

(a)

(a)

(a)

(a)

LE

(a)

LD

hâˆˆ[k], j âˆˆSÌ„h ,
(s(h) xÂ·(h) âˆ’xÂ·j 0 )> PSh Î·>0

min

hâˆˆ[k],
s(h) x>
Â·(h) PSh Î·>0

(b)

Î¸U :=

(a)

(c)

(a)

> Î¸Ì‚L ,

because LD > 0 implies UD > 0. Similarly, we can
prove (ii) â€“ (iv) by the same argument.


(s(h) xÂ·(h) âˆ’ xÂ·j 0 ) PSh y
,
(s(h) xÂ·(h) âˆ’ xÂ·j 0 )> PSh Î·
(19b)

s(h) x>
Â·(h) PSh y
,
>
s(h) xÂ·(h) PSh Î·

max
0

hâˆˆ[k], j âˆˆSÌ„h ,
(s(h) xÂ·(h) +xÂ·j 0 )> PSh Î·<0

(19c)

(s(h) xÂ·(h) + xÂ·j 0 )> PSh y
,
(s(h) xÂ·(h) + xÂ·j 0 )> PSh Î·
(19d)

Î¸U :=
(a)

>

min0

max
0

>

hâˆˆ[k], j âˆˆSÌ„h ,
(s(h) xÂ·(h) âˆ’xÂ·j 0 )> PSh Î·<0

> Î¸Ì‚L ,

or
LD > 0, LE < 0 and

hâˆˆ[k], j âˆˆSÌ„h ,
(s(h) xÂ·(h) +xÂ·j 0 )> PSh Î·>0

(s(h) xÂ·(h) + xÂ·j 0 )> PSh y
,
(s(h) xÂ·(h) + xÂ·j 0 )> PSh Î·
(19a)

for all (j, jÌƒ 0 ) âˆˆ S Ã— Desj (j 0 ). It means that the (j, jÌƒ 0 )-th
constraint does not affect the solution of the optimization
(a)
problem in Eq.(11a). Now, we consider the case of UD >
(a)
0. If LD > 0, the value

can be bounded below by LE /UD when LE > 0, and
(a)
(a)
(a)
LE /LD when LE < 0, while the value can take any
(a)
small values if LD < 0. As a result, for the current op(a)
timal solution Î¸Ì‚L , (j, j 0 )-th constraint does not affect the
solution of the optimization problem Eq.(11a), if

min0

max

hâˆˆ[k],
s(h) x>
Â·(h) PSh Î·<0

(s(h) xÂ·(h) âˆ’ xÂ·j 0 ) PSh y
,
(s(h) xÂ·(h) âˆ’ xÂ·j 0 )> PSh Î·

s(h) x>
Â·(h) PSh y
.
>
s(h) xÂ·(h) PSh Î·

(19e)
(19f)

Selective Inference for Sparse High-Order Interaction Models

Lemma 9. For any h âˆˆ [k] and (j 0 , jÌƒ 0 ) âˆˆ SÌ„h Ã—Des(h) (j 0 ),
X
(a)
LE := s(h) x>
xij 0 [PSh y]i
Â·(h) PSh y +

(iii) Furthermore, consider solving the optimization prob(a)
lem in Eq.(19d), and let Î¸Ì‚U be the current optimal solution. If

i:[PSh y]i <0

(a)

>

(a)

UE

i:[PSh y]i >0

(a)

(a)

(b)

(b)

xij 0 [PSh y]i

i:[PSh y]i <0

>

â‰¥ (s(h) xÂ·(h) âˆ’ xÂ·jÌƒ 0 ) PSh y,
X
:= s(h) x>
xij 0 [PSh Î·]i
Â·(h) Î· âˆ’
i:[PSh Î·]i >0

(b)

UD

â‰¤ (s(h) xÂ·(h) âˆ’ xÂ·jÌƒ 0 )> PSh Î·,
X
:= s(h) x>
xij 0 [PSh Î·]i
Â·(h) Î· âˆ’
i:[PSh Î·]i <0

â‰¥ (s(h) xÂ·(h) âˆ’ xÂ·jÌƒ 0 )> PSh Î·.
Theorem 10. (i) Consider solving the optimization prob(a)
lem in Eq.(19a), and let Î¸Ì‚L be the current optimal solu(a)
tion, i.e., we know that the optimal Î¸L is at least no greater
(a)
than Î¸Ì‚L . If
(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

(a)

{UD < 0} âˆª {LD > 0, LE < 0, LE /LD > Î¸Ì‚L }
âˆª {LD > 0, LE > 0, LE /UD > Î¸Ì‚L }
is true, then the jÌƒ 0 -th constraint in Eq. (10a) for any h âˆˆ [k]
and (j 0 , jÌƒ 0 ) âˆˆ SÌ„h Ã— Des(h) (j 0 ) does not affect the optimal
solution in Eq.(19a).
(ii) Next, consider solving the optimization problem in
(b)
Eq.(19b), and let Î¸Ì‚L be the current optimal solution. If
(b)

(a)

(a)

(a)

(a)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

is true, then the (jÌƒ 0 -th constraint in Eq. (10b) for any
h âˆˆ [k] and (j 0 , jÌƒ 0 ) âˆˆ SÌ„h Ã— Des(h) (j 0 ) does not affect
the optimal solution in Eq.(19e).

xij 0 [PSh y]i

i:[PSh y]i >0

(b)
LD

(a)

âˆª {UD < 0, LE > 0, LE /LD > Î¸Ì‚U }

â‰¥ (s(h) xÂ·(h) + xÂ·jÌƒ 0 )> PSh Î·,
X
:= s(h) x>
Â·(h) PSh y âˆ’
â‰¤ (s(h) xÂ·(h) âˆ’ xÂ·jÌƒ 0 ) PSh y,
X
:= s(h) x>
Â·(h) PSh y âˆ’

(a)

{LD > 0} âˆª {UD < 0, LE < 0, LE /UD > Î¸Ì‚U }

>

(b)
UE

(a)

(iv) Finally, consider solving the optimization problem in
(b)
Eq.(19e), and let Î¸Ì‚U be the current optimal solution. If

â‰¤ (s(h) xÂ·(h) + xÂ·jÌƒ 0 )> PSh Î·,
X
:= s(h) x>
xij 0 [PSh Î·]i
Â·(h) Î· +
i:[PSh Î·]i >0

LE

(a)

is true, then the jÌƒ 0 -th constraint in Eq. (10a) for any h âˆˆ [k]
and (j 0 , jÌƒ 0 ) âˆˆ SÌ„h Ã— Des(h) (j 0 ) does not affect the optimal
solution in Eq.(19d).

â‰¥ (s(h) xÂ·(h) + xÂ·jÌƒ 0 ) PSh y,
X
Î·
+
xij 0 [PSh Î·]i
:= s(h) x>
Â·(h)
i:[PSh Î·]i <0

UD

(a)

âˆª {UD < 0, LE > 0, LE /LD > Î¸Ì‚U }

xij 0 [PSh y]i

>

LD

(a)

{LD > 0} âˆª {UD < 0, LE < 0, LE /UD > Î¸Ì‚U }

â‰¤ (s(h) xÂ·(h) + xÂ·jÌƒ 0 ) PSh y,
X
:= s(h) x>
Â·(h) PSh y +

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

(b)

{UD < 0} âˆª {LD > 0, LE < 0, LE /LD < Î¸Ì‚L }
âˆª {LD > 0, LE > 0, LE /UD < Î¸Ì‚L }
is true, then the jÌƒ 0 -th constraint in Eq. (10b) for any h âˆˆ [k]
and (j 0 , jÌƒ 0 ) âˆˆ SÌ„h Ã— Des(h) (j 0 ) does not affect the optimal
solution in Eq.(19b).

