Appendix A: Proof of Theorem 4.1
Theorem 4.1. The counterfactual ETT is empirically estimable for arbitrary action-choice dimension (i.e., |X| = k for k ≥ 2)
when agents condition on their intent I = i and estimate the response Y to their final action choice X = a.
Proof. We start by writing the corresponding ETT expansion and note that,
E[YX=a |X = i]
X
=
E[YX=a |X = i, I = i0 ]P (I = i0 |X = i)

(1)
(2)

i0

=

X

E[YX=a |I = i0 ]P (I = i0 |X = i)

(3)

E[YX=a |Ix=a = i0 ]P (I = i0 |X = i)

(4)

E[Y |do(X = a), I = i0 ]P (I = i0 |X = i)

(5)

E[Y |do(X = a), I = i0 ]1(i0 = i)

(6)

i0

=

X
i0

=

X
i0

=

X
i0

= E[Y |do(X = a), I = i]

(7)

Eq. (2) expands the ETT using the law of total probability to sum over all intent conditions. Eq. (3) follows from the conditional
⊥ X|I that holds, allowing us to remove X = i. Eq. (4) follows because Ix = I given that (I ⊥
⊥ X)Gx ,
independence Yx ⊥
where Gx is the interventional submodel where all causal parents of X are severed (as represented by the counterfactual
antecedent notation). Eq. (5) is a notational re-arranging because all variables (Yx and Ix ) are in terms of the interventional
submodel Mx (and thus Gx ), licensing us to express the quantity using the do(x) notation. Eqs. (6,7) follow from the fact that,
observationally, an agent’s final arm choice will always coincide with their intent (i.e., P (i|x) = 1 ∀i = x, 0 otherwise), which
nullifies all summed expressions where the two differ.

Appendix B: Additional Simulations
RDC+

To illustrate that the success of T S
is not limited to the simulations of the main text, we performed experiments across a
wide swath of different payout parameterizations. In each simulation below, T S RDC+ experiences statistically significantly less
cumulative regret than its competitors, demonstrating its capacity to successfully navigate arbitrary parameter configurations.
Though it would be intractable to exhaustively test performance in all payout parameterizations, we have attempted to follow
a principled approach in which optimal arms are arranged in permutations of their relation to intent. The following is an
overview of our parameter choices:
1. Greedy Casino: the optimal arm choice is never the same as intent (see paper; not in appendix).
2. Paradoxical Switching: the optimal arm choice is always the same as intent, but paradoxically, the arm with the highest
experimental payout E[Yx1 ] = 0.5 is only the optimal choice in a single intent condition.
3. Generous Casino: the optimal arm choice is always the same as intent.
4. Odd Man Out: a single intent arm is suboptimal, the others are optimal.
5. Sometimes Switch: the optimal arm choice is the same as intent in two intent-conditions, but different in the other two.
Note: for all parameterizations that follow, P (D = 1) = P (D = 0) = 0.5, P (B = 1) = P (B = 0) = 0.5, and the agent’s
intent is decided by the structural equation: X ← fX (B, D) = B + 2 ∗ D.

Paradoxical Switching

Figure 1: Plots of TS variant performances in the Paradoxical Switching scenario.

(a)
E[y1 |X, B, D]
X=0
X=1
X=2
X=3

D=0
B=0 B=1
*0.90
0.20
0.30 *0.40
0.10
0.35
0.10
0.10

D=1
B=0 B=1
0.45
0.45
0.50
0.40
*0.60
0.35
0.30 *0.60

(b)
X=0
X=1
X=2
X=3

E[y1 |X]
0.90
0.40
0.60
0.60

E[y1 |do(X)]
0.50
0.40
0.35
0.20

Table 1: (a) Payout rates decided by reactive slot machines as a function of arm choice X, sobriety D, and machine conspicuousness B. Players’ natural arm choices under D, B are indicated by asterisks. (b) Payout rates according to the observational,
E(y1 |X), and experimental E(y1 |do(X)), distributions, where Y = y1 represents winning (shown in the table).
The Paradoxical Switching parameterization exemplifies a curious scenario wherein E[Yx1 ] = 0.5 > E[Yx0 ] ∀x0 6= x1 ,
but for which x1 is the optimal arm choice in only one intent condition (I = x1 ). Agents unempowered by RDC will face a
paradox in that the arm with the highest experimental payout is not always optimal. Again, T S RDC+ experienced significantly
less regret (M = 36.91) than its chief competitor, T S RDC , (M = 64.70), t(1998) = 22.43, p < .001.

The Generous Casino

Figure 2: Plots of TS variant performances in the Generous Casino scenario.

(a)
E[y1 |X, B, D]
X=0
X=1
X=2
X=3

D=0
B=0 B=1
*0.60
0.20
0.30 *0.60
0.50
0.30
0.20
0.50

D=1
B=0 B=1
0.50
0.30
0.20
0.50
*0.60
0.20
0.30 *0.60

(b)
X=0
X=1
X=2
X=3

E[y1 |X]
0.60
0.60
0.60
0.60

E[y1 |do(X)]
0.40
0.40
0.40
0.40

Table 2: (a) Payout rates decided by reactive slot machines as a function of arm choice X, sobriety D, and machine conspicuousness B. Players’ natural arm choices under D, B are indicated by asterisks. (b) Payout rates according to the observational,
E(y1 |X), and experimental E(y1 |do(X)), distributions, where Y = y1 represents winning (shown in the table).
The Generous Casino parameterization is the dual of the Greedy Casino in which the agent’s intent is always the optimal
choice. This parameterization illustrates that there is no inherent “distrust” of intent in the RDC+ algorithm, and furthers
its generalizability. In this simulation, T S RDC+ experienced significantly less regret (M = 33.99) than its chief competitor,
T S RDC , (M = 78.64), t(1998) = 25.35, p < .001.

Odd Man Out

Figure 3: Plots of TS variant performances in the Odd Man Out scenario.

(a)
E[y1 |X, B, D]
X=0
X=1
X=2
X=3

D=0
B=0 B=1
*0.40
0.40
0.20 *0.60
0.50
0.30
0.20
0.50

D=1
B=0 B=1
0.50
0.30
0.30
0.50
*0.60
0.20
0.30 *0.60

(b)
X=0
X=1
X=2
X=3

E[y1 |X]
0.40
0.60
0.60
0.60

E[y1 |do(X)]
0.40
0.40
0.40
0.40

Table 3: (a) Payout rates decided by reactive slot machines as a function of arm choice X, sobriety D, and machine conspicuousness B. Players’ natural arm choices under D, B are indicated by asterisks. (b) Payout rates according to the observational,
E(y1 |X), and experimental E(y1 |do(X)), distributions, where Y = y1 represents winning (shown in the table).
The Odd Man Out parameterization explores the scenario where a single intent arm is suboptimal. It illustrates the capacity
of RDC+ to account for asymmetrical exceptions in parameterizations. In this simulation, T S RDC+ experienced significantly
less regret (M = 52.39) than its chief competitor, T S RDC , (M = 78.72), t(1998) = 12.75, p < .001.

Sometimes Switch

Figure 4: Plots of TS variant performances in the Sometimes Switch scenario.

(a)
E[y1 |X, B, D]
X=0
X=1
X=2
X=3

D=0
B=0 B=1
*0.90
0.20
0.40 *0.30
0.25
0.40
0.10
0.10

D=1
B=0 B=1
0.45
0.45
0.50
0.40
*0.40
0.35
0.10 *0.50

(b)
X=0
X=1
X=2
X=3

E[y1 |X]
0.90
0.30
0.40
0.50

E[y1 |do(X)]
0.50
0.40
0.35
0.20

Table 4: (a) Payout rates decided by reactive slot machines as a function of arm choice X, sobriety D, and machine conspicuousness B. Players’ natural arm choices under D, B are indicated by asterisks. (b) Payout rates according to the observational,
E(y1 |X), and experimental E(y1 |do(X)), distributions, where Y = y1 represents winning (shown in the table).
The Sometimes Switch scenario hosts two intent arms that are optimal and two that are not. Furthermore, it disrupts any
symmetrical relationships between and within the observational and experimental payouts (as they are all different within
arms). In this simulation, T S RDC+ experienced significantly less regret (M = 65.41) than its chief competitor, T S RDC ,
(M = 76.64), t(1998) = 5.48, p < .001.

