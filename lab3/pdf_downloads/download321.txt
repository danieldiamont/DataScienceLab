Long Version of Proof of Theorem 3
Supplement to ICML submission “Learning in POMDPs with Monte Carlo Tree Search”
While RS-BA-POMCP is potentially more efficient, it is not directly whether it still converges to an -optimal
value function. Here we show the main steps in proving that it is sound. These main steps are similar to the proof
in POMCP. We point out however, that the technicalities of proving the components are far more involved.

Notation
We will use the following notation.
Action-observation histories.
• hd is an action-observation history at depth d of a simulation,
• hd = (a0 , z1 , . . . , ad−1 , zd ).
‘Full’ histories. In addition to actions and observations, full histories also include the states.
• H0 the (unknown) full history (of real experience) at the root of the simulation: i.e., if there have been k
steps of ‘real’ experience H0 = (s−k , a−k , s−k+1 , z−k−1 , . . . , a−1 , s0 , z0 )
• Hd is a full history (of simulated experience) at depth d in the lookahead tree: Hd = (H0 , a0 , s1 , z1 , a1 , s2 , z2 , . . . , ad−1 , sd , zd )
(Hd−1 , ad−1 , sd , zd ) = hs0:d , hd i.
(i)
• Hd the full history at depth d corresponding to simulation i.
• In our proof, we will also need to indicate if a particular full history Hd is consistent with a full history at the
root of simulation:
(
1 if Hd is consistent with the full history at the root H0 ,
Cons(H0 , Hd ) =
0 otherwise.
Dynamics Function. We fold transition and observations function into one:
• D denotes the dynamics model
s0 z
t zt
• Dsst−1
(s , z ) = D(st , zt |st−1 , at−1 ) = Pr(st , zt |st−1 , at−1 )
at−1 = Dsa = Dst−1
D,at−11 1 t t
E
s z
s|S| z |Z|
• Dsa denotes the vector: Dsa
, . . . , Dsa
Counts
0
• χssaz denotes how often hs0 , zi occurred after hs, ai
• χsa is

 the vector of counts
 for hs, ai.
• χ = χs1 a1 , . . . , χs|S| a|A| is the total collection of all such count vectors.
• χ(Hd ) denotes the vector of counts at simulated full history Hd .
• If χ0 is the count vector at the root of simulation, we have that χ(Hd ) = χ0 + ∆(Hd ), with ∆(Hd ) the vector
of counts of all (s, a, s0 , z) quadruples occurring in Hd .
Dirichlet distributions
• let x = hx1 . . . xK i ∈ ∆K and α = hα1 . . . αK i be a count vector
QK
i −1
• Dir(x|α) = Pr(x;
α) = B(α) i=1 xα
i
P
Γ( i αi )
• with B(α) = Q Γ(α
the Dirichlet normalization constant, with Γ the gamma function.
i)
i
• So, in translated in terms of dynamics function and counts, we have:
 0 χssa0 z
Q
sz
– for a particular s, a: Dir(Dsa |χsa ) = Pr(Dsa ; χsa ) = B(χsa ) hs0 ,zi∈S×Z Dsa
1

– we will also abuse notation and write Dir(D|χ) =

Q

hs,ai

Dir(Dsa |χsa ).

Var.
• ẋ denotes a root sampled quantity x.
• I{condition} is the indicator function which is 1 iff condition is true and 0 otherwise.

Definitions
Definition 1. The expected full-history expected transition BA-POMDP rollout distribution is the distribution
over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π. It is given
by
P π (Hd+1 ) = Dχ(Hd ) (sd+1 , zd+1 |as , sd )π(ad |hd )P π (Hd )
(1)
with P π (H0 ) = b0 (hs0 , χ0 i) the belief ‘now’ (at the root of the online planning).

Definition 2. The full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full
histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination
with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by
π
P̃K
(Hd ) ,

Kd
1 X
o
In
(i) ,
Kd i=1 Hd =Hd

where
• K is the number of simulations that comprise the empirical distribution,
• Kd is the number of simulations that reach depth d (not all simulations might be equally long),
(i)
• Hd is the history specified by the i-th particle at stage d.
Remark: throughout this proof we assume that there is only 1 initial count vector at the root. Or put better:
we assume that there is one unique H0 at which all simulations start. However, for ‘real’ steps t > 0 we could be
. In this case, root sampling from the
in different Htreal all corresponding to the same observed real history hreal
t
belief can be thought of root sampling the initial full history H0 ∼ b(Htreal ). As such, our proof shows convergence
in probability of
p
π
∀H0 ∀Hd P̃K
(Hd |H0 ) → P π (Hd |H0 ).
d
for each such sampled H0 . It is clear that that directly implies that
h
i p
π
π
∀Hd P̃K
(H
)
=
E
P̃
(H
|H
)
→ EH0 [P π (Hd |H0 )] = P π (Hd ).
d
H
d
0
K
0
d
d
In the below, we omit the explicit conditioning on H0 .

Proof of Main Theorem
The proof depends on a lemma that follows below.
Theorem 3. The full-history RS-BA-POMDP rollout distribution (Def. 2) converges in probability to full-history
BA-POMDP rollout distribution (Def. 1):
∀Hd

p

π
P̃K
(Hd ) → P π (Hd ).
d

(2)

Proof. For ease of notation we prove this for stage d + 1. Note that a history Hd+1 = (Hd , ad , sd+1 , zd+1 ), only
differs from Hd in that it has one extra transition for the (sd , ad , sd+1 , zd+1 ) quadruple, implying that χ(Hd+1 ) only
π
differs from χ(Hd ) in the counts χsd ad for sd ad . Therefore, the expression for P̃K
(Hd ) derived in Lemma 4 below
d
(cf. equation (20)) can be written in recursive form as

2

P̃ π (Hd+1 )

=

Cons(H0 , Hd )

d
Y

π(at |ht )

t=0

=

=

=

d−1
Y

Y
hs,ai

B(χsa (H0 ))
B(χsa (Hd+1 ))

Y B(χsa (H0 )) B(χsa (Hd ))
B(χsa (Hd )) B(χsa (Hd+1 ))
t=0
hs,ai



d−1
Y B(χsa (Hd ))
Y
Y B(χsa (H0 ))


Cons(H0 , Hd )
π(at |ht )π(ad |hd ) 
B(χsa (Hd ))
B(χsa (Hd+1 ))
t=0
hs,ai
hs,ai


d−1
Y
Y B(χsa (H0 ))
Cons(H0 , Hd )
 π(ad |hd ) B(χsd ad (Hd ))
π(at |ht )
B(χ
(H
))
B(χsd ad (Hd+1 ))
sa
d
t=0

Cons(H0 , Hd )

π(at |ht )π(ad |hd )

hs,ai

=

P̃ π (Hd )π(ad |hd )

B(χsd ad (Hd ))
B(χsd ad (Hd+1 ))

with base case P̃ π (H0 ) = 1, and
B(χsd ad (H0 )) B(χsd ad (Hd ))
B(χsd ad (H0 ))/B(χsd ad (Hd+1 ))
B(χsd ad (Hd ))
=
·
=
B(χsd ad (Hd+1 ))
B(χsd ad (Hd+1 )) B(χsd ad (H0 ))
B(χsd ad (H0 ))/B(χsd ad (Hd ))

(3)

the result of dividing out the contribution of the old counts for sd ad and multiplying in the new contribution, and
similar for the observations probabilities. Now, we investigate these terms more closely.
Again remember that the sole difference between Hd+1 = (HdP
, ad , sd+1 , zd+1 ) andHd is that it has one extra
0
transition for the (sd , ad , sd+1 , zd+1 ) quadruple. Let us write T = (s0 ,z) χssdzad (Hd ) for the total of the counts for
s
zd+1
sd , ad and N = χsd+1
(Hd ) for the number of counts for that such a transition was to (sd+1P
zd+1 ). Because Hd+1
d ad
0
only has 1 extra transition, we also know that for this history, the total counts is one higher: (s0 ,z) χssdzad (Hd+1 ) =
s
zd+1
T + 1 and since that transition was to (sd+1 zd+1 ) the counts χsdd+1
(Hd+1 ) = N + 1. Now let us expand the term
ad
from (3):
Q
0
Γ(T )/ s0 z Γ(χssdzad (Hd ))
B(χsd ad (Hd ))
Q
=
B(χsd ad (Hd+1 ))
Γ(T + 1)/ s0 z Γ(χss0dzad (Hd+1 ))
Q
s0 z
Γ(T )
s0 z Γ(χsd ad (Hd+1 ))
Q
=
s0
Γ(T + 1)
s0 z Γ(χsd ad (Hd ))
Q
0
sd+1 zd+1
(Hd+1 )) s0 z6=(sd+1 zd+1 ) Γ(χssdzad (Hd+1 ))
Γ(T ) Γ(χsd ad
Q
=
zd+1
Γ(T + 1) Γ(χssdd+1
(Hd )) s0 z6=(sd+1 zd+1 ) Γ(χss0dzad (Hd ))
ad
s

=

z

d+1
Γ(T ) Γ(N + 1)
Γ(T ) Γ(χsdd+1
(Hd+1 ))
ad
=
sd+1 zd+1
Γ(T + 1) Γ(χsd ad
Γ(T + 1) Γ(N )
(Hd ))

Now, the gamma function has the property that Γ(x + 1) = xΓ(x) [DeGroot, 2004], which means that we get
=

Γ(T ) N Γ(N )
N
= .
T Γ(T ) Γ(N )
T

Therefore we get

s

z

d+1
B(χsd ad (Hd ))
χsd+1
(Hd )
a
= P d d s0 z
B(χsd ad (Hd+1 ))
χ
0
(s ,z) sd ad (Hd )

and thus

s

χsd+1
a
P̃ π (Hd+1 ) = P̃ π (Hd )π(ad |hd ) P d d

zd+1

(Hd )
.
s0 z (H )
χ
d
(s0 ,z) sd ad

(4)

the r.h.s. of this equation is identical to (1) except for the difference in between P̃ π (Hd ) and P π (Hd ). This can be
resolved by forward induction with base step: P̃ π (H0 ) = b0 (hs0 , χ0 , ψ0 i) = P π (H0 ), and the induction step (show
P̃ π (Hd+1 ) = P π (Hd+1 ) given P̃ π (Hd ) = P π (Hd )) directly following from (1) and (4). Therefore we can conclude
that ∀d P̃ π (Hd ) = P π (Hd ).
3

Since Lemma 4 already established that ∀Hd
∀Hd

p

π
P̃K
(Hd ) → P̃ π (Hd ), we directly have
d
p

π
P̃K
(Hd ) → P π (Hd ),
d

thus proving the result.
The proof depends on the following lemma:
Lemma 4. The full-history RS-BA-POMDP rollout distribution converges in probability to the following quantity:

" d
#
Y
Y
B(χsa (H0 )) 
p
π
∀Hd P̃K
(Hd ) → b0 (s0 )
π(at−1 |ht−0 ) 
(5)
d
B(χsa (Hd )
t=1
hs,ai

with B(α) =

Γ(α1 +...·+αk )
Γ(α1 )·...·Γ(αk )

the normalization term of a Dirichlet distribution with parametric vector α.

Proof. Via the weak law of large numbers, we have that the empirical mean of a random variable converges in
probability to its expectation.


Kd
1 X
p
InH =H (i) o → E InH =H (i) o
d
d
Kd i=1
d
d

∀Hd

This expectation can be rewritten as follows

 X


(i)
n
o
E I H =H (i) =
P̃ π Hd InH
d

d

(i)

(i)
d =Hd

o

= P̃ π (Hd )

(6)

Hd

where P̃ π (Hd ) denotes the (true, non-empirical) probability that the RS-BA-POMDP rollout generates full history
Hd . This is an expectation over the root sampled model Ḋ:
ˆ


P̃ π Hd |Ḋ Dir(Ḋ|χ̇)dḊ
(7)
P̃ π (Hd ) =
"
#
ˆ
d
Y
=
Cons(H0 , Hd )
Ḋ(st zt |st−1 , at−1 )π(at−1 |ht−1 ) Dir(Ḋ|χ̇)dḊ
(8)
t=1

"
= Cons(H0 , Hd )

d
Y

π(at−1 |ht−1 )

# ˆ " d
Y

t=1

#
Ḋ(st zt |st−1 , at−1 ) Dir(Ḋ|χ̇)dḊ

!
(9)

t=1

Where Cons(H0 , Hd ) is a term that indicates whether (takes value 1 if) Hd is consistent with the full history at the
root H0 .1
1 An earlier version of this proof [anonymous] contained a term b (s ) instead of Cons(H , H ). This was incorrect since it failed to
0 0
0
d
recognize that this proof assumes H0 to be fixed. See also the remark on page 2.

4

Now we can exploit the fact that only the Dirichlet for the transitions specified by Hd matter.
#
ˆ "Y
d
Ḋ(st zt |st−1 , at−1 ) Dir(Ḋ|χ0 )dḊ

(10)

t=1

={split up the integral over one big vector into integrals over smaller vectors}

#
ˆ
ˆ "Y
d
Y
t ,zt

...
Ḋsst−1
Dir(Ḋsa |χsa (H0 )) dḊs1 a1 . . . dḊs|S| a|A|
,at−1
t=1

(11)

hs,ai
0

z
={reorder the transition probabilities: ∆sas
(Hd )is the number of occurences of (s, a, s0 , z)in Hd }
χ



0z
ˆ
ˆ Y Y 
∆sas
Y
(Hd )
0
χ
sz

... 
Ḋsa
Dir(Ḋsa |χsa (H0 )) dḊs1 a1 . . . dḊs|S| a|A|
hs,ai hs0 ,zi

ˆ

ˆ

=

...


Y Y 

hs,ai

ˆ

ˆ

=



...


ˆ

=



Y 

ˆ

=

...



s0 z
Ḋsa

∆χsas0 z (Hd )



B(χ̇sa ) 

Y 



0

sz
Ḋsa

Y 

B(χ̇sa )

0

sz
Ḋsa

0z
χsas
−1
0


 dḊs1 a1 . . . dḊs|S| a|A|

(13)

hs0 ,zi


0z
Y  0 χsas
−1
0
s
z
 dḊs1 a1 . . . dḊs|S| a|A|
 B(χ̇sa )
Ḋsa


0z
∆sas
(Hd )
χ

(14)




0z
Y  0 χsas
−1
0
sz

 dḊs1 a1 . . . dḊs|S| a|A|
Ḋsa

hs0 ,zi

(15)

hs0 ,zi


Y

B(χ̇sa )

Y 

hs0 ,zi

hs,ai

ˆ

Y

hs0 ,zi






hs,ai



Y

...

sz
Ḋsa

0z
∆sas
(Hd )
χ



hs,ai

ˆ

0

hs0 ,zi

Y

(12)

hs,ai

0

sz
Ḋsa

0z
0z
χsas
−1+∆sas
(Hd )
0
χ


 dḊs1 a1 . . . dḊs|S| a|A|

(16)

hs0 ,zi

hs,ai

Now we reverse the order of integration and multiplication, which is possible since the different s, a pairs over which
we integrate are disjoint.2 We obtain:
Y

=

B(χsa (H0 ))

ˆ Y 

0z
0z
χsas
+∆sas
(Hd )−1
0
χ
Ḋsa (s0 , z)
dḊsa

(17)

hs0 ,zi

hs,ai

={since we integrate over the entire vector Ḋsa , the integral equals 1/B(χsa (H0 ) + ∆sa
χ (Hd ))}
Y
1
B(χsa (H0 ))
B(χsa (H0 ) + ∆sa
χ (Hd ))

(18)

hs,ai

Y B(χsa (H0 ))
B(χsa (Hd ))

=

(19)

hs,ai

Therefore

#


Y B(χsa (H0 ))
,
P̃ π (Hd ) = Cons(H0 , Hd )
π(at |ht ) 
B(χsa (Hd ))
t=0
"d−1
Y

(20)

hs,ai

proving (5).
2 E.g,

consider two sets A1 =
X

2
X Y

a1 ∈A1 a2 ∈A2 i=1

n

ai =

a1 , a1

o

X

X

(1)

(2)

and A2 =

n
o
(1) (2) (3)
a2 , a2 , a2 . Equation (16) is of the same form as
(1) (1)

a1 a2 = a1 a2

(1) (2)

+ a1 a2

(1) (3)

+ a1 a2

(2) (1)

+ a1 a2

(2) (2)

+ a1 a2

(2) (3)

+ a1 a2

a1 ∈A1 a2 ∈A2




 


(1)
(2)
(3)
(2)
(1)
(2)
(3)
(1)
(2)
(1)
(2)
(3)
a2 + a2 + a2
+ a1
a2 + a2 + a2
= a1 + a1
a2 + a2 + a2


2
X
X
Y
X
=
a1  
a2  =
ai
(1)

= a1


a1 ∈A1

a2 ∈A2

i=1 ai ∈Ai

5

References
Morris H. DeGroot. Optimal Statistical Decisions. Wiley-Interscience, April 2004.

6

