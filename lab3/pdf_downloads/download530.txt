Failures of Gradient-Based Deep Learning

Supplemental material

A. Reduced Noise through Decomposition - Experiment
A.1. Experiment
For this experiment, consider the problem of training a predictor, which given a “positive media reference” x to a certain
stock option, will distribute our assets between the k = 500 stocks in the S&P500 index in some manner. One can, again,
come up with two rather different strategies for solving the problem.
• An end-to-end approach: train a deep network Nw that given x outputs a distribution over the k stocks. The objective
for training is maximizing the gain obtained by allocating our money according to this distribution.
• A decomposition approach: train a deep network Nw that given x outputs a single stock, y 2 [k], whose future gains
are the most positively correlated to x. Of course, we may need to gather extra labeling for training Nw based on this
criterion.
We make the (non-realistic) assumption that every instance of media reference is strongly and positively correlated to a
single stock y 2 [k], and it has no correlation with future performance of other stocks. This obviously makes our problem
rather toyish; the stock exchange and media worlds have highly complicated correlations. However, it indeed arises from,
and is motivated by, practical problems.
To examine the problem in a simple and theoretically clean manner, we design a synthetic experiment defined by the
k
following optimization problem: Let X ⇥ Z ⇢ Rd ⇥ {±1} be the sample space, and let y : X ! [k] be some labelling
k 1
function. We would like to learn a mapping Nw : X ! S
, with the objective being:
min L(w) :=
w

E

x,z⇠X⇥Z

⇥

⇤
z> Nw (x) .

To connect this to our story, Nw (x) is our asset distribution, z indicates the future performance of the stocks, and thus,
we are seeking minimization of our expected future negative gains, or in other words, maximization of expected profit.
We further assume that given x, the coordinate zy(x) equals 1, and the rest of the coordinates are sampled i.i.d from the
uniform distribution over {±1}.
Whereas in Section 3.1’s experiment, the difference between the end-to-end and decomposition approaches could be summarized by a different loss function choice, in this experiment, the difference boils down to the different gradient estimators
we would use, where we are again taking as a given fact that exact gradient computations are expensive for large-scale
problems, implying the method of choice to be SGD. For the purpose of the experimental discussion, let us write the two
estimators explicitly as two unconnected update rules. We will later analyze their (equal) expectation.

For an end-to-end approach, we sample a pair (x, z), and use rw ( z> Nw (x)) as a gradient estimate. It is clear that this
is an unbiased estimator of the gradient.
For a decomposition approach, we sample a pair (x, z), completely ignore z, and instead, pay the extra costs and gather
the required labelling to get y(x). We will then use rw ( e>
y(x) Nw (x)) as a gradient estimate. It will be shown later that
this too is an unbiased estimator of the gradient.
Figure 7 clearly shows that optimizing using the end-to-end estimator is inferior to working with the decomposition one,
in terms of training time and final accuracy, to the extent that for large k, the end-to-end estimator cannot close the gap in
performance in reasonable time.
A.2. Analysis
We examine the experiment from a SNR perspective. First, let us show that indeed, both estimators are unbiased estimators
of the true gradient. As stated above, it is clear, by definition of L, that the end-to-end estimator is an unbiased estimator

Failures of Gradient-Based Deep Learning

k = 10

k = 1000

0

1

k = 2000

0

0

1

2,500

0

0

2,500

1

0

2,500

Figure 7. Decomposition vs. end-to-end Experiment: Loss as a function of the number of training iterations, for input dimension
d = 1000 and for various k values. The red and blue curves correspond to the losses of the end-to-end and decomposition estimators,
respectively.

of rw L(w). To observe this is also the case for the decomposition estimator, we write:
rw L(w) = rw E [ z> Nw (x)]
x,z

= E[ E [rw ( z> Nw (x))]]
x z|x

(1)

(2)

= E[ E [ z> rw (Nw (x))]] = E[ e>
y(x) rw (Nw (x))]
x z|x

x

where (1) follows from the chain rule, and (2) from the assumption on the distribution of z given x. It is now easy to see
that the decomposition estimator is indeed a (different) unbiased estimator of the gradient, hence the “signal” is the same.
Intuition says that when a choice between two unbiased estimators is presented, we should choose the one with the lower
variance. In our context, (Ghadimi & Lan, 2013) showed that when running SGD (even on non-convex objectives), arriving
at a point where krw L(w)k2  ✏ requires order of ⌫¯2 /✏2 iterations, where
⌫¯2 = max E krtw (x, q)k2
t

x,q

krw L(w(t) )k2 ,

wt is the weight vector at time t, q is sampled along with x (where it can be replaced by z or y(x), in our experiment), and
rtw is the unbiased estimator for the gradient. This serves as a motivation for analyzing the problem through this lens.
Motivated by (Ghadimi & Lan, 2013)’s result, and by our results regarding Section 3.1, we examine the quantity
Ex,q krtw (x, q)k2 , or “noise”, explicitly. For the end-to-end estimator, this quantity equals
Ek

x,z

z> rw Nw (x)k2 = E k
x,z

k
X
i=1

zi rw Nw (x)i k2

Denoting by Gi := rw Nw (x)i , we get:
=E E k
x z|x

k
X
i=1

zi Gi k2 = E
x

k
X
i=1

kGi k2

(3)

where the last equality follows from expanding the squared sum, and taking expectation over z, while noting that mixed
terms cancel out (from independence of z’s coordinates), and that z2i = 1 for all i.
As for the decomposition estimator, it is easy to see that
Ek
x

2
2
e>
y(x) rw Nw (x)k = E kGy(x) k .
x

(4)

Observe that in 3 we are summing up, per x, k summands, compared to the single element in 4. When randomly initializing
a network it is likely that the values of kGi k2 are similar, hence we obtain that at the beginning of training, the variance of
the end-to-end estimator is roughly k times larger than that of the decomposition estimator.

Failures of Gradient-Based Deep Learning

B. Proofs
B.1. Proof of Theorem 1
Proof
Given two square-integrable functions f, g on an Euclidean space Rn , let hf, giL2 = Ex [f (x)g(x)] and kf kL2 =
p
Ex [f 2 (x)] denote inner product and norm in the L2 space of square-integrable functions (with respect to the relevant
distribution). Also, define the vector-valued function

@
pw (x),
@w
and let g(x) = (g1 (x), g2 (x), . . . , gn (x)) for real-valued functions g1 , . . . , gn . Finally, let Eh denote an expectation with
respect to h chosen uniformly at random from H. Let |H| = d.
g(x) =

We begin by proving the result for the squared loss. To prove the bound, it is enough to show that Eh krFh (w) ak2 
for any vector a independent of h. In particular, let us choose a = Ex [pw (x)g(x)]. We thus bound the following:
E krFh (w)
h

E [pw (x)g(x)] k2 = E k E [(pw (x)
x

h

x

h

=E
h

x

n
X

h

hh, gj i2L2

j=1
n ✓
(⇤) X



=

j=1

E [pw (x)g(x)] k2

h(x)) g(x)]

= E k E [h(x)g(x)] k2 = E
=

1
kgj k2L2
|H|

◆

n
X
j=1

=

G2
|H|

x

n ⇣
X
j=1

E [h(x)gj (x)]
x

d

1 X
hhi , gj i2L2
|H| i=1

!

⌘2

n

1 X
E[g 2 (x)]
|H| j=1 x j

⇥
⇤
1
G(w)2
E kg(x)k2 
,
|H| x
|H|

where (⇤) follows from the functions in H being mutually orthogonal, and satisfying khkL2  1 for all h 2 H.

To handle a classification loss, note that by its definition and the fact that h(x) 2 { 1, +1},

@
rFh (w) = E r0 (h(x)pw (x)) ·
pw (x)
x
@w
✓ 0
◆
r (pw (x)) + r0 ( pw (x))
r0 (pw (x)) r0 ( pw (x))
@
=E
+ h(x) ·
·
pw (x)
x
2
2
@w
 0

✓
◆
r (pw (x)) + r0 ( pw (x)) @
r0 (pw (x)) r0 ( pw (x))
@
=E
·
pw (x) + E h(x) ·
·
pw (x) .
x
x
2
@w
2
@w
⇣ 0
⌘
0
@
Letting g(x) = r (pw (x)) 2r ( pw (x)) · @w
pw (x) (which satisfies Ex [kg(x)k2 ]  G2 since r is 1-Lipschitz) and a =
h 0
i
0
( pw (x))
@
Ex r (pw (x))+r
· @w
pw (x) (which does not depend on h), we get that
2
E krFh (w)
h

ak2 = E k E[h(x)g(x)]k2 .
h

x

Proceeding now exactly in the same manner as the squared loss case, the result follows.

B.2. Proof of Theorem 3
Proof We first state and prove two auxiliary lemmas.
Lemma 1 Let h1 , . . . , hn be real-valued functions on some Euclidean space, which belong to some weighted L2 space.
Suppose that khi kL2 = 1 and maxi6=j |hhi , hj iL2 |  c. Then for any function g on the same domain,
✓
◆
n
1X
1
hhi , gi2L2  kgk2L2
+c .
n i=1
n

Failures of Gradient-Based Deep Learning

Proof For simplicity, suppose first that the functions are defined over some finite domain equipped with a uniform distribution, so that h1 , . . . , hn and g can be thought of as finite-dimensional vectors, and the L2 inner product and norm reduce
to the standard inner product and norm in Euclidean space. Let H = (h1 , . . . , hn ) denote the matrix whose i-th column is
hi . Then
!
n
n
X
X
2
>
>
hhi , gi = g
hi hi g = g > HH > g  kgk2 kHH > k = kgk2 kH > Hk,
i=1

i=1

where k · k for a matrix denotes the spectral norm. Since H > H is simply the n ⇥ n matrix with entry hhi , hj i in location
i, j, we can write it as I + M , where I is the n ⇥ n identity matrix, and M is a matrix with 0 along the main diagonal, and
entries of absolute value at most c otherwise. Therefore, letting k · kF denote the Frobenius norm, we have that the above
is at most
kgk2 (kIk + kM k)  kgk2 (1 + kM kF ) = kgk2 (1 + cn) ,
from which the result follows.
Finally, it is easily verified that the same proof holds even when h1 , . . . , hn , g are functions over some Euclidean
space, belonging to some weighted L2 space. In that case, H is a bounded linear operator, and it holds that
kH ⇤ Hk = kHk2 = kH ⇤ k2 = kHH ⇤ k where H ⇤ is the Hermitian adjoint of H and the norm is the operator norm. The
rest of the proof is essentially identical.

Lemma 2 If w, v are two unit vectors in Rd , and x is a standard Gaussian random vector, then
⇥
⇤
E sign(w> x)sign(v> x)  |hw, vi|
x

Proof Note that w> x, v> x are jointly zero-mean Gaussian, each with variance 1 and with covariance E[w> xx> v] =
w> v. Therefore,
⇥
⇤
E sign(w> x)sign(v> x) = Pr(w> x
x

Pr(w> x
>

= 2 Pr(w x

0, v> x

0) + Pr(w> x  0, v> x  0)

0, v> x  0)
>

0, v x

0)

Pr(w> x  0, v> x
>

2 Pr(w x

0)

>

0, v x  0),

which by a standard fact on the quadrant probability of bivariate normal distributions, equals

2

✓

◆
✓
◆
1 sin 1 (w> v)
cos 1 (w> v)
1
1
+
2
= +
sin
4
2⇡
2⇡
2 ⇡
1
1⇣
⇡⌘
2 sin 1 (w> v)
= +
2 sin 1 (w> v)
=
.
2 ⇡
2
⇡

1

(w> v)

cos

1

(w> v)

The absolute value of the above can be easily verified to be upper bounded by |w> v|, from which the result follows.
With these lemmas at hand, we turn to prove our theorem. By a standard measure
p concentration argument, we can find
dk unit vectors u1 , u2 , . . . , udk in Rd such that their inner product is at most O( k log(d)/d) (where the O(·) notation
Qk
is w.r.t. d). This induces dk functions hu1 , hu2 , . . . , hudk where hu (x1 , . . . , xk ) = l=1 sign(u> xl ). Their L2 norm
(w.r.t. the distribution over xk1 = (x1 , . . . , xk )) is 1, as they take values in { 1, +1}. Moreover, since x1 , . . . , xk are i.i.d.

Failures of Gradient-Based Deep Learning

standard Gaussian, we have by Lemma 2 that for any i 6= j,
" k
#
k
Y
Y
>
>
hhui , huj iL2 = E
sign(ui xl )
sign(uj xl )
l=1

k
Y

=

l=1

l=1

⇥
⇤
>
E sign(u>
i xl )sign(uj xl )

⇥
⇤k
>
= E sign(u>
i xl )sign(uj xl )
!k
r
k log(d)
>
k
 |ui uj |  O
.
d
Using this and Lemma 1, we have that for any function g,
0
!k 1
!k
r
r
dk
X
1
1
k
log(d)
k
log(d)
2
2
2
A  kgkL · O
hhui , giL2  kgkL2 · @ k + O
.
2
dk i=1
d
d
d

Moreover, since this bound is derived based only on an inner product condition between u1 , . . . , udk , the same result would
hold for U u1 , . . . , U udk where U is an arbitrary orthogonal matrix, and in particular if we pick it uniformly at random:
2
3
!!
r
dk
X
1
1
k
log(d)
2
2
E4 k
hhU ui , giL2 5  kgkL2 ·
+O
.
U
d i=1
dk
d

Now,
⇥ note that
⇤ for any fixed i, U ui is uniformly distributed on the unit sphere, so the left hand side simply equals
Eu hhu , gi2L2 , and we get
!k
r
⇥
⇤
k
log(d)
E hhu , gi2L2  kgk2 · O
.
(5)
u
d
With this key inequality at hand, the proof is now very similar to the one of Theorem 1. Given the predictor pw (xk1 ),
@
where w 2 Rn , define the vector-valued function g(xk1 ) = @w
pw (xk1 ), and let g(xk1 ) = (g1 (xk1 ), g2 (xk1 ), . . . , gn (xk1 )) for
real-valued functions g1 , . . . , gn . To prove the bound, ⇥it is enough to⇤upper bound Eu krFu (w) ak2 for any vector a
independent of u. In particular, let us choose a = Exk1 pw (xk1 )g(xk1 ) . We thus bound the following:
E krFu (w)
u

⇥
⇤
⇥
E pw (xk1 )g(xk1 ) k2 = E k E pw (xk1 )
u

xk
1

hu (xk1 ) g(xk1 )

xk
1

⇤

n
X
⇥
⇤
= E k E hu (xk1 )g(xk1 ) k2 = E
u

=E
u

u

xk
1

n
X
j=1

hhu , gj i2L2 =

n
X
j=1

j=1

✓

⇥
⇤
E pw (xk1 )g(xk1 ) k2

xk
1

⇥
⇤
E hu (xk1 )gj (xk1 )

xk
1

◆2

Ehhu , gj i2L2
u

!k
!k
r
n
X
k log(d)
k log(d)
2 k

kgj k · O
=
E [gj (x1 )] · O
d
d
xk
j=1
j=1 1
!k
!k
r
r
k log(d)
k log(d)
k 2
2
= E kg(x1 )k · O
 G(w) · O
,
d
d
xk
1
(⇤)

n
X

2

r

where (⇤) follows from (5). By definition of Var(H, F, w), the result follows. Generalization for the classification loss is
obtained in the exact same way to the one used in the proof of Theorem 1.

Failures of Gradient-Based Deep Learning

C. Technical Lemmas
Lemma 3 Any parity function over d variables is realizable by a network with one fully connected layer of width d˜ >
with ReLU activations, and a fully connected output layer with linear activation and a single unit.

3d
2

⇤
Proof Let the weights entering each of the first 3d
2 hidden units be set to v , and the rest to 0. Further assume that for
1
i 2 [d/2], the biases of the first 3i + {1, 2, 3} units are set to (2i 2 ), 2i, (2i + 12 ) respectively, and that their
weights in the output layer are 1, 2, and 1. It is not hard to see that the weighted sum of those triads of neurons is 12 if
hx, v⇤ i = 2i, and 0 otherwise. Observe that there’s such a triad defined for each even number in the range [d]. Therefore,
the output of this net is 0 if y = 1, and 12 otherwise. It is easy to see that scaling of the output layer’s weights by 4, and
introduction of a 1 bias value to it, results in a perfect predictor.

D. Command Lines for Experiments
Our experiments are implemented in a simple manner in python. We use the tensorflow package for optimization.
To run experiment 2.1, use:
python ./parity.py d
where d is the desired dimension.
To run experiment A, use:
python ./dec_vs_e2e.py [e2e|dec] N k
where e2e|dec is the desired experiment, N is the desired input dimension, k is the dimension of the predicted distribution.

