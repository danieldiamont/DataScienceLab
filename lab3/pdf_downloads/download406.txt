Just Sort It! A Simple and Effective Approach to Active Preference Learning
Supplementary Material

Lucas Maystre 1 Matthias Grossglauser 1

The supplementary material consists of three parts. In Section A, we present formal proofs for the results given in
Section 3 of the main text. In Section B we show that in the
Poisson model, Ω(n) comparison outcomes are necessary to
discriminate between two neighboring items. In Section C,
we present figures that complement the ones presented in
the experimental evaluation (Section 4 of the main text).

A Proofs
Section A.1 contains the proofs of Lemmas 2 and 3. Section A.2 presents the proof for our result on the displacement
of the output of a single call to Quicksort (Theorem 1), and
Section A.3 that of our result on the displacement of the
Copeland aggregation of multiple outputs.
A.1

Lemmas 2 and 3

We start by briefly presenting a result from graph theory
that will be useful in the proof of Lemma 2. A tournament
is a directed graph obtained by assigning a direction to
every edge of a complete graph. The score sequence of a
tournament is defined as the nondecreasing sequence of the
vertices’ outdegrees. The following proposition is due to
Landau (1953).
Proposition 1. Let (s1 , . . . , sn ) with 0 ≤ s1 ≤ · · · ≤ sn
be the score sequence of a tournament on n vertices. Then,
k−1
n+k−2
≤ sk ≤
2
2

∀ k ∈ [n].

We use a tournament on n vertices to represent the outcome
of a comparison between each pair of items. In particular,
we represent the outcome i ≺ j by an edge (i, j). In this
case, the outdegree of a vertex i corresponds to the number
of items which “won” in a comparison against i. Note that
the comparison outcomes do not need to be transitive, i.e.,
the tournament can contain cycles.
1

School of Computer and Communication Sciences, EPFL,
Lausanne, Switzerland. Correspondence to: Lucas Maystre <lucas.maystre@epfl.ch>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

The proof of Lemma 2 is adapted from standard results on
Quicksort, see, e.g., Dubhashi & Panconesi (2009, Section
3.3.3). These results are based on the fact that it is likely
that the random choice of pivot leads to a well-balanced
partition into subsets L and R. In our setting, the comparison
outcomes do not need to be consistent with an ordering of
the items, therefore we cannot use the standard argument
based on the pivot’s rank. Instead, we use the tournament
representation of the comparison outcomes and analyze the
pivot’s out-degree (using Proposition 1) to ensure that the
partition is balanced often enough.
Proof of Lemma 2. We show that the maximum call depth
of Quicksort is at most ⌈48 log n⌉ with high probability. The
statement follows by noticing that at most n comparisons
are used at each level of the call tree.
By Lemma 1, Quicksort samples a comparison outcome
for each pair of items at most once. Therefore, we can represent these (a priori unobserved) pairwise outcomes as a
tournament T = ([n], A). At each step of the recursion, we
select a pivot p uniformly at random in the set V (line 3),
and compare it to the rest of the items in the set (line 5). Let
TV denote the subgraph of T induced by V . Given that the
comparison outcomes follow from the edges of the tournament, L is equal to the set of incoming neighbors of p in
TV . (Correspondingly, R is equal to the set of the outgoing
neighbors.) Hence, the outdegree of p in TV determines how
balanced the partition is. The probability that the outdegree
of p lies in the middle half of the score sequence is 1/2, and
if it does, Proposition 1 tells us that
7|V | − 5
|V | − 7
≤ outdeg(p) ≤
.
8
8
In this case, at the end of the partition |L| and |R| are of
size at most 7|V |/8, and in at most log8/7 (n) ≤ 8 log n
such partitions we get to a subset of size one and match the
terminating case. Even though we do not select the pivot
in the middle half every time, it is unlikely that more than
c · 8 log n recursions are needed (for some small constant c)
to select the pivot in the middle range at least 8 log n times.
Let zd i.i.d ∼ Bern(1/2) be the indicator variable for the
event “the pivot is selected in the middle half at level of

A Simple and Effective Approach to Active Preference Learning

recursion d”. Using a Chernoff bound, we have


⌈48 log n⌉
X
1
zd ≤ 8 log n ≤ 2 ,
P
n
d=1

i.e., the depth of a leaf in the call tree is at most ⌈48 log n⌉
with probability at least 1 − 1/n2 . As there are at most n
leaves in the tree, the maximum depth is bounded by the
same value with probability at least 1 − 1/n.
In order to prove Lemma 3, we introduce some additional
notation. For any σ ∈ Sn and V ⊆ [n], let σV : V →
{1, . . . , |V |} be the ordering induced by σ on V . We generalize the definition of displacement as
X
|σV (i) − τV (i)|.
∆V (σ, τ ) =
i∈V

.
For conciseness, we use the shorthand ∆V (σ) = ∆V (σ, id),
where id is the identity permutation.

Proof of Lemma 3. Denote by V the collection of working
sets that were used as input to one of the recursive calls to
Quicksort. For V ∈ V, let EV be the set of pairs sampled by
Quicksort to partition V and which results in S
an error. Note
that EV ∩ EV ′ = ∅ for V 6= V ′ , and that V EV = E.
We will show that for all V ∈ V,
X
∆V (σ) ≤ ∆L (σ) + ∆R (σ) + 2
|i − j|,
(1)
(i,j)∈EV

where L and R are the two sets obtained at the end of the
partition operation. The lemma follows by taking V = [n]
and recursively bounding ∆L (σ) and ∆R (σ).
Consider the partition operation on V , with pivot p, resulting
in partitions L and R. Let σ̃ be the ordering on V that
a) ranks L at the bottom, p in the middle and R at the top,
and b) matches the identity permutation on L and R, i.e.,
∆L (σ̃) = ∆R (σ̃) = 0. In a sense, σ̃ is the ordering that
would be obtained if there were no further errors in the
remaining recursive calls. Using the triangle inequality, we
have that
∆V (σ) ≤ ∆V (σ, σ̃) + ∆V (σ̃).

(2)

By definition of σ̃, we have that

t=0

1 2 3 4 5 6 7 8 9

∆V (σ̃)

0

0

0

0

0

0

0

0

0

t=1

1 2 4 6 5 3 7 8 9

∆V (σ̃)

0

t=2

1 4 6 8 5 2 3 7 9

∆V (σ̃)

0

t=3

1 4 6 8 9 5 2 3 7

∆V (σ̃)

0

0

2

2

1

3

3

2

4

4

0

0

4

3

4

1

0

4

5

0

1

5

0

0

1

→0

→6

→ 18

→ 25

Figure 4. Illustration of the decomposition of ∆V (σ̃) into contributions of individual errors over a sequence of steps. In this example,
V = {1, . . . , 9}, p = 5 and there are five errors. At step t = 1, we
process the errors (5, 3) and (5, 6); at step t = 2, we process the
errors (5, 2) and (5, 8), and finally, at step t = 3, we process the
error (5, 9). The shifts caused by an error are
P highlighted in red and
green. In this example, ∆V (σ̃) = 25 < 2 (i,j)∈EV |i−j| = 26.

loss of generality, we can assume that V consists of con.
secutive integers, and that κ = |EV− | ≤ |EV+ |. We proceed
as follows: starting from the ranking idV , we progressively
incorporate errors into the ranking, ending with σ̃ once all
errors have been treated. To understand the impact of each
error on ∆V (σ̃), we look at errors in the following specific
sequence.
1. At steps t = 1, . . . , κ, we consider the t-th “smallest”
errors in EV− and EV+ . That is, we process (p, i) ∈
EV− and (p, i′ ) ∈ EV+ such that |p − i| and |p − i′ |,
respectively, are smallest among errors not yet treated.
2. At steps t = κ+1, . . . , |EV+ |, we process the remaining
errors in EV+ , once again in increasing order of distance
to p.
Figure 4 illustrates the state of the ranking at different steps
on a concrete example. We start with the first case, i.e.,
t ≤ κ. The effect of the errors (p, i) and (p, i′ ) on ∆V (σ̃)
is as follows.
• All items j < i and j > i′ are not affected by the two
errors: their position remains the same.

(3)

• The position of the pivot p remains the same, as the
two errors balance out.

where the first equality follows from a), and the second
follows from b).

• Item i is shifted by |p − i| + 1 positions to the right,
just right of p. Similary, item i′ is shifted by |p − i′ | + 1
positions to the left, just left of p.

Finally, we bound ∆V (σ̃). Let EV− = {(p, i) ∈ EV : i <
p}, and similarly EV+ = {(p, i) ∈ EV : i > p}. Without

• The |p − i| − 1 items that are between p (excluded)
and i are shifted by 1 position to the left. Similarly, the

∆V (σ, σ̃) = ∆L (σ, σ̃) + ∆R (σ, σ̃)
= ∆L (σ) + ∆R (σ),

A Simple and Effective Approach to Active Preference Learning

|p − i′ | − 1 items that are between p and i′ are shifted
by 1 position to the right.
Hence, the two errors contribute 2(|p − i| + |p − i′ |) towards
∆V (σ̃). Now consider the second case, when t > κ. The
effect of an error (p, i) is as follows.
• All items j > i and all the items on the left of p are not
affected by the error: their position remains the same.
• The (at most) |p−i| items that are between p (included)
and i are shifted by 1 position to the right.
• Item i is shifted by at most |p − i| positions to the left,
just left of p.
As a result, the error contributes at most 2|p − i| to the
displacement. Adding up the contributions of all the errors,
it follows that
X
|i − j|.
(4)
∆V (σ̃) ≤ 2
(i,j)∈EV

Combining (3) and (4) using (2) we obtain (1), which concludes the proof.
A.2

Theorem 1

From now on, we focus on parameters drawn from a Poisson
process of rate λ, as described in (1) in the main text. We
consider a worst-case scenario and assume that Quicksort
samples a comparison outcome for every pair of items. Let
zij be the indicator random variable of the event “the comparison between i and j resulted in an error”. By Lemma 3,
we have
X
|i − j|zij
(5)
∆(σ) ≤ 2
i<j

In the following, we will bound some of the statistical properties of the random variables {zij }. We start with a lemma
that bounds their mean.
Lemma 4. For any 1 ≤ i < j ≤ n,

j−i
λ
E [zij ] ≤
.
λ+1
Proof. Let dij = θi − θj be the (random) distance between items i and j. This distance is a sum of k = j − i
independent exponential random variables, and therefore
dij ∼ Gamma(k, λ). The comparison outcome is generated as per the BT model; conditioned on the distance dij ,
the random variable zij is a Bernoulli trial with probability
[1 + exp(dij )]−1 . Therefore, we have that
k

λ
E [zij ] ≤ E [exp(−dij )] =
λ+1

Next, we bound their covariance. Note that the random variables {zij } are in general not unconditionally independent.
They become independent only when conditioned on θ.
Lemma 5. For any 1 ≤ i < j ≤ n and any 1 ≤ u < v ≤ n,
let A = {i .. j −1} and B = {u .. v−1}.

if A ∩ B = ∅,

0



j−i


λ

if A = B,
Cov [zij , zuv ] ≤
λ+1



j−i+v−u


λ+1


otherwise.

λ+2

Proof. If A and B are disjoint, the distances dij and duv are
independent random variables. Conditioned on the distances,
the comparison outcomes are independent Bernoulli trials,
and we conclude that zij and zuv are independent. In the two
remaining cases, we bound E [zij zuv ] ≥ Cov [zij , zuv ]. If
A = B, then zij = zuv and we have
 2
E [zij zuv ] = E zij
= E [zij ]
and we apply Lemma 4. Finally, if A and B are neither
equivalent nor disjoint, the two comparison outcomes are
independent Bernoulli trials conditioned on the distances
dij and duv , but the distances are not independent. Consider
the case where i < u < j < v. Even though dij and duv
are dependent, the distances diu , duj , djv are independent
Gamma random variables of rate λ and shape u − i, j − u
and v − j, respectively, and
E [zij zuv ] ≤ E [exp{−(diu + duj ) − (duj + djv )}]
u−i 
j−u 
v−j

λ
λ
λ
=
λ+1
λ+2
λ+1
j−i+v−u

λ+1
≤
λ+2
The other cases are treated analogously.
Lemmas 4 and 5 will be useful in proving the first part
of Theorem 1. For the second part, we need a result from
Ailon (2008), which characterizes the pairwise marginals
of the distribution over rankings induced by Quicksort with
comparisons sampled from a BT model.

Theorem 3 (Ailon, 2008, Theorem 4.1). Let σ be the output of Quicksort using comparison outcomes sampled from
BT(θ). Then, for any i, j ∈ [n],
P [σ(i) < σ(j) | θ] = p(i ≺ j | θ)
Note that the result is non-trivial as i and j might not have
been directly compared to each other: their relative position
might have been deduced by transitivity from other comparison outcomes. We are now ready to prove Theorem 1.

A Simple and Effective Approach to Active Preference Learning

Proof of Theorem 1. We begin with the first part of the theorem, which bounds the displacement ∆(σ). For clarity
of exposition, we use the notation zi→k instead of zij if
j = i + k. Using (5) and Lemma 4, we can bound the
expected displacement as
E [∆] ≤

n−1
n−i
XX

i=1 k=1
∞
X

≤n

2kE [zi→k ]

2k

k=1



λ
λ+1

k

= 2nλ(λ + 1).

A.3

In a similar way, using Lemma 5, we can bound the variance
of the displacement as
Var [∆] ≤

n−i
n−1
XX

Combining
(6) and (7), and using a union bound over the

n
pairs,
we
see that with probability 1 − 1/n there is no
k
pairs of items (i, j) separated by at least 3(λ + 1)e log n
position with i < j but σ(j) < σ(i). Finally, suppose that
there is an i such |σ(i) − i| = k. Without loss of generality,
we can assume that i < σ(i). This means that there are k
items larger than i that are on the left of i in σ. In particular,
there is an item j > i such that |i − j| ≥ k and σ(j) < σ(i).
This concludes the proof.

4k 2 Var [zi→k ]

Theorem 2

In order to prove Theorem 2, we first need a basic result
on the order statistics of exponential random variables. Let
x1 , . . . , xn , be i.i.d. exponential random variables of rate λ.
Let x(1) , . . . , x(n) be their order statistics, i.e., the random
variables arranged in increasing order. Then,

i=1 k=1

+2

n−1
n−i
XX

i=1 k=1

∞
X
2

2k

i+k n−u
X
X

2ℓCov [zi→k , zu→ℓ ]

u=i+1 ℓ=1
k

4k

k=1
5

i
X
j=1

λ
λ+1
k=1


k X
ℓ
∞
∞
X
λ+1
λ+1
+ 2n
2ℓ
2k 2
·
λ+2
λ+2

≤n

x(i) =

ℓ=1

≤ 1500n(λ + 1).

Combining the bounds for the mean and the variance with
Chebyshev’s inequality, we have that


P ∆(σ) ≥ 50n(λ2 + 1) ≤ λ/n,

which concludes the proof of the first part of the claim.

The second part of the theorem bounds the maximum displacement for any single item. We start by showing that
with high probability, there is no pair of items separated by
at least O(λ log n) positions that is “flipped” in the output
of Quicksort. Let i and j be two items such that i < j and
let k = |i − j|. Then dij ∼ Gamma(k, λ), and using a
Chernoff bound we obtain

If k ≥ 3(λ + 1)e log n, we find that
(6)

Using the fact that the pairwise marginals of Quicksort
match the pairwise comparison outcome probabilities (Theorem 3), we find
P [σ(j) < σ(i)] = p(j ≺ i)
≤ exp(−3 log n) = n−3 .

(8)

where y1 , . . . , yn are i.i.d. exponential random variables of
rate λ (see, e.g., Arnold et al., 2008, Section 4.6).

Proof of Theorem 2. We consider the order statistics of the
n − 1 i.i.d. exponential random variables x1 , . . . , xn−1
which define the distances between neighboring items. Let
n̂ = ⌈n/ log2 n⌉, and denote by B ⊂ [n] the set of items
at both ends of x(1) , . . . , x(n̂−1) . These “bad” items are
close to their nearest neighbor, and we simply invoke Theorem 1 to claim that each of these items is shifted by at most
O(λ log n) positions with high probability. Consider now
the “good” items, i.e., those in G = [n] \ B. Using (8) and
for n large enough,


P x(n̂) ≤ 1/(eλ log2 n)


n̂
X
yj /n ≤ 1/(eλ log2 n)
≤ P
j=1

≤ exp(−n̂/e) ≤ 1/n.

P [dij ≤ k/(eλ)] ≤ exp(−k/e).

P [dij ≤ k/(eλ)] ≤ P [dij ≤ 3 log n] ≤ n−3 .

1
yj ,
n−j+1

(7)

The second-to-last inequality follows from a Chernoff bound
similar to that used in the proof of Theorem 1. Therefore,
with high probability all items in G are at distance larger
than c/(λ log2 n) from their nearest neighbor.
We will now show that after m = O(λ2 log5 n) runs of
Quicksort, σ̂(i) = i with high probability for all i ∈ G.
Let i ∈ G, j ∈ [n] be a pair of items, and without loss
of generality assume that i < j. Let tk be the indicator
random variable for the event “σ(i) < σ(j) in the k-th
run of Quicksort”, and let p = P [tk = 1]. Then, using

A Simple and Effective Approach to Active Preference Learning

Theorem 3,
1
1
1 − exp(−dij )
= p(i ≺ j) − =
2
2
2[1 + exp(−dij )]
1 − exp[−1/(eλ log2 n)]
≥
4
1
≥
8eλ log2 n
with high probability. In the last inequality, we used the
fact that 1 − e−x ≥ x/2 for x ∈ [0, 1]. The random variables t1 , . . . , tm are independent Bernoulli trials, and using
a Chernoff bound we obtain
"m
#
X
tk ≤ n/2
P [σ̂(j) < σ̂(i)] = P
k=1




m
≤ exp[−2m(p − 1/2) ] ≤ exp −
.
32e2 λ2 log4 n
2

With m = 96e2 λ2 log5 n, we have P [σ̂(j) < σ̂(i)] ≤ n−3 ,
and using a union bound we see that with probability 1 −
1/n we have σ̂(i) = i for all i ∈ G. Therefore, the total
displacement is
∆(σ̂) =

X

×106

4.0
3.5
Displacement

p−

4.5

3.0

Quicksort
uncertainty
random
Mergeort

2.5
2.0
1.5
1.0
0.5
0.0
0.1

0.2

0.4

0.5

0.6

0.7

0.8

Number of comparisons

0.9 1.0
×106

Figure 5. Results on the GIFGIF dataset. The experiment is repeated 10 times, and we report the mean and the standard deviation.
The variant of uncertainty sampling performs extremely poorly.

the correct decision is
"m
# m/2  
X
X m
m
P
zi ≤ m/2 ≤
P [zi = 0]
k
i=1
k=1


m/2  
X m
m
· 2−m
k
2λn − 1
k=1


1
m
= exp
.
2
2λn − 1

≤ exp

|σ̂(i) − i| ≤ |B| · 3(λ + 1)e log n

i∈B

= O(λn/ log n).

0.3



This concludes the proof.

Therefore, if m = o(λn) the probability of making a mistake is bounded from below by a positive constant.

B Discriminating the Closest Items

C Additional Figures

The distance between the two closest items is dmin =
mini |θi+i − θi| = mini xi , i.e., the minimum of n − 1
independent exponential random variables of rate λ. Therefore, dmin ∼ Exp((n−1)λ), and for n ≥ 2 with probability
at least 1 − e−1/2 ≈ 0.39 we have dmin ≤ (λn)−1 . Suppose that we compare the two closest items m times, and
let zi be the indicator random variable for the event “the
outcome of the i-th comparison is incorrect”. Assuming that
dmin ≤ (λn)−1 and that λn ≥ 1/2,

In this section, we present a few additional figures that
complement the ones presented in Section 4 of the main
text.

1
1
≤
1 + exp[−1/(λn)]
2 − 1/(λn)


1
1
= · 1+
2
2λn − 1


1
1
,
≤ exp
2
2λn − 1

P [zi = 0] ≤

where we used the inequality ex ≥ 1+x twice. Given the m
comparison outcomes, we use a majority vote to decide the
relative order of the two items. The probability of making

Figure 5 presents the results on the GIFGIF dataset including
a variant of uncertainty sampling. This variant samples, at
each iteration, n − 1 comparisons consisting of adjacent
pairs in the ranking θ̂. This strategy performs surprisingly
poorly.
Figure 6 presents results on synthetic datasets with n = 200
and λ ∈ {1, 2, 5, 10}. For the reader’s convenience, we
plot every graph on both a linear and a logarithmic scale.
Unsurprisingly, the gains of adaptive sampling are greater
when the noise is smaller.

References
Ailon, N. Reconciling Real Scores with Binary Comparisons: A Unified Logistic Model for Ranking. In Advances in Neural Information Processing Systems 21,
Vancouver, BC, Canada, 2008.

A Simple and Effective Approach to Active Preference Learning

Arnold, B. C., Balakrishnan, N., and Nagaraja, H. N. A First
Course in Order Statistics. SIAM, 2008.
Dubhashi, D. P. and Panconesi, A. Concentration of Measure for the Analysis of Randomized Algorithms. Cambridge University Press, 2009.
Landau, H. G. On Dominance Relations and the Structure of Animal Societies: III The Condition for a Score
Structure. Bulletin of Mathematical Biophysics, 15(2):
143–148, 1953.

A Simple and Effective Approach to Active Preference Learning
104

8000
uncertainty
entropy
KL-div
Mergesort
Quicksort
random

λ =1

7000

Displacement

6000
5000
4000

λ = 1, log scale

103

3000
2000
102

1000
0

104

8000
uncertainty
entropy
KL-div
Mergesort
Quicksort
random

λ =2

7000

Displacement

6000
5000
4000

λ = 2, log scale

103

3000
2000
1000
102

0

104

8000
uncertainty
entropy
KL-div
Mergesort
Quicksort
random

λ =5

7000

Displacement

6000
5000
4000

λ = 5, log scale

103

3000
2000
1000
0

104

8000
uncertainty
entropy
KL-div
Mergesort
Quicksort
random

λ = 10

7000

Displacement

6000
5000
4000

λ = 10, log scale

3000
103

2000
1000
0
0

500

1000

1500

2000

2500

3000

Number of comparisons c

3500

4000

0

500

1000

1500

2000

2500

3000

3500

4000

Number of comparisons c

Figure 6. Results on synthetic datasets for n = 200 and increasing values of λ. Every experiment is repeated 10 times, and we report the
mean and the standard deviation.

