Fairness in Reinforcement Learning

A. Omitted Proofs
A.1. Omitted Proofs for Section 2
Proof of Lemma 1. Let µ̂⇡T denote the distribution of ⇡ on
states of M after following ⇡ for T steps starting from s.
Then we know
E

s⇠µ⇡

T
1 X ⇡
E
V (st )
T t=1 M

⇡
VM
(s)

=

n
X

(µ⇡ (si )

⇡
µ̂⇡T (si )) VM
(si )

|µ⇡ (si )

⇡
µ̂⇡T (si )| VM
(si )

i=1




n
X
i=1

✏

1

.

The last inequality is due to the following observations:
⇡
(i) VM
(si )  1 1 as rewards are in [0, 1] and (ii)
n
⌃i=1 |µ⇡ (si ) µ̂⇡T (si )|  ✏ since T is at least the ✏-mixing
time of ⇡.
A.2. Omitted Proofs for Section 3
We first state the following useful Lemma about M .
Lemma 11. Let M be the MDP in Definition 6. Then for
n i+1
⇤
any i 2 {1, . . . , n}, VM
(si ) < 1+2
2(1
) .
Proof.
⇤
VM
(si ) = discounted reward before reaching state n

+ discounted reward from staying at state n
"n i 1 #
n i+1
X t
<
+
2
1
t=1
 ✓
◆
n i
n i+1
1
1
=
+
2 1
1
1
n i
n i+1
1
=
+
2(1
)
1
n i
1+
(2
1)
=
2(1
)
1 + 2 n i+1
<
,
2(1
)
via two applications of the summation formula for geometric series.
Proof of Theorem 3. We prove Theorem 3 for the special
case of k = 2 first. Consider coupling the run of a fair
algorithm L on both M (0.5) and M (1). To achieve this,
we can fix the randomness of L up front, and use the same
randomness on both MDPs. The set of observations and

hence the actions taken on both MDPs are identical until
L reaches state sn . Until then, with probability at least
1
, L must play L and R with equal probability in order
to satisfy fairness (since, for M (0.5), the only fair policy
is to play both actions with equal probability at each time
step). We will upper-bound the optimality of uniform play
and lower-bound the number of rounds before which sn is
visited by uniformly random play.
Let f = d 1 1p
e and T = 2n 2f for n
100(f )2 .
3
First observe that the probability of reaching a fixed state si
for any i n f from a random walk of length T is upper
bounded by the probability that the random walk takes i
n f consecutive steps to the right in the first T steps.
This probability is at most p = 2n 2f ( 12 )n f = 2 f
for any fixed i. Since reaching any state i > i0 requires
reaching state i0 , the probability that the T step random
walk arrives in any state si for i
n f is also upper
bounded by p.
⇤
Next, we observe that VM
(si ) is a nondecreasing function
⇤
of i for both MDPs. Then the average VM
values of the visited states of any fair policy can be broken into two pieces:
the average conditioned on (the probability at least 1
event) that the algorithm plays uniformly at random before
reaching state sn and never reaching a state beyond sn f ,
and the average conditioned on (the probability at most
event) that the algorithm does not make uniformly random
choices or the uniform random walk of length T reaches a
state beyond sn f . So, we have that
T
1 X ⇤
E
V (st )  (1
T t=1 M

 (1

p

⇤
) VM
(sn

p

)

1+2
2(1

) + (p + )

f

1
1

f +1

)

+ (p + )

1
1

⇤
The first inequality follows from the fact that VM
(si ) 
1
for
all
i,
and
the
second
from
Lemma
11
along
with
1
⇤
VM
values being nondecreasing in i. Putting it all together,

⇤
Es⇠µ⇤ VM
(s)

1
1
=

1

p
1

T
1 X ⇤
E
V (st )
T t=1 M

1 + 2 f +1
1
(1 p
)
+ (p + )
2(1
)
1

1 + 2 f +1
1
.
2

So ✏-optimality requires
2✏
1

1

p
1



1

1+2 f
2

+1

.

(4)

.

Fairness in Reinforcement Learning

However, if ✏ <
2✏
1

<
<

1
8

we get

1

0.04
1

1

2
1

f

1/4



1

1

1+2⇥e
2
1+2 f
2

3

+1

,

where the third inequality follows when < 14 and >
1
1
2 . This means ✏ < 8 makes ✏-optimality impossible, as
desired.
Throughout we considered the special case of k = 2 and
proved a lower bound of ⌦(2n ) time steps for any fair algorithm satisfying the ✏-optimality condition. However, it
is easy to see that MDP M in Definition 6 can be easily
modified in a way that k 1 of the actions from state si
reach state s1 and only one action in each state si reaches
states smin{i+1,n} . Hence, a lower bound of ⌦(k n ) time
steps can be similarly proved.

arrives in any state si for i
bounded by p.

(c + 1)n/2 is also upper

⇤
Next, we observe that VM
(si ) is a nondecreasing function
⇤
of i, for both MDPs. Then the average VM
values of the visited states of any fair policy can be broken into two pieces:
the average conditioned on the 1
fairness and never
reaching a state beyond s(c+1)n/2 , and the average when
fairness might be violated or the uniform random walk of
length T reaches a state beyond s(c+1)n/2 . So, we have
that
T
1 X ⇤
E
V (st )  (1
T t=1 M

⇤
) VM
(s(c+1)n/2 )

p

+ (p + )
 (1

2
T = ( 1+2↵
)n 2f for n
100(f )2 . By a similar process as in Theorem 3, the probability of reaching state si
for any i
n f from a random walk of length T is
2
bounded by p = ( 1+2↵
) f , and so the probability that the
T steps random walk arrives in any state si for i n f
is bounded by p. Carrying out the same process used to
prove Theorem 3 then once more implies that ✏-optimality
requires Equation 4 to hold when < 14 , ↵ < 14 and > 12 .
Hence, ✏ < 18 violates this condition as desired.

)

1
log(
)
be n = d 1 2↵ e.
any i, Q⇤M (si , R)

Then given the parameter ranges, for
Q⇤M (si , L) > ↵ in M(1). Therefore,
any approximate-action fair algorithm should play actions
R and L with equal probability.
Let T = 2cn = ⌦((21/(1 ) )c ). First observe that the
probability of reaching a fixed state si for any i
(c +
1)n/2 from a random walk of length T is upper bounded by
the probability that the random walk takes i (c + 1)n/2
consecutive steps to the right in the first T steps. This probability is at most p = 2cn 2 (c+1)n/2 = 2(c 1)n/2 for any
fixed i. Then the probability that the T steps random walk

1 + (2
1)
2(1
)

1

(1

c)n
2

.

1

⇤
The first inequality follows from the fact that VM
(si ) 
1
for
all
i,
and
the
second
from
(the
line
before
the
last
1
⇤
in) Lemma 11 along with VM
values being nondecreasing
in i. Putting it all together,
T
1 X ⇤
E
V (st )
T t=1 M

⇤
Es⇠µ⇤ VM
(s)

1

(1

1
(p + )

Finally, throughout we considered the special case of k =
2. The same trick as in the proof of Theorem 3 can be used
k
to prove the lower bound of ⌦(( 1+k↵
)n ) time steps for any
fair algorithm satisfying the ✏-optimality condition.
Proof of Theorem 5. We also prove Theorem 5 for the special case of k = 2 first, again considering the MDP in
Definition 6. We set the size of the state space in M to

1

p

= (p + )
Proof of Theorem 4. We mimic the argument used to prove
Theorem 3 with the difference that, until visiting sn , L
may not play R with probability more than 12 + ↵ (as
opposed to 12 in Theorem 3). Let f = d 1 1p
e and
3

1

=
=

1

p

1

"
"

p
1

1

1

1 + (2

1
1
2

So ✏-optimality requires
2✏

(1

c)n
2

1
1

p

1

1 + (2
1)
)
2(1
)

(2

"

p
1

1
2

(1

1)
2
1)
2

(2

(1

c)n
2

1)
2

(1

c)n
2

#

#

.

c)n
2

#

.

Rearranging and using < 14 , we get that ✏-optimality requires
h
ih
i
(c 1)n
(1 c)n
2
4✏
0.75 2 2
1 (2
1)
and expand n to get

1
✏
0.75
4

1 (2

(c

2

1) log( 1 )
2↵
2(1
)

(1

1)

⇥

c) log( 1 )
2↵
2(1
)

⌘

xy
.
4

Fairness in Reinforcement Learning
1
(c 1) log( 2↵
)
2(1
)
2

Noting that x is minimized when
is maxilog( 1 )
mized, and that this quantity is maximized when 2(1 2↵) is
minimized (as c 1 is negative), we get that ✏-optimality
requires
h
i
c 1
0.75 2 1
y
✏
4
from ↵ <
requires
✏

1
8.

h

Similarly, ↵ <
c

0.75

21

1

ih
1

(2

1)

1
1

c

i

.

1

Note that 0.75 2 1 is minimized when is small, so
> c implies that ✏-optimality requires
i
⇥
⇤h
1 c
0.75 2 1 1 (2
1) 1
✏
4
i
1 c
1 h
1 (2
1) 2(1 ) .
16
Conversely, 1 (2 1)
so as
lim (2
!1

1
1

c

T
1 X ⇡
E
V (⇡ t (s)) < Ṽ
T t=1 M

implies that ✏-optimality

1
8

4

c

For any state s0 , let p(s0 ) denote all the paths of length T
in M that start in s0 , q(s0 ) denote all the paths of length T
in M that start in s0 such that all the states in every path of
length T in q(s0 ) are in and r(s0 ) all the paths of length
T in M that start in s0 such that at least one state in every
path of length T in r(s0 ) is not in . Suppose

Otherwise, ⇡ already witnesses the claim. We show that a
walk of 2T steps from s following ⇡ will terminate in s0
with probability of at least T . First,

E

T
X

⇡
VM
(⇡ t (s), T ) = E

t=1

1)

c

=e

T
X
X

P[p(⇡ t (s))]VM (p(⇡ t (s)))

t=1 p(⇡ t (s))

=E

T
X
X

P[q(⇡ t (s))]VM (q(⇡ t (s)))

t=1 q(⇡ t (s))

is minimized when is large,
1
1

.

+E

c 1

T
X
X
t=1

P[r(⇡ t (s))]VM (r(⇡ t (s)))

r(⇡ t (s))

we get that ✏-optimality requires
✏

1
1
16

e

c 1

since p(⇡ t (s)) = q(⇡ t (s)) [ r(⇡ t (s)), which is a disjoint
union. Next,

.

T
X
X

Finally, the same trick as in the proof of Theorem 3 can be
used to prove the ⌦((k 1/(1 ) )c ) lower bound for k > 2
actions.

E

A.3. Omitted Proofs for Section 4

=E

t=1 q(⇡ t (s))

• there exists an exploitation policy ⇡ in M such that
T
1 X ⇡
E
V
T t=1 M

T
X
X

P⇡M [q(⇡ t (s))]VM (q(⇡ t (s)))

t=1 q(⇡ t (s))

Proof of Lemma 8. We first show that either

T
X
1
max E
V ⇡¯ ⇡
¯ t (s), T
T ⇡¯ 2⇧ t=1 M

P[q(⇡ t (s))]VM (q(⇡ t (s)))

⇡ t (s), T 

where the random variables ⇡ t (s) and ⇡
¯ t (s) denote the
states reached from s after following ⇡ and ⇡
¯ for t steps,
respectively, or
• there exists an exploration policy ⇡ in M such that the
probability that a walk of 2T steps from s following ⇡
will terminate in s0 exceeds T .
Let ⇡ be a policy in M satisfying
T
T
X
0
1 X ⇡ t
1
E
VM (⇡ (s), T ) = max E
V ⇡ (¯
⇡ t (s), T ) := Ṽ .
T t=1
T ⇡¯ 2⇧ t=1 M

E

T
X

⇡
VM
(⇡ t (s), T ),

t=1

where the equality is due to Definition 9 and the definition
⇡
of q, and the inequality follows because VM
(⇡ t (s), T ) is
the sum over all the T -paths in M , not just those that avoid
the absorbing state s0 . Therefore by our original assumption on ⇡,

E

T
X
X
t=1

E

P[q(⇡ t (s))]VM (q(⇡ t (s)))

q(⇡ t (s))

T
X
t=1

⇡
VM
(⇡ t (s), T ) < T Ṽ

T .

Fairness in Reinforcement Learning

Lemma 12 (Lemma 5, Kearns and Singh (2002)). Let M
be an MDP and the set of known states of M . For any
s, s0 2 and action a 2 A, let P̂M (s, a, s0 ) denote the
empirical probability transition estimates obtained from
¯
the visits to s. Moreover, for any state s 2
let R̂(s)
denote the empirical estimates of the average reward obtained from visits to s. Then with probability at least 1
,
✓
◆
min{✏, ↵}2
|P̂M (s, a, s0 ) PM (s, a, s0 )| = O
,
4
n2 H✏

This implies
E

T
X
X

P[r(⇡ t (s))]VM (r(⇡ t (s)))

t=1 r(⇡ t (s))

=E

T
X

⇡
VM
(⇡ t (s), T )

t=1

E

T
X
X

P[q(⇡ t (s))]VM (q(⇡ t (s)))

t=1 q(⇡ t (s))

= T Ṽ

E

T
X
X

P[q(⇡ t (s))]VM (q(⇡ t (s)))

T ,

and
¯
|R̂M (s)

t=1 q(⇡ t (s))

where the last step is the result of applying the previous
inequality. However,
E

T
X
X

P[r(⇡ t (s))]VM (r(⇡ t (s)))

t=1 r(⇡ t (s))

 TE

T
X
X

P[r(⇡ t (s))],

t=1 r(⇡ t (s))

T

 T 2 P⇡2T

and rearranging yields P⇡2T

T

as desired.

Next, note that the exploitation policy (if it exists) can be
derived by computing the optimal policy in M . Moreover,
the exploration policy (if it exists) in the exploitation MDP
M can indeed be derived by computing the optimal policy
in the exploration MDP M[n]\ as observed by (Kearns and
Singh, 2002). Finally, by Observation 5, any optimal policy
↵
in M̂ ↵ (M̂[n]\
) is an optimal policy in M̂ (M̂[n]\ )
To prove Lemma 10, we need some useful background
adapted from Kearns and Singh (2002).
Definition 8 (Definition 7, Kearns and Singh (2002)). Let
M and M̂ be two MDPs with the same set of states and
actions. We say M̂ is a -approximation of M if
• For any state s,
R̄M (s)

 R̄M̂ (s)  R̄M (s) + .

• For any states s and s0 and action a,
PM (s, a, s0 )

 PM̂ (s, a, s0 )  PM (s, a, s0 ) + .

min{✏, ↵}2
n2 H✏

4

◆

.
2

Lemma 12 shows that M̂ and M̂[n]\ are O( min{✏,↵}
)n2 H ✏ 4
approximation MDPs for M and M[n]\ , respectively.
Lemma 13 (Lemma 4, Kearns and Singh (2002)). Let M
2
be an MDP and M̂ its O( min{✏,↵}
)-approximation. Then
n2 H ✏ 4
for any policy ⇡ 2 ⇧ and any state s and action a
⇡
VM
(s)

because it is immediate that VM (r(⇡ t (s)))  T for all
PT P
⇡ t (s). So T  T E t=1 r(⇡t (s)) P[r(⇡ t (s))]. Finally,
if we let P⇡2T denote the probability that a walk of 2T steps
following ⇡ terminates in s0 , i.e. the probability that ⇡ escapes to anPunknown state within 2T steps, then for each
t 2 [T ], E r(⇡t (s))  T P⇡2T . It follows that

R̄M (s)| = O

✓

⇡
⇡
min{✏, ↵}  VM̂
(s)  VM
(s) + min{✏,

↵
},
4

and
Q⇡M (s, a)

↵
min{ , ✏}  Q⇡M̂ (s, a)
4

↵
 Q⇡M (s, a) + min{ , ✏}.
4

Proof of Lemma 10. By Definition 7 and Lemma 12, M̂ is
2
a O( min{✏,↵}
)-approximation of M . Then the statement
n2 H ✏ 4
directly follows by applying Lemma 13.
Rest of the Proof of Theorem 6. The only remaining part
of the proof of Theorem 6 is the analysis of the probability
of failure of Fair-E3 . To do so, we break down the probability of failure of Fair-E3 by considering the following
(exhaustive) list of possible failures:
1. At some known state the algorithm has a poor approximation of the next step, causing M̂ to not be a
2
O( min{✏,↵}
)-approximation of M .
n2 H ✏ 4
2. At some known state the algorithm has a poor approximation of the Q⇤M values for one of the actions.
3. Following the exploration policy for 2T✏⇤ steps fails to
yield enough visits to unknown states.
4. At some known state, the approximation value of that
state in M̂ is not an accurate estimate for the value of
the state in M .
We allocate 4 of our total probability of failure to each of
these sources:

Fairness in Reinforcement Learning

1. Set 0 = 4n in Lemma 10.
2. Set 0 = 4nk in Theorem 7.
3. By Lemma 8, each attempted exploration is a Bernoulli
trial with probability of success of at least 4T✏ ⇤ . In the
✏
worst case we might need to make every state known
before exploiting, leading to the nmQ trajectories (mQ
as Equation 3 in Definition 7) of length H✏ . Therefore,
the probability of taking fewer than nmQ trajectories of
length H✏ would be bounded by 4 if the number of 2T✏⇤
steps explorations is at least
mexp = O

✓

⇣ n ⌘◆
T✏⇤ nmQ
log
.
✏

(5)

4. Set 0 = 4mexp (mexp as defined in Equation 5) in
Lemma 10, as Fair-E3 might make 2T✏⇤ steps explorations up to mexp times.

A.4. Relaxing Assumption 2
Throughout Sections 4.3 and 4.4 we assumed that T✏⇤ , the
✏-mixing time of the optimal policy ⇡ ⇤ , was known (see
Assumption 2). Although Fair-E3 uses the knowledge of
T✏⇤ to decide whether to follow the exploration or exploitation policy, Lemma 8 continues to hold even without this
assumption. Note that Fair-E3 is parameterized by T✏⇤ and
for any input T✏⇤ runs in time poly(T✏⇤ ). Thus if T✏⇤ is
unknown, we can simply run Fair-E3 for T✏⇤ = 1, 2, . . .
sequentially and the running time and sample complexity
will still be poly(T✏⇤ ). Similar to the analysis of Fair-E3
when T✏⇤ is known we have to run the new algorithm for
⇤
sufficiently many steps so that the possibly low VM
values
of the visited states in the early stages are dominated by
⇤
the near-optimal VM
values of the visited states for large
enough guessed values of T✏⇤ .

B. Observations on Optimality and Fairness
Observation 1. For any MDP M , there exists an optimal
policy ⇡ ⇤ such that ⇡ ⇤ is fair.
Proof. In time t, let state st denote the state from which
⇡ chooses an action. Let a⇤ = argmaxa Q⇤M (st , a) and
A⇤ (st ) = {a 2 A | Q⇤M (st , a) = Q⇤M (st , a⇤ )}. The policy of playing an action uniformly at random from A⇤ (st )
in state st for all t, is fair and optimal.

Proof. Assume that ⇡ ⇤ is not approximate-action fair.
Given state s, the action that ⇡ ⇤ takes from s is uniquely
determined since ⇡ ⇤ is deterministic we may denote it by
a⇤ . Then there exists a time step in which ⇡ ⇤ is in state
s and chooses action a⇤ (s) such that there exists another
action a with
Q⇤M (s, a) > Q⇤M (s, a⇤ (s)) + ↵,
a contradiction of the optimality of ⇡ ⇤ .
Observations 1 and 2 state that policies with optimal performance are fair; we now state that playing an action uniformly at random is also fair.
Observation 3. An algorithm that, in every state, plays
each action uniformly at random (regardless of the history)
is fair.
Proof. Let L denote an algorithm that in every state
plays uniformly at random between all available actions.
Then L(s, ht 1 )a = L(s, ht 1 )a0 regardless of state
s, (available) action a, or history ht 1 . Q⇤M (s, a) >
Q⇤M (s, a0 ) + ↵ ) L(s, ht 1 )a
L(s, ht 1 )a0 then
follows immediately, which guarantees both fairness and
approximate-action fairness.
Observation 4. Let M be an MDP and M ↵ the ↵restricted MDP of M . Let ⇡ be a policy in M ↵ . Then ⇡
is ↵-action fair.
Proof. Assume ⇡ is not ↵-action fair. Then there must exist round t, state s, and action a such that Q⇤M (s, a) >
Q⇤M (s, a0 ) + ↵ and L(s, ht 1 )a < L(s, ht 1 )a0 . Therefore L(s, ht 1 )a0 > 0, so M ↵ must include action a0 from
state s. But this is a contradiction, as in state s M ↵ only
includes actions a0 such that Q⇤M (s, a0 ) + ↵ Q⇤M (s, a).
⇡ is therefore ↵-action fair.
Observation 5. Let M be an MDP and M ↵ the ↵restricted MDP of M . Let ⇡ ⇤ be an optimal policy in M ↵ .
Then ⇡ ⇤ is also optimal in M .
Proof. If ⇡ ⇤ is not optimal in M , then there exists a state s and action a such that Q⇤M (s, a) >
Ea⇤ (s)⇠⇡⇤ (s) Q⇤M (s, a⇤ (s)) where a⇤ (s) is drawn from
⇡ ⇤ (s) and the expectation is taken over choices of a⇤ (s).
This is a contradiction because action a is available from
state s in M ↵ by Definition 5.

Approximate-action fairness, conversely, can be satisfied
by any optimal policy, even a deterministic one.

C. Omitted Details of Fair-E3

Observation 2. Let ⇡ ⇤ be an optimal policy in MDP M .
Then ⇡ ⇤ is approximate-action fair.

We first formally define the exploitation MDP M and the
exploration MDP M[n]\ :

Fairness in Reinforcement Learning

Definition 9 (Definition 9, Kearns and Singh (2002)). Let
M = (SM , AM , PM , RM , T, ) be an MDP with state
space SM and let ⇢ SM . We define the exploration MDP
M = (SM , AM , PM , RM , T, ) on where
• SM = [ {s0 }.
• For any state s 2 , R̄M (s) = R̄M (s), rewards in M
are deterministic, and R̄M (s0 ) = 0.
• For any action a, PM (s0 , a, s0 ) = 1. Hence, s0 is an
absorbing state.
• For any states s1 , s2 2
and any action a,
PM (s1 , a, s2 ) = PM (s1 , a, s2 ), i.e. transitions between states in are preserved in M .
• For any state s1 2 and any action a, PM (s1 , a, s0 ) =
⌃s2 2/ PM (s1 , a, s2 ). Therefore, all the transitions between a state in and states not in are directed to s0
in M .
Definition 10 (Implicit, Kearns and Singh (2002)). Given
MDP M and set of known states , the exploration MDP
M[n]\ on is identical to the exploitation MDP M except for its reward function. Specifically, rewards in M[n]\
are deterministic as in M , but for any state s 2 ,
R̄M[n]\ (s) = 0, and R̄M[n]\ (s0 ) = 1.
We next define the approximation MDPs M̂ and M̂[n]\
which are defined over the same set of states and actions as
in M and M[n]\ , respectively.
Let M be an MDP and the set of known states of M . For
any s, s0 2 and action a 2 A, let P̂M (s, a, s0 ) denote the
empirical probability transition estimates obtained from the
¯
visits to s. Moreover, for any state s 2 let R̂M (s) denote the empirical estimates of the average reward obtained
from visits to s. Then M̂ is identical to M except that:
¯
• in any known state s 2 , R̂M̂ (s) = R̂M (s).
0
• for any s, s 2 and action a 2 A, PM̂ (s, a, s0 ) =
P̂M (s, a, s0 ).
Also M̂[n]\ is identical to M[n]\ except that:
• for any s, s0 2

P̂M[n]\ (s, a, s0 ).

and action a 2 A, PM̂[n]\ (s, a, s0 ) =

