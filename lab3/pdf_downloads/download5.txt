Constrained Policy Optimization

10. Appendix
10.1. Proof of Policy Performance Bound
10.1.1. P RELIMINARIES
Our analysis will make extensive use of the discounted future state distribution, dπ , which is deﬁned as
dπ (s) = (1 − γ)

∞
�

γ t P (st = s|π).

t=0

It allows us to express the expected discounted total reward compactly as
J(π) =

1
E [R(s, a, s� )] ,
1 − γ s∼dπ

(17)

a∼π
s� ∼P

where by a ∼ π, we mean a ∼ π(·|s), and by s� ∼ P , we mean s� ∼ P (·|s, a). We drop the explicit notation for the sake
of reducing clutter, but it should be clear from context that a and s� depend on s.
First, we examine some useful properties of dπ that become apparent in vector form for ﬁnite state spaces. Let ptπ ∈ R|S|
denote the vector with components
ptπ (s) = P (st = s|π), and let Pπ ∈ R|S|×|S| denote the transition matrix with
�
�
�
= Pπt µ and
components Pπ (s |s) = daP (s |s, a)π(a|s); then ptπ = Pπ pt−1
π
dπ

∞
�

(γPπ )t µ

=

(1 − γ)

=

(1 − γ)(I − γPπ )−1 µ.

t=0

(18)

This formulation helps us easily obtain the following lemma.
Lemma 1. For any function f : S → R and any policy π,
(1 − γ) E [f (s)] + E π [γf (s� )] − E π [f (s)] = 0.
s∼µ

s∼d
a∼π
s� ∼P

s∼d

(19)

Proof. Multiply both sides of (18) by (I − γPπ ) and take the inner product with the vector f ∈ R|S| .
Combining this with (17), we obtain the following, for any function f and any policy π:
J(π) = E [f (s)] +
s∼µ

1
E [R(s, a, s� ) + γf (s� ) − f (s)] .
1 − γ s∼dπ

(20)

a∼π
s� ∼P

This identity is nice for two reasons. First: if we pick f to be an approximator of the value function V π , then (20) relates
the true discounted return of the policy (J(π)) to the estimate of the policy return (Es∼µ [f (s)]) and to the on-policy average
TD-error of the approximator; this is aesthetically satisfying. Second: it shows that reward-shaping by γf (s� ) − f (s) has
the effect of translating the total discounted return by Es∼µ [f (s)], a ﬁxed constant independent of policy; this illustrates
the ﬁnding of Ng. et al. (1999) that reward shaping by γf (s� ) + f (s) does not change the optimal policy.
It is also helpful to introduce an identity for the vector difference of the discounted future state visitation distributions on
.
.
two different policies, π � and π. Deﬁne the matrices G = (I − γPπ )−1 , Ḡ = (I − γPπ� )−1 , and Δ = Pπ� − Pπ . Then:
G−1 − Ḡ−1

=
=

(I − γPπ ) − (I − γPπ� )

γΔ;

left-multiplying by G and right-multiplying by Ḡ, we obtain
Ḡ − G = γ ḠΔG.

Constrained Policy Optimization

Thus
�

dπ − dπ

=
=
=

�
�
(1 − γ) Ḡ − G µ
γ(1 − γ)ḠΔGµ
γ ḠΔdπ .

(21)

For simplicity in what follows, we will only consider MDPs with ﬁnite state and action spaces, although our attention is
on MDPs that are too large for tabular methods.
10.1.2. M AIN R ESULTS
In this section, we will derive and present the new policy improvement bound. We will begin with a lemma:
Lemma 2. For any function f : S → R and any policies π � and π, deﬁne
�
�
�� �
π (a|s)
� .
�
�
− 1 (R(s, a, s ) + γf (s ) − f (s)) ,
Lπ,f (π ) = E π
s∼d
π(a|s)

(22)

a∼π
s� ∼P

� .
and �πf = maxs |Ea∼π� ,s� ∼P [R(s, a, s� ) + γf (s� ) − f (s)]|. Then the following bounds hold:

1
1−γ
1
J(π � ) − J(π) ≤
1−γ
J(π � ) − J(π) ≥

�
�
�
Lπ,f (π � ) − 2�πf DT V (dπ ||dπ ) ,
�
�
�
�
Lπ,f (π � ) + 2�πf DT V (dπ ||dπ ) ,
�

(23)
(24)

where DT V is the total variational divergence. Furthermore, the bounds are tight (when π � = π, the LHS and RHS are
identically zero).
.
Proof. First, for notational convenience, let δf (s, a, s� ) = R(s, a, s� ) + γf (s� ) − f (s). (The choice of δ to denote this
quantity is intentionally suggestive—this bears a strong resemblance to a TD-error.) By (20), we obtain the identity


J(π � ) − J(π) =

1
1−γ





 E � [δf (s, a, s� )] − E π [δf (s, a, s� )] .
s∼d

s∼dπ
a∼π
s� ∼P

a∼π �
s� ∼P

�

�

Now, we restrict our attention to the ﬁrst term in this equation. Let δ̄fπ ∈ R|S| denote the vector of components δ̄fπ (s) =
Ea∼π� ,s� ∼P [δf (s, a, s� )|s]. Observe that
� �
�
�
E � [δf (s, a, s� )] = dπ , δ̄fπ
s∼dπ
a∼π �
s� ∼P

�
� � �
�
�
�
= dπ , δ̄fπ + dπ − dπ , δ̄fπ

This term is then straightforwardly bounded by applying Hölder’s inequality; for any p, q ∈ [1, ∞] such that 1/p+1/q = 1,
we have
� � ��
� � ��
� � �
� � �
�
�
�
�
�
�
� � �
� � �
dπ , δ̄fπ + �dπ − dπ � �δ̄fπ � ≥ E � [δf (s, a, s� )] ≥ dπ , δ̄fπ − �dπ − dπ � �δ̄fπ � .
p

q

s∼dπ
a∼π �
s� ∼P

p

q

The lower bound leads to (23), and the upper bound leads to (24).

We choose p = 1 �and q = ∞; however,
we believe that this step is very interesting, and different choices for dealing with
�
π�
π π�
the inner product d − d , δ̄f may lead to novel and useful bounds.

Constrained Policy Optimization

�
� �
� ��
�
�
�
�
� �
With �dπ − dπ � = 2DT V (dπ ||dπ ) and �δ̄fπ � = �πf , the bounds are almost obtained. The last step is to observe that,
1
∞
by the importance sampling identity,
�

dπ , δ̄fπ

�

�

=

=

E [δf (s, a, s� )]

s∼dπ
a∼π �
s� ∼P

Eπ

s∼d
a∼π
s� ∼P

��

π � (a|s)
π(a|s)

�

�
δf (s, a, s� ) .

After grouping terms, the bounds are obtained.

This lemma makes use of many ideas that have been explored before; for the special case of f = V π , this strategy (after
�
bounding DT V (dπ ||dπ )) leads directly to some of the policy improvement bounds previously obtained by Pirotta et al.
and Schulman et al. The form given here is slightly more general, however, because it allows for freedom in choosing f .
Remark. It is reasonable to ask if there is a choice of f which maximizes the lower bound here. This turns out to trivially
�
�
π�
be f = V π . Observe that Es� ∼P [δV π� (s, a, s� )|s, a] = Aπ (s, a). For all �states, Ea∼π
� � [A (s, a)] = 0 (by the deﬁnition
�

�

�

�

�

of Aπ ), thus δ̄Vπ π� = 0 and �πV π� = 0. Also, Lπ,V π� (π � ) = −Es∼dπ ,a∼π Aπ (s, a) ; from (20) with f = V π , we can
�

see that this exactly equals J(π � ) − J(π). Thus, for f = V π , we recover an exact equality. While this is not practically
�
useful to us (because, when we want to optimize a lower bound with respect to π � , it is too expensive to evaluate V π for
each candidate to be practical), it provides insight: the penalty coefﬁcient on the divergence captures information about the
�
mismatch between f and V π .
�

Next, we are interested in bounding the divergence term, �dπ − dπ �1 . We give the following lemma; to the best of our
knowledge, this is a new result.
�

Lemma 3. The divergence between discounted future state visitation distributions, �dπ − dπ �1 , is bounded by an average
divergence of the policies π � and π:
�

�dπ − dπ �1 ≤
where DT V (π � ||π)[s] = (1/2)

�

a

2γ
E [DT V (π � ||π)[s]] ,
1 − γ s∼dπ

|π � (a|s) − π(a|s)|.

Proof. First, using (21), we obtain
�

�dπ − dπ �1

=
≤

γ�ḠΔdπ �1

γ�Ḡ�1 �Δdπ �1 .

�Ḡ�1 is bounded by:
�Ḡ�1 = �(I − γPπ� )−1 �1 ≤

∞
�
t=0

t

γ t �Pπ� �1 = (1 − γ)−1

(25)

Constrained Policy Optimization

To conclude the lemma, we bound �Δdπ �1 .
�Δdπ �1

=
≤
=
≤
=

�
�
�
� ���
�
Δ(s� |s)dπ (s)�
�
�
�
s
s�
�
|Δ(s� |s)| dπ (s)
s,s�

�
�
�
� ���
�
�
�
P (s |s, a) (π (a|s) − π(a|s))� dπ (s)
�
�
�
a
s,s�
�
P (s� |s, a) |π � (a|s) − π(a|s)| dπ (s)
s,a,s�

�
s,a

=

|π � (a|s) − π(a|s)| dπ (s)

2 E π [DT V (π � ||π)[s]] .
s∼d

The new policy improvement bound follows immediately.
.
Theorem 1. For any function f : S → R and any policies π � and π, deﬁne δf (s, a, s� ) = R(s, a, s� ) + γf (s� ) − f (s),
� .
�πf = max |Ea∼π� ,s� ∼P [δf (s, a, s� )]| ,

s

.
Lπ,f (π � ) = E π
s∼d
a∼π
s� ∼P

��

�
�
π � (a|s)
− 1 δf (s, a, s� ) , and
π(a|s)
�

±
(π � )
Dπ,f

where DT V (π � ||π)[s] = (1/2)
The following bounds hold:

�

a

2γ�πf
. Lπ,f (π � )
±
=
E [DT V (π � ||π)[s]] ,
1−γ
(1 − γ)2 s∼dπ

|π � (a|s) − π(a|s)| is the total variational divergence between action distributions at s.
+
−
Dπ,f
(π � ) ≥ J(π � ) − J(π) ≥ Dπ,f
(π � ).

(4)

Furthermore, the bounds are tight (when π = π, all three expressions are identically zero).
�

�

Proof. Begin with the bounds from lemma 2 and bound the divergence DT V (dπ ||dπ ) by lemma 3.
10.2. Proof of Analytical Solution to LQCLP
Theorem 2 (Optimizing Linear Objective with Linear and Quadratic Constraints). Consider the problem
p∗ = min g T x
x

s.t. bT x + c ≤ 0

(26)

T

x Hx ≤ δ,

where g, b, x ∈ Rn , c, δ ∈ R, δ > 0, H ∈ Sn , and H � 0. When there is at least one strictly feasible point, the optimal
point x∗ satisﬁes
x∗ = −

1 −1
H (g + ν ∗ b) ,
λ∗

Constrained Policy Optimization

where λ∗ and ν ∗ are deﬁned by
�
� ∗
λ c−r
∗
,
ν =
s
+
�
�
�
. 1 r2
−q +
fa (λ) = 2λ
∗
s
λ = arg max
�
�
.
λ≥0
fb (λ) = − 12 λq + λδ

λ
2

�

c2
s

�
−δ −

rc
s

if λc − r > 0
otherwise,

with q = g T H −1 g, r = g T H −1 b, and s = bT H −1 b.
.
.
Furthermore, let Λa = {λ|λc − r > 0, λ ≥ 0}, and Λb = {λ|λc − r ≤ 0, λ ≥ 0}. The value of λ∗ satisﬁes
∗

λ ∈

�

λ∗a

.
= Proj

��

q − r2 /s
, Λa
δ − c2 /s

�

, λ∗b

.
= Proj

��

q
, Λb
δ

��

,

with λ∗ = λ∗a if fa (λ∗a ) > fb (λ∗b ) and λ∗ = λ∗b otherwise, and Proj(a, S) is the projection of a point x on to a set S. Note:
the projection of a point x ∈ R onto a convex segment of R, [a, b], has value Proj(x, [a, b]) = max(a, min(b, x)).
Proof. This is a convex optimization problem. When there is at least one strictly feasible point, strong duality holds by
Slater’s theorem. We exploit strong duality to solve the problem analytically.
�
�
�
λ� T
x Hx − δ + ν bT x + c
2
�
�
λ
1
T
= max min xT Hx + (g + νb) x + νc − λδ
λ≥0 x
2
2

p∗ = min max g T x +
x

λ≥0
ν≥0

Strong duality

ν≥0

1
=⇒ x∗ = − H −1 (g + νb)
λ
�
�
1
1
T
−1
(g + νb) H (g + νb) + νc − λδ
= max −
λ≥0
2λ
2
ν≥0
�
�
�
1 �
1
= max −
q + 2νr + ν 2 s + νc − λδ
λ≥0
2λ
2

∇x L(x, λ, ν) = 0
Plug in x∗
.
.
.
Notation: q = g T H −1 g, r = g T H −1 b, s = bT H −1 b.

ν≥0

1
∂L
= − (2r + 2νs) + c
∂ν � 2λ �
λc − r
=⇒ ν =
s
+
� 2
� 2
�
�
�
1
r
− q + λ2 cs − δ −
2λ
s
= max
�
�
λ≥0
− 12 λq + λδ
=⇒

Optimizing single-variable convex quadratic function over R+
rc
s

if λ ∈ Λa

if λ ∈ Λb

Notation:

.
Λa = {λ|λc − r > 0, λ ≥ 0},
.
Λb = {λ|λc − r ≤ 0, λ ≥ 0}

Observe that when c < 0, Λa = [0, r/c) and Λb = [r/c, ∞); when c > 0, Λa = [r/c, ∞) and Λb = [0, r/c).
Notes on interpreting the coefﬁcients in the dual problem:

• We are guaranteed to have r2 /s − q ≤ 0 by the Cauchy-Schwarz inequality. Recall that q = g T H −1 g, r = g T H −1 b,
s = bT H −1 b. The Cauchy-Scwarz inequality gives:
�H −1/2 b�22 �H −1/2 g�22 ≥

��

H −1/2 b

�T �

�
��
� �
�2
=⇒ bT H −1 b g T H −1 g ≥ bT H −1 g
∴ qs ≥ r2 .

H −1/2 g

� �2

Constrained Policy Optimization

• The coefﬁcient c2 /s − δ relates to whether or not the plane of the linear constraint intersects the quadratic trust region.
An intersection occurs if there exists an x such that c + bT x = 0 with xT Hx ≤ δ. To check whether this is the case,
we solve
(27)
x∗ = arg min xT Hx : c + bT x = 0
x

and see if x Hx ≤ δ. The solution to this optimization problem is x∗ = cH −1 b/s, thus x∗T Hx∗ = c2 /s. If
c2 /s − δ ≤ 0, then the plane intersects the trust region; otherwise, it does not.
∗T

∗

If c2 /s − δ > 0 and c < 0, then the quadratic trust region lies entirely within the linear constraint-satisfying halfspace, and
we can remove the linear constraint without changing the optimization problem. If c2 /s − δ > 0 and c > 0, the problem
is infeasible (the intersection of the quadratic trust region and linear constraint-satisfying halfspace is empty). Otherwise,
we follow the procedure below.
Solving the dual for λ: for any A > 0, B > 0, the problem
�
�
. 1 A
+ Bλ
max f (λ) = −
λ≥0
2 λ
�
√
has optimal point λ∗ = A/B and optimal value f (λ∗ ) = − AB.

We can use this solution form to obtain the optimal point on each segment of the piecewise continuous dual function for λ:
objective

optimal point (before projection)

�
�
�
r2
λ c2
rc
−q +
−δ −
s
2 s
s
�
�
. 1 q
+ λδ
fb (λ) = −
2 λ
. 1
fa (λ) =
2λ

�

.
λa =
.
λb =

�

�

optimal point (after projection)

q − r2 /s
δ − c2 /s

λ∗a = Proj(λa , Λa )

q
δ

λ∗b = Proj(λb , Λb )

The optimization is completed by comparing fa (λ∗a ) and fb (λ∗b ):
� ∗
λa fa (λ∗a ) ≥ fb (λ∗b )
∗
λ =
λ∗b otherwise.

10.3. Experimental Parameters
10.3.1. E NVIRONMENTS
In the Circle environments, the reward and cost functions are
v T [−y, x]
,
1 + |�[x, y]�2 − d|
C(s) = 1 [|x| > xlim ] ,
R(s) =

where x, y are the coordinates in the plane, v is the velocity, and d, xlim are environmental parameters. We set these
parameters to be
d
xlim

Point-mass
15
2.5

Ant
10
3

Humanoid
10
2.5

In Point-Gather, the agent receives a reward of +10 for collecting an apple, and a cost of 1 for collecting a bomb. Two
apples and eight bombs spawn on the map at the start of each episode. In Ant-Gather, the reward and cost structure was
the same, except that the agent also receives a reward of −10 for falling over (which results in the episode ending). Eight
apples and eight bombs spawn on the map at the start of each episode.

Constrained Policy Optimization

Figure 5. In the Circle task, reward is maximized by moving along the green circle. The agent is not allowed to enter the blue regions,
so its optimal constrained path follows the line segments AD and BC.

10.3.2. A LGORITHM PARAMETERS
In all experiments, we use Gaussian policies with mean vectors given as the outputs of neural networks, and with variances
that are separate learnable parameters. The policy networks for all experiments have two hidden layers of sizes (64, 32)
with tanh activation functions.
We use GAE-λ (Schulman et al., 2016) to estimate the advantages and constraint advantages, with neural network value
functions. The value functions have the same architecture and activation functions as the policy networks. We found that
having different λGAE values for the regular advantages and the constraint advantages worked best. We denote the λGAE
used for the constraint advantages as λGAE
.
C
For the failure prediction networks Pφ (s → U ), we use neural networks with a single hidden layer of size (32), with output
of one sigmoid unit. At each iteration, the failure prediction network is updated by some number of gradient descent steps
using the Adam update rule to minimize the prediction error. To reiterate, the failure prediction network is a model for the
probability that the agent will, at some point in the next T time steps, enter an unsafe state. The cost bonus was weighted
by a coefﬁcient α, which was 1 in all experiments except for Ant-Gather, where it was 0.01. Because of the short time
horizon, no cost bonus was used for Point-Gather.
For all experiments, we used a discount factor of γ = 0.995, a GAE-λ for estimating the regular advantages of λGAE =
0.95, and a KL-divergence step size of δKL = 0.01.
Experiment-speciﬁc parameters are as follows:
Parameter
Batch size
Rollout length
Maximum constraint value d
Failure prediction horizon T
Failure predictor SGD steps per itr
Predictor coeff α
λGAE
C

Point-Circle
50,000
50-65
5
5
25
1
1

Ant-Circle
100,000
500
10
20
25
1
0.5

Humanoid-Circle
50,000
1000
10
20
25
1
0.5

Point-Gather
50,000
15
0.1
(N/A)
(N/A)
(N/A)
1

Ant-Gather
100,000
500
0.2
20
10
0.01
0.5

Note that these same parameters were used for all algorithms.
, but for the higher-dimensional environments, it was necWe found that the Point environment was agnostic to λGAE
C
to a value < 1. Failing to discount the constraint advantages led to substantial overestimates of the
essary to set λGAE
C
= 0.5 was obtained by a
constraint gradient magnitude, which led the algorithm to take unsafe steps. The choice λGAE
C
hyperparameter search in {0.5, 0.92, 1}, but 0.92 worked nearly as well.
10.3.3. P RIMAL -D UAL O PTIMIZATION I MPLEMENTATION
Our primal-dual implementation is intended to be as close as possible to our CPO implementation. The key difference
is that the dual variables for the constraints are stateful, learnable parameters, unlike in CPO where they are solved from
scratch at each update.

Constrained Policy Optimization

The update equations for our PDO implementation are
�
θk+1 = θk + sj

2δ
H −1 (g − νk b)
(g − νk b)T H −1 (g − νk b)

νk+1 = (νk + α (JC (πk ) − d))+ ,
where sj is from the backtracking line search (s ∈ (0, 1) and j ∈ {0, 1, ..., J}, where J is the backtrack budget; this is
the same line search as is used in CPO and TRPO), and α is a learning rate for the dual parameters. α is an important
hyperparameter of the algorithm: if it is set to be too small, the dual variable won’t update quickly enough to meaningfully
enforce the constraint; if it is too high, the algorithm will overcorrect in response to constraint violations and behave too
conservatively. We experimented with a relaxed learning rate, α = 0.001, and an aggressive learning rate, α = 0.01. The
aggressive learning rate performed better in our experiments, so all of our reported results are for α = 0.01.
Selecting the correct learning rate can be challenging; the need to do this is obviated by CPO.

