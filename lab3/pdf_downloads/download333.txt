Appendix: Understanding Black-box Predictions via Influence Functions
Pang Wei Koh 1 Percy Liang 1

A. Deriving the influence function Iup,params
For completeness, we provide a standard derivation of the
influence function Iup,params in the context of loss minimization (M-estimation). This derivation is based on asymptotic arguments and is not fully rigorous; see van der Vaart
(1998) and other statistics textbooks for a more thorough
treatment.
Recall that Î¸Ì‚ minimizes the empirical risk:
n

def

R(Î¸) =

1X
L(zi , Î¸).
n i=1

(1)

We further assume that R is twice-differentiable and
strictly convex in Î¸, i.e.,
2

The perturbed parameters Î¸Ì‚,z can be written as
Î¸âˆˆÎ˜

(3)

Define the parameter change âˆ† = Î¸Ì‚,z âˆ’ Î¸Ì‚, and note that,
as Î¸Ì‚ doesnâ€™t depend on , the quantity we seek to compute
can be written in terms of it:
dÎ¸Ì‚,z
dâˆ†
=
.
d
d

(4)

Since Î¸Ì‚,z is a minimizer of (3), let us examine its firstorder optimality conditions:
0 = âˆ‡R(Î¸Ì‚,z ) + âˆ‡L(z, Î¸Ì‚,z ).

Solving for âˆ† , we get:
h
iâˆ’1
âˆ† â‰ˆ âˆ’ âˆ‡2 R(Î¸Ì‚) + âˆ‡2 L(z, Î¸Ì‚)
h
i
âˆ‡R(Î¸Ì‚) + âˆ‡L(z, Î¸Ì‚) .

(7)

Since Î¸Ì‚ minimzes R, we have âˆ‡R(Î¸Ì‚) = 0. Keeping only
O() terms, we have
(8)

(2)

exists and is positive definite. This guarantees the existence
of HÎ¸Ì‚âˆ’1 , which we will use in the subsequent derivation.

Î¸Ì‚,z = arg min {R(Î¸) + L(z, Î¸)} .

where we have dropped o(kâˆ† k) terms.

âˆ† â‰ˆ âˆ’ âˆ‡2 R(Î¸Ì‚)âˆ’1 âˆ‡L(z, Î¸Ì‚).

n

1X 2
âˆ‡ L(zi , Î¸Ì‚)
HÎ¸Ì‚ = âˆ‡ R(Î¸Ì‚) =
n i=1 Î¸
def

Next, since Î¸Ì‚,z â†’ Î¸Ì‚ as  â†’ 0, we perform a Taylor expansion of the right-hand side:
h
i
0 â‰ˆ âˆ‡R(Î¸Ì‚) + âˆ‡L(z, Î¸Ì‚) +
(6)
h
i
âˆ‡2 R(Î¸Ì‚) + âˆ‡2 L(z, Î¸Ì‚) âˆ† ,

(5)

1

Stanford University, Stanford, CA. Correspondence to:
Pang Wei Koh <pangwei@cs.stanford.edu>, Percy Liang <pliang@cs.stanford.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Combining with (2) and (4), we conclude that:
dÎ¸Ì‚,z 
= âˆ’HÎ¸Ì‚âˆ’1 âˆ‡L(z, Î¸Ì‚)

d =0
def

= Iup,params (z).

(9)
(10)

B. Influence at non-convergence
Consider a training point z. When the model parameters
Î¸Ìƒ are close to but not at a local minimum, Iup,params (z) is
approximately equal to a constant (which does not depend
on z) plus the change in parameters after upweighting z and
then taking a single Newton step from Î¸Ìƒ. The high-level
idea is that even though the gradient of the empirical risk at
Î¸Ìƒ is not 0, the Newton step from Î¸Ìƒ can be decomposed into
a component following the existing gradient (which does
not depend on the choice of z) and a second component
responding to the upweighted z (which Iup,params (z) tracks).
Pn
def
Let g = n1 i=1 âˆ‡Î¸ L(zi , Î¸Ìƒ) be the gradient of the empirical risk at Î¸Ìƒ; since Î¸Ìƒ is not a local minimum, g 6= 0.
After upweighting z by , the gradient at Î¸Ìƒ goes from
g 7â†’ g + âˆ‡Î¸ L(z, Î¸Ìƒ), and the empirical Hessian goes from
HÎ¸Ìƒ 7â†’ HÎ¸Ìƒ + âˆ‡2Î¸ L(z, Î¸Ìƒ). A Newton step from Î¸Ìƒ therefore
changes the parameters by:
h
iâˆ’1 h
i
def
N,z = âˆ’ HÎ¸Ìƒ + âˆ‡2Î¸ L(z, Î¸Ìƒ)
g + âˆ‡Î¸ L(z, Î¸Ìƒ) .
(11)

Appendix: Understanding Black-box Predictions via Influence Functions
2
Ignoring
 terms in g,  , and higher, we get N,z â‰ˆ
âˆ’HÎ¸Ìƒâˆ’1 g + âˆ‡Î¸ L(z, Î¸Ìƒ) . Therefore, the actual change

due to a Newton step N,z is equal to a constant âˆ’HÎ¸Ìƒâˆ’1 g
(that doesnâ€™t depend on z) plus  times Iup,params (z) =
âˆ’HÎ¸Ìƒâˆ’1 âˆ‡Î¸ L(z, Î¸Ìƒ) (which captures the contribution of z).

References
van der Vaart, A. W. Asymptotic statistics. Cambridge
University Press, 1998.

