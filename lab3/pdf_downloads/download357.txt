Doubly Greedy Primal-dual Coordinate Descent for Sparse Empirical Risk Minimization

A. Appendix A: Convergence Analysis

(t)

A.1. Proof of Theorem 4.2
Recall primal, dual and Lagrangian forms:
n
1X
def
P (x) =
i (hAi , xi) + g(x),
n i=1
def

L(x, y)

=

g(x) +

def

D(y)

n
1X
n i=1

1 T
y Ax
n

(20)
⇤
i (yi )

(21)

x

=

L(x
X
i

=

,y

L(x
X

i2supp(x(t)



kx̄(t)

(t)

L(x̄

)

(t)

,y

(t)

)

(t)


(t)
p

(t)

x(t) k0 ⇥

,y

(t)

)

(t)

max L(x(t) , y (t) )

L((x̄i

i

+x

(t)
L((x̄i

(t)

,y

(t)

(t)
xi )ei

(t)
p

=

)

(t)

xi )ei + x(t) , y (t) )

= kx̄(t) x(t) k0 L(x(t) , y (t) ) L(x(t+1) , y (t) )
And by adding L(x(t+1) , y (t) ) L(x(t) , y (t) ) to both sides we
finishes the proof.
Recall i(t) is the selected coordinate to update in dual variable
y (t) .



L(x

,y )

+

(t)
p

L(xt , y t )

(t 1)
p

1
+⌘( hAi(t) , x(t) x̄(t) ii(t) )2
n
, where g 2 n1 @ ⇤i(t) (y (t) ).

L(x

(t+1)

,y

(t)

g)2

(t)

g)(yi(t)

(t 1)

1)

(23)

yi(t) )

(t)

yi(t) )2

(24)

L(xt , y (t

t

t

t

1)

)

(D(y (t) )
t

t

L(x , y ) + L(x , y )

,y )

D(y (t

1)

))

D(y (t
t

L(x , y

1)

(t 1)

L(x(t+1) , y t ) L(xt , y t )
)
1
(t)
(t 1)
+ hAi(t) , x(t) x̄(t) i(yi(t) yi(t) )
n
Here the last inequality comes from inequalities (24) and
(23).


Meanwhile, with the update rule of dual variable:
1
1
(t)
(t)
⇤
yi(t)
arg max hAi(t) , x(t) i
(
yi(t) )2
i(t) ( )
n
2⌘
(t)
(t 1)
Therefore 9 g 2 @ ⇤i(t) (y (t) ) such that yi(t)
yi(t) =
(t)
1
⌘( n (hAi(t) , x i g). Therefore:
1
(t)
(t 1)
(23) =
( hAi(t) , x̄(t) i g)(yi(t) yi(t) )
n
(t)

=
1
⌘( hAi(t) , x̄(t) i
n

D(y (t

(t 1)
p

L(x(t+1) , y t )

Lemma A.2. (Primal-Dual Progress).
(t)
(t 1)
d
d
(t+1)
t

D(y (t)

L(xt , y t ) L(xt , y (t 1) )
1
(t)
(t 1)
( hAi(t) , x(t) i g)(yi(t) yi(t) )
n

(D(y (t) )

+x

}

(t 1)
yi(t) )2

+ (yi(t)
2

Therefore,

(t)

(t 1)
p

{z

primal progress

=

(t 1)

=
(t)
xi )ei

(t)
p

|

Similarly we get,

)

(t)
L((x̄i

L(x

x̄(t) )

,y

(t)

+

• Primal progress:

Proof. This lemma is a direct result by our greedy update rule of
our primal variables.
(t)

(t 1)
d

(t)
(y
2 i(t)

Lemma A.1. (Primal Progress):
1

}

1
( hAi(t) , x̄(t) i
n



def

kx(t)

(t 1)
d

dual progress

(t)
d

Recall the primal gap defined as p = L(x(t+1) , y (t) )
(t) def
D(y (t) ), and dual gap d = D⇤
D(y (t) ). In the proof,
we will connect the objective change in primal/dual update with
the primal/dual gap and show how the sub-optimality: (t) =
(t)
(t)
p +
d enjoys linear convergence.
L(x(t+1) , y (t) )

{z

By Danskins’ theorem, D(y) is -strongly convex. Therefore for any g 2 @ ⇤i(t) (y (t) ), we have,

Recall with the choice of regularizer of our model, g(x) = h(x) +
kxk1 , where h(x) = µ2 kxk22 satisfies µ-strong convexity, µsmooth and separable. The conjugate of loss function(e.g. smooth
hinge loss used in our experiments): ⇤ is -strongly convex.

L(x(t) , y (t) )

(t 1)

• Dual progress:

x
def

For simplicity, we will use x̄(t) = x̄(y (t ) throughout this paper.
Similarly, we also use ȳ(x) : Rd ! Rn to be the optimal dual
variable with respect to some x.

1
x̄(t) k0

(t)
d

|

where x̄(y) : Rn ! Rd is the optimal primal variable with respect
to some y, namely,
x̄(y) = arg min L(x, y)

(t)

(t)

Proof. The primal and dual gap comes from both primal and dual
progresses:

(22)

min L(x, y) ⌘ L(x̄(y), y)

=

(t 1)

Our goal is to prove that d
+ p

p
d
(t)
(t)
to
show
linear
convergence
in
sub-optimality.
p
d
(t)
1
Since L(x(t+1) , y t )
L(xt , y t ) 
p , this
kx(t) x̄(t) k0
lemma is the middle step that connects to the primal part, and
the remaining part represents the dual progress and will be analyzed later.

(t)

(y
yi(t) )2
2 i(t 1)
1
(t 1)
h Ai(t) , x̄(t) x(t) i(yi(t)
n
1
(t)
(t 1)
( + )(yi(t) yi(t) )2
⌘
2

• Summing together we have:

(t)

yi(t) )
(25)

))
)

Doubly Greedy Primal-dual Coordinate Descent for Sparse Empirical Risk Minimization



=



(t)
(t 1)
d
d
(t+1)
t

+

(t)
p

The first inequality follows (a + b)2 = a2 b2 2ab 
1 2
a2
b2 + 12 a2 + 2b2 =
a + b2 , and replace a by
2

(t 1)
p

L(x
, y ) L(xt , y t )
2
(t)
+ hAi(t) , x̄(t) x(t) i(yi(t)
n
1
(t)
(t 1)
( + )(yi(t) yi(t) )2
⌘
2

1
hAi(t) , x(t) i
n

L(x(t+1) , y t ) L(xt , y t )
1
+⌘( hAi(t) , x(t) x̄(t) i)2
n

(t 1)

Meanwhile, since kA(x̄(t)
we get Lemma A.3.

g)

And similarly for

x(t) )k1  Rkx̄(t)

x(t) k, together

Proof of Theorem 4.2.
g)2

(t)
d
Lemma A.2



Afterwards, we upper bound the dual progress ( n1 hAi(t) , x(t)
(t)
x̄(t) i)2 ( n1 hAi(t) , x̄(t) i g)2 by dual gap d :
g)2
(26)

P
Proof. For simplicity, we denote ⇤ (y) = n1 i ⇤i (yi ). To
begin with,
2 1
(t)
⇤
D(y) 
k Ax̄(t) @ ⇤ (y (t) )k2
d = D
n
2n 1

k Ax̄(t) @ ⇤ (y (t) )k21
n
In our algorithm, the greedy choice of i(t) makes sure k n1 Ax(t)
@ ⇤ (y (t) )ki(t) = k n1 Ax(t)
@ ⇤ (y (t) )k1 . However, here
we need the relation between k n1 Ax̄(t)
@ ⇤ (y (t) )ki(t) and
(t)
⇤
(t)
1
k n Ax̄
@ (y )k1 (assumed to be reached at coordinate
def

i(t) .

Now we have established the connection between the primal and
dual progress (change in primal/dual gap) with primal and dual gap,
and the only redundant part is kx(t) x̄(t) k, but since µ2 kx(t)
x̄(t) k  L(x(t) , y (t) ) L(x̄(t) , y (t) ), which could be absorbed
in the primal gap. Therefore, back to the main inequality (26):

1
⌘( hAi(t) , x̄(t) i
n

Lemma A.3. (Dual Progress).
1
1
( hAi(t) , x(t) x̄(t) i)2 ( hAi(t) , x̄(t) i
n
n
5R2 (t)
(t)

kx
x̄(t) k2
d +
2n
2n2
(t)
, where g 2 n1 @ ⇤i(t) (yi(t) ).

def

and b =

the third inequality.

yi(t) )

L(x(t+1) , y t ) L(xt , y t )
2⌘
1
+ hAi(t) , x(t) x̄(t) i( hAi(t) , x(t) i
n
n
1
1
⌘ 2 ( + )(h Ai(t) , x(t) i g)2
⌘
2
n

(t)
1
( ⇤i(t) )0 (yi(t) )
n

i⇤ ). We bridge their gap by = n1 A(x̄(t) x(t) ). Since
1
1 ⇤ 0 (t) 2
( hAi(t) , x̄(t) i
( (t) ) (yi(t) ))
n
n i
1
1 ⇤ 0 (t)
=
( hAi(t) , x(t) i
( (t) ) (yi(t) ) + i(t) )2
n
n i
⌘2
1 ⇣
(t)

hAi(t) , x(t) i ( ⇤i(t) )0 (yi(t) ) + i2(t)
2
2n
1 1
2
=
Ax(t) @ ⇤ (y (t) ) 1 + i2(t)
2 n
1 1
1 ⇤ 0 (t) 2

( hAi⇤ , x(t) i
( i⇤ ) (yi⇤ )) + k k21
2 n
n
1 ⇤ 0 (t)
1 1
2
2
=
( hAi⇤ , x̄(t) i
( i⇤ ) (yi⇤ )
i⇤ ) + k k 1
2 n
n
1 1
1 ⇤ 0 (t) 2 3

( hAi⇤ , x̄(t) i
( i⇤ ) (yi⇤ )) + k k21
4 n
n
2
1 1
3
(t)
⇤
(t) 2
2
=
k Ax̄
@ (y )k1 + k k1
4 n
2
3
k
2

k k1
d +
2n
2

Lemma A.3





=

Lemma A.1



=

(t 1)
d

+

(t)
p

(t 1)
p

L(x(t+1) , y t ) L(xt , y t )
1
+h Ax(t) r'(y (t) ), y (t) y (t 1) i
n
1
2h Ax̄(t) r'(y (t) ), y (t) y (t 1) i
n
⌘
(t)
L(x(t+1) , y t ) L(xt , y t )
2n d
5⌘R2 (t)
+
kx
x̄(t) k2
2n2
⌘
(t)
L(x(t+1) , y t ) L(xt , y t )
2n d
5⌘R2
+
L(x(t) , y (t) ) L(x̄(t) , y (t) )
µn2
5⌘R2
⌘
(t)
(1
) L(x(t+1) , y t ) L(xt , y t )
µn2
2n d
5⌘R2
+
L(x(t+1) , y t ) L(x̄(t) , y t )
µn2
5⌘R2
1
⌘
(t)
(t)
(1
)
+
p
µn2 kx(t) x̄(t) k0 1
2n d
5⌘R2
L(x(t+1) , y t )
µn2
5⌘R2
(1
)
µn2 kx(t)
⌘
(t)
2n d

L(x̄(t) , y t )
1
x̄(t) k0

1

5⌘R2
µn2

Therefore, we have
kx(t) x̄(t) k0
5⌘R2 (t)
⌘
(t)
(1
) p +(1+ ) d 
(t)
(t)
µn2
2n
kx
x̄ k0 1
i.e. linear convergence. Notice when
2n2 µ
⌘ (t) 
2
(10R + n µ)kx(t) x̄(t) k0
1
(t)
(t 1)

(t)
1 + ⌘ 2n

(t)
p

(t 1)
+
d

(27)

Specifically, when inequality holds for (27), and suppose kx(t)
x̄(t) k0  s, then it requires O(s( n + 1) log 1✏ ) iterations to
2
achieve ✏ primal and dual sub-optimality, where  = R
.
µ

(t 1)
p

Doubly Greedy Primal-dual Coordinate Descent for Sparse Empirical Risk Minimization

B

Appendix B: Additional Experimental Results

Finally, we show result for
parameters.

= 0.01, 0.1, and µ = 0.01, 0.1, 1. Here are some comments for results under different

The winning margin of DGPD is larger on data sets of dense feature matrix than that of sparse feature matrix. One reason
for this is, for data of sparse feature matrix, features of higher frequency are more likely to be active than those of lower
frequency, and therefore, the feature sub-matrix corresponding to the active primal variables are often denser than submatrix
matrix corresponding to the inactive ones. This results in a less overall speedup.
RCV1-Time-l01-mu1

Aloi-RF-Time-l01-mu1
Mnist-RF-Time-l01-mu1

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -2

relative primal objective

relative primal objective

10 -10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

100

200

300

400

500

600

700

800

900

-5

relative primal objective

10

10 -5

10 -10

10

DGPD
DualRCD
PrimalRCD
SPDC-dense

-15

100

1000

200

300

400

500

600

700

800

900

10 -4

10 -6

10 -8

10 -2

1000

10 0

10 2

time

time

time

Aloi-RB-Time-l01-mu1
Mnist-RB-Time-l01-mu1
DGPD
DualRCD
PrimalRCD
SPDC

10 -3

10 -2

10 -3

10 -4

10 -4

10 0

DGPD
DualRCD
PrimalRCD
SPDC

relative primal objective

10 -2

Sector-Time-l01-mu1

DGPD
DualRCD
PrimalRCD
SPDC

10 -1

relative primal objective

relative primal objective

10 -1

10 0

10 2

10

-5

10 -10

10 -15

10 2

0

5

Aloi-RF-Iteration-l01-mu1

RCV1-Iteration-l01-mu1

relative primal objective

relative primal objective

10 -10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

200

400

600

800

10

-5

relative primal objective

10

10 -5

-10

10

-2

10

-4

10

-6

10

-8

DGPD
DualRCD
PrimalRCD
SPDC-dense

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

0

1000

15

time

Mnist-RF-Iteration-l01-mu1

0

10

time

time

200

400

600

800

1000

0

50

100

iter

iter

150

iter

Aloi-RB-Iteration-l01-mu1

10

-2

10

-3

10

-4

Sector-Iteration-l01-mu1
DGPD
DualRCD
PrimalRCD
SPDC

DGPD
DualRCD
PrimalRCD
SPDC

10 -1

DGPD
DualRCD
PrimalRCD
SPDC

10 -2

10

relative primal objective

10

-1

relative primal objective

relative primal objective

Mnist-RB-Iteration-l01-mu1

-3

10 -4

10

10

-5

-10

10 -5

0

20

40

60

iter

80

100

10 -6

0

50

100

150

200

10 -15

0

200

iter

Figure 2. Relative Objective versus Time (the upper 2 rows) and versus # iterations (the lower 2 rows) for

400

600

iter

= 0.1, µ = 1.

800

1000

Doubly Greedy Primal-dual Coordinate Descent for Sparse Empirical Risk Minimization

We also observe that in order to achieve the best performance of DGPD, both primal and dual sparsity must hold, and the
sparsity is partially controled by the L1/L2 penalty. In particular, when the L1 penalty has too much weight, the primal
iterate would become too sparse to yield a reasonable prediction accuracy, which then results in a particularly dense dual
iterate due to its non-zero loss on most of the samples. Another example is, when the L2 penalty becomes too large, the
classifier would tend to mis-classify many examples in order to gain a large margin, which results in dense dual iterates.
However, in practice such hyperparameter settings are less likely to be chosen due to its inferior prediction performance.
RCV1-Time-l01-mu01

Aloi-RF-Time-l01-mu01
Mnist-RF-Time-l01-mu01

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -2

relative primal objective

relative primal objective

10 -10

100

200

300

400

500

600

700

800

900

10 -10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

relative primal objective

10 -5

10 -5

100

1000

200

300

400

500

600

700

800

900

10 -4

10 -6

10 -8

10 -2

1000

10 0

10 2

time

time

time

Aloi-RB-Time-l01-mu01
Mnist-RB-Time-l01-mu01

10 -2

10 -3

10 -2

10 -3

10 -4

10 -4

10 0

Sector-Time-l01-mu01

DGPD
DualRCD
PrimalRCD
SPDC

10 -1

DGPD
DualRCD
PrimalRCD
SPDC

relative primal objective

-1

relative primal objective

relative primal objective

10

DGPD
DualRCD
PrimalRCD
SPDC

10 0

10 2

10

-5

10 -10

10 -15

10 2

0

5

RCV1-Iteration-l01-mu01

Mnist-RF-Iteration-l01-mu01

relative primal objective

relative primal objective

10 -10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

200

400

600

800

10

relative primal objective

10 -5

10 -5

-10

10

-2

10

-4

10

-6

10

-8

DGPD
DualRCD
PrimalRCD
SPDC-dense

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

0

1000

15

time

Aloi-RF-Iteration-l01-mu01

0

10

time

time

200

400

600

800

1000

0

50

100

iter

iter

150

iter

Aloi-RB-Iteration-l01-mu01

10

-2

10

10

Sector-Iteration-l01-mu01
DGPD
DualRCD
PrimalRCD
SPDC

10 -1

-3

-4

DGPD
DualRCD
PrimalRCD
SPDC

10 -2

relative primal objective

10

-1

DGPD
DualRCD
PrimalRCD
SPDC

relative primal objective

relative primal objective

Mnist-RB-Iteration-l01-mu01

10 -3

10 -4

10

10

-5

-10

10 -5

0

20

40

60

iter

80

100

10 -6

0

50

100

150

200

10 -15

0

200

iter

Figure 3. Relative Objective versus Time (the upper 2 rows) and versus # iterations (the lower 2 rows) for

400

600

800

iter

= 0.1, µ = 0.1.

1000

Doubly Greedy Primal-dual Coordinate Descent for Sparse Empirical Risk Minimization

RCV1-Time-l001-mu1
Mnist-RF-Time-l001-mu1

Aloi-RF-Time-l001-mu1
DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15
100

200

300

400

500

600

700

800

900

10

relative primal objective

10

relative primal objective

relative primal objective

10 -2
10 -5

-5

-10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

1000

100

200

300

400

time

500

600

700

800

900

10 -4

10 -6

10 -8

10 -2

1000

10 0

Mnist-RB-Time-l001-mu1

10 2

time

time

Aloi-RB-Time-l001-mu1
Sector-Time-l001-mu1

10

10 -2

10 -3

10 -4

DGPD
DualRCD
PrimalRCD
SPDC

-1

10 -2

10 -3

10 -4

10 0

DGPD
DualRCD
PrimalRCD
SPDC

relative primal objective

-1

relative primal objective

relative primal objective

10

DGPD
DualRCD
PrimalRCD
SPDC

10 2

10 0

time

10 -5

10 -10

10 -15

10 2

0

5

10

time

Mnist-RF-Iteration-l001-mu1

RCV1-Iteration-l001-mu1

Aloi-RF-Iteration-l001-mu1

DGPD
DualRCD
PrimalRCD
SPDC-dense

-10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15
0

200

400

600

800

-5

relative primal objective

10

10

-5

relative primal objective

relative primal objective

10 -2
10

10 -10

10

1000

0

200

400

600

800

10 -8

1000

0

50

100

Sector-Iteration-l001-mu1
DGPD
DualRCD
PrimalRCD
SPDC

10 -1

10 -3

DGPD
DualRCD
PrimalRCD
SPDC

10 -2

10

relative primal objective

10 -2

150

iter

Aloi-RB-Iteration-l001-mu1

relative primal objective

10 -1

relative primal objective

10 -6

iter

Mnist-RB-Iteration-l001-mu1

10

10 -4

DGPD
DualRCD
PrimalRCD
SPDC-dense

-15

iter

DGPD
DualRCD
PrimalRCD
SPDC

15

time

-3

10 -4

-4

10 -5

10 -10

10 -5

0

20

40

60

iter

80

100

10 -6

0

50

100

150

200

10 -15

0

200

iter

Figure 4. Relative Objective versus Time (the upper 2 rows) and versus # iterations (the lower 2 rows) for

400

600

iter

= 0.01, µ = 1.

800

1000

Doubly Greedy Primal-dual Coordinate Descent for Sparse Empirical Risk Minimization

Mnist-RF-Time-l001-mu01

RCV1-Time-l001-mu01

Aloi-RF-Time-l001-mu01

DGPD
DualRCD
PrimalRCD
SPDC

10 -10
DGPD
DualRCD
PrimalRCD
SPDC-dense

100

200

300

400

500

600

700

800

900

10 -5

10 -2

obj

10 -5

relative primal objective

relative primal objective

10 -1

10

10 -4
DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

1000

100

200

300

400

time

500

800

900

10 -3

-4

10

-4

10

-5

10 -5

10 -6

-6

0

10

2

10

0

10

600

800

10

10

-1

0

10

-2

200

400

600

800

10 -5

1000

10

0

10

10

Sector-Iteration-l001-mu01
DGPD
DualRCD
PrimalRCD
SPDC

10 -4

10 -5

10 -5

10 -10
DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -6

10 1

iter

10 2

2

iter

10 -3

obj

1

-1

10 -2

10 -6

14

10 -3

Aloi-RB-Iter-l001-mu01
10

10 -5

12

DGPD
DualRCD
PrimalRCD
SPDC

iter

10 -4

10

10 -4

1000

DGPD
DualRCD
PrimalRCD
SPDC

8

RCV1-Iter-l001-mu01

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

10 -3

10 0

6

-10

Mnist-RB-Iter-l001-mu01

10 -2

4

time

10 -5

iter

10 -1

2

relative primal objective

-15

400

2

obj

relative primal objective
DGPD
DualRCD
PrimalRCD
SPDC-dense

200

DGPD
DualRCD
PrimalRCD
SPDC-dense

Aloi-RF-Iteration-l001-mu01

10 -10

0

-10

time

Mnist-RF-Iteration-l001-mu01

10

10

10 -15

time

10 -5

10 2

Sector-Time-l001-mu01

relative primal objective

10 -3

10

10 0

DGPD
DualRCD
PrimalRCD
SPDC

obj

10 -2

obj

10 -2

10

10 -2

1000

time

10 -1

10 -5

relative primal objective

700

Aloi-RB-Time-l001-mu01
DGPD
DualRCD
PrimalRCD
SPDC

10 -1

obj

600

10 -5

time

Mnist-RB-Time-l001-mu01

10

10 -3

-10

10 0

10 1

10 2

10 -15

0

200

iter

Figure 5. Relative Objective versus Time (the upper 2 rows) and versus # iterations (the lower 2 rows) for

400

600

iter

= 0.01, µ = 0.1.

800

Doubly Greedy Primal-dual Coordinate Descent for Sparse Empirical Risk Minimization

RCV1-Time-l001-mu001
Mnist-RF-Time-l001-mu001

Aloi-RF-Time-l001-mu001
DGPD
DualRCD
PrimalRCD
SPDC-dense

10

-10

relative primal objective

10 -5

relative primal objective

relative primal objective

10 -2
10 -5

10

-10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

100

200

300

400

500

600

700

800

900

DGPD
DualRCD
PrimalRCD
SPDC-dense

10 -15

1000

100

200

300

400

time

500

600

700

800

900

10 -4

10 -6

10 -8

10 -2

1000

10 0

Mnist-RB-Time-l001-mu001

10 2

time

time

Aloi-RB-Time-l001-mu001
Sector-Time-l001-mu001
DGPD
DualRCD
PrimalRCD
SPDC

10 -3

10 -4

10 -2

10 -3

10 -4

10 0

DGPD
DualRCD
PrimalRCD
SPDC

relative primal objective

10 -2

DGPD
DualRCD
PrimalRCD
SPDC

10 -1

relative primal objective

relative primal objective

10 -1

10 2

10 0

time

10 -5

10 -10

10 -15

10 2

0

5

10

time

Mnist-RF-Iteration-l001-mu001

RCV1-Iteration-l001-mu001

Aloi-RF-Iteration-l001-mu001

DGPD
DualRCD
PrimalRCD
SPDC-dense

-10

relative primal objective

10

relative primal objective

relative primal objective

10 -2
10 -5

10 -5

10

-10

DGPD
DualRCD
PrimalRCD
SPDC-dense

10

200

400

600

800

10

1000

0

200

400

600

800

1000

0

50

100

Aloi-RB-Iteration-l001-mu001
DGPD
DualRCD
PrimalRCD
SPDC

Sector-Iteration-l001-mu001
DGPD
DualRCD
PrimalRCD
SPDC

10 -1

10 -2

10 -3

DGPD
DualRCD
PrimalRCD
SPDC

10 -2

10

relative primal objective

relative primal objective

10 -1

150

iter

iter

Mnist-RB-Iteration-l001-mu001

relative primal objective

10 -6

10 -8

-15

iter

10

10 -4

DGPD
DualRCD
PrimalRCD
SPDC-dense

-15

0

15

time

-3

10 -4

-4

10 -5

10 -10

10 -5

0

20

40

60

iter

80

100

10 -6

0

50

100

150

200

10 -15

0

200

iter

Figure 6. Relative Objective versus Time (the upper 2 rows) and versus # iterations (the lower 2 rows) for

400

600

800

iter

= 0.01, µ = 0.01.

1000

