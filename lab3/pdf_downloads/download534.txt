Online Learning with Local Permutations and Delayed Feedback

A. Proofs
A.1. Analysis Of The Delayed Permuted Mirror Descent Algorithm
We will use throughout the proofs the well known Pythagorean Theorem for Bregman divergences, and the ’projection’
lemma that considers the projection step in the algorithm.
Lemma 1. Pythagorean Theorem for Bregman divergences
Let v be the projection of w onto a convex set W w.r.t Bregman divergence 4ψ : v = argminu∈W 4ψ (u, w), then:
4ψ (u, w) ≥ 4ψ (u, v) + 4ψ (v, w)
Lemma 2. Projection Lemma
Let W be a closed convex set and let v be the projection of w onto W, namely,
v = argminkx − wk2 . Then, for every u ∈ W, kw − uk2 − kv − uk2 ≥ 0
x∈W

The following lemma gives a bound on the distance between two consequent predictions when using the Euclidean mirror
map:
Lemma 3. Let g ∈ Rn s.t. kgk2 < G, W a convex set, and η > 0 be fixed. Let w ∈ W and w2 = w − η · g. Then, for
w0 = argminkw2 − uk22 , we have that kw − w0 k ≤ η · G
u∈W

Proof. From the projection lemma: kw2 − wk22 ≥ kw0 − wk22 and so: kw2 − wk2 ≥ kw0 − wk2 . From definition:
kw2 − wk2 = kη · gk2 ≤ η · G. and so we get: kw0 − wk2 ≤ kw2 − wk2 ≤ η · G
We prove a modification of Lemma 2 given in (Menache et al., 2014) in order to bound the distance between two consequent
predictions when using the negative entropy mirror map:
Lemma 4. Let g ∈ Rn s.t. kgk1 ≤ G for some G > 0 and let η > 0 be fixed, with η <
w in the n − simplex, if we define w0 to be the new distribution vector

√1 .
2·G

For any distribution vector

wi · exp (−η · gi )
∀i ∈ {1, ..., n} , wi0 = Pn
j=1 wj · exp (−η · gj )
Then kw − w0 k1 ≤ 3ηG
√1
2·G

we get that ∀i : |η · gi | < 1. We have that:

!
n
n 

X
X
exp
(−η
·
g
)


i
kw − w0 k1 =
|wi − wi0 | =

wi · 1 − Pn


w
·
exp
(−η
·
g
)
j
j=1 j

Proof. Since kgk∞ < G and η <

i=1

i=1

Since kwk1 = 1, we can apply Holder’s inequality, and upper bound the above by




exp (−η · gi )


max 1 − Pn

i 

w
·
exp
(−η
·
g
)
j
j
j=1
Using the inequality 1 − x ≤ exp (−x) ≤

1
1+x

for all |x| ≤ 1, we know that

1 − η · gi ≤ exp (−η · gi ) ≤

1
1 + η · gi

and since −ηG ≤ η · gi ≤ ηG we have that
1 − η · gi ≤ exp (−η · gi ) ≤

1
1
≤
1 + η · gi
1 − ηG

and so we get:
1−

1
1+ηgi

1 + ηG

exp (−η · gi )
1 − η · gi
≤1−
1
w
·
exp
(−η
·
g
)
j
j
j=1
1−ηG

≤ 1 − Pn

Online Learning with Local Permutations and Delayed Feedback

Using again the fact that −ηG ≤ η · gi ≤ ηG, we have
1
1−ηG

1−

=⇒

1 + ηG

1 − ηG
exp (−η · gi )
≤1−
1
w
·
exp
(−η
·
g
)
j
j
j=1
1−ηG

≤ 1 − Pn

1
exp (−η · gi )
−η 2 G2
2
=1−
≤ 1 − Pn
≤ 1 − (1 − ηG) = 2ηG + η 2 G2
1 − η 2 G2
1 − η 2 G2
w
·
exp
(−η
·
g
)
j
j=1 j

Now, since ηG < 1, we get that:
−η 2 G2
exp (−η · gi )
≤ 1 − Pn
≤ 2ηG + ηG = 3ηG
2
2
1−η G
j=1 wj · exp (−η · gj )
and so we can conclude that









 −η 2 G2 
exp (−η · gi )
ηG




≤
max
,
|3ηG|
≤
max
max 1 − Pn
,
3ηG

 1 − η 2 G2 
i
i
i 
1 − η 2 G2
j=1 wj · exp (−η · gj ) 
Since η <

√1 ,
2G

2

we get (η · G) < 12 . Thus we get:




exp (−η · gi )


max 1 − Pn
 ≤ max (2ηG, 3ηG) ≤ 3ηG
i
i 

w
·
exp
(−η
·
g
)
j
j
j=1

which gives us our desired bound.
With the above two lemmas in hand, we bound the distance between consequent predictors by cηG, where c is a different
constant in each mirror map: c = 1 for the euclidean case, and c = 3 for the negative entropy mirror map.
Note that both mapping are 1-strongly convex with respect to their respective norms. For other mappings with a different
strong convexity constant, one would need to scale the step sizes according to the strong convexity parameter in order to
get the bound.
A.1.1. P ROOF OF T HEOREM 1
We provide an upper bound on the regret of the algorithm, by competing against the best fixed action in each one of the
sets of iterations- the first τ iterations and the last M − τ iterations in each block. This is an upper bound on competing
against the best fixed predictor in hindsight for the entire sequence. Formally, we bound:
" T
#
T
X
X
R(T ) = E
ft (wt ) −
ft (w∗ )
t=1

t=1

T 
−1
MX
·i+τ
M
X


≤ E
ft (wt ) − ft wf∗ +
i=0

t=M ·i+1



M ·(i+1)

X

ft (wt ) − ft (ws∗ )

t=M ·i+τ +1

where
T
M

wf∗

= argmin
w∈W

−1
X

MX
·i+τ

i=0 t=M ·i+1

T
M

ft (w) and

ws∗

= argmin
w∈W

−1
X

M ·(i+1)

X

ft (w)

i=0 t=M ·i+τ +1

where expectation is taken over the randomness of the algorithm.


The diameter of the domain W is bounded by B 2 , and so 4ψ wf∗ , w0f ≤ B 2 and 4ψ (ws∗ , w0s ) ≤ B 2 . We start with a
general derivation that will apply both for ws and for wf simultaneously. For the following derivation we use the notation
wj , wj+1 omitting the f, s superscript, for denoting subsequent updates of the predictor vector, whether it is ws or wf .

Online Learning with Local Permutations and Delayed Feedback





Denote by gj the gradient used to update wj , i.e., ∇ψ wj+ 21 = ∇ψ (wj ) − η · gj , and wj+1 = argmin4ψ w, wj+ 12 .
w∈W



1
and thus:
Looking at the update step in the algorithm, we have that gj = η · ∇ψ (wj ) − ∇ψ wj+ 21
1
η
1
=
η

hwj − w∗ , gj i =

D
E


· wj − w∗ , ∇ψ (wj ) − ∇ψ wj+ 12





· 4ψ (w∗ , wj ) + 4ψ wj , wj+ 12 − 4ψ w∗ , wj+ 12

We now use the Pythagorean Theorem to get:




1 
≤ · 4ψ (w∗ , wj ) + 4ψ wj , wj+ 12 − 4ψ (w∗ , wj+1 ) − 4ψ wj+1 , wj+ 12
η
When we sum terms for all updates of the predictor, wf or ws respectively, the terms 4ψ (w∗ , wj ) − 4ψ (w∗ , wj+1 ) will
result
sum,

 canceling all
 terms expect the first and last. Thus we now concentrate on bounding the term:
 in a telescopic
4ψ wj , wj+ 21 − 4ψ wj+1 , wj+ 21 .



D

E

4ψ wj , wj+ 12 − 4ψ wj+1 , wj+ 12 = ψ (wj ) − ψ (wj+1 ) − wj − wj+1 , ∇ψ wj+ 21
D

E 1
≤
wj − wj+1 , ∇ψ (wj ) − ∇ψ wj+ 21
− · kwj − wj+1 k2
2
ψ 1-strong convex
1
= hwj − wj+1 , η · gj i − · kwj − wj+1 k2
2
1
≤ η · G · kwj − wj+1 k − · kwj − wj+1 k2
2
2
(η · G)
≤
2

2
√
√
where the last inequality stems from the fact that kwj − wj+1 k · √12 − η·G
≥0
2

T
We now continue with the analysis referring to wf and ws separately. Summing over j = τ + 1 to M
+ 1 · τ for wf
T
T
(these are the M
τ iterations in which the first sub-algorithm is in use), and from j = 1 to M
· (M − τ ) for ws (these are
T
the M (M − τ ) iterations in which the second sub-algorithm is in use) we get:
For wf :
T
+1 ·τ
(M
E
X) D f
wj − wf∗ , gj

j=τ +1
T
+1 ·τ
(M

E
X) D f
f
wj − wf∗ , ∇fT1 (j−τ ) wj−τ
=

j=τ +1

=

T
+1 ·τ
(M
X)

j=τ +1

=

T
+1 ·τ
(M
X)

j=τ +1

≤

T
+1 ·τ
(M
X)

j=τ +1

1
≤ ·
η


 

E
1 D f
s
· wj − wf∗ , ∇ψ wjf − ∇ψ wj+
1
2
η






1 
f
f
· 4ψ wf∗ , wjf + 4ψ wjf , wj+
− 4ψ wf∗ , wj+
1
1
2
2
η








1 
f
f
f
f
, wj+
· 4ψ wf∗ , wjf + 4ψ wjf , wj+
− 4ψ wf∗ , wj+1
− 4ψ wj+1
1
1
2
2
η

T
+1 ·τ
(M
X)

j=τ +1

4ψ



wf∗ , wjf



− 4ψ



f
wf∗ , wj+1



1
+ ·
η

T
+1 ·τ
(M
X)

j=τ +1





f
f
f
4ψ wjf , wj+
− 4ψ wj+1
, wj+
1
1
2

2

Online Learning with Local Permutations and Delayed Feedback
T
+1 ·τ
(M








X)
1
1
f
f
f
f
f
∗
∗
= · 4ψ wf , wτ +1 − 4ψ wf , w T +1 ·τ + ·
, wj+
4ψ wjf , wj+
− 4ψ wj+1
1
1
(M )
2
2
η
η j=τ +1

2

≤



1 T
(ηf · G)
1
· 4ψ wf∗ , wτf +1 +
·
·τ ·
ηf
ηf M
2

≤

1
T
ηf · G2
· B2 +
·τ ·
ηf
M
2

For ws :
T
M

·(M −τ )

X


 s

wj − ws∗ , gj

j=1
T
M

·(M −τ )

X

=


 s

wj − ws∗ , ∇fT2 (j) wjs

j=1
T
M

·(M −τ )

X

=

j=1
T
M

·(M −τ )

X

=

j=1
T
M

≤

·(M −τ )

X
j=1

1
≤ ·
η

T
M



E

1 D s
s
· wj − ws∗ , ∇ψ wjs − ∇ψ wj+
1
2
η





1 
s
s
· 4ψ ws∗ , wjs + 4ψ wjs , wj+
− 4ψ ws∗ , wj+
1
1
2
2
η






1 
s
s
s
s
· 4ψ ws∗ , wjs + 4ψ wjs , wj+
− 4ψ ws∗ , wj+1
− 4ψ wj+1
, wj+
1
1
2
2
η

·(M −τ )

X
j=1






 1
s
s
s
s
− 4ψ wj+1
, wj+
4ψ ws∗ , wjs − 4ψ ws∗ , wj+1
+ · 4ψ wjs , wj+
1
1
2
2
η

 1

1
= · 4ψ (ws∗ , w1s ) − 4ψ ws∗ , w(s T +1)·τ + ·
M
η
η

T
M

·(M −τ )

X





s
s
s
4ψ wjs , wj+
− 4ψ wj+1
, wj+
1
1
2

2

j=1
2

(ηs · G)
1
1 T
· (M − τ ) ·
· 4ψ (ws∗ , w1s ) +
·
ηs
ηs M
2
2
1
T
ηs · G
≤
· B2 +
· (M − τ ) ·
ηs
M
2
≤

We are after bounding the regret, which in itself is upper bounded by the sum of the regret accumulated by each subalgorithm, considering iterations in the first τ and last M −τ per block separately, as mentioned above. Using the convexity
of ft for all t, we bound these terms:
T

T
−1 M ·i+τ
−1 M ·(i+1)
M
M
X
X
X
X

E
ft (wt ) − ft wf∗ +
ft (wt ) − ft (ws∗ )
i=0 t=M ·i+1

T
−1
M
X

≤E

MX
·i+τ

i=0 t=M ·i+τ +1
T

−1
X


 M
∗
wt − wf , ∇ft (wt ) +

i=0 t=M ·i+1

T
M ·τ D
 E
X
= E
wjf − wf∗ , ∇fT1 (j) wjf
+
j=1



M ·(i+1)

X

hwt − ws∗ , ∇ft (wt )i

i=0 t=M ·i+τ +1
T
M



·(M −τ )

X




wjs − ws∗ , ∇fT2 (j)+τ wj


s



j=1

In the last equality of the above derivation, we simply replace notations, writing the gradient ∇ft (wt ) in notation of T1

Online Learning with Local Permutations and Delayed Feedback

and T2 . T1 contains all time points in the first τ iterations of each block, and T2 contains all time points in the first M − τ
iterations of each block.
T
P( M
+1)·τ
f
f
∗
for wf and
Note that what we have bounded so far is
j=τ +1 hwj − wf , ∇fT1 (j−τ ) (wj−τ )i
T

PM
·(M −τ ) 
 s
wj − ws∗ , ∇fT2 (j) wjs for ws , which are not the terms we need to bound in order to get a regret
j=1
bound since they use the delayed gradient, and so we need to take a few more steps in order to be able to bound the regret.
We begin with wf :
T
M

−1
X

T

MX
·i+τ




wt −

wf∗ , ∇ft

M ·τ D
 E
X
wjf − wf∗ , ∇fT1 (j) wjf
(wt ) =

i=0 t=M ·i+1



j=1
T

M ·τ D
 E D
 E
X
f
f
=
wj+τ
− wf∗ , ∇fT1 (j) wjf
+ wjf − wj+τ
, ∇fT1 (j) wjf

j=1
T
+1 ·τ
(M
E

E D

X) D f
f
f
f
− wjf , ∇fT1 (j−τ ) wj−τ
+ wj−τ
wj − wf∗ , ∇fT1 (j−τ ) wj−τ
=

j=τ +1

1
ηf · G2
T
≤
·τ ·
+
· B2 +
ηf
M
2
1
T
ηf · G2
≤
· B2 +
·τ ·
+
ηf
M
2
ηf · G2
T
1
·τ ·
+
· B2 +
≤
ηf
M
2

T
+1 ·τ
(M

E
X) D f
f
wj−τ − wjf , ∇fT1 (j−τ ) wj−τ

j=τ +1
T
+1 ·τ
(M
X)



f
f
kwj−τ
− wjf k · k∇fT1 (j−τ ) wj−τ
k

j=τ +1
T
+1 ·τ τ
(M
X) X

j=τ +1

f
f
kwj−i
− wj−i+1
k·G

i=1

The last term in the above derivation, is the sum of differences between consecutive predictors. This difference, is determined by the mirror map in use, the step size ηf , and the bound over the norm of the gradient used in the update stage of the
algorithm, G. This is because every consecutive predictor is received by taking a gradient step from the previous predictor,
in the dual space, with a step size ηf , and projecting back to the primal space by use of the bregman divergence with the
f
specific mirror map in use. We denote the bound on this difference by Ψ(ηf ,G) , i.e., ∀j, j + 1 : kwjf − wj+1
k ≤ Ψ(ηf ,G) .
Continuing our derivation, we have:
1
T
ηf · G2
≤
· B2 +
·τ ·
+
ηf
M
2

T
+1 ·τ τ
(M
X) X

j=τ +1

Ψ(ηf ,G) · G

i=1

1
T
ηf · G2
T
· B2 +
·τ ·
+
· τ 2 · Ψ(ηf ,G) · G
ηf
M
2
M

≤

Since this upper bound does not depend on the permutation,and holds for every sequence, it holds also in expectation, i.e.
T

−1 M ·i+τ
M
X
X

1
T
ηf · G2
T
E
ft (wt ) − ft wf∗  ≤
· B2 +
·τ ·
+
· τ 2 · Ψ(ηf ,G) · G
η
M
2
M
f
i=0
t=M ·i+1

We now turn to ws
T
M

−1
X

M ·(i+1)

X

ft (wt ) − ft (ws∗ )

i=0 t=M ·i+τ +1
T
M

≤

−1
X

M ·(i+1)

X

i=0 t=M ·i+τ +1

hwt − ws∗ , ∇ft (wt )i

Online Learning with Local Permutations and Delayed Feedback
T
M

·(M −τ )

X

=



 s
wj − ws∗ , ∇fT2 (j)+τ wjs

j=1
T
M

X

=

T
M

·(M −τ )

·(M −τ )

X


 s

wj − ws∗ , ∇fT2 (j) wjs +

j=1

≤


 s


wj − ws∗ , ∇fT2 (j)+τ wjs − ∇fT2 (j) wjs

j=1

T
ηs · G2
1
· B2 +
· (M − τ ) ·
+
ηs
M
2

T
M

·(M −τ )

X






wjs − ws∗ , ∇fT2 (j)+τ wjs − ∇fT2 (j) wjs

j=1





We now look at the expression wjs − ws∗ , ∇fT2 (j)+τ wjs − ∇fT2 (j) wjs for any j.
We first notice that for any j, wjs only depends on gradients of time points: T2 (1) , T2 (2) , ..., T2 (j − 1).
We also notice that given the functions received at these time points, i.e, given fT2 (1) , fT2 (2) , ..., fT2 (j−1) , wjs is no longer
a random variable.
We have that for all j, T2 (j) and T2 (j) + τ are both time points that are part of the same M -sized block. Suppose we have
observed n functions of the block to which T2 (j) and T2 (j) + τ belong. All of these n functions are further in the past
than both T2 (j) and T2 (j) + τ , because of the delay of size τ . We have M − n functions in the block that have not been
observed yet, and since we performed a random permutation within each block, all remaining functions in the block have
the same expected value. Formally, given wjs , the expected value of the current and delayed gradient are the same, since we



PM −n
have: E[∇fT2 (j)+τ wjs |wjs ] = M1−n · i=1 ∇fT2 (j)+i wjs = E[∇fT2 (j) wjs |wjs ]. As mentioned above, this stems
from the random permutation we performed within the block - all M − n remaining functions (that were not observed yet
in this block) have an equal (uniform) probability of being in each location, and thus the expected value of the gradients is
equal. From the law of total expectation we have that




E[∇fT2 (j)+τ wjs ] = E[E[∇fT2 (j)+τ wjs |wjs ]] = E[E[∇fT2 (j) wjs |wjs ]] = E[∇fT2 (j) wjs ]


ans thus E[∇fT2 (j)+τ wjs − ∇fT2 (j) wjs ] = 0.




We get that E[ wjs − ws∗ , ∇fT2 (j)+τ wjs − ∇fT2 (j) wjs ] = 0
So we have that the upper bound on the expected regret of the time point in which we predict with ws is:
T
−1
M
X

E



M ·(i+1)

X

ft (wt ) − ft (ws∗ ) ≤

i=0 t=M ·i+τ +1

ηs · G2
T
1
· (M − τ ) ·
· B2 +
ηs
M
2

Summing up the regret of the two sub-algorithms, we get:

E

" T
X

T
−1
M
X
ft (wt ) − ft (w∗ ) ≤ E 
#

T
M

MX
·i+τ


∗

ft (wt ) − ft wf +

i=0 t=M ·i+1

t=1



M ·(i+1)

X

ft (wt ) − ft (ws∗ )

i=0 t=M ·i+τ +1

2

≤

−1
X

2

2

B
Tτ G
Tτ
B2
T · (M − τ ) G2
+ ηf ·
·
+
· G · Ψ(ηf ,G) +
+ ηs ·
·
ηf
M
2
M
ηs
M
2

which gives us the bound.
For Ψ(ηf ,G) ≤ c · ηf · G where c is some constant, choosing the step sizes, ηf , ηs optimally:
B·

ηf =
G·

√

q
T ·τ ·

M
1
2

√
B · 2M
p
 , ηs =
G · T · (M − τ )
+c·τ

Online Learning with Local Permutations and Delayed Feedback

we get the bound:
" T
#
X
∗
E
ft (wt ) − ft (w )
t=1

r
=

T ·τ
·B·G·
M

r

1
+c·τ +
2

r

T ·τ
1
·B·G· q
+
M
1
2 + cτ

r

T ·τ
cτ
·B·G· q
M
1
2 + cτ

r

2 · T · (M − τ )
·B·G
M
r
r
r
T ·τ
1
2 · T · (M − τ )
≤c·
·B·G·
+c·τ +
·B·G
M
2
M
!
!!
r
r
r
√
T · τ2
T · (M − τ )
τ2
T·
=O
+
=O
+1
M
M
M

+

A.2. Lower Bound For Algorithms With No Permutation Power
Theorem 3. For every (possible randomized) algorithm A, there exists a choice of linear, 1-Lipschitz functions over
[−1, 1] ⊂ R, with τ a fixed size delay of feedback, such that the expected regret of A after T rounds (with respect to the
algorithm’s randomness), is
" T
#
T
T
√ 
X
X
X
∗
∗
τ T , where w = argmin
ft (w)
E [RA (T )] = E
ft (wt ) −
ft (w ) = Ω
t=1

w∈W

t=1

t=1

Proof. First, we note that in order to show that for every algorithm, there exists a choice of loss functions by an oblivious
adversary, such that the expected regret of the algorithm is bounded from below, it is enough to show that there exists a
distribution over loss function sequences such that for any algorithm, the expected regret is bounded from below, where
now expectation is taken over both the randomness of the algorithm and the randomness of the adversary. This is because
if there exists such a distribution over loss function sequences, then for any algorithm, there exists some sequence of loss
functions that can lead to a regret at least as high. To put it formally, if we mark E the expectation over the randomness of
alg

the algorithm, and

E

f1 ,...,fT

the expectation over the randomness of the adversary, then:

∃ a (randomized) adversary s.t. ∀ algorithm A,

E [RA (T )] > Ω
f1 ,...,fT alg
√ 
∀ algorithm A, ∃f1 , ..., fT s.t. E [RA (T )] > Ω
τT
E

√

τT



→

alg

Thus, we prove the first statement above, that immediately gives us the second statement which gives the lower bound.
We consider the setting where W = [−1, 1], and ∀t ∈ [1, T ] : ft (wt ) = αt · wt where αt ∈ {1, −1}. We divide the T
rounds to blocks of size τ . αt is chosen in the following way: if αt is the first α in the block, it is randomly picked, i.e,
Pr (α = ±1) = 21 . Following this random selection, the next τ − 1 α’s of the block will be identical to the first α in it, so
that we now have a block of τ consecutive functions in which α is identical. We wish to lower bound the expected regret
of any algorithm in this setting.
Consider a sequence of predictions by the algorithm w1 , w2 , ..., wT . Denote by αi,j the j’th α in the i’th block, and
similarly for wi,j , fi,j . We denote the entire sequence of α’s by ᾱ(1→T ) , and the sequence of α’s until time point j in block
i by ᾱ(1→i,j) . Notice that wi,j is a function of the α’s that arrive up until time point i · τ + j − τ − 1. We denote these α’s
as ᾱ(1→i,j−τ −1) .
Then the expected sum of losses is:
 T

" T
#
τ
τ X
X
X
E
ft (wt ) = E 
fi,j (wi,j )
t=1

i=1 j=1

Online Learning with Local Permutations and Delayed Feedback
T

=

τ
τ X
X

E [fi,j (wi,j )]

i=1 j=1
T

=

τ
τ X
X

Eᾱ(1→T ) [αi,j · wi,j ]

i=1 j=1
T

=

τ
τ X
X




Eᾱ(1→i,j−τ −1) Eᾱ(i,j−τ →T ) αi,j · wi,j |ᾱ(1→i,j−τ −1)

i=1 j=1
T

=

τ
τ X
X




Eᾱ(1→i,j−τ −1) wi,j · Eᾱ(i,j−τ →T ) αi,j |ᾱ(1→i,j−τ −1)

i=1 j=1
T

=

τ
τ X
X




Eᾱ(1→i,j−τ −1) wi,j · Eᾱ(i,1→i,j) αi,j |ᾱ(1→i,j−τ −1)

i=1 j=1
T

=

τ
τ X
X



Eᾱ(1→i,j−τ −1) wi,j · Eαi,1 [αi,1 ]

i=1 j=1
T

=

τ
τ X
X




1
1
Eᾱ(1→i,j−τ −1) wi,j ·
· 1 + · (−1)
=0
2
2
i=1 j=1
1
2

to be either +1 or −1.
P

T
We now continue to the expected sum of losses for the optimal choice of w∗ = argminw∈W
f
(w)
. Note that in
t
t=1
The last equality is true because every first α in any block has probability

this setting, w∗ ∈ {+1, −1} and is with opposite sign to the majority of α’s in the sequence.

"
E

T
X


 T

 T
τ
τ
τ X
τ X
X
X
fi,j (w∗ ) = E 
αi,j · w∗ 
ft (w∗ ) = E 
#

t=1

i=1 j=1

i=1 j=1

 T

 T

τ
τ
X
X
= E
τ · αi,1 · w∗  = τ · E 
αi,1 · w∗ 
i=1

i=1


= −τ · E |



T
τ

X

αi,1 |

i=1

Using Khintchine inequality we have that:
v

u T
r
u X
τ
√

X
T
u
2




t
−τ · E |
αi,1 · 1| ≤ −τ · C ·
1
= −Ω
τ ·T
= −τ · C ·
τ
i=1
i=1


T
τ



where C is some constant.
Thus we get that for a sequence of length T the expected regret is:
" T
#
" T
#
√

X
X
∗
E
ft (wt ) − E
ft (w ) = Ω
τ ·T
t=1

t=1

Online Learning with Local Permutations and Delayed Feedback

A.3. Proof of Theorem 2
Proof. First, we note that to show that for every algorithm, there exists a choice of loss functions by an oblivious adversary,
such that the expected regret of the algorithm is bounded from below, it is enough to show that there exists a distribution
over loss function sequences such that for any algorithm, the expected regret is bounded from below, where now expectation
is taken over both the randomness of the algorithm and the randomness of the adversary. This is because if there exists
such a distribution over loss function sequences, then for any algorithm, there exists some sequence of loss functions that
can lead to a regret at least as high. To put it formally, if we mark E the expectation over the randomness of the algorithm,
alg

and

E

f1 ,...,fT

the expectation over the randomness of the adversary, then:
∃ a (randomized) adversary s.t. ∀ algorithm A,

E [RA (T )] > Ω
f1 ,...,fT alg
√ 
∀ algorithm A, ∃f1 , ..., fT s.t. E [RA (T )] > Ω
τT
E

√

τT



→

alg

Thus, we prove the first statement above, that immediately gives us the second statement which is indeed our lower bound.
We consider the setting where W = [−1, 1], and ∀t ∈ [1, T ] : ft (wt ) = αt · wt where αt ∈ {1, −1}. We start by
constructing our sequence of α’s. We divide the T iterations to blocks of size τ3 . In each block, all α’s are identical, and are
chosen to be +1 or −1 w.p. 21 . This choice gives us blocks of τ3 consecutive functions in which α is identical within each
block. Let M be a permutation window of size smaller than τ3 . We notice first that since M < τ3 and the sequence of α’s is
organized in blocks of size τ3 , then even after permutation, the time difference between the first and last time we encounter
an α is ≤ τ , which means we will not get the feedback from the first time we encountered this α before encountering the
next one, and we will not be able to use it for correctly predicting α’s of this (original) block that arrive later. This is the
main idea that stands in the basis of this lower bound.
Formally, consider a sequence of w1 , w2 , ..., wT chosen by the algorithm. Denote by αi,j the j’th α in the i’th block,
and similarly for wi,j , fi,j . We denote the entire sequence of α’s by ᾱ(1→T ) , and the sequence of α’s until time point
j in block i by ᾱ(1→i,j) . For simplicity we will denote βt as the α that was presented attime t, after permutation, i.e.
βt := ασ− 1(()t) . Notice that wi,j is a function of the β’s that arrive up until time point i · τ3 + j − τ − 1. We denote these

β’s as β̄(1→i,j−τ −1) . I.e wi,j = g β̄(1→i,j−τ −1) where g is some function.
Going back to our main idea of the construction, we can put it in this new terminology- since the delay is τ and the
permutation window is M < τ3 , for any i, j, the first time we encountered ασ−1 (i,j) is less than τ iterations ago, and thus,

βi,j is independent of β̄(1→i,j−τ −1) , while wi,j is a function of it: wi,j = g β̄(1→i,j−τ −1) .
With this in hand, we look at the sum of losses of the predictions of the algorithm, w1 , w2 , ..., wT :
T τ τ

" T
#
/3 3
X
X
X
E
ft (wt ) = E 
fi,j (wi,j )
t=1

i=1 j=1
T/ τ
3

=

τ

3
XX

E [fi,j (wi,j )]

i=1 j=1
T τ

=

τ

/3 3
X
X

Eβ̄(1→T ) [βi,j · wi,j ]

i=1 j=1
T τ

=

τ

/3 3
X
X

h

i
Eβ̄(1→i,j−τ −1) Eβ̄(i,j−τ →T ) βi,j · wi,j |β̄(1→i,j−τ −1)

i=1 j=1
T/ τ
3

=

τ

3
XX

h

i
Eβ̄(1→i,j−τ −1) wi,j · Eβ̄(i,j−τ →T ) βi,j |β̄(1→i,j−τ −1)

i=1 j=1
T/ τ
3

=

τ

3
XX

i=1 j=1

h

i
Eβ̄(1→i,j−τ −1) wi,j · Eβ̄(i,j−τ →T ) ασ−1 (i,j)

Online Learning with Local Permutations and Delayed Feedback
T τ

=

τ

/3 3
X
X




1
1
Eβ̄(1→i,j−τ −1) wi,j ·
· 1 + · (−1)
=0
2
2
i=1 j=1

where the last equality stems from the fact that βi,j = ασ−1 (i,j) is equal to the expected value of the first time we
encountered the α that corresponds to ασ−1 (i,j) , i.e, the first α that came from the same block of ασ−1 (i,j) . This expectation
is 0 since we choose α = 1 or α = −1 with probability 12 for each block.
P

T
We now continue to the expected sum of losses for the optimal choice of w∗ = argminw∈W
t=1 ft (w) . Note that
after permutation, the expected sum of losses of the optimal w remains the same since it is best predictor over the entire
sequence, and so for simplicity we look at the sequence of α’s as it is chosen initially. Also, in this setting, w∗ ∈ {+1, −1}
and is with opposite sign to the majority of α’s in the sequence.

"
E

T
X

T τ τ


T τ τ
/3 3
/3 3
X
X
X
X
ft (w∗ ) = E 
fi,j (w∗ ) = E 
αi,j · w∗ 
#

t=1

i=1 j=1

i=1 j=1


T τ

T τ
/3
/3
X
X
τ
τ
αi,1 · w∗ 
· αi,1 · w∗  = · E 
= E
3
3
i=1
i=1
 T τ

/3
X
τ
= − · E |
αi,1 |
3
i=1
Using Khintchine inequality we have that:
v
u T/ τ 
s
u X
3
τ
τ
T
τ
u
αi,1 · 1| ≤ − · C · t
12  = − · C · τ
− · E |
3
3
3
3
i=1
i=1


T τ

/3
X



r
= −Ω

τ
·T
3


= −Ω

√

τ ·T



where C is some constant.
Thus we get that overall expected regret for any algorithm with permutation power M <

E

" T
X
t=1

as in the adversarial case.

#
ft (wt ) − E

"

T
X
t=1

#
∗

ft (w ) = Ω

√

τ ·T



τ
3

is:

