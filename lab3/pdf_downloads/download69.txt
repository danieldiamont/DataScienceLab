Supplementary Material to ‚ÄúDynamic Word Embeddings‚Äù

Robert Bamler 1 Stephan Mandt 1

Table 1. Hyperparameters for skip-gram filtering and skip-gram
smoothing.

PARAMETER C OMMENT
L = 104
L0 = 103
d = 100
d = 200
Ntr = 5000
Ntr0 = 5000

vocabulary size
batch size for smoothing
embedding dimension for SoU and Twitter
embedding dimension for Google books
number of training steps for each t (filtering)
number of pretraining steps with minibatch
sampling (smoothing; see Algorithm 2)
Ntr = 1000 number of training steps without minibatch
sampling (smoothing; see Algorithm 2)
cmax = 4
context window size for positive examples
Œ∑=1
ratio of negative to positive examples
Œ≥ = 0.75
context exponent for negative examples
D = 10‚àí3 diffusion const. per year (Google books & SoU)
D=1
diffusion const. per year (Twitter)
œÉ02 = 1
variance of overall prior
Œ± = 10‚àí2 learning rate (filtering)
Œ±0 = 10‚àí2 learning rate during minibatch phase (smoothing)
Œ± = 10‚àí3 learning rate after minibatch phase (smoothing)
Œ≤1 = 0.9
decay rate of 1st moment estimate
Œ≤2 = 0.99
decay rate of 2nd moment estimate (filtering)
Œ≤2 = 0.999 decay rate of 2nd moment estimate (smoothing)
Œ¥ = 10‚àí8 regularizer of Adam optimizer

1. Dimensionality Reduction in Figure 1
To create the word-clouds in Figure 1 of the main text we
mapped the fitted word embeddings from Rd to the twodimensional plane using dynamic t-SNE (Rauber et al.,
2016). Dynamic t-SNE is a non-parametric dimensionality reduction algorithm for sequential data. The algorithm finds a projection to a lower dimension by solving
a non-convex optimization problem that aims at preserving
nearest-neighbor relations at each individual time step. In
1
Disney Research, 4720 Forbes Avenue, Pittsburgh,
PA 15213, USA. Correspondence to:
Robert Bamler
<Robert.Bamler@disneyresearch.com>,
Stephan
Mandt
<Stephan.Mandt@disneyresearch.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

addition, projections at neighboring time steps are aligned
with each other by a quadratic penalty with prefactor Œª ‚â• 0
for sudden movements.
There is a trade-off between finding good local projections
for each individual time step (Œª ‚Üí 0), and finding smooth
projections (large Œª). Since we want to analyze the smoothness of word embedding trajectories, we want to avoid
bias towards smooth projections. Unfortunately, setting
Œª = 0 is not an option since, in this limit, the optimization
problem is invariant under independent rotations at each
time, rendering trajectories in the two-dimensional projection plane meaningless. To still avoid bias towards smooth
projections, we anneal Œª exponentially towards zero over
the course of the optimization. We start the optimizer with
Œª = 0.01, and we reduce Œª by 5% with each training step.
We run 100 optimization steps in total, so that Œª ‚âà 6√ó10‚àí6
at the end of the training procedure. We used the opensource implementation,1 set the target perplexities to 200,
and used default values for all other parameters.

2. Hyperparemeters and Construction of n¬±
1:T
Table 1 lists the hyperparameters used in our experiments.
For the Google books corpus, we used the same context
window size cmax and embedding dimension d as in (Kim
et al., 2014). We reduced d for the SoU and Twitter corpora
to avoid overfitting to these much smaller data sets.
In constrast to word2vec, we construct our positive and
negative count matrices n¬±
ij,t deterministically in a preprocessing step. As detailed below, this is done such that it
resembles as closely as possible the stochastic approach in
word2vec (Mikolov et al., 2013). In every update step,
word2vec stochastically samples a context window size
uniformly in an interval [1, ¬∑ ¬∑ ¬∑ , cmax ], thus the context size
fluctuates and nearby words appear more often in the same
context than words that are far apart from each other in
the sentence. We follow a deterministic scheme that results in similar statistics. For each pair of words (w1 , w2 )
in a given sentence, we increase the counts n+
iw1 jw2 by
max (0, 1 ‚àí k/cmax ), where 0 ‚â§ k ‚â§ cmax is the number of words that appear between w1 and w2 , and iw1 and
jw2 are the words‚Äô unique indices in the vocabulary.
1

https://github.com/paulorauber/thesne

Supplementary Material to ‚ÄúDynamic Word Embeddings‚Äù

Algorithm 1 Skip-gram filtering; see section 4 of the main
text.
Remark: All updates are analogous for word and context vectors; we drop their indices for simplicity.
Input: number of time steps T , time stamps œÑ1:T , positive and negative examples n¬±
1:T , hyperparameters.
Init. prior means ¬µÃÉik,1 ‚Üê 0 and variances Œ£ÃÉi,1 = Id√ód
Init. variational means ¬µik,1 ‚Üê 0 and var. Œ£i,1 = Id√ód
for t = 1 to T do
if t 6= 1 then
Update approximate Gaussian prior with params.
¬µÃÉik,t and Œ£ÃÉi,t using ¬µik,t‚àí1 and Œ£i,t‚àí1 , see Eq. 13.
end if
Compute entropy Eq [log q(¬∑)] analytically.
Compute expected log Gaussian prior with parameters
¬µÃÉik,t and Œ£ÃÉk,t analytically.
Maximize Lt in Eq. 11, using black-box VI with the
reparametrization trick.
Obtain ¬µik,t and Œ£i,t as outcome of the optimization.
end for
We also used a deterministic variant of word2vec to construct the negative count matrices n‚àí
t . In word2vec, Œ∑ negative samples (i, j) are drawn for each positive sample (i, j 0 )
by drawing Œ∑ independent values for j from a distribution
Pt0 (j) defined below. We define n‚àí
ij,t such that it matches
the expectation value of the number of times that word2vec
would sample the negative word-context pair (i, j). Specifically, we define
PL
+
j=1 nij,t
Pt (i) = PL
,
(1)
+
i0 ,j=1 ni0 j,t
Œ≥
Pt (j)
(2)
Pt0 (j) = PL
 ,
0 Œ≥
j 0 =1 Pt (j )
 X

L
+
0
n‚àí
=
n
(3)
0
0
ij,t
i j ,t Œ∑Pt (i)Pt (j).
i0 ,j 0 =1

We chose Œ≥ = 0.75 as proposed in (Mikolov et al., 2013),
and we set Œ∑ = 1. In practice, it is not necessary to explicitly construct the full matrices n‚àí
t , and it is more efficient
to keep only the distributions Pt (i) and Pt0 (j) in memory.

Algorithm 2 Skip-gram smoothing; see section 4. We drop
indices i, j, and k for word, context, end embedding dimension, respectively, when they are clear from context.
Input: number of time steps T , time stamps œÑ1:T , wordcontext counts n+
1:T , hyperparameters in Table 1
Obtain n‚àí
‚àÄt
using
Eqs. 1‚Äì3
t
Initialize ¬µu,1:T , ¬µv,1:T ‚Üê 0
Initialize ŒΩu,1:T , ŒΩv,1:T , œâu,1:T ‚àí1 , and œâv,1:T ‚àí1 such
that Bu> Bu = Bv> Bv = Œ† (see Eqs. 5 and 11)
for step = 1 to Ntr0 do
Draw I ‚äÇ {1, . . . , L0 } with |I| = L0 uniformly
Draw J ‚äÇ {1, . . . , L0 } with |J | = L0 uniformly
for all i ‚àà I do
[s]
Draw ui,1:T ‚àº N (0, I)
Solve Bu,i xui,1:T = ui,1:T for xui,1:T
end for
Obtain xvj,1:T by repeating last loop ‚àÄj ‚àà J
Calculate gradient estimates of L for minibatch
(I, J ) using Eqs. 10, 14, and 15
Obtain update steps d[¬∑] for all variational parameters
using Adam optimizer with parameters in Table 1
Update ¬µu,1:T ‚Üê ¬µu,1:T + d[¬µu,1:T ], and analogously
for ¬µv,1:T , œâu,1:T ‚àí1 , and œâv,1:T ‚àí1
Update ŒΩu,1:T and ŒΩv,1:T according to Eq. 18
end for
Repeat above loop for Ntr more steps, this time without
minibatch sampling (i.e., setting L0 = L)

provided in Algorithm 2.
Variational distribution. For now, we focus on the word
embeddings, and we simplify the notation by dropping the
indices for the vocabulary and embedding dimensions. The
variational distribution for a single embedding dimension
of a single word embedding trajectory is
q(u1:T ) = N (¬µu,1:T , (Bu> Bu )‚àí1 ).

Here, ¬µu,1:T is the vector of mean values, and Bu is the
Cholesky decomposition of the precision matrix. We restrict the latter to be bidiagonal,
Ô£´

3. Skip-gram Filtering Algorithm
The skip-gram filtering algorithm is described in section 4
of the main text. We provide a formulation in pseudocode
in Algorithm 1.

4. Skip-gram Smoothing Algorithm
In this section, we give details for the skip-gram smoothing
algorithm, see section 4 of the main text. A summary is

(4)

Ô£¨
Ô£¨
Ô£¨
Bu = Ô£¨
Ô£¨
Ô£≠

ŒΩu,1

œâu,1
ŒΩu,2

Ô£∂
œâu,2
..
.

..

.
ŒΩu,T ‚àí1

Ô£∑
Ô£∑
Ô£∑
Ô£∑
Ô£∑
œâu,T ‚àí1 Ô£∏
ŒΩT

(5)

with ŒΩu,t > 0 for all t ‚àà {1, . . . , T }. The variational parameters are ¬µu,1:T , ŒΩu,1:T , and œâ1:T ‚àí1 . The variational
distribution of the context embedding trajectories v1:T has
the same structure.

Supplementary Material to ‚ÄúDynamic Word Embeddings‚Äù

With the above form of Bu , the variational distribution is a
Gaussian with an arbitrary tridiagonal symmetric precision
matrix Bu> Bu . We chose this variational distribution because it is the exact posterior of a hidden time-series model
with a Kalman filtering prior and Gaussian noise (Blei &
Lafferty, 2006). Note that our variational distribution is a
generalization of a fully factorized (mean-field) distribution, which is obtained for œâu,t = 0 ‚àÄt. In the general
case, œâu,t 6= 0, the variational distribution can capture correlations between all time steps, with a dense covariance
matrix (Bu> Bu )‚àí1 .
Gradient estimation. The skip-gram smoothing algorithm uses stochastic gradient ascent to find the variational
parameters that maximize the ELBO,




L = Eq log p(U1:T , V1:T , n¬±
1:T ) ‚àí Eq log q(U1:T , V1:T ) .
(6)
Here, the second term is the entropy, which can be evaluated analytically. We obtain for each component in vocabulary and embedding space,
‚àíEq [log q(u1:T )] = ‚àí

X

log(ŒΩu,t ) + const.

(7)

t

[s]

u1:T = ¬µu,1:T + xu,1:T

(8)

with
=

We obtain

[s]
xu,1:T

where

[s]
u,1:T

‚àº N (0, I).

(9)

in Œò(T ) time by solving the bidiagonal
[s]

[s]

(10)

Here, Œ† ‚àà RT √óT is the precision matrix of the prior
u1:T ‚àº N (0, Œ†‚àí1 ). It is tridiagonal and therefore the
[s]
matrix-multiplication Œ†u1:T can be carried out efficiently.
The non-zero matrix elements of Œ† are
Œ†11 = œÉ0‚àí2 + œÉ1‚àí2
Œ†T T = œÉ0‚àí2 + œÉT‚àí2‚àí1
‚àí2
Œ†tt = œÉ0‚àí2 + œÉt‚àí1
+ œÉt‚àí2

Œ†1,t+1 = Œ†t+1,1 =

‚àÄt ‚àà {2, . . . , T ‚àí 1}

‚àíœÉt‚àí2 .

(11)

[s]

The term Œìu,1:T on the right-hand side of Eq. 10 comes
from the expectation value of the log-likelihood under q. It
is given by
L h
X

i
  [s]> [s] 
[s]
‚àí
‚àí
n+
+
n
œÉ
‚àíu
v
‚àí
n
ij,t
ij,t
i,t
j,t
ij,t vj,t

j=1

We focus again on a single dimension of a single word embedding trajectory u1:T , and we drop the indices i and k.
[s]
We draw S independent samples u1:T with s ‚àà {1, . . . , S}
from q(u1:T ) by parameterizing

[s]
Bu‚àí1 u,1:T

S
i
‚àÇL
1 X h [s]
[s]
‚âà
Œìu,1:T ‚àí Œ†u1:T .
‚àÇ¬µu,1:T
S s=1

[s]

The first term on the right-hand side of Eq. 6 cannot be evaluated analytically. We approximate its gradient by sampling from q using the reparameterization trick (Kingma &
Welling, 2014; Rezende et al., 2014). A naive calculation
would require ‚Ñ¶(T 2 ) computing time since the derivatives
of L with respect to ŒΩu,t and œâu,t for each t depend on
0
the count matrices n¬±
t0 of all t . However, as we show next,
there is a more efficient way to obtain all gradient estimates
in Œò(T ) time.

[s]
xu,1:T

The derivatives of L with respect to ¬µu,1:T can be obtained
using Eq. 8 and the chain rule. We find

Œìui,t =

and analogously for ‚àíEq [log q(v1:T )].

[s]

using the Adam optimizer (Kingma & Ba, 2014), which effectively averages over several gradient estimates in its first
moment estimate.

[s]

linear system Bu xu,1:T = u,1:T . Samples v1:T for the
context embedding trajectories are obtained analogously.
Our implementation uses S = 1, i.e., we draw only a single sample per training step. Averaging over several samples is done implicitly since we calculate the update steps

(12)
where we temporarily restored the indices i and j for words
and contexts, respectively. In deriving Eq. 12, we used the
relations ‚àÇ log œÉ(x)/‚àÇx = œÉ(‚àíx) and œÉ(‚àíx) = 1 ‚àí œÉ(x).
The derivatives of L with respect to ŒΩu,t and œâu,t are
more intricate. Using the parameterization in Eqs. 8‚Äì9, the
derivatives are functions of ‚àÇBu‚àí1 /‚àÇŒΩt and ‚àÇBu‚àí1 /‚àÇœât , respectively, where Bu‚àí1 is a dense (upper triangular) T √ó T
matrix. An efficient way to obtain these derivatives is to
use the relation
‚àÇBu‚àí1
‚àÇBu ‚àí1
= ‚àíBu‚àí1
B
‚àÇŒΩt
‚àÇŒΩt u

(13)

and similarly for ‚àÇBu‚àí1 /‚àÇœât . Using this relation and
Eqs. 8‚Äì9, we obtain the gradient estimates
S
‚àÇL
1 X [s] [s]
1
‚âà‚àí
yu,t xu,t ‚àí
,
‚àÇŒΩu,t
S s=1
ŒΩu,t

(14)

S
1 X [s] [s]
‚àÇL
‚âà‚àí
y x
.
‚àÇœâu,t
S s=1 u,t u,t+1

(15)

The second term on the right-hand side of Eq. 14 is the
derivative of the entropy, Eq. 7, and
h
i
[s]
[s]
[s]
yu,1:T = (Bu> )‚àí1 Œìu,1:T ‚àí Œ†u1:T .
(16)

Supplementary Material to ‚ÄúDynamic Word Embeddings‚Äù
[s]

The values yu,1:T can again be obtained in Œò(T ) time by
bringing Bu> to the left-hand side and solving the corresponding bidiagonal linear system of equations.
Sampling in vocabulary space. In the above paragraph,
we described an efficient strategy to obtain gradient estimates in only Œò(T ) time. However, the gradient estimation
scales quadratic in the vocabulary size L because all L2 elements of the positive count matrices n+
t contribute to the
gradients. In order speed up the optimization, we pretrain
the model using a minibatch of size L0 < L in vocabulary
space as explained below. The computational complexity
of a single training step in this setup scales as (L0 )2 rather
than L2 . After Ntr0 = 5000 training steps with minibatch
size L0 , we switch to the full batch size of L and train the
model for another Ntr = 1000 steps.
The subsampling in vocabulary space works as follows. In
each training step, we independently draw a set I of L0
random distinct words and a set J of L0 random distinct
contexts from a uniform probability over the vocabulary.
We then estimate the gradients of L with respect to only
the variational parameters that correspond to words i ‚àà I
and contexts j ‚àà J . This is possible because both the prior
of our dynamic skip-gram model and the variational distribution factorize in the vocabulary space. The likelihood of
the model, however, does not factorize. This affects only
[s]
[s]
the definition of Œìuik,t in Eq. 12. We replace Œìuik,t by an
[s]0

estimate Œìuik,t based on only the contexts j ‚àà J in the
current minibatch,
  [s]> [s] 
L Xh +
[s]
nij,t + n‚àí
Œìui,t = 0
ij,t œÉ ‚àíui,t vj,t
L
j‚ààJ
i
[s]
‚àí n‚àí
(17)
ij,t vj,t .
Here, the prefactor L/L0 restores the correct ratio between
evidence and prior knowledge (Hoffman et al., 2013).
Enforcing positive definiteness. We update the variational parameters using stochastic gradient ascent with the
Adam optimizer (Kingma & Ba, 2014). The parameters ŒΩu,1:T are the eigenvalues of the matrix Bu , which
is the Cholesky decomposition of the precision matrix
of q. Therefore, ŒΩu,t has to be positive for all t ‚àà
{1, . . . , T }. We use mirror ascent (Ben-Tal et al., 2001;
Beck & Teboulle, 2003) to enforce ŒΩu,t > 0. Specifically,
we update ŒΩt to a new value ŒΩt0 defined by
s
2
1
1
0
2
ŒΩu,t d[ŒΩu,t ] + ŒΩu,t
(18)
ŒΩu,t = ŒΩu,t d[ŒΩu,t ] +
2
2
where d[ŒΩu,t ] is the step size obtained from the Adam optimizer. Eq. 18 can be derived from the general mirror ascent
0
update rule Œ¶0 (ŒΩu,t
) = Œ¶0 (ŒΩu,t ) + d[ŒΩu,t ] with the mirror

map Œ¶ : x 7‚Üí ‚àíc1 log(x) + c2 x2 /2, where we set the parameters to c1 = ŒΩu,t and c2 = 1/ŒΩu,t for dimensional
reasons. The update step in Eq. 18 increases (decreases)
ŒΩu,t for positive (negative) d[ŒΩu,t ], while always keeping
its value positive.
Natural basis. As a final remark, let us discuss an optional extension to the skip-gram smoothing algorithm that
converges in less training steps. This extension only increases the efficiency of the algorithm. It does not change
the underlying model or the choice of variational distribution. Observe that the prior of the dynamic skip-gram
model connects only neighboring time-steps with each
other. Therefore, the gradient of L with respect to ¬µu,t
depends only on the values of ¬µu,t‚àí1 and ¬µu,t+1 . A naive
implementation of gradient ascent would thus require T ‚àí1
update steps until a change of ¬µu,1 affects updates of ¬µu,T .
This problem can be avoided with a change of basis from
¬µu,1:T to new parameters œÅu,1:T ,
¬µu,1:T = AœÅu,1:T

(19)

with an appropriately chosen invertible matrix A ‚àà RT √óT .
Derivatives of L with respect to œÅu,1:T are given by the
chain rule, ‚àÇL/‚àÇœÅu,1:T = (‚àÇL/‚àÇ¬µu,1:T )A. A natural (but
inefficient) choice for A is to stack the eigenvectors of the
prior precision matrix Œ†, see Eq. 11, into a matrix. The
eigenvectors of Œ† are the Fourier modes of the Kalman filtering prior (with a regularization due to œÉ0 ). Therefore,
there is a component œÅu,t that corresponds to the zero-mode
of Œ†, and this component learns an average word embedding over all times. Higher modes correspond to changes
of the embedding vector over time. A single update to the
zero immediately affects all elements of ¬µu,1:T , and therefore changes the word embeddings at all time steps. Thus,
information propagates quickly along the time dimension.
The downside of this choice for A is that the transformation in Eq. 19 has complexity ‚Ñ¶(T 2 ), which makes update
steps slow.
As a compromise between efficiency and a natural basis,
we propose to set A in Eq. 19 to the Cholesky decomposition of the prior covariance matrix Œ†‚àí1 ‚â° AA> . Thus, A
is still a dense (upper triangular) matrix, and, in our experiments, updates to the last component œÅu,T affect all components of ¬µu,1:T in an approximately equal amount. Since Œ†
is tridiagonal, the inverse of A is bidiagonal, and Eq. 19 can
be evaluated in Œò(T ) time by solving A¬µu,1:T = œÅu,1:T for
¬µu,1:T . This is the parameterization we used in our implementation of the skip-gram smoothing algorithm.

References
Beck, Amir and Teboulle, Marc. Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Opti-

Supplementary Material to ‚ÄúDynamic Word Embeddings‚Äù

mization. Operations Research Letters, 31(3):167‚Äì175,
2003.
Ben-Tal, Aharon, Margalit, Tamar, and Nemirovski,
Arkadi. The Ordered Subsets Mirror Descent Optimization Method with Applications to Tomography. SIAM
Journal on Optimization, 12(1):79‚Äì108, 2001.
Blei, David M and Lafferty, John D. Dynamic Topic Models. In Proceedings of the 23rd International Conference
on Machine Learning, pp. 113‚Äì120. ACM, 2006.
Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John William. Stochastic Variational Inference.
Journal of Machine Learning Research, 14(1):1303‚Äì
1347, 2013.
Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Darshan, and Petrov, Slav. Temporal Analysis of Language
Through Neural Language Models. In Proceedings of
the ACL 2014 Workshop on Language Technologies and
Computational Social Science, pp. 61‚Äì65, 2014.
Kingma, Diederik and Ba, Jimmy.
for Stochastic Optimization.
arXiv:1412.6980, 2014.

Adam: A Method
arXiv preprint

Kingma, Diederik P and Welling, Max. Auto-Encoding
Variational Bayes. In Proceedings of the 2nd International Conference on Learning Representations (ICLR),
2014.
Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed Representations of
Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26, pp.
3111‚Äì3119. 2013.
Rauber, Paulo E., FalcaÃÉo, Alexandre X., and Telea, Alexandru C. Visualizing Time-Dependent Data Using Dynamic t-SNE. In EuroVis 2016 - Short Papers, 2016.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In The 31st International Conference on Machine Learning (ICML), 2014.

