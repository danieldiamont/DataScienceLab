Supplementary Material to “Dynamic Word Embeddings”

Robert Bamler 1 Stephan Mandt 1

Table 1. Hyperparameters for skip-gram filtering and skip-gram
smoothing.

PARAMETER C OMMENT
L = 104
L0 = 103
d = 100
d = 200
Ntr = 5000
Ntr0 = 5000

vocabulary size
batch size for smoothing
embedding dimension for SoU and Twitter
embedding dimension for Google books
number of training steps for each t (filtering)
number of pretraining steps with minibatch
sampling (smoothing; see Algorithm 2)
Ntr = 1000 number of training steps without minibatch
sampling (smoothing; see Algorithm 2)
cmax = 4
context window size for positive examples
η=1
ratio of negative to positive examples
γ = 0.75
context exponent for negative examples
D = 10−3 diffusion const. per year (Google books & SoU)
D=1
diffusion const. per year (Twitter)
σ02 = 1
variance of overall prior
α = 10−2 learning rate (filtering)
α0 = 10−2 learning rate during minibatch phase (smoothing)
α = 10−3 learning rate after minibatch phase (smoothing)
β1 = 0.9
decay rate of 1st moment estimate
β2 = 0.99
decay rate of 2nd moment estimate (filtering)
β2 = 0.999 decay rate of 2nd moment estimate (smoothing)
δ = 10−8 regularizer of Adam optimizer

1. Dimensionality Reduction in Figure 1
To create the word-clouds in Figure 1 of the main text we
mapped the fitted word embeddings from Rd to the twodimensional plane using dynamic t-SNE (Rauber et al.,
2016). Dynamic t-SNE is a non-parametric dimensionality reduction algorithm for sequential data. The algorithm finds a projection to a lower dimension by solving
a non-convex optimization problem that aims at preserving
nearest-neighbor relations at each individual time step. In
1
Disney Research, 4720 Forbes Avenue, Pittsburgh,
PA 15213, USA. Correspondence to:
Robert Bamler
<Robert.Bamler@disneyresearch.com>,
Stephan
Mandt
<Stephan.Mandt@disneyresearch.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

addition, projections at neighboring time steps are aligned
with each other by a quadratic penalty with prefactor λ ≥ 0
for sudden movements.
There is a trade-off between finding good local projections
for each individual time step (λ → 0), and finding smooth
projections (large λ). Since we want to analyze the smoothness of word embedding trajectories, we want to avoid
bias towards smooth projections. Unfortunately, setting
λ = 0 is not an option since, in this limit, the optimization
problem is invariant under independent rotations at each
time, rendering trajectories in the two-dimensional projection plane meaningless. To still avoid bias towards smooth
projections, we anneal λ exponentially towards zero over
the course of the optimization. We start the optimizer with
λ = 0.01, and we reduce λ by 5% with each training step.
We run 100 optimization steps in total, so that λ ≈ 6×10−6
at the end of the training procedure. We used the opensource implementation,1 set the target perplexities to 200,
and used default values for all other parameters.

2. Hyperparemeters and Construction of n±
1:T
Table 1 lists the hyperparameters used in our experiments.
For the Google books corpus, we used the same context
window size cmax and embedding dimension d as in (Kim
et al., 2014). We reduced d for the SoU and Twitter corpora
to avoid overfitting to these much smaller data sets.
In constrast to word2vec, we construct our positive and
negative count matrices n±
ij,t deterministically in a preprocessing step. As detailed below, this is done such that it
resembles as closely as possible the stochastic approach in
word2vec (Mikolov et al., 2013). In every update step,
word2vec stochastically samples a context window size
uniformly in an interval [1, · · · , cmax ], thus the context size
fluctuates and nearby words appear more often in the same
context than words that are far apart from each other in
the sentence. We follow a deterministic scheme that results in similar statistics. For each pair of words (w1 , w2 )
in a given sentence, we increase the counts n+
iw1 jw2 by
max (0, 1 − k/cmax ), where 0 ≤ k ≤ cmax is the number of words that appear between w1 and w2 , and iw1 and
jw2 are the words’ unique indices in the vocabulary.
1

https://github.com/paulorauber/thesne

Supplementary Material to “Dynamic Word Embeddings”

Algorithm 1 Skip-gram filtering; see section 4 of the main
text.
Remark: All updates are analogous for word and context vectors; we drop their indices for simplicity.
Input: number of time steps T , time stamps τ1:T , positive and negative examples n±
1:T , hyperparameters.
Init. prior means µ̃ik,1 ← 0 and variances Σ̃i,1 = Id×d
Init. variational means µik,1 ← 0 and var. Σi,1 = Id×d
for t = 1 to T do
if t 6= 1 then
Update approximate Gaussian prior with params.
µ̃ik,t and Σ̃i,t using µik,t−1 and Σi,t−1 , see Eq. 13.
end if
Compute entropy Eq [log q(·)] analytically.
Compute expected log Gaussian prior with parameters
µ̃ik,t and Σ̃k,t analytically.
Maximize Lt in Eq. 11, using black-box VI with the
reparametrization trick.
Obtain µik,t and Σi,t as outcome of the optimization.
end for
We also used a deterministic variant of word2vec to construct the negative count matrices n−
t . In word2vec, η negative samples (i, j) are drawn for each positive sample (i, j 0 )
by drawing η independent values for j from a distribution
Pt0 (j) defined below. We define n−
ij,t such that it matches
the expectation value of the number of times that word2vec
would sample the negative word-context pair (i, j). Specifically, we define
PL
+
j=1 nij,t
Pt (i) = PL
,
(1)
+
i0 ,j=1 ni0 j,t
γ
Pt (j)
(2)
Pt0 (j) = PL
 ,
0 γ
j 0 =1 Pt (j )
 X

L
+
0
n−
=
n
(3)
0
0
ij,t
i j ,t ηPt (i)Pt (j).
i0 ,j 0 =1

We chose γ = 0.75 as proposed in (Mikolov et al., 2013),
and we set η = 1. In practice, it is not necessary to explicitly construct the full matrices n−
t , and it is more efficient
to keep only the distributions Pt (i) and Pt0 (j) in memory.

Algorithm 2 Skip-gram smoothing; see section 4. We drop
indices i, j, and k for word, context, end embedding dimension, respectively, when they are clear from context.
Input: number of time steps T , time stamps τ1:T , wordcontext counts n+
1:T , hyperparameters in Table 1
Obtain n−
∀t
using
Eqs. 1–3
t
Initialize µu,1:T , µv,1:T ← 0
Initialize νu,1:T , νv,1:T , ωu,1:T −1 , and ωv,1:T −1 such
that Bu> Bu = Bv> Bv = Π (see Eqs. 5 and 11)
for step = 1 to Ntr0 do
Draw I ⊂ {1, . . . , L0 } with |I| = L0 uniformly
Draw J ⊂ {1, . . . , L0 } with |J | = L0 uniformly
for all i ∈ I do
[s]
Draw ui,1:T ∼ N (0, I)
Solve Bu,i xui,1:T = ui,1:T for xui,1:T
end for
Obtain xvj,1:T by repeating last loop ∀j ∈ J
Calculate gradient estimates of L for minibatch
(I, J ) using Eqs. 10, 14, and 15
Obtain update steps d[·] for all variational parameters
using Adam optimizer with parameters in Table 1
Update µu,1:T ← µu,1:T + d[µu,1:T ], and analogously
for µv,1:T , ωu,1:T −1 , and ωv,1:T −1
Update νu,1:T and νv,1:T according to Eq. 18
end for
Repeat above loop for Ntr more steps, this time without
minibatch sampling (i.e., setting L0 = L)

provided in Algorithm 2.
Variational distribution. For now, we focus on the word
embeddings, and we simplify the notation by dropping the
indices for the vocabulary and embedding dimensions. The
variational distribution for a single embedding dimension
of a single word embedding trajectory is
q(u1:T ) = N (µu,1:T , (Bu> Bu )−1 ).

Here, µu,1:T is the vector of mean values, and Bu is the
Cholesky decomposition of the precision matrix. We restrict the latter to be bidiagonal,


3. Skip-gram Filtering Algorithm
The skip-gram filtering algorithm is described in section 4
of the main text. We provide a formulation in pseudocode
in Algorithm 1.

4. Skip-gram Smoothing Algorithm
In this section, we give details for the skip-gram smoothing
algorithm, see section 4 of the main text. A summary is

(4)




Bu = 



νu,1

ωu,1
νu,2


ωu,2
..
.

..

.
νu,T −1






ωu,T −1 
νT

(5)

with νu,t > 0 for all t ∈ {1, . . . , T }. The variational parameters are µu,1:T , νu,1:T , and ω1:T −1 . The variational
distribution of the context embedding trajectories v1:T has
the same structure.

Supplementary Material to “Dynamic Word Embeddings”

With the above form of Bu , the variational distribution is a
Gaussian with an arbitrary tridiagonal symmetric precision
matrix Bu> Bu . We chose this variational distribution because it is the exact posterior of a hidden time-series model
with a Kalman filtering prior and Gaussian noise (Blei &
Lafferty, 2006). Note that our variational distribution is a
generalization of a fully factorized (mean-field) distribution, which is obtained for ωu,t = 0 ∀t. In the general
case, ωu,t 6= 0, the variational distribution can capture correlations between all time steps, with a dense covariance
matrix (Bu> Bu )−1 .
Gradient estimation. The skip-gram smoothing algorithm uses stochastic gradient ascent to find the variational
parameters that maximize the ELBO,




L = Eq log p(U1:T , V1:T , n±
1:T ) − Eq log q(U1:T , V1:T ) .
(6)
Here, the second term is the entropy, which can be evaluated analytically. We obtain for each component in vocabulary and embedding space,
−Eq [log q(u1:T )] = −

X

log(νu,t ) + const.

(7)

t

[s]

u1:T = µu,1:T + xu,1:T

(8)

with
=

We obtain

[s]
xu,1:T

where

[s]
u,1:T

∼ N (0, I).

(9)

in Θ(T ) time by solving the bidiagonal
[s]

[s]

(10)

Here, Π ∈ RT ×T is the precision matrix of the prior
u1:T ∼ N (0, Π−1 ). It is tridiagonal and therefore the
[s]
matrix-multiplication Πu1:T can be carried out efficiently.
The non-zero matrix elements of Π are
Π11 = σ0−2 + σ1−2
ΠT T = σ0−2 + σT−2−1
−2
Πtt = σ0−2 + σt−1
+ σt−2

Π1,t+1 = Πt+1,1 =

∀t ∈ {2, . . . , T − 1}

−σt−2 .

(11)

[s]

The term Γu,1:T on the right-hand side of Eq. 10 comes
from the expectation value of the log-likelihood under q. It
is given by
L h
X

i
  [s]> [s] 
[s]
−
−
n+
+
n
σ
−u
v
−
n
ij,t
ij,t
i,t
j,t
ij,t vj,t

j=1

We focus again on a single dimension of a single word embedding trajectory u1:T , and we drop the indices i and k.
[s]
We draw S independent samples u1:T with s ∈ {1, . . . , S}
from q(u1:T ) by parameterizing

[s]
Bu−1 u,1:T

S
i
∂L
1 X h [s]
[s]
≈
Γu,1:T − Πu1:T .
∂µu,1:T
S s=1

[s]

The first term on the right-hand side of Eq. 6 cannot be evaluated analytically. We approximate its gradient by sampling from q using the reparameterization trick (Kingma &
Welling, 2014; Rezende et al., 2014). A naive calculation
would require Ω(T 2 ) computing time since the derivatives
of L with respect to νu,t and ωu,t for each t depend on
0
the count matrices n±
t0 of all t . However, as we show next,
there is a more efficient way to obtain all gradient estimates
in Θ(T ) time.

[s]
xu,1:T

The derivatives of L with respect to µu,1:T can be obtained
using Eq. 8 and the chain rule. We find

Γui,t =

and analogously for −Eq [log q(v1:T )].

[s]

using the Adam optimizer (Kingma & Ba, 2014), which effectively averages over several gradient estimates in its first
moment estimate.

[s]

linear system Bu xu,1:T = u,1:T . Samples v1:T for the
context embedding trajectories are obtained analogously.
Our implementation uses S = 1, i.e., we draw only a single sample per training step. Averaging over several samples is done implicitly since we calculate the update steps

(12)
where we temporarily restored the indices i and j for words
and contexts, respectively. In deriving Eq. 12, we used the
relations ∂ log σ(x)/∂x = σ(−x) and σ(−x) = 1 − σ(x).
The derivatives of L with respect to νu,t and ωu,t are
more intricate. Using the parameterization in Eqs. 8–9, the
derivatives are functions of ∂Bu−1 /∂νt and ∂Bu−1 /∂ωt , respectively, where Bu−1 is a dense (upper triangular) T × T
matrix. An efficient way to obtain these derivatives is to
use the relation
∂Bu−1
∂Bu −1
= −Bu−1
B
∂νt
∂νt u

(13)

and similarly for ∂Bu−1 /∂ωt . Using this relation and
Eqs. 8–9, we obtain the gradient estimates
S
∂L
1 X [s] [s]
1
≈−
yu,t xu,t −
,
∂νu,t
S s=1
νu,t

(14)

S
1 X [s] [s]
∂L
≈−
y x
.
∂ωu,t
S s=1 u,t u,t+1

(15)

The second term on the right-hand side of Eq. 14 is the
derivative of the entropy, Eq. 7, and
h
i
[s]
[s]
[s]
yu,1:T = (Bu> )−1 Γu,1:T − Πu1:T .
(16)

Supplementary Material to “Dynamic Word Embeddings”
[s]

The values yu,1:T can again be obtained in Θ(T ) time by
bringing Bu> to the left-hand side and solving the corresponding bidiagonal linear system of equations.
Sampling in vocabulary space. In the above paragraph,
we described an efficient strategy to obtain gradient estimates in only Θ(T ) time. However, the gradient estimation
scales quadratic in the vocabulary size L because all L2 elements of the positive count matrices n+
t contribute to the
gradients. In order speed up the optimization, we pretrain
the model using a minibatch of size L0 < L in vocabulary
space as explained below. The computational complexity
of a single training step in this setup scales as (L0 )2 rather
than L2 . After Ntr0 = 5000 training steps with minibatch
size L0 , we switch to the full batch size of L and train the
model for another Ntr = 1000 steps.
The subsampling in vocabulary space works as follows. In
each training step, we independently draw a set I of L0
random distinct words and a set J of L0 random distinct
contexts from a uniform probability over the vocabulary.
We then estimate the gradients of L with respect to only
the variational parameters that correspond to words i ∈ I
and contexts j ∈ J . This is possible because both the prior
of our dynamic skip-gram model and the variational distribution factorize in the vocabulary space. The likelihood of
the model, however, does not factorize. This affects only
[s]
[s]
the definition of Γuik,t in Eq. 12. We replace Γuik,t by an
[s]0

estimate Γuik,t based on only the contexts j ∈ J in the
current minibatch,
  [s]> [s] 
L Xh +
[s]
nij,t + n−
Γui,t = 0
ij,t σ −ui,t vj,t
L
j∈J
i
[s]
− n−
(17)
ij,t vj,t .
Here, the prefactor L/L0 restores the correct ratio between
evidence and prior knowledge (Hoffman et al., 2013).
Enforcing positive definiteness. We update the variational parameters using stochastic gradient ascent with the
Adam optimizer (Kingma & Ba, 2014). The parameters νu,1:T are the eigenvalues of the matrix Bu , which
is the Cholesky decomposition of the precision matrix
of q. Therefore, νu,t has to be positive for all t ∈
{1, . . . , T }. We use mirror ascent (Ben-Tal et al., 2001;
Beck & Teboulle, 2003) to enforce νu,t > 0. Specifically,
we update νt to a new value νt0 defined by
s
2
1
1
0
2
νu,t d[νu,t ] + νu,t
(18)
νu,t = νu,t d[νu,t ] +
2
2
where d[νu,t ] is the step size obtained from the Adam optimizer. Eq. 18 can be derived from the general mirror ascent
0
update rule Φ0 (νu,t
) = Φ0 (νu,t ) + d[νu,t ] with the mirror

map Φ : x 7→ −c1 log(x) + c2 x2 /2, where we set the parameters to c1 = νu,t and c2 = 1/νu,t for dimensional
reasons. The update step in Eq. 18 increases (decreases)
νu,t for positive (negative) d[νu,t ], while always keeping
its value positive.
Natural basis. As a final remark, let us discuss an optional extension to the skip-gram smoothing algorithm that
converges in less training steps. This extension only increases the efficiency of the algorithm. It does not change
the underlying model or the choice of variational distribution. Observe that the prior of the dynamic skip-gram
model connects only neighboring time-steps with each
other. Therefore, the gradient of L with respect to µu,t
depends only on the values of µu,t−1 and µu,t+1 . A naive
implementation of gradient ascent would thus require T −1
update steps until a change of µu,1 affects updates of µu,T .
This problem can be avoided with a change of basis from
µu,1:T to new parameters ρu,1:T ,
µu,1:T = Aρu,1:T

(19)

with an appropriately chosen invertible matrix A ∈ RT ×T .
Derivatives of L with respect to ρu,1:T are given by the
chain rule, ∂L/∂ρu,1:T = (∂L/∂µu,1:T )A. A natural (but
inefficient) choice for A is to stack the eigenvectors of the
prior precision matrix Π, see Eq. 11, into a matrix. The
eigenvectors of Π are the Fourier modes of the Kalman filtering prior (with a regularization due to σ0 ). Therefore,
there is a component ρu,t that corresponds to the zero-mode
of Π, and this component learns an average word embedding over all times. Higher modes correspond to changes
of the embedding vector over time. A single update to the
zero immediately affects all elements of µu,1:T , and therefore changes the word embeddings at all time steps. Thus,
information propagates quickly along the time dimension.
The downside of this choice for A is that the transformation in Eq. 19 has complexity Ω(T 2 ), which makes update
steps slow.
As a compromise between efficiency and a natural basis,
we propose to set A in Eq. 19 to the Cholesky decomposition of the prior covariance matrix Π−1 ≡ AA> . Thus, A
is still a dense (upper triangular) matrix, and, in our experiments, updates to the last component ρu,T affect all components of µu,1:T in an approximately equal amount. Since Π
is tridiagonal, the inverse of A is bidiagonal, and Eq. 19 can
be evaluated in Θ(T ) time by solving Aµu,1:T = ρu,1:T for
µu,1:T . This is the parameterization we used in our implementation of the skip-gram smoothing algorithm.

References
Beck, Amir and Teboulle, Marc. Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Opti-

Supplementary Material to “Dynamic Word Embeddings”

mization. Operations Research Letters, 31(3):167–175,
2003.
Ben-Tal, Aharon, Margalit, Tamar, and Nemirovski,
Arkadi. The Ordered Subsets Mirror Descent Optimization Method with Applications to Tomography. SIAM
Journal on Optimization, 12(1):79–108, 2001.
Blei, David M and Lafferty, John D. Dynamic Topic Models. In Proceedings of the 23rd International Conference
on Machine Learning, pp. 113–120. ACM, 2006.
Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John William. Stochastic Variational Inference.
Journal of Machine Learning Research, 14(1):1303–
1347, 2013.
Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Darshan, and Petrov, Slav. Temporal Analysis of Language
Through Neural Language Models. In Proceedings of
the ACL 2014 Workshop on Language Technologies and
Computational Social Science, pp. 61–65, 2014.
Kingma, Diederik and Ba, Jimmy.
for Stochastic Optimization.
arXiv:1412.6980, 2014.

Adam: A Method
arXiv preprint

Kingma, Diederik P and Welling, Max. Auto-Encoding
Variational Bayes. In Proceedings of the 2nd International Conference on Learning Representations (ICLR),
2014.
Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed Representations of
Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26, pp.
3111–3119. 2013.
Rauber, Paulo E., Falcão, Alexandre X., and Telea, Alexandru C. Visualizing Time-Dependent Data Using Dynamic t-SNE. In EuroVis 2016 - Short Papers, 2016.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In The 31st International Conference on Machine Learning (ICML), 2014.

