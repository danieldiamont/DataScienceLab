Supplementary Material for “Tensor Belief Propagation”

Andrew Wrigley 1 Wee Sun Lee 2 Nan Ye 3

Appendix (Supplementary Material)
Proof of Consistency
To prove consistency of the algorithm, we introduce a few
lemmas.
Lemma 1. Let X1,n , . . . , Xm,n be random variables such
p
that Xi,n → µi as n → ∞. Let Xi,n ∈ [0, M ] and µi ∈
P
p P
[0, M ] for i = 1, . . . , m. Then i Xi,n →
i µi and
Q
p Q
i Xi,n →
i µi .
p

Proof. Since Xi,n → µi , for any  > 0, and any δ > 0,
there exists an Ni such that for n > Ni , P (|Xi,n − µi | >
δ

m ) ≤ m . Let N = max{N1 , . . . , Nm }, and assume n >

δ
N . Then for every i, P (|Xi,n − µi | > m
)≤ m
. By the
union bound, with probability at least
1
−
δ,
for
P
P every i,
|Xi,n − µi | ≤ , which implies
|
X
−
iPi,n
i µi | ≤ .
P
Hence when n > N , p(| i Xi,n − i µi | > ) ≤ δ. It
P
p P
follows that i Xi,n → i µi .
Q
p Q
To show that i Xi,n → i µi , it suffices to show this for
m = 2. The proof then follows by mathematical induction.
For any  > 0 and δ > 0, there exists N1 and N2 such
that for any n > max{N1 , N2 }, we have P (|X1,n − µ1 | >
δ

δ

3M ) < 2 and P (|X2,n − µ2 | > 3M ) < 2 . By the union

bound, with probability at least 1 − δ, |X1,n − µ1 | ≤ 3M

and |X2,n − µ1 | ≤ 3M . Let s1 , s2 ∈ {−1, 1} and assume


3M ≤ M (the result in the case 3M > M holds trivially).
Then, with probability at least 1 − δ,
|X1,n X2,n − µ1 µ2 |


s1   
s2  


≤ max  µ1 +
µ2 +
− µ1 µ2 
s1 ,s2
3M
3M


 µ1 s2  µ2 s1  s1 s2 2 


= max 
+
+
s1 ,s2
3M
3M
9M 2 
≤ ,
1

Australian National University, Canberra, Australia.
National University of Singapore, Singapore. 3 Queensland
University of Technology, Brisbane, Australia. Correspondence
to: Andrew Wrigley <andrew.wrigley@anu.edu.au>, Wee Sun
Lee <leews@comp.nus.edu.sg>, Nan Ye <n.ye@qut.edu.au>.
2

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

where we have used µ1 , µ2 ≤ M and


3M

≤ M.

In the following, given two sequences of random variables
p
{Xn } and {Yn }, we shall use Xn → Yn to denote Xn −
p
Yn → 0.
p

Lemma 2. For any random vectors Xn , Yn , Y , if Xn →
p
p
Yn and Yn → Y as n → ∞, then Xn → Y as n → ∞.
Proof. For any  > 0 and δ > 0, there exists an N such
that for any n > N , we have P (|Xn − Yn | > 2 ) < 2δ
and P (|Yn − Y | > 2 ) < 2δ . Using the union bound, with
probability at least 1 − δ, we have |Xn − Yn | ≤ 2 and
|Yn − Y | ≤ 2 . Hence with probability at least 1 − δ, |Xn −
p
Y | ≤ . Thus Xn → Y as n → ∞.
Lemma
Pn 3. Let Xn is a random variable, and Yn =
1
i=1 Yn,i where Yn,i are i.i.d. random variables in
n
[0, M ] for some constant M . Let the expectation of Yn
p
be Xn . Then Yn → Xn as n → ∞.
Proof. When Xn = x, we have P (|Yn − x| ≤  | Xn =
2
2
x) ≤ 1 − 2e−2n /M according to Hoeffding’s inequality.
Since this holds for any x, we have P (|Yn − Xn | ≤ ) ≤
2
2
p
1 − 2e−2n /M . It follows that Yn → Xn .
Proof. (Proof of consistency) It suffices to show that at the
beginning of each iteration, all the estimated messages are
consistent.
Initially, none of the messages have been estimated, and it
is vacuously true that all messages that have been estimated
so far are consistent.
For the inductive case, it suffices to show that the message
estimated at each iteration is consistent. Specifically, let
(K)
m̃t→s (xs ) be the estimate of the true message mt→s (xs )
p
(K)
using K samples, we show that m̃t→s (xs ) → mt→s (xs )
as K → ∞.
(K)

By the inductive assumption, m̃u→t (xt ) is consistent for
each u ∈ N (t) \ {s}, where N (t) is the set of neighbours
for t. To simplify notation, we denote the true messages in
{mu→t (xt ) : u ∈ N (t) \ {s}} by m1 (xt ), . . . , ml (xt ) and
(K)
(K)
denote their estimates by m̃1 (xt ), . . . , m̃l (xt ).

Tensor Belief Propagation
(K)

Let Φ̃t (xt ) be the estimate of the initial clique potential
at the node. Each multiplication of factors to form the initial clique potential is done by sampling. Lemma 3 shows
that each multiplication converges to its expected value.
The expected value is in turn the product of two numbers,
one of which may be a previously computed random variable. Lemma 1 shows that the product converges to the
true value, and Lemma 2 chains the two process together to
show that the estimate of the estimate for the initial clique
potential converges.
(K)

(K)

(K)

(K)

(K)
m̃t→s (xs )

(xt ) for 2 ≤ j ≤ l.
P
(K)
= xt \xs ṽl (xt ).

2xy = θ
1
x2 + y 2 =
θ
which yields
1
x=
2

(K)

Let ṽ1 (xt ) be the estimate of Φ̃t (xt )m̃1 (xt ) ob(K)
tained in the algorithm, and ṽj (xt ) be the estimate of
ṽj−1 (xt )m̃j

for θ > 1. Specifically, to solve (1) we solve

Then we have

(K)

The random variable ṽ1 (xt ) is the average of K i.i.d.
(K)
random variables with expected value Φ̃t (xt )m̃1 (xt ).
By the construction of ṽ1 and the assumption that each
rank-1 tensor value is in [0, M ], we have that each of the
K random variables are in the range of [0, M 2 ]. It folp
(K)
(K)
lows from Lemma 3 that ṽ1 (xt ) → Φ̃t (xt )m̃1 (xt )
p
(K)
as K → ∞. Since we also have m̃1 (xt ) → m1 (xt )
p
(K)
(K)
and Φ̃t (xt ) → Φt (xt ), it follows from Lemma 1 and
p
(K)
Lemma 2 that ṽ1 (xt ) → Φt (xt )m1 (xt ).
(K)

p

Using induction, we can similarly show that ṽl (xt ) →
Φt (xt )m1 (xt ) . . . ml (xt ). Summing over xt \ xs on both
(K)
sides and applying Lemma 1 again, we have m̃t→s (xs ) →
mt→s (xs ). Since this holds for any xs , the convergence
holds for all xs .

y=

1
2

!
1
−θ ,
θ
!
r
r
1
1
θ+ ∓
−θ
θ
θ
r

1
θ+ ±
θ

r

or
1
x=
2
y=

1
2

!
1
−
−θ ,
θ
!
r
r
1
1
− θ+ ∓
−θ
θ
θ
r

1
θ+ ±
θ

r

(2) is solved analogously. In each case, we use the first
solution and weight each rank-1 term equally.
Parameters for BP, MF, TRW, Gibbs
The following parameters were used for the existing approximate inference algorithms within the libDAI package:
• Loopy BP: Update schedule sequential using a random sequence; maximum 104 iterations; tolerance for
convergence 10−12

Decomposition for Ising models

• Mean-field: Maximum 104 iterations; tolerance for
convergence 10−12

In the case of Ising models, and any pairwise MRFs with
Ising potentials of the form φij (xi , xj ) = exp(wij xi xj ),
the 2 × 2 potential tables are in general rank-2 of the form

• Tree-reweighted BP: Sequential updates using a random sequence; tree sample size of 104 used to set
weights; tolerance for convergence 10−12



θ
1
θ

1
θ

θ


,

θ = exp(wij ).

For tables of this particular form, we note there is a natural rank-2 decomposition that one can compute quickly
by assuming terms in the decomposition are symmetric, by
solving


θ
1
θ

1
θ



θ

=

       
x
y
y
x
⊗
+
⊗
y
x
x
y

(1)

=

       
x
x
y
y
⊗
+
⊗
y
y
x
x

(2)

for θ ≤ 1, and


θ
1
θ

1
θ

θ



• Gibbs: Burn-in 100 passes; restart chain with random
initialisation every 1000 passes; record one sample per
pass (pass = cycle once over all variables); running
time limited as indicated in text.

Tensor Belief Propagation

Supplementary Results
Additional results on the Ising model with different grid sizes and different interaction strengths are shown here.

Attractive interactions

0.5

TBP (100)
TBP (100000)

TRW
Gibbs

TBP (100)
TBP (100000)

0.4
Marginal error

Marginal error

0.4

MF
BP

Mixed interactions

0.5

0.3
0.2

MF
BP

TRW
Gibbs

0.3
0.2
0.1

0.1
0.0

4

6

8
10
12
Ising grid width

14

0.0

16

4

6

8
10
12
Ising grid width

14

16

Ising models: Effect of model size N on marginal error. 100: sample size K = 100, 100000: sample size K = 100000. Gibbs running
time matches the running time of TBP with K = 100000.

Attractive interactions

0.5

TBP (100)
TBP (100000)

Gibbs
TRW

0.3
0.2

MF
BP

Gibbs
TRW

0.3
0.2
0.1

0.1
0.0

TBP (100)
TBP (100000)

0.4
Marginal error

Marginal error

0.4

MF
BP

Mixed interactions

0.5

0.5

1.0

1.5

2.0

Interaction

2.5

3.0

strength

3.5

4.0

0.0

0.5

1.0

1.5

2.0

Interaction

2.5

3.0

strength

3.5

4.0

Ising models: Effect of interaction strength on performance of approximate inference algorithms. Gibbs sampling matches TBP (100000)
runtime.

Tensor Belief Propagation

Distribution of estimated marginals
To give an indication of the variance of the estimates, we show histograms of the marginal estimates on the Ising models.

Mixed, K = 100

Mixed, K = 10000

Attractive, K = 100

Attractive, K = 10000

Estimated values of P (Xi = 1) for 500 runs of tensor propagation for small versus large multiplication sample size K. Each histogram
shows marginal estimates for a single node in the 10 × 10 Ising model grouped into 20 bins. Nodes shown are from the upper-left 3 × 3
corner of the grid. Solid red vertical lines indicate the true marginal and dashed green vertical lines show the mean of the 500 marginal
estimates. The two mixed plots use the same Ising model instance, as do the two attractive plots.

