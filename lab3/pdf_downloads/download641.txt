A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

A. Additional Applications and Experimental Results
In this section, we present the application of our generic framework to one-bit matrix completion as well as additional
experimental results for matrix sensing.
A.1. One-bit Matrix Completion
Compared with matrix completion, we only observe the sign of each noisy entries of the unknown low-rank matrix X⇤ in
one-bit matrix completion (Davenport et al., 2014; Cai & Zhou, 2013). We consider the uniform sampling model, which
has been studied in existing literature (Davenport et al., 2014; Cai & Zhou, 2013; Ni & Gu, 2016). More specifically, we
consider the following observation model, which is based on a differentiable function f : R ! [0, 1]
(
⇤
+1, with probability f (Xjk
),
Yjk =
(A.1)
⇤
1, with probability 1 f (Xjk
),
where we use a binary matrix Y to denote the observation matrix in (A.1). In addition, if the function f is a cumulative
distribution function with respect to Zjk , then we can rewrite the observation model (A.1) as follows
(
⇤
+1, if Xjk
+ Zjk > 0,
Yjk =
(A.2)
⇤
1, if Xjk
+ Zjk < 0,
where we use Z 2 Rd1 ⇥d2 to denote the noise matrix with i.i.d. elements Zjk . A lot of functions can be applied to
observation model (A.1), and we consider the broadly-used logistic function f (Xjk ) = eXjk /(1+eXjk ) as the observation
probability function in our study, which is equivalent to the fact that each noise element Zjk in model (A.2) follows the
standard logistic distribution. Similar to matrix completion, we use ⌦ ✓ [d1 ] ⇥ [d2 ] to denote the index set of the observed
elements. Therefore, given the logistic function f and the index set ⌦, we define F⌦ (U, V) for one-bit matrix completion
as follows
n

F⌦ (U, V) := L⌦ (UV> ) + R(U, V) =

1X
F⌦ (U, V),
n i=1 Si

where L⌦ (UV> ) is the negative log-likelihood function such that
⇢
1 X
>
>
L⌦ (UV ) =
1 Yjk = 1 log f (Uj⇤ V⇤k
) + 1 Yjk =
N
(j,k)2⌦

1 log 1

>
f (Uj⇤ V⇤k
)

.

Therefore, for each component function, we have
F⌦Si (U, V) = L⌦Si (UV> ) + R(U, V),
where {⌦Si }ni=1 denote the mutually disjoint subsets such that [ni=1 ⌦Si = ⌦. In addition, we have |⌦Si | = b for
i = 1, . . . , n such that |⌦| = nb. And L⌦Si (UV> ) is defined as
⇢
1 X
>
>
L⌦Si (UV> ) =
1 Yjk = 1 log f (Uj⇤ V⇤k
) + 1 Yjk = 1 log 1 f (Uj⇤ V⇤k
) .
b
(j,k)2⌦Si

A.2. Theoretical Guarantees for One-bit Matrix Completion
We establish the theoretical guarantee of our algorithm for one-bit matrix completion. We obtain the restricted strong
convexity and smoothness conditions for LN with parameters µ = C1 µ↵ and L = C2 L↵ . In addition, we are able to get
the restricted strong smoothness condition for each component function Li with parameter L0 = c0 L↵ > L. Here, µ↵ and
L↵ are defined as
✓
⇢ 02
⇢
◆
f (x) f 00 (x)
f 02 (x)
f 00 (x)
µ↵  min inf
,
inf
+
,
(A.3)
f 2 (x)
f (x)
f (x))2
1 f (x)
|x|↵
|x|↵ (1
✓
⇢ 02
⇢
◆
f (x) f 00 (x)
f 02 (x)
f 00 (x)
L↵ max sup
,
sup
+
,
(A.4)
f 2 (x)
f (x)
f (x))2
1 f (x)
|x|↵
|x|↵ (1

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

where f (x) is the function used in (A.1), and each element Xjk satisfies |Xjk |  ↵. Note that given the function f (x) and
constant ↵, we can calculate µ↵ and L↵ , which are fixed constants and do not rely on dimension of the unknown low-rank
matrix. For example, if we have logistic function, we can get µ↵ = e↵ /(1 + e↵ )2 and L↵ = 1/4. Furthermore, we define
↵ as follows, which reflects the steepness property of the sample loss function LN (·)
⇢
|f 0 (x)|
sup
.
(A.5)
↵
f (x)
|x|↵ f (x) 1
Moreover, we can derive the upper bound of the rLN (X⇤ ) in terms of spectral norm. If we choose the step size ⌘ = c01 / 1 ,
where c01 = µ0 / c00  , and the inner loop iterations m c02 2 , where c00 , c01 and c02 are some constants, then we have the
following convergence result of our algorithm for the model of matrix completion.
Corollary A.1. Consider one-bit matrix completion under uniform sampling model with log-concave function f in (A.1).
Suppose X⇤ satisfies the incoherence condition. There exist constants {ci }7i=1 such that if we choose parameters ⌘ =
c1 / 1 , where c1 = µ0 / c2  , m
c3 2 , and the number of observations satisfies N
c4 r2 d log d, then for any initial
p
e 0 2 B(c5 r ), with probability at least 1 c6 /d, the output of our Algorithm 1 satisfies
solution Z
⇥
⇤
e S , Z⇤ )  ⇢S d2 (Z
e 0 , Z⇤ ) + c7 max{
E d2 ( Z

2
2 2 rd log d
,
↵, r
1}

N

(A.6)

where the contraction parameter ⇢ < 1.
p
Remark A.2. For one-bit matrix completion, our algorithm achieves O r d log d/N statistical error after
O log(N/(r2 d log d)) number of outer loop iterations. We note that this statistical error is near optimal, compared
p
with the minimax lower bound of one-bit matrix completion O rd log d/N ) established in (Davenport et al., 2014; Cai
e S to achieve ✏ accuracy, the overall computational
& Zhou, 2013). Moreover, Remark 3.10 tells us that for our estimator Z
2
3
complexity required by our algorithm is O (N +  b)r d log(1/✏) . Nevertheless, the overall computational complexity
for the state-of-the-art gradient descent based algorithm (Wang et al., 2016) to obtain ✏ accuracy is O N r3 d log(1/✏) .
Therefore, as long as we have   n, our approach is more efficient than the state-of-the-art gradient descent method.
Furthermore, the overall computational complexities for the state-of-the-art projected gradient descent algorithm (Chen
& Wainwright, 2015) and the conditional gradient descent (a.k.a., Frank-Wolfe) algorithm (Ni & Gu, 2016) to obtain ✏
accuracy are both O N r2 log(1/✏) 2 . If we have 2  nr, our method clearly has a lower computational complexity than
theirs.
A.3. Experimental Results for Matrix Sensing and One-bit Matrix Completion
In this section, we present our experimental results for matrix sensing and one-bit matrix completion respectively.
A.3.1. M ATRIX S ENSING
For matrix sensing, we use the same procedure as in matrix completion to generate the unknown low-rank matrix X⇤ .
Then, we obtain linear measurements from the following observation model yi = hAi , X⇤ i + ✏i , where each element of
the sensing matrix Ai follows i.i.d. standard normal distribution. We also consider the same noisy and noiseless settings
as in matrix completion.
b X⇤ k2 /kX⇤ k2 in log
For the results of the convergence rate, Figure 2(a) and 2(c) illustrate the squared relative error kX
F
F
scale versus number of effective data passes for both methods under setting (i). These results show the linear convergence
rate of our method. Most importantly, it clearly demonstrates the superiority of our approach, since our algorithm shows
better performance after the same number of effective data passes compared with the state-of-the-art gradient descent
algorithm (Zheng & Lafferty, 2015; Wang et al., 2016). Since we get results with similar patterns for other settings, we
leave them out for simplicity. Figure 2(b) shows the empirical recovery probability of different methods under setting (i).
The result implies a phase transition around N = 3rd, which is consistent with the optimal sample complexity that N is
linear with rd. Besides, since we get results with similar patterns for other settings, we leave them out to save space. For
the results of statistical error, Figure 2(d) shows, in the noisy case, how the estimation errors scale with the rescaled sample
size N/(rd), which confirms our theoretical results.
2

Note that the overall computational complexities for the projected gradient descent (Chen & Wainwright, 2015) and conditional
gradient descent (Ni & Gu, 2016) algorithms also depend on some problem dependent parameters, which we omit here but actually can
make their computational complexities worse. Please refer to their papers for more accurate complexity results.

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

GD
LRSVRG

2
0

0.3

4

1

GD
LRSVRG

0.8

GD
LRSVRG

3
2

0.6

1
-2

0.4

-4

0.2

-6

0

5

10

15

20

0

0
-1
0

2

4

6

8

-2

10

0

5

10

15

20

squared relative error

4

d1=140,d 2=120,r=4
d =120,d =100,r=3
1
2
d =100,d =80,r=2

0.25

1

0.2

2

0.15
0.1
0.05
0

4

5

6

7

8

9

N=rd

(a) Noiseless Case

(b) Sample Complexity

(c) Noisy Case

(d) Statistical Error

Figure 2. Numerical results for matrix sensing. (a) and (c) Convergence rates for matrix sensing in the noiseless and noisy case, respecb X⇤ k2F /kX⇤ k2F versus number of effective data passes. They demonstrate the linear convergence rate and the
tively: logarithm of kX
superiority of our method; (b) Empirical probability of exact recovery versus N/(rd), which confirms the optimal sample complexity in
b X⇤ k2F /kX⇤ k2F versus N/(rd), which matches the statistical error of our
the noiseless case that N = O(rd); (d) Statistical error: kX
theory.

A.3.2. O NE - BIT M ATRIX C OMPLETION

We use the same settings of X⇤ for one-bit matrix completion as before. In order to obtain X⇤ , we adopt the similar
procedure as in (Davenport et al., 2014; Bhaskar & Javanmard, 2015; Ni & Gu, 2016). In detail, we first randomly
generate U⇤ 2 Rd1 ⇥r , V⇤ 2 Rd2 ⇥r from a uniform distribution on [ 1/2, 1/2]. Then we get X⇤ by X⇤ = U⇤ V⇤> .
Finally, we scale X⇤ to make it satisfies kX⇤ k1 = ↵ = 1. Here we consider the uniform observation model with function
f (Xij ) = (Xij / ) in (A.1), where is the cumulative distribution function of the standard normal distribution, and
is the noise level, which we set it to be = 0.5.
b X⇤ k2 /kX⇤ k2 , which are
For the results of convergence rate, we compute the logarithm of the squared relative error kX
F
F
displayed in Figure 3(a). Note that, for the ease of illustration, we show the results of convergence rate after the first data
pass. The results not only confirm the linear rate of convergence of our algorithm, but also demonstrate the effectiveness
of our method after the same number of effective data passes. Besides, since we get results with similar patterns for other
settings, we leave them out for simplicity. For the results of statistical error, Figure 3(b) illustrates that with the same
percentage of observations, the squared relative error decreases as the ratio r/d decreases. Although our theoretical results
give O(r2 d log d/|⌦|) statistical error, the simulation results suggest that our method can achieve the minimax statistical
error.
1.2

GD
LRSVRG

0
-0.1
-0.2
-0.3
-0.4
-0.5

0

5

10

(a) Convergence Rate

15

squared relative error

0.1

d1=140,d 2=120,r=4
d1=120,d 2=100,r=3
d1=100,d 2=80,r=2

1
0.8
0.6
0.4
0.2

0.2

0.4

0.6

0.8

1

j+j=d1 d2

(b) Statistical Error

Figure 3. Numerical results for one-bit matrix completion. (a) Convergence rates for one-bit matrix completion: logarithm of squared
b
relative error kX
X⇤ k2F /kX⇤ k2F versus number of effective data passes. It illustrates the superiority of our method after the same
b X⇤ k2F /kX⇤ k2F versus
number of effective data passes; (b) Statistical error for one-bit matrix completion: squared relative error kX
|⌦|/(d1 d2 ), which verifies the statistical rate.

B. Proof of the Main Theory
We provide the proof of our main theoretical results in this section. Since we aim to minimize the following objective
function in terms of Z = [U; V]
1
FeN (Z) = FN (U, V) = LN (UV> ) + kU> U
8

V> Vk2F .

(B.1)

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

Therefore, we obtain the corresponding gradient

rU LN (UV> ) + 12 U(U> U
e
rFN (Z) =
rV LN (UV> ) + 12 V(U> U

V> V)
.
V> V)

(B.2)

B.1. Proof of Theorem 3.8

In order to prove Theorem 3.8, we need following lemmas, and we present the corresponding proofs in Sections D.2 and
D.3, respectively.
Lemma B.1 (Local Curvature Condition). Suppose the sample loss function LN satisfies Conditions 3.3 and 3.4. For
e = [U; V]. In addition, we use
any matrix Z = [U; V] 2 R(d1 +d2 )⇥r , where U 2 Rd1 ⇥r and V 2 Rd2 ⇥r , denote Z
⇤e
R = argminR2Q
kZ
Z
Rk
to
denote
the
optimal
rotation
with
respect
to
Z,
and
H = Z Z⇤ R, then the following
F
e
r
inequality holds
µ
µ0 r
1 e> 2
kX X⇤ k2F +
kHk2F + kZ
ZkF
8
10
16
✓
◆
3L + 1
4r
r
kHk4F
+
· krLN (X⇤ )k22 ,
8
µ
2L

hrFeN (Z), Hi

where X = UV> , and µ0 = min{µ, 1}.
Lemma B.2 (Local Smoothness Condition). Assume the component loss function Li satisfies Condition 3.7. Suppose we
e 2 Rd1 ⇥d2 , we denote Z = [U; V] and
randomly pick i 2 [n]. For any U 2 Rd1 ⇥r , V 2 Rd2 ⇥r and rank-r matrix X
e
e
e > U + rLN (X)> U,
X = UV> . Let GU = rU Fi (U, V) rLi (X)V
+ rLN (X)V,
GtV = rV Fi U, V) rLi (X)
and G = [GU ; GV ]. Then we have
e
EkGk2F  24 2L02 kX

+ kU> U

X⇤ k2F + (2L02 + L2 ) · kX

X⇤ k2F · kZk22

V> Vk2F · kZk22 + 12rkrLN (X⇤ )k22 · kZk22 .

Proof of Theorem 3.8. According to stochastic variance reduced gradient descent Algorithm 1, consider iteration t in the
inner loop, we have the following update
Ut+1 = PC1 (Ut

where we denote

⌘GtU ), and Vt+1 = PC2 (Vt

GtU = rU Fit (Ut , Vt )

GtV = rV Fit Ut , Vt )

⌘GtV ),

e t + rLN (X)V
e t,
rLit (X)V
e > Ut + rLN (Xt )> Ut .
rLi (X)
t

Since it is uniformly picked from [n], we have
= rU FN (Ut , Vt ) and E[GtV ] = rV FN (Ut , Vt ), where the
expectation is taken with respect to it . Recall Z = [Ut ; Vt ], and Rt = argminR2Qr kZt Z⇤ RkF as the optimal
rotation with respect to Zt . Denote Ht = Zt Z⇤ Rt and Gt = [GtU ; GtV ]. By induction, for any t
0, we assume
p
Zt 2 B(c2 r ). Thus, by taking the expectation of Ht+1 over it conditioned on Zt , we have
E[GtU ]
t

EkHt+1 k2F  EkPC1 (Ut
 EkUt

= kHt k2F
= kHt k2F

⌘GtU )

⌘GtU

U⇤ Rt k2F + EkPC2 (Vt

U⇤ Rt k2F + EkVt

⌘GtV

2⌘EhGt , Ht i + ⌘ 2 EkGt k2F
2⌘hrFeN (Zt ), Ht i + ⌘ 2 EkGt k2F ,

⌘GtV )

V⇤ Rt k2F

V⇤ Rt k2F

(B.3)

where the first inequality follows from the definition of H , the second inequality follows from the non-expansive property
of the projection PCi onto Ci and the fact that U⇤ 2 C1 ,V⇤ 2 C2 , and the last equality holds because conditioned on Zt ,
EhHt , Gt i = hHt , EGt i = hHt , rFeN (Zt )i, where FeN is defined in (B.1) . According to Lemma B.1, we can obtain the
lower bound of hrFeN (Zt ), Ht i.
hrFeN (Zt ), Ht i

µ t
kX
8 ✓

t

µ0 r
1
X⇤ k2F +
kHt k2F + kUt> Ut
10
16
◆
4r
r
+
· krLN (X⇤ )k22 ,
µ
2L

Vt> Vt k2F

3L + 1
kHt k4F
8

(B.4)

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

where µ0 = min{µ, 1}. According to Lemma B.2, we have
et
EkGt k2F  24 2L02 kX
+ kUt> Ut

p

X⇤ k2F + (2L02 + L2 ) · kXt

X⇤ k2F · kZt k22

Vt> Vt k2F · kZt k22 + 12rkrLN (X⇤ )k22 · kZt k22 .

(B.5)

Note that for any Z 2 B( r /4), denote R as the optimal rotation with respect to Z, we have kZk2  kZ⇤ k2 +
p
kZ Z⇤ Rk2  2 1 . Thus, we have kZt k22  4 1 . Denote Lm = max{L, L0 }, and we let ⌘ = c1 / 1 , where
c1  min{1/32, µ/(1152L2m )}. Therefore, combining (B.4) and (B.5), we have
2⌘hrFeN (Z), Hi + ⌘ 2 EkGt k2F 

⌘µ0 r
⌘(3L + 1)
e t X⇤ k2
kHt k2F +
kHt k4F + 192⌘ 2 1 L02 kX
F
5
4
✓
◆
8r
r
+⌘
+
· krLN (X⇤ )k22 + 48⌘ 2 1 rkrLN (X⇤ )k22 .
µ
L

Note that according to our assumption, kHt k2F  c22 r with c22  2µ0 /(5(3L + 1)). Thus, according to Condition 3.6, we
further have
⌘µ0 r
e t X⇤ k2 + c3 ⌘r✏2 (N, ),
2⌘hrFeN (Z), Hi + ⌘ 2 EkGt k2F 
kHt k2F + 192⌘ 2 1 L02 kX
(B.6)
F
10
holds with probability at least 1
, where c3 48c1 + 8/µ + 1/L. Therefore, plugging (B.6) into (B.3), with probability
at least 1
, we have
✓
◆
⌘µ0 r
e t X⇤ k2 + c3 ⌘r✏2 (N, ).
EkHt+1 k2F  1
· kHt k2F + 192⌘ 2 1 L02 kX
(B.7)
F
10
e =X
e s 1 accordingly. Denote Z
e s = [U
e s; V
e s ], for any s. According to Algorithm
Finally, for a fixed stage of s, we have X
s
e
1, we randomly choose Z after all of the updates are completed. Therefore, we first take summation of the previous
inequality (B.7) over t 2 {0, 1, · · · , m 1}, and then take expectation with regard to all the history, we can get
EkHm k2F

m 1
⌘µ0 r X
EkHt k2F + 192⌘ 2
10 t=0

EkH0 k2F 

e s = argmin
es
For any s, we denote R
R2Qr kZ
Algorithm 1, we have

es
Note that according to Algorithm 1, we have H0 = H
EkHm k2F

es
Note that Z

1

2 B(

es
EkH

p

r /4),

1 2
kF



⌘mµ
10

r

1

es
mEkX

1

m 1
1 X
EkHt k2F .
m t=0

1

X⇤ k2F + c3 ⌘mr✏2 (N, ).

e s . According to the choice of Z
e s in
Z⇤ R

, thus we further obtain

e s k2 + 192⌘ 2
EkH
F

1L

thus according to Lemma F.3, we have
es
kX

02

es = Z
es
Z⇤ RkF and H

e s k2 =
EkH
F

0

1L

es
X⇤ k2F  3kZ⇤ k22 · d2 (Z

1

02

es
mEkX

, Z⇤ ) = 6

1

X⇤ k2F + c3 ⌘mr✏2 (N, ).

es

1 2
kF .

1 kH

where the first inequality follows from Lemma F.3 and the second inequality holds because kZ⇤ k2 =
we obtain
⌘mµ0 r
e s k2  (1152⌘ 2 2 L02 m + 1) · EkH
e s 1 k2 + c3 ⌘mr✏2 (N, ),
EkH
F
1
F
10
holds with probability at least 1
, which gives us following contraction parameter
✓
◆
10
1
02
⇢= 0
+ 1152⌘L .
µ
⌘m 1

p

2

r.

Therefore,

Note that ⌘ = c1 / 1 , hence we can let ⇢ 2 (0, 1) by choosing sufficiently small constant c1 and sufficiently large number
of iterations m. Therefore, with probability at least 1
, we can get
e s k2  ⇢EkH
e s 1 k2 + 10c3 · r✏2 (N, ).
EkH
F
F
µ0 r

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

C. Proofs of Specific Models
C.1. Proof of Corollary 3.12
In order to prove the theoretical guarantees for matrix sensing, we only need to verify the restricted strong convexity
and smoothness conditions for sample loss function LN , the restricted strong smoothness condition
each component
Pfor
r
function LSi and the upper bound of krLN (X⇤ )k2 . In the following discussions, we use kAk⇤ = i=1 i (A) to denote
the nuclear norm of matrix A, where r is the rank of A.
First, we briefly introduce the definition of ⌃-ensemble which has been used in (Negahban & Wainwright, 2011) to verify
the similar property of random sensing matrix Ai with dependent elements. Let vec(Ai ) 2 Rd1 d2 be the vectorization of
sensing matrix Ai . If vec(Ai ) ⇠ N (0, ⌃), we say that the sensing matrix Ai is sampled from ⌃-ensemble. In addition, we
define ⇡ 2 (⌃) = supkuk2 =1,kvk2 =1 Var(u> Av). Specifically, in classical matrix sensing, we have ⌃ = I and ⇡(I) = 1.

Recall that we have linear measurement operators AN (X) = (hA1 , Xi, hA2 , Xi, . . . , hAN , Xi)> , and ASi (X) =
(hAi1 , Xi, hAi2 , Xi, . . . , hAib , Xi)> for i = 1, . . . , n. In order to prove the restricted strongly convex and smooth conditions of our objective function, we need to ultilize the following lemma, which has been used in (Agarwal et al., 2010;
Negahban & Wainwright, 2011).
Lemma C.1. Suppose each sensing matrix Ai of the linear measurement operator AM is sampled from ⌃-ensemble,
where M is the number of sensing matrices. Then there exist constants C0 and C1 , such that the following inequalities
hold for all 2 Rd1 ⇥d2 with probability at least 1 exp( C0 M )
kA( )k22
1 p
⌃vec( )
M
2
kA( )k22
1 p

⌃vec( )
M
2

d
M
d
2
+ C1 ⇡ 2 (⌃)
2
M
2
2

C1 ⇡ 2 (⌃)

2
,
⇤

(C.1)

2
,
⇤

(C.2)

where d = max{d1 , d2 }.
In order to bound the gradient of sample loss function rLM (X⇤ ) with respect to M observations, we need to ultilize the
following lemma, which has been used in (Negahban & Wainwright, 2011).
Lemma C.2. Suppose each sensing matrix Ai of the linear measurement operator AM is sampled from ⌃-ensemble,
p
where M is the number of sensing matrices. Furthermore, suppose the noise vector ✏ satisfies that k✏k2  2⌫ M . Then
we have the following inequality
r
M
1 X
d
✏i Ai  C⌫
,
M i=1
M
2
holds with probability at least 1

C1 exp( C2 d), where C, C1 and C2 are universal constants.
p
Note that Lemma C.2 requires the noise vector ✏ satisfies k✏k2  2⌫ M for some constant ⌫. For any bounded noise
vector, this condition obviously holds. And if the noise vector follows sub-Gaussian distribution with parameter ⌫, it has
been proved in (Vershynin, 2010) that this condition holds with high probability.
Proof of Corollary 3.12. First, we prove the restricted strong convexity condition for sample loss function LN . First, we
have
LN (X) =

N
1 X
hAi , Xi
2N i=1

Consider two rank-r matrices X, Y 2 Rd1 ⇥d2 . Let

yi

2

=Y

LN (X) hrLN (X), i
N ✓
1 X
=
hAi , Y X⇤ i2 hAi , X
2N i=1

=

N
1 X
hAi , X
2N i=1

X⇤ i

2

✏i .

X, then we have the following equality

LN (Y)

=

kA( )k22
.
2N

⇤ 2

X i

2hAi , X

⇤

X ihAi ,

i

◆
(C.3)

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

Therefore, according to (C.3), in order to establish the restricted strongly convex and smooth conditions for LN , we need
to bound the term kA( )k22 /N . According to (C.1) in Lemma C.1, we get
kA( )k22
N
Furthermore, note that
implies

2
2

C1 ⇡ 2 (⌃)

d
N

2
.
⇤

X has rank at most 2r. Thus, we conclude that k k⇤ 

= Y

kA( )k22
N
Therefore, as long as N

1 p
⌃vec( )
2

C3 ⇡ 2 (⌃)rd/

⇢

min (⌃)

min (⌃)

2C1 r⇡ 2 (⌃)

2

d
N

p

2rk kF , which further

2
.
F

for some sufficiently large constant C3 , we get

kA( )k22
N

4

min (⌃)

9

k k2F .

Since ⌃ = I in matrix sensing, we obtain the restricted strongly convex parameter µ = 4/9.
Second, we prove the restricted strong smoothness condition for LN using (C.2) in Lemma C.1. Similar to the proof of the
restricted strong convexity condition, we get
kA( )k22
5

N

max (⌃)

9

k k2F ,

as long as N
C3 ⇡ 2 (⌃)rd/ min (⌃) for some sufficiently large constant C3 . Therefore, because ⌃ = I in matrix
sensing, we accordingly obtain L = 5/9.
Next, we prove each component loss function LSi is restricted strongly smooth, for i = 1, . . . , n. Recall that we have
LSi (X) =

1
kySi
2b

ASi (UV> )k22 =

1 X
2b

j2⌦Si

hAj , X

X⇤ i

2

✏j .

Thus, for each component loss function LSi , where i = 1, . . . , n, we have
LSi (Y)

LSi (X)

hrLSi (X),

i=

kASi ( )k22
.
2b

(C.4)

Following the same steps as in the proof of restricted strong smoothness condition for LN , for each component function
LSi , we get
kASi ( )k22
 C5
b
if b

C4 ⇡ 2 (⌃)rd/

min (⌃)

max (⌃)k

k2F ,

for some sufficiently large constant C4 . Therefore we have L0 = C5 since ⌃ = I.

Finally, we bound the statistical error term krLN (X⇤ )k22 . According to the definition of LN , we have
rLN (X⇤ ) =

N
1 X
✏ i Ai .
N i=1

Based on Lemma C.2, we have the following inequality holds with probability at least 1
r
N
1 X
d
✏i Ai  C⌫
,
N i=1
N
2
which implies that

krLN (X⇤ )k22  C 2 ⌫ 2

d
.
N

C10 exp( C20 d)

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

C.2. Proof of Corollary 3.14
In order to prove the theoretical guarantees for matrix completion we only need to verify the restricted strong convexity
and smoothness conditions for sample loss function L⌦ , the restricted strong smoothness condition for each component
function L⌦Si and the upper bound of krL⌦ (X⇤ )k2 .
To establish the restricted strong convexity and smoothness conditions for L⌦ , and the restricted strong smoothness condition for L⌦Si we need to ultilize the following lemma, which used in (Negahban & Wainwright, 2012).
Lemma C.3. Suppose the number of observations M satisfying M > c1 rd log d. Furthermore, if for all
have
r
d1 d2 k k1,1 k k⇤
1p
·

n/(d log d),
r
k kF
k kF
c2

2 Rd1 ⇥d2 , we
(C.5)

then the following inequality holds with probability at least 1
kA( )k2
p
n

k k
p F
d1 d2

c3 /d
p
✓
◆
k kF
c5 d1 d2 k k1,1
p
 c4 p
1+
.
nk kF
d1 d2

where c1 , c2 , c3 , c4 , c5 are universal constants .
Moreover, in order to upper bound of the gradient rL⌦ at X⇤ , we need to use the following lemma.

Lemma C.4. (Negahban & Wainwright, 2012) Suppose Ai is uniformly distributed over X . In addition, each noise
✏i follows i.i.d. zero mean distribution with variance ⌫ 2 . Then the following inequality holds with probability at least
1 c1 /d
M
1 X
✏ i Ai
M i=1

2

 c2 ⌫

r

d log d
,
d1 d2 M

where M is the number of observations, and c1 , c2 are universal constants.
Proof of Corollary 3.14. Let |⌦| = N , |⌦Si | = b. For any (j, k) 2 ⌦, we denote Ajk = ej e>
k , where ej , ek are unit
vectors with dimensionality d1 and d2 respectively. Similarly, for any (j, k) 2 ⌦Si , we let Aijk = eij ei>
k . Thus, we can
rewrite the sample loss function as follows (here for simplicity, we use LN and LSi to denote L⌦ and L⌦Si respectively)
LN (X) :=

1 X
2p

(j,k)2⌦

hAjk , Xi

2

Yjk ,

where p = N/(d1 d2 ). In addition, we can rewrite each component loss function as follows
LSi (X) :=

1
2p0

X

(j,k)2⌦Si

hAijk , Xi

2

Yjk ,

where p0 = b/(d1 d2 ). For simplicity we let A and ASi be the corresponding transformation operator with respect to LN
and LSi , respectively. First, we prove the restricted strong convexity and smoothness conditions for LN . Consider any two
rank-r matrices X, Y, which satisfy the incoherence condition. In the following discussion, denote = Y X.
Case 1: If condition (C.5) is violated. Then we obtain
k
where ↵ = r

1/

p

k2F

 C0

p

d1 d2 k k1 · k k⇤

d1 d2 . Furthermore, we get
k

k2F

 2C0

p

r

p
d log d
 2C0 ↵ d1 d2 k k⇤
rN

p
2↵ d1 d2 k kF

r

d log d
,
N

r

d log d
,
rN

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

where the inequality holds because rank( )  2r, which implies the following bound
1
d log d
k k2F  C↵2
.
d1 d2
N

(C.6)

Case 2: If condition (C.5) is satisfied. We first establish the restricted strongly convex condition for LN . In particular, we
have
LN (Y) LN (X) hrLN (X), i
1 X
=
hAjk , Y X⇤ i2 + hAjk , X
2p

X⇤ i 2

(j,k)2⌦

=

kA( )k22
,
2p

2hAjk , X

X⇤ ihAjk ,

i
(C.7)

p
Thus, as long as c5 d1 d2 k k1,1 /k kF

p

N , by the definition of spikiness ration ↵sp ( ), we get
1
1
k k2F  c0 ↵2 .
d1 d2
N

(C.8)

p
p
If c5 d1 d2 k k1,1 /k kF  N , according to Lemma C.3, we obtain
kA( )k22
p

8
k k2F ,
9

which implies the restricted strong convexity parameter µ = 8/9.
Next, for sample loss function LN , we
p smoothness condition by similar proof. According to
p establish the restricted strong
(C.7) and Lemma C.3, as long as c5 d1 d2 k k1,1 /k kF  N , we have
kA( )k22
10

k k2F ,
p
9

which gives us the restricted strong smoothness parameter L = 10/9.
Similarly, we show the restricted strong smoothness condition for each component loss function LSi , where i = 1, . . . , n.
Since we have
kASi ( )k22
LSi (Y) LSi (X) hrLSi (X), i =
,
2p0
p
p
thus according to Lemma C.3, as long as c5 d1 d2 k k1,1 /k kF / b  c6 , we have
kASi ( )k22
 c7 k k2F ,
p0

p
which implies that L0 = c7 . Otherwise, it is sufficient to ensure ↵ = O(1/ n).
Finally, for the statistical error term krLN (X⇤ )k22 , according to the definition of LN , we have
1 X
rLN (X⇤ ) =
✏jk Ajk .
p
(j,k)2⌦

Remember that each elements of the noise matrix follows i.i.d. Gaussian distribution with variance ⌫ 2 /d1 d2 . Therefore,
according to Lemma C.4, we obtain
r
1 X
d log d
✏jk Ajk  C⌫
,
p
N
2
(j,k)2⌦

holds with probability at least 1

C 0 /d, which implies that

d log d
,
(C.9)
N
holds with probability at least 1 C 0 /d. Combining error bounds (C.6), (C.8) and (C.9), we conclude the following upper
bound in Condition 3.6 as C1 max{r 2 1 , ⌫ 2 }d log d/|⌦|.
krLN (X⇤ )k22  C 2 ⌫ 2

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

C.3. Proof of Corollary A.1
In order to prove the theoretical guarantees for one-bit matrix completion, we only need to verify the restricted strong
convexity and smoothness conditions for the sample loss function L⌦ , the restricted strong smoothness condition for each
component function L⌦Si , and the upper bound of krL⌦ (X⇤ )k2 . Note that we have the estimator X satisfies incoherence
p
condition such that kXk1,1  r 1 / d1 d2p
. Thus we should consider the twice differentiable function f (x) = g(x/⌧ ),
where ⌧ is a scale parameter with order O(⌫/ d1 d2 ).
Proof of Corollary A.1. Let |⌦| = N , |⌦Si | = b. For any (j, k) 2 ⌦, we denote Ajk = ej e>
k , where ei , ej are unit vectors
with d1 and d2 dimensions. Similarly, for any (j, k) 2 ⌦Si , we let Aijk = eij ei>
.
Note
that
for simplicity we use A and
k
ASi to denote the corresponding transformation operator with respect to LN and LSi , respectively. We can rewrite the
sample loss function as follows (here for simplicity, we use LN and LSi to denote L⌦ and L⌦Si respectively)
⇢
1 X
LN (X) :=
1 Yjk = 1 log g(hAjk , Xi/⌧ ) + 1 Yjk = 1 log 1 g(hAjk , Xi/⌧ ) ,
N
(j,k)2⌦

Therefore, we have each component loss function LSi as
⇢
1 X
LSi (X) :=
1 Yjk = 1 log g(hAijk , Xi/⌧ ) + 1 Yjk =
b

1 log 1

g(hAijk , Xi/⌧ )

,

(j,k)2⌦Si

Therefore, we get
p
✓
d1 d2 X
rLN (X) =
N⌫
(j,k)2⌦

g 0 (hAjk , Xi/⌧ )
g 0 (hAjk , Xi/⌧ )
1 Yjk = 1 +
1 Yjk =
g(hAjk , Xi/⌧ )
1 g(hAjk , Xi/⌧ )

1

◆

Ajk .

(C.10)

Furthermore, we obtain
r2 LN (X) =

1 X
Bjk (X)vec(Ajk )vec(Ajk )> ,
p⌫ 2

(C.11)

(j,k)2⌦

where we have
✓

◆
g 02 (hAjk , Xi/⌧ )
f 00 (hAjk , Xi)
Bjk (X) =
1 Yjk = 1
g 2 (hAjk , Xi/⌧ )
g(hAjk , Xi/⌧ )
✓
◆
f 00 (hAjk , Xi)
g 02 (hAjk , Xi/⌧ )
+
1 Yjk =
1 g(hAjk , Xi/⌧ ) (1 g(hAjk , Xi/⌧ )2

1

.

First, we establish the strong convexity and smoothness conditions for LN . For any X, M 2 Rd1 ⇥d2 , let W = M+a(X
M) for a 2 [0, 1], x = vec(X) and m = vec(M). According to the mean value theorem, we get
LN (X) = LN (M) + hrLN (M), X

1
Mi + (x
2

m)> r2 LN (W)(x

m),

Moreover, according to (C.11), we further obtain
(x

m)> r2 LN (W)(x

m) =

1 X
Bjk (W)hvec(Ajk )> (x
p⌫ 2
(j,k)2⌦

=

1 X
Bjk (W)hAjk ,
p⌫ 2
(j,k)2⌦

where

=X

m), vec(Ajk )> (x

i2 ,

M. Thus, according to the definition of µ↵ in (A.3), we obtain
1 X
Bjk (W)hAjk ,
p⌫ 2
(j,k)2⌦

i2

µ↵

kA( )k22
,
p⌫ 2

m)i

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

Therefore, following the same steps in the proof of matrix completion, we have
µ↵

kA( )k22
p⌫ 2

C1 µ↵ k k2F ,

which implies
1
Mi + C1 µ↵ k k2F .
2
Therefore it gives us the restricted strong convexity parameter µ = C1 µ↵ . On the other hand, according to the definition
of L↵ in (A.4), we obtain
LN (X)

LN (M) + hrLN (M), X

1 X
Bjk (W)hAjk ,
p⌫ 2
(j,k)2⌦

i 2  L↵

kA( )k22
,
p⌫ 2

Therefore, following the same steps in the proof of the restricted strong smoothness condition in matrix completion, we get
1
Mi + C2 L↵ k k2F ,
2
which implies the restricted strong smoothness parameter L = C2 L↵ . By the similar procedure, for each component
function, we can derive that
LN (X)  LN (M) + hrLN (M), X

LSi (X)  LSi (M) + hrLSi (M), X

1
Mi + C3 L↵ k k2F .
2

Finally, for the term krLN (X⇤ )k22 , according to Lemma C.4, we have
s
d log d
⇤
krLN (X )k2  C ↵
,
|⌦|

(C.12)

where ↵ is defined in (A.5). In addition, we also have the following bounds, which have been shown in proofs of matrix
completion when condition (C.5) is not satisfied
⇢
1
1
rd log d
2
k kF  max C↵2
, C 0 ↵2
.
(C.13)
d1 d2
|⌦|
|⌦|
Therefore, combining error bounds (C.13) and (C.12), we have the following bound in Condition 3.6
C max{r 2 1 , ↵2 }d log d/|⌦|.

D. Proofs of Technical Lemmas
In this section, we present the proofs of several technical lemmas. Before proceeding to the theoretical proof, we first
introduce the following notations and definitions, which are essential for proving the following lemmas. For any Z 2
kZ
R(d1 +d2 )⇥r , we denote Z = [U; V], where U 2 Rd1 ⇥r and V 2 Rd2 ⇥r . Denote X = UV> . Let R = argminR2Q
e
r
⇤e
⇤
d1 ⇥r
d2 ⇥r
Z RkF be the optimal rotation with respect to Z, and H = Z Z R = [HU ; HV ], where HU 2 R
, HV 2 R
.

e as the matrix spanned by the
Moreover, let U1 , U2 , U3 be the left singular matrices of X, U, HU , respectively. Define U
column of U1 , U2 and U3 such that
e = span U1 , U2 , U3 = col(U1 ) + col(U2 ) + col(U3 ).
col(U)

(D.1)

e is a basis vector. In addition, we define the sum of two subspaces
Note that for the above subspace, each column vector of U
e is an orthonormal matrix with at most 3r columns.
U1 , U2 as U1 + U2 = {u1 + u2 | u1 2 U1 , u2 2 U2 }. Obviously, U
e as the matrix spanned by the
Similarly, let V1 , V2 , V3 be the right singular matrices of X, V, HV , respectively. Define V
column of V1 , V2 and V3 such that
e = span V1 , V2 , V3 = col(V1 ) + col(V2 ) + col(V3 ),
col(V)

e has at most 3r columns.
where V

(D.2)

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

D.1. Proof of Lemma 3.5
Proof. Define reference function fY : Rd1 ⇥d2 ! R such that
e = LN (X)
e
fY (X)

e
hrLN (Y), X

LN (Y)

(D.3)

Yi.

e with rank at most 3r.
Since LN satisfies restricted strong convexity Condition 3.3, we have fY (X) 0, for any matrix X
>
e and U> U1 = Ir , we have
Obviously, fY (Y) = 0. Recall the SVD of X is X = U1 ⌃1 V1 . Since col(U1 ) ✓ col(U)
1
1
eU
e > X = X. Thus we have
U
eU
e > [X
0 = fY (Y)  min fY U
⌘

⌘rfY (X)]

eU
e > rfY (X)
= min fY X ⌘ U
⌘
⇢
eU
e > rfY (X)
 min LN X ⌘ U
⌘

LN (Y)

hrLN (Y), X

eU
e > rfY (X)
⌘U

Yi ,

(D.4)

eU
e > has rank at most 3r, and the
where the first inequality holds because rank(AB)  min{rank(A), rank(B)} and U
last inequality follows from (D.3). Since LN satisfies restricted strong smoothness Condition 3.4, we have
LN X

eU
e > rfY (X)  LN (X) + hrLN (X), ⌘ U
eU
e > rfY (X)i + L k⌘ U
eU
e > rfY (X)k2 .
⌘U
F
2

Thus, plugging (D.5) into (D.4), we have
⇢
2
eU
e > rfY (X)k2
eU
e > rfY (X)i + ⌘ L kU
fY (Y)  min fY (X) ⌘hrLN (X) rLN (Y), U
F
⌘
2
⇢
2
e > rfY (X)k2 + ⌘ L kU
e > rfY (X)k2
= fY (X) + min
⌘kU
F
F
⌘
2
1 e>
= fY (X)
kU rfY (X)k2F ,
2L

(D.5)

(D.6)

e >U
e = Ir and rfY (X) = rLN (X)
where the first equality follows from (D.3), the second inequality holds because U
1
rLN (Y), and the last equality holds because the minimizer is ⌘ = 1/L. Thus, plugging the definition of fY into (D.6),
we obtain
LN (X)

LN (Y)

hrLN (Y), X

Yi

1 e>
kU (rLN (X)
2L

rLN (Y))k2F

0.

(D.7)

e is orthonormal matrix with at most 3r columns and XV
eV
e > = X, following the same techniques, we obtain
Since V
LN (X)

LN (Y)

hrLN (Y), X

Yi

Therefore, combining (D.7) and (D.8), we complete the proof.

1
k(rLN (X)
2L

e 2
rLN (Y))Vk
F

0.

(D.8)

D.2. Proof of Lemma B.1
In order to prove the local curvature condition, we need to make use of the following lemmas. In Lemma D.1, we denote
e 2 R(d1 +d2 )⇥r as Z
e = [U; V]. Recall Z = [U; V], then we have kU> U V> Vk2 = kZ
e > Zk2 , and rZ (kU> U
Z
F
F
eZ
e > Z. We refer Wang et al. (2016) to readers for a detailed proof of Lemma D.1. Lemma D.2, proved in
V> Vk2F ) = 4Z
Section E.1, is a variation of the regularity condition of the sample loss function LN (Tu et al., 2015), which is essential to
derive the linear convergence rate in our main theorem.
Lemma D.1. (Wang et al., 2016) Let Z, Z⇤ 2 R(d1 +d2 )⇥r . Denote the optimal rotation with respect to Z as R =
e F , and H = Z Z⇤ R. Consider the gradient of the regularization term kZ
e > Zk2 , we have
argminR2Q
kZ Z⇤ Rk
e
F
r
e = [U; V].
where Z

eZ
e > Z, Hi
hZ

1 e> 2
kZ ZkF
2

1 e>
kZ ZkF · kHk2F ,
2

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

Lemma D.2. Suppose the sample loss function LN satisfies Conditions 3.3 and 3.4. For any rank-r matrices X, Y 2
>
Rd1 ⇥d2 , let the singular value decomposition of X be U1 ⌃1 V1 , then we have
hrLN (X)

rLN (Y), X

1 e>
kU (rLN (X) rLN (Y))k2F
4L
1
e 2 + µ kX
+
k(rLN (X) rLN (Y))Vk
F
4L
2

Yi

Yk2F ,

e 2 Rd1 ⇥r1 is an orthonormal matrix with r1  3r satisfying col(U1 ) ✓ col(U),
e and V
e 2 Rd2 ⇥r2 is an
where U
e
orthonormal matrix with r2  3r satisfying col(V1 ) ✓ col(V).
Now, we are ready to prove Lemma B.1.

Proof of Lemma B.1. According to (B.2), we have
1 e e>
hrFeN (Z), Hi = hrU LN (UV> ), HU i + hrV LN (UV> ), HV i + hZ
Z Z, Hi,
|
{z
} 2|
{z
}
I1

(D.9)

I2

e = [U; V]. Recall that X⇤ = U⇤ V⇤> , and X = UV> . Note that rU LN (UV> ) = rLN (X)V, and
where Z
rV LN (UV> ) = rLN (X)> U. Thus, for the term I1 in (D.9), we have
I1 = hrLN (X), UV>
= hrLN (X)
|

U⇤ V⇤> + HU H>
Vi

rLN (X⇤ ), X
{z
I11

⇤
X⇤ + HU H>
X⇤ + HU H>
V i + hrLN (X ), X
V i.
} |
{z
}

(D.10)

I12

e and V
e in (D.1) and (D.2), respectively. According to
First, we consider the term I11 in (D.10). Recall the definition of U
Lemma D.2, we have
hrLN (X)

rLN (X⇤ ), X

X⇤ i

Second, for the remaining term in I11 , we have
hrLN (X)

1 e>
kU (rLN (X) rLN (X⇤ ))k2F
4L
1
e 2 + µ kX
+
k(rLN (X) rLN (X⇤ ))Vk
F
4L
2

e>
rLN (X⇤ ), HU H>
V i = hU (rLN (X)
e > (rLN (X)
 kU

1 e>
(rLN (X)
 kU
2

X⇤ k2F .

e > HU H> i
rLN (X⇤ )), U
V
e > k2 · kHU H> kF
rLN (X⇤ ))kF · kU

(D.11)

V

rLN (X⇤ ))kF · kHk2F ,

(D.12)

eU
e > HU = HU , the first inequality holds because |hA, Bi|  kAkF · kBkF and
where the equality holds because U
e is orthonormal.
kABkF  kAk2 · kBkF , and the second inequality holds because 2kABkF  kAk2F + kBk2F and U
Similarly, we have
hrLN (X)

rLN (X⇤ ), HU H>
Vi 

Thus combining (D.12) and (D.13), we have
hrLN (X)

rLN (X⇤ ), HU H>
Vi 

1
k(rLN (X)
2

e F · kHk2 .
rLN (X⇤ ))Vk
F

1 e>
kU (rLN (X) rLN (X⇤ ))kF · kHk2F
4
1
e F · kHk2 .
+ k(rLN (X) rLN (X⇤ ))Vk
F
4

(D.13)

(D.14)

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

Therefore, combining (D.11) and (D.14), the term I11 can be lower bounded by
I11

1
e > (rLN (X) rLN (X⇤ ))k2 + k(rLN (X) rLN (X⇤ ))Vk
e 2 + µ kX X⇤ k2
kU
F
F
F
4L
2
1 e>
e F · kHk2
kU (rLN (X) rLN (X⇤ ))kF + k(rLN (X) rLN (X⇤ ))Vk
F
4
µ
L
kX X⇤ k2F
kHk4F ,
2
8

(D.15)

where the last inequality holds because 2ab  ca2 + b2 /c, for any c > 0. Next, for the term I12 in (D.10), we have
p
hrLN (X⇤ ), X X⇤ i  krLN (X⇤ )k2 · kX X⇤ k⇤  2rkrLN (X⇤ )k2 · kX X⇤ kF ,
(D.16)
where the first inequality is due to the Von Neumann trace inequality, and the second inequality is due to the fact that
rank(X X⇤ )  2r. Similar for the remaining term in I12 , we have
p
|hrLN (X⇤ ), HU H>
2rkrLN (X⇤ )k2 · kHU H>
(D.17)
Vi 
V kF .
Thus, combining (D.16) and (D.17), the term I12 can be lower bounded by
✓
◆
p
1
I12
2rkrLN (X⇤ )k2 · kX X⇤ kF + kHk2F
2
✓
◆
µ
L
4r
r
kX X⇤ k2F
kHk4F
+
· krLN (X⇤ )k22 ,
8
4
µ
2L

(D.18)

where the first inequality follows from the fact that 2kABkF  kAk2F + kBk2F , and the last inequality is due to 2ab 
ca2 + b2 /c, for any c > 0. Therefore, plugging (D.15) and (D.18) into (D.10), we obtain the lower bound of I1
✓
◆
3µ
3L
4r
r
I1
kX X⇤ k2F
kHk4F
+
· krLN (X⇤ )k22 .
(D.19)
8
8
µ
2L
On the other hand, for the term I2 in (D.9), according to lemma D.1, we have
I2

1 e> 2
kZ ZkF
2

1 e>
kZ ZkF · kHk2F
2

1 e> 2
kZ ZkF
4

1
kHk4F ,
4

where the last inequality holds because 2ab  a2 + b2 . By plugging (D.19) and (D.20) into (D.9), we have
✓
◆
3µ
1 e> 2
3L + 1
4r
r
hrFeN (Z), Hi
kX X⇤ k2F + kZ
ZkF
kHk4F
+
· krLN (X⇤ )k22 .
8
8
8
µ
2L
e ⇤ = [U⇤ ; V⇤ ], then we obtain
Furthermore, denote Z
e > Zk2 = hZZ>
kZ
F
hZZ>

= kUU>

eZ
e>
Z⇤ Z⇤> , Z
eZ
e>
Z⇤ Z⇤> , Z

e ⇤Z
e ⇤> i + hZ⇤ Z⇤> , Z
eZ
e > i + hZZ> , Z
e ⇤Z
e ⇤> i
Z
e ⇤Z
e ⇤> i
Z

U⇤ U⇤> k2F + kVV>

V⇤ V⇤> k2F

2kUV>

U⇤ V⇤> k2F ,

e ⇤> Z⇤ = 0, and the inequality is due to hAA> , BB> i = kA> Bk2
where the first equality is due to Z
F
according to Lemma F.2, we have
p
e > Zk2 = kZZ> Z⇤ Z⇤> k2
4kX X⇤ k2F + kZ
4( 2 1) r kHk2F ,
F
F

where the first inequality holds because of (D.22), and the second inequality is due to Lemma F.2 and the fact that
2 r . Denote µ0 = min{µ, 1}. Therefore, plugging (D.23) into (D.21), we have
hrFeN (Z), Hi
which completes the proof.

µ
µ0 r
1 e> 2
kX X⇤ k2F +
kHk2F + kZ
ZkF
8
10 ✓
16
◆
3L + 1
4r
r
kHk4F
+
· krLN (X⇤ )k22 ,
8
µ
2L

(D.20)

(D.21)

(D.22)
0. Thus,
(D.23)
2
⇤
r (Z )

=

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

D.3. Proof of Lemma B.2
Proof. Consider the term GU first. Denote X = UV> . According to the definition of GU , we have
kGU k2F = krU Fi (U, V)

2
e
e
rLi (X)V
+ rLN (X)Vk
F

2

1
e
e
rLi (X)V
+ rLN (X)V
+ U(U> U V> V)
2
F
1
2
>
>
e
e
 2 krLi (X)V rLi (X)V + rLN (X)VkF + kU U V Vk2F · kUk22 ,
|
{z
} 2
= rLi (X)V

(D.24)

I1

where the second equality follows from definition of Fi in (2.4), and the inequality holds because kA + Bk2F  2kAk2F +
2kBk2F and kABkF  kAk2 · kBkF . As for the term I1 in (D.24), we further have
e
e
I1 = krLi (X)V rLi (X)V
+ rLN (X)V
rLN (X)V + rLN (X)V
2
⇤
e U k + 3krLN (X)V rLN (X )Vk2 + 3krLN (X⇤ )Vk2
 3kG
F

F

e U k2 + 3krLN (X)V
 3kG
F

rLN (X

⇤

)Vk2F

rLN (X⇤ )V + rLN (X⇤ )Vk2F

F

+ 3rkrLN (X

⇤

)k22

· kVk22 ,

(D.25)

e U = rLi (X)V rLN (X)V rLi (X)V
e
e
where we define G
+ rLN (X)V,
the second inequality holds because
2
2
2
2
kA + B + CkF  3kAkF + 3kBkF + 3kCkF , and the last inequality holds because kABkF  kAk2 · kBkF and V has
rank r. Thus combining (D.24) and (D.25), we have
e U k2 +6 krLN (X)V rLN (X⇤ )Vk2
EkGU k2F  6 EkG
F
|
{z
}
| {z F}
I2

I3

1
+ kU> U
2

V> Vk2F · kUk22 + 6rkrLN (X⇤ )k22 · kVk22 ,

(D.26)

where the expectation is taken with respect to i. Next, we are going to upper bound I2 and I3 , respectively. First, let us
e
consider I2 in (D.26). Since i is uniformly picked from [n], we have E[rLi (X)V] = rLN (X)V and E[rLi (X)V]
=
e
e in (D.2), we have
rLN (X)V.
Recall the definition of V
e U k2 = E [rLi (X)V
EkG
F
 E rLi (X)V

e
rLi (X)V]

E[rLi (X)V

2
F
2
e
e
rLi (X))VkF ·

e
rLi (X)V

 Ek(rLi (X)
n
1X

rLi (X)
n i=1

e > Vk2
kV
2

e V
e
rLi (X)

2
F

e
rLi (X)V]

2
F

· kVk22 ,

(D.27)

where the first inequality holds because Ek⇠ E⇠k22  Ek⇠k22 for any random vector ⇠, the second inequality holds
eV
e > V = V and kABkF  kAk2 · kBkF , and the last inequality holds because kABk2  kAk2 · kBk2 and
because V
e 2 = 1. Similarly, as for the term I3 in (D.26), we have
kVk
I3 =

rLN (X)

eV
e >V
rLN (X⇤ ) V

2
F



rLN (X)

e 2 · kVk2 ,
rLN (X⇤ ) Vk
F
2

(D.28)

eV
e > V = V, and the inequality holds because V
e is orthonormal. Plugging (D.27) and
where the equality holds because V
(D.28) into (D.26), we obtain
n

EkGU k2F 

6X
n i=1

rLi (X)

1
+ kU> U
2

e V
e
rLi (X)

2
F

· kVk22 + 6

rLN (X)

V> Vk2F · kUk22 + 6rkrLN (X⇤ )k22 · kVk22 .

e 2 · kVk2
rLN (X⇤ ) Vk
F
2

(D.29)

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

As for EkGV k2F , by the same techniques, we have
n

EkGV k2F 

6 X e>
U rLi (X)
n i=1
1
+ kU> U
2

e
rLi (X)

2
F

e > rLN (X)
· kUk22 + 6 U

rLN (X⇤ ) k2F · kUk22

V> Vk2F · kVk22 + 6rkrLN (X⇤ )k22 · kUk22 ,

(D.30)

e is defined in (D.1). Recall Z = [U; V] and note that max{kUk2 , kVk2 }  kZk2 . Thus combining (D.29) and
where U
(D.30), we obtain the upper bound of EkGk2F
n

EkGk2F 

12 X
rLi (X)
n i=1 |

+ 12

|

rLN (X)

+ kU> U

e V
e
rLi (X)
⇤

rLN (X )

2
F

e 2
Vk
F

e > rLi (X)
+ U
{z
I4

e > rLN (X)
+ U
{z

e
rLi (X)

2
F

}

· kZk22

rLN (X⇤ ) k2F · kZk22
}

I5

V> Vk2F · kZk22 + 12rkrLN (X⇤ )k22 · kZk22 .

(D.31)

Finally, according to the Lemma 3.5 and the restricted strong smoothness Conditions 3.4 and 3.7, we obtain the upper
bound of I4 and I5
I4  4L0 Li (X)

e
Li (X)

e X
hrLi (X),

e  2L02 kX
Xi

e 2  4L02 (kX
e
Xk
F

X⇤ k2F + kX

X⇤ k2F ),

(D.32)

where the last inequality holds because kA + Bk2F  2kAk2F + 2kBk2F . Similarly we have
LN (X⇤ )

I5  4L LN (X)

hrLN (X⇤ ), X

X⇤ i  2L2 kX

X⇤ k2F .

(D.33)

Hence, plugging (D.32) and (D.33) into (D.31), we obtain
e
EkGk2F  48L02 kX

+ kU> U

X⇤ k2F + 24(2L02 + L2 )kX

X⇤ k2F · kZk22

V> Vk2F + 12rkrLN (X⇤ )k22 · kZk22 ,

which completes the proof.

E. Proof of Technical Lemma in Appendix D
E.1. Proof of Lemma D.2
Proof. By the restricted strong convexity of LN in Condition 3.3, we have
LN (Y)

LN (X) + hrLN (X), Y

Xi +

µ
kX
2

Yk2F .

(E.1)

Besides, according to lemma 3.5, we have
LN (X)

LN (Y)

hrLN (Y), X
+

1
k(rLN (X)
4L

Therefore, combining (E.1) and (E.2), we have
hrLN (X)

which completes the proof.

rLN (Y), X

Yi +

Yi

1 e>
kU (rLN (X)
4L
e 2.
rLN (Y))Vk

rLN (Y))k2F
(E.2)

F

1 e>
kU (rLN (X) rLN (Y))k2F
4L
1
e 2 + µ kX
+
k(rLN (X) rLN (Y))Vk
F
4L
2

Yk2F ,

A Universal Variance Reduction-Based Catalyst for Nonconvex Low-Rank Matrix Recovery

F. Auxiliary lemmas
For the completeness of our proofs, we provide several auxiliary lemmas in this section, which are originally proved in Tu
et al. (2015).
Lemma F.1. (Tu et al., 2015) Assume X, Y 2 Rd1 ⇥d2 are two rank-r matrices. Suppose they have singular value
decomposition X = U1 ⌃1 V1> and Y = U2 ⌃2 V2> . Suppose kX Yk2  r (X)/2, then we have
✓
◆
2
kY Xk2F
1/2
1/2
2
d [U2 ; V2 ]⌃1 , [U1 ; V1 ]⌃2
p
.
2 1
r (X)
Lemma F.2. (Tu et al., 2015) For any matrices Z, Z0 2 R(d1 +d2 )⇥r , we have the following inequality
d2 (Z, Z0 ) 

p

2( 2

1
1)

2
0
r (Z )

kZZ>

Z0 Z0> k2F .

Lemma F.3. (Tu et al., 2015) For any matrices Z, Z0 2 R(d1 +d2 )⇥r , which satisfy d(Z, Z0 )  kZ0 k2 /4, we have the
following inequality
kZZ>

Z0 Z0> kF 

9 0
kZ k2 · d(Z, Z0 ).
4

