Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

Supplemental: Deep Decentralized Multi-task Multi-agent RL under Partial Observability
The following provides additional empirical results. Some plots are reproduced from the main text to ease comparisons.

A. Multi-agent Multi-target (MAMT) Domain Overview

(a) Agents must learn inherent toroidal
transition dynamics in the domain for
fast target capture (e.g., see green target).

(b) In MAMT tasks, no reward is given to
the team above, despite two agents successfully capturing their targets.

(c) Example successful simultaneous
capture scenario, where the team is given
+1 reward.

Figure 6. Visualization of MAMT domain. Agents and targets operate on a toroidal m × m gridworld. Each agent (circle) is assigned
a unique target (cross) to capture, but does not observe its assigned target ID. Targets’ states are fully occluded at each timestep with
probability Pf . Despite the simplicity of gridworld transitions, reward sparsity makes this an especially challenging task. During both
learning and execution, the team receives no reward unless all targets are captured simultaneously by their corresponding agents.

B. Empirical Results: Learning on Multi-agent Single-Target (MAST) Domain
Multi-agent Single-target (MAST) domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.0 (observation flickering disabled). These results mainly illustrate that Dec-DRQN sometimes has some empirical success in
low-noise domains with small state-space. Note that in the 8 × 8 task, Dec-HDRQN significantly outperforms Dec-DRQN,
which converges to a sub-optimal policy despite domain simplicity.
1.0

0.8

4 × 4 (Dec-DRQN)

0.6

5 × 5 (Dec-DRQN)
6 × 6 (Dec-DRQN)

0.4

7 × 7 (Dec-DRQN)

0.2

8 × 8 (Dec-DRQN)

0.0
0.0

5K

10K

15K

Q(s0 , a0 )

Empirical Return

1.0

0.8

4 × 4 (Dec-DRQN)

0.6

5 × 5 (Dec-DRQN)
6 × 6 (Dec-DRQN)

0.4

7 × 7 (Dec-DRQN)

0.2

8 × 8 (Dec-DRQN)

0.0
0.0

20K

5K

Training Epoch

(a) Dec-DRQN empirical returns during training.

20K

1.0

0.8

4 × 4 (Dec-HDRQN)

0.6

5 × 5 (Dec-HDRQN)
6 × 6 (Dec-HDRQN)

0.4

7 × 7 (Dec-HDRQN)

0.2

8 × 8 (Dec-HDRQN)

5K

10K

15K

20K

Training Epoch

(c) Dec-HDRQN empirical returns during training.

Q(s0 , a0 )

Empirical Return

15K

(b) Dec-DRQN anticipated values during training.

1.0

0.0
0.0

10K

Training Epoch

0.8

4 × 4 (Dec-HDRQN)

0.6

5 × 5 (Dec-HDRQN)
6 × 6 (Dec-HDRQN)

0.4

7 × 7 (Dec-HDRQN)

0.2

8 × 8 (Dec-HDRQN)

0.0
0.0

5K

10K

15K

20K

Training Epoch

(d) Dec-HDRQN anticipated values during training.

Figure 7. Multi-agent Single-target (MAST) domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.0 (observation
flickering disabled). All plots conducted (at each training epoch) for a batch of 50 randomly-initialized episodes. Anticipated value plots
(on right) were plotted for the exact starting states and actions undertaken for the episodes used in the plots on the left.

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

C. Empirical Results: Learning on MAMT Domain
Multi-agent Single-target (MAMT) domain results, with 2 agents and Pf = 0.3 (observation flickering disabled). We
also evaluated performance of inter-agent parameter sharing (a centralized approach) in Dec-DRQN (which we called
Dec-DRQN-PS). Additionally, performance of a Double-DQN was deemed to have negligible impacts (Dec-DDRQN).
1.0

0.8

3 × 3 (Dec-DRQN)

0.6

4 × 4 (Dec-DRQN)
5 × 5 (Dec-DRQN)

0.4

6 × 6 (Dec-DRQN)

0.2

7 × 7 (Dec-DRQN)

0.0
0.0

10K

20K

30K

40K

50K

60K

Q(s0 , a0 )

Empirical Return

1.0

0.8

3 × 3 (Dec-DRQN)

0.6

4 × 4 (Dec-DRQN)
5 × 5 (Dec-DRQN)

0.4

6 × 6 (Dec-DRQN)

0.2

7 × 7 (Dec-DRQN)

0.0
0.0

70K

10K

20K

Training Epoch

(a) Dec-DRQN empirical returns during training.

4 × 4 (Dec-DRQN-PS)
5 × 5 (Dec-DRQN-PS)

0.4

6 × 6 (Dec-DRQN-PS)

0.2

7 × 7 (Dec-DRQN-PS)

10K

20K

30K

40K

50K

60K

Q(s0 , a0 )

Empirical Return

3 × 3 (Dec-DRQN-PS)

0.6

3 × 3 (Dec-DRQN-PS)
4 × 4 (Dec-DRQN-PS)
5 × 5 (Dec-DRQN-PS)

0.4

6 × 6 (Dec-DRQN-PS)

0.2

7 × 7 (Dec-DRQN-PS)

10K

20K

30K

40K

50K

60K

70K

Training Epoch

(d) Dec-DRQN-PS (with parameter sharing), anticipated values
during training.

1.0

1.0

0.8

3 × 3 (Dec-DDRQN)

0.6

4 × 4 (Dec-DDRQN)
5 × 5 (Dec-DDRQN)

0.4

6 × 6 (Dec-DDRQN)

0.2

7 × 7 (Dec-DDRQN)

10K

20K

30K

40K

50K

60K

Q(s0 , a0 )

Empirical Return

70K

0.6

0.0
0.0

70K

(c) Dec-DRQN-PS (with parameter sharing), empirical returns
during training.

0.8

3 × 3 (Dec-DDRQN)

0.6

4 × 4 (Dec-DDRQN)
5 × 5 (Dec-DDRQN)

0.4

6 × 6 (Dec-DDRQN)

0.2

7 × 7 (Dec-DDRQN)

0.0
0.0

70K

10K

20K

30K

40K

50K

60K

70K

Training Epoch

Training Epoch

(e) Dec-DDRQN (double DRQN) empirical returns during training.

(f) Dec-DDRQN (double DRQN) anticipated values during training.

1.0

1.0

0.8

3 × 3 (Dec-HDRQN)

0.6

4 × 4 (Dec-HDRQN)
5 × 5 (Dec-HDRQN)

0.4

6 × 6 (Dec-HDRQN)

0.2

7 × 7 (Dec-HDRQN)

10K

20K

30K

40K

50K

60K

70K

Training Epoch

(g) Dec-HDRQN (our approach) empirical returns during training.

Q(s0 , a0 )

Empirical Return

60K

0.8

Training Epoch

0.0
0.0

50K

1.0

0.8

0.0
0.0

40K

(b) Dec-DRQN anticipated values during training.

1.0

0.0
0.0

30K

Training Epoch

0.8

3 × 3 (Dec-HDRQN)

0.6

4 × 4 (Dec-HDRQN)
5 × 5 (Dec-HDRQN)

0.4

6 × 6 (Dec-HDRQN)

0.2

7 × 7 (Dec-HDRQN)

0.0
0.0

10K

20K

30K

40K

50K

60K

70K

Training Epoch

(h) Dec-HDRQN (our approach) anticipated values during training.

Figure 8. MAMT domain results for Dec-DRQN and Dec-HDRQN, with 2 agents and Pf = 0.3. All plots conducted (at each training
epoch) for a batch of 50 randomly-initialized episodes. Anticipated value plots (on right) were plotted for the exact starting states and
actions undertaken for the episodes used in the plots on the left.

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

1.0

0.8

3 × 3 (agent 1)
3 × 3 (agent 2)

Q(o0 , a0 )

4 × 4 (agent 1)
0.6

4 × 4 (agent 2)
5 × 5 (agent 1)
5 × 5 (agent 2)
6 × 6 (agent 1)

0.4

6 × 6 (agent 2)
7 × 7 (agent 1)
7 × 7 (agent 2)

0.2

0.0
0.0

10K

20K

30K

40K

50K

60K

70K

Training Epoch

Figure 9. Comparison of agents’ anticipated value plots using Dec-HDRQN during training. MAMT domain, with 2 agents and Pf =
0.3. All plots conducted (at each training epoch) for a batch of 50 randomly-initialized episodes. For a given task, agents have similar
anticipated value convergence trends due to shared reward; differences are primarily caused by random initial states and independently
sampled target occlusion events for each agent.

1.0
0.8

0.8
3 × 3 (Dec-DRQN)
0.6

4 × 4 (Dec-DRQN)
3 × 3 (Dec-HDRQN)

0.4

4 × 4 (Dec-HDRQN)
0.2
0.0
0.0

10K

20K

30K

40K

Training Epoch

(a) Empirical returns during training. For batch of 50 randomlyinitialized games.

Q(s0 , a0 )

Empirical Return

1.0

3 × 3 (Dec-DRQN)
0.6

4 × 4 (Dec-DRQN)

0.4

3 × 3 (Dec-HDRQN)
4 × 4 (Dec-HDRQN)

0.2
0.0
0.0

10K

20K

30K

40K

Training Epoch

(b) Anticipated values during training. For specific starting states
and actions undertaken in the same 50 randomly-initialized games
as Fig. 10a.

Figure 10. MAMT domain results for Dec-DRQN and Dec-HDRQN, with n = 3 agents. Pf = 0.6 for the 3 × 3 task, and Pf = 0.1 for
the 4 × 4 task.

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

E. Empirical Results: Learning Sensitivity to
Dec-HDRQN Recurrent Training
Tracelength Parameter τ

Empirical Return

1.0
β
β
β
β
β
β

0.8
0.6
0.4
0.2
0.0
0.0

10K

20K

30K

=
=
=
=
=
=

0.0
0.1
0.2
0.3
0.4
0.5

β
β
β
β
β

=
=
=
=
=

0.6
0.7
0.8
0.9
1.0

40K

Training Epoch

1.0

Empirical Return

D. Empirical Results: Learning Sensitivity to
Dec-HDRQN Negative Learning Rate β

β
β
β
β
β
β

0.6
0.4
0.2
0.0
0.0

10K

20K

30K

=
=
=
=
=
=

0.0
0.1
0.2
0.3
0.4
0.5

β
β
β
β
β

=
=
=
=
=

0.6
0.7
0.8
0.9
1.0

40K

Training Epoch

1.0
β
β
β
β
β
β

0.8

Q̄

0.6
0.4
0.2
10K

20K

1
2
4
8

0.2
10K

20K

30K

40K

(a) Dec-HDRQN sensitivity to tracelength τ . 6x6 MAMT domain
with Pf = 0.25.
1.0
0.8
Tracelength
Tracelength
Tracelength
Tracelength

0.6
0.4

=
=
=
=

1
2
4
8

0.2

(b) Sensitivity of Dec-HDRQN predicted action-values to β during
training. For specific starting states and actions undertaken in the
same 50 randomly-initialized games of Fig. 11a.

0.0
0.0

0.4

=
=
=
=

Training Epoch

Q(s0 , a0 )

Q(o0 , a0 )

0.8

Tracelength
Tracelength
Tracelength
Tracelength

0.6

0.0
0.0

(a) Sensitivity of Dec-HDRQN empirical returns to β during training. For batch of 50 randomly-initialized games.
1.0

0.8

30K

=
=
=
=
=
=

0.0
0.1
0.2
0.3
0.4
0.5

β
β
β
β
β

=
=
=
=
=

0.6
0.7
0.8
0.9
1.0

40K

Training Epoch

(c) Sensitivity of Dec-HDRQN average Q values to β during training. For random minibatch of 32 experienced observation inputs.
Figure 11. Learning sensitivity to β for 6 × 6, 2 agent MAMT
domain with Pf = 0.25. All plots for agent i = 0. β = 1
corresponds to Decentralized Q-learning, β = 0 corresponds to
Distributed Q-learning (not including the distributed policy update step).

0.0
0.0

10K

20K

30K

40K

Training Epoch

(b) Sensitivity of Dec-HDRQN predicted action-values to recurrent training tracelength parameter. For specific starting states and
actions undertaken in the same 50 randomly-initialized games of
Fig. 12a.
Figure 12. Learning sensitivity to recurrent training tracelength
parameter for 6 × 6, 2 agent MAMT domain with Pf = 0.25.
All plots for agent i = 0. β = 1 corresponds to Decentralized
Q-learning, β = 0 corresponds to Distributed Q-learning (not including the distributed policy update step).

Deep Decentralized Multi-task Multi-Agent RL under Partial Observability

F. Empirical Results: Multi-tasking
Performance Comparison
The below plots show multi-tasking performance of both
the distillation and Multi-HDRQN approaches. Both approaches were trained on the 3 × 3 through 6 × 6 MAMT
tasks. Multi-DRQN failed to achieve specialized-level performance on all tasks, despite 500K training epochs. By
contrast, the proposed MT-MARL distillation approach
achieves nominal performance after 100K epochs.
Empirical Return

1.0
0.8
3×3
4×4
5×5
6×6

0.6
0.4
0.2
0.0
0.0

100K

200K

300K

400K

500K

Dec-MHDRQN Epoch

(a) MT-MARL via Multi-HDQRN.
Empirical Return

1.0
0.8
3×3
4×4
5×5
6×6

0.6
0.4
0.2
0.0
0.0

20K

40K

60K

80K

100K

Distillation Epoch

(b) MT-MARL via specialized and distilled Dec-HDRQN.
Figure 13. Multi-task performance on MAMT domain, n = 2
agents and Pf = 0.3.

