Sub-sampled Cubic Regularization for Non-convex Optimization

A. Appendix
A.1. Concentration Inequalities and Sampling Schemes
For the sake of simplicity we shall drop the iteration subscript k in the following results of this section.
A.1.1. G RADIENT S AMPLING
First, we extend the Vector Bernstein inequality as it can be found in (Gross, 2011) to the average of independent, zeromean vector-valued random variables.
Lemma 18 (Vector Bernstein Inequality). Let x1 , . . . , xn be independent vector-valued random variables with common dimension d and assume that each one is centered, uniformly bounded and also the variance is bounded above:
h
i
2
E [xi ] = 0 and kxi k2 ≤ µ as well as E kxi k ≤ σ 2
Let

n

z=

1X
xi .
n i=1

Then we have for 0 <  < σ 2 /µ
2
1
P ( kzk ≥ ) ≤ exp −n · 2 +
8σ
4



(35)

Proof: Theorem 6 in (Gross, 2011) gives the following Vector Bernstein inequality for independent, zero-mean vectorvalued random variables


!


n
X

√
t2


P 
xi  ≥ t + V ≤ exp −
,
(36)


4V
n=1

where V =

Pn

h

2

E kxi k

i

is the sum of the variances of the centered vectors xi .
√
First, we shall define  = t + V , which allows us to rewrite the above equation as
i=1

P


 n

!
X 
1


xi  ≥  ≤ exp −



4
i=1

√ !2 

2 !
1

− V
 = exp −
√
√ −1
.
4
V
V

(37)

Based on the observation that

2


1

1 2
1
√
−
−1 ≤−
+
4
4
2V
4
V
2
2



⇔ −
+ 2√ − 1 ≤ −
+1
V
2V
V
2

⇔0≤
− 2√ + 2
2V
V


√ 2

⇔0≤ √
− 2
2V

(38)

always holds, we can formulate a slightly weaker Vector Bernstein version as follows

P



!


n

X
2
1


xi  ≥  ≤ exp −
+
.



8V
4
i=1

(39)

Sub-sampled Cubic Regularization for Non-convex Optimization

Since the individual variance is assumed to be bounded above, we can write
V =

n
X

h
i
2
E kxi k ≤ nσ 2 .

(40)

i=1

Pn
This term also constitutes an upper
bound on the variance of y =
i=1 xi , because the xi are independent and thus
P
n
uncorrelated . However, z = n1 i=1 xi and we must account for the averaging term. Since the xi are centered we have
E [z] = 0, and thus



2 
!|  n
n
n
1 X

h
i
h
i
X
X
1


2
2

V ar(z) = E kz − E [z]k = E kzk = E  
xi   = 2 E 
xi
xj 
n

n
i=1
i=1
j=1




n
n X
n
X
X
X
X



1
1
1
(41)
x|j xi  = 2
E x|j xi = 2 
E [(x|i xi )] +
= 2E
E [(x|i xj )]
n
n
n
i,j
i,j
i=1
i=1
j6=i

=

1
n2

n
X

h
i
1
2
E kxi k ≤ σ 2 ,
n
i=1



where we used the fact that the expectation of a sum equals the sum of the expectations and the cross-terms E x|j xi =
0, j 6= i because of the independence assumption. Hence, we can bound the term V ≤ n1 σ 2 for the random vector sum z.
Now, since n > 1 and  > 0, as well as P (z > a) is falling in a and exp(−x) falling in x, we can use this upper bound on
the variance of z in (39), which gives the desired inequality


2
1
P ( kzk ≥ ) ≤ exp −n · 2 +
8σ
4

(42)


This result was applied in order to find the probabilistic bound on the deviation of the sub-sampled gradient from the full
gradient as stated in Lemma 6, for which we will give the proof next.
Proof of Lemma 6:
To apply vector Bernstein’s inequality (35) we need to center the gradients. Thus we define
xi = gi (x) − ∇f (x), i = 1, . . . , |Sg |

(43)

and note that from the Lipschitz continuity of f (A3), we have
2

kxi k = kgi (x) − ∇f (x)k ≤ kgi (x)k + k∇f (x)k ≤ 2κf and kxi k ≤ 4κ2f , i = 1, . . . , |Sg |.

(44)

With σ 2 := 4κ2f and
z=

1 X
1 X
1 X
xi =
gi (x) −
∇f (x) = g(x) − ∇f (x)
|Sg |
|Sg |
|Sg |
i∈Sg

i∈Sg

(45)

i∈Sg

in equation (35), we can require the probability of a deviation larger or equal to  to be lower than some δ ∈ (0, 1]
!
2
1 !
P ( kg(x) − ∇f (x)k > ) ≤2d exp −|Sg | ·
+
≤δ
32κ2f
4
2
1 !
− ≥ log((2d)/δ)
2
32κf
4
s
√
log ((2d)/δ) + 1/4
⇔ ≥ 4 2κf
.
|Sg |
⇔|Sg | ·

(46)

Sub-sampled Cubic Regularization for Non-convex Optimization

Conversely, the probability of a deviation of
√

s

 < 4 2κf

log ((2d)/δ) + 1/4
|Sg |

(47)

is higher or equal to 1 − δ.

Of course, any sampling scheme that guarantees the right hand side of (16) to be smaller or equal to M times the squared
step size, directly satisfies the sufficient gradient agreement condition (A5). Consequently, plugging the former into the
latter and rearranging for the sample size gives Theorem 7 as we shall prove now.
Proof of Theorem 7:
By use of Lemma 6 we can write
2

kg(x) − ∇f (x)k ≤ M ksk
s
√
log(1/δ + 1/4)
2
≤ M ksk
⇔ 4 2κf
|Sg |
|Sg | ≥

(48)

32κ2f log (1/δ + 1/4)
M 2 ksk

4



Sub-sampled Cubic Regularization for Non-convex Optimization

A.1.2. H ESSIAN S AMPLING
Lemma 19 (Matrix Bernstein Inequality). Let A1 , .., An be independent random Hermitian matrices with common
dimension d × d and assume that each one is centered, uniformly bounded and also the variance is bounded above:
  
E [Ai ] = 0 and kAi k2 ≤ µ as well as E A2i 2 ≤ σ 2
Introduce the sum

n

Z=
Then we have

1X
Ai
n i=1



2 
}
P ( kZk ≥ ) ≤ 2d · exp −n · min{ 2 ,
4σ 2µ

Proof: Theorem 12 in (Gross, 2011) gives the following Operator-Bernstein inequality
 n

!


X 
2 


Ai  ≥  ≤ 2d · exp min{
,
} ,
P 


4V 2µ
i=1

(49)

(50)

Pn
where V = nσ 2 . As well shall see, this is an upper bound on the variance of Y = i=1 Ai since the Ai are independent
and have an expectation of zero (E [Y ] = 0).


 
 "
# 
 X

  
 
 
X
X






2
2
2 





V ar(Y) = E Y − E [Y]  = E (
Ai )  = E
Ai Aj  = 
E [Ai Aj ]


 
  i,j

i
i,j

 
(51)

X
 X
X
XX


 


 2 

E A2i  ≤ nσ 2 ,
E Ai  ≤
=
E [Ai Ai ] +
E [Ai Aj ]

= 


 
i

i

j6=i

i

i

where we used the fact that the expectation of a sum equals the sum of the expectations and the cross-terms E [Aj Ai ] =
0, j 6= i because of the independence assumption.
Pn
However, Z = n1 i=1 Ai and thus
 "
 "
#
#
n
n



X
X
  2  
1
1
1




V ar(Z) = E Z  = E (
Ai )2  = 2 E (
Ai )2  ≤ σ 2 .
(52)

 n 
 n
n i=1
i=1
Hence, we can bound V ≤ n1 σ 2 for the average random matrix sum Z. Furthermore, since n > 1 and , µ > 0 as well as
exp(−α) decreasing in α ∈ R we have that






exp −
≤ exp −
.
(53)
2µ
n2µ
Together with the Operator-Bernstein inequality, (52) and (53) give the desired inequality (49).

This result exhibits that sums of independent random matrices provide normal concentration near its mean in a range
determined by the variance of the sum. We apply it in order to derive the bound on the deviation of the sub-sampled
Hessian from the full Hessian as stated in Lemma 8, which we shall prove next.
Proof of Lemma 8: Bernstein’s Inequality holds as f ∈ C 2 and thus the Hessian is symmetric by Schwarz’s Theorem.
Since the expectation of the random matrix needs to be zero, we center the individual Hessians,
Xi = Hi (x) − H(x), i = 1, ..., |SH |

Sub-sampled Cubic Regularization for Non-convex Optimization

and note that now from the Lipschitz continuity of g (A3):
 
kXi k2 ≤ 2κg , i = 1...|SH | and X2i 2 ≤ 4κ2g , i = 1...|SH |.
Hence, for  ≤ 4κg , we are in the small deviation regime of Bernstein’s bound with a sub-gaussian tail. Then, we may
plug
|SH |
1 X
Xi = B(x) − H(x)
|SH | i=1

into (49), to get

 2
 |SH |
.
P ( kB(x) − H(x)k ≥ ) ≤ 2d · exp −
16κ2g

(54)

Finally, we shall require the probability of a deviation of  or higher to be lower than some δ ∈ (0, 1]
2 |SH |
2d · exp −
16κ2g




!

=δ

2 |SH |
= log(δ/2d)
16κ2g
s
log(2d/δ)
⇔  = 4κg
,
|SH |
⇔ −

(55)

which is equivalent to kB(x) − H(x)k staying within this particular choice of  with probability (1 − δ), generally
perceived as high probability.

Proof of Theorem 9: Since kAvk ≤ kAkop kvk for every v ∈ V we have for the choice of the spectral matrix norm and
euclidean vector norm that any B that fulfils k(B(x) − H(x))k ≤ C ksk also satisfies condition A4. Furthermore
k(B − H(x))k ≤ C ksk
s
log(2d/δ)
⇔ 4κg
≤ C ksk
|SH |
⇔ |SH | ≥

16κ2g

log(2d/δ)
,
(C ksk)2

(56)
C > 0.


Note that there may be a less restrictive sampling conditions that satisfy A4 since condition (56) is based on the worst
case bound kAvk ≤ kAkop kvk which indeed only holds with equality if v happens to be (exactly in the direction of) the
largest eigenvector of A.
Finally, we shall state a Lemma which illustrates that the stepsize goes to zero as the algorithm converges. The proof can
be found in Section 5 of (Cartis et al., 2011a).
Lemma 20. Let {f (xk )} be bounded below by some finf > −∞. Also, let sk satisfy A1 and σk be bounded below by
some σinf > 0. Then we have for all successful iterations that
ksk k → 0, as k → ∞

(57)

Sub-sampled Cubic Regularization for Non-convex Optimization

A.1.3. I LLUSTRATION
In the top row of Figure 2 we illustrate the Hessian sample sizes that result when applying SCR with a practical version of
Theorem 9 to the datasets used in our experiments 4 . In the bottom row of Figure 2, we benchmark our algorithm to the
deterministic as well as two naive stochastic versions of ARC with linearly and exponentially increasing sample sizes.

1. A 9 A

2. COVTYPE

3. GAUSSIAN

Figure 2. Suboptimality (top row) and sample sizes (bottom row) for different cubic regularization methods on a9a, covtype and gaussian. Note that the automatic sampling scheme of SCR follows an exponential curve, which means that it can indeed save a lot of
computation in the early stage of the optimization process.

Note that both the linear and the exponential sampling schemes do not quite reach the same performance as SCR even
though they were carefully fine tuned to achieve the best possible performance. Furthermore, the sampling size was
manually set to reach the full sample size at the very last iteration. This highlights another advantage of the automatic
sampling scheme that does not require knowledge of the total number of iterations.
A.2. Convergence Analysis
A.2.1. P RELIMINARY RESULTS
Proof of Lemma 10:
The lower bound σinf follows directly from Step 7 in the algorithm design (see Algorithm 1). Within the upper bound, the
2M +C+κg
),
constant σ0 accounts for the start value of the penalty parameter. Now, we show that as soon as some σk > 3(
2
the iteration is very successful and σk+1 < σk . Finally, γ2 allows for σk being ’close to’ the successful threshold, but
increased ’one last time’.
Any iteration with f (xk + sk ) ≤ m(sk ) yields a ρk ≥ 1 ≥ η2 and is thus very successful. From a 2nd-order Taylor
4

see Section A.3 for details

Sub-sampled Cubic Regularization for Non-convex Optimization

approximation of f (xk + sk ) around xk we have:
1
σ
3
f (xk + sk ) − mk (sk ) = (∇f (xk ) − g(xk ))| sk + s|k (H(xk + tsk ) − Bk )sk − ksk k
2
3
1
1
σk
2
3
≤ e|k sk + ksk k kH(xk + tsk ) − H(x)k + kH(xk ) − Bk k ksk k −
ksk k
2
2
3


C + κg
σk
3
≤ kek k ksk k +
−
ksk k
2
3


C + κg
σk
3
3
≤ M ksk k +
−
ksk k
2
3


2M + C + κg
σk
3
=
−
ksk k
2
3

(58)

Requiring the right hand side to be non-positive and solving for σk gives the desired result.

Proof of Lemma 11 : By definition of the stochastic model mk (sk ) we have
1
1
3
f (xk ) − mk (sk ) = − s|k g(xk ) − s|k Bk sk − σk ksk k
2
3
1
2
3
= s|k Bk sk + σk ksk k
2
3
1
3
≥ σk ksk k ,
6

(59)

where we applied equation (11) first and equation (12) secondly.

Before proving the lower bound on the stepsize ksk k we first transfer the rather technical result from Lemma 4.6 in (Cartis
et al., 2011a) to our framework of stochastic gradients. For this purpose, let ek be the gradient approximation error, i.e.
ek := gk − ∇f (xk ).
Lemma 21. Let f ∈ C 2 , Lipschitz continuous gradients (A3) and TC (A2) hold. Then, for each (very-)successful k,
we have
2

(1 − κθ ) k∇f (xk+1 )k ≤ σk ksk k +


 Z 1


 (H(xk + tsk ) − H(xk ))dt + k(H(xk ) − Bk )sk k + κθ κg ksk k + (1 + κθ κg ) kek k · ksk k


ksk k
ksk k
0
|
{z
}

(60)

=dk

with κθ ∈ (0, 1) as in TC (13).

Proof: We shall start by writing
k∇f (xk + sk )k ≤ k∇f (xk + sk ) − ∇mk (sk )k + k∇mk (sk )k
≤ k∇f (xk + sk ) − ∇mk (sk )k + θk kgk (xk )k,
|
{z
} |
{z
}
(a)

(61)

(b)

where the last inequality results from TC (Eq. (13)). Now, we can find the following bounds on the individual terms:
(a) By (5) we have
k∇f (xk + sk ) − ∇mk k = k∇f (xk + sk ) − gk (xk ) − Bk sk − σk sk ksk kk .

(62)

Sub-sampled Cubic Regularization for Non-convex Optimization

We can rewrite the right-hand side by a Taylor expansion of ∇fk+1 (xk + sk ) around xk to get


Z 1



H(xk + tsk )sk dt − gk (xk ) − Bk sk − σk sk ksk k
(62) = ∇f (xk ) +
.

(63)

0

Contrary to the case of deterministic gradients, the first and third summand no longer cancel out. Applying the triangle
inequality repeatedly, we thus get an error term in the final bound on (a):

Z 1


2

H((xk + tsk ) − Bk )sk dt
k∇f (xk + sk ) − ∇mk k ≤ 
 + σk ksk k + k∇f (xk ) − gk (xk )k
0

Z 1


(64)

H((x
+
ts
)dt
−
H(x
)
≤
k
k
k  · ksk k + k(H(xk ) − Bk )sk k

0

2

+ σk ksk k + kek k .
(b) To bound the second summand, we can write
kg(xk )k ≤ k∇f (xk )k + kek k
≤ k∇f (xk + sk )k + k∇f (xk ) − ∇f (xk + sk )k + kek k

(65)

≤ k∇f (xk + sk )k + κg ksk k + kek k
Finally, using the definition of θk as in (13) (which also gives θk ≤ κθ and θk ≤ κθ hk ) and combining (a) and (b) we get
the above result.

Proof of Lemma 12: The conditions of Lemma 21 are satisfied. By multiplying dk ksk k out in equation (60), we get
(1 − κθ ) k∇f (xk+1 )k ≤
Z 1



 (H(xk + tsk ) − H(xk ))dt ksk k + k(H(xk ) − Bk )sk k + κθ κg ksk k2 + (1 + κθ κg ) kek k + σk ksk k2 .



(66)

0

Now, applying the strong agreement conditions (A4) and (A5), as well as the Lipschitz continuity of H, we can rewrite this
as
1
2
(1 − κθ ) k∇f (xk+1 )k ≤ ( κg + C + (1 + κθ κg )M + σmax + κθ κg ) ksk k ,
2
for all sufficiently large, successful k. Solving for the stepsize ksk k give the desired result.

(67)


A.2.2. L OCAL CONVERGENCE
Before we can study the convergence rate of SCR in a locally convex neighbourhood of a local minimizer w∗ we first need
to establish three crucial properties:
1. a lower bound on ksk k that depends on kgk k.
2. an upper bound on ksk k that depends on kgk+1 k.
3. an eventually full sample size
4. conditions under which all steps are eventually very successful.
With this at hand we will be able to relate kgk+1 k to kgk k, show that this ratio eventually goes to zero at a quadratic rate
and conclude from a Taylor expansion around gk that the iterates themselves converge as well.

Sub-sampled Cubic Regularization for Non-convex Optimization

Assumption 22 (Sampling Scheme). Let gk and Bk be sampled such that 17 and 19 hold in each iteration k. Furthermore,
for unsuccessful iterations, assume that the sample size is not decreasing.
We have already established a lower stepsize bound in Lemma 12 so let us turn our attention directly to 2.:
Lemma 23 (Upper bound on stepsize). Suppose that sk satisfies (11) and that the Rayleigh coefficient
Rk (sk ) :=

s|k Bk sk
2

ksk k

(68)

is positive, then
ksk k ≤

1
1
1
kgk k =
k∇f (wk ) + ek k ≤
( k∇f (wk )k + kek k)
Rk (sk )
Rk (sk )
Rk (sk )

(69)

Proof: Given the above assumptions we can rewrite (11) as follows
2

3

Rk (sk ) ksk k = −s|k gk − σk ksk k ≤ ksk k kgk k ,

(70)

where we used Cauchy-Schwarz inequality as well as the fact that σk > 0, ∀k. Solving (70) for ksk k gives (69).

Lemma 24 (Eventually full sample size). Let {f (xk )} be bounded below by some finf > −∞. Also, let A1, A3 hold and
let gk and Bk be sampled according to A22. Then we have w.h.p. that
|Sg,k | → n and |SB,k | → n as k → ∞

(71)

The sampling schemes from Theorem 7 and Theorem 9 imply that the sufficient agreement assumptions A5 and A4 hold
with high probability. Thus, we can deduce from Lemma 10 that after a certain number of consecutive unsuccessful iterates
the penalty parameter is so high (σk ≥ σsup ) that we are guaranteed to find a successful step. Consequently, the number
of successful iterations must be infinite (|S| = ∞) when we consider the asymptotic convergence properties of SCR. We
are left with two possible scenarios:
(i) If the number of unsuccessful iterations is finite (|U| ≤ ∞ & |S| = ∞) we have that ∃ k̂ after which all iterates are
successful, i.e. k ∈ S, ∀ k > k̂. From Lemma 20 we know that for all successful iterations ksk k → 0 as k → ∞.
Consequently, due to the sampling scheme as specified in Theorem 7 and Theorem 9, ∃ k̄ ≥ k̂ with |Sg,k | = |SB,k | =
n, ∀ k ≥ k̄.
(ii) If the number of unsuccessful iterations is infinite (|U| = ∞ & |S| = ∞) we know for the same reasons that for
the subsequence of successful iterates {k = 0, 1, . . . ∞|k ∈ S} again ksk k → 0, as k ∈ S → ∞ and hence ∃ k̃ with
|Sg,k | = |SB,k | = n, ∀ k ≥ k̃ ∈ S. Given that we do specifically not decrease the sample size in unsuccessful iterations
we have that |Sg,k | = |SB,k | = n, ∀ k ≥ k̃.
As a result the sample sizes eventually equal n with high probability in all conceivable scenarios which proves the assertion5 .

Now that we have (asymptotic) stepsize bounds and gradient (Hessian) agreement we are going to establish that, when
converging, all SCR iterations are indeed very successful asymptotically.
Lemma 25 (Eventually successful iterations). Let f ∈ C 2 , ∇f uniformly continuous and Bk bounded above. Let Bk and
gk be sampled according to A22, as well as sk satisfy (11). Furthermore, let
wk → w∗ , as k → ∞,

(72)

with ∇f (w∗ ) = 0 and H(w∗ ) positive definite. Then there exists a constant Rmin > 0 such that for all k sufficiently large
Rk (sk ) ≥ Rmin .
Furthermore, all iterations are eventually very successful w.h.p.
5

We shall see that, as a result of Lemma 25, the case of an infinite number of unsuccessful steps can actually not happen

(73)

Sub-sampled Cubic Regularization for Non-convex Optimization

Proof: Since f is continuous, the limit (72) implies that {f (wk )} is bounded below. Since H(w∗ ) is positive definite per
assumption, so is H(wk ) for all k sufficiently large. Therefore, there exists a constant Rmin such that
s|k H(wk )sk
2

ksk k

> 2Rmin > 0, for all k sufficiently large.

(74)

As a result of Lemma 24 we have that kek k → 0 as k → ∞. Hence, Lemma 23 yields ksk k ≤ 1/Rmin k∇fk k which
implies that the step size converges to zero as we approximate w∗ . Consequently, we are able to show that eventually all
iterations are indeed very successful. Towards this end we need to ensure that the following quantity rk becomes negative
for sufficiently large k:
(75)
rk := f (wk + sk ) − m(sk ) +(1 − η2 ) (m(sk ) − f (wk )),
{z
}
|
{z
}
|
(i)

(ii)

where η2 ∈ (0, 1) is the ”very successful” threshold.
(i) By a (second-order) Taylor approximation around f (wk ) and applying the Cauchy-Schwarz inequality, we have:
1
σk
3
f (wk + sk ) − m(sk ) =(∇f (w) − gk )| sk + s|k ((H(wk + τ sk ) − Bk )sk −
ksk
2
3
1
≤ kek k ksk k + k((H(wk + τ sk ) − Bk )sk k ksk k ,
2

(76)

where the term kek k ksk k is extra compared to the case of deterministic gradients.
(ii) Regarding the second part we note that if sk satisfies (11), we have by the definition of Rk and equation (73) that
2
1
3
f (wk ) − mk (sk ) = s|k Bsk + σk ksk k
2
3
1
2
≥ Rmin ksk k ,
2
which negated gives the desired bound on (ii). All together, the upper bound on rk is written as


1
2 kek k
k((H(wk + τ sk ) − Bk )sk k
2
rk ≤ ksk k
+
− (1 − η2 )Rmin .
2
ksk k
ksk k

(77)

(78)

Let us add and subtract H(wk ) to the second summand and apply the triangle inequality
1
2
rk ≤ ksk k
2




2 kek k
k(H(wk + τ sk ) − Hk )sk k + k(Hk − Bk )sk k
+
− (1 − η2 )Rmin .
ksk k
ksk k

(79)

Now applying kAvk ≤ kAk kvk we get
1
2
rk ≤ ksk k
2




2 kek k
+ kH(wk + τ sk ) − Hk k + k(Hk − Bk )k − (1 − η2 )Rmin .
ksk k

(80)

We have already established in Lemma 24 that kek k → 0 and k(Hk − Bk )k → 0. Together with Lemma 23 and the
assumption k∇fk k → 0 this implies ksk k → 0. Furthermore, since τ ∈ [0, 1] we have that kwk + τ sk k ≤ kwk + sk k ≤
ksk k. Hence, H(wk + τ sk ) and H(wk ) eventually agree. Finally, η2 < 1 and Rmin > 0 such that rk is negative for all k
sufficiently large, which implies that every such iteration is very successful.

Proof of Theorem 13:
From Lemma 10 we have σk ≤ σsup . Furthermore, all assumptions needed for the step size bounds of Lemma 12 and 23
hold. Finally, Lemma 25 gives that all iterations are eventually successful. Thus, we can combine the upper (69) and lower
(24) bound on the stepsize for all k sufficiently large to obtain
p
1
( k∇f (wk )k + kek k) ≥ ksk k ≥ κs k∇f (wk+1 )k
(81)
Rmin

Sub-sampled Cubic Regularization for Non-convex Optimization

which we can solve for the gradient norm ratio
k∇f (wk+1 )k
2

k∇f (wk )k


≤

1
Rmin κs


1+

kek k
k∇f (wk )k

2
.

(82)

Consequently, as long as the right hand side of (82) stays below infinity, i.e. kek k / k∇f (wk )k 6→ ∞, we have quadratic
convergence of the gradient norms. From Lemma 24 we have that kek k → 0 as k → ∞ w.h.p. and furthermore κs is
bounded above by a constant and Rmin is a positive constant itself which gives quadratic convergence of the gradient norm
ratio with high probability. Finally, the convergence rate of the iterates follows from a Taylor expansion around gk .

A.2.3. F IRST ORDER GLOBAL CONVERGENCE
Note that the preliminary results Lemma 11 and 12 allow us to lower bound the function decrease of a successful step in
terms of the full gradient ∇fk+1 . Combined with Lemma 10, this enables us to give a deterministic global convergence
guarantee while using only stochastic first order information6 .
Proof of Theorem 14:
We will consider two cases regarding the number of successful steps for this proof.
Case (i): SCR takes only finitely many successful steps. Hence, we have some index k0 which yields the very last
successful iteration and all further iterates stay at the same point xk0 +1 . That is xk0 +1 = xk0 +i , ∀ i ≥ 1. Let us assume
that k∇f (xk0 +1 )k =  > 0, then
k∇f (xk )k = , ∀ k ≥ k0 + 1.
(83)
Since, furthermore, all iterations k ≥ k0 + 1 are unsuccessful σk increases by γ, such that
σk → ∞ as k → ∞.

(84)

However, this is in contradiction with Lemma 10, which states that σk is bounded above. Hence, the above assumption
cannot hold and we have k∇f (xk0 +1 )k = k∇f (x∗ )k = 0.
Case (ii): sARC takes infinitely many successful steps. While unsuccessful steps keep f (xk ) constant, (very) successful
steps strictly decrease f (xk ) and thus the sequence {f (xk )} is monotonically decreasing. Furthermore, it is bounded
below per assumption and thus the objective values converge
f (xk ) → finf , as k → ∞.

(85)

All requirements of Lemma 11 and Lemma 12 hold and we thus can use the sufficient function decrease equation (31) to
write
1
3/2
(86)
f (xk ) − finf ≥ f (xk ) − f (xk+1 ) ≥ η1 σinf κ3s k∇f (xk+1 )k .
6
Since (f (xk ) − finf ) → 0 as k → ∞, σinf > 0, η1 > 0 and κ3s ≥ 0 (as σsup < ∞), we must have k∇f (xk )k → 0, giving
the result.

A.2.4. S ECOND ORDER GLOBAL CONVERGENCE AND WORST CASE ITERATION COMPLEXITY
For the proofs of Theorem 15 and Theorem 17 we refer the reader to Theorem 5.4 in (Cartis et al., 2011a) and Corollary
5.3 in (Cartis et al., 2011b). Note that, as already laid out above in the proofs of Lemma 10 and Lemma 11, the constants
involved in the convergence Theorems change due to the stochastic gradients used in our framework.
A.3. Details concerning experimental section
We here provide additional results and briefly describe the baseline algorithms used in the experiments as well as the choice
of hyper-parameters. All experiments were run on a CPU with a 2.4 GHz nominal clock rate.
6
Note that this result can also be proven without Lipschitz continuity of H and less strong agreement conditions as done in Corollary
2.6 in (Cartis et al., 2011a).

Sub-sampled Cubic Regularization for Non-convex Optimization

Datasets The real-world datasets we use represent very common instances of Machine Learning problems and are part of
the libsvm library (Chang & Lin, 2011), except for cifar which is from Krizhevsky & Hinton (2009). A summary of their
main characteristic can be found in table 1. The multiclass datasets are both instances of so-called image classification
problems. The mnist images are greyscale and of size 28 × 28. The original cifar images are 32 × 32 × 3 but we converted
them to greyscale so that the problem dimensionality is comparable to mnist. Both datasets have 10 different classes, which
multiplies the problem dimensionality of the multinomial regression by 10.

a9a
a9a nc
covtype
covtype nc
higgs
higgs nc
mnist
cifar

type
Classification
Classification
Classification
Classification
Classification
Classification
Multiclass
Multiclass

n
32, 561
32, 561
581, 012
581, 012
11, 000, 000
11, 000, 000
60, 000
50, 000

d
123
123
54
54
28
28
7, 840
10, 240

κ(H∗)
761.8
1, 946.3
3 · 109
25, 572, 903.1
1, 412.0
2, 667.7
10, 281, 848
1 · 109

λ
1e−3
1e−3
1e−3
1e−3
1e−4
1e−4
1e−3
1e−3

Table 1. Overview over the real-world datasets used in our experiments with convex and non-convex (nc) regularizer. κ(H∗) refers to
the condition number of the Hessian at the optimizer and λ is the regularization parameter applied in the loss function and its derivatives.

1. A9A

2. COVTYPE

3. HIGGS

Figure 3. Results from Section 5 over epochs. Top (bottom) row shows the log suboptimality of convex (non-convex) regularized logistic
regressions over epochs (average of 10 independent runs).

Benchmark methods
• Stochastic Gradient Descent (SGD): To bring in some variation, we select a mini-batch of the size dn/10e on the real
world classification- and dn/100e on the multiclass problems. On the artificial datasets we only sample 1 datapoint
per iteration and update the parameters with respect to this point. We use a problem-dependent, constant step-size as
this yields faster initial convergence (Hofmann et al., 2015),(Roux et al., 2012).
• SAGA: is a variance-reduced variant of SGD that only samples 1 datapoint per iteration and uses a constant step-size.

Sub-sampled Cubic Regularization for Non-convex Optimization

• Broyden-Fletcher-Goldfarb-Shanno (BFGS) is the most popular and stable Quasi-Newton method.
• Limited-memory BFGS is a variant of BFGS which uses only the recent K iterates and gradients to construct an
approximate Hessian. We used K = 20 in our experiments. Both methods employs a line-search technique that
satisfies the strong Wolfe condition to select the step size.
• NEWTON is the classic version of Newton’s method which we apply with a backtracking line search.
For L-BFGS and BFGS we used the implementation available in the optimization library of scipy. All other methods are
our own implementation. The code for our implementation of SCR is publicly available on the authors’ webpage.
Initialization. All of our experiments were started from the initial weight vector w0 := (0, . . . , 0).
Choice of parameters for ARC and SCR. The regularization parameter updating is analog to the rule used in the
reported experiments of (Cartis et al., 2011a), where γ = 2. Its goal is to reduce the penalty rapidly as soon as convergence
sets in, while keeping some regularization in the non asymptotic regime. A more sophisticated approach can be found in
(Gould et al., 2012). In our experiments we start with σ0 = 1, η1 = 0.2, and η2 = 0.8 as well as an initial sample size of
5%.
Influence of dimensionality To test the influence of the dimensionality on the progress of the above applied methods we
created artificial datasets of three different sizes, labeled as gaussian s, gaussian m and gaussian l.

gaussian s
gaussian m
gaussian l

type
Classification
Classification
Classification

n
50, 000
50, 000
50, 000

d
100
1, 000
10, 000

κ(H ∗ )
2, 083.3
98, 298.9
1, 167, 211.3

λ
1e−3
1e−3
1e−3

Table 2. Overview over the synthetic datasets used in our experiments with convex regularizer

The feature vectors X = (x1 , x2 , ..., xd ), xi ∈ Rn were drawn from a multivariate Gaussian distribution
X ∼ N (µ, Σ)

(87)

with a mean of zero µ = (0, . . . , 0) and a covariance matrix that has reasonably uniformly distributed off-diagonal elements
in the interval (−1, 1).
As expected, the classic Newton methods suffers heavily from an increase in the dimension. The regularized Newton
methods on the other hand scale comparably very well since they only need indirect access to the Hessian via matrixvector products. Evidently, these methods outperform the quasi-newton approaches even in high dimensions. Among
these, the limited memory version of BFGS is significantly faster than its original variant.
Multiclass regression In this section we leave the trust region method out because our implementation is not optimized
towards solving multi-class problems. We do not run Newton’s method or BFGS either as the above results suggests that
they are unlikely to be competitive. Furthermore, Figure 5 does not show logarithmic but linear suboptimality because
optimizing these problems to high precision takes very long and yields few additional benefits. For example, the 25th SCR
iteration drove the gradient norm from 3.8 · 10−5 to 5.6 · 10−8 after building up a Krylov space of dimensionality 7800. It
took 9.47 hours and did not change any of the first 13 digits of the loss. As can be seen, SCR provides early progress at a
comparable rate to other methods but gives the opportunity to solve the problem to high precision if needed.

Sub-sampled Cubic Regularization for Non-convex Optimization

1. GAUSSIAN S

2. GAUSSIAN M

3. GAUSSIAN L

Figure 4. Top (bottom) row shows the log suboptimality of convex regularized logistic regressions over time (epochs) (average of 10
independent runs).

1. CIFAR

2. MNIST

Figure 5. Top (bottom) row shows suboptimality of the empirical risk of convex regularized multinominal regressions over time (epochs)

