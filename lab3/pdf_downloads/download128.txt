Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning

8. Appendix

imate Q, and expand the KL-divergence term to get

L(ut , ⌃t , ⌘t )

1
1 >
1
>
Given a TVLG dynamics model and quadratic cost approx=
Q>
x,t xt + Qu,t ut + xt Qxx,t xt + tr(Qxx,t ⌃x,t )
⌘
2
2
t
imation, we can approximate our Q and value functions to
1
1
second order with the following dynamic programming up+ u>
Quu,t ut + tr(Quu,t ⌃t ) + x>
t Qxu,t ut
dates, which proceed from the last time step t = T to the
2 t
2
first step t = 1:
1⇥
¯ t | log |⌃t | d + tr(⌃
¯ 1 ⌃t )
+
log |⌃
t
2
>¯ 1
>
>
+(K̄t xt + k̄t ut ) ⌃t (K̄t xt + k̄t ut )
Qx,t = cx,t + fx,t Vx,t+1 , Qxx,t = cxx,t + fx,t Vxx,t+1 fx,t ,
⇤
>
>
¯ 1
+tr(K̄>
✏t .
Qu,t = cu,t + fu,t Vx,t+1 , Quu,t = cuu,t + fu,t Vxx,t+1 fu,t ,
t ⌃t K̄t ⌃x,t )
8.1. Derivation of LQR-FLM

>
Qxu,t = cxu,t + fx,t
Vxx,t+1 fu,t ,

1
Vx,t = Qx,t Qxu,t Quu,t
Qu,t ,
1
Vxx,t = Qxx,t Qxu,t Quu,t Qux,t

.

Here, similar to prior work, we use subscripts to denote
derivatives. It can be shown (e.g., in (Tassa et al., 2012))
that the action ut that minimizes the second-order approximation of the Q-function at every time step t is given by
ut =

1
Quu,t
Qux,t xt

1
Quu,t
Qu,t

.

This action is a linear function of the state xt , thus we
can construct an optimal linear policy by setting Kt =
1
1
Quu,t
Qux,t and kt = Quu,t
Qu,t . We can also show
that the maximum-entropy policy that minimizes the approximate Q-function is given by
p(ut |xt ) = N (Kt xt + kt , Quu,t ).
This form is useful for LQR-FLM, as we use intermediate policies to generate samples to fit TVLG dynamics.
Levine & Abbeel (2014) impose a constraint on the total
KL-divergence between the old and new trajectory distributions induced by the policies through an augmented cost
function c̄(xt , ut ) = ⌘1 c(xt , ut ) log p(i 1) (ut |xt ), where
solving for ⌘ via dual gradient descent can yield an exact
solution to a KL-constrained LQR problem, where there is
a single constraint that operates at the level of trajectory
distributions p(⌧ ). We can instead impose a separate KLdivergence constraint at each time step with the constrained
optimization
min Ex⇠p(xt ),u⇠N (ut ,⌃t ) [Q(x, u)]

ut ,⌃t

s.t. Ex⇠p(xt ) [DKL (N (ut , ⌃t )kp(i

1)

)]  ✏t .

The new policy will be centered around ut with covariance
term ⌃t . Let the old policy be parameterized by K̄t , k̄t ,
and C̄t . We form the Lagrangian (dividing by ⌘t ), approx-

Now we set the derivative of L with respect to ⌃t equal to
0 and get
✓
◆ 1
1
1
¯
⌃t =
Quu,t + ⌃t
.
⌘t
Setting the derivative with respect to ut equal to 0, we get
✓
◆
1
1
1
u t = ⌃t
Qu,t + Qux,t xt Ĉt (K̂t xt + k̂t ) ,
⌘t
⌘t
Thus our updated mean has the parameters
✓
◆
1
1
k t = ⌃t
Qu,t Ĉt k̂t ,
⌘t
✓
◆
1
1
Qux,t Ĉt K̂t .
Kt = ⌃ t
⌘t

As discussed by Tassa et al. (2012), when the updated Kt
and kt are not actually the optimal solution for the current
quadratic Q-function, the update to the value function is a
bit more complex, and is given by
>
>
>
Vx,t = Q>
x,t + Qu,t Kt + kt Quu,t Kt + kt Qux,t ,

Vxx,t = Qxx,t + K>
t Quu,t Kt + 2Qxu,t Kt .
8.2. PI2 update through constrained optimization
The structure of the proof for the PI2 update follows (Peters et al., 2010), applied to the cost-to-go S(xt , ut ). Let
us first consider the cost-to-go S(xt , ut ) of a single trajectory or path (xt , ut , xt+1 , ut+1 , . . . , xT , uT ) where T is
the maximum number of time steps. We can rewrite the
Lagrangian in a sample-based form as
L(p(i) , ⌘t ) =
✓X
⌘
X⇣
p(i)
(i)
p S(xt , ut ) + ⌘t
p(i) log (i 1)
p

✏

◆

.

Taking the derivative of L(p(i) , ⌘t ) with respect to a single
optimal policy p(i) and setting it to zero results in
✓
◆
(i 1)
@L
p(i)
1
(i) p
= S(xt , ut ) + ⌘t log (i 1) + p
@p(i)
p
p(i) p(i 1)
= S(xt , ut ) + ⌘t log

p(i)

p(i

1)

= 0.

Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning

Solve the derivative for p(i) by exponentiating both sides
log
p(i) = p(i

p(i)

=
✓

p(i 1)

1)

exp

1
S(xt , ut ) ,
⌘t
◆
1
S(xt , ut ) .
⌘t

This gives us a probability update rule for a single sample
that only considers cost-to-go of one path. However, when
sampling from a stochastic policy p(i 1) (ut |xt ), there are
multiple paths that start in state xt with action ut and continue with a noisy policy afterwards. Hence, the updated
policy p(i) (ut |xt ) will incorporate all of these paths as

✓
◆
1
(i)
(i 1)
p (ut |xt ) / p
(ut |xt )Ep(i 1) exp
S(xt , ut ) .
⌘t
The updated policy is additionally subject to normalization,
which corresponds to computing the normalized probabilities in Eq. (2).
8.3. Detailed Experimental Setup
8.3.1. S IMULATION E XPERIMENTS
All of our cost functions use the following generic loss term
on a vector z
q
1
`(z) = ↵kzk22 +
+ kzk22 .
(8)
2
↵ and are hyperparameters that weight the squared `2 loss
and Huber-style loss, respectively, and we set = 10 5 .

On the gripper pusher task, we have three such terms. The
first sets z as the vector difference between the block and
goal positions with ↵ = 10 and = 0.1. z for the second measures the vector difference between the gripper and
block positions, again with ↵ = 10 and = 0.1, and the
last loss term penalizes the magnitude of the fourth robot
joint angle with ↵ = 10 and = 0. We include this last
term because, while the gripper moves in 3D, the block is
constrained to a 2D plane and we thus want to encourage
the gripper to also stay in this plane. These loss terms are
weighted by 4, 1, and 1 respectively.
On the reacher task, our only loss term uses as z the vector difference between the arm end effector and the target,
with ↵ = 0 and = 1. For both the reacher and door opening tasks, we also include a small torque penalty term that
penalizes unnecessary actuation and is typically several orders of magnitude smaller than the other loss terms.
On the door opening task, we use two loss terms. For the
first, z measures the difference between the angle in radiants of the door hinge and the desired angle of 1.0, with
↵ = 1 and = 0. The second is term is time-varying:
for the first 25 time steps, z is the vector difference between the bottom of the robot end effector and a position

Figure 9. The initial conditions for the gripper pusher task that we
train TVLG policies on. The top left and bottom right conditions
are more difficult due to the distance from the block to the goal
and the configuration of the arm. The top left condition results are
reported in Section 6.1.

above the door handle, and for the remaining time steps, z
is the vector difference from the end effector to inside the
handle. This encourages the policy to first navigate to a position above the handle, and then reach down with the hook
to grasp the handle. Because we want to emphasize the second loss during the beginning of the trajectory and gradually switch to the first loss, we do a time-varying weighting
between the loss terms. The weight of the second loss term
is fixed to 1, but the weight of the first loss term at time step
2
t is 5 Tt .
For the neural network policy architectures, we use two
fully-connected hidden layers of rectified linear units (ReLUs) with no output non-linearity. On the reacher task,
the hidden layer size is 32 units per layer, and on the door
opening task, the hidden layer size is 100 units per layer.
All of the tasks involve varying conditions for which we
train one TVLG policy per condition and, for reacher and
door opening, train a neural network policy to generalize
across all conditions. For gripper pusher, the conditions
vary the starting positions of the block and the goal, which
can have a drastic effect on the difficulty of the task. Figure 9 illustrates the four initial conditions of the gripper
pusher task for which we train TVLG policies. For reacher,
analogous to OpenAI Gym, we vary the initial arm configuration and position of the target and train TVLG policies from 16 randomly chosen conditions. Note that, while
OpenAI Gym randomizes this initialization per episode, we
always reset to the same condition when training TVLG
policies as this is an additional assumption we impose.
However, when we test the performance of the neural network policy, we collect 300 test episodes with random initial conditions. For the door opening task, we initialize the

Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning

robot position within a small square in the ground plane.
We train TVLG policies from the four corners of this square
and test our neural network policies with 100 test episodes
from random positions within the square.
For the gripper pusher and door opening tasks, we train
TVLG policies with PILQR, LQR-FLM and PI2 with 20
episodes per iteration per condition for 20 iterations. In
Appendix 8.4, we also test PI2 with 200 episodes per iteration. For the reacher task, we use 3 episodes per iteration
per condition for 10 iterations. Note that we do not collect
any additional samples for training neural network policies.
For the prior methods, we train DDPG with 150 episodes
per epoch for 80 epochs on the reacher task, and TRPO uses
600 episodes per iteration for 120 iterations. On door opening, TRPO uses 400 episodes per iteration for 80 iterations
and DDPG uses 160 episodes per epoch for 100 epochs,
though note that DDPG is ultimately not successful.
8.3.2. R EAL ROBOT E XPERIMENTS
For the real robot tasks we use a hybrid cost function that
includes two loss terms of the form of Eq. 8. The first loss
term `arm (z) computes the difference between the current
position of the robot’s end-effector and the position of the
end-effector when the hockey stick is located just in front
of the puck. We set ↵ = 0.1 and = 0.0001 for this cost
function. The second loss term `goal (z) is based on the distance between the puck and the goal that we estimate using
a motion capture system. We set ↵ = 0.0 and = 1.0.
Both `arm and `goal have a linear ramp, which makes the
cost increase towards the end of the trajectory. In addition,
We include a small torque cost term `torque to penalize unnecessary high torques. The combined function sums over
all the cost terms: `total = 100.0 `goal + `arm + `torque .
We give a substantially higher weight to the cost on the distance to the goal to achieve a higher precision of the task
execution.
Our neural network policy includes two fully-connected
hidden layers of rectified linear units (ReLUs). Each of
the hidden layers consists of 42 neurons. The inputs of the
policy include: puck and goal positions measured with a
motion capture system, robot joint angles, joint velocities,
the end-effector pose and the end-effector velocity. During
PILQR-MDGPS training, we use data augmentation to regularize the neural network. In particular, the observations
were augmented with Gaussian noise to mitigate overfitting
to the noisy sensor data.
8.4. Additional Simulation Results
Figure 10 shows additional simulation results obtained for
the gripper pusher task for the three additional initial conditions. The instances presented here are not as challenging
as the one reported in the paper. Our method (PILQR) is

Figure 10. Single condition comparisons of the gripper-pusher
task in three additional conditions. The top, middle, and bottom
plots correspond to the top right, bottom right, and bottom left
conditions depicted in Figure 9, respectively. The PILQR method
outperforms other baselines in two out of the three conditions.
The conditions presented in the top and middle figure are significantly easier than the other conditions presented in the paper.

Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning

Figure 11. Additional results on the door opening task.

able to outperform other baselines except for the first two
conditions presented in the first rows of Figure 10, where
LQR-FLM performs equally well due to the simplicity of
the task. PI2 is not able to make progress with the same
number of samples, however, its performance on each condition is comparable to LQR-FLM when provided with 10
times more samples.
We also test PI2 with 10 times more samples on the reacher
and door opening tasks. On the reacher task, PI2 improves
substantially with more samples, though it is still worse
than the four other methods. However, as Figure 11 shows,
PI2 is unable to succeed on the door opening task even with
10 times more samples. The performance of PI2 is likely to
continue increasing as we provide even more samples.

