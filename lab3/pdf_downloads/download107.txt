Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning

Appendix
In the appendices we present the proofs, and additional
lemmas that are used in the proofs.

A. Lemma 1
Lemma 1 proves that if (5) is satisfied for some action
a ∈ A(I) on iteration T , then the value of action a and
all its descendants on every iteration played so far can be
set to the T -near counterfactual best response value. The
same lemma holds if one replaces the T -near counterfactual
best response values with exact counterfactual best response
values. The proof for Lemma 1 draws from recent work
on warm starting CFR using only an average strategy profile (Brown & Sandholm, 2016).
Lemma 1. Assume T iterations of CFR with RM
have been played in a two-player zero-sum game. If
 PT
T
t
t
T ψ σ̄−i ,T (I, a) ≤ t=1 v σ (I) and one sets v σ (I, a) =
T
ψ σ̄−i ,T (I, a) for each t ≤ T and for each I 0 ∈ D(I, a)
T
T
t
t
sets v σ (I 0 , a0 ) = ψ σ̄−i ,T (I 0 , a0 ) and v σ (I 0 ) = ψ σ̄−i ,T (I 0 )
then after T 0 additional iterations of CFR with RM, the
0
bound on exploitability of σ̄ T +T is no worse than having
played T + T 0 iterations of CFR with RM unaltered.
Proof. The proof builds upon Theorem 2 in (Brown & Sand
PT
T
σt
holm, 2016). Assume T ψ σ̄−i ,T (I, a) ≤
t=1 v (I).
We wish to warm start to T iterations. For each I 0 ∈ D(I, a)
T
T
t
t
set v σ (I 0 , a0 ) = ψ σ̄−i ,T (I 0 , a0 ) and v σ (I 0 ) = ψ σ̄−i ,T (I 0 )
T
t
and set v σ (I, a) = ψ σ̄−i ,T (I, a) for all t ≤ T . For every
other action, leave regret unchanged. For each I 0 ∈ D(I, a)
we know by construction that Φ(RT (I 0 )) is within the
CFR bound yIT0 after changing regret. By assumption

PT
T
σt
T
T ψ σ̄−i ,T (I, a) ≤
t=1 v (I), so R (I, a) ≤ 0 and
therefore Φ(RT (I)) is unchanged. Finally, since the T iterations were played according to CFR with RM and regret
is unchanged for every other information set I 00 , so the
conditions for Theorem 2 in (Brown & Sandholm, 2016)
hold for every information set, and therefore we can warm
start to T iterations of CFR with RM with no penalty to the
convergence bound.

B. Proof of Theorem 1
Proof. From Lemma 1 we can immediately set regret for
T
t
a ∈ A(I) to v σ (I, a) = ψ σ̄−i ,T (I, a). By construction of
T 0 , Rt (I, a) is guaranteed to be nonpositive for T ≤ t ≤
0
T + T 0 and therefore σ t (I, a) = 0. Thus, σ̄iT +T (I 0 ) for
I 0 ∈ D(I, a) is identical regardless of what is played in
D(I, a) during T ≤ t ≤ T + T 0 .

T +T 0
0
Since
(T
+
T 0 ) ψ σ̄−i ,T +T (I, a)
≤


PT +T 0 σt
T
σ̄−i
,T
0
T ψ
(I, a) + T U (I, a) and
t=1 v (I) ≥


t
v σ (I) + T 0 L(I) , so by the definition of T 0 ,

PT +T 0 σt
T +T 0
0
(T + T 0 ) ψ σ̄−i ,T +T (I, a) ≤
t=1 v (I). So if
0
regrets in D(I, a) and RT +T (I, a) are set according to
Lemma 1, then after T 00 additional iterations of CFR with
0
00
RM, the bound on exploitability of σ̄ T +T +T is no worse
0
00
than having played T + T + T iterations of CFR with
RM from scratch.
PT

t=1

C. Proof of Theorem 2
Proof. Consider an information set I and action a ∈ A(I)
∗
where for every opponent Nash equilibrium strategy σ−P
(I) ,
∗

∗

CBV σ−P (I) (I, a) < CBV σ−P (I) (I). Let i = P (I). Let
δ
=
minσ−i ∈Σ∗ CBV σ−i (I) − CBV σ−i (I, a)
∗
0
where Σ is the set of Nash equilibria. Let σ−i
=
arg maxσ−i ∈Σ−i |CBV σ−i (I)−CBV σ−i (I,a)≤ 3δ u−i (σ−i , BR(σ−i ))
4
0
Since σ−i
is not a Nash equilibrium strategy and
CFR converges to a Nash equilibrium strategy for
both players, so there exists a Tδ such that for
T
T
all T ≥ Tδ , CBV σ̄−i (I) − CBV σ̄−i (I, a) > 3δ
4 .
2

2

0
0
Let TI,a
= 4|I| δ∆2 |A| .
For T ≥ TI,a
since
P
P
T
T
σ̄−i
T
T
σt
Ri ≤ I∈Ii R (I), so CBV
(I) − t=1 v (I) ≤ 2δ .
0
Let TI,a = max(TI,a , Tδ ) and δI,a = 4δ . Then for
T

T ≥ TI,a , CBV σ̄−i (I, a) −

PT

t

v σ (I)
T

t=1

≤ −δI,a .

D. Proof of Corollary 1
Proof. Let I 6∈ IS . Then I ∈ D(I 0 , a0 ) for some I 0 and
a0 ∈ A(I 0 ) such that for every opponent Nash equilibrium
∗
σ−P
σ∗
∗
(I 0 ) (I 0 , a0 ) < CBV −P (I 0 ) (I 0 ).
strategy σ−P
(I 0 ) , CBV
Applying Theorem 2, this means there exists a TI 0 ,a0 and
T
δI 0 ,a0 > 0 such that for T ≥ TI 0 ,a0 , CBV σ̄−i (I 0 , a0 ) −
t

v σ (I 0 )
≤
T
TI 0 ,a0 for I 0 and
PT

−δI 0 ,a0 . So (5) always applies for T ≥
a0 and I will always be pruned. Since (8)
does not require knowledge of regret, it need not be stored
for I.
t=1

Since D(I 0 , a0 ) will always be pruned for T ≥ TI 0 ,a0 , so
(T

0

2
0)

for any T ≥ IC,a2 iterations for some constant C > 0,
T
πiσ̄ (I) ≤ √CT , which satisfies the threshold of the average strategy. Thus, the average strategy in D(I, a) can be
discarded.

E. Lemma 2
Lemma 2. If for all T ≥ T 0 iterations of CFR with BRP,
 PT
T
t
T CBV σ̄ (I, a) − t=1 v σ (I) ≤ −xT for some x > 0,
then any history h0 such that h · a vh0 for some h ∈ I need
only be traversed at most O ln(T ) times.

Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning

Proof. Let a ∈ A(I) be an action such that for all T ≥
 PT
T
t
T 0 , T CBV σ̄ (I, a) − t=1 v σ (I) ≤ −xT for some
T
T
x > 0. ψ σ̄−i ,T (I, a) ≤ CBV σ̄−i , so from Theorem 1,
xT
c iterations
D(I, a) can be pruned for m ≥ b U (I,a)−L(I)
on iteration T . Thus, over iterations T ≤ t ≤ T + m,
only a constant number of traversals must be done. So each
C
iteration requires only m
work when amortized, where C
is a constant. Since x, U (I, a), and L(I) are constants, so
on each iteration t ≥ T 0 , only an average of Ct traversals of
D(I, a) is required. Summing over all t ≤ T for T ≥ T 0 ,
0
and recognizing that T
 is a constant, we get that action a is
only taken O ln(T ) over T iterations. Thus, any history
h0 such that h·a v h0 for some h ∈ I need only be traversed
at most O ln(T ) times.

F. Proof of Theorem 3
Proof. Consider an h∗ 6∈ S. Then there exists some
h · a v h∗ such that h ∈ S but h · a 6∈ S. Let I = I(h) and
i = P (I). Since h·a 6∈ S but h ∈ S, so for every Nash equi∗
∗
librium σ ∗ , CBV σ (I, a) < CBV σ (I). From Theorem 2,
there exists a TI,a and δI,a > 0 such that after T ≥ TI,a
T

PT

vσ

t (I)

iterations of CFR, CBV σ̄−i (I, a) − t=1T
≤ −δI,a .
∗
Thus from
Lemma
2,
h
need
only
be
traversed
at most

O ln(T ) times.

