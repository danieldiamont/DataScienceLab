Supplementary Material: Multiple Clustering Views from Multiple
Uncertain Experts
Yale Chang Junxiang Chen Michael H. Cho
Peter J. Castaldi Edwin K. Silverman Jennifer G. Dy

1

Parameter Settings

All approaches need to specify K, the number of clusters.
When applying our MCVC to the synthetic and benchmark datasets:
â€¢ If there is only one ground-truth expert view, we set K to be the true number of clusters in that view.
â€¢ If there are more than one ground-truth expert view, we set K to be the maximal value among the true
number of clusters in all expert views.
For SemiCrowd, ITML, MPCKMeans, CSPA, since we only apply them to one expert view, we set K to be
the true number of clusters in that view.
For COPD data, we set K = 4 for all approaches according to a recent study on COPD [3].
Besides the number of clusters, the parameters that are specific to each approach are set as follows.
1.1

Proposed Approach: MCVC

For variational inference in our approach, we use the following parameter settings
1. G, the number of components in truncated Dirichlet Process, is set to be M/2, where M is the total
number of experts. In this way, we try to enforce the constraint that on average, there should be at least
two experts in each view. In all experiments, the number of expert views recovered by our approach is
smaller than G = M/2. Therefore, the value of G we use is large enough to discover the true number
of expert views.
2. For the parameters of prior distributions:
â€¢ p(Î±m ), p(Î²m ): set the parameters of prior Beta distributions to be (10, 1) to incorporate the prior
knowledge that each expertâ€™s accuracy parameters should be far away from 0.5 (random guess) and
close to 1. The choice can be illustrated from Figure 1. Under this setting, there is very small
probability that the accuracy parameters can be close to 0.5.
10

8

alpha
alpha
alpha
alpha

=
=
=
=

1
2
5
10

6

4

2

0
0.0

0.2

0.4

0.6

0.8

1.0

Figure 1: Probability density function of Beta distribution Beta(Î±, 1), Î± = 10 can effectively make the accuracy
parameters have very small probability to be close to 0.5.

â€¢ p(Î½g ): set its concentration parameter Î³ = 1, the recovered number of experts is stable across a
range of different Î³ values (from 0.5 to 10).
(g)
(g)
â€¢ p(Wij ), p(bi ): set the mean and standard deviation of prior Gaussian distributions to be 0 and 1
respectively.
â€“1â€“

3. Initializations of variational parameters:
â€¢ the parameters of variational Beta distribution are initialized to be equal to those of the prior distribution;
â€¢ the means of variational Gaussian distributions are initialized by randomly drawing samples from
1
), the standard deviations of variational Gaussian distributions are
Gaussian distribution N (0, D
initialized to be 0.001. This initialization strategy is similar to Xavier initialization [5] in order to
avoid gradient saturation;
(g)
â€¢ Î·i,: are initialized using W (g) , b(g) according to the discriminative clustering model described in
the main paper;
â€¢ Ï†m,: are initialized by sampling from a Dirichlet distribution with all its parameters being equal to
1, introducing most randomness for the initialization.
4. The number of random initializations for optimization is set to be 50 and the results are stable across
different runs in all our experiments.
5. Cluster samples in the testing set:
After learning our model using the training set, we can obtain the variational distributions of weight
q(W (g) ) and offset q(b(g) ). We can cluster xt , a sample from the testing set by integrating out W (g) , b(g)
through Monte Carlo approximation to the integration:
Z Z
(g)
(g)
p(Zt = k|xt ) =
p(Zt |W (g) , b(g) ; xt )q(W (g) )q(b(g) )dW (g) db(g)
(1)
L

â‰ˆ

1X
(g) [
d
(g) , b
(g) ; x )
p(Zt |W
l
t
l
L

(2)

l=1

d
[
(g) , b
(g) are the l-th sample from q(W (g) ) and q(b(g) ) respectively. We set L = 100 in all
where W
l
l
experiments. xt is assigned to the cluster corresponding to the largest probability:
(g)

yË†t = arg max p(Zt

= k|xt )

(3)

k=1,Â·Â·Â·K

where yË†t is the predicted cluster label for xt .
1.2

Meta Clustering

After computing the similarity matrix between multiple experts, we apply spectral clustering [6] to assign
experts to different views. The number of views are automatically determined using the eigen-gap heuristic
[7].
1.3

SemiCrowd

We follow parameter settings recommended by the author in [9]. In particular, d0 , d1 , two thresholds used
to filter out uncertain sample pairs in the average similarity matrix, are set to be 0 and 0.8 respectively.
Parameters of the `1 regularized matrix completion algorithm is set according to heuristics in the literature
[8].
1.4

ITML

As is suggested by the author [4], we set lower and upper bounds associated with the constraint terms to
be the 5th and 95th percentiles of the observed distribution of distances between pairs of points within the
dataset. Other parameters related to the convergence of the algorithm are set to be default.
1.5

MPCKMeans

We specify the constraints according to the instructions listed on the authorâ€™s website and directly run the
authorâ€™s Java implementation [1]
â€“2â€“

1.6

CSPA

After computing the average similarity matrix, we use spectral clustering [6] to obtain the cluster labels.

2
2.1

Variational Inference
Variational Distribution

Denote h = {Î±1:M , Î²1:M , c1:M , Î½1:G , Z (1:G) , W (1:G) , b(1:G) } as the collection of latent variables, where G
is the number of components of the truncated Dirichlet Process. We assume the variational distribution has
the following factorization formula:
q(Î±1:M , Î²1:M , c1:M , Î½1:G , Z (1:G) , W (1:G) , b(1:G) )

(4)

(1:G)

(1:G)

(1:G)

= q(Î±1:M ) Â· q(Î²1:M ) Â· q(c1:M ) Â· q(Î½1:G ) Â· q(Z
) Â· q(W
) Â· q(b
M
G
i
Y
Yh
=
[q(Î±m ) Â· q(Î²m ) Â· q(cm )]
q(Î½g )q(Z (g) )q(W (g) )q(b(g) )

=

m=1

g=1

M
Y

G
Y

[q(Î±m ) Â· q(Î²m ) Â· q(cm )]

g=1

m=1

ï£®
ï£°q(Î½g )

n
Y
i=1

(g)

q(Zi )

d Y
K
Y

(g)

q(Wij )

i=1 j=1

)

(5)
(6)

K
Y

ï£¹
(g)
q(bi )ï£»

(7)

i=1

where the marginal variational distribution of each random variable is
(m) (m)
â€¢ q(Î±m ) = Beta(Ï„Î±1 , Ï„Î±2 );
(m) (m)
â€¢ q(Î²m ) = Beta(Ï„Î² 1 , Ï„Î² 2 );
â€¢ q(cm ) = Cat(Ï†m,: );
(g) (g)
â€¢ q(Î½g ) = Beta(Ï„Î½ 1 , Ï„Î½ 2 );
(g)
(g)
â€¢ q(Zi ) = Cat(Î·i,: );
(g)

(g)

(g)2

â€¢ q(Wij ) = N (ÂµWij , ÏƒWij );
(g)

(g)

(g)2

â€¢ q(bi ) = N (Âµbi , Ïƒbi ).
We use Î¸ to denote all the variational parameters, which consist of the following
(m) (m) (m) (m)
â€¢ For m = 1 Â· Â· Â· M , consider {Ï„Î±1 , Ï„Î±2 , Ï„Î² 1 , Ï„Î² 2 , Ï†m,: âˆˆ âˆ†Gâˆ’1 }, where âˆ†Gâˆ’1 is a simplex in Gdimensional space.
(g) (g) (g)
(g)
(g)2
â€¢ For g = 1 Â· Â· Â· G, consider Ï„Î½ 1 , Ï„Î½ 2 , Î·i,: âˆˆ âˆ†Kâˆ’1 (i = 1 Â· Â· Â· n), ÂµWij , ÏƒWij (i = 1 Â· Â· Â· d, j = 1 Â· Â· Â· K),
(g)

(g)2

Âµbi , Ïƒbi (i = 1 Â· Â· Â· K)
Besides the simplex constraints on the parameters of categorical distribution, both the parameters of Beta
distribution and the standard deviation of Gaussian distribution should have positive constraints.
2.2

Evidence Lower Bound

Given variational distribution q(h; Î¸), the log-likelihood log p(S (1:M ) ) can be decomposed as
h
i
log p(S (1:M ) ) = Lq(h;Î¸) + KL q(h; Î¸) | p(h|S (1:M ) )
â‰¥ Lq(h;Î¸)

(8)
(9)

i
log p(h, S (1:M ) ) âˆ’ log q(h; Î¸)

(10)

i
= Eq(h;Î¸) log p(S (1:M ) |h) + log p(h) âˆ’ log q(h; Î¸)
h
i
= Eq(h;Î¸) log p(S (1:M ) |h) âˆ’ KL [q(h; Î¸) | p(h)]

(11)

= Eq(h;Î¸)

h
h

â€“3â€“

(12)

The first term prefers the variational distribution q(h; Î¸) that maximizes the expected conditional likelihood,
the second term forces the variational distribution to be close to the prior distribution.
The overall objective of variational inference is to maximize the evidence lower bound (ELBO) Lq(h;Î¸)
w.r.t. Î¸.
Let f (h; Î¸) = log p(S (1:M ) |h) + log p(h) âˆ’ log q(h; Î¸), the evidence lower bound can be expressed as


Lq(h;Î¸) = Eq(h;Î¸) f (h; Î¸)
(13)
We first compute the three terms in f (h; Î¸) and then evaluate their expectations w.r.t. the variational distribution q(h; Î¸).
2.3

ELBO: The First Term log p(S (1:M ) |h)

Consider the first term:
ï£«
log p(S (1:M ) |h) = log ï£­

ï£¶

M
Y

(m)
p(Sij |h)ï£¸

Y

(14)

m=1 (i,j)âˆˆE (m)

=

M
X

(m)

X

log p(Sij |h)

(15)

m=1 (i,j)âˆˆE (m)

=

M 
X
m=1



1 log Î²m +

X
(i,j)âˆˆE (m)

X

(m)

Sij



log

1 âˆ’ Î² 

(i,j)âˆˆE (m)

m

Î²m

(16)

i
Î±m Î²m
(1 âˆ’ Î±m )(1 âˆ’ Î²m )
(i,j)âˆˆE (m)

 X

1 âˆ’ Î±m
(m)
+
Aij
log(
)
Î²m
(m)
+



X

(m)

(m)

Sij Aij



log

h

(i,j)âˆˆE

To compute Eq(h;Î¸) [log p(S (1:M ) | h)], the following formulas can be used:
(m)

(m)

(m)

Eq(h;Î¸) [log Î±m ] = Eq(Î±m ) [log Î±m ] = Ïˆ(Ï„Î±1 ) âˆ’ Ïˆ(Ï„Î±1 + Ï„Î±2 )
(m)

(m)

(17)
(m)

Eq(h;Î¸) [log(1 âˆ’ Î±m )] = Eq(Î±m ) [log(1 âˆ’ Î±m )] = Ïˆ(Ï„Î±2 ) âˆ’ Ïˆ(Ï„Î±1 + Ï„Î±2 )
Eq(h;Î¸) [log Î²m ] = Eq(Î²m ) [log Î²m ] =

(m)
Ïˆ(Ï„Î² 1 )

âˆ’

(m)
Ïˆ(Ï„Î² 1

+

(m)

(m)
Ï„Î² 2 )

(m)

(19)
(m)

Eq(h;Î¸) [log(1 âˆ’ Î²m )] = Eq(Î²m ) [log(1 âˆ’ Î²m )] = Ïˆ(Ï„Î² 2 ) âˆ’ Ïˆ(Ï„Î² 1 + Ï„Î² 2 )
(m)

(c )

(c )

Eq(h;Î¸) [Aij ] = Eq(cm ,Z (cm ) ) [I(Zi m = Zj m )]

(c )
(c ) 
= Eq(cm ) Eq(Z (cm ) ) [I(Zi m = Zj m )]
= Eq(cm )

K
hX

(c ) (c )

Î·ikm Î·jkm

i

(18)

(20)
(21)
(22)
(23)

k=1


G 
K
X
X
g g
=
Ï†mg
Î·ik Î·jk
g=1

k=1

â€“4â€“

(24)

2.4

ELBO: The Second Term log p(h)

Consider the second term:
log p(h) = log

Y
M 

âˆž 
Y

p(Î±m )p(Î²m )p(cm |Î½1:âˆž )
p(Î½g )p(Z (g) |W (g) , b(g) )p(W (g) )p(b(g) )
(25)

m=1

=

M
X



g=1


log p(Î±m ) + log p(Î²m ) + log p(cm |Î½1:âˆž )

m=1
âˆž 
X

+

(26)

log p(Î½g ) + log p(Z (g) |W (g) , b(g) ) + log p(W (g) ) + log p(b(g) )



g=1

where
log p(Î±m ) =

(m)
(Ï„Î±1 0

log p(Î²m ) =

(m)
(Ï„Î² 1 0

log p(cm |Î½1:âˆž ) =

âˆž
X

âˆ’ 1) log Î±m +

(m)
(Ï„Î±2 0

âˆ’ 1) log Î²m +

(m)
(Ï„Î² 2 0


cmg log Î½g +

g=1

gâˆ’1
X


âˆ’ 1) log(1 âˆ’ Î±m ) âˆ’ log


log(1 âˆ’ Î½j )

(m)

(m)


(27)

(m)

(m)

(m)

(m)

Î“(Ï„Î² 1 0 )Î“(Ï„Î² 2 0 )


(28)

Î“(Ï„Î² 1 0 + Ï„Î² 2 0 )
(29)

j=1


log p(Î½g ) = (Î³ âˆ’ 1) log(1 âˆ’ Î½g ) âˆ’ log

Î“(Î³)
Î“(1 + Î³)


(30)

= (Î³ âˆ’ 1) log(1 âˆ’ Î½g ) + log Î³
log p(Z (g) |W (g) , b(g) ) = log

(m)

Î“(Ï„Î±1 0 + Ï„Î±2 0 )


âˆ’ 1) log(1 âˆ’ Î²m ) âˆ’ log

(m)

Î“(Ï„Î±1 0 )Î“(Ï„Î±2 0 )

N
Y

(31)

(g)

p(Zi |W (g) , b(g) )

(32)

i=1

= log

K
N Y
Y

(g)

p Zi

I[Zi(g) =k]

(33)

= k|W (g) , b(g) )

(34)

= k|W (g) , b(g)

i=1 k=1

=

N X
K
X

(g)

I[Zi

(g)

= k] log p(Zi

i=1 k=1

=

N X
K
X

(g)

I[Zi


K
X

(g)T
(g)
(g)T
(g)
= k] (wk xi + bk ) âˆ’ log
exp(wj xi + bj )

i=1 k=1

j=1

(35)
log p(W (g) ) =

d X
K 
X

âˆš
(g)
âˆ’ log( 2Ï€ÏƒWij 0 ) âˆ’

i=1 j=1

log p(b

(g)

)=

K 
X
i=1

(g)
(Wij


(g)
âˆ’ ÂµWij 0 )2
(g)2
2ÏƒWij 0

(g)
(g)
âˆš
(bi âˆ’ Âµbi 0 )2
(g)
âˆ’ log( 2Ï€Ïƒbi 0 ) âˆ’
(g)2
2Ïƒbi 0

(36)


(37)

To compute Eq(h;Î¸) [log p(h)], the following formulas can be used besides the formulas used to compute

â€“5â€“

Eq(h;Î¸) [log p(S (1:M ) | h)]:
(g)

(g)

(g)

Eq(h;Î¸) [log Î½g ] = Eq(Î½g ) [log Î½g ] = Ïˆ(Ï„Î½ 1 ) âˆ’ Ïˆ(Ï„Î½ 1 + Ï„Î½ 2 )
(g)
Ïˆ(Ï„Î½ 2 )

Eq(h;Î¸) [log(1 âˆ’ Î½g )] = Eq(Î½g ) [log(1 âˆ’ Î½g )] =
Eq(h;Î¸)



âˆ’

(g)
Ïˆ(Ï„Î½ 1

+

(38)
(g)
Ï„Î½ 2 )

Eq(h;Î¸) [cmg ] = Eq(cm ) [cmg ] = Ï†mg

(g)
(g)
I[Zi = k] = Î·ik

(g)
Eq(h;Î¸) [wk ]
(g)
(g)
Eq(h;Î¸) [(Wij âˆ’ ÂµWij 0 )2 ]
(g)
Eq(h;Î¸) [(bi

âˆ’

(g)
Âµbi 0 )2 ]

(39)
(40)
(41)

=

(g)
(g)
Eq(W (g) ) [wk ] = ÂµW:k
(g)2
(g)
(g)
ÏƒWij + (ÂµWij âˆ’ ÂµWij 0 )2

(43)

=

(g)2
Ïƒbi

(44)

=

+

(g)
(Âµbi

âˆ’

(42)

(g)
Âµbi 0 )2

2.4.1

Upper Bound of Log-sum Function

P

(g)T
K
The computation of Eq(h;Î¸) log
xi + bj ) does not have closed form and can only be
j=1 exp(wj
approximated through sampling. However, we can use the upper bound of log-sum function based on the
log concavity to derive a lower bound of ELBO [2].
log

K
X

(g)T

exp(wj

K

X
(g)T
(g)
(g)
(g)
(g)
exp(wj xi + bj ) âˆ’ log(ri ) âˆ’ 1
xi + bj ) â‰¤ ri

(45)

j=1

j=1
(g)

We need introduce new variational parameters ri > 0 to optimize. The expectation term can also be upper
bounded as follows

K

X
(g)T
exp(wj xi + bj )
Eq(h;Î¸) log
j=1



K
X
(g)T
(g)
(g)
(g)
exp(wj xi + bj ) âˆ’ log(ri ) âˆ’ 1
â‰¤ Eq(h;Î¸) ri

(46)

j=1
(g)

= ri

K
X

Eq(h;Î¸) [exp(

j=1

=

(g)
ri

d
X

(g)

(g)

(g)

wlj xil + bj )] âˆ’ log(ri ) âˆ’ 1

(47)

l=1

K
X

!
d
 b(g)  Y
 w(g) xil 
(g)
Eq(W (g) ) e lj
Eq(b(g) ) e j
âˆ’ log(ri ) âˆ’ 1
j

j=1

(48)

lj

l=1

where the expectation can be evaluated using the mean of log-normal distribution.
(g)2


Ïƒb 
 (g) 
(g)
Eq(b(g) ) ebj = exp Âµbj + j
j
2
(g)2

ÏƒWlj x2il 
 (g) 
(g)
Eq(W (g) ) ewlj xil = exp ÂµWlj xil +
lj
2
Therefore, the upper bound can be written as

K

X
(g)T
Eq(h;Î¸) log
exp(wj xi + bj )

(49)
(50)

j=1

â‰¤

(g)
ri

K
X
j=1


exp

(g)2

(g)
Âµbj

+

Ïƒbj

2

+

d 
X

(g)2

(g)
ÂµWlj xil

l=1

â€“6â€“

+

ÏƒWlj x2il 
2

!

(g)

âˆ’ log(ri ) âˆ’ 1

(51)

The upper bound becomes minimal when
(g)

ri

1

=



PK

exp

j=1

(g)2

(g)
Âµbj

+

Ïƒb

j

+

2



Pd

l=1

(g)2

(g)
ÂµWlj xil

+

ÏƒW x2il 

(52)

!

lj

2

Then the upper bound becomes

Eq(h;Î¸) log

K
X

(g)T
exp(wj xi

+ bj )



(53)

j=1

â‰¤ log

K
X


exp

(g)2

(g)
Âµbj

+

Ïƒbj

+

2

j=1

d 
X

(g)2

(g)
ÂµWlj xil

+

ÏƒWlj x2il 

!
(54)

2

l=1

To avoid numerical overflow when computing this bound due to the potential large exponent of the exponential function, we can use the logsumexp() function implemented in Scipy. The idea is to extract the
maximal value in the sequence and compute its log, then each exponent in the sequence will be less than 1,
leading to stable numerical behavior.
2.5

ELBO: The Third Term log q(h; Î¸)

Consider the third term:

M 
X
log q(h; Î¸) =
log q(Î±m ) + log q(Î²m ) + log q(cm )

(55)

m=1

+

G 
X

log q(Î½g ) +

g=1

n X
K
X

(g)
I[Zi

=

(g)
k] log q(Zi

= k) +

K
d X
X

(g)
log q(Wij )

+



(g)
log q(bi )

i=1

i=1 j=1

i=1 k=1

K
X

(56)
where
(m)

(m)

(m)

(m)

log q(Î²m ) = (Ï„Î² 1 âˆ’ 1) log Î²m + (Ï„Î² 2 âˆ’ 1) log(1 âˆ’ Î²m ) âˆ’
log q(cm ) =

G
X

 Î“(Ï„ (m) )Î“(Ï„ (m) ) 

Î±1
Î±2
(m)
(m)
Î“(Ï„Î±1 + Ï„Î±2 )
(m) 
 Î“(Ï„ (m)
Î² 1 )Î“(Ï„Î² 2 )
log
(m)
(m)
Î“(Ï„Î² 1 + Ï„Î² 2 )

log q(Î±m ) = (Ï„Î±1 âˆ’ 1) log Î±m + (Ï„Î±2 âˆ’ 1) log(1 âˆ’ Î±m ) âˆ’ log

cmg log Ï†mg

(57)

(58)

(59)

g=1
(g)

(g)

log q(Î½g ) = (Ï„Î½ 1 âˆ’ 1) log Î½g + (Ï„Î½ 2 âˆ’ 1) log(1 âˆ’ Î½g ) âˆ’ log
(g)

log q(Zi

(g)

= k) = log Î·ik

âˆš
(g)
(g)
log q(Wij ) = âˆ’ log( 2Ï€ÏƒWij ) âˆ’
(g)
log q(bi )

 Î“(Ï„ (g) )Î“(Ï„ (g) ) 
Î½1
(g)

Î½2
(g)

(60)

Î“(Ï„Î½ 1 + Ï„Î½ 2 )
(61)

(g)
(Wij

(g)
âˆ’ ÂµWij )2
(g)2
2ÏƒWij

(g)
(g)
âˆš
(bi âˆ’ Âµbi )2
(g)
= âˆ’ log( 2Ï€Ïƒbi ) âˆ’
(g)2
2Ïƒbi

(62)

(63)

The computation of Eq(h;Î¸) [log q(h; Î¸)] can be done using formulas from the previous two subsections.
â€“7â€“

3

Weights of Different Experts

As we can see from the derivation of the first-term log p(S (1:M ) |h) of ELBO, if we view Equation (16) as
(m)
(m)
Î²m
.
a function of Aij , the weight of Sij (constraints provided by the m-th expert) is: log (1âˆ’Î±Î±mm)(1âˆ’Î²
m)

References
[1] M. Bilenko. Java implementation of mpckmeans, 2004.
[2] G. Bouchard. Efficient bounds for the softmax function and applications to approximate inference
in hybrid models. In NIPS 2007 workshop for approximate Bayesian inference in continuous/hybrid
systems. Citeseer, 2007.
[3] P. J. Castaldi, J. Dy, J. Ross, Y. Chang, G. R. Washko, D. Curran-Everett, A. Williams, D. A. Lynch,
B. J. Make, J. D. Crapo, et al. Cluster analysis in the copdgene study identifies subtypes of smokers
with distinct patterns of airway disease and emphysema. Thorax, pages thoraxjnlâ€“2013, 2014.
[4] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon. Information-theoretic metric learning. In
Proceedings of the 24th international conference on Machine learning, pages 209â€“216. ACM, 2007.
[5] X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks.
In Aistats, volume 9, pages 249â€“256, 2010.
[6] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. Advances in
neural information processing systems, 2:849â€“856, 2002.
[7] U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395â€“416, 2007.
[8] J. Yi, R. Jin, A. K. Jain, and S. Jain. Crowdclustering with sparse pairwise labels: A matrix completion
approach. In AAAI Workshop on Human Computation, volume 2. Citeseer, 2012.
[9] J. Yi, R. Jin, S. Jain, T. Yang, and A. K. Jain. Semi-crowdsourced clustering: Generalizing crowd
labeling by robust distance metric learning. In Advances in Neural Information Processing Systems,
pages 1772â€“1780, 2012.

â€“8â€“

