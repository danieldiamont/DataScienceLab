Globally Optimal Gradient Descent for a ConvNet with
Gaussian Inputs
Supplementary Material
Contents
A Proof of Lemma 3.2

1

B Proof of Proposition 4.1

2

C Missing Proofs for Section 5
C.1 Proof of Proposition 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 Proof of Theorem 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3
3
6

D Missing Proofs for Section 7.1
D.1 Proof of Proposition 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.2 Proof of Proposition 7.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

12
12
13

E Uniqueness of Global Minimum in the Population Risk

16

F Experimental Setup for Section 7.2

16

A

Proof of Lemma 3.2

First assume that Î¸u,v 6= 0, Ï€ . Then we have
s
!
 u Â· v 2 
 u Â· v  u Â· v
âˆ‚g
ui
1
kvk
+
=
1âˆ’
+ Ï€ âˆ’ arccos
âˆ‚ui
2Ï€
kuk
kuk kvk
kuk kvk kuk kvk
!
!
uÂ·v
1
vi
ui
uÂ·v
kukkvk
kuk kvk
âˆ’r
âˆ’
+

2
2Ï€
kuk kvk kuk2 kuk kvk
uÂ·v
1 âˆ’ kukkvk
!!
uÂ·v
ui
uÂ·v
vi
kukkvk
q
+
2 kuk kvk âˆ’
2
uÂ·v
kuk kuk kvk
1âˆ’
kukkvk

!!
 u Â· v 
vi
ui
uÂ·v
Ï€ âˆ’ arccos
âˆ’
=
kuk kvk
kuk kvk kuk2 kuk kvk
s
!
 u Â· v 2 
 u Â· v  u Â· v
1
ui
kvk
1âˆ’
+ Ï€ âˆ’ arccos
+
2Ï€
kuk
kuk kvk
kuk kvk kuk kvk
!!

 u Â· v 
1
vi
ui
uÂ·v
kuk kvk Ï€ âˆ’ arccos
âˆ’
=
2Ï€
kuk kvk
kuk kvk kuk2 kuk kvk


1

1
ui
kvk
2Ï€
kuk

s

 u Â· v 2
 u Â· v 
1 
Ï€ âˆ’ arccos
vi =
+
kuk kvk
2Ï€
kuk kvk

1
ui
1 
Ï€ âˆ’ Î¸u,v vi
kvk
sin Î¸u,v +
2Ï€
kuk
2Ï€
1âˆ’

Hence,

âˆ‚g
1
u
1 
Ï€ âˆ’ Î¸u,v v
=
kvk
sin Î¸u,v +
âˆ‚u
2Ï€
kuk
2Ï€

(1)

Now we assume that u is parallel to v. We first show that g is differentiable in this case. Without
loss of generality we can assume that u and v lie on the u1 axis. This follows since g is a function of
kuk, kvk and Î¸u,v and therefore g(Â·, v) has a directional derivative in direction d at u if and only if
g(Â·, Rv) has a directional derivative in direction Rd at Ru where R is a rotation matrix. Hence g(Â·, v)
is differentiable at u if and only if g(Â·, Rv) is differentiable at Ru. Furthermore, if v and u are on the
u1 axis, then by symmetry the partial derivatives with respect to other axes at u are all equal, hence
we only need to consider the partial derivative with respect to the u1 and u2 axes.
Let v = (1, 0, ..., 0) and u = (u, 0, ..., 0) where u 6= 0. In order to show differentiability, we will
prove that g(u, v) has continuous partial derivatives at u (by equality (1) the partial derivatives are
clearly continuous at points that are not on the u1 axis. Define u = (u, , 0, ..., 0). Then
!


1
sin Î¸u ,v + Ï€ âˆ’ Î¸u ,v cos Î¸u ,v âˆ’ g(u, v)
2Ï€ ku k kvk
âˆ‚g
(u, v) = lim
â†’0
âˆ‚u2

By Lâ€™hopitalâ€™s rule and the calculation of equality (1) we get
âˆ‚g

1
kvk
sin Î¸ = 0
(u, v) = lim
â†’0
âˆ‚u2
2Ï€
ku k
âˆ‚g
(u0 , v) = 0 since limu0 â†’u sin Î¸u0 ,v = 0.
Furthermore, by equality (1) we see that limu0 â†’u âˆ‚u
2
âˆ‚g
âˆ‚g
For a fixed Î¸u,v equal to 0 or Ï€, âˆ‚u
(u, v) is the same as âˆ‚kuk
(u, v). Hence,
1

âˆ‚g
1
kvk
(u, v) =
âˆ‚u1
2Ï€





sin Î¸u,v + Ï€ âˆ’ Î¸u,v cos Î¸u,v

!

1
=

2 if u > 0
0 if u < 0

and the partial derivative is continuous since
âˆ‚g 0
lim
(u , v) =
u0 â†’u âˆ‚u1

1

2 if u > 0
0 if u < 0

Finally, we see that for the case where u and v are parallel, the values we got for the partial
derivatives coincide with equation Eq. 1. This concludes the proof.

B

Proof of Proposition 4.1

We will prove the claim by induction on k. For the base case we will show that Set-Splitting-by-2-Sets
is NP-complete. We will prove this via a reduction from a variant of the 3-SAT problem with the
restriction of equal number of variables and clauses, which we denote Equal-3SAT. We will first prove
that Equal-3SAT is NP-complete.
Lemma B.1. Equal-3SAT is NP-complete.
2

Proof. This can be shown via a reduction from 3SAT. Given a formula Ï† with n variables and m
clauses we can increase n âˆ’ m by 1 by adding a new clause of the form (x âˆ¨ y) for new variables x
and y. Furthermore, we can decrease n âˆ’ m by 1 by adding two new identical clauses of the form (z)
for a new variable z. In each case the formula with the new clause(s) is satisfiable if and only if Ï† is.
Therefore given a formula Ï† we can construct a new formula Ïˆ with equal number of variables and
clauses such that Ï† is satisfiable if and only if Ïˆ is.
We will now give a reduction from Equal-3SAT to Set-Splitting-by-2-Sets.
Lemma B.2. Set-Splitting-by-2-Sets is NP-complete.
Proof. The following reduction is exactly the reduction from 3SAT to Splitting-Sets and we include
it here for completeness. Let Ï† be a formula with set of variables V and equal number of variables
and clauses. We construct the sets S and C as follows. Define
S = {xÌ„ | x âˆˆ V } âˆª V âˆª {n}
where xÌ„ is the negation of variable x and n is a new variable not in V . For each clause c with set
of variables or negations of variables Vc that appear in the clause (for example, if c = (xÌ„ âˆ¨ y) then
Vc = {xÌ„, y}) construct a set Sc = Vc âˆª {n}. Furthermore, for each variable x âˆˆ V construct a set
Sx = {x, xÌ„}. Let C be the family of subsets Sc and Sx for all clauses c and x âˆˆ V . Note that |C|â‰¤ |S|
which is required by the definition of Set-Splitting-by-2-Sets.
Assume that Ï† is satisfiable and let A be the satisfying assignment. Define S1 = {x|A(x) =
true} âˆª {xÌ„|A(x) = f alse} and S2 = {x|A(x) = f alse} âˆª {xÌ„|A(x) = true} âˆª {n}. Note that S1 âˆª S2 = S.
Assume by contradiction that there exists a set T âˆˆ C such that T âŠ† S1 or T âŠ† S2 . If T âŠ† S1 then T
is not a set Sc for some clause c because n âˆˆ
/ S1 . However, by the construction of S1 a variable and
its negation cannot be in S1 . Hence T âŠ† S1 is impossible. If T âŠ† S2 then as in the previous claim
T cannot be a set Sx for a variable x. Hence T = Sc for some clause c. However, this implies that
A(c) = f alse, a contradiction.
Conversely, assume there exists splitting sets S1 and S2 and w.l.o.g. n âˆˆ S1 . We note that it
follows that no variable x and its negation xÌ„ are both contained in one of the sets S1 or S2 . Define
the following assignment A for Ï†. For all x âˆˆ V if x âˆˆ S1 let A(x) = f alse, otherwise let A(x) = true.
Note that A is a well defined assignment. Assume by contradiction that there is a clause c in Ï† which
is not satisfiable. Since S2 splits Sc it follows that there exists a variable x such that it or its negation
xÌ„ are in S2 (recall that n âˆˆ S1 ). If x âˆˆ S2 then A(x) = true and if xÌ„ âˆˆ S2 then A(xÌ„) = true since
x âˆˆ S1 . In both cases c is satisfiable, a contradiction.
This proves the base case. We will now prove the induction step by giving a reduction from SetSplitting-by-k-Sets to Set-Splitting-by-(k+1)-Sets. Given S = {1, 2, ..., d} and C = {Cj }j such that |C|
â‰¤ (k âˆ’ 1)d, define S 0 = {1, 2, ..., d + 1} and C 0 = C âˆª{Dj }j where Dj = {j, d + 1} for all 1 â‰¤ j â‰¤ d.
Note that |C 0 | â‰¤ kd < k(d + 1). Assume that there are S1 , ..., Sk that split the sets in C. Then if we
Sk+1
define Sk+1 = {d + 1}, it follows that i=1 Si = S and S1 , ..., Sk , Sk+1 are disjoint and split the sets
in C 0 .
Conversely, assume that S1 , ..., Sk , Sk+1 split the sets in C 0 . Let w.l.o.g. Sk+1 be the set that
contains d + 1. Then for all 1 â‰¤ j â‰¤ d we have Dj 6âŠ† Sk+1 . It follows that for all 1 â‰¤ j â‰¤ d, j âˆˆ
/ Sk+1 ,
Sk
or equivalently, Sk+1 = {d + 1}. Hence, i=1 Si = S and S1 , ..., Sk are disjoint and split the sets in C,
as desired.

C

Missing Proofs for Section 5

C.1

Proof of Proposition 5.1

1. For w 6= 0, the claim follows from Lemma 3.2. As in the proof of Lemma 3.2 we can assume
w.l.o.g. that w = (0, 0, ..., 0) and wâˆ— = (1, 0, ..., 0). Let f (w, wâˆ— ) = 2kg(w, wâˆ— ) + (k 2 âˆ’
3

âˆ—

k
k) kwkkw
. It suffices to show that
Ï€
then by Lâ€™hopitalâ€™s rule

lim+

â†’0

âˆ‚f
âˆ—
âˆ‚w2 (w, w )

does not exist. Indeed, let w = (0, , 0, ..., 0)

f (w , wâˆ— ) âˆ’ f (w, wâˆ— )
k
k
k2 âˆ’ k

kwâˆ— k
= lim+ kwâˆ— k
sin Î¸w ,wâˆ— + (k 2 âˆ’ k)
= +

||
Ï€
Ï€
Ï€
â†’0 Ï€

and
lim

â†’0âˆ’

f (w , wâˆ— ) âˆ’ f (w, wâˆ— )
k
k2 âˆ’ k
k

kwâˆ— k
= lim
kwâˆ— k
sin Î¸w ,wâˆ— âˆ’ (k 2 âˆ’ k)
=âˆ’ âˆ’

||
Ï€
Ï€
Ï€
â†’0âˆ’ Ï€

Hence the left and right partial derivatives with respect to variable w2 are not equal, and thus
âˆ‚f
âˆ—
âˆ‚w2 (w, w ) does not exist.
2. We first show that w = 0 is a local maximum if and only if k > 1. Indeed, by considering the
loss function as a function of the variable x = kwk, for any fixed angle Î¸w,wâˆ— we get a quadratic
function of the form `(x) = ax2 âˆ’ bx, where a > 0 and b â‰¥ 0. Since f (Î¸) = sin Î¸ + (Ï€ âˆ’ Î¸) cos Î¸
is a non-negative function for 0 â‰¤ Î¸ â‰¤ Ï€ and f (Î¸) = 0 if and only if Î¸ = Ï€, it follows that
b = 0 if and only if k = 1 and Î¸w,wâˆ— = Ï€. Therefore if k > 1, then for all fixed angles Î¸w,wâˆ— ,
the minimum of `(x) is attained at x > 0, which implies that w = 0 is a local maximum. If
k = 1 and Î¸w,wâˆ— = Ï€ the minimum of `(x) is attained at x = 0, and thus w = 0 is not a local
maximum in this case.
We will now find the other critical points of `. By Lemma 3.2 we get
"


k2 âˆ’ k 
k
w
k
w
k2 âˆ’ k
k+
w âˆ’ kwâˆ— k
sin Î¸w,wâˆ— âˆ’
kwâˆ— k
Ï€ âˆ’ Î¸w,wâˆ— wâˆ— âˆ’
Ï€
Ï€
kwk
Ï€
Ï€
kwk
"
!
#

1
k 2 âˆ’ k k kwâˆ— k
k 2 âˆ’ k kwâˆ— k
k
= 2
k+
âˆ’
sin Î¸w,wâˆ— âˆ’
wâˆ’
Ï€ âˆ’ Î¸w,wâˆ— wâˆ—
k
Ï€
Ï€ kwk
Ï€
kwk
Ï€

1
âˆ‡`(w) = 2
k

#

(2)
and assume it vanishes.
Denote Î¸ , Î¸w,wâˆ— . If Î¸ = 0 then let w = Î±wâˆ— for some Î± > 0. It follows that
k+

k
k2 âˆ’ k k2 âˆ’ k 1
âˆ’
âˆ’ =0
Ï€
Ï€ Î± Î±

or equivalently Î± = 1, and thus w = wâˆ— .
If Î¸ = Ï€ then kwk =

k2 âˆ’k
k2 +(Ï€âˆ’1)k

2

k âˆ’k
kwâˆ— k and thus w = âˆ’( k2 +(Ï€âˆ’1)k
)wâˆ— . By setting Î¸ = Ï€ in the loss
2

k âˆ’k
function, one can see that w = âˆ’( k2 +(Ï€âˆ’1)k
)wâˆ— is a one-dimensional local minimum, whereas
2

k âˆ’k
)wâˆ—
by fixing kwk and decreasing Î¸, the loss function decreases. It follows that w = âˆ’( k2 +(Ï€âˆ’1)k


is a saddle point. If Î¸ 6= 0, Ï€ then w and wâˆ— are linearly independent and thus Ï€k Ï€ âˆ’ Î¸ = 0
which is a contradiction.
2

k âˆ’k
It remains to show that u = âˆ’Î³(k)wâˆ— where Î³(k) = k2 +(Ï€âˆ’1)k
is a degenerate saddle point. We
2
will show that the Hessian at u denoted by âˆ‡ `(u), has only nonnegative eigenvalues and at least
Ëœ
one zero eigenvalue. Let `(w)
, `(w, Rwâˆ— ), where the second entry denotes the ground truth
weight vector and R is a rotation matrix. Denote by fd1 ,d2 the second directional derivative of
a function f in directions d1 and d2 . Similarly to the proof of Lemma 3.2, since ` depends only
on kwk, kwâˆ— k and Î¸w,wâˆ— , we notice that

`d1 ,d2 (w) = `ËœRd1 ,Rd2 (Rw)
4

or equivalently
T T 2Ëœ
Ëœ
dT1 âˆ‡2 `(w)d2 = (Rd1 )T âˆ‡2 `(Rw)Rd
2 = d1 R âˆ‡ `(Rw)Rd2

for any w and directions d1 and d2 . It follows that
Ëœ
âˆ‡2 `(w) = RT âˆ‡2 `(Rw)R
Ëœ
for all w. Since R is an orthogonal matrix, we have that âˆ‡2 `(w) and âˆ‡2 `(Rw)
are similar
matrices and thus have the same eigenvalues. Therefore, we can w.l.o.g. rotate wâˆ— such that it
will be on the w1 axis.
By symmetry we have
âˆ‚`
âˆ‚`
âˆ‚`
âˆ‚`
(u) =
(u),
(u) =
(u)
âˆ‚w1 âˆ‚wi
âˆ‚w1 âˆ‚wj
âˆ‚wi âˆ‚w1
âˆ‚wj âˆ‚w1
and

âˆ‚`
âˆ‚`
âˆ‚`
âˆ‚`
(u) =
(u)
(u) =
(u),
âˆ‚wi2
âˆ‚wj2
âˆ‚wi âˆ‚wj
âˆ‚ws âˆ‚wt

for i 6= j, s 6= t such that i, j, s, t 6= 1. It follows that we only need to consider second partial
derivatives with respect to 3 axes w1 ,w2 and w3 . Denote u = (âˆ’Î³(k), , 0, ..., 0) and wâˆ— =
2
Î²(k)
(1, 0, ..., 0) and Î²(k) = k Ï€âˆ’k and note that Î³(k) = Î²(k)+k
. Then by equation Eq. 2 we have
âˆ‡`(u )x âˆ’ âˆ‡`(u)x
âˆ‚`
(u) = lim
â†’0
âˆ‚w22

"
#

1
k


k + Î²(k)  âˆ’ Ï€ kwâˆ— k ku k sin Î¸u ,wâˆ— âˆ’ Î²(k) kwâˆ— k ku k
k2
= lim

(3)



â†’0

Î²(k) 
1
=0
= 2 k + Î²(k) âˆ’
k
Î³(k)
Furthermore,
âˆ‚`
âˆ‡`(u )y âˆ’ âˆ‡`(u)y
(u) = lim
â†’0
âˆ‚w1 âˆ‚w2

"

1
k2 âˆ’ k + Î²(k) Î³(k) +

#
k
Ï€

âˆ—

kw k

Î³(k)
ku k

âˆ—

sin Î¸u ,wâˆ— + Î²(k) kw k

= lim

Î³(k)
ku k

âˆ’

k
Ï€ (Ï€

âˆ’ Î¸u ,wâˆ— )



â†’0

(4)
where Î¸u ,wâˆ— = arccos( âˆš âˆ’Î³(k)
2
2

 +Î³ (k)

).

By Lâ€™Hopitalâ€™s rule we have
âˆ‚Î¸u ,wâˆ—
âˆ‚w2

Î³(k) cos Î¸u ,wâˆ—
âˆ‚`
Î³(k) sin Î¸u ,wâˆ—
(u) = lim âˆ’
+
3
â†’0
âˆ‚w1 âˆ‚w2
Ï€k ku k
Ï€kku k


1
âˆ‚Î¸u ,wâˆ— Î³(k) cos Î¸u ,wâˆ—
=
lim
+1
Ï€k â†’0 âˆ‚w2
ku k
5

âˆ’

Î²(k)Î³(k)
3

ku k

+

âˆ‚Î¸u ,wâˆ—
âˆ‚w2

Ï€k

(5)

Since

âˆ‚Î¸u ,wâˆ—
1
(u ) = âˆ’
âˆ‚w2
âˆš ||

Î³(k)
3

2 +Î³ 2 (k)

(2 + Î³ 2 (k)) 2

=âˆ’

Î³(k)
(2 + Î³ 2 (k))||

it follows that





 âˆ‚`

 âˆ‚Î¸u ,wâˆ—   Î³(k) cos Î¸u ,wâˆ—

1





lim 
+ 1

 âˆ‚w1 âˆ‚w2 (u) = Ï€k â†’0

âˆ‚w2
ku k




1
Î³(k) cos Î¸u ,wâˆ—

â‰¤
lim 
+ 1 = 0
Î³(k)Ï€k â†’0
ku k

(6)

and thus âˆ‚wâˆ‚`
(u) = 0.
1 âˆ‚w2
Taking derivatives of the gradient with respect to w1 is easier because the expressions in Eq. 2
w
that depend on Î¸w,wâˆ— and kwk
are constant. Therefore,
k + Î²(k)
âˆ‚`
(u) =
âˆ‚w12
k2
and

âˆ‚`
(u) = 0
âˆ‚w2 âˆ‚w1

Finally let uÌƒ = (0, âˆ’Î³(k), , 0, ..., 0) then it is easy to see that
âˆ‚`
âˆ‡`(uÌƒ )w2 âˆ’ âˆ‡`(u)w2
=0
(u) = lim
â†’0
âˆ‚w2 âˆ‚w3

.
Therefore, overall we see that âˆ‡2 `(u) is a diagonal matrix with zeros and
diagonal, which proves our claim.

C.2

k+Î²(k)
k2

> 0 on the

Proof of Theorem 5.2

For the following lemmas let wt+1 = wt âˆ’ Î»âˆ‡`(wt ), Î¸t be the angle between wt and wâˆ— (t â‰¥ 0) and
2
define Î»Ìƒ = Î±(k)Î» where Î±(k) = k1 + kÏ€kâˆ’k
2 . Note that Î±(k) â‰¤ 1 for all k â‰¥ 1 The following lemma
shows that for Î» < 1, the angle between wt and wâˆ— decreases in each iteration.
Lemma C.1. If 0 < Î¸t < Ï€ and Î» < 1 then Î¸t+1 < Î¸t .
Proof. This follows from the fact that adding
!
Î»
k 2 âˆ’ k k kwâˆ— k
k 2 âˆ’ k kwâˆ— k
âˆ’ 2 k+
âˆ’
sin Î¸t âˆ’
wt
k
Ï€
Ï€ kwt k
Ï€ kwt k
2

to wt does not change Î¸t for Î» < 1, since
decreases Î¸t .

k+ k Ï€âˆ’k
k2

â‰¤ 1 for k â‰¥ 1. In addition, adding

We will need the following two lemmas to establish a lower bound on kwt k.
Lemma C.2. If

Ï€
2

< Î¸t < Ï€ then kwt+1 k â‰¥

sin Î¸t
sin Î¸t+1

6

âˆ—

k sin Î¸t
min{kwt k , kwÎ±(k)Ï€
}.

Î»
Ï€k




Ï€ âˆ’ Î¸ wâˆ—

Proof. Let
!
Î»
k 2 âˆ’ k k kwâˆ— k
k 2 âˆ’ k kwâˆ— k
ut = w t âˆ’ 2 k +
âˆ’
sin Î¸t âˆ’
wt
k
Ï€
Ï€ kwt k
Ï€ kwt k
Notice that if kwt k â‰¤

kwâˆ— k sin Î¸t
Î±(k)Ï€

then

kut k = (1 âˆ’ Î»Ìƒ) kwt k +
(1 âˆ’ Î»Ìƒ) kwt k +

Î» kwâˆ— k
Î»(k 2 âˆ’ k) kwâˆ— k
â‰¥
sin Î¸t +
Ï€k
Ï€k 2

Î»(k 2 âˆ’ k) kwâˆ— k sin Î¸t
Î»k kwâˆ— k sin Î¸t
+
=
Ï€k 2
Ï€k 2

(1 âˆ’ Î»Ìƒ) kwt k +
âˆ—

Î»Ìƒ kwâˆ— k sin Î¸t
â‰¥ kwt k
Î±(k)Ï€
âˆ—

k sin Î¸t
k sin Î¸t
Similarly, if kwt k â‰¥ kwÎ±(k)Ï€
then kut k â‰¥ kwÎ±(k)Ï€
. Furthermore, by a simple geometric observaÏ€
tion we see that kwt+1 k cos(Î¸t+1 âˆ’ 2 ) = kut k cos(Î¸t âˆ’ Ï€2 ) if Î¸t+1 > Ï€2 and kwt+1 k cos( Ï€2 âˆ’ Î¸t+1 ) =
sin Î¸t
kut k cos(Î¸t âˆ’ Ï€2 ) if Î¸t+1 â‰¤ Ï€2 . This is equivalent to kwt+1 k = sin
Î¸t+1 kut k. It follows that kw t+1 k â‰¥
sin Î¸t
sin Î¸t+1

âˆ—

k sin Î¸t
min{kwt k , kwÎ±(k)Ï€
} as desired.

Lemma C.3. If 0 < Î¸t â‰¤

Ï€
2

1
2

and 0 < Î» <

âˆ—

then kwt+1 k â‰¥ min{kwt k , kw8 k }

Proof. First assume that k â‰¥ 2. Let ut be as in Lemma C.2, then
kut k â‰¥ (1 âˆ’ Î»Ìƒ) kwt k +
It follows that if kwt k â‰¥

(k2 âˆ’k)kwâˆ— k
Î±(k)Ï€k2

â‰¥

then kut k â‰¥ kwt k. Since wt+1 = ut +

kwâˆ— k
2Ï€
Î»
Ï€k Ï€

Î»Ìƒ(k 2 âˆ’ k) kwâˆ— k
Î±(k)Ï€k 2
âˆ—

âˆ—

2

k
(k âˆ’k)kw k
then kut k â‰¥ kw
2Ï€ . Otherwise if kw t k â‰¤
Î±(k)Ï€k2

Ï€
âˆ’ Î¸ wâˆ— and 0 < Î¸t â‰¤ 2 we have kwt+1 k â‰¥ kut k â‰¥

âˆ—

k
min{ kw
2Ï€ , kw t k}.
âˆ—
Now let k = 1. Note that in this case Î»Ìƒ = Î». First assume that Î¸t < Ï€3 . If kwt k â‰¥ kw4 k then,
âˆ—
âˆ—
Î»kw k sin Î¸t
using the samenotation
â‰¥ kw2t k â‰¥ kw8 k . Since
Ï€
 as in Lemma C.2, kut k â‰¥ (1 âˆ’ Î») kwt k +

wt+1 = ut +

Î»
Ï€

Ï€ âˆ’ Î¸t wâˆ— and 0 < Î¸t â‰¤

the facts 0 < Î¸t â‰¤

Ï€
2

and cos Î¸t >

1
2

Ï€
2

we have kwt+1 k â‰¥ kut k â‰¥

kwâˆ— k
8 .

If kwt k <

kwâˆ— k
4

then by

âˆš

âˆ—

we get

 
 
 
 
Î»

Î»
2
âˆ—


kwt+1 k = kut k + 2 kut k  Ï€ âˆ’ Î¸t w  cos Î¸t +  Ï€ âˆ’ Î¸t wâˆ— 
 â‰¥
Ï€
Ï€
2

2

2

(1 âˆ’ Î»)2 kwt k +

(1 âˆ’ Î»)Î»
Î»2
2
kwt k kwâˆ— k +
kwâˆ— k â‰¥
2
4

2

2

2

(1 âˆ’ Î»)2 kwt k + 2(1 âˆ’ Î»)Î» kwt k + 4Î»2 kwt k =
2

2

(1 + 3Î»2 ) kwt k â‰¥ kwt k

âˆ—

Finally, assume Î¸t â‰¥ Ï€3 . As in the proof of Lemma C.2, if kwt k â‰¥ kw kÏ€sin Î¸t â‰¥ 23 kwÏ€ k then
âˆš
âˆ—
âˆ—
kwt+1 k â‰¥ kut k â‰¥ 23 kwÏ€ k . Otherwise, if kwt k < kw kÏ€sin Î¸t then kwt+1 k â‰¥ kut k â‰¥ kwt k. This
concludes our proof.
We can now show that in each iteration kwt k is bounded away from 0 by a constant.

7

Proposition C.4. Assume GD is initialized at w0 such that Î¸0 6= Ï€ and runs for T iterations with
learning rate 0 < Î» < 12 . Then for all 0 â‰¤ t â‰¤ T ,
kwt k â‰¥ min{kw0 k sin Î¸0 ,

kwâˆ— k sin2 Î¸0 kwâˆ— k
,
}
Î±(k)Ï€
8

Proof. Let Î¸0 > Î¸1 > ... > Î¸T (by Lemma C.1). Let i be the last index such that Î¸i > Ï€2 (if such i
does not exist let i = âˆ’1). Since sin Î¸j > sin Î¸0 for all 0 â‰¤ j â‰¤ i, by applying Lemma C.2 at most
j + 1 times we have
kwâˆ— k sin2 Î¸0
}
kwj+1 k â‰¥ min{kw0 k sin Î¸0 ,
Î±(k)Ï€
for all 0 â‰¤ j â‰¤ i.
Finally, by Lemma C.3 and the fact that Î¸j â‰¤

Ï€
2

for all i < j â‰¤ T , we get

kwj k â‰¥ min{kwi+1 k ,

kwâˆ— k
}
8

for all i + 1 < j â‰¤ T , from which the claim follows.
The following lemma shows that âˆ‡` is Lipschitz continuous at points that are bounded away from
0.
Lemma C.5. Assume kw1 k , kw2 k â‰¥ M , w1 , w2 and wâˆ— are on the same two dimensional half-plane
defined by wâˆ— , then
kâˆ‡`(w1 ) âˆ’ âˆ‡`(w2 )k â‰¤ L kw1 âˆ’ w2 k
for L = 1 +

3kwâˆ— k
M .

Proof. Recall that by equality Eq. 1,

âˆ‚g
1
w
1 
(w, wâˆ— ) =
kwâˆ— k
sin Î¸w,wâˆ— +
Ï€ âˆ’ Î¸w,wâˆ— wâˆ—
âˆ‚w
2Ï€
kwk
2Ï€
Let Î¸1 and Î¸2 be the angles between w1 ,wâˆ— and w2 ,wâˆ— , respectively. By the inequality
for 0 â‰¤ x â‰¤ x0 < Ï€ and since

|Î¸1 âˆ’Î¸2 |
2

â‰¤

Ï€
2

x0 sin x
sin x0

â‰¥x

we have

2|
Ï€ sin |Î¸1 âˆ’Î¸
|Î¸1 âˆ’ Î¸2 |
2
â‰¤
2
2

Furthermore kw1 âˆ’ w2 k is minimized (for fixed angles Î¸1 and Î¸2 ) when kw1 k = kw2 k = M and is
2|
equal to 2M sin |Î¸1 âˆ’Î¸
. Thus, under our assumptions we have,
2
2|
Ï€ sin |Î¸1 âˆ’Î¸
|Î¸1 âˆ’ Î¸2 |
Ï€ kw1 âˆ’ w2 k
2
â‰¤
â‰¤
2
2
4M

Thus we get

 

 
 1
 kwâˆ— k
1 
âˆ—
âˆ—

Ï€
âˆ’
Î¸
w
âˆ’
Ï€
âˆ’
Î¸
w
1
2
 2Ï€
 â‰¤ 4M kw1 âˆ’ w2 k
2Ï€

For the first summand, we will first find the parameterization of a two dimensional vector of length
sin Î¸ where Î¸ is the angle between the vector and the positive x axis. Denote this vector by (a, b),
then the following holds
a2 + b2 = sin2 Î¸
and

b
= tan Î¸
a
8

The solution to these equations is (a, b) = ( sin22Î¸ , sin2 Î¸). Hence (here we use the fact that w1 ,w2 are
on the same half-plane)


 1

1
w1
w2
âˆ—
âˆ—


kw
sin
Î¸
âˆ’
kw
sin
Î¸
k
k
1
2 =
 2Ï€
kw1 k
2Ï€
kw2 k
r
2
sin 2Î¸1
1
sin 2Î¸2 2  2
âˆ—
kw k
âˆ’
+ sin Î¸1 âˆ’ sin2 Î¸2 â‰¤
2Ï€
2
2
p
1
kwâˆ— k (Î¸1 âˆ’ Î¸2 )2 + 4(Î¸1 âˆ’ Î¸2 )2 â‰¤
2Ï€
âˆš
âˆš
5
5 kwâˆ— k
âˆ— Ï€ kw 1 âˆ’ w 2 k
kw k
=
kw1 âˆ’ w2 k
Ï€
4M
4M
where the first inequality follows from the fact that | sin xâˆ’sin y| â‰¤ |xâˆ’y| and the second inequality
from previous results. In conclusion, we have
âˆš


 ( 5 + 1) kwâˆ— k
 âˆ‚g
âˆ‚g
âˆ—
âˆ— 

kw1 âˆ’ w2 k
 âˆ‚w (w1 , w ) âˆ’ âˆ‚w (w2 , w ) â‰¤
4M
w
Similarly, in order to show that the function f (w) = kwk
is Lipschitz continuous, we parameterize
the unit vector by (cos Î¸, sin Î¸) where Î¸ is the angle between the vector and the positive x axis. We
now obtain


p
 w1
w2 
2
2


 kw1 k âˆ’ kw2 k  = (cos Î¸1 âˆ’ cos Î¸2 ) + (sin Î¸1 âˆ’ sin Î¸2 ) â‰¤

p

2(Î¸1 âˆ’ Î¸2 )2 â‰¤

Ï€ kw1 âˆ’ w2 k
âˆš
2M

Now we can conclude that



1 k2 âˆ’ k 
âˆ‚g
âˆ‚g
2
âˆ—
âˆ— 

kâˆ‡`(w1 ) âˆ’ âˆ‡`(w2 )k â‰¤
+
(w1 , w ) âˆ’
(w2 , w ) +
kw1 âˆ’ w2 k + 
2
k
Ï€k
k âˆ‚w
âˆ‚w
âˆš


 (k 2 âˆ’ k) kwâˆ— k   w
2
2
 
âˆ’ k) kwâˆ— k ( 5 + 1) kwâˆ— k 
 1 âˆ’ w2  â‰¤ 1 + k âˆ’ k + (k âˆš
+
kw1 âˆ’ w2 k â‰¤


Ï€k 2
kw1 k kw2 k
k
Ï€k 2
2M k
2M k 2
âˆš
kwâˆ— k
( 5 + 1) kwâˆ— k
3 kwâˆ— k
1+ âˆš
+
â‰¤1+
2M
M
2M

Given that ` is Lipschitz continuous we can now follow standard optimization analysis (Nesterov
(2004)) to show that limtâ†’âˆž kâˆ‡`(wt )k = 0.
Proposition C.6. Assume GD is initialized at w0 such that Î¸0 6= Ï€ and runs with a constant learning
rate 0 < Î» < min{ L2 , 12 } where L = OÌƒ(1). Then for all T
T
X

2

kâˆ‡`(wt )k â‰¤

t=0

1
`(w0 )
Î»(1 âˆ’ Î»2 L)

Proof. We will need the following lemma
Lemma C.7. Let f : Rn â†’ R be a continuously differentiable function on a set D âŠ† Rn and x, y âˆˆ D
such that for all 0 â‰¤ Ï„ â‰¤ 1, x + Ï„ (y âˆ’ x) âˆˆ D and kâˆ‡f (x + Ï„ (y âˆ’ x)) âˆ’ âˆ‡f (x)k â‰¤ L kx âˆ’ yk. Then
we have
L
2
|f (y) âˆ’ f (x) âˆ’ hâˆ‡f (x), y âˆ’ xi| â‰¤ kx âˆ’ yk
2
9

Proof. The proof exactly follows the proof of Lemma 1.2.3 in Nesterov (2004) and note that the proof
only requires Lipschitz continuity of the gradient on the set S = {x + Ï„ (y âˆ’ x) | 0 â‰¤ Ï„ â‰¤ 1} and that
S âŠ† D.
By Proposition C.4, for all t, kwt k â‰¥ M 0 where
M 0 = min{kw0 k sin Î¸0 ,

kwâˆ— k sin2 Î¸0 kwâˆ— k
,
}
Î±(k)Ï€
8

. Furthermore, by a simple geometric observation we have
min
0â‰¤Ï„ â‰¤1,kw1 k,kw2 kâ‰¥M 0 ,arccos



kÏ„ w1 + (1 âˆ’ Ï„ )w2 k = M 0 cos



w1 Â·w2
kw1 kkw2 k

=Î¸

Î¸
2

.
It follows by Lemma C.5 that for any t and x1 , x2 âˆˆ St , {wt + Ï„ (wt+1 âˆ’ wt ) | 0 â‰¤ Ï„ â‰¤ 1},
kâˆ‡`(x1 ) âˆ’ âˆ‡`(x2 )k â‰¤ L kx1 âˆ’ x2 k
âˆ—

k
where L = 1 + 3kw
and M = M 0 cos Î¸20 (Note that cos Î¸t âˆ’Î¸2 t+1 â‰¥ cos Î¸20 for all t by Lemma C.1).
M
Hence by Lemma C.7, for any t we have

`(wt+1 ) â‰¤ `(wt ) + hâˆ‡`(wt ), wt+1 âˆ’ wt i +
`(wt ) âˆ’ Î»(1 âˆ’

L
2
kwt+1 âˆ’ wt k =
2

Î»
2
L) kâˆ‡`(wt )k
2

which implies that
T
X

2

kâˆ‡`(wt )k â‰¤

t=0



1
1
`(w
)
âˆ’
`(w
)
â‰¤
`(w0 )
0
T
Î»
Î»(1 âˆ’ 2 L)
Î»(1 âˆ’ Î»2 L)

We are now ready to prove the theorem.
Proof of Theorem 5.2. First, we observe that for a randomly initialized point w0 , 0 â‰¤ Î¸0 â‰¤ Ï€(1 âˆ’ Î´)
âˆ—
k
with probability 1âˆ’Î´. Hence by Proposition C.6 we have for L = 1+ 3kw
where M = min{sin(Ï€(1âˆ’
M
2

(Ï€(1âˆ’Î´)) 1
, 8 } cos( Ï€(1âˆ’Î´)
) and Î±(k) = k +
Î´)), sin Î±(k)Ï€
2
T
X
t=0

2

kâˆ‡`(wt )k â‰¤

k2 âˆ’k
Ï€ ,

and for Î» =

1
L

(we assume w.l.o.g. that L > 2),

4L k k 2 âˆ’ k 
1
`(w
)
=
2L`(w
)
â‰¤
+
0
0
k2 2
2Ï€
Î»(1 âˆ’ Î»2 L)

Therefore,
4L k
k2 2

2

min {kâˆ‡`(wt )k } â‰¤

0â‰¤tâ‰¤T

+
T

k2 âˆ’k
2Ï€



It follows that gradient descent reaches a point wt such that kâˆ‡`(wt )k <  after T iterations where

T >

4L k
k2 2

+

k2 âˆ’k
2Ï€

 2

2

âˆš
We will now show that if kâˆ‡`(wt )k <  then wt is O( )-close to the global minimum wâˆ— . First
note that if Ï€2 â‰¤ Î¸t â‰¤ Ï€(1âˆ’Î´) then a vector of the form v = Î±wâˆ— +Î²w where Î± â‰¥ 0 is of minimal norm
10

equal to Î± sin(Ï€ âˆ’ Î¸t ) kwâˆ— k when it is perpendicular to w. Since the gradient is a vector of this form,
âˆ—
k sin Ï€Î´
we have kâˆ‡`(wt )k > Ï€Î´kw Ï€k
â‰¥ Î´ sink Ï€Î´ â‰¥ . Hence, from now on we assume that 0 â‰¤ Î¸t < Ï€2 .
Similarly to the previous argument, we have
 > kâˆ‡`(wt )k >

kwâˆ— k (Ï€ âˆ’ Ï€2 ) sin Î¸t
sin Î¸t
â‰¥
Ï€k
2k

Hence, Î¸t < arcsin(2k) = O(). It follows by the triangle inequality that


!

k 2 âˆ’ k k kwâˆ— k
k 2 âˆ’ k kwâˆ— k
k(Ï€ âˆ’ Î¸t ) âˆ— 


2
2
k  > k kâˆ‡`(wt )k =  k +
âˆ’
sin Î¸t âˆ’
wt âˆ’
w â‰¥


Ï€
Ï€ kwt k
Ï€ kwt k
Ï€


!

 k kwâˆ— k
k 2 âˆ’ k k 2 âˆ’ k kwâˆ— k
kÎ¸t kwâˆ— k

âˆ—
âˆ’
wt âˆ’ kw  âˆ’
sin Î¸t âˆ’
â‰¥
 k+


Ï€
Ï€ kwt k
Ï€
Ï€


!

k 2 âˆ’ k k 2 âˆ’ k kwâˆ— k
k kwâˆ— k 


âˆ’
wt âˆ’
wt  âˆ’
 k+


Ï€
Ï€ kwt k
kwt k


 âˆ— k kwâˆ— k  k kwâˆ— k
kÎ¸t kwâˆ— k
kw âˆ’
wt 
âˆ’
sin Î¸t âˆ’
â‰¥


kwt k
Ï€
Ï€
k kwâˆ— k
kÎ¸t kwâˆ— k
k2 âˆ’ k 
| kwt k âˆ’ kwâˆ— k | âˆ’ k kwâˆ— k Î¸t âˆ’
sin Î¸t âˆ’
Ï€
Ï€
Ï€
where the last inequality follows since the arc of a circle is larger than its corresponding segment.
Therefore we get | kwt k âˆ’ kwâˆ— k | < O(). By the bounds on Î¸t and | kwt k âˆ’ kwâˆ— k | and the
inequality cos x â‰¥ 1 âˆ’ x for x â‰¥ 0, we can give an upper bound on kwt âˆ’ wâˆ— k:
k+

2

2

2

kwt âˆ’ wâˆ— k = kwt k âˆ’ 2 kwt k kwâˆ— k cos Î¸t + kwâˆ— k =
kwt k (kwt k âˆ’ kwâˆ— k cos Î¸t ) + kwâˆ— k (kwâˆ— k âˆ’ kwt k cos Î¸t ) â‰¤
(kwâˆ— k + O())(O() + Î¸t kwâˆ— k) + kwâˆ— k (O(2 ) + Î¸t kwâˆ— k) = O()
2

Finally, to prove the claim it suffices to show that `(w) â‰¤ dkw âˆ’ wâˆ— k . Denote the input vector
x = (x1 , x2 , ..., xk ) where xi âˆˆ Rm for all 1 â‰¤ i â‰¤ k. Then we get
h Pk

Pk
Ïƒ(wT xi )
Ïƒ(wâˆ— T xi ) i2
`(w) = Ex
âˆ’ i=1
k
k
h Pk |Ïƒ(wT x ) âˆ’ Ïƒ(wâˆ— T x )| i2
i
i
i=1
â‰¤ Ex
k
h Pk |wT x âˆ’ wâˆ— T x | i2
i
i
i=1
â‰¤ Ex
k
h Pk kw âˆ’ wâˆ— k kx k i2
i
i=1
â‰¤ Ex
k
2
2
â‰¤ kw âˆ’ wâˆ— k Ex kxk
i=1

(7)

2

= dkw âˆ’ wâˆ— k

where the second inequality follows from Lipschitz continuity of Ïƒ, the third inequality from the
2
Cauchy-Schwarz inequality and the last equality since kxk follows a chi-squared distribution with d
degrees of freedom.


11

D

Missing Proofs for Section 7.1

D.1

Proof of Proposition 7.1

Define wp = (w2 , w1 ), wâˆ—p1 = (0, âˆ’wâˆ— ) and wâˆ—p2 = (wâˆ— , 0). We first prove the following lemma.
Lemma D.1. Let l be defined as in Eq. 16. Then
"
1
k 2 âˆ’ 3k + 2 
2(k âˆ’ 1) sin Î¸wr ,wl
âˆ‡l(w) = 2 k +
w+
w
k
Ï€
Ï€
(k âˆ’ 1)(Ï€ âˆ’ Î¸wr ,wl )
(k 2 âˆ’ 3k + 2) kwâˆ— k
wp âˆ’
w
Ï€
Ï€ kwk
k kwâˆ— k sin Î¸w,wâˆ—
k(Ï€ âˆ’ Î¸w,wâˆ— ) âˆ—
âˆ’
wâˆ’
w
Ï€ kwk
Ï€
(k âˆ’ 1) sin Î¸wl ,wâˆ—r kwâˆ— k
(k âˆ’ 1)(Ï€ âˆ’ Î¸wl ,wâˆ—r ) âˆ—
âˆ’
wâˆ’
w p2
Ï€ kwk
Ï€
#
(k âˆ’ 1) sin Î¸wr ,wâˆ—l kwâˆ— k
(k âˆ’ 1)(Ï€ âˆ’ Î¸wr ,wâˆ—l ) âˆ—
âˆ’
wâˆ’
w p1
Ï€ kwk
Ï€
+

Proof. The gradient does not follow immediately from Lemma 3.2 because the loss has expressions
with of the function g but with different dependencies on the parameters in A. We will only calculate
âˆ‚g(wr ,wl )
, the other expressions are calculated in the same manner.
âˆ‚w
Recall that
1
2
g(wr , wl ) =
kwk (sin Î¸wr ,wl + (Ï€ âˆ’ Î¸wr ,wl ) cos Î¸wr ,wl )
2Ï€
It follows that
1
âˆ‚ cos Î¸wr ,wl
1
âˆ‚g(wr , wl )
2
= (sin Î¸wr ,wl + (Ï€ âˆ’ Î¸wr ,wl ) cos Î¸wr ,wl )w +
kwk (Ï€ âˆ’ Î¸wr ,wl )
âˆ‚w
Ï€
2Ï€
âˆ‚w
Let w = (w1 , w2 ) then cos Î¸wr ,wl =

w1 w2
.
w12 +w22

(8)

Then,

2w1 cos Î¸wr ,wl
âˆ‚ cos Î¸wr ,wl
w2
w2 (w12 + w22 ) âˆ’ 2w12 w2
âˆ’
=
=
2
2
2
2
2
âˆ‚w1
(w1 + w2 )
kwk
kwk
and

or equivalently

âˆ‚ cos Î¸wr ,wl
w1 (w12 + w22 ) âˆ’ 2w22 w1
w1
2w2 cos Î¸wr ,wl
=
=
âˆ’
2
2
2
2
2
âˆ‚w2
(w1 + w2 )
kwk
kwk
âˆ‚ cos Î¸wr ,wl
âˆ‚w

=

wp
kwk2

âˆ’

2w cos Î¸wr ,wl
kwk2

. It follows that

âˆ‚g(wr , wl )
sin Î¸wr ,wl w (Ï€ âˆ’ Î¸wl ,wr )
=
+
wp
âˆ‚w
Ï€
2Ï€

We will prove that wt+1 6= 0 and that it is in the interior of the fourth quadrant. Denote w = wt
and âˆ‡l(w) = k12 B1 (w) + B2 (w) + B3 (w) where
B1 (w) = k +

k 2 âˆ’ 3k + 2 
2(k âˆ’ 1) sin Î¸wr ,wl
w+
wâˆ’
Ï€
Ï€

(k âˆ’ 1) sin Î¸wr ,wâˆ—l kwâˆ— k
(k âˆ’ 1) sin Î¸wl ,wâˆ—r kwâˆ— k
(k 2 âˆ’ 3k + 2) kwâˆ— k
k kwâˆ— k sin Î¸w,wâˆ—
wâˆ’
wâˆ’
wâˆ’
w
Ï€ kwk
Ï€ kwk
Ï€ kwk
Ï€ kwk
12

B2 (w) =

(k âˆ’ 1)(Ï€ âˆ’ Î¸wr ,wl )
wp
Ï€

and

(k âˆ’ 1)(Ï€ âˆ’ Î¸wr ,wâˆ—l ) âˆ—
k(Ï€ âˆ’ Î¸w,wâˆ— ) âˆ— (k âˆ’ 1)(Ï€ âˆ’ Î¸wl ,wâˆ—r ) âˆ—
w âˆ’
w p2 âˆ’
w p1
Ï€
Ï€
Ï€
Let w = (w, âˆ’mw) for w, m â‰¥ 0. Straightforward calculation shows that cos Î¸wl ,wâˆ—r = âˆš
B3 (w) = âˆ’

and cos Î¸wr ,wâˆ—l = âˆš

m
.
2(m2 +1)

Hence

Ï€
4

â‰¤ Î¸wl ,wâˆ—r , Î¸wr ,wâˆ—l â‰¤

Ï€
2.

1
2(1+m2 )

Since w is in the fourth quadrant we

3Ï€
4

â‰¤ Î¸w,wâˆ— â‰¤ Ï€. Therefore, adding âˆ’Î»B3 (w) can only increase kwk. This follows since in
also have
the worst case (the least possible increase of kwk)
âˆ’B3 (w) =

kâˆ’1 âˆ—
kâˆ’2 âˆ— kâˆ’2 âˆ—
k âˆ— kâˆ’1 âˆ—
w +
w p2 +
w p1 = (
w ,âˆ’
w )
4
2
2
4
4

which is in the fourth quadrant for k â‰¥ 2. In addition, since âˆ’wp is in the fourth quadrant then
adding âˆ’Î»B2 (w) increases kwk.
âˆ—
k
If kwk < kw
16 then âˆ’B1 (w) points in the direction of w since in this case âˆ’B1 (w) = Î±w where
Î±â‰¥

 k 2 âˆ’ 3k + 2
Ï€

+

(k âˆ’ 1) k âˆ’ 1 k 2 âˆ’ 3k + 2
k
âˆ’
âˆ’
âˆ’
kwâˆ— k > 0
Ï€
8Ï€
16Ï€
16

for k â‰¥ 2. If âˆ’B1 (w) points in the direction of âˆ’w then by the assumption that Î» âˆˆ (0, 31 ) we have
kÎ»B1 (w)k < kwk. Thus we can conclude that wt+1 6= 0.
Now, let w = (w1 , w2 ), Î¸t be the angle between w = wt and the positive x axis and first assume
that w1 > âˆ’w2 . In this case âˆ’B3 (w) least increases (or even most decreases) Î¸t when
 2k âˆ’ 3
k
3(k âˆ’ 1) âˆ—
kâˆ’1 âˆ—
2 âˆ’ k âˆ—
âˆ’B3 (w) = wâˆ— +
wp2 +
w p1 =
wâˆ— ,
w
4
4
2
4
4
which is a vector in the fourth quadrant for k â‰¥ 2. Otherwise, âˆ’B3 (w) is a vector in the fourth
âˆ—
quadrant as well. Note that we used the facts Ï€4 â‰¤ Î¸wl ,wâˆ—r , Î¸wr ,wâˆ—l â‰¤ Ï€2 and 3Ï€
4 â‰¤ Î¸w,w â‰¤ Ï€. Since
âˆ’Î»B1 (w) does not change Î¸t and âˆ’Î»B2 (w) increases Î¸t but never to an angle greater than or equal
to Ï€2 , it follows that 0 < Î¸t+1 < Ï€2 .
If w1 â‰¤ âˆ’w2 then by defining all angles with respect to the negative y axis, we get the same
argument as before. This shows that wt+1 is in the interior of the fourth quadrant, which concludes
our proof.

D.2

Proof of Proposition 7.2

We will need the following auxiliary lemmas.
Lemma D.2. Let w be in the fourth quadrant, then g(wl , wr ) â‰¥

1
2Ï€

âˆš

3
2

âˆ’

Ï€
6



2

kwk .

Proof. First note that the function s(Î¸) = sin Î¸ + (Ï€ âˆ’ Î¸) cos Î¸ is decreasing as a function of Î¸ âˆˆ [0, Ï€].
Let w = (w, âˆ’mw) for w, m â‰¥ 0. Straightforward calculation shows that cos Î¸wl ,wr = âˆ’ mm
2 +1 . As a
1
function of m âˆˆ [0, âˆž), cos Î¸wl ,wr is minimized for m = 1 with value âˆ’ 2 , i.e., when Î¸(wl , wr ) = 2Ï€
3
âˆš


2
2
3
1
Ï€
1
and this is the largest angle possible. Thus g(wl , wr ) â‰¥ 2Ï€
s( 2Ï€
3 ) kwk = 2Ï€
2 âˆ’ 6 kwk .
Lemma D.3. Let


3Ï€
Ï€
3Ï€
+ Î¸) + ( âˆ’ Î¸) cos(
+ Î¸) +
4
4
4
s
s
2


cos Î¸
cos Î¸ cos Î¸ 
sin Î¸2
sin Î¸ sin Î¸ 
2k âˆ’ 2
1âˆ’
+ (Ï€ âˆ’ arccos âˆš ) âˆš
+ 2k âˆ’ 2
1âˆ’
+ (Ï€ âˆ’ arccos âˆš ) âˆš
2
2
2
2
2
2
Ï€
Ï€
, then in the interval Î¸ âˆˆ [0, 4 ], f (Î¸) is maximized at Î¸ = 4 for all k â‰¥ 2.
f (Î¸) = 2k sin(

13

f (Î¸)
k
Proof. We will maximize the function 2(kâˆ’1)
= kâˆ’1
f1 (Î¸) + f2 (Î¸) + f3 (Î¸) where f1 (Î¸), f2 (Î¸), f3 (Î¸)
correspond to the three
summands
in
the
expression
of
f (Î¸).
âˆš
Since for h(x) = 1 âˆ’ x2 + (Ï€ âˆ’ arccos(x))x we have h0 (x) = Ï€ âˆ’ arccos(x), it follows that
âˆš Î¸ ) sin
âˆš Î¸ , f 0 (Î¸) = (Ï€ âˆ’ arccos sin
âˆš Î¸ ) cos
âˆš Î¸ and f 0 (Î¸) = âˆ’( Ï€ âˆ’ Î¸) sin( 3Ï€ + Î¸). It
f20 (Î¸) = âˆ’(Ï€ âˆ’ arccos cos
3
1
4
4
2
2
2
2
therefore suffices to show that

sin Î¸ cos Î¸
cos Î¸ sin Î¸
k
Ï€
3Ï€
d1 (Î¸) := (Ï€ âˆ’ arccos âˆš ) âˆš âˆ’ (Ï€ âˆ’ arccos âˆš ) âˆš âˆ’
( âˆ’ Î¸) sin(
+ Î¸) â‰¥ 0
kâˆ’1 4
4
2
2
2
2
for Î¸ âˆˆ [0, Ï€4 ].
By applying the inequalities arccos(x) â‰¤
x âˆˆ [ 21 , âˆš12 ] we get d1 (Î¸) â‰¥ d2 (Î¸) where
d2 (Î¸) =

Ï€
2

âˆ’ x for x âˆˆ [0, 1] and arccos(x) â‰¥

Ï€
2

âˆ’xâˆ’

1
10

for

1  sin Î¸
k
Ï€ cos Î¸
Ï€
3Ï€
Ï€ sin Î¸  cos Î¸
âˆš âˆ’
âˆš âˆ’
+ âˆš
+ âˆš +
( âˆ’ Î¸) sin(
+ Î¸) =
2
2
10
kâˆ’1 4
4
2
2
2
2
Ï€
1 
k
Ï€
3Ï€
Ï€
âˆš cos Î¸ âˆ’ âˆš + âˆš sin Î¸ âˆ’
( âˆ’ Î¸) sin(
+ Î¸)
kâˆ’1 4
4
2 2
2 2 10 2

We notice that d2 (0) â‰¥ 0 and d2 ( 34 ) â‰¥ 0 for all k â‰¥ 2. In addition,
Ï€
1 
k
Ï€
3Ï€
k
Ï€
3Ï€
d02 (Î¸) = âˆ’ âˆš sin Î¸ âˆ’ âˆš + âˆš cos Î¸ +
sin(
+ Î¸) âˆ’
( âˆ’ Î¸) cos(
+ Î¸)
kâˆ’1
4
kâˆ’1 4
4
2 2
2 2 10 2
and d02 (0) > 0 for all k â‰¥ 2. It follows that in order to show that d2 (Î¸) â‰¥ 0 for Î¸ âˆˆ [0, 43 ] and k â‰¥ 2, it
suffices to show that d002 (Î¸) â‰¤ 0 for Î¸ âˆˆ [0, 43 ] and k â‰¥ 2. Indeed,
Ï€
1 
2k
3Ï€
k
Ï€
3Ï€
Ï€
cos(
+ Î¸) +
( âˆ’ Î¸) sin(
+ Î¸) â‰¤
d002 (Î¸) = âˆ’ âˆš cos Î¸ + âˆš + âˆš sin Î¸ +
kâˆ’1
4
kâˆ’1 4
4
2 2
2 2 10 2
1
3Ï€
2k
3Ï€
k Ï€
âˆš +
max{sin Î¸, sin(
+ Î¸)} +
cos(
+ Î¸) â‰¤ 0
k
âˆ’
1
4
4
k
âˆ’
1
4
10 2
for all Î¸ âˆˆ [0, 34 ] and k â‰¥ 2. Note that the first inequality follows since cos Î¸ â‰¥ sin Î¸ and the second
3Ï€
3
3
since cos( 3Ï€
4 +Î¸) â‰¥ max{sin Î¸, sin( 4 +Î¸)}, both for Î¸ âˆˆ [0, 4 ]. This shows that d1 (Î¸) â‰¥ 0 for Î¸ âˆˆ [0, 4 ].
3 Ï€
3
Ï€
0
Now assume that Î¸ âˆˆ [ 4 , 4 ]. Since d1 ( 4 ) â‰¥ 0 and d1 ( 4 ) â‰¥ 0, it suffices to prove that d1 (Î¸) â‰¤ 0 for
Î¸ âˆˆ [ 43 , Ï€4 ]. Indeed, for all Î¸ âˆˆ [ 34 , Ï€4 ]
cos Î¸ cos Î¸
sin Î¸ sin Î¸
d01 (Î¸) = âˆ’(Ï€ âˆ’ arccos âˆš ) âˆš âˆ’ (Ï€ âˆ’ arccos âˆš ) âˆš +
2
2
2
2
cos2 Î¸
sin2 Î¸
k
3Ï€
k
Ï€
3Ï€
q
+ q
+
sin(
+ Î¸) âˆ’
( âˆ’ Î¸) cos(
+ Î¸) â‰¤
k
âˆ’
1
4
k
âˆ’
1
4
4
cos2 Î¸
sin2 Î¸
2 1âˆ’ 2
2 1âˆ’ 2
âˆ’(Ï€ âˆ’ arccos

cos( Ï€4 ) cos( Ï€4 )
sin( 3 ) sin( 3 )
âˆš ) âˆš
âˆ’ (Ï€ âˆ’ arccos âˆš 4 ) âˆš 4 +
2
2
2
2

cos2 ( 43 )
sin2 ( Ï€4 )
3Ï€ 3
Ï€ 3
3Ï€ 3
q
q
+
+ 2 sin(
+ ) âˆ’ 2( âˆ’ ) cos(
+ )<0
2( 3 )
4
4
4
4
4
4
sin2 ( Ï€
)
cos
2 1âˆ’ 2 4
2 1âˆ’ 2 4
We conclude that d1 (Î¸) â‰¥ 0 for all Î¸ âˆˆ [0, Ï€4 ] as desired.

14

Proof of Proposition 7.2. First assume that w1 â‰¥ âˆ’w2 . Let Î¸ be the angle between w and the
w1
w2
positive x axis. Then cos Î¸ = kwk
. Therefore we get
and tan Î¸ = âˆ’ w
1
cos Î¸wl ,wâˆ—r =
and
cos Î¸wr ,wâˆ—l =

cos Î¸
w1
âˆš = âˆš
kwk 2
2

cos Î¸ tan Î¸
sin Î¸
âˆ’w2
âˆš =
âˆš
= âˆš
kwk 2
2
2

We can rewrite `(w) as
"
1 k 2 âˆ’ 3k + 2
k
2
`(w) = 2
(kwk âˆ’ kwâˆ— k)2 + kwk + 2(k âˆ’ 1)g(wr , wl )âˆ’
k
2Ï€
2

3Ï€
Ï€
3Ï€
kwk kwâˆ— k 
2k sin(
+ Î¸) + ( âˆ’ Î¸) cos(
+ Î¸) +
2Ï€
4
4
4
s
s


cos Î¸ cos Î¸ 
sin Î¸ sin Î¸ 
cos Î¸2
sin Î¸2
2k âˆ’ 2
+ (Ï€ âˆ’ arccos âˆš ) âˆš
+ 2k âˆ’ 2
+ (Ï€ âˆ’ arccos âˆš ) âˆš
1âˆ’
1âˆ’
+
2
2
2
2
2
2
#
k
âˆ— 2
âˆ—
âˆ—
kw k + 2(k âˆ’ 1)g(wr , wl )
2
Hence by Lemma D.2 and Lemma D.3 we can lower bound `(w) as follows
"
âˆš
1 k 2 âˆ’ 3k + 2
k
kâˆ’1
3 Ï€
2
2
âˆ— 2
`(w) â‰¥ 2
(kwk âˆ’ kw k) + kwk +
âˆ’
kwk âˆ’
k
2Ï€
2
Ï€
2
6
2Ï€  k
kâˆ’1
(k âˆ’ 1) kwk kwâˆ— k âˆš
2
3+
+ kwâˆ— k +
Ï€
3
2
Ï€

âˆš

3 Ï€ âˆ— 2
âˆ’
kw k
2
6

#

By setting kwk = Î± kwâˆ— k we get
"
âˆš
`(w)
1 k 2 âˆ’ 3k + 2
k 2 kâˆ’1
3 Ï€ 2
2
(Î± âˆ’ 1) + Î± +
âˆ’
Î± âˆ’
2 â‰¥ k2
âˆ—
2Ï€
2
Ï€
2
6
kw k
(k âˆ’ 1) âˆš
k kâˆ’1
2Ï€ 
Î±+ +
3+
Ï€
3
2
Ï€

âˆš

3 Ï€
âˆ’
2
6

#

Solving for Î± that minimizes the latter expression we obtain
âˆš

k2 âˆ’3k+2
+ (kâˆ’1)
3 + 2Ï€
h(k)
âˆ—
Ï€
Ï€
3
âˆš
Î± =
=
2(kâˆ’1)
3
k2 âˆ’3k+2
Ï€
h(k) + 1
k+
+ Ï€ ( 2 âˆ’6
Ï€
Plugging Î±âˆ— back to the inequality we get
`(w) â‰¥

1  h(k) + 1 âˆ— 2
h(k) + 1  âˆ— 2
2h(k) + 1
2
(Î± ) âˆ’ h(k)Î±âˆ— +
kw k = 2
kwâˆ— k
2
k
2
2
k (2h(k) + 2)

and for wÌƒ = âˆ’Î±âˆ— wâˆ— it holds that `(wÌƒ) =

2h(k)+1
âˆ— 2
k2 (2h(k)+2) kw k .

15

Finally, assume w1 â‰¤ âˆ’w2 . In this case, let Î¸ be the angle between w and the negative y axis.
w1
2
Then cos Î¸ = âˆ’w
kwk and tan Î¸ = âˆ’ w2 . Therefore
cos Î¸wl ,wâˆ—r =

w1
cos Î¸ tan Î¸
sin Î¸
âˆš =
âˆš
= âˆš
kwk 2
2
2

and
cos Î¸wr ,wâˆ—l =

âˆ’w2
cos Î¸
âˆš = âˆš
kwk 2
2

Notice that from now on we get the same analysis as in the case where w1 â‰¥ âˆ’w2 , where we switch
between expressions with wl , wâˆ—r and expressions with wr , wâˆ—l . This concludes our proof.


E

Uniqueness of Global Minimum in the Population Risk

Without loss of generality we assume that the filter is of size 2 and the stride is 1. The proof of the
âˆ—
âˆ—
âˆ—
general case follows the same lines. Assume that
 `(w) = 0 and denote
P w = (w1 , w2 ), w = (w1 , w2 ).
1
âˆ— 2
Recall that `(w) = EG (f (x; W ) âˆ’ f (x; W )) where f (x; W ) = k i Ïƒ (wi Â· x) and for all 1 â‰¤ i â‰¤ k
wi = (0iâˆ’1 , w, 0dâˆ’iâˆ’1 ). By equating `(w) to 0 we get that (f (x; W ) âˆ’ f (x; W âˆ— ))2 = 0 almost surely.
Since (f (x; W ) âˆ’ f (x; W âˆ— ))2 is a continuous function it follows that f (x; W ) âˆ’ f (x; W âˆ— ) = 0 for all
x. In particular this is true for x1 = (x, 0, 0, ..., 0), x âˆˆ R. Thus Ïƒ (xw1 ) = Ïƒ (xw1âˆ— ) for all x âˆˆ R
which implies that w1 = w1âˆ— . The equality holds also for x2 = (0, x, 0, ..., 0), x âˆˆ R which implies that
Ïƒ (xw2 ) + Ïƒ (xw1 ) = Ïƒ (xw2âˆ— ) + Ïƒ (xw1âˆ— ) for all x âˆˆ R. By the previous result, we get Ïƒ (xw2 ) = Ïƒ (xw2âˆ— )
for all x âˆˆ R and thus w2 = w2âˆ— . We proved that w = wâˆ— and therefore wâˆ— is the unique global
minimum.

F

Experimental Setup for Section 7.2

In our experiments we estimated the probability of convergence to the global minimum of a randomly
initialized gradient descent for many different ground truths wâˆ— of a convolutional neural network
with overlapping filters. For each value of number of hidden neurons, filter size, stride length and
ground truth distribution we randomly selected 30 different ground truths wâˆ— with respect to the
given distribution. We tested with all combinations of values given in Table 1.
Furthermore, for each combination of values of number of hidden neurons, filter size and stride
length we tested with deterministic ground truths: ground truth with all entries equal to 1, all entries
equal to -1 and with entries that form an increasing sequence from -1 to 1, -2 to 0 and 0 to 2 or
decreasing sequence from 1 to -1, 0 to -2 and 2 to 0.
For each ground truth, we ran gradient descent 20 times and for each run we recorded whether it
reached a point very close to the unique global minimum or it repeatedly (5000 consecutive iterations)
incurred very low gradient values and stayed away from the global minimum. We then calculated the
empirical probability pÌ‚ = #times reached20global minimum . To compute the one-sided confidence interval
we used the Wilson method (Brown et al. (2001)) which gives a lower bound
q
2
z2
zÎ±
pÌ‚)
+ 4kÎ±2
pÌ‚ + 2n + zÎ± pÌ‚(1âˆ’
n
(9)
z2
1 + nÎ±
where zÎ± is the Z-score with Î± = 0.05 and in our experiments n = 20. Note that we initialized gradient
descent inside a large hypercube such that outside the hypercube the gradient does not vanish (this
can be easily proved after writing out the gradient for each setting).
For all ground truths we got pÌ‚ â‰¥ 0.15, i.e., for each ground truth we reached the global minimum
1
at least 3 times. Hence the confidence interval lower bound Eq. 9 is greater than 17
in all settings.
16

Table 1: Parameters values for experiments in Section 7.2
Number of hidden neurons
Filter size
stride length

Ground truth distribution

50,100
2,8,16
1,min{ f4 , 1}, min{ f2 , 1} where f is the filter size
(For instance, for f = 16 we used strides 1,4,8
and for f = 2 we used stride 1)
The entries of the ground truth are i.i.d.
uniform random variables over the interval [a, b]
where (a, b) âˆˆ {(âˆ’1, 1), (âˆ’2, 0), (0, 2)}

This suggests that with a few dozen repeated runs of a randomly initialized gradient descent, with
high probability it will converge to the global minimum.

References
Brown, Lawrence D, Cai, T Tony, and DasGupta, Anirban. Interval estimation for a binomial proportion. Statistical science, pp. 101â€“117, 2001.
Nesterov, Yurii. Introductory lectures on convex optimization. pp. 22â€“29, 2004.

17

