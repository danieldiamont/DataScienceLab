Behavior Policy Gradient Supplemental Material

Josiah P. Hanna 1 Philip S. Thomas 2 3 Peter Stone 1 Scott Niekum 1

A. Proof of Theorem 1
In Appendix A, we give the full derivation of our primary theoretical contribution — the importance-sampling (IS) variance
gradient. We also present the variance gradient for the doubly-robust (DR) estimator.
We first derive an analytic expression for the gradient of the variance of an arbitrary, unbiased off-policy policy evaluation estimator, OPE(H, θ). Importance-sampling is one such off-policy policy evaluation estimator. From our general
derivation we derive the gradient of the variance of the IS estimator and then extend to the DR estimator.
A.1. Variance Gradient of an Unbiased Off-Policy Policy Evaluation Method
We first present a lemma from which

∂
∂θ

MSE[IS(H, θ)] and

∂
∂θ

MSE[DR(H, θ)] can both be derived.

Lemma 1 gives the gradient of the mean squared error (MSE) of an unbiased off-policy policy evaluation method.
Lemma 1.

#
"
L

X
∂
∂
∂
2
2
MSE[OPE(H, θ)] = E OPE(H, θ) (
log πθ (At |St )) +
OPE(H, θ) H ∼ πθ

∂θ
∂θ
∂θ
t=0

Proof. We begin by decomposing Pr(H|π) into two components—one that depends on π and the other that does not. Let

wπ (H) :=

L
Y

π(At |St ),

t=0

and
p(H) := Pr(H|π)/wπ (H),
for any π such that H ∈ supp(π) (any such π will result in the same value of p(H)). These two definitions mean that
Pr(H|π) = p(H)wπ (H).
The MSE of the OPE estimator is given by:
2

MSE[OPE(H, θ)] = Var[OPE(H, θ)] + (E[OPE(H, θ)] − ρ(πe )) .
|
{z
}
bias2

Since the OPE estimator is unbiased, i.e., E[OPE(H, θ)] = ρ(πe ), the second term is zero and so:
MSE(OPE(H, θ)) = Var(OPE(H, θ))



=E OPE(H, θ)2 H ∼ πθ − E[OPE(H, θ)|H ∼ πθ ]2



=E OPE(H, θ)2 H ∼ πθ − ρ(πe )2

Behavior Policy Gradient: Supplemental Material

To obtain the MSE gradient, we differentiate MSE(OPE(H, θ)) with respect to θ:



∂
∂  
MSE[OPE(H, θ)] =
E OPE(H, θ)2 H ∼ πθ − ρ(πe )2
∂θ
∂θ


∂
= EH∼πθ OPE(H, θ)2
∂θ
∂ X
Pr(H|θ) OPE(H, θ)2
=
∂θ
H
X
∂
∂
=
Pr(H|θ)
OPE(H, θ)2 + OPE(H, θ)2
Pr(H|θ)
∂θ
∂θ
H
X
∂
∂
=
Pr(H|θ)
OPE(H, θ)2 + OPE(H, θ)2 p(H) wπθ (H)
∂θ
∂θ

(1)

H

Consider the last factor of the last term in more detail:
L
∂
∂ Y
πθ (At |St )
wπθ (H) =
∂θ
∂θ t=0
! L
L
X
(a) Y
=
πθ (At |St )
t=0

=wπθ (H)

t=0

∂
∂θ πθ (At |St )

!

πθ (At |St )

L
X
∂
log (πθ (At |St )) ,
∂θ
t=0

where (a) comes from the multi-factor product rule. Continuing from (1) we have that:

"
#
L

X
∂
∂
∂

MSE(OPE(H, θ)) =E OPE(H, θ)2
log (πθ (At |St )) +
OPE(H, θ)2 H ∼ πθ .

∂θ
∂θ
∂θ
t=0

A.2. Behavior Policy Gradient Theorem
We now use Lemma 1 to prove the Behavior Policy Gradient Theorem which is our main theoretical contribution.
Theorem 1.

"
#
L

X
∂
∂

2
MSE[IS(H, θ)] = E − IS(H, θ)
log πθ (At |St )H ∼ πθ

∂θ
∂θ
t=0
where the expectation is taken over H ∼ πθ .

Proof. We first derive
Lemma 1.

∂
∂θ

IS(H, θ)2 . Theorem 1 then follows directly from using

∂
∂θ

IS(H, θ)2 as

∂
∂θ

OPE(H, θ)2 in

Behavior Policy Gradient: Supplemental Material

2
wπe
g(H)
wθ

2
∂
∂ wπe (H)
2
IS(H, θ) =
g(H)
∂θ
∂θ wθ (H)


wπe (H) ∂
wπe (H)
=2 · g(H)
g(H)
wθ (H) ∂θ
wθ (H)

 L
wπe (H)
wπe (H) X ∂
(a)
= − 2 · g(H)
g(H)
log πθ (At |St )
wθ (H)
wθ (H) t=0 ∂θ
IS(H, θ)2 =



= − 2 IS(H, θ)2

L
X
∂
log πθ (At |St )
∂θ
t=0

where (a) comes from the multi-factor product rule and using the likelihood-ratio trick (i.e.,

∂
∂θ πθ (A|S)

πθ (A|S)

= log πθ (A|S))

Substituting this expression into Lemma 1 completes the proof:

"
#
L

X
∂
∂

2
MSE[IS(H, θ)] = E − IS(H, θ)
log πθ (At |St )H ∼ πθ

∂θ
∂θ
t=0

A.3. Doubly Robust Estimator
Our final theoretical result is a corollary to the Behavior Policy Gradient Theorem: an extension of the IS variance gradient
to the Doubly Robust (DR) estimator. Recall that for a single trajectory DR is given as:
DR(H, θ) := v̂ πe (S0 ) +

L
X

γt

t=0

wπe ,t
(Rt − q̂ πe (St , At ) + v̂ πe (St+1 ))
wθ,t

function of πe under an approximate model, q̂ πe is the action-value function of πe under the
where v̂ πe is the state-value
Qt
model, and wπ,t := j=0 π(Aj |Sj ).
The gradient of the mean squared error of the DR estimator is given by the following corollary to the Behavior Policy
Gradient Theorem:
Corollary 1.
∂
MSE [DR(H, θ)]
∂θ

=

E[(DR(H, θ)2

t
L
L
X
X
∂
wπ ,t X ∂
log πθ (At |St ) − 2 DR(H, θ)(
γ t δt e
log πθ (Ai |Si ))]
∂θ
wθ,t i=0 ∂θ
t=0
t=0

where δt = Rt − q̂(St , At ) + v̂(St+1 ) and the expectation is taken over H ∼ πθ .
Proof. As with Theorem 1, we first derive
∂
2
∂θ OPE(H, θ) in Lemma 1.

2

DR(H, θ) =

πe

∂
∂θ

DR(H, θ)2 . Corollary 1 then follows directly from using

v̂ (S0 ) +

L
X
t=0

γ

t wπe ,t

wθ,t

!2
πe

πe

(Rt − q̂ (St , At ) + v̂ (St+1 ))

∂
∂θ

DR(H, θ)2 as

Behavior Policy Gradient: Supplemental Material

∂
∂
DR(H, θ)2 =
∂θ
∂θ

πe

v̂ (S0 ) +

L
X

γ

t wπe ,t

wθ,t

t=0

∂
=2 DR(H, θ)
∂θ
= − 2 DR(H, θ)(

πe

v̂ (S0 ) +

!2
πe

(Rt − q̂ (St , At ) + v̂ (St+1 ))
L
X

γ

t wπe ,t

wθ,t

t=0
L
X

γt

t=0

πe

!
πe

πe

(Rt − q̂ (St , At ) + v̂ (St+1 ))

t
X
wπe ,t
∂
(Rt − q̂ πe (St , At ) + v̂ πe (St+1 ))
log πθ (Ai |Si ))
wθ,t
∂θ
i=0

Thus the DR(H, θ) gradient is:

#
t
L
L

X
X
X
∂
∂

πe
πe
t wπe ,t
= E DR(H, θ)
log πθ (At |St ) − 2 DR(H, θ)(
(Rt − q̂ (St , At ) + v̂ (St+1 ))
log πθ (Ai |Si ))H ∼ πθ
γ

∂θ
wθ,t
∂θ
t=0
t=0
i=0
"

2

The expression for the DR behavior policy gradient is more complex than the expression for the IS behavior policy gradient.
Lowering the variance of DR involves accounting for the covariance of the sum of terms. Intuitively, accounting for the
covariance increases the complexity of the expression for the gradient.

B. BPG’s Off-Policy Estimates are Unbiased
This appendix proves that the estimate of BPG is an unbiased estimate of ρ(πe ). If only trajectories from a single θ i were
used then clearly IS(·, θ i ) is an unbiased estimate of ρ(πe ). The difficulty is that the BPG’s estimate at iteration n depends
on all θ i for i = 1 . . . n and each θ i is not independent of the others. Nevertheless, we prove here that BPG produces
an unbiased estimate of ρ(πe ) at each iteration. Specifically, we will show that E [IS(Hn , θ n |θ 0 = θ e )] is an unbiased
estimate of ρ(πe ), where the IS estimate is conditioned on θ 0 = θ e . To make the dependence of θ i on θ i−1 explicit, we
will write f (Hi−1 ) := θ i where Hi−1 ∼ πθi−1 . Notice that, even though BPG’s off-policy estimates are unbiased, they
are not statistically independent. This means that concentration inequalities, like Hoeffding’s inequality, cannot be applied
directly. We conjecture that the conditional independence properties of BPG (specifically that Hi is independent of Hi−1
given θi ), are sufficient for Hoeffding’s inequality to be applicable.

E [IS(Hn , θ n |θ = θ e )] =

X

Pr(h0 |θ 0 )

h0

X

Pr(h1 |f (h0 )) · · ·

h1

X

Pr(hn |f (hn−1 )) IS(hn )

hn

|
=ρ(πe )

X

Pr(h0 |θ 0 )

h0

X

{z

ρ(πe )

}

Pr(h1 |f (h0 )) · · ·

h1

=ρ(πe )

C. Supplemental Experiment Description
This appendix contains experimental details in addition to the details contained in Section 5 of the paper.
Gridworld: This domain is a 4x4 Gridworld with a terminal state with reward 10 at (3, 3), a state with reward −10 at
(1, 1), a state with reward 1 at (1, 3), and all other states having reward −1. The action set contains the four cardinal directions and actions move the agent in its intended direction (except when moving into a wall which produces no movement).
The agent begins in (0,0), γ = 1, and L = 100. All policies use softmax action selection with temperature 1 where the
probability of taking an action a in a state s is given by:
eθsa
θsa0
a0 e

π(a|s) = P

Behavior Policy Gradient: Supplemental Material

We obtain two evaluation policies by applying REINFORCE to this task, starting from a policy that selects actions uniformly at random. We then select one evaluation policy from the early stages of learning – an improved policy but still far
from converged –, π1 , and one after learning has converged, π2 . We run our set of experiments once with πe := π1 and a
second time with πe := π2 . The ground truth value of ρ(πe ) is computed with value iteration for both πe .
Stochastic Gridworld: The layout of this Gridworld is identical to the deterministic Gridworld except the terminal state
is at (9, 9) and the +1 reward state is at (1, 9). When the agent moves, it moves in its intended direction with probability
0.9, otherwise it goes left or right with equal probability. Noise in the environment increases the difficulty of building an
accurate model from trajectories.
Continuous Control: We evaluate BPG on two continuous control tasks: Cart-pole Swing Up and Acrobot. Both tasks
are implemented within RLLAB (Duan et al., 2016) (full details of the tasks are given in Appendix 1.1). The single task
modification we make is that in Cart-pole Swing Up, when a trajectory terminates due to moving out of bounds we give
a penalty of −1000. This modification increases the variance of πe . We use γ = 1 and L = 50. Policies are represented
as conditional Gaussians with mean determined by a neural network with two hidden layers of 32 tanh units each and
a state-independent diagonal covariance matrix. In Cart-pole Swing Up, πe was learned with 10 iterations of the TRPO
algorithm (Schulman et al., 2015) applied to a randomly initialized policy. In Acrobot, πe was learned with 60 iterations.
The ground truth value of ρ(πe ) in both domains is computed with 1,000,000 Monte Carlo roll-outs.
Domain Independent Details In all experiments
a constant
control variate (or baseline) in the gradient

 we subtract

estimate from Theorem 1. The baseline is bi = E − IS(H)2 H ∼ θ i−1 and our new gradient estimate is:

#
L

X
∂

log πθ (At |St )H ∼ πθ
E (− IS −bi )

∂θ
t=0
"

2

hP
i
L
∂
Adding or subtracting a constant does not change the gradient in expectation since bi · E
t=0 ∂θ log πθ (At |St ) = 0.
BPG with a baseline has lower variance so that the estimated gradient is closer in direction to the true gradient.
We use batch sizes of 100 trajectories per iteration for Gridworld experiments and size 500 for the continuous control tasks.
The step-size parameter was determined by a sweep over [10−2 , 10−6 ]
Early Stopping Criterion In all experiments we run BPG for a fixed number of iterations. In general, BPS can
continue for a fixed number of iterations or until the variance of the IS estimator stops decreasing. The true variance
is unknown but can be estimated by sampling a set of k trajectories with θ i and computing the uncentered variance:
Pk
1
2
j=0 OPE(Hj , θ j ) . This measure can be used to empirically evaluate the quality of each θ or determine when a BPS
k
algorithm should terminate behavior policy improvement.

References
Duan, Yan, Chen, Xi, Houthooft, Rein, Schulman, John, and Abbeel, Pieter. Benchmarking deep reinforcement learning
for continuous control. In In Proceedings of the 33rd International Conference on Machine Learning, 2016.
Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael, and Abbeel, Pieter. Trust region policy optimization.
In International Conference on Machine Learning, ICML, 2015.

