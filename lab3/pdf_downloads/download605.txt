Supplementary material: Multilabel Classification with Group Testing and Codes

A. Constructions
We present some additional group testing constructions in
this section.
A.1. Random Constructions-Proofs
Proposition. (Random Construction Prop. 1.) An m × d
random binary {0, 1} matrix A where each entry is 1 with
1
, is (k, 3k log d)-disjunct with very
probability ρ = k+1
high probability, if m = O(k 2 log d).
Proof. For the case of e = 1, the proof follows from Theorem 8.1.3, Corollary 8.1.4 and 8.1.5 in (Du & Hwang,
2000). The bound on the number of classifiers is m ≤
d
3(k + 1)2 log d and the probability is (k + 1) k+1
[1 −
1
1 k m
k+1 (1 − k+1 ) ] .
To show that the matrix is also (k, e)-disjunct with high
probability, we have to just show that |supp(A(i) ) \
supp(A(j) )| > e for any two distinct columns A(i) and
A(j) (Corollary 8.3.2 in (Du & Hwang, 2000)). For any
two fixed columns, |supp(A(i) ) \ supp(A(j) )| is a bik
nomial random variable Bin(m, (k+1)
If we choose,
2 ).
2
m = (3 + )(k + 1) log d,  > 0, we find the expectation
of this variable to be 3k log d. Therefore we can choose
e = 3k log d, and the matrix is going to be (k, e)-disjunct
with high probability.
Theorem. (Restating Theorem 1.) Suppose we wish to recover a k sparse binary vector y ∈ Rd . A random binary {0, 1} matrix A where each entry is 1 with probability
ρ = 1/k recovers 1 − ε proportion of the support of y correctly with high probability, for ε > 0, for m = O(k log d).
This matrix will also detect e = Ω(m) errors.
Proof. This is a modification of Theorem 1 in (Mazumdar
& Mohajer, 2014). Suppose, T ⊂ [d] is the set of defectives. The recovery will be successful as long as we return
a set T 0 such that r ≡ |T ∩ T 0 | ≥ (1 − ε)|T |.
Our object of interest is the probability of error Pe , the
probability of existence of a pair T and T 0 , |T |, |T 0 | ≤ k
such that less than e tests fails to distinguish this pair, where
r < (1 − ε)|T |.
We assume the testing matrix A is chosen randomly from
the ensemble of all m × d matrix in the following way.
Each entry of A is 1 with probability ρ ≡ k1 , and it is zero
with the remaining probability. In other words, in each test

we include an item with probability ρ. We will show that
the probability of error Pe in this case is o(1) which will
implies existence of a matrix A that achieves Pe of o(1).
The probability that any one test will be successful to distinguish between T and T 0 is therefore (here we are taking
the sizes of T and T 0 exactly equal to k and not less than
equal to, which is permissible without much loss of generality),


1 k−r 
1 k 
1− 1−
2(1 − ρ)k (1 − (1 − ρ)k−r ) = 2 1 −
k
k

 k − r  2 

≥ 2 · 3−1 1 − exp −
≥
1 − exp(−ε) ,
k
3
where in the second line we have used inequalities 1 − x ≤
exp(−x) for all x and 1 − x ≥ 3−x for any x ≤ 0.17
(which is true for any k ≥ 6). We have also used the fact
that r < (1 − ε)|T | ≤ (1 − ε)k.
Hence the probability that A successfully distinguish between T and T 0 in less than e tests is,
m−i  2 
i
X m 2 
1−exp(−ε)
.
1− 1−exp(−ε)
3
3
i
i≤e

This probability is going to be upper bounded by
exp(−δm) whenever e < 32 (1 − exp(−ε))m for some
δ > 0. Therefore, for this ensemble,
Pe ≤

k   2
X
d
i=0

i

exp(−δm) → 0,

for an m such that m = O(k log d). Therefore e =
O(k log d).
A.2. Concatenated code based constructions
Many code based constructions have been proposed with
the optimal length of m = Θ(k 2 logk d) (Mazumdar,
2016). One such code based construction of interest is the
Algebraic-Geometric codes.
Considering q = r2 , where r is an integer, using the results in (Tsfasman et al., 2007), we can generate a family
of Algebraic-Geometric (AG) codes of length mq , satisfying mq ≥ ra+1 − ra + 1, where a is an even integer. Using
the Kautz-Singleton mechanism, we can convert this AG
code to a binary code that has constant weight w = mq .
The length of the binary code will be m = qmq .

Multilabel classification via group testing

Proposition 7. We can construct an Algebraic-Geometric
code matrix that recovers 1 − ε proportion of nonzeros in y with high probability, for ε > 0, with m ≥
16k
 log2k d log(d/ε). This matrix will also detect e =
8 log(d/ε) −

8 log(d/ε)
√
2k−1

− 1 log2k d errors.

Proof. The proof follows from the results developed
in (Mazumdar, 2016). For a q-ary Algebraic-Geometric
Code with q ≥ 2k, that is converted to a binary code using Kautz-Singleton mechanism, we have the 1 − ε recovlog d
ery guarantees for m ≥ 16k
log 2k log(d/ε). We know if the
code has a distance h, then e = h/2. The q-ary AG code
satisfies
h ≥ 2m/q − 2 logq d −

2m
.
√
q( q − 1)

We get the value for e upon substitution.
For MLGT, we have the following results for different constructions:
• If A is constructed via randomized construction of
Prop. 1 with m = O(k 2 log d) rows, then the average
error rate is t/k − 32 log d for t > 3/2 log d.
• If A is constructed via randomized construction of
Thm. 1 with m = O(k log d) rows, then the average
error rate is (t/k − O(log d) + εk/d) for t > k log d.
• If A is constructed deterministically via KautzSingleton Reed-Solomon codes construction of
Prop. 2 with m = O(k 2 logk d) rows, then the average
error rate is k logt d − O(1) for t > k logk d.
k
• If A is constructed via expander graph-based construction of Prop. 6 with m = O(k 2 log(d/k)) rows,
then the average error rate is t/k − log(d/k) for t >
k/2 log(d/k).
The error rate is zero for smaller number of misclassifications t.

B. Experiments
Datasets: We use some popular publicly available multilabel datasets in our experiments. All datasets were obtained from The Extreme Classification Repository2 (Bhatia et al., 2015). Details about the datasets and the references for their original sources can be found in the repository. Table 4 gives the statistics of these datasets. In
the table, d = #labels, k̄ =average sparsity per instance,
n = #instances and p = #features.
Details of the experiments:
2
https://manikvarma.github.io/downloads/
XC/XMLRepository.html

Dataset
Mediamill
Bibtex
Delicious
RCV1-2K
EurLex-4K
AmazonCat-13K
Wiki10-31K

Table 4. Dataset statistics
d
n
k̄
101
4.38
30993
159
2.40
4880
983
19.03
12920
2456
4.79
623847
3993
5.31
15539
13330
5.04
1186239
30938 18.64
14146

p
120
1839
500
47236
5000
203882
101938

• We use simple least squares binary classifiers for
training and prediction in MLGT. This is because, this
classifier is extremely simple and fast. Also, we use
least squares regressors for other compared methods
(hence, it is a fair comparison). We note that MLGT
performs well with this simple classifier. We can improve the performance of MLGT further by using a
more advanced classifier.
• In the prediction algorithm of MLGT, we have a parameter e, the number of errors the algorithm should
try to correct. The ideal value for e will depend on the
GT matrix used, the values of m, k and d. However,
note that we can test for different values of e at no additional cost. That is, once we compute the Boolean
AND between the predicted reduced vector and the
GT matrix (the dominant operation), we can get different prediction vectors for a range of e and choose
an e that gives the highest training P@k.
• The Orthogonal Matching Pursuit (OMP) algorithm
used for MLCS is as implemented by the SPAMS
library
http://spams-devel.gforge.
inria.fr/.
• Many of the datasets have very sparse feature matrices. In such cases, we reduced the feature dimension
by choosing only the prominent features. That is the
features that have nonzero values for at least half of
the considered training points.
• The results we obtained for the dataset Delicious were
consistently poor. This is because, the average sparsity for this dataset is k̄ = 19.03. We selected data
instances with sparsity at most kmax = 12. Still, the
GT matrices used were not k-disjunct for this case.
Also, the feature dimension is small (p = 500). Results with CS for this data were poor as well in terms
of Hamming loss. However, the precision was better.
• For AmazonCat-13K, the training precision for CS
method is perfect. However, the Hamming error is
poor because they returned many false classes.
• The runtimes reported in the main paper (using
cputime in Matlab) includes generation of compression matrices, multiplying the matrix to the label vectors (boolean OR/SVD computation), training the m

Multilabel classification via group testing

classifiers, and prediction of n training and nt test
points. All runtime experiments were conducted on
an Intel Core i7-5557U CPU @ 3.10GHz machine.
• The runtimes were averaged over 3 trials for smaller
datasets (first 4). So were the errors. But for larger
datasets (last 2), results for just one trial is reported.
For larger datasets, our MLGT runtime is almost half
of the next best timing and yields the lowest error.
• For SLEEC, there are seven parameters to be tuned.
We set these parameters to the values provided by the
authors online. The ideal parameters to be set in this
algorithm for each of the datasets we used (expect
RCV1-2k) were provided by the authors online. In
general, we found SLEEC to be very expensive for
large datsets. Also, there is no procedure to select
these seven parameters and are seem to be selected
in a trial and error fashion.

