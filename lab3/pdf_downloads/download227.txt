Measuring Sample Quality with Kernels

Jackson Gorham 1 Lester Mackey 2

Abstract
Approximate Markov chain Monte Carlo
(MCMC) offers the promise of more rapid sampling at the cost of more biased inference. Since
standard MCMC diagnostics fail to detect these
biases, researchers have developed computable
Stein discrepancy measures that provably determine the convergence of a sample to its
target distribution. This approach was recently
combined with the theory of reproducing kernels
to define a closed-form kernel Stein discrepancy
(KSD) computable by summing kernel evaluations across pairs of sample points. We develop
a theory of weak convergence for KSDs based
on Steinâ€™s method, demonstrate that commonly
used KSDs fail to detect non-convergence even
for Gaussian targets, and show that kernels with
slowly decaying tails provably determine convergence for a large class of target distributions.
The resulting convergence-determining KSDs
are suitable for comparing biased, exact, and
deterministic sample sequences and simpler to
compute and parallelize than alternative Stein
discrepancies. We use our tools to compare biased samplers, select sampler hyperparameters,
and improve upon existing KSD approaches
to one-sample hypothesis testing and sample
quality improvement.

1. Introduction
When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand
R the evaluation of intractable expectations EP [h(Z)] = p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods
(Brooks et al., 2011) are often employed to approximate
these integrals with asymptotically correct sample aver1
Stanford University, Palo Alto, CA USA 2 Microsoft Research New England, Cambridge, MA USA. Correspondence
to: Jackson Gorham <jgorham@stanford.edu>, Lester Mackey
<lmackey@microsoft.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

Pn
ages EQn [h(X)] = n1 i=1 h(xi ). However, many exact
MCMC methods are computationally expensive, and recent
years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness
for increased sampling speed.
Since standard MCMC diagnostics, like mean and trace
plots, pooled and within-chain variance measures, effective
sample size, and asymptotic variance (Brooks et al., 2011),
do not account for asymptotic bias, Gorham & Mackey
(2015) defined a new family of sample quality measures
â€“ the Stein discrepancies â€“ that measure how well EQn
approximates EP while avoiding explicit integration under
P . Gorham & Mackey (2015); Mackey & Gorham (2016);
Gorham et al. (2016) further showed that specific members of this family â€“ the graph Stein discrepancies â€“ were
(a) efficiently computable by solving a linear program and
(b) convergence-determining for large classes of targets P .
Building on the zero mean reproducing kernel theory of
Oates et al. (2016b), Chwialkowski et al. (2016) and Liu
et al. (2016) later showed that other members of the Stein
discrepancy family had a closed-form solution involving
the sum of kernel evaluations over pairs of sample points.
This closed form represents a significant practical advantage, as no linear program solvers are necessary, and the
computation of the discrepancy can be easily parallelized.
However, as we will see in Section 3.2, not all kernel Stein
discrepancies are suitable for our setting. In particular, in
dimension d â‰¥ 3, the kernel Stein discrepancies previously
recommended in the literature fail to detect when a sample is not converging to the target. To address this shortcoming, we develop a theory of weak convergence for the
kernel Stein discrepancies analogous to that of (Gorham &
Mackey, 2015; Mackey & Gorham, 2016; Gorham et al.,
2016) and design a class of kernel Stein discrepancies that
provably control weak convergence for a large class of target distributions.
After formally describing our goals for measuring sample quality in Section 2, we outline our strategy, based
on Steinâ€™s method, for constructing and analyzing practical
quality measures at the start of Section 3. In Section 3.1,
we define our family of closed-form quality measures â€“ the
kernel Stein discrepancies (KSDs) â€“ and establish several

Measuring Sample Quality with Kernels

appealing practical properties of these measures. We analyze the convergence properties of KSDs in Sections 3.2
and 3.3, showing that previously proposed KSDs fail to detect non-convergence and proposing practical convergencedetermining alternatives. Section 4 illustrates the value
of convergence-determining kernel Stein discrepancies in
a variety of applications, including hyperparameter selection, sampler selection, one-sample hypothesis testing, and
sample quality improvement. Finally, in Section 5, we conclude with a discussion of related and future work.
Notation We will use Âµ to denote a generic probability measure and â‡’ to denote the weak convergence of a
sequence of probability measures. We will use kÂ·kr for
r âˆˆ [1, âˆ] to represent the `r norm on Rd and occasionally refer to a generic norm kÂ·k with associated dual norm
âˆ—
kak , supbâˆˆRd ,kbk=1 ha, bi for vectors a âˆˆ Rd . We
let ej be the j-th standard basis vector. For any function
0
g : Rd â†’ Rd , we define M0 (g) , supxâˆˆRd kg(x)k2 ,
M1 (g) , supx6=y kg(x) âˆ’ g(y)k2 /kx âˆ’ yk2 , and âˆ‡g as
the gradient with components (âˆ‡g(x))jk , âˆ‡xk gj (x). We
further let g âˆˆ C m indicate that g is m times continuously differentiable and g âˆˆ C0m indicate that g âˆˆ C m
and âˆ‡l g is vanishing at infinity for all l âˆˆ {0, . . . , m}.
(m,m)
(m,m)
We define C (m,m) (respectively, Cb
and C0
)
to be the set of functions k : Rd Ã— Rd â†’ R with
(x, y) 7â†’ âˆ‡lx âˆ‡ly k(x, y) continuous (respectively, continuous and uniformly bounded, continuous and vanishing at
infinity) for all l âˆˆ {0, . . . , m}.

2. Quality measures for samples
Consider a target distribution P with continuously differentiable (Lebesgue) density p supported on all of Rd . We
assume that the score function b , âˆ‡ log p can be evaluated1 but that, for most functions of interest, direct integration under P is infeasible. We will therefore approximate integration under P using a weighted sample Qn =
P
n
d
i=1 qn (xi )Î´xi with sample points x1 , . . . , xn âˆˆ R and
qn a probability mass function. We will make no assumptions about the origins of the sample points; they may be
the output of a Markov chain or even deterministically generated.
Each
Pn Qn offers an approximation EQn [h(X)] =
i=1 qn (xi )h(xi ) for each intractable expectation
EP [h(Z)], and our aim is to effectively compare the
quality of the approximation offered by any two samples
targeting P . In particular, we wish to produce a quality
measure that (i) identifies when a sequence of samples is
converging to the target, (ii) determines when a sequence
of samples is not converging to the target, and (iii) is
efficiently computable. Since our interest is in approx1

No knowledge of the normalizing constant is needed.

imating expectations, we will consider discrepancies
quantifying the maximum expectation error over a class of
test functions H:
dH (Qn , P ) , sup |EP [h(Z)] âˆ’ EQn [h(X)]|.

(1)

hâˆˆH

When H is large enough, for any sequence of probability
measures (Âµm )mâ‰¥1 , dH (Âµm , P ) â†’ 0 only if Âµm â‡’ P . In
this case, we call (1) an integral probability metric (IPM)
(MuÌˆller, 1997). For example, when H = BLkÂ·k2 , {h :
Rd â†’ R | M0 (h) + M1 (h) â‰¤ 1}, the IPM dBLkÂ·k2 is
called the bounded Lipschitz or Dudley metric and exactly
metrizes convergence in distribution. Alternatively, when
H = WkÂ·k2 , {h : Rd â†’ R | M1 (h) â‰¤ 1} is the set of
1-Lipschitz functions, the IPM dWkÂ·k in (1) is known as the
Wasserstein metric.
An apparent practical problem with using the IPM dH as a
sample quality measure is that EP [h(Z)] may not be computable for h âˆˆ H. However, if H were chosen such that
EP [h(Z)] = 0 for all h âˆˆ H, then no explicit integration under P would be necessary. To generate such a class
of test functions and to show that the resulting IPM still
satisfies our desiderata, we follow the lead of Gorham &
Mackey (2015) and consider Charles Steinâ€™s method for
characterizing distributional convergence.

3. Steinâ€™s method with kernels
Steinâ€™s method (Stein, 1972) provides a three-step recipe
for assessing convergence in distribution:
1. Identify a Stein operator T that maps functions g :
Rd â†’ Rd from a domain G to real-valued functions
T g such that
EP [(T g)(Z)] = 0 for all g âˆˆ G.
For any such Stein operator and Stein set G, Gorham
& Mackey (2015) defined the Stein discrepancy as
S(Âµ, T , G) , sup |EÂµ [(T g)(X)]| = dT G (Âµ, P ) (2)
gâˆˆG

which, crucially, avoids explicit integration under P .
2. Lower bound the Stein discrepancy by an IPM dH
known to dominate weak convergence. This can be
done once for a broad class of target distributions
to ensure that Âµm â‡’ P whenever S(Âµm , T , G) â†’
0 for a sequence of probability measures (Âµm )mâ‰¥1
(Desideratum (ii)).
3. Provide an upper bound on the Stein discrepancy ensuring that S(Âµm , T , G) â†’ 0 under suitable convergence of Âµm to P (Desideratum (i)).

Measuring Sample Quality with Kernels

While Steinâ€™s method is principally used as a mathematical tool to prove convergence in distribution, we seek,
in the spirit of (Gorham & Mackey, 2015; Gorham et al.,
2016), to harness the Stein discrepancy as a practical tool
for measuring sample quality. The subsections to follow
develop a specific, practical instantiation of the abstract
Steinâ€™s method recipe based on reproducing kernel Hilbert
spaces. An empirical analysis of the Stein discrepancies
recommended by our theory follows in Section 4.

The Langevin Stein operator and kernel Stein set together
define our quality measure of interest, the kernel Stein discrepancy (KSD) S(Âµ, TP , Gk,kÂ·k ). When kÂ·k = kÂ·k2 , this
definition recovers the KSD proposed by Chwialkowski
et al. (2016) and Liu et al. (2016). Our next result shows
that, for any kÂ·k, the KSD admits a closed-form solution.
Proposition 2 (KSD closed form). Suppose k âˆˆ C (1,1) ,
and, for each j âˆˆ {1, . . . d}, define the Stein kernel
k0j (x, y) ,

3.1. Selecting a Stein operator and a Stein set

1 d
p(x) dx (p(x)g(x))

= g(x)b(x) + g 0 (x).

Inspired by the generator method of Barbour (1988; 1990)
and GoÌˆtze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting
Langevin Stein operator
(TP g)(x) ,

1
p(x) hâˆ‡, p(x)g(x)i

(3)

= bj (x)bj (y)k(x, y) + bj (x)âˆ‡yj k(x, y)

A standard, widely applicable univariate Stein operator is
the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017),
(T g)(x) ,

1
p(x)p(y) âˆ‡xj âˆ‡yj (p(x)k(x, y)p(y))

= hg(x), b(x)i + hâˆ‡, g(x)i

for functions g : Rd â†’ Rd was independently developed, without connection to Steinâ€™s method, by Oates et al.
(2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P
only through its score function b = âˆ‡ log p and hence is
computable even when the normalizing constant of p is not.
While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined
in (Gorham et al., 2016), we will focus on the Langevin
operator for the sake of brevity.
Hereafter, we will let k : Rd Ã— Rd â†’ R be the reproducing
kernel of a reproducing kernel Hilbert space (RKHS) Kk
of functions from Rd â†’ R. That is, Kk is a Hilbert space
of functions such that, for all x âˆˆ Rd , k(x, Â·) âˆˆ Kk and
f (x) = hf, k(x, Â·)iKk whenever f âˆˆ Kk . We let kÂ·kKk be
the norm induced from the inner product on Kk .
With this definition, we define our kernel Stein set Gk,kÂ·k
as the set of vector-valued functions g = (g1 , . . . , gd ) such
that each component function gj belongs to Kk and the vecâˆ—
tor of their norms kgj kKk belongs to the kÂ·k unit ball:2
âˆ—

+ bj (y)âˆ‡xj k(x, y) + âˆ‡xj âˆ‡yj k(x, y).
i
Pd
1/2
If j=1 EÂµ k0j (X, X)
< âˆ, then S(Âµ, TP , Gk,kÂ·k ) =
q
iid
kwk where wj , EÂµÃ—Âµ [k0j (X, XÌƒ)] with X, XÌƒ âˆ¼ Âµ.
h

The proof is found in Section
Pn C. Notably, when Âµ is the
discrete measure Qn = i=1 qn (xi )Î´xi , the KSD reduces
to
evaluating each k0j at pairs of support points as wj =
qP
n
j
0
0
i,i0 =1 qn (xi )k0 (xi , xi )qn (xi ), a computation which
is easily parallelized over sample pairs and coordinates j.
Our Stein set choice was motivated by the work of Oates
et
who used the sum of Stein kernels k0 =
Pdal. (2016b)
j
k
to
develop
nonparametric control variates. Each
j=1 0
term wj in Proposition 2 can also be viewed as an instance
of the maximum mean discrepancy (MMD) (Gretton et al.,
2012) between Âµ and P measured with respect to the Stein
kernel k0j . In standard uses of MMD, an arbitrary kernel
function is selected, and one must be able to compute expectations of the kernel function under P . Here, this requirement is satisfied automatically, since our induced kernels are chosen to have mean zero under P .
For clarity we will focus on the specific kernel Stein set
choice Gk , Gk,kÂ·k2 for the remainder of the paper, but our
results extend directly to KSDs based on any kÂ·k, since all
KSDs are equivalent in a strong sense:
Proposition 3 (Kernel Stein set equivalence). Under the assumptions of Proposition 2, there are constants cd , c0d > 0 depending only on d and kÂ·k
such that cd S(Âµ, TP , Gk,kÂ·k ) â‰¤ S(Âµ, TP , Gk,kÂ·k2 ) â‰¤
c0d S(Âµ, TP , Gk,kÂ·k ).

Gk,kÂ·k , {g = (g1 , . . . , gd ) | kvk â‰¤ 1 for vj , kgj kKk }.

The short proof is found in Section D.

The following result, proved in Section B, establishes that
this is an acceptable domain for TP .

3.2. Lower bounding the kernel Stein discrepancy

(1,1)

Proposition 1 (Zero mean test functions). If k âˆˆ Cb
and EP [kâˆ‡ log p(Z)k2 ] < âˆ, then EP [(TP g)(Z)] = 0 for
all g âˆˆ Gk,kÂ·k .
2
Our analyses and algorithms support each gj belonging to a
different RKHS Kkj , but we will not need that flexibility here.

We next aim to establish conditions under which the KSD
S(Âµm , TP , Gk ) â†’ 0 only if Âµm â‡’ P (Desideratum (ii)).
Recently, Gorham et al. (2016) showed that the Langevin
graph Stein discrepancy dominates convergence in distribution whenever P belongs to the class P of distantly dissipative distributions with Lipschitz score function b:

Measuring Sample Quality with Kernels

Definition 4 (Distant dissipativity (Eberle, 2015; Gorham
et al., 2016)). A distribution P is distantly dissipative if
Îº0 , lim inf râ†’âˆ Îº(r) > 0 for
: kx âˆ’ yk2 = r}.
Îº(r) = inf{âˆ’2 hb(x)âˆ’b(y),xâˆ’yi
kxâˆ’yk2

(4)

2

Examples of distributions in P include finite Gaussian
mixtures with common covariance and all distributions
strongly log-concave outside of a compact set, including
Bayesian linear, logistic, and Huber regression posteriors
with Gaussian priors (see Gorham et al., 2016, Section 4).
Moreover, when d = 1, membership in P is sufficient
to provide a lower bound on the KSD for most common
kernels including the Gaussian, MateÌrn, and inverse multiquadric kernels.
Theorem 5 (Univariate KSD detects non-convergence).
Suppose that P âˆˆ P and k(x, y) = Î¦(x âˆ’ y) for Î¦ âˆˆ C 2
with a non-vanishing generalized Fourier transform. If
d = 1, then S(Âµm , TP , Gk ) â†’ 0 only if Âµm â‡’ P .
The proof in Section E provides a lower bound on the
KSD in terms of an IPM known to dominate weak convergence. However, our next theorem shows that in higher
dimensions S(Qn , TP , Gk ) can converge to 0 without the
sequence (Qn )nâ‰¥1 converging to any probability measure.
This deficiency occurs even when the target is Gaussian.
Theorem 6 (KSD fails with light kernel tails). Suppose
(1,1)
k âˆˆ Cb
and define the kernel decay rate
Î³(r) , sup{max(|k(x, y)|, kâˆ‡x k(x, y)k2 ,
|hâˆ‡x , âˆ‡y k(x, y)i|) : kx âˆ’ yk2 â‰¥ r}.
If d â‰¥ 3, P = N (0, Id ), and Î³(r) = o(râˆ’Î± ) for Î± , ( 12 âˆ’
1 âˆ’1
, then S(Qn , TP , Gk ) â†’ 0 does not imply Qn â‡’ P .
d)
Theorem 6 implies that KSDs based on the commonly used
Gaussian kernel, MateÌrn kernel, and compactly supported
kernels of Wendland (2004, Theorem 9.13) all fail to detect non-convergence when d â‰¥ 3. In addition, KSDs
based on the inverse multiquadric kernel (k(x, y) = (c2 +
2
kx âˆ’ yk2 )Î² ) for Î² < âˆ’1 fail to detect non-convergence
for any d > 2Î²/(Î² + 1). The proof in Section F shows
that the violating sample sequences (Qn )nâ‰¥1 are simple to
construct, and we provide an empirical demonstration of
this failure to detect non-convergence in Section 4.
The failure of the KSDs in Theorem 6 can be traced to
their inability to enforce uniform tightness. A sequence
of probability measures (Âµm )mâ‰¥1 is uniformly tight if for
every  > 0, there is a finite number R() such that
lim supm Âµm (kXk2 > R()) â‰¤ . Uniform tightness
implies that no mass in the sequence of probability measures escapes to infinity. When the kernel k decays more
rapidly than the score function grows, the KSD ignores excess mass in the tails and hence can be driven to zero by a

non-tight sequence of increasingly diffuse probability measures. The following theorem demonstrates uniform tightness is the missing piece to ensure weak convergence.
Theorem 7 (KSD detects tight non-convergence). Suppose
that P âˆˆ P and k(x, y) = Î¦(xâˆ’y) for Î¦ âˆˆ C 2 with a nonvanishing generalized Fourier transform. If (Âµm )mâ‰¥1 is
uniformly tight, then S(Âµm , TP , Gk ) â†’ 0 only if Âµm â‡’ P .
Our proof in Section G explicitly lower bounds the KSD
S(Âµ, TP , Gk ) in terms of the bounded Lipschitz metric
dBLkÂ·k (Âµ, P ), which exactly metrizes weak convergence.
Ideally, when a sequence of probability measures is not uniformly tight, the KSD would reflect this divergence in its
reported value. To achieve this, we consider the inverse
2
multiquadric (IMQ) kernel k(x, y) = (c2 + kx âˆ’ yk2 )Î²
for some Î² < 0 and c > 0. While KSDs based on IMQ
kernels fail to determine convergence when Î² < âˆ’1 (by
Theorem 6), our next theorem shows that they automatically enforce tightness and detect non-convergence whenever Î² âˆˆ (âˆ’1, 0).
Theorem 8 (IMQ KSD detects non-convergence). Sup2
pose P âˆˆ P and k(x, y) = (c2 + kx âˆ’ yk2 )Î² for c > 0
and Î² âˆˆ (âˆ’1, 0). If S(Âµm , TP , Gk ) â†’ 0, then Âµm â‡’ P .
The proof in Section H provides a lower bound on the KSD
in terms of the bounded Lipschitz metric dBLkÂ·k (Âµ, P ).
The success of the IMQ kernel over other common characteristic kernels can be attributed to its slow decay rate.
When P âˆˆ P and the IMQ exponent Î² > âˆ’1, the function class TP Gk contains unbounded (coercive) functions.
These functions ensure that the IMQ KSD S(Âµm , TP , Gk )
goes to 0 only if (Âµm )mâ‰¥1 is uniformly tight.
3.3. Upper bounding the kernel Stein discrepancy
The usual goal in upper bounding the Stein discrepancy
is to provide a rate of convergence to P for particular
approximating sequences (Âµm )âˆ
m=1 . Because we aim to
directly compute the KSD for arbitrary samples Qn , our
chief purpose in this section is to ensure that the KSD
S(Âµm , TP , Gk ) will converge to zero when Âµm is converging to P (Desideratum (i)).
(2,2)

Proposition 9 (KSD detects convergence). If k âˆˆ Cb
2
and âˆ‡ log p is Lipschitz with EP [kâˆ‡ log p(Z)k2 ] < âˆ,
then S(Âµm , TP , Gk ) â†’ 0 whenever the Wasserstein distance dWkÂ·k2 (Âµm , P ) â†’ 0.
Proposition 9 applies to common kernels like the Gaussian,
MateÌrn, and IMQ kernels, and its proof in Section I provides an explicit upper bound on the KSD inP
terms of the
n
Wasserstein distance dWkÂ·k2 . When Qn = n1 i=1 Î´xi for
iid

xi âˆ¼ Âµ, (Liu et al., 2016, Thm. 4.1) further implies that
S(Qn , TP , Gk ) â‡’ S(Âµ, TP , Gk ) at an O(nâˆ’1/2 ) rate under
continuity and integrability assumptions on Âµ.

Measuring Sample Quality with Kernels

4. Experiments

4.2. The importance of kernel choice

We next conduct an empirical evaluation of the KSD quality measures recommended by our theory, recording all
timings on an Intel Xeon CPU E5-2650 v2 @ 2.60GHz.
Throughout, we will refer to the KSD with IMQ base ker2
nel k(x, y) = (c2 + kx âˆ’ yk2 )Î² , exponent Î² = âˆ’ 12 ,
and c = 1 as the IMQ KSD. Code reproducing all experiments can be found on the Julia (Bezanson et al.,
2014) package site https://jgorham.github.io/
SteinDiscrepancy.jl/.

Theorem 6 established that kernels with rapidly decaying tails yield KSDs that can be driven to zero by offtarget sample sequences. Our next experiment provides
an empirical demonstration of this issue for a multivariate Gaussian target P = N (0, Id ) and KSDs based on
2
the popular Gaussian (k(x, y) = âˆšeâˆ’kxâˆ’yk2 /2 ) and MateÌrn
âˆš
(k(x, y) = (1 + 3kx âˆ’ yk2 )eâˆ’ 3kxâˆ’yk2 ) radial kernels.

4.1. Comparing discrepancies
Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures,
the Wasserstein distance dWkÂ·k2 , which can be computed
for simple univariate targets (Vallender, 1974), and the
spanner graph Stein discrepancy of Gorham & Mackey
(2015). We adopt a bimodal Gaussian mixture with p(x) âˆ
2
2
1
1
eâˆ’ 2 kx+âˆ†e1 k2 + eâˆ’ 2 kxâˆ’âˆ†e1 k2 and âˆ† = 1.5 as our target
P and generate a first sample point sequence i.i.d. from the
target and a second sequence i.i.d. from one component of
the mixture, N (âˆ’âˆ†e1 , Id ). As seen in the left panel of
Figure 1 where d = 1, the IMQ KSD decays at an nâˆ’0.51
rate when applied to the first n points in the target sample
and remains bounded away from zero when applied to the
to the single component sample. This desirable behavior is
closely mirrored by the Wasserstein distance and the graph
Stein discrepancy.
The middle panel of Figure 1 records the time consumed
by the graph and kernel Stein discrepancies applied to the
i.i.d. sample points from P . Each method is given access to
d cores when working in d dimensions, and we use the released code of Gorham & Mackey (2015) with the default
Gurobi 6.0.4 linear program solver for the graph Stein discrepancy. We find that the two methods have nearly identical runtimes when d = 1 but that the KSD is 10 to 1000
times faster when d = 4. In addition, the KSD is straightforwardly parallelized and does not require access to a linear program solver, making it an appealing practical choice
for a quality measure.
Finally, the right panel displays the optimal Stein funcEQn [bj (X)k(X,y)+âˆ‡xj k(X,y)]
tions, gj (y) =
, recovered by
S(Qn ,TP ,Gk )
the IMQ KSD when d = 1 and n =P103 . The associated
d
EQn [k0j (X,y)]
test functions h(y) = (TP g)(y) = j=1
are
S(Qn ,TP ,Gk )
the mean-zero functions under P that best discriminate the
target P and the sample Qn . As might be expected, the
optimal test function for the single component sample features large magnitude values in the oversampled region far
from the missing mode.

Following the proof Theorem 6 in Section F, we construct
an off-target sequence (Qn )nâ‰¥1 that sends S(Qn , TP , Gk )
to 0 for these kernel choicesPwhenever d â‰¥ 3. Specifically,
n
for each n, we let Qn = n1 i=1 Î´xi where, for all i and j,
kxi k2 â‰¤ 2n1/d log n and kxi âˆ’ xj k2 â‰¥ 2 log n. To select
these sample points, we independently sample candidate
points uniformly from the ball {x : kxk2 â‰¤ 2n1/d log n},
accept any points not within 2 log n Euclidean distance of
any previously accepted point, and terminate when n points
have been accepted.
For various dimensions, Figure 2 displays the result of
applying each KSD to the off-target sequence (Qn )nâ‰¥1
and an â€œon-targetâ€ sequence of points sampled i.i.d. from
P . For comparison, we also display the behavior of the
IMQ KSD which provably controls tightness and dominates weak convergence for this target by Theorem 8. As
predicted, the Gaussian and MateÌrn KSDs decay to 0 under
the off-target sequence and decay more rapidly as the dimension d increases; the IMQ KSD remains bounded away
from 0.
4.3. Selecting sampler hyperparameters
The approximate slice sampler of DuBois et al. (2014)
is a biased MCMC procedure designed to accelerate inference when the target density takes the form p(x) âˆ
QL
Ï€(x) l=1 Ï€(yl |x) for Ï€(Â·) a prior distribution on Rd and
Ï€(yl |x) the likelihood of a datapoint yl . A standard slice
sampler must evaluate the likelihood of all L datapoints to
draw each new sample point xi . To reduce this cost, the
approximate slice sampler introduces a tuning parameter 
which determines the number of datapoints that contribute
to an approximation of the slice sampling step; an appropriate setting of this parameter is imperative for accurate inference. When  is too small, relatively few sample points will
be generated in a given amount of sampling time, yielding sample expectations with high Monte Carlo variance.
When  is too large, the large approximation error will produce biased samples that no longer resemble the target.
To assess the suitability of the KSD for tolerance parameter selection, we take as our target P the bimodal Gaussian
mixture model posterior of (Welling & Teh, 2011). For an
array of  values, we generated 50 independent approximate slice sampling chains with batch size 5, each with a

Measuring Sample Quality with Kernels
i.i.d. from mixture
target P

i.i.d. from single
mixture component

d=1

d=4

i.i.d. from mixture
target P
â—

â—

Graph Stein
discrepancy

â—â—â—â—â—
â—
â—
â— â—
â—
â—â—â—â—
â— â—â—â—â—â—
â—
â—
â—
â—
â—
â—
â— â—â—

â—
â—â—
â—â—
â—
â—
â—
â—

10âˆ’1

â—â—
â—â—
â—
â—
â—
â—

10âˆ’1.5

â—

10âˆ’2

â—
â—

â—

â—
â—

102

â—
â—
â—
â—
â—
â—
â—

â—
â—
â—
â—
â—
â—â—â—â—

101
100

â—

10âˆ’2

â—
â—

â—
â—
â—
â—
â—â—
â—
â—
â—

10âˆ’1

â—
â—
â—
â—
â—
â—
â—â—
â—â—

â—
â—
â—
â—
â—â—
â—â—â—

0.6

10âˆ’3

0.4

0.2
h = TP g

Wasserstein

â— â—
â—
â—
â—
â—â—
â—
â—
â—
â—

Computation time (sec)

IMQ KSD

â—

10âˆ’0.5

0.8

â—

103

â—

g

Discrepancy

Discrepancy value

100

i.i.d. from single
mixture component

0.1
0.0
âˆ’0.1

âˆ’2.5

10

101 102 103 104

101 102 103 104

101 102 103 104

Number of sample points, n

101 102 103 104

Number of sample points, n

âˆ’3

0

3

âˆ’3

0

3

x

Figure 1. Left: For d = 1, comparison of discrepancy measures for samples drawn i.i.d. from either the bimodal Gaussian mixture target
P or a single mixture component (see Section 4.1). Middle: On-target discrepancy computation time using d cores in d dimensions.
Right: For n = 103 and d = 1, the Stein functions g and discriminating test functions h = TP g which maximize the KSD.

i.i.d. from target P

â— â— â—â—
â—â—â—â—â—

100

â— â—
â—â—â—â—
â—â—â—

â— â—
â—â—â—
â—â—â—â—

Gaussian

â—
â—

â—

10âˆ’1

â—â—
â—â—
â—â—
â—
â—

â—

â—

â—â—â—
â—â—â—
â—

4.4. Selecting samplers

â—â—
â—â—â—
â—â—

â—

â— â— â—â—
â—â—â—â—â—

100

â— â—
â—â—â—â—
â—â—â—

â—
â—

10âˆ’1

â—â—
â—â—
â—â—
â—
â—

â—

â—

â—â—
â—â—
â—â—
â—

â—

â—â—â—â—
â— â— â—â—â—

â—â—â—â—â—
â— â— â—â—

â—â—â—
â—â—
â—
â—
â—

â—

â—

â—â—â—â—â—
â—
â—

10âˆ’2
2

10

3

10

4

10

d=8

â—â—

â—â—â—â—
â—â—

105 102

103

104

Inverse Multiquadric

â— â— â—â—â—â—â—â—â—

â—
â—

d=5

â—â—
â—â—
â—â—
â—

100
10

â—

d = 20
â—

â—

âˆ’1

Dimension

â— â—
â—â—â—
â—â—â—â—

MatÃ©rn

Kernel Stein discrepancy

â—

10âˆ’2

â—

its small sample size. The sample produced by the KSDselected chain best resembles the posterior target. Using 4
cores, the longest KSD computation with n = 103 sample
points took 0.16s.

Offâˆ’target sample
â—

105

Number of sample points, n

Figure 2. Gaussian and MateÌrn KSDs are driven to 0 by an offtarget sequence that does not converge to the target P = N (0, Id )
(see Section 4.2). The IMQ KSD does not share this deficiency.

budget of 148000 likelihood evaluations, and plotted the
median IMQ KSD and effective sample size (ESS, a standard sample quality measure based on asymptotic variance
(Brooks et al., 2011)) in Figure 3. ESS, which does not
detect Markov chain bias, is maximized at the largest hyperparameter evaluated ( = 10âˆ’1 ), while the KSD is minimized at an intermediate value ( = 10âˆ’2 ). The right panel
of Figure 3 shows representative samples produced by several settings of . The sample produced by the ESS-selected
chain is significantly overdispersed, while the sample from
 = 0 has minimal coverage of the second mode due to

Ahn et al. (2012) developed two biased MCMC samplers
for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). In the full version of
SGFS (termed SGFS-f), a d Ã— d matrix must be inverted to
draw each new sample point. Since this can be costly for
large d, the authors developed a second sampler (termed
SGFS-d) in which only a diagonal matrix must be inverted
to draw each new sample point. Both samplers can be
viewed as discrete-time approximations to a continuoustime Markov process that has the target P as its stationary distribution; however, because no Metropolis-Hastings
correction is employed, neither sampler has the target as
its stationary distribution. Hence we will use the KSD â€“ a
quality measure that accounts for asymptotic bias â€“ to evaluate and choose between these samplers.
Specifically, we evaluate the SGFS-f and SGFS-d samples
produced in (Ahn et al., 2012, Sec. 5.1). The target P is
a Bayesian logistic regression with a flat prior, conditioned
on a dataset of 104 MNIST handwritten digit images. From
each image, the authors extracted 50 random projections of
the raw pixel values as covariates and a label indicating
whether the image was a 7 or a 9. After discarding the first
half of sample points as burn-in, we obtained regression
coefficient samples with 5 Ã— 104 points and d = 51 dimensions (including the intercept term). Figure 4 displays
the IMQ KSD applied to the first n points in each sample.
As external validation, we follow the protocol of Ahn et al.
(2012) to find the bivariate marginal means and 95% confidence ellipses of each sample that align best and worst with
those of a surrogate ground truth sample obtained from a

Measuring Sample Quality with Kernels
Îµ = 0 (n = 230)

ESS (higher is better)
â—

3.0

2

2.5

â—
â—

1

KSD (lower is better)

0

â—

â—

x2

Log median diagnostic

Îµ = 10âˆ’1 (n = 1000)

3

3.5

2.0

Îµ = 10âˆ’2 (n = 416)

0.5

âˆ’1

0.0

âˆ’2
âˆ’3

âˆ’0.5
0

10âˆ’4

10âˆ’3

10âˆ’2

10âˆ’1

Tolerance parameter, Îµ

âˆ’2

âˆ’1

0

1

2

âˆ’2

âˆ’1

0

1

2

âˆ’2

âˆ’1

0

1

2

x1

Figure 3. Left: Median hyperparameter selection criteria across 50 independent approximate slice sampler sample sequences (see Section 4.3); IMQ KSD selects  = 10âˆ’2 ; effective sample size selects  = 10âˆ’1 . Right: Representative approximate slice sampler
samples requiring 148000 likelihood evaluations with posterior equidensity contours overlaid; n is the associated sample size.

Hamiltonian Monte Carlo chain with 105 iterates. Both the
KSD and the surrogate ground truth suggest that the moderate speed-up provided by SGFS-d (0.0017s per sample vs.
0.0019s for SGFS-f) is outweighed by the significant loss
in inferential accuracy. However, the KSD assessment does
not require access to an external trustworthy ground truth
sample. The longest KSD computation took 400s using 16
cores.
4.5. Beyond sample quality comparison
While our investigation of the KSD was motivated by the
desire to develop practical, trustworthy tools for sample
quality comparison, the kernels recommended by our theory can serve as drop-in replacements in other inferential
tasks that make use of kernel Stein discrepancies.
4.5.1. O NE - SAMPLE HYPOTHESIS TESTING
Chwialkowski et al. (2016) recently used the KSD
S(Qn , TP , Gk ) to develop a hypothesis test of whether a
given sample from a Markov chain was drawn from a target distribution P (see also Liu et al., 2016). However, the
authors noted that the KSD test with their default Gaussian
base kernel k experienced a considerable loss of power as
the dimension d increased. We recreate their experiment
and show that this loss of power can be avoided by using
our default IMQ kernel with Î² = âˆ’ 21 and c = 1. Following (Chwialkowski et al., 2016, Section 4) we draw
iid
iid
zi âˆ¼ N (0, Id ) and ui âˆ¼ Unif[0, 1] to generate a sample
(xi )ni=1 with xi = zi + ui e1 for n = 500 and various dimensions d. Using the authorsâ€™ code (modified to include
an IMQ kernel), we compare the power of the Gaussian
KSD test, the IMQ KSD test, and the standard normality test of Baringhaus & Henze (1988) (B&H) to discern
whether the sample (xi )500
i=1 came from the null distribution
P = N (0, Id ). The results, averaged over 400 simula-

tions, are shown in Table 1. Notably, the IMQ KSD experiences no power degradation over this range of dimensions,
thus improving on both the Gaussian KSD and the standard
B&H normality tests.
Table 1. Power of one sample tests for multivariate normality, averaged over 400 simulations (see Section 4.5.1)

B&H
Gaussian
IMQ

d=2
1.0
1.0
1.0

d=5
1.0
1.0
1.0

d=10
1.0
0.88
1.0

d=15
0.91
0.29
1.0

d=20
0.57
0.12
1.0

d=25
0.26
0.02
1.0

4.5.2. I MPROVING SAMPLE QUALITY
Liu & Lee (2016) recently used the KSD S(Qn , TP , Gk )
as a means of improving the quality of a sample. Specifically, given an initial sample Qn supported on x1 , . . . , xn ,
they minimize S(QÌƒn , TP , Gk ) over all measures QÌƒn supported on the same sample points to obtain a new sample
that better approximates P over the class of test functions
H = TP Gk . In all experiments, Liu & Lee (2016) employ
2
1
a Gaussian kernel k(x, y) = eâˆ’ h kxâˆ’yk2 with bandwidth
h selected to be the median of the squared Euclidean distance between pairs of sample points. Using the authorsâ€™
code, we recreate the experiment from (Liu & Lee, 2016,
Fig. 2b) and introduce a KSD objective with an IMQ ker2
nel k(x, y) = (1 + h1 kx âˆ’ yk2 )âˆ’1/2 with bandwidth selected in P
the same fashion. The starting sample is given by
n
Qn = n1 i=1 Î´xi for n = 100, various dimensions d, and
each sample point drawn i.i.d. from P = N (0, Id ). For
the initial sample and the optimized samples produced by
each KSD, Figure 5 displays the mean squared error (MSE)
1
2
d kEP [Z] âˆ’ EQÌƒn [X]k2 averaged across 500 independently
generated initial samples. Out of the box, the IMQ kernel
produces better mean estimates than the standard Gaussian.

Measuring Sample Quality with Kernels
â—

SGFSâˆ’d
â—

SGFSâˆ’d

â—

â— â—

â—â—â—â—â—â—

â—

â— â—â—

18500

âˆ’0.3

â—

âˆ’0.4

x42

âˆ’0.2

x51

â— â—â—
â—
â—
â—â—

Sampler
âˆ’0.3

SGFSâˆ’d

â—

âˆ’0.2

â—
â—

âˆ’0.5 âˆ’0.4 âˆ’0.3 âˆ’0.2 âˆ’0.1 0.0

0.0

x7

x8

SGFSâˆ’f

SGFSâˆ’f

SGFSâˆ’f

18000

âˆ’0.1

1.2
1.1
1.0
0.9
0.8

WORST

â—

BEST

x34

â—

0.0

âˆ’0.4
âˆ’0.5

â—

âˆ’0.6

WORST

17500

0.1

x25

âˆ’0.3
0.2

BEST

IMQ kernel Stein discrepancy

19000

âˆ’0.1
102 102.5 103 103.5 104 104.5

âˆ’0.1

0.0

0.1

x32

Number of sample points, n

0.2

âˆ’1.5 âˆ’1.4 âˆ’1.3 âˆ’1.2 âˆ’1.1

x2

Average MSE, ||EP Z âˆ’ EQ~n X||22 d

Figure 4. Left: Quality comparison for Bayesian logistic regression with two SGFS samplers (see Section 4.4). Right: Scatter plots of
n = 5 Ã— 104 SGFS sample points with overlaid bivariate marginal means and 95% confidence ellipses (dashed blue) that align best and
worst with surrogate ground truth sample (solid red).

10âˆ’2

â—

â—

â—

â—

â—

10âˆ’2.5

Sample

10âˆ’3
10

â—

âˆ’3.5

10

Initial Qn
Gaussian KSD
IMQ KSD

âˆ’4

10âˆ’4.5
2 10

50

75

100

Dimension, d

Figure 5. Average quality of mean estimates (Â±2 standard errors)
under optimized samples QÌƒn for target P = N (0, Id ); MSE averaged over 500 independent initial samples (see Section 4.5.2).

5. Related and future work
The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect
certain forms of non-convergence but fail to detect others
due to the finite number of test functions tested. For example, when P = N (0, 1), the score statistic (Fan et al.,
2006) only monitors sample means and variances.
For an approximation Âµ with continuously differentiable
density r, Chwialkowski et al. (2016, Thm. 2.1) and
Liu et al. (2016, Prop. 3.3) established that if k is C0 universal (Carmeli et al., 2010, Defn. 4.1) or integrally
strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and
Pd
j
2
EÂµ [k0 (X, X) + kâˆ‡ log p(X)
j=1 k0 ,
r(X) k2 ] < âˆ for k0 ,
then S(Âµ, TP , Gk ) = 0 only if Âµ = P . However, this property is insufficient to conclude that probability measures
with small KSD are close to P in any traditional sense. Indeed, Gaussian and MateÌrn kernels are C0 universal and
ISPD, but, by Theorem 6, their KSDs can be driven to zero
by sequences not converging to P . On compact domains,

where tightness is no longer an issue, the combined results
of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007,
Lem. 1), and (Simon-Gabriel & SchoÌˆlkopf, 2016, Thm. 55)
give conditions for a KSD to dominate weak convergence.
While assessing sample quality was our chief objective, our
results may hold benefits for other applications that make
use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into
the Monte Carlo control functionals framework of Oates
et al. (2016b); Oates & Girolami (2015), the variational
inference approaches of Liu & Wang (2016); Liu & Feng
(2016); Ranganath et al. (2016), and the Stein generative
adversarial network approach of Wang & Liu (2016).
In the future, we aim to leverage stochastic, low-rank, and
sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of
sample and data points while still guaranteeing control over
weak convergence. A reader may also wonder for which
distributions outside of P the KSD dominates weak convergence. The following theorem, proved in Section J, shows
that no KSD with a C0 kernel dominates weak convergence
when the target has a bounded score function.
Theorem 10 (KSD fails for bounded scores). If âˆ‡ log p is
(1,1)
bounded and k âˆˆ C0 , then S(Qn , TP , Gk ) â†’ 0 does
not imply Qn â‡’ P .
However, Gorham et al. (2016) developed convergencedetermining graph Stein discrepancies for heavy-tailed
targets by replacing the Langevin Stein operator TP
with diffusion Stein operators of the form (T g)(x) =
1
p(x) hâˆ‡, p(x)(a(x) + c(x))g(x)i. An analogous construction should yield convergence-determining diffusion KSDs
for P outside of P. Our results also extend to targets P
supported on a convex subset X of Rd by choosing k to
satisfy p(x)k(x, Â·) â‰¡ 0 for all x on the boundary of X .

Measuring Sample Quality with Kernels

References
Ahn, S., Korattikara, A., and Welling, M. Bayesian posterior sampling via stochastic gradient Fisher scoring. In
Proc. 29th ICML, ICMLâ€™12, 2012.
Bachman, G. and Narici, L. Functional Analysis. Academic Press textbooks in mathematics. Dover Publications, 1966. ISBN 9780486402512.
Baker, J. Integration of radial functions. Mathematics Magazine, 72(5):392â€“395, 1999.

Fan, Y., Brooks, S. P., and Gelman, A. Output assessment
for Monte Carlo simulations via the score statistic. J.
Comp. Graph. Stat., 15(1), 2006.
Fukumizu, K., Gretton, A., Sun, X., and SchoÌˆlkopf, B. Kernel measures of conditional dependence. In NIPS, volume 20, pp. 489â€“496, 2007.
Geyer, C. J. Markov chain Monte Carlo maximum likelihood. Computer Science and Statistics: Proc. 23rd
Symp. Interface, pp. 156â€“163, 1991.

Barbour, A. D. Steinâ€™s method and Poisson process convergence. J. Appl. Probab., (Special Vol. 25A):175â€“184,
1988. ISSN 0021-9002. A celebration of applied probability.

Gorham, J. and Mackey, L. Measuring sample quality with
Steinâ€™s method. In Cortes, C., Lawrence, N. D., Lee,
D. D., Sugiyama, M., and Garnett, R. (eds.), Adv. NIPS
28, pp. 226â€“234. Curran Associates, Inc., 2015.

Barbour, A. D. Steinâ€™s method for diffusion approximations. Probab. Theory Related Fields, 84(3):297â€“322,
1990. ISSN 0178-8051. doi: 10.1007/BF01197887.

Gorham, J., Duncan, A., Vollmer, S., and Mackey,
L.
Measuring sample quality with diffusions.
arXiv:1611.06972, Nov. 2016.

Baringhaus, L. and Henze, N. A consistent test for multivariate normality based on the empirical characteristic
function. Metrika, 35(1):339â€“348, 1988.

GoÌˆtze, F. On the rate of convergence in the multivariate
CLT. Ann. Probab., 19(2):724â€“739, 1991.

Bezanson, J., Edelman, A., Karpinski, S., and Shah, V.B.
Julia: A fresh approach to numerical computing. arXiv
preprint arXiv:1411.1607, 2014.
Brooks, S., Gelman, A., Jones, G., and Meng, X.-L. Handbook of Markov chain Monte Carlo. CRC press, 2011.
Carmeli, C., De Vito, E., Toigo, A., and UmanitaÌ, V. Vector
valued reproducing kernel hilbert spaces and universality. Analysis and Applications, 8(01):19â€“61, 2010.
Chatterjee, S. and Shao, Q. Nonnormal approximation by
Steinâ€™s method of exchangeable pairs with application to
the Curie-Weiss model. Ann. Appl. Probab., 21(2):464â€“
483, 2011. ISSN 1050-5164. doi: 10.1214/10-AAP712.
Chen, L., Goldstein, L., and Shao, Q. Normal approximation by Steinâ€™s method. Probability and its Applications.
Springer, Heidelberg, 2011. ISBN 978-3-642-15006-7.
doi: 10.1007/978-3-642-15007-4.

Gretton, A., Borgwardt, K., Rasch, M., SchoÌˆlkopf, B., and
Smola, A. A kernel two-sample test. J. Mach. Learn.
Res., 13(1):723â€“773, 2012.
Herb, R. and Sally Jr., P.J. The Plancherel formula, the
Plancherel theorem, and the Fourier transform of orbital
integrals. In Representation Theory and Mathematical
Physics: Conference in Honor of Gregg Zuckermanâ€™s
60th Birthday, October 24â€“27, 2009, Yale University,
volume 557, pp. 1. American Mathematical Soc., 2011.
Korattikara, A., Chen, Y., and Welling, M. Austerity in
MCMC land: Cutting the Metropolis-Hastings budget.
In Proc. of 31st ICML, ICMLâ€™14, 2014.
Ley, C., Reinert, G., and Swan, Y. Steinâ€™s method for comparison of univariate distributions. Probab. Surveys, 14:
1â€“52, 2017. doi: 10.1214/16-PS278.
Liu, Q. and Feng, Y. Two methods for wild variational
inference. arXiv preprint arXiv:1612.00081, 2016.

Chwialkowski, K., Strathmann, H., and Gretton, A. A kernel test of goodness of fit. In Proc. 33rd ICML, ICML,
2016.

Liu, Q. and Lee, J. Black-box importance sampling.
arXiv:1610.05247, October 2016. To appear in AISTATS 2017.

DuBois, C., Korattikara, A., Welling, M., and Smyth, P.
Approximate slice sampling for Bayesian posterior inference. In Proc. 17th AISTATS, pp. 185â€“193, 2014.

Liu, Q. and Wang, D. Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm. arXiv:1608.04471, August 2016.

Eberle, A. Reflection couplings and contraction rates for
diffusions. Probab. Theory Related Fields, pp. 1â€“36,
2015. doi: 10.1007/s00440-015-0673-1.

Liu, Q., Lee, J., and Jordan, M. A kernelized Stein discrepancy for goodness-of-fit tests. In Proc. of 33rd ICML,
volume 48 of ICML, pp. 276â€“284, 2016.

Measuring Sample Quality with Kernels

Mackey, L. and Gorham, J. Multivariate Stein factors
for a class of strongly log-concave distributions. Electron. Commun. Probab., 21:14 pp., 2016. doi: 10.1214/
16-ECP15.
MuÌˆller, A. Integral probability metrics and their generating
classes of functions. Ann. Appl. Probab., 29(2):pp. 429â€“
443, 1997.
Oates, C. and Girolami, M. Control functionals for QuasiMonte Carlo integration. arXiv:1501.03379, 2015.
Oates, C., Cockayne, J., Briol, F., and Girolami, M. Convergence rates for a class of estimators based on steins
method. arXiv preprint arXiv:1603.03220, 2016a.
Oates, C. J., Girolami, M., and Chopin, N. Control functionals for Monte Carlo integration. Journal of the
Royal Statistical Society: Series B (Statistical Methodology), pp. n/aâ€“n/a, 2016b. ISSN 1467-9868. doi:
10.1111/rssb.12185.
Ranganath, R., Tran, D., Altosaar, J., and Blei, D. Operator
variational inference. In Advances in Neural Information
Processing Systems, pp. 496â€“504, 2016.
Simon-Gabriel, C. and SchoÌˆlkopf, B. Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions. arXiv preprint
arXiv:1604.05251, 2016.
Sriperumbudur, B. On the optimal estimation of probability
measures in weak and strong topologies. Bernoulli, 22
(3):1839â€“1893, 2016.
Sriperumbudur, B., Gretton, A., Fukumizu, K., SchoÌˆlkopf,
B., and Lanckriet, G. Hilbert space embeddings and metrics on probability measures. J. Mach. Learn. Res., 11
(Apr):1517â€“1561, 2010.
Stein, C. A bound for the error in the normal approximation
to the distribution of a sum of dependent random variables. In Proc. 6th Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley, Calif., 1970/1971), Vol. II: Probability theory, pp.
583â€“602. Univ. California Press, Berkeley, Calif., 1972.
Stein, C., Diaconis, P., Holmes, S., and Reinert, G. Use
of exchangeable pairs in the analysis of simulations. In
Steinâ€™s method: expository lectures and applications,
volume 46 of IMS Lecture Notes Monogr. Ser., pp. 1â€“26.
Inst. Math. Statist., Beachwood, OH, 2004.
Steinwart, I. and Christmann, A. Support Vector Machines.
Springer Science & Business Media, 2008.
Stewart, J. Positive definite functions and generalizations,
an historical survey. Rocky Mountain J. Math., 6(3):409â€“
434, 09 1976. doi: 10.1216/RMJ-1976-6-3-409.

Vallender, S. Calculation of the Wasserstein distance
between probability distributions on the line. Theory
Probab. Appl., 18(4):784â€“786, 1974.
Wainwright, M.
High-dimensional statistics:
A
non-asymptotic viewpoint.
2017.
URL http:
//www.stat.berkeley.edu/Ëœwainwrig/
nachdiplom/Chap5_Sep10_2015.pdf.
Wang, D. and Liu, Q. Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial
Learning. arXiv:1611.01722, November 2016.
Welling, M. and Teh, Y. Bayesian learning via stochastic
gradient Langevin dynamics. In ICML, 2011.
Wendland, H. Scattered data approximation, volume 17.
Cambridge university press, 2004.
Zellner, A. and Min, C. Gibbs sampler convergence criteria. JASA, 90(431):921â€“927, 1995.

Measuring Sample Quality with Kernels

A. Additional appendix notation
d
We use f âˆ— h to denote the convolution between f and h, and, for absolutely integrable f : Rq
â†’ R, we say fË†(Ï‰) ,
R
P
d
2
2
(2Ï€)âˆ’d/2 f (x)eâˆ’ihx,Ï‰i dx is the Fourier transform of f . For g âˆˆ Kkd we define kgkKd ,
j=1 kgj kKk . Let L
k
R
denote the Banach space of q
real-valued functions f with kf kL2 , f (x)2 dx < âˆ. For Rd -valued g, we will overload
Pd
2
d
g âˆˆ L2 to mean kgkL2 ,
j=1 kgj kL2 < âˆ. We define the operator norm of a vector a âˆˆ R as kakop , kak2

and of a matrix A âˆˆ RdÃ—d as kAkop , supxâˆˆRd ,kxk2 =1 kAxk2 . We further define the Lipschitz constant M2 (g) ,
supx6=y kâˆ‡g(x) âˆ’ âˆ‡g(y)kop /kx âˆ’ yk2 and the ball B(x, r) , {y âˆˆ Rd | kx âˆ’ yk2 â‰¤ r} for any x âˆˆ Rd and r â‰¥ 0.

B. Proof of Proposition 1: Zero mean test functions
Fix any g âˆˆ G. Since k âˆˆ C (1,1) , supxâˆˆRd k(x, x) < âˆ, and supxâˆˆRd kâˆ‡x âˆ‡y k(x, x)kop < âˆ, Cor. 4.36 of (Steinwart &
Christmann, 2008) implies that M0 (gj ) < âˆ and M1 (gj ) < âˆ for each j âˆˆ {1, . . . , d}. As EP [kb(Z)k2 ] < âˆ, the proof
of (Gorham & Mackey, 2015, Prop. 1) now implies EP [(TP g)(Z)] = 0.

C. Proof of Proposition 2: KSD closed form
Our proof generalizes that of (Chwialkowski et al., 2016, Thm. 2.1). For each dimension j âˆˆ {1, . . . , d}, we define
1
the operator TPj via (TPj g0 )(x) , p(x)
âˆ‡xj (p(x)g0 (x)) = âˆ‡xj g0 (x) + bj (x)g0 (x) for g0 : Rd â†’ R. We further let
Î¨k : Rd â†’ Kk denote the canonical feature map of Kk , given by Î¨k (x) , k(x, Â·). Since k âˆˆ C (1,1) , the argument of
(Steinwart & Christmann, 2008, Cor. 4.36) implies that
TP g(x) =

Pd

j
j=1 (TP gj )(x)

=

Pd

j=1

Pd

j
j=1 hgj , TP Î¨k (x)iKk

TPj hgj , Î¨k (x)iKk =

(5)

for all g = (g1 , . . . , gd ) âˆˆ Gk,kÂ·k and x âˆˆ Rd . Moreover, (Steinwart & Christmann, 2008, Lem. 4.34) gives
hTPj Î¨k (x), TPj Î¨k (y)i = hbj (x)Î¨k (x) + âˆ‡xj Î¨k (x), bj (y)Î¨k (y) + âˆ‡yj Î¨k (y)iKk
= bj (x)bj (y)k(x, y) + bj (x)âˆ‡yj k(x, y) + bj (y)âˆ‡xj k(x, y) + âˆ‡xj âˆ‡yj k(x, y) = k0j (x, y)

(6)

for all x, y âˆˆ Rd and j âˆˆ {1, . . . , d}. The representation (6) and our Âµ-integrability assumption together imply that, for
each j, TPj Î¨k is Bochner Âµ-integrable (Steinwart & Christmann, 2008, Definition A.5.20), since




EÂµ TPj Î¨k (X)

Kk


= EÂµ

q



k0j (X, X)

< âˆ.

Hence, we may apply the representation (6) and exchange expectation and RKHS inner product to discover
h
i
h
i  h
i2


wj2 = E k0j (X, XÌƒ) = E hTPj Î¨k (X), TPj Î¨k (XÌƒ)iKk = EÂµ TPj Î¨k (X)  .
Kk

(7)

iid

for X, XÌƒ âˆ¼ Âµ. To conclude, we invoke the representation (5), Bochner Âµ-integrability, the representation (7), and the
Fenchel-Young inequality for dual norms twice:
S(Âµ, TP , Gk,kÂ·k ) =

sup EÂµ [(TP g)(X)] =
gâˆˆGk,kÂ·k

= sup
kvkâˆ— â‰¤1

Pd

sup
âˆ—

kgj kK =vj ,kvk â‰¤1

j
j=1 hgj , EÂµ [TP Î¨k (X)]iKk

k





j
v
[T
Î¨
(X)]
E

j
Âµ
k
P
j=1

Pd

Kk

= sup
kvkâˆ— â‰¤1

Pd

j=1 vj wj

= kwk.

D. Proof of Proposition 3: Stein set equivalence
By Proposition 2, S(Âµ, TP , Gk,kÂ·k ) = kwk and S(Âµ, TP , Gk,kÂ·k2 ) = kwk2 for some vector w, and by (Bachman & Narici,
1966, Thm. 8.7), there exist constants cd , c0d > 0 depending only on d and kÂ·k such that cd kwk â‰¤ kwk2 â‰¤ c0d kwk.

Measuring Sample Quality with Kernels

E. Proof of Theorem 5: Univariate KSD detects non-convergence
While the statement of Theorem 5 applies only to the univariate case d = 1, we will prove all steps for general d when
possible. Our strategy is to define a reference IPM dH for which Âµm â‡’ P whenever dH (Âµm , P ) â†’ 0 and then upper
bound dH by a function of the KSD S(Âµm , TP , Gk ). To construct the reference class of test functions H, we choose some
integrally strictly positive definite (ISPD) kernel kb : Rd Ã— Rd â†’ R, that is, we select a kernel function kb such that
Z
kb (x, y)dÂµ(x)dÂµ(y) > 0
Rd Ã—Rd
d
for all finite non-zero signed Borel measures
et al., 2010, Section 1.2). For this proof, we will

 Âµ on R (Sriperumbudur
2
choose the Gaussian kernel kb (x, y) = exp âˆ’kx âˆ’ yk2 /2 , which is ISPD by (Sriperumbudur et al., 2010, Section 3.1).


2
Since r(x) , exp âˆ’kxk2 /2 is bounded and continuous and never vanishes, the kernel kËœb (x, y) = kb (x, y)r(x)r(y) is
also ISPD. Let H , {h âˆˆ K Ëœ | khk Ëœ â‰¤ 1}. By (Sriperumbudur, 2016, Thm. 3.2), since kËœb is ISPD with kËœb (x, Â·) âˆˆ
kb

kb

C0 (Rd ) for all x, we know that dH (Âµm , P ) â†’ 0 only if Âµm â‡’ P . With H in hand, Theorem 5 will follow from our next
theorem which upper bounds the IPM dH (Âµ, P ) in terms of the KSD S(Âµ, TP , Gk ).
Theorem 11 (Univariate KSD lower bound). Let d = 1, and consider the set of univariate functions H = {h âˆˆ
KkËœb | khkkËœb â‰¤ 1}. Suppose P âˆˆ P and k(x, y) = Î¦(x âˆ’ y) for Î¦ âˆˆ C 2 with generalized Fourier transform Î¦Ì‚ and
F (t) , supkÏ‰kâˆ â‰¤t Î¦Ì‚(Ï‰)âˆ’1 finite for all t > 0. Then there exists a constant MP > 0 such that, for all probability
measures Âµ and  > 0,

1/2
1/4
2
âˆ’1
MP F 12 log
(1
+
M
(b)M
)
S(Âµ, TP , Gk ).
dH (Âµ, P ) â‰¤  + Ï€2
1
P
Ï€
Remarks
An explicit value for the Stein factor MP can be derived from the proof in Section E.1 and the results
of Gorham et al. (2016). After optimizing the
q bound dH (Âµ, P ) over  > 0, the Gaussian, inverse multiquadric, and
MateÌrn (v > 1) kernels achieve rates of O(1/ log( S(Âµ,T1P ,Gk ) )), O(1/ log( S(Âµ,T1P ,Gk ) )), and O(S(Âµ, TP , Gk )1/(v+1/2) )
respectively as S(Âµ, TP , Gk ) â†’ 0.
In particular, since Î¦Ì‚ is non-vanishing, F (t) is finite for all t. If S(Âµm , TP , Gk ) â†’ 0, then, for any fixed  > 0, we have
lim supmâ†’âˆ dH (Âµm , P ) â‰¤ . Taking  â†’ 0 shows that limmâ†’âˆ dH (Âµm , P ) â†’ 0, which implies that Âµm â‡’ P .
E.1. Proof of Theorem 11: Univariate KSD lower bound
2

Fix any probability measure Âµ and h âˆˆ H, and define the tilting function Î(x) , (1 + kxk2 )1/2 . The proof will proceed
in three steps.
Step 1: Uniform bounds on M0 (h), M1 (h) and supxâˆˆRd kÎ(x)âˆ‡h(x)k2 We first bound M0 (h), M1 (h) and
2
supxâˆˆRd kÎ(x)âˆ‡h(x)k2 uniformly over H. To this end, we define the finite value c0 , supxâˆˆRd (1 + kxk2 )r(x) = 2eâˆ’1/2 .
d
For all x âˆˆ R , we have
|h(x)| = |hh, kËœb (x, Â·)iKkËœ | â‰¤ khkK Ëœ kËœb (x, x)1/2 â‰¤ 1.
b

kb

Moreover, we have âˆ‡x kb (x, y) = (y âˆ’ x)kb (x, y) and âˆ‡r(x) = âˆ’xr(x). Thus for any x, by (Steinwart & Christmann,
2008, Corollary 4.36) we have
2
2
kâˆ‡h(x)k2 â‰¤ khkK Ëœ hâˆ‡x , âˆ‡y kËœb (x, x)i1/2 â‰¤ [d r(x)2 + kxk2 r(x)2 ]1/2 kb (x, x)1/2 â‰¤ [(d âˆ’ 1)1/2 + (1 + kxk2 )1/2 )]r(x),
kb

where in the last inequality we used the triangle inequality. Hence kâˆ‡h(x)k2 â‰¤ (d âˆ’ 1)1/2 + 1 and kÎ(x)âˆ‡h(x)k2 â‰¤
(d âˆ’ 1)1/2 + c0 for all x, completing our bounding of M0 (h), M1 (h) and supxâˆˆRd kÎ(x)âˆ‡h(x)k2 uniformly over H.
Step 2: Uniform bound on kgh kL2 for Stein solution gh

We next show that there is a solution to the P Stein equation

(TP gh )(x) = h(x) âˆ’ EP [h(Z)]

(8)

Measuring Sample Quality with Kernels
2

with gh (x) â‰¤ MP /(1 + kxk2 )1/2 for every h âˆˆ H. When d = 1, this will imply that kgh kL2 is bounded uniformly over
H. To proceed, we will define a tilted distribution PÌƒ âˆˆ P and a tilted function f , show that a solution gÌƒf to the PÌƒ Stein
equation is bounded, and construct a solution gh to the Stein equation of P based on gÌƒf .
Define PÌƒ via the tilted probability density pÌƒ(x) âˆ p(x)/Î(x) with score function bÌƒ(x) , âˆ‡ log pÌƒ(x) = b(x) âˆ’ Î¾(x)
2
2
xx>
for Î¾(x) , âˆ‡ log Î(x) = x/(1 + kxk2 ). Since b is Lipschitz and âˆ‡Î¾(x) = (1 + kxk2 )âˆ’1 [I âˆ’ 2 1+kxk
2 ] has its
2

operator norm uniformly bounded by 3, bÌƒ is also Lipschitz. To see that PÌƒ is also distantly dissipative, note first that
|hÎ¾(x) âˆ’ Î¾(y), x âˆ’ yi| â‰¤ kÎ¾(x) âˆ’ Î¾(y)k2 Â· kx âˆ’ yk2 â‰¤ kx âˆ’ yk2 since supx kÎ¾(x)k2 â‰¤ 1/2. Because P is distantly dissi2
pative, we know hb(x) âˆ’ b(y), x âˆ’ yi â‰¤ âˆ’ 21 Îº0 kx âˆ’ yk2 for some Îº0 > 0 and all kx âˆ’ yk2 â‰¥ R for some R > 0. Thus
for all kx âˆ’ yk2 â‰¥ max(R, 4/Îº0 ), we have
1
1 Îº0
2
2
hbÌƒ(x) âˆ’ bÌƒ(y), x âˆ’ yi = hb(x) âˆ’ b(y), x âˆ’ yi + hÎ¾(x) âˆ’ Î¾(y), x âˆ’ yi â‰¤ âˆ’ Îº0 kx âˆ’ yk2 + kx âˆ’ yk2 â‰¤ âˆ’
kx âˆ’ yk2 ,
2
2 2
so PÌƒ is also distantly dissipative and hence in P.
Let f (x) , Î(x)(h(x) âˆ’ EP [h(Z)]). Since EPÌƒ [f (Z)] = EP [h(Z) âˆ’ EP [h(Z)]] = 0, Thm. 5 and Sec. 4.2 of (Gorham
et al., 2016), imply that the PÌƒ Stein equation (TPÌƒ gÌƒf )(x) = f (x) has a solution gÌƒf with M0 (gf ) â‰¤ M0P M1 (f ) for M0P a
kxk
constant independent of f and h. Since âˆ‡f (x) = âˆ‡Î(x)(h(x) âˆ’ EP [h(Z)]) + Î(x)âˆ‡h(x) and kâˆ‡Î(x)k2 = (1+kxk22)1/2
2

is bounded by 1, M0 (gf ) â‰¤ M0P (2 + (d âˆ’ 1)1/2 + c0 ) , MP , a constant independent of h.
Finally, we note that gh (x) , gÌƒf (x)/Î(x) is a solution to the P Stein equation (8) satisfying gh (x) â‰¤ MP /Î(x) =
âˆš
2
MP /(1 + kxk2 )1/2 . Hence, in the case d = 1, we have kgh kL2 â‰¤ MP Ï€.
Step 3: Approximate TP gh using TP Gk In our final step, we will use the following lemma, proved in Section E.2, to
show that we can approximate TP gh arbitrarily well by a function in a scaled copy of TP Gk .
Lemma 12 (Stein approximations with finite RKHS norm). Suppose that g : Rd â†’ Rd is bounded and belongs to L2 âˆ© C 1
and that h = TP g is Lipschitz. Moreover, suppose k(x, y) = Î¦(x âˆ’ y) for Î¦ âˆˆ C 2 with generalized Fourier transform Î¦Ì‚.
Then for every  > 0, there is a function g : Rd â†’ Rd such that supxâˆˆRd |(TP g )(x) âˆ’ (TP g)(x)| â‰¤  and
kg kKd â‰¤ (2Ï€)âˆ’d/4 F
k



12d log 2
(M1 (h)
Ï€

+ M1 (b)M0 (g))âˆ’1

1/2

kgkL2 ,

where F (t) , supkÏ‰kâˆ â‰¤t Î¦Ì‚(Ï‰)âˆ’1 .
When d = 1, Lemma 12 implies that for every  > 0 there is a function g : R â†’ R such that M0 (TP g âˆ’ h) â‰¤  and
2
(M1 (h) + M1 (b)MP )âˆ’1 )1/2 . Hence we have
kg kKk â‰¤ 3( Ï€2 )1/4 MP F ( 12 log
Ï€
|EP [h(Z)] âˆ’ EÂµ [h(X)]| â‰¤ |EÂµ [h(X) âˆ’ (TP g )(X)]| + |EÂµ [(TP g )(X)]|
â‰¤  + kg kKk S(Âµ, TP , Gk )
1/2
âˆš 
2
(M1 (h) + M1 (b)MP )âˆ’1
S(Âµ, TP , Gk ).
â‰¤  + (2Ï€)âˆ’1/4 MP Ï€F 12 log
Ï€
Taking a supremum over h âˆˆ H yields the advertised result.
E.2. Proof of Lemma 12: Stein approximations with finite RKHS norm
R
Qd sin x
Let us define the function S : Rd â†’ R via the mapping S(x) , j=1 xj j . Then S âˆˆ L2 and Rd kxk2 S(x)4 < âˆ.
R
We will then define the density function Ï(x) , Z âˆ’1 S(x)4 , where Z , Rd S(x)4 dx = (2Ï€/3)d is the normalization
constant. One can check that ÏÌ‚(Ï‰)2 â‰¤ (2Ï€)âˆ’d I[kÏ‰kâˆ â‰¤ 4].
Let Y be a random variable with density Ï. For each Î´ > 0, let us define ÏÎ´ (x) = Î´ âˆ’d Ï(x/Î´) and for any function f let us denote fÎ´ (x) , E[f (x + Î´Y )]. Since h = TP g is assumed Lipschitz, this implies |hÎ´ (x) âˆ’ h(x)| =
|EÏ [h(x + Î´Y ) âˆ’ h(x)]| â‰¤ Î´ M1 (h) EÏ [kY k2 ] for all x âˆˆ Rd .

Measuring Sample Quality with Kernels

Next, notice that for any Î´ > 0 and x âˆˆ Rd ,
(TP gÎ´ )(x) = EÏ [hb(x), g(x + Î´Y )i] + E[hâˆ‡, g(x + Î´Y )i],

and

hÎ´ (x) = EÏ [hb(x + Î´Y ), g(x + Î´Y )i] + E[hâˆ‡, g(x + Î´Y )i].
Thus because we assume b is Lipschitz, we can deduce from above for any x âˆˆ Rd ,
|(TP gÎ´ )(x) âˆ’ hÎ´ (x)| = |EÏ [hb(x) âˆ’ b(x + Î´Y ), g(x + Î´Y )i]|
â‰¤ EÏ [kb(x) âˆ’ b(x + Î´Y )k2 kg(x + Î´Y )k2 ]
â‰¤ M0 (g) M1 (b) Î´ EÏ [kY k2 ].
Thus for any  > 0, letting Ëœ = /((M1 (h) + M1 (b)M0 (g))EÏ [kY k2 ]), we have by the triangle inequality
|(TP gËœ)(x) âˆ’ (TP g)(x)| â‰¤ |(TP gËœ)(x) âˆ’ hËœ(x)| + |hËœ(x) âˆ’ h(x)| â‰¤ .
Thus it remains to bound the RKHS norm of gÎ´ . By the Convolution Theorem (Wendland, 2004, Thm. 5.16), we have
gË†Î´ (Ï‰) = (2Ï€)d/2 gÌ‚(Ï‰)ÏË†Î´ (Ï‰), and so the squared norm of gÎ´ in Kkd is equal to (Wendland, 2004, Thm. 10.21)
(
)Z
Z
Z
|gË†Î´ (Ï‰)|2
|gÌ‚(Ï‰)|2 ÏË†Î´ (Ï‰)2
âˆ’d/2
d/2
âˆ’d/2
âˆ’1
(2Ï€)
dÏ‰ = (2Ï€)
dÏ‰ â‰¤ (2Ï€)
Î¦Ì‚(Ï‰)
sup
|gÌ‚(Ï‰)|2 dÏ‰,
âˆ’1
d
d
d
Î¦Ì‚(Ï‰)
Î¦Ì‚(Ï‰)
kÏ‰kâˆ â‰¤4Î´
R
R
R
where in the inequality we used the fact that ÏË†Î´ (Ï‰) = ÏÌ‚(Î´Ï‰). By Plancherelâ€™s theorem (Herb & Sally Jr., 2011, Thm. 1.1),
we know that f âˆˆ L2 implies that kf kL2 = kfË†kL2 . Thus we have kgÎ´ kKd â‰¤ (2Ï€)âˆ’d/4 F (4Î´ âˆ’1 )1/2 kgkL2 . The final result
k
R
and
also
follows from noticing that R sin4 (x)/x4 dx = 2Ï€
3
Z
Rd

kxk2

 dâˆ’1
Z
d
d
d Z
Y
Y
X
sin4 xj
sin4 xj
(sin xj )4 Y sin4 xk
2Ï€
dx
â‰¤
kxk
dx
=
dx
=
2d(log
2)
,
1
4
4
4
3
x
x
|x
|
x
3
d
d
j
R
j
j
k
j=1
j=1
j=1 R
k6=j

which implies EÏ [kY k2 ] â‰¤

3d log 2
.
Ï€

F. Proof of Theorem 6: KSD fails with light kernel tails
First, define the generalized inverse function Î³ âˆ’1 (s) , inf{r â‰¥ 0 | Î³(r) â‰¤ s}. Next, fix an n â‰¥ 1, let âˆ†n ,
max(1, Î³ âˆ’1 (1/n)), and define rn , âˆ†n n1/d . Select n distinct points x1 , . . . , xn âˆˆ Rd so that zi,i0 , xi âˆ’ xi0 satisfies
kzi,i0 k2 > âˆ†n for all i 6= i0 and
Pn kxi k2 â‰¤ rn for all i. By (Wainwright, 2017, Lems. 5.1 and 5.2), such a point set always
exists. Now define Qn = n1 i=1 Î´xi . We will show that if âˆ†n grows at an appropriate rate then S(Qn , TP , Gk ) â†’ 0 as
n â†’ âˆ.
Since the target distribution P is N (0, Id ), the associated gradient of the log density is b(x) = âˆ’x. Thus
k0 (x, y) ,

d
X

k0j (x, y) = hx, yik(x, y) âˆ’ hy, âˆ‡x k(x, y)i âˆ’ hx, âˆ‡y k(x, y)i + hâˆ‡x , âˆ‡y k(x, y)i.

j=1

From Proposition 2, we have
S(Qn , TP , Gk )2 =

n
n
1 X
1 X
1 X
0
k
(x
,
x
)
=
k0 (xi , xi ) + 2
k0 (xi , xi0 ).
0 i
i
2
2
n
n i=1
n
0
0
i,i =1

(2,2)

Since k âˆˆ Cb

i6=i

, Î³(0) < âˆ. Thus by Cauchy-Schwarz, the first term of (9) is upper bounded by

n
n
1 X
1 X
2
k
(x
,
x
)
â‰¤
kxi k2 k(xi , xi ) + kxi k2 (kâˆ‡x k(xi , xi )k2 + kâˆ‡y k(xi , xi )k2 ) + |hâˆ‡x , âˆ‡y k(xi , xi )i|
0
i
i
n2 i=1
n2 i=1

â‰¤

Î³(0) 2
Î³(0) 1/d
[r + 2rn + 1] â‰¤
(n âˆ†n + 1)2 .
n n
n

(9)

Measuring Sample Quality with Kernels

To handle the second term of (9), we will use the assumed bound on k and its derivatives from Î³. For any fixed i 6= i0 , by
the triangle inequality, Cauchy-Schwarz, and fact Î³ is monotonically decreasing we have
|k0 (xi , xi0 )| â‰¤ kxi k2 kxi0 k2 |k(xi , xi0 )| + kxi k2 kâˆ‡y k(xi , xi0 )k2 + kxi0 k2 kâˆ‡x k(xi , xi0 )k2 + |hâˆ‡x , âˆ‡y k(xi , xi0 )i|
â‰¤ rn2 Î³(kzi,i0 k2 ) + rn Î³(kzi,i0 k2 ) + rn Î³(kzi,i0 k2 ) + Î³(kzi,i0 k2 )
â‰¤ (n1/d âˆ†n + 1)2 Î³(âˆ†n ).
Our upper bounds on the Stein discrepancy (9) and our choice of âˆ†n now imply that
S(Qn , TP , Gk ) = O(n1/dâˆ’1/2 Î³ âˆ’1 (1/n) + nâˆ’1/2 ).
Moreover, since Î³(r) = o(râˆ’Î± ), we have Î³ âˆ’1 (1/n) = o(n1/Î± ) = o(n1/2âˆ’1/d ), and hence S(Qn , TP , Gk ) â†’ 0 as n â†’ âˆ.
However, the sequence (Qn )nâ‰¥1 is not uniformly tight and hence converges to no probability measure. This follows as,
for each r > 0,
(r + 4r/âˆ†m )d
5d rd
1
Qm (kXk2 â‰¤ r) â‰¤
â‰¤
â‰¤
m
m
5
for m = d5d+1 rd e, since at most (r + 4r/âˆ†m )d points with minimum pairwise Euclidean distance greater than âˆ†m can
fit into a ball of radius r (Wainwright, 2017, Lems. 5.1 and 5.2).

G. Proof of Theorem 7: KSD detects tight non-convergence
For any probability measure Âµ on Rd and  > 0, we define its tightness rate as
R(Âµ, ) , inf{r â‰¥ 0 | Âµ(kXk2 > r) â‰¤ }.

(10)

Theorem 7 will follow from the following result which upper bounds the bounded Lipschitz metric dBLkÂ·k2 (Âµ, P ) in terms
of the tightness rate R(Âµ, ), the rate of decay of the generalized Fourier transform Î¦Ì‚, and the KSD S(Âµ, TP , Gk ).
Theorem 13 (KSD tightness lower bound). Suppose P âˆˆ P and let Âµ be a probability measure with tightness rate R(Âµ, )
defined in (10). Moreover, suppose the kernel k(x, y) = Î¦(x âˆ’ y) with Î¦ âˆˆ C 2 and F (t) , supkÏ‰kâˆ â‰¤t Î¦Ì‚(Ï‰)âˆ’1 finite for
all t > 0. Then there exists a constant MP such that, for all , Î´ > 0,
dBLkÂ·k2 (Âµ, P ) â‰¤  + min(, 1)(1 +  +
1/2

+ (2Ï€)âˆ’d/4 Vd
where Î¸d , d
dimension d.

R1
0

Î´ âˆ’1 dÎ¸dâˆ’1
MP )
Î¸d

MP (R(Âµ, ) + 2Î´)d/2 F



12d log 2
(1
Ï€

+ M1 (b)MP )âˆ’1

1/2

S(Âµ, TP , Gk ),


exp âˆ’1/(1 âˆ’ r2 ) rdâˆ’1 dr for d > 0 (and Î¸0 , eâˆ’1 ), and Vd is the volume of the unit Euclidean ball in

Remarks
An explicit value for the Stein factor MP can be derived from the proof in Section G.1 and the results of
Gorham et al. (2016). When bounds on R and F are known, the final expression can be optimized over  and Î´ to produce
rates of convergence in dBLkÂ·k2 .
Consider now a sequence of probability measures (Âµm )mâ‰¥1 that is uniformly tight. This implies that lim supm R(Âµm , ) <
âˆ for all  > 0. Moreover, since Î¦Ì‚ is non-vanishing, F (t) is finite for all t. Thus if S(Âµm , TP , Gk ) â†’ 0, then for any fixed
âˆ’1
 < 1, lim supm dBLkÂ·k2 (Âµm , P ) â‰¤ (2 +  + Î´ Î¸dÎ¸d dâˆ’1 MP ). Taking  â†’ 0 yields dBLkÂ·k2 (Âµm , P ) â†’ 0.
G.1. Proof of Theorem 13: KSD tightness lower bound
Fix any h âˆˆ BLkÂ·k2 . By Theorem 5 and Section 4.2 of (Gorham et al., 2016), there exists a g âˆˆ C 1 which solves the Stein
equation TP g = h âˆ’ E[h(Z)] and satisfies M0 (g) â‰¤ MP for MP a constant independent of h and g. To show that we
can approximate TP g arbitrarily well by a function in a scaled copy of TP Gk , we will form a truncated version of g using
a smoothed indicator function described in the next lemma.

Measuring Sample Quality with Kernels

Lemma 14 (Smoothed indicator function). For any compact set K âŠ‚ Rd and Î´ > 0, define the set inflation K 2Î´ , {x âˆˆ
Rd | kx âˆ’ yk2 â‰¤ 2Î´, âˆ€y âˆˆ K}. There is a function vK,Î´ : Rd â†’ [0, 1] such that
vK,Î´ (x) = 1 for all x âˆˆ K and vK,Î´ (x) = 0 for all x âˆˆ
/ K 2Î´ ,


âˆ’1
kâˆ‡vK,Î´ (x)k2 â‰¤ Î´ Î¸dÎ¸d dâˆ’1 I x âˆˆ K 2Î´ \ K ,
where Î¸d , d

R1
0

(11)
(12)


exp âˆ’1/(1 âˆ’ r2 ) rdâˆ’1 dr for d > 0 and Î¸0 , eâˆ’1 .

This lemma is proved in Section G.2.
Fix any , Î´ > 0, and let K = B(0, R(Âµ, )) with R(Âµ, ) defined in (10). This set is compact since our sequence is
uniformly tight. Hence, we may define gK,Î´ (x) , g(x) vK,Î´ (x) as a smooth, truncated version of g based on Lemma 14.
Since
(TP g)(x) âˆ’ (TP gK,Î´ )(x) = (1 âˆ’ vK,Î´ (x))[hb(x), g(x)i + hâˆ‡, gi(x)] + hâˆ‡vK,Î´ (x), g(x)i
= (1 âˆ’ vK,Î´ (x))(TP g)(x) + hâˆ‡vK,Î´ (x), g(x)i,
properties (11) and (12) imply that (TP g)(x) = (TP gK,Î´ )(x) for all x âˆˆ K, (TP gK,Î´ )(x) = 0 when x âˆˆ
/ K 2Î´ , and
|(TP g)(x) âˆ’ (TP gK,Î´ )(x)| â‰¤ |(TP g)(x)| + kâˆ‡vK,Î´ (x)k2 kg(x)k2
â‰¤ |(TP g)(x)| +

Î´ âˆ’1 dÎ¸dâˆ’1
kg(x)k2
Î¸d

â‰¤1+

Î´ âˆ’1 dÎ¸dâˆ’1
MP
Î¸d

for x âˆˆ K 2Î´ \ K by Cauchy-Schwarz.
Moreover, since vK,Î´ has compact support and is in C 1 by (11), gK,Î´ âˆˆ C 1 with kgK,Î´ kL2 â‰¤ Vol(K 2Î´ )1/2 M0 (g) â‰¤
Vol(K 2Î´ )1/2 MP . Therefore, Lemma 12 implies that there is a function g âˆˆ Kkd such that |(TP g )(x) âˆ’ (TP gK,Î´ )(x)| â‰¤ 
for all x with norm
kg kKd â‰¤ (2Ï€)âˆ’d/4 F ( 12dÏ€log 2 (1 + M1 (b)MP âˆ’1 ))1/2 Vol(K 2Î´ )1/2 MP .
k

(13)

Using the fact that TP gK,Î´ and TP g are identical on K, we have |(TP g )(x) âˆ’ (TP g)(x)| â‰¤  for all x âˆˆ K. Moreover,
when x âˆˆ
/ K, the triangle inequality gives
|(TP g )(x) âˆ’ (TP g)(x)| â‰¤ |(TP g )(x) âˆ’ TP gK,Î´ | + |TP gK,Î´ âˆ’ (TP g)(x)| â‰¤ 1 +  +

Î´ âˆ’1 dÎ¸dâˆ’1
MP .
Î¸d

By the triangle inequality and the definition of the decay rate, we therefore have
|EÂµ [h(X)] âˆ’ EP [h(Z)]| = |EÂµ [(TP g)(X)]| â‰¤ |E[(TP g)(X) âˆ’ (TP g )(X)]| + |EÂµ [(TP g )(X)]|
â‰¤ |EÂµ [((TP g)(X) âˆ’ (TP g )(X))I[X âˆˆ K]]| + |EÂµ [((TP g)(X) âˆ’ (TP g )(X))I[X âˆˆ
/ K]]| + |EÂµ [(TP g )(X)]|
â‰¤  + min(, 1)(1 +  +

Î´ âˆ’1 dÎ¸dâˆ’1
MP )
Î¸d

â‰¤  + min(, 1)(1 +  +

Î´ âˆ’1 dÎ¸dâˆ’1
MP )
Î¸d

+ kg kKd S(Âµm , TP , Gk )

+ (2Ï€)âˆ’d/4 Vol(B(0, R(Âµ, ) + 2Î´))1/2 F

k



12d log 2
(1
Ï€

+ M1 (b)MP )âˆ’1

1/2

MP S(Âµ, TP , Gk ).

The advertised result follows by substituting Vol(B(0, r)) = Vd rd and taking the supremum over all h âˆˆ BLkÂ·k .
G.2. Proof of Lemma 14: Smoothed indicator function
For all x âˆˆ Rd , define the standard normalized bump function Ïˆ âˆˆ C âˆ as


2
Ïˆ(x) , Idâˆ’1 exp âˆ’1/(1 âˆ’ kxk2 ) I[kxk2 < 1],
where the normalizing constant is given by
Z
Id =
B(0,1)



2
exp âˆ’1/(1 âˆ’ kxk2 ) dx = Î¸d Vd

Measuring Sample Quality with Kernels

for Vd being the volume of the unit Euclidean ball in d dimensions (Baker, 1999).


Letting W be a random variable with density Ïˆ, define vK,Î´ (x) , E I x + Î´W âˆˆ K Î´ as the smoothed approximation of
x 7â†’ I[x âˆˆ K], where Î´ > 0 controls the amount of smoothing. Since supp(W ) = B(0, 1), we can immediately conclude
(11) and also supp(âˆ‡vK,Î´ ) âŠ† K 2Î´ \ K.


R
Î´
dy by
Thus to prove (12), it remains to consider x âˆˆ K 2Î´ \ K. We see âˆ‡vK,Î´ (x) = Î´ âˆ’dâˆ’1 B(x,Î´) âˆ‡Ïˆ( xâˆ’y
Î´ )I y âˆˆ K
Leibniz rule. Letting KxÎ´ , Î´ âˆ’1 (K Î´ âˆ’ x), then by Jensenâ€™s inequality we have
kâˆ‡vK,Î´ (x)k2 â‰¤ Î´

âˆ’dâˆ’1




Z
Z


âˆ’1
âˆ‡Ïˆ x âˆ’ y  dy = Î´ âˆ’1
kâˆ‡Ïˆ(z)k
dz
â‰¤
Î´
kâˆ‡Ïˆ(z)k2 dz
2


Î´
B(x,Î´)âˆ©K Î´
B(0,1)âˆ©KxÎ´
B(0,1)
2

Z

where we used the substitution z , (x âˆ’ y)/Î´. By differentiating Ïˆ, using (Baker, 1999) with the substitution r = kzk2 ,
and employing integration by parts we have
1



âˆ’1
2r
exp
(dVd rdâˆ’1 ) dr
kâˆ‡Ïˆ(z)k2 dz =
2 2
1 âˆ’ r2
B(0,1)
0 (1 âˆ’ r )
"

r=1 Z 1
 #


âˆ’1
d
âˆ’1
dâˆ’2
dâˆ’1

(d âˆ’ 1)r
exp
+
=
dr
âˆ’r
exp
Î¸d
1 âˆ’ r2 r=0
1 âˆ’ r2
0

Z

Idâˆ’1

=

Z

d âˆ’1
dÎ¸dâˆ’1
[e I[d = 1] + I[d 6= 1]Î¸dâˆ’1 ] =
Î¸d
Î¸d

yielding (12).

H. Proof of Theorem 8: IMQ KSD detects non-convergence
We first use the following theorem to upper bound the bounded Lipschitz metric dBLkÂ·k (Âµ, P ) in terms of the KSD
S(Âµ, TP , Gk ).
2

Theorem 15 (IMQ KSD lower bound). Suppose P âˆˆ P and k(x, y) = (c2 + kxk2 )Î² for c > 0, and Î² âˆˆ (âˆ’1, 0). Choose
any Î± âˆˆ (0, 21 (Î² + 1)) and a > 12 c. Then there exist an 0 > 0 and a constant MP such that, for all Âµ,
dBLkÂ·k (Âµ, P ) â‰¤


inf



2++

âˆˆ[0,0 ),Î´>0

D(a,c,Î±,Î²)1/2 (S(Âµ,TP ,Gk )âˆ’Î¶(a,c,Î±,Î²))
Î±Îº0 




= O 1/ log S(Âµ,T1P ,Gk )

as

Î´ âˆ’1 dÎ¸dâˆ’1
MP
Î¸d

1/Î±

+ 2Î´



1/2

 + (2Ï€)âˆ’d/4 MP Vd

d/2 q

Ã—

FIM Q ( 12dÏ€log 2 (1 + M1 (b)MP )âˆ’1 )S(Âµ, TP , Gk )

S(Âµ, TP , Gk ) â†’ 0,

(14)
(15)


R1
where Î¸d , d 0 exp âˆ’1/(1 âˆ’ r2 ) rdâˆ’1 dr for d > 0 and Î¸0 , eâˆ’1 , Vd is the volume of the Euclidean unit ball in
d-dimensions, the function D is defined in (20), the function Î¶ is defined in (17), and finally
FIM Q (t) ,

Î“(âˆ’Î²)
21+Î²

 âˆš Î²+d/2
d
c

tÎ²+d/2âˆš
KÎ²+d/2 (c dt)

(16)

where Kv is the modified Bessel function of the third kind. Moreover, if lim supm S(Âµm , TP , Gk ) < âˆ then (Âµm )mâ‰¥1 is
uniformly tight.
Remark
The Stein factor MP can be determined explicitly based on the proof of Theorem 15 in Section H.1 and the
results of Gorham et al. (2016).
Note that FIM Q (t) is finite for all t > 0, so fix any  âˆˆ [0, 0 ) and Î´ > 0.
Î´ âˆ’1 dÎ¸dâˆ’1
MP ).
Î¸d

If S(Âµm , TP , Gk ) â†’ 0, then

lim supm dBLkÂ·k (Âµm , P ) â‰¤ (2 +  +
Thus taking  â†’ 0 yields dBLkÂ·k (Âµm , P ) â†’ 0. Since
dBLkÂ·k (Âµm , P ) â†’ 0 only if Âµm â‡’ P , the statement of Theorem 8 follows.

Measuring Sample Quality with Kernels

H.1. Proof of Theorem 15: IMQ KSD lower bound
Fix any Î± âˆˆ (0, 21 (Î² + 1)) and a > 12 c. Then there is some gÌŠ âˆˆ Gk such that TP gÌŠ is bounded below by a constant
2Î±
Î¶(a, c, Î±, Î²) and has a growth rate of kxk2 as kxk2 â†’ âˆ. Such a function exists by the following lemma, proved in
Section H.2.
Lemma 16 (Generalized multiquadric Stein sets yield coercive functions). Suppose P âˆˆ P and k(x, y) = Î¦c,Î² (x âˆ’ y)
2
for Î¦c,Î² (x) , (c2 + kxk2 )Î² , c > 0, and Î² âˆˆ R \ N0 . Then, for any Î± âˆˆ (0, 21 (Î² + 1)) and a > 12 c, there exists a function
gÌŠ âˆˆ Gk such that TP gÌŠ is bounded below by


D(a, c, Î±, Î²)1/2 M1 (b)R02 + kb(0)k2 R0 + d
,
Î¶(a, c, Î±, Î²) , âˆ’
2Î±
a2(1âˆ’Î±)

(17)
âˆ’2Î±

where the function D is defined in (20) and R0 , inf{r > 0 | Îº(r0 ) â‰¥ 0, âˆ€r0 â‰¥ r}. Moreover, lim infkxk2
Î±
Îº as kxk2 â†’ âˆ.
D(a,c,Î±,Î²)1/2 0

(TP gÌŠ)(x) â‰¥

Our next lemma connects the growth rate of TP gÌŠ to the tightness rate of a probability measure evaluated with the Stein
discrepancy. Its proof is found in Section H.3.
Lemma 17 (Coercive functions yield tightness). Suppose there is a g âˆˆ G such that TP g is bounded below by Î¶ âˆˆ R and
âˆ’u
lim inf kxk2 â†’âˆ kxk2 (TP g)(x) > Î· for some Î·, u > 0. Then for all  sufficiently small and any probability measure Âµ the
tightness rate (10) satisfies

R(Âµ, ) â‰¤

1
(S(Âµ, TP , G) âˆ’ Î¶)
Î·

1/u
.

In particular, if lim supm S(Âµm , TP , Gk ) is finite, (Âµm )mâ‰¥1 is uniformly tight.
We can thus plugâˆšthe tightness rate estimate of Lemma 17 applied to the function gÌŠ into Theorem 13. Since kwkâˆ â‰¤ t
implies kwk2 â‰¤ dt, we can use the formula for the generalized Fourier transform of the IMQ kernel in (18) to see Î¦Ì‚(Ï‰)
Î±
is monotonically decreasing in kwk2 to establish (16). By taking Î· â†’ D(a,c,Î±,Î²)
1/2 Îº0 we obtain (14).
âˆš

To
= O(e(c d+Î»)t ) as t â†’ âˆ for any Î» > 0 by (19). Hence, by choosing  =
âˆš prove (15), 1notice that FIM Q (t)
1/Î±
c d/ log( S(Âµ,TP ,Gk ) ) and Î´ = 1/
we obtain the advertised decay rate as S(Âµ, TP , Gk ) â†’ 0. The uniform tightness
conclusion follows from Lemma 17.
H.2. Proof of Lemma 16: Generalized multiquadric Stein sets yield coercive functions
By (Wendland, 2004, Thm. 8.15), Î¦c,Î² has a generalized Fourier transform of order max(0, dÎ²e) given by

âˆ’Î²âˆ’d/2
21+Î² kÏ‰k2
d
Î¦
(Ï‰)
=
KÎ²+d/2 (ckÏ‰k2 ),
c,Î²
Î“(âˆ’Î²)
c

(18)

where Kv (z) is the modified Bessel function of the third kind. Furthermore, by (Wendland, 2004, Cor. 5.12, Lem. 5.13,
Lem. 5.14), we have the following bounds on Kv (z) for v, z âˆˆ R:
eâˆ’z
Kv (z) â‰¥ Ï„v âˆš for z â‰¥ 1 where Ï„v =
z

r

âˆš |v|âˆ’1/2
Ï€
1
Ï€3
1
for |v| â‰¥ and Ï„v = |v|+1
for |v| < ,
2
2
2
2
Î“(|v| + 1/2)

(19)

Kv (z) â‰¥ eâˆ’1 Ï„v z âˆ’|v| for z â‰¤ 1, (since x 7â†’ xv Kâˆ’v (x) is non-decreasing and Kv = Kâˆ’v )
!
r
2Ï€ âˆ’z+v2 /z |v|âˆ’1
Kv (z) â‰¤ min
e
,2
Î“(|v|)z âˆ’|v| for z > 0.
z
2

Now fix any a > c/2 and Î± âˆˆ (0, 12 (Î² + 1)), and consider the functions gj (x) = âˆ‡xj Î¦a,Î± (x) = 2Î±xj (a2 + kxk2 )Î±âˆ’1 .
We will show that g = (g1 , . . . , gd ) âˆˆ Kkd . Note that gË†j (Ï‰) = (iÏ‰j )Î¦d
a,Î± (Ï‰). Using (Wendland, 2004, Thm. 10.21), we

Measuring Sample Quality with Kernels

know kgj kKk

 q
 q






d
d



= gË†j / Î¦c,Î²  , and thus kgkKd = gÌ‚/ Î¦c,Î² 
 2 . Hence
k
2
L

2

kgkKd =

L

d Z
X

k

j=1

=

d
gË†j (Ï‰)gË†j (Ï‰)/Î¦
c,Î² (Ï‰) dÏ‰

Rd

d Z
X
Rd

j=1

2
Î²âˆ’2Î±âˆ’d/2+2 KÎ±+d/2 (akÏ‰k2 )

Z
= c0
Rd

where c0 =

22(1+Î±) /Î“(âˆ’Î±)2 a2Î±+d
.
21+Î² /Î“(âˆ’Î²) cÎ²+d/2
c

2
22(1+Î±) /Î“(âˆ’Î±)2 a2Î±+d 2
Î²âˆ’2Î±âˆ’d/2 KÎ±+d/2 (akÏ‰k2 )
Ï‰
kÏ‰k
dÏ‰
2
21+Î² /Î“(âˆ’Î²) cÎ²+d/2 j
KÎ²+d/2 (ckÏ‰k2 )

kÏ‰k2

KÎ²+d/2 (ckÏ‰k2 )

dÏ‰,

We can split the integral above into two, with the first integrating over B(0, 1) and the

second integrating over B(0, 1) = Rd \ B(0, 1). Thus using the inequalities from (19) with v0 , Î² + d/2, we have
Z
Z
2
2Î±+dâˆ’2
Î“(Î± + d/2)2 (akwk2 )âˆ’2Î±âˆ’d
Î²âˆ’2Î±âˆ’d/2+2 2
Î²âˆ’2Î±âˆ’d/2+2 KÎ±+d/2 (akÏ‰k2 )
dÏ‰ â‰¤
kÏ‰k2
kÏ‰k2
dÏ‰
âˆ’Î²âˆ’d/2
KÎ²+d/2 (ckÏ‰k2 )
B(0,1)
B(0,1)
eâˆ’1 Ï„v0 Â· kcÏ‰k2
Z
e cÎ²+d/2
2Î²âˆ’4Î±âˆ’d+2 ckÏ‰k2
kÏ‰k2
e
= 22Î±+dâˆ’2 Î“(Î± + d/2)2
dÏ‰
Ï„v0 a2Î±+d B(0,1)
Î²+d/2 Z 1
2Î±+dâˆ’2
2 e c
r2Î²âˆ’4Î±+1 ecr dr,
= d Vd 2
Î“(Î± + d/2)
Ï„v0 a2Î±+d 0
where Vd is the volume of the unit ball in d-dimensions and in the last step we used the substitution r = kÏ‰k2 (Baker,
1999). Since Î± < 21 (Î² + 1) and the function r 7â†’ rt is integrable around the origin when t > âˆ’1, we can bound the
integral above by
Z 1
Z 1
ec
.
r2Î²âˆ’4Î±+1 ecr dr â‰¤ ec
r2Î²âˆ’4Î±+1 dr =
2Î² âˆ’ 4Î± + 2
0
0
We can apply the technique to the other integral, yielding
Z
Z
2
âˆ’2akÏ‰k2 +2(Î±+d/2)2 /(akÏ‰k2 )
Î²âˆ’2Î±âˆ’d/2+2 KÎ±+d/2 (akÏ‰k2 )
Î²âˆ’2Î±âˆ’d/2+2 2Ï€/(akÏ‰k2 ) Â· e
p
kÏ‰k2
dÏ‰ â‰¤
kÏ‰k2
dÏ‰
KÎ²+d/2 (ckÏ‰k2 )
Ï„v0 eâˆ’ckÏ‰k2 / ckÏ‰k2
B(0,1)c
B(0,1)c
âˆš Z
2Ï€ c
Î²âˆ’2Î±âˆ’d/2+3/2 (câˆ’2a)kÏ‰k2 +2(Î±+d/2)2 /(akÏ‰k2 )
â‰¤
kÏ‰k2
e
dÏ‰
aÏ„v0 B(0,1)c
âˆš Z
2Ï€ c âˆ Î²âˆ’2Î±+d/2+1/2 (câˆ’2a)r+2(Î±+d/2)2 /(ar)
r
e
dr
= d Vd
aÏ„v0 1
Since c âˆ’ 2a < 0, we can upper bound the last integral above by the quantity
Z âˆ
2
rÎ²âˆ’2Î±+d/2+1/2 e(câˆ’2a)r+2(Î±+d/2) /(ar) dr
1
Z âˆ
(câˆ’2a)+2(Î±+d/2)2 /a
â‰¤e
rÎ²âˆ’2Î±+d/2+1/2 e(câˆ’2a)r dr
1

= e(câˆ’2a)+2(Î±+d/2)

2

/a

(2a âˆ’ c)âˆ’Î²+2Î±âˆ’d/2âˆ’3/2 Î“(Î² âˆ’ 2Î± + d/2 + 3/2, 2a âˆ’ c),

Râˆ
where Î“(s, x) , x tsâˆ’1 eâˆ’t dt is the upper incomplete gamma function. Hence, the function g belongs to Kkd with norm
upper bounded by D(a, b, Î±, Î²)1/2 where
D(a, c, Î±, Î²) , d Vd 21+2Î±âˆ’Î²

a2Î±+d Î“(âˆ’Î²)
cÎ²+d/2 Î“(âˆ’Î±)2

22Î±+dâˆ’2 Î“(Î± + d/2)2 ec+1 cÎ²+d/2
+
Ï„v0 (2Î² âˆ’ 4Î± + 2)a2Î±+d

!
âˆš
2Ï€ c (câˆ’2a)+2(Î±+d/2)2 /a
âˆ’Î²+2Î±âˆ’d/2âˆ’3/2
e
(2a âˆ’ c)
Î“(Î² âˆ’ 2Î± + d/2 + 3/2, 2a âˆ’ c) .
aÏ„v0

(20)

Measuring Sample Quality with Kernels

Now define gÌŠ = âˆ’D(a, c, Î±, Î²)âˆ’1/2 g so that gÌŠ âˆˆ Gk . We will lower bound the growth rate of TP gÌŠ and also construct a
uniform lower bound. Note
2

2(1 âˆ’ Î±)kxk2
D(a, c, Î±, Î²)1/2
hb(x), xi
d
(TP gÌŠ)(x) = âˆ’
.
2 1âˆ’Î± âˆ’
2 1âˆ’Î± +
2
2
2
2Î±
(a + kxk2 )
(a + kxk2 )
(a2 + kxk2 )2âˆ’Î±

(21)

The latter two terms are both uniformly bounded in x. By the distant dissipativity assumption, there is some Îº > 0 such
2Î±
1
1
1
that lim supkxk2 â†’âˆ kxk
2 hb(x), xi â‰¤ âˆ’ 2 Îº. Thus the first term of (21) grows at least at the rate 2 Îºkxk2 . This assures
âˆ’2Î±

lim infkxk2

2

(TP gÌŠ)(x) â‰¥

Î±
Îº
D(a,c,Î±,Î²)1/2

as kxk2 â†’ âˆ.

Moreover, because b is Lipschitz, we have
2

|hb(x), xi| â‰¤ |hb(x) âˆ’ b(0), x âˆ’ 0i| + |hb(0), xi| â‰¤ M1 (b)kxk2 + kb(0)kkxk2 ,
Hence for any x âˆˆ B(0, R0 ), we must have âˆ’hb(x), xi â‰¥ âˆ’M1 (b)R02 âˆ’ kb(0)k2 R0 . By choice of R0 , for all x âˆˆ
/ B(0, R0 ),
the distant dissipativity assumption implies âˆ’hb(x), xi â‰¥ 0. Hence applying this to (21) shows that TP gÌŠ is uniformly lower
bounded by Î¶(a, c, Î±, Î²).
H.3. Proof of Lemma 17: Coercive functions yield tightness
âˆ’u

Pick g âˆˆ Gk such that lim inf kxk2 â†’âˆ kxk2 (TP g)(x) > Î· and inf xâˆˆRd (TP g)(x) â‰¥ Î¶. Let us define Î³(r) ,
inf{(TP g)(x) âˆ’ Î¶ | kxk2 â‰¥ r} â‰¥ 0 for all r > 0. Thus for sufficiently large r, we have Î³(r) â‰¥ Î·ru . Then, for any
measure Âµ by Markovâ€™s inequality,
Âµ(kXk2 â‰¥ r) â‰¤

EÂµ [Î³(kXk2 )]
EÂµ [(TP g)(X) âˆ’ Î¶]
â‰¤
.
Î³(r)
Î³(r)

Thus we see that Âµ(kXk2 â‰¥ r ) â‰¤  whenever  â‰¥ (S(Âµ, TP , Gk ) âˆ’ Î¶)/Î³(r ). This implies that for sufficiently small , if

1/u
1
r â‰¥
(S(Âµ, TP , Gk ) âˆ’ Î¶)
,
Î·
we must have Âµ(kXk2 â‰¥ r ) â‰¤ . Hence whenever lim supm S(Âµm , TP , Gk ) is bounded, we must have (Âµm )mâ‰¥1 is
uniformly tight as lim supm R(Âµm , ) is finite.

I. Proof of Proposition 9: KSD detects convergence
We will first state and prove a useful lemma.
Lemma 18 (Stein output upper bound). Let Z âˆ¼ P and X âˆ¼ Âµ. If the score function b = âˆ‡ log p is Lipschitz with
2
EP [kb(Z)k2 ] < âˆ, then, for any g : Rd â†’ Rd with max(M0 (g), M1 (g), M2 (g)) < âˆ,
q
2
|EÂµ [(TP g)(X)]| â‰¤ (M0 (g)M1 (b) + M2 (g)d)dWkÂ·k2 (Âµ, P ) + 2M0 (g) M1 (g) EP [kb(Z)k2 ] dWkÂ·k2 (Âµ, P ),
where the Wasserstein distance dWkÂ·k2 (Âµ, P ) = inf Xâˆ¼Âµ,Zâˆ¼P E[kX âˆ’ Zk2 ].
q
2
Proof By Jensenâ€™s inequality, we have EP [kb(Z)k2 ] â‰¤ EP [kb(Z)k2 ] < âˆ, which implies that EP [(TP g)(Z)] = 0
(Gorham & Mackey, 2015, Prop. 1). Thus, using the triangle inequality, Jensenâ€™s inequality, and the Fenchel-Young
inequality for dual norms,
|EÂµ [(TP g)(X)]| = |E[(TP g)(Z) âˆ’ (TP g)(X)]|
= |E[hb(Z), g(Z) âˆ’ g(X)i + hb(Z) âˆ’ b(X), g(X)i + hI, âˆ‡g(Z) âˆ’ âˆ‡g(X)i]|
â‰¤ E[|hb(Z), g(Z) âˆ’ g(X)i|] + (M0 (g)M1 (b) + M2 (g)d)E[kX âˆ’ Zk2 ],
âˆš
To handle the other term above, notice that by Cauchy-Schwarz and the fact that min(a, b) â‰¤ ab for a, b â‰¥ 0,
E[|hb(Z), g(Z) âˆ’ g(X)i|] â‰¤ E[min(2M0 (g), M1 (g)kX âˆ’ Zk2 )kb(Z)k2 ]
h
i
1/2
â‰¤ (2M0 (g)M1 (g))1/2 E kX âˆ’ Zk2 kb(Z)k2
q
2
â‰¤ 2M0 (g)M1 (g)E[kX âˆ’ Zk2 ] EP [kb(Z)k2 ].

Measuring Sample Quality with Kernels

The stated inequality now follows by taking the infimum of these bounds over all joint distributions (X, Z) with X âˆ¼ Âµ
and Z âˆ¼ P .
Now we are ready to prove Proposition 9. In the statement below, let us use Î± âˆˆ Nd as a multi-index for the differentiation
operator DÎ± , that is, for a differentiable function f : Rd â†’ R we have for all x âˆˆ Rd ,
DÎ± f (x) ,

(dx1

)Î±1

d|Î±|
f (x)
. . . (dxd )Î±d

Pd
where |Î±| = j=1 Î±j . Pick any g âˆˆ Gk , and choose any multi-index Î± âˆˆ Nd such that |Î±| â‰¤ 2. Then by Cauchy-Schwarz
and (Steinwart & Christmann, 2008, Lem. 4.34), we have
sup |DÎ± gj (x)| = sup |DÎ± hgj , k(x, Â·)iKk | â‰¤ sup kgj kKk kDÎ± k(x, Â·)kKk = kgj kKk sup (DxÎ± DyÎ± k(x, x))1/2 .

xâˆˆRd

xâˆˆRd

xâˆˆRd

xâˆˆRd

Pd
2
Since j=1 kgj kKk â‰¤ 1 for all g âˆˆ Gk and DxÎ± DyÎ± k(x, x) is uniformly bounded in x for all |Î±| â‰¤ 2, the elements of
the vector g(x), matrix âˆ‡g(x), and tensor âˆ‡2 g(x) are uniformly bounded in x âˆˆ Rd and g âˆˆ Gk . Hence, for some Î»k ,
supgâˆˆGk max(M0 (g), M1 (g), M2 (g)) â‰¤ Î»k < âˆ, so the advertised result follows from Lemma 18 as


q
2
S(Âµ, TP , Gk ) â‰¤ Î»k (M1 (b) + d)dWkÂ·k2 (Âµ, P ) + 2EP [kb(Z)k2 ] dWkÂ·k2 (Âµ, P ) .

J. Proof of Theorem 10: KSD fails for bounded scores
Pn
Fix some n â‰¥ 1, and let Qn = n1 i=1 Î´xi where xi , ine1 âˆˆ Rd for i âˆˆ {1, . . . , n}. This implies kxi âˆ’ xi0 k2 â‰¥ n for
all i 6= i0 . We will show that when M0 (b) is finite, S(Qn , TP , Gk ) â†’ 0 as n â†’ âˆ.
Pd
We can express k0 (x, y) , j=1 k0j (x, y) as
k0 (x, y) = hb(x), b(y)ik(x, y) + hb(x), âˆ‡y k(x, y)i + hb(y), âˆ‡x k(x, y)i + hâˆ‡x , âˆ‡y k(x, y)i.
From Proposition 2, we have
S(Qn , TP , Gk )2 =

n
n
1 X
1 X
1 X
0) =
k
(x
,
x
k0 (xi , xi ) + 2
k0 (xi , xi0 ).
0
i
i
2
2
n
n i=1
n
0
0
i,i =1

(22)

i6=i

(1,1)

Let Î³ be the kernel decay rate defined in the statement of Theorem 6. Then as k âˆˆ C0 , we must have Î³(0) < âˆ and
limrâ†’âˆ Î³(r) = 0. By the triangle inequality


n
n
1 X

Î³(0)
1 X


lim  2
k0 (xi , xi ) â‰¤ lim 2
|k0 (xi , xi )| â‰¤ lim
(M0 (b) + 1)2 = 0.
nâ†’âˆ n
nâ†’âˆ n
 nâ†’âˆ n
i=1
i=1
We now handle the second term of (22). By repeated use of Cauchy-Schwarz we have
|k0 (xi , xi0 )| â‰¤ |hb(xi ), b(xi0 )ik(xi , xi0 )| + |hb(xi ), âˆ‡y k(xi , xi0 )i| + |hb(xi0 ), âˆ‡x k(xi , xi0 )i| + |hâˆ‡x , âˆ‡y k(xi , xi0 )i|
â‰¤ kb(xi )k2 kb(xi0 )k2 |k(xi , xi0 )| + kb(xi )k2 kâˆ‡y k(xi , xi0 )k2 + kb(xi0 )k2 kâˆ‡x k(xi , xi0 )k2
+ |hâˆ‡x , âˆ‡y k(xi , xi0 )i|
â‰¤ Î³(n)(M0 (b) + 1)2 .
By assumption, Î³(r) â†’ 0 as r â†’ âˆ. Furthermore, since the second term of (22) is upper bounded by the average of the
terms k0 (xi , x0i ) for i 6= i0 , we have S(Qn , TP , Gk ) â†’ 0 as n â†’ âˆ. However, (Qn )nâ‰¥1 is not uniformly tight and hence
does not converge to the probability measure P .

