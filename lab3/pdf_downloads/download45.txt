Random Fourier Features for Kernel Ridge Regression:
Approximation Bounds and Statistical Guarantees
Appendix: Proofs

A. Preliminaries
Our upper and lower bound analysis relies predominantly on Fourier analysis, so we now introduce some additional notation and state some useful facts about these.
A.1. Properties of Fourier Transforms
Definition 16 (Fourier Transform). The Fourier transform of a continuous function f : Rd ! C in L1 (Rn ) is defined to
be the function Ff : Rd ! C as follows:
Z
T
(Ff )(⇠) =
f (t)e 2⇡it ⇠ dt.
Rd

We also sometimes use the notation fˆ for the Fourier transform of f . We often informally refer to f as representing the
function in time domain and fˆ as representing the function in frequency domain.
The original function f can also be obtained from fˆ by the inverse Fourier transform:
Z
T
f (t) =
fˆ(⇠)e2⇡i⇠ t d⇠
Rd

Definition 17 (Convolution). The convolution of two functions f : Rd ! C and g : Rd ! C is defined to be the function
(f ⇤ g) : Rd ! C given by
Z
(f ⇤ g)(⌘) =

Rd

f (t)g(⌘

t) dt.

The convolution theorem shows that the Fourier transform of the convolution of two functions is simply the product of the
individual Fourier transforms:
Claim 18 (Convolution Theorem). Given functions f : Rd ! C and g : Rd ! C whose convolution is h = f ⇤ g, we have
ĥ(⇠) = fˆ(⇠) · ĝ(⇠)
for all ⇠ 2 Rd .
Now, suppose d = 1, i.e., the functions we consider take inputs in R. We define the rectangle function and normalized sinc
function, which we use extensively in our analysis.
Definition 19 (Rectangle Function). We define the rectangle function recta : R ! C as
8
>
if |x| > a/2
<0
1
recta (x) = 2
if |x| = a/2 .
>
:
1
if |x| < a/2
If a = 1, then we often omit the subscript and simply write rect.

Definition 20 (Normalized Sinc Function). We define the normalized sinc function sinc : R ! C as
sinc(x) =

sin(⇡x)
.
⇡x

Random Fourier Features for Kernel Ridge Regression

It is well known that the Fourier transform of the rectangle function (with a = 1) is the normalized sinc function:
F(rect) = sinc
We use to denote the Dirac delta function. Recall that the Dirac delta function satisfies the following useful property for
any function f :
Z 1
f (x) (x a) dx = f (a),
1

i.e. the integral of a function multiplied by a shifted Dirac delta functions picks out the value of the function at a particular
point. Thus, it is not hard to see that the Fourier transform of a is the constant function which is 1 everywhere:
Z 1
(F )(⇠) =
e 2⇡it⇠ · (t) dt = e 2⇡i·0·⇠ = 1
1

for all ⇠. Similarly, the Fourier transform of a shifted delta function is as follows:
Z 1
(F (· a))(⇠) =
e 2⇡it⇠ · (t a) dt = e

2⇡ia⇠

.

1

Moreover, it is not hard to see that convolving a function by a shifted delta funciton results in a shift of the original function:
(f ⇤ (·

a))(x) = f (x

a).

Thus, by the convolution theorem, we obtain the following identity:
Claim 21. Given a function f : R ! C, we have
(Ff (·

a))(⇠) = (F(f ⇤ (·

a)))(⇠) = fˆ(⇠) · e

2⇡ia⇠

.

Similarly,
Claim 22. Given a function f : R ! C, we have
(F(f (x) · e2⇡iax ))(⇠) = fˆ(⇠

a).

Finally, we introduce a useful function known as the Dirac comb function:
Definition 23. The Dirac comb function with period T is defined as f satisfying
f (x) =

1
X

(x

jT ).

j= 1

It is a standard fact that the Fourier transform of a Dirac comb function is another Dirac comb function which is scaled and
has the inverse period:
Claim 24. Let
f (x) =

1
X

(x

jT )

j= 1

be the Dirac comb function with period T . Then,

✓
1
1 X
(Ff )(⇠) =
⇠
T j= 1
We use the Dirac comb function in our lower bound constructions.

j
T

◆

.

Random Fourier Features for Kernel Ridge Regression

A.2. Properties of Gaussian Distributions
We also need several useful facts about Gaussian distributions. The following is a standard fact about the cumulative
distribution function of the standard Gaussian distribution:
Claim 25 ((Feller, 1968)). For any x > 0, we have
1
p
2⇡

Z

1

2

e

t2 /2

dt 

x

e x /2
p .
x 2⇡

Moreover, as a direct consequence, for any , x > 0, we have that
p
Also, if x

1, then

✓

1
x

Z

1
2⇡
1
x3

◆

2

1

t2 /2

e

2

dt 

x

1
·p e
2⇡

x2 /2

e x /2
p
x 2⇡

1
p
2⇡

Z

1

2

e

.

t2

dt.

x

We also need the following property about Gaussian samples.
Claim 26. Let t 10, and a1 , a2 , . . . , at be sampled according to the Gaussian distribution given by probability density
2
function p12⇡ e x /2 . Also, let a⇤ = max1jt |aj |. Then,

1
Pr p e
2⇡
Proof. Choose q1 such that

Z

Note that by Claim 25, we have

p
Thus, q1  2 log t.
Also, since

1
t

Z

 14 , we have that q1
1
=
t

Z

1
q1

1
2 log t

1
p e
2⇡

q1

1
p e
2⇡

p

6
5.

1

a⇤2 /2

1
p e
2⇡
x2 /2



p
8 log t
t

x2 /2

dx =

1
.
2

1
.
t

(14)

1
1
dx  p
 .
p
2
t
2 2⇡t log t

Thus, by another application of Claim 25,
x2 /2

✓

dx

1
q1

1
q13

and so,
1
p e
2⇡

q12 /2



◆

1
p e
2⇡

q12 /2

1
1
·p e
4q1
2⇡

p
4q1
8 log t

.
t
t

Therefore,


1
Pr p e
2⇡

a⇤ 2 /2

p
8 log t

t

Pr[a⇤
=1
1
1
,
2

as desired.

✓
1
e

q1 ]
1

1
t

◆t

q12 /2

,

Random Fourier Features for Kernel Ridge Regression

B. Proof of Lemma 2
Note that A

B implies that B

1

A

1

so for the bias term we have:

T

1

f (K̃ + In )

f  (1

)

1 T

f (K + In )

We now consider the variance term. Denote s = rank(K̃), and let
eigenvalues of a matrix A. We have:
⇣
s (K̃) = Tr (K̃ + In )

1

s
⌘ X
K̃ =
i=1

=s

s
X

i (K̃)

(1 +

i=1

 (1
i (A)



2 (A)

···

n (A)

denote the

1

+
s
X

+

i (K) +

= s (K) +

B implies that

)

i (K)

i=1
n
X

n

where we use the fact that A
minimax theorem).

1 (A)

i=1

s
X

=s

(15)

f.

i (K̃)
(
K̃)
+
i

i=1

s

1

)

i (K)

+
+

+

1+
·s
1+

s
X
i=1

i (K)

+

·s
1+
1

s (K) +

i (B)

·s
1+

(this is a simple consequence of the Courant-Fischer

Combining the above variance bound with the bias bound in (15) yields:
b (f )  (1
R
K̃

)

b (f ) completes the proof.
and the bound R(f˜)  R
K̃

1

b K (f ) +
R

(1 +

rank(K̃)
·
)
n
·

2
⌫

C. Proof of Proposition 4

Since k is positive definite and k(0) = 1, |k(x, z)|  1 for all x and z. This implies that the maximum eigenvalue of
K is bounded by n, and the lower bound follows immediately. The upper bound on ⌧ (⌘) follows from the fact that
kz(⌘)k22 = n and all eigenvalues of K + In are bounded from below by . The bound also establishes that the integral
converges. We now have,
Z
Z
⌧ (⌘)d⌘ =
p(⌘)z(⌘)⇤ (K + In ) 1 z(⌘)d⌘
Rd
Rd
Z
=
Tr p(⌘)(K + In ) 1 z(⌘)z(⌘)⇤ d⌘
d
R
✓Z
◆
= Tr
p(⌘)(K + In ) 1 z(⌘)z(⌘)⇤ d⌘
d
✓ R
◆
Z
= Tr (K + In ) 1
p(⌘)z(⌘)z(⌘)⇤ d⌘
Rd

=

Tr (K + In )

1

K = s (K) .

The second equality is due to the fact that z(⌘) is a rank one matrix, and third equality is due to linearity of the trace
operation and the fact that all diagonal entries are positive.

Random Fourier Features for Kernel Ridge Regression

D. Proof of Lemma 6 and Theorem 7
To prove Lemma 6 we need the following lemma which is essentially a restatement of Corollary 7.3.3 from (Tropp, 2015).
However, the minimum t in the following statement is much lower than the bound that appears in (Tropp, 2015) which is
unnecessarily loose (possibly, a typo in (Tropp, 2015)). For completeness, we include a proof.
Lemma 27. Let B be a fixed d1 ⇥ d2 matrix. Construct a d1 ⇥ d2 random matrix R that satisfies
E [R] = B

and

kRk2  L.

Let M1 and M2 be semidefinite upper bounds for the expected squares:
E [RR⇤ ]

M1

and E [R⇤ R]

M2 .

Define the quantities
m = max(kM1 k2 , kM2 k2 )

and

Form the matrix sampling estimator

d = (Tr (M1 ) + Tr (M2 ))/m.
n

1X
R̄n =
Rk
n
k=1
p
where each Rk is an independent copy of R. Then, for all t
m/n + 2L/3n,
✓
◆
nt2 /2
Pr(kR̄n Bk2 t)  4d exp
.
m + 2Lt/3

(16)

Proof. The proof mirrors the proof of Corollary 6.2.1 in (Tropp, 2015), using Theorem 7.3.1 instead of Theorem 6.1.1
(both from (Tropp, 2015)).
Since E [R] = B, we can write

n

Z ⌘ R̄n
where we have define Sk ⌘ n

1

(Rk

B=

1X
(Rk
n

E [R]) =

k=1

n
X

Sk ,

k=1

E [R]). These random matrices are i.i.d and each has zero mean.

Now, we can bound each of the summands:
kSk k2 

1
1
2L
(kRk k2 + kE [R] k2 )  (kRk k2 + E [kRk2 ]) 
,
n
n
n

where the first inequality is the triangle inequality and the second is Jensen’s inequality.
To find semidefinite upper bounds V1 and V2 on the matrix-valued variances we note that
E [S1 S⇤1 ]

=

n

2

=

n

2

E [RR⇤ ]

n

2

E [RR⇤ ] .

E [(R

E [R])⇤ ]

E [R])(R

E [R] E [R]

⇤

Likewise, E [S⇤1 S1 ]
n 2 E [R⇤ R]. Since the summands are i.i.d, if we define V1 ⌘ n
⇤
have E [ZZ ] V1 and E [Z⇤ Z] V2 .
We now calculate,
⌫ ⌘ max(kV1 k2 , kV2 k2 ) =
and

1

M1 and V2 ⌘ n

1

M2 , we

m
n

Tr (V1 ) + Tr (V2 )
= d.
max(kV1 k2 , kV2 k2 )

p
Noticing, that the condition t
m/n + 2L/3n meets the required lower bound in Theorem 7.3.1 in (Tropp, 2015) we
can now apply this theorem, which along with the above calculations translates to (16).

Random Fourier Features for Kernel Ridge Regression

We can now prove Lemma 6.

Proof of Lemma 6. Let K+ In = V T ⌃2 V be an eigendecomposition of K+ In . Note that the
guarantee (2) is equivalent to
K
so by multiplying by ⌃

1

V on the left and V T ⌃
1

k⌃
holds with probability of at least 1

ZZ⇤

(K + In )
1

K+

-spectral approximation

(K + In ) ,

on the right we find that it suffices to show that

VZZ⇤ V T ⌃

1

⌃

1

VKV T ⌃

1

(17)

k2 

⇢. Let
Yl =

Note that E [Yl ] = ⌃ 1 VKV T ⌃
result above to prove (17).

1

and

1
s

p(⌘ l )
⌃
p⌧˜ (⌘ l )

Ps

l=1

1

Vz(⌘ l )z(⌘ l )⇤ V T ⌃

Yl = ⌃

1

VZZ⇤ V T ⌃

1

1

.

. Thus, we can use the matrix concentration

⇥ ⇤
To apply this bound we need to bound the norm of Yl and the stable rank E Y2l . Since Yl is always a rank one matrix
we have
kYl k2

=
=
=
=

since ⌧˜(⌘ l )

=
=
=
=
=

1

···

1

⌧ (⌘ l ). We also have
Y2l

Let

p(⌘ l )
Tr ⌃ 1 Vz(⌘ l )z(⌘ l )⇤ V T ⌃
p⌧˜ (⌘ l )
p(⌘ l )
z(⌘ l )⇤ V T ⌃ 1 ⌃ 1 Vz(⌘ l )
p⌧˜ (⌘ l )
p(⌘ l )
z(⌘ l )⇤ (K + In ) 1 z(⌘ l )
p⌧˜ (⌘ l )
s⌧˜ · ⌧ (⌘ l )
 s⌧˜
⌧˜(⌘ l )

n

p(⌘ l )2
⌃ 1 Vz(⌘ l )z(⌘ l )⇤ V T ⌃ 1 ⌃ 1 Vz(⌘ l )z(⌘ l )⇤ V T ⌃
p⌧˜ (⌘ l )2
p(⌘ l )2
⌃ 1 Vz(⌘ l )z(⌘ l )⇤ (K + In ) 1 z(⌘)z(⌘ l )⇤ V T ⌃ 1
p⌧˜ (⌘ l )2
p(⌘ l )⌧ (⌘ l )
⌃ 1 Vz(⌘ l )z(⌘ l )⇤ V T ⌃ 1
p⌧˜ (⌘ l )2
⌧ (⌘ l )
Yl
p⌧˜ (⌘ l )
s⌧˜ ⌧ (⌘ l )
Yl s⌧˜ Yl
⌧˜(⌘ l )

be the eigenvalues of K. We have
E [s⌧˜ Yl ]

=

s⌧˜ ⌃

1

VKV T ⌃

=

s⌧˜ In

=

s⌧˜ · diag (

⌃

1

2

1 /( 1

+ ), . . . ,

n /( n

+ )) := D .

1

Random Fourier Features for Kernel Ridge Regression

So,
s

Pr

1X
Yl
s

⌃

1

T

VKV ⌃

!

1

l=1

2

◆
s 2 /2
kDk2 + 2s⌧˜ /3
✓
◆
s⌧˜ · s (K)
s 2
8
exp
s⌧˜ · 1 /( 1 + )
2s⌧˜ (1 + 2 /3)
✓
◆
s 2
16s (K) exp
2s⌧˜ (1 + 2 /3)
✓
◆
3s 2
16s (K) exp
⇢
8s⌧˜
8Tr (D)
exp
kDk2






where the third inequality is due to the assumption that
s.

and the last inequality is due to the bound on

= kKk2

1

✓

Proof of Theorem 7. Define ⌧˜(⌘) = p(⌘) · n and note that ⌧˜(⌘)
⌧ (⌘) by Proposition 4 and that s⌧˜ = n . Finally,
note that p⌧˜ (⌘) = p(⌘), the classic Fourier features sampling probability.

E. Proof of Lemmas 11 and 12
Let R( ) ✓ Cn denote the range of . Here we first prove that the operator
linear operator. Indeed, for y 2 L2 (dµ) we have:
k

yk22

Z

=
(by Jensen’s inequality)

Z



Z

=

is defined on all L2 (dµ) and is a bounded
2

Rd

z(⇠)y(⇠)dµ(⇠)
2

Rd

kz(⇠)y(⇠)k22 dµ(⇠)

Rd

ky(⇠)k22 · kz(⇠)k22 dµ(⇠)

n · kyk2L2 (dµ) .

=

Thus, R( ) is a linear subspace of Cn . Therefore, there is a unique adjoint operator ⇤ : R( ) ! L2 (dµ), such that
h y, xiCn = hy, ⇤ xiL2 (dµ) for every y 2 L2 (dµ) and x 2 R( ). It is easy to verify that ( ⇤ x)(⌘) = z(⌘)⇤ x . We now
have the following:
Proposition 28.
⇤

Proof. We have that for every x 2 Cn ,
⇤

x

=
=
=

so

⇤

= K.

We are now ready to prove the two lemmas.

Z
Z

Rd
Rd

✓Z

z(⇠)(

=K

⇤

x)(⇠)dµ(⇠)

z(⇠)z(⇠)⇤ xdµ(⇠)

Rd

◆
z(⇠)z(⇠)⇤ dµ(⇠) x = Kx

Random Fourier Features for Kernel Ridge Regression

Proof of Lemma 11. The minimizer of the right-hand side of (11) can be obtained from the usual normal equations, and
simplified using the matrix inversion lemma for operators (Ogawa, 1988):
p
y? =
p(⌘)( ⇤ + IL2 (dµ) ) 1 ⇤ z(⌘)
p
⇤
=
p(⌘) ⇤ (
+ In ) 1 z(⌘)
p
=
p(⌘) ⇤ (K + In ) 1 z(⌘) .
p
So, y ? (⇠) = p(⌘)z(⇠)⇤ (K + In ) 1 z(⌘). We now have
Z
ky ? k2L2 (dµ) = p(⌘)
|z(⇠)⇤ (K + In ) 1 z(⌘)|2 dµ(⇠)
Rd
Z
= p(⌘)
z(⌘)⇤ (K + In ) 1 z(⇠)z(⇠)⇤ (K + In ) 1 z(⌘)dµ(⇠)
Rd
✓Z
◆
= p(⌘)z(⌘)⇤ (K + In ) 1
z(⇠)z(⇠)⇤ dµ(⇠) (K + In ) 1 z(⌘)
=
=
=

Rd

⇤

1

K(K + In )

1

⇤

1

(K + In

In )(K + In )

⇤

1

p(⌘)z(⌘) (K + In )
p(⌘)z(⌘) (K + In )
p(⌘)z(⌘) (K + In )

z(⌘)

1

⇤

z(⌘)

z(⌘)
2

p(⌘)z(⌘) (K + In )

z(⌘)

and
k y?

p

p(⌘)z(⌘)k22

=
=

⇤
p(⌘)k
(K + In ) 1 z(⌘) z(⌘)k22
p(⌘)k(K(K + In ) 1 In )z(⌘)k22

=

p(⌘)k (K + In

=

p(⌘)k

=

2

In )(K + In )

(K + In )

1

1

In z(⌘)k22

z(⌘)k22

p(⌘)z(⌘)⇤ (K + In )

2

z(⌘) .

Now plugging these into (11) gives:
ky ? k2L2 (dµ) +

1

k y?

p

p(⌘)z(⌘)k22

= p(⌘)z(⌘)⇤ (K + In )

1

⇤

2

+ p(⌘)z(⌘) (K + In )
⇤

= p(⌘)z(⌘) (K + In )

z(⌘)

1

p(⌘)z(⌘)⇤ (K + In )

2

z(⌘)

z(⌘)

z(⌘)

= ⌧ (⌘)

Proof of Lemma 12. The optimization problem (11) can equivalently be reformulated as the following problem:
⌧ (⌘) = minimum
y 2 L2 (dµ);
subject to:

kyk2L2 (dµ) + kuk22

u 2 Cn
p
p
y+
u = p(⌘)z(⌘)

First we show that for any ↵ 2 Cn , the argument of the minimization problem in (12) is no bigger than ⌧ (⌘). That is
because for the optimal solution to above optimization, namely ū and ȳ, we have:
p
p
ȳ +
ū = p(⌘)z(⌘)

Random Fourier Features for Kernel Ridge Regression

hence,
|

p
p
p(⌘)↵⇤ z(⌘)| = |↵⇤ ( ȳ +
ū)|
p
⇤
⇤
= |↵ ȳ + ↵
ū|
p
⇤
⇤
 |↵ ȳ| + |↵
ū|
p
⇤
= |h↵, ȳiCn | + |↵
ū|
p
⇤
= |h ↵, ȳiL2 (dµ) | + |↵⇤ ū|
p
 k ⇤ ↵kL2 (dµ) · kȳkL2 (dµ) +
k↵⇤ k2 · kūk2

where the last inequality follows from Cauchy-Schwarz inequality (|↵⇤ ȳ| = |(↵⇤ ȳ)⇤ | = |( ȳ)⇤ ↵| =
|hȳ, ⇤ ↵iL2 (dµ) |  k ⇤ ↵kL2 (dµ) · kȳkL2 (dµ) ). By another use of Cauchy-Schwarz we have:
⇣
p(⌘)|↵⇤ z(⌘)|2  k
⇣
 k

therefore, for every ↵ 2 Cn ,

⌘2
p
↵kL2 (dµ) kȳkL2 (dµ) +
k↵⇤ k2 · kūk2
⌘ ⇣
⌘
⇤
↵k2L2 (dµ) + k↵⇤ k22 · kȳk2L2 (dµ) + kūk22
⇤

p(⌘)|↵⇤ z(⌘)|2
 kȳk2L2 (dµ) + kūk22 = ⌧ (⌘)
k ⇤ ↵k2L2 (dµ) + k↵k22

(18)

¯ =
Now it is enough to show that at the optimal ↵ the dual problem gives the leverage scores. We show that ↵
In ) 1 z(⌘) matches the leverage scores. First note that for any ↵ 2 Cn we have
k

⇤

↵k2L2 (dµ) + k↵k22 = h

⇤

↵,
⇤

=h

⇤

p

p(⌘)(K +

↵iL2 (dµ) + ↵⇤ ↵

↵, ↵iCn + ↵⇤ ↵

= hK↵, ↵iCn + ↵⇤ ↵
= ↵⇤ (K + In )↵
¯ =
Now by substituting ↵
k

⇤

p

p(⌘)(K + In )

1

z(⌘) we have:

¯ ⇤ z(⌘)|2
p(⌘)|↵
p(⌘)2 |z(⌘)⇤ (K + In ) 1 z(⌘)|2
=
2
2
⇤
¯ L2 (dµ) + k↵k
¯ 2
↵k
p(⌘)z(⌘) (K + In ) 1 (K + In )(K + In )
= p(⌘)|z(⌘)⇤ (K + In )

1

1 z(⌘)

z(⌘)|
(19)

= ⌧ (⌘)

F. Proof of Theorem 13
Recall from Lemma 11 that
⌧ (⌘) =

min

y2L2 (dµ)

1

k y

p

p(⌘)z(⌘)k22 + kyk2L2 (dµ)

(20)

To upper bound ⌧ (⌘) for any ⌘, we will exhibit some test function, y⌘ (·), and compute the quantity under the minimum.
y⌘ (·) will be a ‘softened spike function’ given by:
Definition 29 (Softened spike function). For any ⌘, and any u define:
p
2 2
p(⌘)
y⌘,u (t) =
· e (t ⌘) u /4 · v · sinc (v(t
p(t)
p
where v = 2(R + u 2 log n ).

⌘))

(21)

Random Fourier Features for Kernel Ridge Regression

The reweighted function g⌘,u (t) = p(t) · y⌘,u (t) is just a Gaussian with standard deviation ⇥(1/u) multiplied by a sinc
function with width Õ(1/(u + R)), both centered at ⌘. Taking the Fourier transform of this function yields a Gaussian with
standard deviation ⇥(u) convolved with a box of width Õ(u) + R. This width is wide enough such that when centered
between [ R, R] the box covers nearly all the mass of the Gaussian, and so the Fourier transform is nearly identically 1
on the range [ R, R]. Shifting by ⌘, means that it is very close to a pure cosine wave with frequency ⌘ on this range, and
hence makes the first term of (20) small. We make this argument formal below.
F.1. Bounding

1

p

k y⌘,u

p(⌘)z(⌘)k22

Lemma 30 (Test Function Fourier Transform Bound). For any integer n, every parameter 0 <
any kernel density function p(⌘) if xj 2 [ R, +R] for all j 2 [n] for any radius R > 0, then:
1

k y

p

p(⌘)z(⌘)k22 =

n
1X

p
p(⌘) · z(⌘)j

ĝ⌘,u (xj )

j=1

2

 p(⌘).

 n and every ⌘, u, and
(22)

where g⌘,u (t) ⌘ p(t)y⌘,u (t).
Proof. We have g⌘,u (t) = p(t)y⌘,u (t) = p(⌘)e (t
Z
p
ĝ⌘,u (xj ) = p(⌘) e
=
=
p

2

2

2

p
p

⌘)2 u2 /4

· v · sinc (v(t

2⇡itxj

e

(t ⌘)2 u2 /4

e

2⇡itxj

R

p(⌘)e

2⇡ixj ⌘

Z

R

e

⌘)) and ĝ⌘,u (xj ) = ( y)j . We thus have:

· v · sinc (v(t

t2 u2 /4

⌘)) dt

· v · sinc (vt) dt
(23)

p(⌘) · z(⌘)j · h(xj )

where h(x) = 2 u ⇡ e 4⇡ x /u ⇤rectv (x) by the fact that multiplication in time domain becomes convolution in the Fourier
p
2 2
2 2
2
domain (Claim 18), F(e t u /4 ) = 2 u ⇡ e 4⇡ x /u , and F(v · sinc (vt)) = rectv (x).
R 2p⇡ 4⇡2 x2 /u2
For any x, we have
= 1. Additionally, for any x 2 [ R, R] we have by Claim 25 and the fact
ph(x)  R u e
that v = 2R + 2u 2 log n :
Z x+ v2 p
2 ⇡ 4⇡2 x2 /u2
h(x) =
e
dx
v
u
x 2
p
Z 1
2 ⇡ 4⇡2 x2 /u2
1 2
e
dx
v/2 R u
2
2
2
1
u
p ·
1
e 4⇡ (v/2 R) /u
(by second part of Claim 25)
4 ⇡ v/2 R
p
1
1
1
·p
(since v = 2R + 2u 2 log n )
p p
n
4 ⇡ 2 log n
1
1 p
(by assumption n
2).
n
Plugging into (23) gives
ĝ⌘,u (xj )

p

p(⌘) · z(⌘)j

2

= p(⌘) |h(xj )


1|

2

p(⌘)
,
n

and so,
n
1 Xh
j=1

proving the claim.

ĝ(xj )

p

p(⌘) · z(⌘)j

i2

 n · p(⌘) ·

n

< p(⌘)

Random Fourier Features for Kernel Ridge Regression

F.2. Bounding ky⌘,u k2L2 (dµ)

p
Having established Lemma 30, we note that showing that the weighted Fourier transform of y⌘,u is close to p(⌘)z(⌘)
reduces to bounding the norm of the test function. To that effect, we show the following:
p
Lemma 31 (Test Function `2 Norm Bound). For any integer n, any parameter 0 <  n2 , every |⌘|  10 log n , and
every 2000 log n  u  500 log1.5 n , if y⌘,u (t) is defined as in (20), as per Definition 29, then we have
⇣
⌘
p
kyk2L2 (dµ)  12 R + u 2 log n

(24)

Before proving Lemma 31, we first prove a claim:
p
Claim 32. Suppose |⌘|  100 log n , and

p
p
c log n
c log n
t⌘+
b
b

⌘
for some absolute constant c > 0. If b

100c · log n then,
t2
2

e
Proof. Let

2

+ ⌘2

 3.

p
⌘. Then, note that | |  c log n /b, and so,

=t

e

t2
2

2

+ ⌘2

+⌘)2
2

(

=e

2
2

⌘

=e
 e|

2
2

⌘|

 e|

2

+ ⌘2

|·|⌘|

 e(c

p

p
log n /b)(100 log n )

 e  3,
since b

100c · log n .

Now, we are ready to prove Lemma 31:
Proof of Lemma 31. Recall that for the Gaussian kernel, we have p(⌘) =
Z

2

R

|y⌘,u (t)| dµ(t) = p(⌘)
=

p
+

For the integral over |t
Z

|t ⌘| 20

p

log n
u

e

t2 /2

⌘|
·e

20

p

Z p
R

2⇡et

2⇡p(⌘) · v
p

Z

/2

·e

⌘+
⌘

2⇡p(⌘) · v 2

log n
u

(t ⌘)2 u2 /2

2

2

Z

20

20

(t ⌘)2 u2 /2
p

p

log n
u

et

log n
u

|t ⌘|

20

p

2

log n
u

/2

2
p1 e ⌘ /2 .
2⇡

We calculate:
2

· v 2 (sinc (v(t

⌘))) dt

(t ⌘)2 u2 /2

(sinc (v(t

·e

et

2

/2

(t ⌘)2 u2 /2

·e

2

⌘))) dt
2

(sinc (v(t

⌘))) dt

(25)

we have:

(sinc (v(t

2

⌘))) dt 

1

p

v · 20
Z
1

v |t ⌘|

log n
u

20

p

2

log n
u

Z

|t ⌘| 20

et

2

/2

p

·e

log n
u

et

2

/2

(t ⌘)2 u2 /2

·e

dt

(t ⌘)2 u2 /2

dt

(26)

Random Fourier Features for Kernel Ridge Regression

The first inequality above is because by definition of sinc (·) we have the following for all |t
⌘)) |2 =

|sinc (v(t

sin2 (⇡v(t ⌘))
1


2
(⇡v(t ⌘))
(v(t ⌘))2
v·

p
20 log n
u

⌘|

:

1

p
20 log n
u

2

The last inequality in (26) is because of the following reason:
1

v·

p
20 log n
u

1
1
p
·
v v · 20 log n 2
u
p
1
1
⇣ 1.5 ⌘ (since v = 2(R + u 2 log n )
 ·
v 800 log n
=

2

2u

u



Now note that t2  2(t

1
v

t2  2(t

⌘)2 + 2⌘ 2

 2(t

⌘)2 + 200 log n

 2(t

⌘)2 + (t

2
(t
3

Z

⌘)2 u2 /2

p
20 log n
u

⌘|

:

p
(by the assumption |⌘|  10 log n )
p
20 log n
(by the assumption |t ⌘|
)
u

⌘)2 u2

where the last inequality follows from u
1
v

2 log n , see Definition 29)

(since u  500 log1.5 n )

⌘)2 + 2⌘ 2 . We have the following for all |t



p

|t ⌘| 20

p

log n
u

2000log n

et

2

/2

·e

600 (because n

(t ⌘)2 u2 /2

dt 


1
v

Z

1/2). Hence,

|t ⌘| 20

p

log n
u

e

(t ⌘)2 u2 /3

dt

1 100
·n
v

(27)

Now, the first integral in (25):
Z

⌘+20
⌘ 20

p

p

log n
u

log n
u

e

t2 /2

·e

(t ⌘)2 u2 /2

2

⌘))) dt  3e

(sinc (v(t

=

⌘2
2

⌘2
2

Z

R

(sinc (v(t

3e
.
v

2

⌘))) dt
(28)

where the inequality follows from Claim 32 with c = 20 and b = u, hsince, by assumption, u i2000 log n and |t| 
p
p
p
n
n
|⌘| + |t ⌘|  10 log n + 20
log n  100 log n whenever t 2 ⌘ 20 log
, ⌘ + 20 log
.
u
u
u
By incorporating (27) and (28) into (25) we have:
Z

2

R

where the last inequality uses that

|y⌘,u (t)| dt 
p

2⇡p(⌘) =

p

2⇡p(⌘) · v

p
2
p2⇡ e ⌘ /2
2⇡

2

⇣1

 1.

v

·n

100

⌘2
3e 2 ⌘
+
 6v
v

(29)

Random Fourier Features for Kernel Ridge Regression

Proof of Theorem 13. By the assumptions of the theorem n is an integer, parameter 0 <

 n/2, and R > 0, and all

⌘2
2

x1 , ..., xn 2 [ R, R] and p(⌘) =
, therefore all the preconditions of Lemmas 31, and 30 are satisfied and hence
the lemmas go through and the upper bounds in (22) and (24) hold true. The theorem follows by setting u = 2000log n
and then plugging upper bounds (22) and (24) into (20).
p1 e
2⇡

G. Proof of Theorem 14
With the choice of the Gaussian kernel with
that

we have p(⌘) = (2⇡)

1

= (2⇡)

⌧ (⌘) = maxn
↵2C

1/2

exp( ⌘ 2 /2). Recall from Lemma 12

p(⌘) · |↵⇤ z(⌘)|2
.
↵k2L2 (dµ) + k↵k22

(30)

⇤

k

In particular, this gives us a method of bounding the leverage scores from below, namely, by exhibiting some ↵ and
computing the quantity under the maximum.
The rest of this section is organized as follows. In Section G.1, we construct our candidate set of data points x1 , x2 , . . . , xn
along with the vector ↵. In particular, ↵ will be chosen to be a vector of samples of a function f ,b,v at each of the data
points. Section G.2 then describes basic Fourier properties of the function f ,b,v and ↵ that we will require later. The
remaining sections then bound each of the relevant quantities that appear in (30) for our specific choice of x1 , x2 , . . . , xn
and ↵. In particular, Section G.3 shows a lower bound for ↵⇤ z(⌘), while Section G.4 shows an upper bound for k↵k22 and
Section G.5 shows an upper bound for k ⇤ ↵k2L2 (dµ) .
G.1. Construction of Data Point Set and the Vector of Coefficients ↵
Definition 33. For parameters

, b > 0 and v > 0, let the function f
f

,b,v (x)

= 2 cos(2⇡ x)
= 2 cos(2⇡ x)

Lemma 34. For any
F (f

✓
Z

1
p
e
2⇡b
x+ v2
v
2

x

p

,b,v

be defined as follows:

(.)2 /2b2

1
e
2⇡b

=e

2⇡ 2 b2 (z

)2

Proof. Note that
F

✓

(v · sinc (v(z

1
p
e
2⇡b

(·)2 /2b2

)) + e

◆

(z) = e

⇤ rectv (x)

t2 /2b2

> 0, v > 0, and b > 0, if we define the function f

,b,v ) (z)

◆

,b,v

dt

as in Definition 33, then

2⇡ 2 b2 (z+ )2

2⇡ 2 b2 z 2

(v · sinc (v(z +

))).

.

Thus, by the convolution theorem (see Claim 18),
F

✓

1
p
e
2⇡b

(·)2 /2b2

◆

⇤ rectv (z) = e

2⇡ 2 b2 z 2

· v · sinc (v(z)) .

Now by the duality of phase shift in time domain and frequency shift in the Fourier domain,
F(f

,b,v )(z) = F

=F
=e

✓
✓

(e2⇡i
p

(·)

1
e
2⇡b

2⇡ 2 b2 (z

+e

2⇡i (·)

(·)2 /2b2
)2

)·

✓

p

◆
⇤ rectv (z

· v · sinc (v(z

1
e
2⇡b

(·)2 /2b2

)+F
)) + e

✓

⇤ rectv

p

1
e
2⇡b

2⇡ 2 b2 (z+ )2

◆◆

(z)

(·)2 /2b2

◆
⇤ rectv (z +

· v · sinc (v(z +

)) .

)

Random Fourier Features for Kernel Ridge Regression

Intuition for Theorem 14 If, instead of a discrete set of data points, we had a continuum of points, ↵ would be a
function (or, alternatively, an infinite-dimensional vector coresponding to the evaluation of the function on the continuum
of points). The intuition is that in this case, we would essentially like to choose ↵ to be the function f ,b,v for some
suitable choice of parameters , b, v. In this case, the computation of bounds for the various quantities appearing in (30)
would be relatively simple and involve bounding integrals. However, since our data points are actually discrete and ↵ is
finite-dimensional, we must instead choose ↵ to be the vector of samples of f ,b,v on the data points, and the bounds we
deduce require computing Fourier transforms of f ,b,v multiplied by suitable Dirac combs (see Lemma 36). Computation
of the necessary bounds is further complicated by the fact that the data points are bounded in [ R, R], which requires us
to truncate the aforementioned Dirac combs and have appropriate Fourier tail bounds (see Lemma 37).
Let us provide some intuition about the quantities |↵⇤ z(⌘)|2 , k ⇤ ↵k2L2 (dµ) and k↵k22 that arise in (30) along these lines.
If we have ⇡ 2R equally spaced data points between R and R, then note that the points are separated by distance ⇡ 1.
This approximately corresponds to dealing with the continuous case in which ↵ is a function f ,b,v and, therefore, sums in
the discrete case can be approximated by corresponding integrals over continuous functions. Suppose = ⌘ and v = R.
Note that the quantity ↵⇤ z(⇠) corresponds to
Z 1
↵⇤ z(⇠) ⇡
f⌘,b,R (x)e 2⇡i⇠x dx
1

⇡ F(f⌘,b,R )(⇠)
=e

2⇡ 2 b2 (⇠ ⌘)2

· R · sinc (R(⇠

⌘)) + e

2⇡ 2 b2 (⇠+⌘)2

(31)

· R · sinc (R(⇠ + ⌘)) .

Thus, ↵⇤ z(⇠) (which we bound rigorously in Section G.3) can be approximated as follows:
8⇡ 2 b2 ⌘ 2

↵⇤ z(⇠) ⇡ R(1 + e
where the last transition uses the fact that sinc (·)
in Section G.4) is roughly
k↵k22

⇡

Z

1

2

f⌘,b,R (x) dx =

1

(32)

sinc (2R⌘)) ⇡ ⌦(R),

1/4. Next, note that the quantity k↵k22 (which we bound rigorously
Z

⇡4

1

2

4 cos (2⇡⌘x)

1

Z

3R
2
3R
2

✓Z

1
1

p

1
2⇡b

Z

x+ R
2

2
2
1
p
e t /2b dt
R
2⇡b
x 2
◆2
2
2
e t /2b dt
dx

!2

dx

(33)

⇡ O(R).
Finally, note that k ⇤ ↵k2L2 (dµ) (which we bound rigorously in Section G.5) is roughly
Z 1
2
1
k ⇤ ↵k2L2 (dµ) ⇡
|↵⇤ z(⇠)|2 · p e ⇠ /2 d⇠
2⇡
1
Z 1⇣
⌘2
2 2
2
2 2
2
1
⇡
e 2⇡ b (⇠ ⌘) · R · sinc (R(⇠ ⌘)) + e 2⇡ b (⇠+⌘) · R · sinc (R(⇠ + ⌘)) · p e
2⇡
1
Z 1
2
1
2
⇡ p e ⌘ /2 R2
sinc (R(⇠ ⌘)) d⇠
2⇡
1
⇡ O(p(⌘)R),

⇠ 2 /2

d⇠

(34)

using (31).
Now, going back to the discrete case, consider what happens if we scale up the number of points from 2R to n, keeping the
points evenly spaced in the interval [ R, R]. In this case, the spacing between points decreases by a factor of ⇡ n/2R.
Thus, this corresponds to the measure of integration over R scaling up by a factor of . Hence, |↵⇤ z(⌘)| and k↵k22 can be
expected to scale up by a factor of , while k ⇤ ↵k2L2 (dµ) would scale up by a factor of 2 . Thus, along with (32), (33),
and (34), we get that
k

p(⌘) · |↵⇤ z(⌘)|2
⇡
↵k2L2 (dµ) + k↵k22

⇤

( R)2 p(⌘)
p(⌘)
p(⌘)
⇡R·
⇡R·
2 p(⌘)R +
R
p(⌘) + /
p(⌘) + 2Rn

1,

Random Fourier Features for Kernel Ridge Regression

which is within a constant factor of the expression in Theorem 14.
Definition 35 (Construction of data points and ↵). We first define the set of data points xj for j = 1, 2, . . . , n for odd n as
follows:
✓
◆
n+1
2R
xj = j
·
2
n
Thus, the data points are on a grid of width

2R
n

extending from

R to R.

The vector ↵ is chosen to be the tuple of evaluations of f⌘,b,v at the individual xj , for some parameters b, v, and ⌘. More
specifically, for 1  j  n, we define
↵j = f⌘,b,v (xj )

Z

= 2 cos(2⇡⌘xj )

G.2. Basic Properties of f

,b,v

xj + v2
v
2

xj

p

1
e
2⇡b

t2 /2b2

(35)

dt.

and ↵

By the Nyquist-Shannon sampling theorem, we have the following lemma.
Lemma 36. For any parameters
any w > 0,
0
1
X
F @f ,b,v (·) ·
(·
j= 1

> 0, v > 0, and b > 0, if we define the function f
1

jw)A (z) = w
+w

1

1
X

v

e

2⇡ 2 b2 (z jw

1

v

1
X

e

2⇡ 2 b2 (z jw

1

· sinc v(z

jw

1

+ )2

· sinc v(z

jw

1

j= 1

Proof. By the Nyquist-Shannon sampling theorem, we have
0
1
0
1
X
F @f ,b,v (·)
(· jw)A (z) = @w
j= 1

=

1

1
X

j= 1

1
X

(·

1

jw

j= 1

w

1

F(f

as in Definition 33, then for

)2

j= 1
1

,b,v

,b,v )(z

) ⇤ F(f
jw

1

,b,v )(·)

)

+

) .

1

A (z)

(36)

).

Thus, by Lemma 34, we find that (36) can be written as
1
X

j= 1

w

1

F(f

,b,v )(z

jw

1

)=w
+w

1

1

1
X

j= 1
1
X

j= 1

e

2⇡ 2 b2 (z jw

1

)2

· v · sinc v(z

jw

1

e

2⇡ 2 b2 (z jw

1

+ )2

· v · sinc v(z

jw

1

)
+

) ,

which completes the proof.
Lemma 37. For every odd integer n 3 and parameters 0 <  n2 , ⌘ > 0, v  R, and b 
function f ,b,v as in Definition 33, then
0
1
✓
◆ ✓
◆
X
p
2R
2R
A (z) 
F@
f⌘,b,v j ·
·
· j·
n
n
n
n
|j|> 2

for all z.

pR
4 log n

, if we define the

Random Fourier Features for Kernel Ridge Regression

Proof. By definition of f⌘,b,v , we have the following for all x:

|f⌘,b,v (x)| 
Therefore, if |j| >

n
2,

Z

x+ v2
x

✓

2R
j·
n

◆






where we have used the fact that j ·
we have

F@

p

2
e
2⇡b

t2 /2b2

dt.

then

f⌘,b,v

0

v
2

X

f⌘,b,v

|j|> n
2

✓

2R
n

v
2

2R
j·
n

◆

j·

·

✓

·

2R
n

R
2

Z 1
2
2
2
p
e t /2b dt
2R
v
2⇡b j· n 2
Z 1
2
2
2
p
e t /2b dt
2⇡b jR
n
jR 2
1
2
nb
p ·
· e 2 ·( nb )
2⇡ jR
jR 2
1
2b
· e 2 ·( nb ) ,
R
j·

R
n,

(37)

along with Claim 25. Therefore, again using Claim 25,

1
◆
✓
◆
X
2R A
2R
j·
(z) 
f⌘,b,v j ·
n
n
n
|j|> 2



X 2b
·e
R
n

|j|> 2

2b

·
R

nb
R

Z

1
2·

( jR
nb )

1
(n 1)R
2nb

2

e

t2 /2

dt

!

Z 1
2
n
· p
e t /2 dt
4 log n
log n
p
2
1
n
1

·p
· e 2 ·( log n )
4 log n
log n
p
1

·
n
4 log3/2 (n )
p

n,


since n

3, R

p
4b log n , and

 n/2.

Lemma 38. For every odd integer n 3, any parameter 0 <  n2 , every frequency ⌘ and ⇠, and any parameter v  R
R
and b  4plog
, if ↵ is defined as in (35) of Definition 35, then we have,
n
⇤

↵ z(⇠)

1 ✓
nv X
e
2R j= 1
p

n.

2⇡ 2 b2 (⇠

jn
2R

⌘)2

✓

sinc v(⇠

jn
2R

◆

⌘) + e

2⇡ 2 b2 (⇠

jn
2
2R +⌘)

✓

sinc v(⇠

jn
+ ⌘)
2R

◆◆
(38)

Random Fourier Features for Kernel Ridge Regression

Proof. Note that

↵⇤ z(⇠) =

n
X

2⇡ixj ⇠

↵j e

j=1

=

X

|j| n
2

0

f⌘,b,v (2Rj/n) · e

2⇡i( 2Rj
n )⇠

1
◆
2Rj
A (⇠)
=F@
f⌘,b,v (2Rj/n) ·
·
n
n
|j| 2
0
1
0
✓
◆
✓
◆ ✓
1
X
X
2Rj
2Rj
@
A
@
=F
f⌘,b,v (·) ·
·
(⇠) F
f⌘,b,v
·
·
n
n
n
j= 1
✓

X

|j|> 2

1
◆
2Rj A
(⇠).
n

(39)

By Lemma 36 (applied with w = 2R/n), we have the following expression for the first term in (39):
0

F@

1
X

j= 1

✓

f⌘,b,v (·) ·

1
◆
1
2Rj A
nv X
(⇠) =
e
n
2R j= 1

·

jn
2R

2⇡ 2 b2 (⇠

1
nv X
+
e
2R j= 1

2⇡ 2 b2 (⇠

⌘)2

✓ ✓
· sinc v ⇠

jn
2
2R +⌘)

✓ ✓
· sinc v ⇠

jn
2R

⌘

◆◆

jn
+⌘
2R

◆◆

.

(40)

p
4b log n and v  R, it follows from Lemma 37 that the second term in (39) can be

Now, by the assumption that R
bounded as

0

F@

X

|j|> n
2

f⌘,b,v (2Rj/n) ·

✓

◆

1

p
2Rj A
(⇠) 
n.
n

·

(41)

Thus, the desired result follows by combining (39), (40), and (41).
G.3. Bounding ↵⇤ z(⌘)
Lemma 39. For every odd integer n
R
parameter v  R and 2p
b p
n
4

v 2
17, any parameter 0 <  ( R
) · n/16, every frequency |⌘| 
if ↵ is defined as in (35) of Definition 35, then we have

R
,
log(n )

|↵⇤ z(⌘)|

Proof. Since v  R and b  p
4

↵⇤ z(⌘)

R
log(n )

1
nv X ⇣
e
2R j= 1
p

n.

and

2⇡ 2 b2 (

n
10R ,

and any

nv
.
5R

 n/2, Lemma 38 implies that

jn 2
2R )

sinc (v( jn/2R)) + e

2⇡ 2 b2 (2⌘

jn 2
2R )

sinc (v(2⌘

jn/2R))

⌘
(42)

Random Fourier Features for Kernel Ridge Regression

Hence,
|↵⇤ z(⌘)|

1 ⇣
X

nv
2R

2⇡ 2 b2 (

e

jn 2
2R )

sinc (v( jn/2R))

j= 1

⌘
p
sinc (v(2⌘ jn/2R))
n
nv 2⇡2 b2 (0)2
nv 2⇡2 b2 (2⌘)2
e
sinc (v(0)) +
e
sinc (v(2⌘))
2R
2R
⇣
⌘ p
X
jn 2
jn 2
2 2
2 2
nv
e 2⇡ b ( 2R ) + e 2⇡ b (2⌘ 2R )
n
2R
|j| 1
⌘ p
jn 2
2 2
3 ⇣ nv ⌘ nv X ⇣ 2⇡2 b2 ( jn )2
2R
e
+ e 2⇡ b (2⌘ 2R )
n,
4 2R
2R
+e

jn 2
2R )

2⇡ 2 b2 (2⌘

(43)

|j| 1

since |sinc (·) |  1 and sinc (·)
P
2 2
Now we show that |j| 1 e 2⇡ b (
2⇡ 2 b2 (

e
have

jn 2
2R )

e

jn

for all |j|
X

e

1
4.
jn 2
2R )

2⇡ 2 b2 (2⌘

+e

jn 2
2R )

1. Also recall that |⌘| 

2⇡ 2 b2 (

jn 2
2R )

+e

is small. Note that by the assumption of b

n
10R ,

2⇡ 2 b2 (2⌘

and so, (2⌘

jn 2
2R )

|j| 1

by assumption n

jn 2
2R )

X



e

|j|n

jn 2
( 4R
)

+e

|j| 1

 5e

n
4

for all |j|

R
p
,
2 n

we have

1. Hence, we

|j|n
4

(44)

17. The lemma follows by combining (43) and (44).

G.4. Bounding k↵k22

Lemma 40. For every odd integer n and parameters b, ⌘, v > 0, if ↵ is defined as in (35) of Definition 35, then we have
k↵k22  4n.
Now we are ready for the proof of Lemma 40.
Proof of Lemma 40. Let w = 2R/n. Then, we observe that
k↵k22 =




n
X

↵j2

j=1

X

|j|

n

1

2
p
cos(2⇡jw⌘)
2⇡b

Z

2
p
cos(2⇡jw⌘)
2⇡b

Z

2

X ✓

|j| n 2 1

X ✓

|j| n 2 1

p

2
2⇡b

Z

1

e

1

because | cos(·)|  1. Hence,
k↵k22 

X

|j| n 2 1

= 4n
as desired.

x2 /2b2

jw+ v2
jw
1
1

e

x2 /2b2

v
2

e

x2 /2b2

!2

◆2

◆2

4
(45)

Random Fourier Features for Kernel Ridge Regression

G.5. Bounding k

⇤

↵k2L2 (dµ)

Note that all the results so far hold for any kernel p(⌘) and are independent of the kernel function. Now, we upper bound
k ⇤ ↵kL2 (dµ) . This quantity depends on the particular choice of kernel, which is assumed to be Gaussian.
p
Lemma 41. For every odd integer n
17, any parameter 10
 n2 , every |⌘|  100 log n , and any
n <
R
R
1000 log1.5 n  R  500pnlog n , and 2p
 b  4plog
, if ↵ is defined as in (35) of Definition 35 with paramen
n
ter v = R, then for the Gaussian kernel with p(⇠) =

⇤

k

2
p1 e ⇠ /2 ,
2⇡

↵k2L2 (dµ)  6

we have:

n2
· p(⌘) + 3 n.
R

(46)

Proof. Recall from Lemma 38 that:

|↵⇤ z(⇠)|2 

1 ✓
nv X
e
2R j= 1

+e
1 ✓
X

n2

2

⇤

↵k2L2 (dµ) 

e



1

+

= 2n2

Z
Z

⇤

0

2

1

+1

2⇡ 2 b2 (⇠

1
n @ X
e
2 j= 1

+1

+e
Z

jn
2R

⌘)2

✓
sinc v(⇠

jn
2
2R +⌘)

⌘)2

jn
2
2R +⌘)

2⇡ 2 b2 (⇠

0

1
X

n2 @

e

jn
2R

R1

1

⌘)2

2⇡ b (⇠

⌘)

2

✓
sinc v(⇠

1
1
1

0

n2 @

0
@

1
X

e

2⇡ 2 b2 (⇠

sinc v(⇠

j= 1

e

2 2

2⇡ b (⇠

jn
2R

jn
2
2R +⌘)

⌘)

2

◆

jn
+ ⌘)
2R

jn
2R

◆!

jn
2R

sinc v(⇠
✓

sinc v(⇠

⌘)

2

+2

2

⇣p

⌘2

.

⌘2

p(⇠) d⇠

n

◆

p(⇠) d⇠ +

✓

j= 1

1
X

⌘)

◆! 2

✓

j= 1

+1

◆

|↵⇤ z(⇠)|2 p(⇠) d⇠, and so, we have

jn
+ ⌘)
2R

sinc v(⇠

jn
2R

✓

sinc v(⇠

✓

2 2

jn
2R

sinc v(⇠

jn
2
2R +⌘)

⌘)

!
◆
p
jn
+ ⌘) +
n
2R

sinc v(⇠
✓

↵k2L2 (dµ) =

2⇡ 2 b2 (⇠

jn
2R

✓

j= 1

Now, by the definition of the L2 (dµ) norm, k

k

2⇡ 2 b2 (⇠

2⇡ 2 b2 (⇠

+e

Z

jn
2R

2⇡ 2 b2 (⇠

jn
2R

◆

Z

1

12

2

1

⇣p

n

⌘) A p(⇠) d⇠
◆

12

jn
+ ⌘) A p(⇠) d⇠ + 2 n
2R
◆

12

⌘) A p(⇠) d⇠ + 2 n,

(47)

where we have used the inequality (a1 + a2 )2  2a21 + 2a22 in the second step, and the last equality occurs because the
kernel probability distribution function p(⇠) is symmetric in our case, along with the fact that the underlying sum is over

Random Fourier Features for Kernel Ridge Regression

all j. Now, the integral in (47) can be split into two integrals as follows:
Z

1
1

0

p(⇠) @

1
X

jn
2R

2⇡ 2 (⇠

e

⌘)2 b2

j= 1

=

Z
+

p(⇠) @

p
10 log n

Z

1
X

2⇡ (⇠

✓

2 2

⌘) b

p

|⇠| 10 log n

p(⇠) @

1
X

e

jn
2R

2⇡ 2 (⇠

◆

jn
2R

· sinc v(⇠

j= 1

0

12

⌘) A d⇠

jn
2R

2

e

◆

jn
2R

· sinc v(⇠

0

p
10 log n

✓

⌘) A d⇠

✓

⌘)2 b2

◆

jn
2R

· sinc v(⇠

j= 1

12

12

⌘) A d⇠.

(48)

p
p
First, we consider
log n , and hence,
p the case in which |⇠|  10 log n1 . nBy the assumption of the lemma, |⌘|  100
pn
|⇠ ⌘|  110 log n . This implies that |⇠ ⌘|  2 ( 2R ), since we are assuming that R 
. Therefore, for
500

any integer j 6= 0,

e

2⇡ 2 (⇠

jn
2R

⌘)2 b2

e

e

2⇡ 2 (⇠

jn
2R

⌘)2 b2



2 2
( jn
R ) b

log(n/ )

.

Hence, we have
X

|j| 1



X

|j| 1

X

R
p
2 n

and n

n 2 2
j( R
) b

e

|j| 1

 3e
where we used assumptions b

2 2
( jn
R ) b

e

n/4

(49)

,

17.

Now, using (49), we see that the first integral in (48) can be bounded as follows:
Z

p
10 log n
p
10 log n

2

Z

0

p(⇠) @

2

Z

e

p
10 log n

Z

2⇡ (⇠

⇣
p(⇠) e

p
10 log n
p

10 log n

p
10 log n
p

10 log n
1

jn
2R

2

j= 1
p
10 log n

+2
Z

1
X

2 2

⌘) b

2⇡ 2 b2 (⇠ ⌘)2

0

p(⇠) @

X

e

✓

jn
2R

· sinc v(⇠

sinc (v(⇠

2⇡ 2 (⇠

jn
2R

⌘))
⌘)2 b2

2

⌘2

⇠ 2 /2

⇣

e

2⇡ 2 b2 (⇠ ⌘)2

⇠ 2 /2

 3e

⌘ 2 /2

for |⇠

⌘| 

sinc (v(⇠

p
10 log n
b

⌘) A d⇠

d⇠
✓

jn
2R

2

⌘)) + 9e

2
2
2
1
p e ⇠ /2 e b (⇠ ⌘) · sinc (v(⇠ ⌘))2 d⇠ + 18
=2
2⇡
1
Z 1
2
2
2
1
p e ⇠ /2 e b (⇠ ⌘) · sinc (v(⇠ ⌘))2 d⇠ + 18e

2⇡
1

Next, by Claim 32, we have e

12

sinc v(⇠

|j| 1

1
p e
2⇡

◆

. Hence,

Z

1
1

n/2

.

◆

12

⌘) A d⇠

n/2

⌘

1
p e
2⇡

d⇠
⇠ 2 /2

e

n/2

d⇠
(50)

Random Fourier Features for Kernel Ridge Regression

Z

⌘+

10

10

⌘

p

p

log n
b

1
p e
2⇡

log n
b

1
3· p e
2⇡
1
3· p e
2⇡
3p(⌘)
=
v

⇠ 2 /2

⌘ 2 /2

2

⌘ /2

Z
Z

e

b2 (⇠ ⌘)2

+1

e

2

· sinc (v(⇠

b2 (⇠ ⌘)2

1
+1

⌘)) d⇠
2

· sinc (v(⇠

⌘)) d⇠

2

sinc (v(⇠

⌘)) d⇠

1

(51)

Note that the last line follows from the fact that v ·sinc (v⌘) is the Fourier transform of rectv (x), and so, by the convolution
theorem,
Z 1
(v · sinc (vx))2 dx = (rectv (x) ⇤ rectv (x)) (0)
1

= v.

Moreover,
Z

|⇠ ⌘|

10

p

log n
b

1
p e
2⇡

⇠ 2 /2

e

b2 (⇠ ⌘)2

2

· sinc (v(⇠

⌘)) d⇠ 
=

✓ ◆50 Z
n

✓ ◆50
n

1
1

1
p e
2⇡

⇠ 2 /2

d⇠
(52)

,

since the sinc (·) function is bounded by 1 in absolute value. Thus, (50), (51), and (52) imply that
Z

p
10 log n
p
10 log n

p(⇠)

1
⇣ X

e

2⇡ 2 (⇠

jn
2R

⌘)2 b2

j= 1

✓

jn
2R

· sinc v(⇠

⌘)

◆⌘

2

3p(⌘)
d⇠ 
+
v

✓ ◆50
n

+ 18e

n/2

.

(53)

p
Next, we bound the second integral in (48). Consider ⇠ satisfying |⇠| 10 log n . Note that the following upper bound
holds for any ⇠ and ⌘:
✓
◆
1
1
X
X
jn
jn
2
2 2
jn
2⇡ 2 (⇠ 2R
⌘)2 b2
e
· sinc v(⇠
⌘) 
e 2⇡ (⇠ 2R ⌘) b
2R
j= 1
j= 1
X
jn
2
2 2
=1+
e 2⇡ (⇠ 2R ⌘) b
|j| 1

1+
where we have used the fact that
Z

b
R

p
|⇠| 10 log n

2

Z



n

✓



p 1
4 log n

0

p(⇠) @

 2,

p

Z

1

e

2⇡ 2 (⇠ x ⌘)2 b2

1

1
X

e

2

2⇡ (⇠

jn
2R

2 2

⌘) b

✓

sinc v(⇠

p(⇠) d⇠

,

by Claim 25. Combining (47), (48), (53), and (55) now yields the desired result.

jn
2R

dx
(54)

 1/ 2. Thus,

j= 1

p
|⇠| 10 log n
◆50

2R
n

◆

12

⌘) A d⇠

(55)

Random Fourier Features for Kernel Ridge Regression

Proof of Theorem 14. Note that we can choose data points x1 , x2 , . . . , xn and the vector ↵ according to the construction
R
in Definition 35 with v = R and b = 4plog
. Thus, Lemmas 39, 40, and 41, as well as (30), imply that
n
⌧ (⌘)

k

p(⌘) · |↵⇤ z(⌘)|2
↵k2L2 (dµ) + k↵k22

⇤

n 2
5

p(⌘) ·

2

6 nR · p(⌘) + 3 n + (4n)
✓
◆
R
p(⌘)
,
150 p(⌘) + 2Rn 1
as desired.

H. Proof of Corollary 15
p
First claim of the corollary (upper bound on statistical dimension): Let t = 10 log n . We have:
Z
Z
Z
s (K) =
⌧ (⌘)d⌘ =
⌧ (⌘)d⌘ +
⌧ (⌘)d⌘
R

[ 1, t][[t,1]

[ t,t]

By the naive bound in Proposition 4 and Claim 25 we have:
Z
Z
⌧ (⌘)d⌘  n
[ 1, t][[t,1]

2

[ 1, t][[t,1]

e

n ·

t2 /2

t

!

e ⌘ /2
p
d⌘
2⇡

(56)

1

p
Further, by the more refined bound of Theorem 13, for any ⌘  10 log n = t we have
Z
Z
⌧ (⌘)d⌘ 
25(R + 3000 log1.5 n ) d⌘
[ t,t]

[ t,t]

 50t · (R + 3000 log1.5 n )
⇣p
⌘
=O
log n · R + log2 n .

Combining (56) and (57) gives the lemma.

Second claim of the corollary: Note that for all |⌘| 

⌧ (⌘)

therefore,

p

2 log

n
R

n
R

we have p(⌘)

p R
2⇡n

, hence we have:

p(⌘) + 2R/n  7p(⌘)

hence, by Theorem 14, we have:

And for |⌘| >

p
2 log

(57)

R
150

we have:

✓ ◆
1
7

p(⌘) + 2R/n  3R/n
s (K) =

Z

1

⌧ (⌘)d⌘

1

Z p2 log nR

R
d⌘ +
1050

p
n
2 log R
r
n
= ⌦(R log
)
R

Z

|⌘|>

p

2 log

n
R

R
150

✓

p(⌘)
3R/n

◆

d⌘
(58)

Random Fourier Features for Kernel Ridge Regression

I. Proof of Theorem 8 and 10
Proof of Theorem 8. We show a lower bound on the number of samples required under the random feature map of Rahimi
and Recht by exhibiting a set of data points for which the appropriate number of samples does not suffice.
Our goal is to show that if we take s samples ⌘1 , ⌘2 , . . . , ⌘s from the distribution defined by p, for s too small, then there is
an ↵ = (↵1 , ↵2 , . . . , ↵n ) 2 Rn such that with at least constant probability,
↵T (K + In )↵ <

2 T
↵ (ZZ⇤ + In )↵.
3

(59)

By (3), we have
↵T K↵ =

X
j,k

=

↵j ↵k · k(xj , xk )

XZ
j,k

=

=

Z
Z

1
1
1

1

2⇡i⌘(xj

e

xk )

1

0
@

n
X

↵j e

j=1

p(⌘)

1

n
X

↵j ↵k p(⌘) d⌘

1

2⇡i⌘xj A

n
X

↵k e

2⇡i⌘xk

k=1
2

↵j e2⇡i⌘xj

!

p(⌘) d⌘

d⌘.

j=1

Also, by the definition of Z and ' (see Section 2.2), we have

T

⇤

↵ ZZ ↵ =

n
X

2

↵j '(xj )

j=1

=

2
2

s X
n
X

k=1

1
↵j · p e2⇡i⌘k xj
s
j=1

s
n
1X X
=
↵j e2⇡i⌘k xj
s
j=1

2

,

k=1

where ⌘1 , ⌘2 , . . . , ⌘s are the s samples from the distribution given by p. Hence, (59) is equivalent to
Z

1
1

p(⌘)

n
X

2

↵j e2⇡i⌘xj

j=1

s
n
1
2 1X X
2
d⌘ +
k↵k2 < ·
↵j e2⇡i⌘k xj
3
3 s
j=1

2

.

(60)

k=1

We again use the same construction of n data points x1 , x2 , . . . , xn 2 R, according to the construction in Definition 33.
Moreover, we define ⌘ ⇤ to be
⌘ ⇤ = max |⌘j |
1js

and let ↵ = (↵1 , ↵2 , . . . , ↵n ) be given by
↵j = f⌘⇤ ,b,v (xj ),
p
where b = R/4 log n and v = . We will show that this choice of data points and ↵ satisfy (60).
First, we upper bound the first term on the left side of (60). Note that by Claim 26, with probability at least 1/2 over the

Random Fourier Features for Kernel Ridge Regression

samples z1 , z2 , . . . , zs , we have
Z

1

p(⌘)

1

n
X

2

↵j e

2⇡i⌘xj

d⌘ =

j=1

Z

=k

1
1
⇤

1
p e
2⇡

⌘ 2 /2

m
X

2

↵j e

2⇡i⌘xj

d⌘

j= m

↵k2L2 (dµ)

6n2
· p(⌘ ⇤ ) + 3 n
R
p
48n2
log s

·
+ 3 n.
R
s


(61)

where we have let ⌘ = ⌘ ⇤ and applied Lemma 41.
Next, we bound the right side of (60) from below. Note that
s
n
1X X
↵j e2⇡i⌘k xj
s
j=1

2

n
⇤
1 X
↵j e2⇡i⌘ xj
s j=1

k=1

=

by Lemma 39 applied with ⌘ = ⌘ ⇤ .

2

1 ⇤ ⇤ 2
(↵ z(⌘ ))
s
1 ⇣ n ⌘2
n2
=
,
s 5
25s

(62)

We also require the following estimate of k↵k22 , which is provided by Lemma 40:
k↵k22  4n.

(63)

Note that by combining (61), (62), and (63), we have that with probability at least 1/2,
Z

1
1

p(⌘)

n
X

2

↵j e

2⇡i⌘xj

j=1

1
48n2
d⌘ +
k↵k22 
·
3
R


p

log s
4
+3 n+
n
s
3

2n2
75s

s
n
2 1X X
 ·
↵j e2⇡i⌘k xj
3 s
j=1

2

,

k=1

since s  n /400 and also because R

3000 log1.5 (n ). This completes the proof.

Proof of Theorem 10. By the assumptions of the theorem n is an integer, parameter 0 <
⌘2
2

 n/2, and R > 0, and all

x1 , ..., xn 2 [ R, R] and p(⌘) =
, therefore all the preconditions of Proposition 4, and Theorem 13 are satisfied
and hence the theorem and proposition go through and for every ⌘ we have:
p1 e
2⇡

⌧ (⌘)  ⌧¯R (⌘)
Hence applying Lemma 6 with ⌧˜(⌘) = ⌧¯R (⌘) gives the desired spectral approximation with 83
p
R
ples where s⌧¯R = R ⌧¯R (⌘)d⌘. Now we show that s⌧¯R = O(R log(n ) + log2 n ).
p
Let t = 10 log n . We have:
Z
Z
Z
s⌧¯R =
⌧¯R (⌘)d⌘ =
⌧¯R (⌘)d⌘ +
⌧¯R (⌘)d⌘
R

[ t,t]

[ 1, t][[t,1]

2

s⌧¯R ln(16s⌧¯R /⇢) sam-

Random Fourier Features for Kernel Ridge Regression

By Definition 9 and Claim 25 we have:
Z

⌧¯R (⌘)d⌘ = n
[ 1, t][[t,1]

Z

n ·
n ·
p
Furthermore, for any ⌘  10 log n = t we have
Z
Z
⌧ (⌘)d⌘ 
[ t,t]

Combining the bounds above gives the result.

2

[ 1, t][[t,1]

!
2
e t /2
2p
2⇡t
!
2
e t /2
t

e ⌘ /2
p
d⌘
2⇡

1

25(R + 3000 log1.5 n ) d⌘
[ t,t]

 50t · (R + 3000 log1.5 n )
⇣p
⌘
=O
log n · R + log2 n .

Sampling from p̄R (·): Sampling from p̄R (·) amounts to sampling from a mixture of the uniform distribution on
p
p
log1.5 n )
[ 10 log(n ), +10 log(n )] and from the tail of the Gaussian distribution: with probability 25 max(R,3000
·
s⌧¯R
p
20 log(n ) sample from the uniform distribution and with remaining probability sample from the tail of the Gaussian.
Sampling from the tail of the Gaussian can be easily accomplished via rejection sampling at unit expected cost. Indeed, we
only need to generate a sample from the tail with probability proportional to the mass of the tail. On the other hand, once
we do, the expected cost of obtaining a sample via rejection sampling is inversely proportional to the amount of mass in
the tail, leading to unit cost in expectation.

