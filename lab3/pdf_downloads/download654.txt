Dual Supervised Learning (Supplementary Document)

Yingce Xia 1 Tao Qin 2 Wei Chen 2 Jiang Bian 2 Nenghai Yu 1 Tie-Yan Liu 2

A. Theoretical Analysis
As we know, the final goal of the dual learning is to give
correct predictions for the unseen test data. That is to say,
we want to minimize the (expected) risk of the dual models,
which is defined as follows1 :


`1 (f (x), y) + `2 (g(y), x)
, ∀f ∈ F, g ∈ G,
R(f, g) = E
2
where F = {f (x; θxy ); θxy ∈ Θxy }, G = {g(x; θyx ); θyx ∈
Θyx }, Θxy and Θyx are parameter spaces, and the E is taken over the underlying distribution P . Besides, let D denote the product space of the two models satisfying probabilistic duality, i.e., the constraint in Eqn.(4). For ease of
reference, define Hdual as (F × G) ∩ D.
Define the empirical risk on the n sample as follows: for
any f ∈ F, g ∈ G,
1 Xn `1 (f (xi ), yi ) + `2 (g(yi ), xi )
.
Rn (f, g) =
i=1
n
2
Following (Bartlett & Mendelson, 2002), we introduce
Rademacher complexity for dual supervised learning, a
measure for the complexity of the hypothesis.
Definition 1. Define the Rademacher complexity of DSL,
RDSL
n , as follows:
RDSL
n

n
1 X
i
sup 
= E
σi `1 (f (xi ), yi )+`2 (g(yi ), xi )  ,
z,σ (f,g)∈H
n i=1
dual

h

where z = {z1 , z2 , · · · , zn } ∼ P n , zi = (xi , yi ) in which
xi ∈ X and yi ∈ Y, σ = {σ1 , · · · , σm } are i.i.d sampled
with P (σi = 1) = P (σi = −1) = 0.5.
Based on RDSL
n , we have the following theorem for dual
supervised learning:
1

School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China
2
Microsoft Research, Beijing, China. Correspondence to: Tao
Qin <taoqin@microsoft.com>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
1
The parameters θxy and θyx in the dual models will be omitted when the context is clear.

Theorem 1 ((Mohri et al., 2012)). Let 21 `1 (f (x), y) +
1
2 `2 (g(y), x) be a mapping from X × Y to [0, 1]. Then, for
any δ ∈ (0, 1), with probability at least 1 − δ, the following
inequality holds for any (f, g) ∈ Hdual ,
r
1
1
DSL
ln( ). (1)
R(f, g) ≤ Rn (f, g) + 2Rn +
2n
δ
Similarly, we define the Rademacher complexity for the standard supervised learning RSL
n under our framework by
replacing the Hdual in Definition 1 by F × G. With probability at least 1−δ, the generationq
error bound of supervised

learning is smaller than 2RSL
n +

1
2n

ln( 1δ ).

Since Hdual ∈ F × G, by the definition of Rademacher
≤ RSL
complexity, we have RDSL
n . Therefore, DSL enjoys
n
a smaller generation error bound than supervised learning.
The approximation of dual supervised learning is defined
as
∗
R(fF∗ , gF
) − R∗
(2)
in which
∗
R(fF∗ , gF
) = inf R(f, g), s.t. (f, g) ∈ Hdual ;

R∗ = inf R(f, g).
The approximation error for supervised learning is similarly defined.
Define Py|x = {P (y|x; θxy )|θxy ∈ Θxy },
∗
∗
Px|y = {P (x|y; θyx )|θyx ∈ Θyx }. Let Py|x
and Px|y
denote the two conditional probabilities derived from P . We
have the following theorem:
∗
∗
Theorem 2. If Py|x
∈ Py|x and Px|y
∈ Px|y , then supervised learning and DSL has the same approximation error.

Proof. By definition, we can verify both of the two approximation errors are zero.

B. Details about the Language Models for
Marginal Distributions
We use the LSTM language models (Sundermeyer et al.,
2012; Mikolov et al., 2010) to characterize
QTxthe marginal
distribution of a sentence x, defined as i=1
P (xi |x<i ),

Dual Supervised Learning

where xi is the i-th word in x, Tx denotes the number of
words in x, and the index < i indicates {1, 2, · · · , i − 1}.
The embedding dimension and hidden node are both 1024.
We apply 0.5 dropout to the input embedding and the last
hidden layer before softmax. The validation perplexities
of the language models are shown in Table 1, where the
validation sets are the same as those for machine translation
tasks.
Table 1. Validation Perplexities of Language Models

En↔Fr
En
Fr
88.72 58.90

En↔De
En
De
101.44 90.54

En↔Zh
En
Zh
70.11 113.43

As shown in Table 1, the perplexities of different language
models vary a lot, but out DSL can make improvements
on all the translation tasks (Please refer to Table 1 of the
main text). This shows that DSL is not very sensitive to the
qualities of the two marginal distributions.
For the marginal distributions for sentences of sentiment
classification, we choose the LSTM language model again
like those for machine translation applications. The two
differences are: (i) the vocabulary size is 10000; (ii) the
word embedding dimension is 500. The perplexity of this
language model is 58.74.

References
Bartlett, Peter L and Mendelson, Shahar. Rademacher and
gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3(Nov):
463–482, 2002.
Mikolov, Tomas, Karafiát, Martin, Burget, Lukas, Cernockỳ, Jan, and Khudanpur, Sanjeev. Recurrent neural network based language model. In Interspeech, volume 2, pp. 3, 2010.
Mohri, Mehryar, Rostamizadeh, Afshin, and Talwalkar,
Ameet. Foundations of machine learning. MIT press,
2012.
Sundermeyer, Martin, Schlüter, Ralf, and Ney, Hermann.
Lstm neural networks for language modeling. In Interspeech, pp. 194–197, 2012.

