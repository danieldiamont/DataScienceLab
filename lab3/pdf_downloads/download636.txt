Variational Policy for Guiding Point Processes

A. Derivations of the Optimal Measure
The problem of finding the optimal measure is as follows:


min EQ [S(x)] + DKL (Q||P) , s.t.
Q

Z

dQ = 1

(18)

The minimum in (18) is attained at optimal measure Q⇤ given by:
exp( 1 S(x))
dQ⇤
=
dP
EP [exp( 1 S(x))]

(19)

Next, we show the derivations of (19), which contain two parts. First, we will show the following inequality:
✓ h
log EP exp

1

S(x)

i◆


6 EQ [S(x)] + DKL (Q||P)

(20)

The second part is to show the minimum of the above inequality is reached at (19).
To prove the first part, we first express EP in the left-hand-side of (20) as a function of the expectation EQ . More specifically,
we have:
✓ h
✓Z
◆
i◆
1
1
log EP exp
S(x)
= log
exp
S(x) dP
(21)
✓Z
◆
1
dP
= log
exp
S(x)
dQ
(22)
dQ
✓
◆
Z
1
dP
> log exp
S(x)
dQ
(23)
dQ
where (23) is due to the Jensen’s inequality that puts the log operator inside the integral. The measure P is absolute
dP
continuous with respect to Q, hence the derivative dQ
exists.
Moreover, using the property that log(ab) = log a + log b and log(1/a) =
inequality can be written as:
Z

log

✓

exp

1

log a, the right-hand-side of the above

◆
◆
Z ✓
dP
1
dP
dQ =
S(x) + log
dQ
dQ
dQ
Z
Z
1
dP
=
S(x)dQ + log
dQ
dQ
Z
Z
1
dQ
=
S(x)dQ
log
dQ
dP
1
=
EQ [S(x)] DKL (Q||P)

S(x)

(24)

Hence, combining (23) and (24), we have:
✓ h
log EP exp
Finally, since

1

S(x)

> 0, multiply both sides of (25) by
✓

h

log EP exp

i◆

>

1

EQ [S(x)]

DKL (Q||P)

(25)

yields:
1

S(x)

i◆

6 EQ [S(x)] + DKL (Q||P)

(26)

This finishes the proof of (20), the first part of the theorem. Next, we will show the minimum is reached at Q⇤ given by (19).

Variational Policy for Guiding Point Processes

To prove the second part, we will substitute (19) to the right-hand-side of (25) to show that the infimum is reached with this
Q⇤ . More specifically,
Z
dQ⇤
EQ⇤ [S(x)] + DKL (Q⇤ ||P) = EQ⇤ [S(x)] +
log
dQ⇤
dP
Z
exp( 1 S(x))
= EQ⇤ [S(x)] +
log
dQ⇤
EP [exp( 1 S(x))]
✓ h
Z
Z
i◆
1
1
= EQ⇤ [S(x)] +
S(x)dQ⇤
log EP exp
S(x)
dQ⇤
(27)
✓ h
Z
i◆ Z
1
= EQ⇤ [S(x)]
S(x)dQ⇤
log EP exp
S(x)
dQ⇤
✓ h
i◆
1
= EQ⇤ [S(x)] EQ⇤ [S(x)]
log EP exp
S(x)
(28)
✓ h
i◆
1
=
log EP exp
S(x)
where (27) is due to the property log(a/b) = log a log b and (28) is because Q⇤ is a probability measure hence
Hence the infimum is reached and this finishes the proof of the second part.

R

dQ⇤ = 1.

Variational Policy for Guiding Point Processes

B. Proof of Theorem 3
Theorem 3. For the intensity control problem in (4), we have:
M Z
X
i=1

Z

T

ui (s)

1

= exp D(u) , where D(u) is expressed as

dP
dQ(u)

i (s)ds

0

T

log ui (s) dNi (s)
0

Proof. Intuitively, the derivative dP/dQ(u) means the relative density of probability distribution P with respect to Q. The
change of probability measure happens because the intensity of the point process that drives the SDE in (2) is changed
from (t) to (u, t) in (4). Hence dP/dQ(u) describes the change of probability measure for point processes and is the
likelihood ratio between the uncontrolled and controlled point process (Brémaud, 1981):
exp L( )
dP
=
= exp D(u) ,
dQ(u)
exp L( (u))

PM
where L is the log-likelihood for the multi-dimension point process with L( ) = i=1 L( i ). It is defined as the summation
of log-likelihood L( i ) of each dimension i, where L( i ) is defined as follows (Aalen et al., 2008):
L( i (t)) =
where
the operation
P
f
(t
i ).
i

R

Z

T

log( i (t))dNi (t)
0

Z

T

(29)

i (t)dt
0

f (t)dN (t) is defined as the summation of the value of function f at each event time:

Hence, D(u) denotes the difference of the log-likelihood between these two point processes:
D(u) = L( (t)) L( ˜ (u(t), t))
M ✓Z T ⇣
⌘
X
˜ i (ui (s), s)
=
i (s) ds
i=1

=

=

0

M ✓Z T
X
i=1

M ✓Z
X
i=1

Z

0

T

0

⇣
⇣

ui (s) i (s)
ui (s)

1

⌘

i (s)

i (s)ds

⌘

Z

ds
Z

T
0

◆
⇣ ˜ (u (s), s) ⌘
i i
log
dNi (s)
i (s)
◆
⇣
⌘
log ui (s) dNi (s)

f (t)dN (t) :=

T
0
T

0

R

(30)

◆
⇣
⌘
log ui (s) dNi (s)

where M is the dimension of point process. (30) comes from the form of control in (4).
dimension of (t), N (t), u(t).

i (t), Ni (t), ui (t)

denote the i-th

Variational Policy for Guiding Point Processes

C. Derivations of the Optimal Control Policy in (14)
We will formulate our objective function based on the form of optimal measure Q⇤ in (10). More specifically, we find a
control u which pushes the controlled measure Q(u), as close to the optimal measure as possible. This leads to minimizing
the Kullback-Leibler (KL) distance:
u⇤ = argmin DKL (Q⇤ ||Q(u))
(31)
u>0

This objective function is in sharp contrast to traditional methods that solve the optimal control problem by computing the
solution the HJB PDE, which have severe limitations in scalability and feasibility to nonlinear jump diffusion SDEs.
Next we simplify the objective function. According to the definition of KL divergence and chain rule of derivatives, we have:


⇤

DKL (Q ||Q(u)) = EQ⇤ log

✓

dQ⇤
dQ(u)

◆



= EQ⇤ log

✓

dQ⇤ dP
dP dQ(u)

◆

(32)

The derivative dQ⇤ /dP is given in (19) and dP/dQ(u) is given in Theorem 3. Hence, we then substitute dQ⇤ /dP and
dP/dQ(u) to (32). After removing terms which are independent of u, the objective function (31) is simplified as:
u⇤ = argmin EQ⇤ [D(u)]
u>0

Next we parameterize u(t) as a piecewise constant function on [0, T ] as follows.
8
.
>
>
< ..
uk
u(t) =
>
>
: ..
.

for t 2 [k t, (k + 1) t)

More specifically, the k-th piece is defined on [k t, (k + 1) t) as uk , where k = 0, · · · , K
Then we have:
E

Q⇤

[D(u)] =

M X
K ✓
X

E

Q⇤

i=1 k=1

hZ

tk+1
tk

(uki

1) i (s)ds

i

E

Q⇤

hZ

tk+1
tk

1, tk = k t and T = tK .

log(uki )dNi (s)

i◆

(33)

where uki is the i-th dimension of uk . To compute uki , we can neglect the two summation terms in (33) and only focus on
the parts that involves uki . Then we move uki outside of the expectation and discard any constant terms. This yields the
function that only involves uki :
f (uki )

=

uki EQ⇤

⇥

Z

tk+1
i (s)ds
tk

⇤

log(uki )EQ⇤

⇥

Z

tk+1

dNi (s)
tk

We can then show f (uki ) is convex in uki . More specifically, it is in the form of f (x) = ax
f 00 (x) > 0. Finally, setting f 0 (uki ) = 0 yields uk⇤
i :
uk⇤
i

⇥Rt
⇤
EQ⇤ tkk+1 dNi (s)
=
⇥Rt
⇤
EQ⇤ tkk+1 i (s)ds

⇤

(34)

log(x)b with a > 0, b > 0 and

(35)

⇤
However, uk⇤
i is still not computable since the expectation is taken under the optimal probability measure Q . Since we
only known the SDE of the uncontrolled dynamics and can only compute the expectation under P, we need to change the
expectation from EQ⇤ to EP to compute uk⇤
i .

To do this, we will use the following lemma.

Variational Policy for Guiding Point Processes
dQ⇤
dP

Lemma 4. Let the probability measure Q⇤ be defined as

exp(
EP [exp(

=

measurable function. Then we have:
EQ⇤ [g(x)] =

h
EP exp

1

EP [exp(

1

S(x))
S(x))]

1

S(x) g(x)
1

S(x))]

in (10), and g(x) : ⌦ ! < be any

i

Proof.
EQ⇤ [g(x)] =
=

=

=

Z
Z

g(x)dQ⇤
g(x)

R⇣

1

exp(

S(x))dP

EP [exp(

g(x) exp

h

EP [exp(

EP exp
EP [exp(

1

1

1

S(x))]
⌘
1
S(x) dP

S(x))]

S(x) g(x)
1

S(x))]

i

Finally, applying Lemma 4 to (35) yields the following expression for the optimal policy:
⇥
⇤
Rt
EP exp( 1 S(x)) t k+1 dNi (s)
k
⇥
⇤
Rt
⇥ R tk+1
⇤
⇥
⇤
EP exp( 1 S(x)) tkk+1 dNi (s)
EQ⇤ tk dNi (s)
EP exp( 1 S(x))
k⇤
⇤ =
ui =
⇥Rt
⇤= ⇥
⇥
⇤
R tk+1
Rt
1
EP exp( 1 S(x)) t k+1 i (s)ds
EQ⇤ tkk+1 i (s)ds
E
exp(
S(x))
(s)ds
P
i
k
t
⇥
⇤
k
EP exp(

The derivation of the optimal policy is now complete.

1

S(x))

(36)

Variational Policy for Guiding Point Processes

D. Derivations of the Control Cost
We will derive the control cost in (9), which comes naturally from the dynamics. According to the definition of the KL
divergence, we have:
DKL (Q||P) := EQ [log(

dQ
)] = EQ [C(u)]
dP

(37)

Hence, the next step is to compute the derivative dQ
dP . This derivative means the relative density of probability distribution Q
with respect to P. According to (Brémaud, 1981), we have:
dQ
= exp
dP
Using the relationship that
EQ [log(

✓XZ
i

T

log
0

i (ui (t), t)

=

⇣ ˜ (u (t), t) ⌘
i i
dNi (ui (t), t)
i (t)
i (t)ui (t),

Z

T

( ˜ i (ui (t), t)

0

◆
(t))dt
,
i

we have:

XZ T
Z T
⇣ ˜ (u (t), t) ⌘
dQ
i i
)] = EQ
log
dÑi (ui (t), t)
( ˜ i (ui (t), t)
i (t))dt
dP
i (t)
0
0
i
XZ T
Z T⇣
⇣
⌘
1 ⌘˜
= EQ
log ui (t) dÑi (ui (t), t)
1
i (ui (t), t)dt
ui (t)
0
0
i
XZ T
Z T⇣
⇣
⌘
1 ⌘˜
˜
= EQ
log ui (t) i (ui (t), t)dt +
1
i (ui (t), t)dt
ui (t)
0
0
i

Note that (40) to (41) follows from the Campbell theorem (Daley & Vere-Jones, 2007). Therefore, the control cost is:
C(u) =
=

Z
Z

T

i

0
T
0

X ⇣
X ⇣
i

(38)

log(ui (t)) +

1
ui (t)

log(ui (t)) +

1
ui (t)

⌘
1 ˜ i (ui (t), t)dt
⌘
1 ui (t) i (t)dt

(39)
(40)
(41)

