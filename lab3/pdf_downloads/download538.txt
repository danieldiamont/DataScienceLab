Differentially Private Ordinary Least Squares

A. Extended Introductory Discussion

A.2. Omitted Preliminary Details

Due to space constraint, a few details from the introductory parts (Sections 1,2) were omitted. We bring them in
this appendix. We especially recommend the uninformed
reader to go over the extended OLS background we provide in Appendix A.3.

Linear Algebra and Pseudo-Inverses. Given a matrix M
we denote its SVD as M = U SV T with U and V being
orthonormal matrices and S being a non-negative diagonal
matrix whose entries are the singular values of M . We use
Ïƒmax (M ) and Ïƒmin (M ) to denote the largest and smallest
singular value resp. Despite the risk of confusion, we stick
to the standard notation of using Ïƒ 2 to denote the variance
of a Gaussian, and use Ïƒj (M ) to denote the j-th singular
value of M . We use M + to denote the Moore-Penrose inverse of M , defined as M + = V S âˆ’1 U T where S âˆ’1 is a
âˆ’1
matrix with Sj,j
= 1/Sj,j for any j s.t. Sj,j > 0.

A.1. Proof Of Privacy of Algorithm 1
Theorem A.1. Algorithm 1 is (, Î´)-differentially private.
Proof. The proof of the theorem is based on the fact
the Algorithm 1 is the result of composing the differentially private Propose-Test-Release algorithm of (Dwork &
Lei, 2009) with the differentially private analysis of the
Johnson-Lindenstrauss transform of (Sheffet, 2015).
More specifically, we use Theorem B.1 from (Sheffet,
2015) that states that given a matrix A whose all of its
2
singular
p values at greater than T (, Î´) where T (, Î´) =
2
2B
2r ln(4/Î´) + 2 ln(4/Î´) , publishing RA is (, Î´)
differentially private for a r-row matrix R whose entries
sampled are i.i.d normal Gaussians. Since we have that all
of the singular values of A0 are greater than w (as specified
in Algorithm 1), outputting RA0 is (/2, Î´/2)-differentially
private. The rest of the proof boils down to showing that
(i) the if-else-condition is (/2, 0)-differentially private and
that (ii) w.p. â‰¤ Î´/2 any matrix A whose smallest singular value is smaller than w passes the if-condition (step 3).
If both these facts hold, then knowing whether we pass
the if-condition or not is (/2)-differentially private and
the output of the algorithm is (/2, Î´)-differentially private,
hence basic composition gives the overall bound of (, Î´)differential privacy.
To prove (i) we have that for any pair of neighboring matrices A and B that differ only on the i-th row, denoted a i and
T
T
b i resp., we have B T B âˆ’ b ib T
i = A A âˆ’ a ia i . Applying
Weylâ€™s inequality we have
b ib T
Ïƒmin (B T B) â‰¤ Ïƒmin (B T B âˆ’ b ib T
i ) + Ïƒmax (b
i)
T

â‰¤ Ïƒmin (A A) +

a ia T
Ïƒmax (a
i)
2

+

Ïƒmax (bbib T
i)

â‰¤ Ïƒmin (AT A) + 2B

2

hence |Ïƒmin (A)2 âˆ’Ïƒmin (B)2 | â‰¤ 2B 2 , so adding Lap( 4B )
is (/2)-differentially private.
To prove (ii), note that by standard tail-bounds on the
Laplace distribution we have that Pr[Z
<
âˆ’
4B 2 ln(1/Î´)
Î´
] â‰¤ 2 . Therefore, w.p. 1 âˆ’ Î´/2 it holds that

any matrix A that passes the if-test of the algorithm must
have Ïƒmin (A)2 > w2 . Also note that a similar argument shows that for any 0 < Î² < 1, any matrix A s.t.
2
Ïƒmin (A)2 > w2 + 4B ln(1/Î²)
passes the if-condition of

the algorithm w.p. 1 âˆ’ Î².

The Gaussian Distribution.
A univariate Gaussian N (Âµ, Ïƒ 2 ) denotes the Gaussian distribution
whose
mean is Âµ and variance Ïƒ 2 , with PDF(x) =
âˆš
2
( 2Ï€Ïƒ )âˆ’1 exp(âˆ’ xâˆ’Âµ
bounds
2Ïƒ 2 ). Standard concentration
p
on Gaussians give that Pr[x > Âµ + 2Ïƒ ln(1/Î½)] < Î½
Âµ, Î£)
for any Î½ âˆˆ (0, 1e ). A multivariate Gaussian N (Âµ
for some positive semi-definite Î£ denotes the multivariate Gaussian distribution where the mean of the
j-th coordinate is the Âµj and the co-variance between
coordinates j and k is Î£j,k . The PDF of such Gaussian is defined only on the subspace colspan(Î£),
x) =
where for every x âˆˆ colspan(Î£) we have PDF(x

âˆ’1/2

1
rank(Î£)
T
+
Ëœ
x âˆ’ Âµ ) Î£ (x
x âˆ’ Âµ)
(2Ï€)
Â· det(Î£)
exp âˆ’ 2 (x
Ëœ
and det(Î£)
is the multiplication of all non-zero singular values of Î£.
A matrix Gaussian distribution
denoted N (MaÃ—b , U, V ) has mean M , variance U
on its rows and variance V on its columns. For full
rank U and V it holds that PDFN (M,U,V ) (X) =
(2Ï€)âˆ’ab/2 (det(U ))âˆ’b/2 (det(V ))âˆ’a/2
Â·

exp(âˆ’ 21 trace V âˆ’1 (X âˆ’ M )T U âˆ’1 (X âˆ’ M ) ).
In
our case, we will only use matrix Gaussian distributions
with N (MaÃ—b , IaÃ—a , V ) and so each row in this matrix is
an i.i.d sample from a b-dimensional multivariate Gaussian
N ((M )jâ†’ , V ).
We will repeatedly use the rules regarding linear operations
on Gaussians. That in, for any c, it holds that cN (Âµ, Ïƒ 2 ) =
Âµ, Î£) =
N (c Â· Âµ, c2 Ïƒ 2 ). For any C it holds that C Â· N (Âµ
Âµ, CÎ£C T ). And for any C is holds that N (M, U, V ) Â·
N (CÂµ
C = N (M C, U, C T V C). In particular, for any c (which
can be viewed as a b Ã— 1-matrix) it holds that N (M, U, V ) Â·
c = N (Mcc, U, c T V c ) = N (Mcc, c T V c Â· U ).
We will also require the following proposition.
2

Proposition A.2. Given Ïƒ 2 , Î»2 s.t. 1 â‰¤ ÏƒÎ»2 â‰¤ c2 for some
constant c, let X and Y be two random Gaussians s.t. X âˆ¼
N (0, Ïƒ 2 ) and Y âˆ¼ N (0, Î»2 ). It follows that 1c PDFY (x) â‰¤
PDFX (x) â‰¤ cPDFcY (x) for any x.
Corollary A.3. Under the same notation as in Proposition A.2, for any set S âŠ‚ R it holds that 1c Prxâ†Y [x âˆˆ
S] â‰¤ Prxâ†X [x âˆˆ S] â‰¤ cPrxâ†cY [x âˆˆ S] =

Differentially Private Ordinary Least Squares

cPrxâ†Y [x âˆˆ S/c]

haustive account of OLS and we refer the interested reader
to (Rao, 1973; Muller & Stewart, 2006).

Proof. The proof is mere calculation.

xi , yi )}ni=1 where for all i we have
Given n observations {(x
p
x i âˆˆ R and yi âˆˆ R, we assume the existence of a pdimensional vector Î² âˆˆ Rp s.t. the label yi was derived by
yi = Î² Tx i + ei where ei âˆ¼ N (0, Ïƒ 2 ) independently (also
known as the homoscedastic Gaussian model). We use the
matrix notation where X denotes the (n Ã— p)-matrix whose
rows are x i , and use y , e âˆˆ Rn to denote the vectors whose
i-th entry is yi and ei resp. To simplify the discussion, we
assume X has full rank.

PDFX (x)
=
PDFcY (x)

r

2

x
c2 Î»2 exp(âˆ’ 2Ïƒ
2)
Â·
2
2
Ïƒ
exp(âˆ’ 2cx2 Î»2 )

x2 1
1
â‰¤ c Â· exp( ( 2 2 âˆ’ 2 )) â‰¤ c Â· exp(0) = c
2 c Î»
Ïƒ
r
x2
)
PDFX (x)
Î»2 exp(âˆ’ 2Ïƒ
2
Â·
=
PDFY (x)
Ïƒ 2 exp(âˆ’ x22 )
â‰¥c

âˆ’1

2Î»
x2 1
exp( 2 ( Î»2 âˆ’ Ïƒ12 ))

â‰¥

exp(0)
c

= câˆ’1

The parameters of the model are therefore Î² and Ïƒ 2 , which
we set to discover. To that end, we minimize minz kyy âˆ’
Xzz k2 and solve
Î² = (X T X)âˆ’1 X Ty = (X T X)âˆ’1 X T (XÎ²
Î² +ee) = Î² +X +e
Î²Ì‚

The Tk -Distribution.
The Tk -distribution, where k
is referred to as the degrees of freedom of the distribution, denotes the distribution over the reals created by independently sampling Z âˆ¼ N (0, 1) and kÎ¶k2 âˆ¼ Ï‡2k ,
and taking the quantity âˆš Z 2 . Its PDF is given by


kÎ¶k /k
âˆ’ k+1
2
2
x
. It
k

PDFTk (x) âˆ 1 +
is a known fact that
as k increases, Tk becomes closer and closer to a normal
Gaussian. The T -distribution is often used to determine
suitable bounds on the rate of converges, as we illustrate
in Section A.3. As the T -distribution is heavy-tailed, existing tail boundsp
on the T -distribution (which are of the
form:R if Ï„Î½ = C k((1/Î½)2/k âˆ’ 1) for some constant C
âˆž
then Ï„Î½ PDFTk (x)dx < Î½) are often cumbersome to work
with. Therefore, in many cases in practice, it common to
assume Î½ = Î˜(1) (most commonly, Î½ = 0.05) and use
existing tail-bounds on normal Gaussians.
Differential Privacy facts. It is known (Dwork et al.,
2006b) that if ALG outputs a vector in Rd such that for
any A and A0 it holds that kALG(A) âˆ’ ALG(A0 )k1 â‰¤ B,
then adding Laplace noise Lap(1/) to each coordinate of
the output of ALG(A) satisfies -differential privacy. Similarly, (2006b) showed that if for any neighboring A and
A0 it holds that kALG(A) âˆ’ ALG(A0 )k22 â‰¤ âˆ†2 then adding
Gaussian noise N (0, âˆ†2 Â· 2 ln(2/Î´)
) to each coordinate of
2
the output of ALG(A) satisfies (, Î´)-differential privacy.
Another standard result (Dwork et al., 2006a) gives that the
composition of the output of a (1 , Î´1 )-differentially private
algorithm with the output of a (2 , Î´2 )-differentially private
algorithm results in a (1 +2 , Î´1 +Î´2 )-differentially private
algorithm.
A.3. Detailed Background on Ordinary Least Squares
For the unfamiliar reader, we give a short description of the
model under which OLS operates as well as the confidence
bounds one derives using OLS. This is by no means an ex-

Î²
As e âˆ¼ N (00n , Ïƒ 2 InÃ—n ), it holds that Î²Ì‚
âˆ¼
Î² , Ïƒ 2 (X T X)âˆ’1 ), or alternatively, that for every coorN (Î²
Î² âˆ¼ N (Î²j , Ïƒ 2 (X T X)âˆ’1
dinate j it holds that Î²Ì‚j = e T
j Î²Ì‚
j,j ).
Hence we get

Ïƒ

Î²Ì‚ âˆ’Î²j
q j
(X T X)âˆ’1
j,j

âˆ¼ N (0, 1). In addition, we de-

note the vector
Î² = (XÎ²
Î² + e ) âˆ’ X(Î²
Î² + X +e ) = (I âˆ’ XX + )ee
Î¶ = y âˆ’ X Î²Ì‚
and since XX + is a rank-p (symmetric) projection matrix,
we have Î¶ âˆ¼ N (0, Ïƒ 2 (I âˆ’ XX + )). Therefore, kÎ¶Î¶ k2 is
equivalent to summing the squares of (n âˆ’ p) i.i.d samples
from N (0, Ïƒ 2 ). In other words, the quantity kÎ¶Î¶ k2 /Ïƒ 2 is
sampled from a Ï‡2 -distribution with (n âˆ’ p) degrees of
freedom.
We sidetrack from the OLS discussion to give the following
Î² , as the next
bounds on the l2 -distance between Î² and Î²Ì‚
claim shows.
Claim A.4. For any 0 < Î½ < 1/2, the following holds w.p.
â‰¥ 1âˆ’Î½ over the randomness of the model (the randomness
over e )
Î² âˆ’ Î²Ì‚
Î² k2 = kX +e k2
kÎ²
= O Ïƒ 2 log(p/Î½) Â· kX + k2F
2

+



(6)

2

Î² k = kÎ²
Î² + X ek
kÎ²Ì‚

2
p
Î² k + Ïƒ Â· kX + kF Â· log(p/Î½) )
= O( kÎ²
q


 1
ln(1/Î½)
2
2
Î¶
kÎ¶
k
âˆ’
Ïƒ
=
O(
 nâˆ’p

nâˆ’p )
Proof. Since e âˆ¼ N (00n , Ïƒ 2 InÃ—n ) then X +e âˆ¼
N (00n , Ïƒ 2 (X T X)âˆ’1 ). Denoting the SVD decomposition
(X T X)âˆ’1 = V SV T with S denoting the diagonal maâˆ’2
âˆ’2
trix whose entries are Ïƒmax
(X), . . . , Ïƒmin
(X), we have
T +
2
that V X e âˆ¼ N (00n , Ïƒ S). And so, each coordinate of V T X +e is distributed like an i.i.d Gaussian. So

Differentially Private Ordinary Least Squares

w.p. pâ‰¥ 1 âˆ’ Î½/2 non of these Gaussians is a factor of
O(Ïƒ ln(p/Î½)) greater than its standard deviation. And so
w.p. â‰¥ 1 âˆ’ Î½/2P
it holds that kX +e k2 =PkV T X +e k2 â‰¤
âˆ’2
âˆ’2
2
Since
O(Ïƒ log(p/Î½)
i Ïƒi (X) ).
i Ïƒi (X) =
T
âˆ’1
+
+ T
trace((X X) ) = trace(X (X ) ) = kX + k2F , the
bound of (6) is proven.
Î² k2 is an immediate corollary of (6) using
The bound on kÎ²Ì‚
the triangle inequality.8 The bound on kÎ¶Î¶ k2 follows from
tail bounds on the Ï‡2nâˆ’p distribution, as detailed in Section 2.
Î² and Î¶ are
Returning to OLS, it is important to note that Î²Ì‚
Î² depends solely on
independent of one another. (Note, Î²Ì‚
X +e = (X + X)X +e = X + PU e , whereas Î¶ depends on
(I âˆ’ XX + )ee = PU âŠ¥ e . As e is spherically symmetric, the
Î² is
two projections are independent of one another and so Î²Ì‚
independent of Î¶ .) As a result of the above two calculations, we have that the quantity
def

tÎ²Ì‚j (Î²j ) =

Î²Ì‚j âˆ’Î²j
q
Î¶k
kÎ¶
âˆš
(X T X)âˆ’1
j,j Â· nâˆ’p

=

Ïƒ

Î²Ì‚ âˆ’Î²j
q j
(X T X)âˆ’1
j,j

.

kÎ¶Î¶ k
âˆš
Ïƒ nâˆ’p

is distributed like a T -distribution with (n âˆ’ p) degrees of
freedom. Therefore, we can compute an exact probability
estimation for this quantity. That is, for any measurable
S âŠ‚ R we have
i Z
h
Î² and Î¶ satisfying tÎ²Ì‚j (Î²j ) âˆˆ S =
PDFTnâˆ’p (x)dx
Pr Î²Ì‚
S

The importance of the t-value t(Î²j ) lies in the fact that it
can be fully estimated from the observed data X and y (for
any value of Î²j ), which makes it a pivotal quantity. Therefore, given X and y , we can use t(Î²j ) to describe the likelihood of any Î²j â€” for any z âˆˆ R we can now give an
estimation of how likely it is to have Î²j = z (which is
PDFTnâˆ’p (t(z))). The t-values enable us to perform multitude of statistical inferences. For example, we can say
which of two hypotheses is more likely and by how much
(e.g., we are 5-times more likely that the hypothesis Î²j = 3
is true than the hypothesis Î²j = 14 is true); we can compare between two coordinates j and j 0 and report we are
more confident that Î²j > 0 than Î²j 0 > 0; or even compare
among the t-values we get across multiple datasets (such
as the datasets we get from subsampling rows from a single dataset).
In particular, we can use t(Î²j ) to Î±-reject unlikely values
of Î²j . Given 0 < Î± < 1, we denote cÎ± as the number for
which the interval (âˆ’cÎ± , cÎ± ) contains a probability mass
of 1 âˆ’ Î± from the Tnâˆ’p -distribution. And so we derive a
8

Observe, though e is spherically symmetric, and is likely to
be approximately-orthogonal to Î² , this does not necessarily hold
for X +e which isnâ€™t spherically symmetric. Therefore, we result
Î² using the triangle bound.
to bounding the l2 -norm of Î²Ì‚

corresponding confidence interval IÎ± centered at Î²Ì‚j where
Î²j âˆˆ IÎ± with confidence of level of 1 âˆ’ Î±.
We comment as to the actual meaning of this confidence
interval. Our analysis thus far applied w.h.p to a vector y
derived according to this model. Such X and y will result in the quantity tÎ²Ì‚j (Î²j ) being distributed like a Tnâˆ’p distribution â€” where Î²j is given as the model parameters
and Î²Ì‚j is the random variable. We therefore have that guarantee that for X and 
y derived according to this model,
 the
q
2
def
Î¶
kÎ¶
k
âˆ’1
event EÎ± = Î²Ì‚j âˆˆ Î²j Â± cÎ± Â· (X T X)j,j Â· nâˆ’p happens w.p. 1 âˆ’ Î±. However, the analysis done over a given
dataset X and y (once y has been drawn) views the quantity tÎ²Ì‚j (Î²j ) with Î²Ì‚j given and Î²j unknown. Therefore the
event EÎ± either holds or does not hold. That is why the
alternative terms of likelihood or confidence are used, instead of probability. We have
q a confidence 2level of 1 âˆ’ Î±
Î¶
Ë†
that indeed Î²j âˆˆ Î²j Â± cÎ± Â· (X T X)âˆ’1 Â· kÎ¶ k , because this
j,j

nâˆ’p

event does happen in 1âˆ’Î± fraction of all datasets generated
according to our model.
Rejecting the Null Hypothesis. One important implication of the quantity t(Î²j ) is that we can refer specifically to
the hypothesis that Î²j = 0, called the null hypothesis. This
def

quantity, t0 = tÎ²Ì‚j (0) =

âˆš
Î²Ì‚j nâˆ’p
q
,
kÎ¶Î¶ k (X T X)âˆ’1
j,j

represents how

large is Î²Ì‚j relatively to the empirical estimation of standard
deviation Ïƒ. Since it is known that as the number of degrees
of freedom of a T -distribution tends to infinity then the T distribution becomes a normal Gaussian, it is common to
think of t0 as a sample from a normal Gaussian N (0, 1).
This allows us to associate t0 with a p-value, estimating the
event â€œÎ²j and Î²Ì‚j have different signs.â€ Formally, we define
Râˆž
2
p0 = |t0 | âˆš12Ï€ eâˆ’x /2 dx. It is common to reject the null
hypothesis when p0 is sufficiently small (typically, below
0.05).9
Specifically, given Î± âˆˆ (0, 1/2), we say we Î±-reject the
null hypothesis
R âˆž if p0 <2 Î±. Let Ï„Î± be the number s.t.
Î¦(Ï„Î± ) = Ï„Î± âˆš12Ï€ eâˆ’x /2 dx = Î±. (Standard bounds
p
give that Ï„Î± < 2 ln(1/Î±).) This means we Î±-reject
the null hypothesis
if t0 > Ï„Î± or t0 < âˆ’Ï„Î± , meaning if
q
|Î²Ì‚j | > Ï„Î±

kÎ¶Î¶ k
âˆš
(X T X)âˆ’1
j,j nâˆ’p .
Î¶

We can now lower bound the number of i.i.d sample points
needed in order to Î±-reject the null hypothesis. This bound
will be our basis for comparison â€” between standard OLS
and the differentially private version.10
9
R âˆž Indeed, it is more accurate to associate with t0 the value
PDFTnâˆ’p (x)dx and check that this value is < Î±. However,
|t0 |
as most uses take Î± to be a constant (often Î± = 0.05), asymptotically the threshold we get for rejecting the null hypothesis are the
same.
10
This theorem is far from being new (except for maybe fo-

Differentially Private Ordinary Least Squares

Theorem A.5 (Theorem 2.2 restated.). Fix any positive
definite matrix Î£ âˆˆ RpÃ—p and any Î½ âˆˆ (0, 12 ). Fix parameters Î² âˆˆ Rp and Ïƒ 2 and a coordinate j s.t. Î²j 6= 0.
Let X be a matrix whose n rows are i.i.d samples from
Î² )i is sampled
N (00, Î£), and y be a vector where yi âˆ’ (XÎ²
i.i.d from N (0, Ïƒ 2 ). Fix Î± âˆˆ (0, 1). Then w.p. â‰¥ 1 âˆ’ Î½
we have
p that the (1 âˆ’ Î±)-confidence interval is of length
O(cÎ± Ïƒ 2 /(nÏƒmin (Î£))) provided n â‰¥ C1 (p + ln(1/Î½))
for some sufficiently large constant C1 . Furthermore, there
exists a constant C2 such that w.p. â‰¥ 1 âˆ’ Î± âˆ’ Î½ we (correctly) reject the null hypothesis provided
(
)
Ïƒ 2 c2Î± + Ï„Î±2
n â‰¥ max C1 (p + ln(1/Î½)), C2 2 Â·
Î²j Ïƒmin (Î£)
cÎ±
denotes
the
number
for
which
PDF
(x)dx
=
1
âˆ’
Î±.
(If
we
are
content
T
nâˆ’p
âˆ’cÎ±
with approximating Tnâˆ’p
p with a normal Gaussian than
one can set cÎ± â‰ˆ Ï„Î± < 2 ln(1/Î±).)
Here
R cÎ±

Proof. The discussion above
â‰¥ 1âˆ’Î±
q shows that w.p.
2
âˆ’1 kÎ¶Î¶ k
T
we have |Î²j âˆ’ Î²Ì‚j | â‰¤ cÎ± (X X)j,j nâˆ’p ; and in orderqto Î±-reject the null hypothesis we must have |Î²Ì‚j | >
kÎ¶Î¶ k2
Ï„Î± (X T X)âˆ’1
j,j nâˆ’p . Therefore, a sufficient condition for
OLS to Î±-reject the null-hypothesis
is to have n large
q
2

kÎ¶Î¶ k
enough s.t. |Î²j | > (cÎ± + Ï„Î± ) (X T X)âˆ’1
j,j nâˆ’p . We therefore argue that w.p.â‰¥ 1 âˆ’ Î½ this inequality indeed holds.
Î¶

We assume each row of X i.i.d vector x i âˆ¼ N (00p , Î£), and
recall that according to the model kÎ¶Î¶ k2 âˆ¼ Ïƒ 2 Ï‡2 (n âˆ’ p).
Straightforward concentration bounds on Gaussians and on
the Ï‡2 -distribution give:
âˆš
(i) W.p. â‰¤ Î± it holds that kÎ¶Î¶ k > Ïƒ ( n âˆ’ p + 2 ln(2/Î±))).
(This is part of the standard OLS analysis.)
âˆš
(ii) W.p.pâ‰¤ Î½ it holds that Ïƒmin (X T X) â‰¤ Ïƒmin (Î£)( n âˆ’
âˆš
( p + 2 ln(2/Î½)))2 . (Rudelson & Vershynin, 2009)
Therefore, due to the lower bound n = â„¦(p +
ln(1/Î½)), w.p.â‰¥ 1 âˆ’ Î½ âˆ’ Î± we have that none
of
hold.
In such a case we have
q these events p
âˆ’1
T
(X X)j,j â‰¤
Ïƒmax ((X T X)âˆ’1 ) = O( âˆš 1
)
nÏƒmin (Î£)
âˆš
and kÎ¶Î¶ k = O(Ïƒ n âˆ’ p).
This implies that the
confidence
interval
of
level
1
âˆ’ Î± has length of
q
 q

2
âˆ’1 kÎ¶Î¶ k
Ïƒ2
T
cÎ± (X X)j,j Â· nâˆ’p = O cÎ± nÏƒmin
(Î£) ; and that in
order to Î±-reject
that
null-hypothesis
it
suffices to have
q


2

Ïƒ
|Î²j | = â„¦ (cÎ± + Ï„Î± ) nÏƒmin
(Î£) . Plugging in the lower
bound on n, we see that this inequality holds.

We comment that for sufficiently large constants C1 , C2 ,
cusing on the setting where every row in X is sampled from an
i.i.d multivariate Gaussians), it is just stated in a non-standard
way, discussing solely the power of the t-test in OLS. For further
discussions on sample size calculations see (Muller & Stewart,
2006).

it holds that all the constants hidden in the O- and â„¦notations of the proof are close to 1. I.e., they are all
within the interval (1 Â± Î·) for some small Î· > 0 given
C1 , C2 âˆˆ â„¦(Î· âˆ’2 ).

B. Projecting the Data using Gaussian
Johnson-Lindenstrauss Transform
B.1. Main Theorem Restated and Further Discussion
Theorem B.1 (Theorem 3.1 restated.). Let X be a n Ã— p
matrix, and parameters Î² âˆˆ Rp and Ïƒ 2 are such that
Î² + e with each coordiwe generate the vector y = XÎ²
nate of e sampled independently from N (0, Ïƒ 2 ). Assume
Ïƒmin (X) â‰¥ C Â· w and that n is sufficiently large s.t.
all of the singular values of the matrix [X; y ] are greater
than C Â· w for some large constant C, and so Algorithm 1
projects the matrix A = [X; y ] without altering it, and publishes [RX; Ryy ].
Fix Î½ âˆˆ (0, 1/2) and r = p + â„¦(ln(1/Î½)). Fix coordinate
Î² , Î¶ÌƒÎ¶ and ÏƒÌƒ 2 as
j. Then w.p. â‰¥ 1 âˆ’ Î½ we have that deriving Î²Ìƒ
follows
Î²
Î²Ìƒ

= (X T RT RX)âˆ’1 (RX)T (Ryy ) = Î² + (RX)+ Ree

Î¶ÌƒÎ¶

=
=

ÏƒÌƒ 2

=

âˆš1 Ry
y âˆ’ âˆš1r (RX)Î²Ìƒ
Î²
r

T
1
âˆš
I âˆ’ (RX)(X RT RX)âˆ’1 (RX)T ) Ree
r

r
kÎ¶ÌƒÎ¶ k2
râˆ’p

then the pivot quantity
Î²Ìƒj âˆ’ Î²j

tÌƒ(Î²j ) =
ÏƒÌƒ

q

(X T RT RX)âˆ’1
j,j

has a distribution D satisfying eâˆ’a PDFTrâˆ’p (x) â‰¤
PDFD (x) â‰¤ ea PDFTrâˆ’p (eâˆ’a x) for any x âˆˆ R, where we
râˆ’p
denote a = nâˆ’p
.
Comparison with Existing Bounds. Sarlosâ€™ work (2006)
utilizes the fact that when r, the numbers of rows in R,
is large enough, then âˆš1r R is a Johnson-Lindenstrauss
matrix. q
Specifically, given r and Î½ âˆˆ (0, 1) we denote
Î· = â„¦(

p ln(p) ln(1/Î½)
),
r

and so r = O( p ln(p)Î·ln(1/Î½)
).
2

Î² = arg minz 1r kRXzz âˆ’ Ryy k2 . In
Let us denote Î²Ìƒ
this setting, Sarlosâ€™ work (SarloÌs, 2006) (Theorem 12(3)) guarantees that w.p.qâ‰¥ 1 âˆ’ Î½ we have

p log(p) log(1/Î½)
Î² âˆ’ Î²Ìƒ
Î² k2 â‰¤ Î·kÎ¶Î¶ k/Ïƒmin (X) = O
Î¶
kÎ²Ì‚
kÎ¶
k
.
rÏƒmin (X T X)
Î² âˆ’ Î²Ìƒ
Î² k and using the
NaÄ±Ìˆvely bounding |Î²Ì‚j âˆ’ Î²Ìƒj | â‰¤ kÎ²Ì‚
Î² j âˆ’ Î² j from Section A.311
confidence interval for Î²Ì‚
11
Where we approximate cÎ± , the tail bound of the Tnâˆ’p distribution with the tail
p bound on a Gaussian, i.e., use the approximation cÎ± â‰ˆ O( ln(1/Î±)).

Differentially Private Ordinary Least Squares

gives a confidence interval of q
level 1 âˆ’ (Î± + Î½) 
cenp ln(p) log(1/Î½)
tered at Î²Ìƒj with length of O
kÎ¶Î¶ k +
rÏƒmin (X T X)
q

log(1/Î±)
Î¶k
O
(X T X)âˆ’1
=
j,j nâˆ’p kÎ¶
q

p ln(p) log(1/Î½)+log(1/Î±)
O
kÎ¶Î¶ k .
This implies that
rÏƒmin (X T X)
our confidence interval has decreased its degrees of
freedom from n âˆ’ p to roughly r/p ln(p), and furthermore,
that it no longer depends on (X T X)âˆ’1
j,j but rather on
T
1/Ïƒmin (X X). It is only due to the fact that we rely on
Gaussians and by mimicking carefully the original proof
that we can deduce that the tÌƒ-value has (roughly) r âˆ’ p
degrees of freedom and depends solely on (X T X)âˆ’1
j,j .
(In the worst case, we have that (X T X)âˆ’1
j,j is proportional
to Ïƒmin (X T X)âˆ’1 , but it is not uncommon to have matrices
where the former is much larger than the latter.) As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for
finding a DP estimator Î² dp of the linear regression give a
Î² dp âˆ’ Î²Ì‚
Î² k = OÌƒ(p/). Such
data-independent12 bound of kÎ²
bounds are harder to compare with the interval length given
by Corollary 3.2. Indeed, as we discuss in Section 3 under â€œRejecting the null-hypothesis,â€ enough samples from
a multivariate Gaussian whose covariance-matrix is well
conditioned give a bound which is well below the worstupper bound of O(p/). (Yet, it is possible that these techniques also do much better on such â€œwell-behavedâ€ data.)
What the works of Sarlos and alternative works regrading
differentially private linear regression do not take into account are questions such as generating a likelihood for Î²j
nor do they discuss rejecting the null hypothesis.
B.2. Proof of Theorem 3.1
Î² and Î¶ÌƒÎ¶ , where our goal
We now turn to our analysis of Î²Ìƒ
is to show that the distribution of the tÌƒ-values as specified in Theorem 3.1 is well-approximated by the Trâˆ’p distribution. For now, we assume the existence of fixed
Î² + e . (Later,
vectors Î² âˆˆ Rp and e âˆˆ Rn s.t. y = XÎ²
we will return to the homoscedastic model where each coordinate of e is sampled i.i.d from N (0, Ïƒ 2 ) for some Ïƒ 2 .)
In other words, we first examine the case where R is the
sole source of randomness in our estimation. Based on the
assumption that e is fixed, we argue the following.
Claim B.2. In our model,
given X and
Î²
the output M
= RX, we have that Î²Ìƒ
âˆ¼
N Î² + X +e , kPU âŠ¥ e k2 (M T M )âˆ’1
and
Î¶ÌƒÎ¶
âˆ¼


kPU âŠ¥ e k2
T
âˆ’1
T
N 0n,
(IrÃ—r âˆ’ M (M M ) M ) .
Where
r
PU âŠ¥ denotes the projection operator onto the subspace
orthogonal to colspan(X); i.e., PU = XX + and
PU âŠ¥ = (IrÃ—r âˆ’ XX + ).
12

In other words, independent of X, Î¶ .

Proof. The
matrix
R
is
sampled
from
N (0rÃ—p , IrÃ—r , IpÃ—p ). Given X and RX = M , we
learn the projection of each row in R onto the subspace
spanned by the columns of X. That is, denoting u T as
the i-th row of R and v T as the i-th row of M , we have
that X Tu = v . Recall, initially u âˆ¼ N (00n , InÃ—n ) â€“
a spherically symmetric Gaussian. As a result, we can
denote u = PU u Ã— PU âŠ¥ u where the two projections are
independent samples from N (00n , PU ) and N (00n , PU âŠ¥ )
resp. However, once we know that v = X Tu we have that
PU u = X(X T X)âˆ’1 X Tu = X(X T X)âˆ’1v so we learn
PU u exactly, whereas we get no information about PU âŠ¥
so PU âŠ¥ u is still sampled from a Gaussian N (00n , PU âŠ¥ ).
As we know for each row of R that u T PU = v T X + , we
therefore have that
R = RPU + RPU âŠ¥ = M X + + RPU âŠ¥
where RPU âŠ¥ âˆ¼ N (0rÃ—n , IrÃ—r , PU âŠ¥ ). From here on, we
just rely on the existing results about the linearity of Gaussians.
R âˆ¼ N (M X + , IrÃ—r , PU âŠ¥ )
â‡’ Ree âˆ¼ N (M X +e , kPU âŠ¥ e k2 IrÃ—r )
â‡’ M + Ree âˆ¼ N (X +e , kPU âŠ¥ e k2 (M T M )âˆ’1 )
Î² = Î² + M + Ree implies Î²Ìƒ
Î² âˆ¼ N (Î²
Î² +
so Î²Ìƒ
+
X e , kPU âŠ¥ e k2 (M T M )âˆ’1 ).
And as Î¶ÌƒÎ¶
=
T
âˆ’1
T
âˆš1 (IrÃ—r
e
âˆ’
M
(M
M
)
M
)Re
then
we
r
kP

have Î¶ÌƒÎ¶
âˆ¼
N (00r , U râŠ¥
+
(IrÃ—r âˆ’ M M )M = 0rÃ—p .

e k2

(IrÃ—r âˆ’ M M + )) as

Claim B.2 was based on the assumption that e is fixed.
However, given X and y there are many different ways to
Î² + e . However, the
assign vectors Î² and e s.t. y = XÎ²
distributions we get in Claim B.2 are unique. To see that,
Î² and
recall Equations (1) and (2): Î² + X +e = X +y = Î²Ì‚
PU âŠ¥ e = PU âŠ¥ y = (I âˆ’ XX + )yy = Î¶ . We therefore have
2
Î² âˆ¼ N (Î²Ì‚
Î² , kÎ¶Î¶ k2 (M T M )âˆ’1 ) and Î¶ÌƒÎ¶ âˆ¼ N (00n , kÎ¶Î¶rk (I âˆ’
Î²Ìƒ
M M + )). We will discuss this further, in Section 4, where
we will not be able to better analyze the explicit distributions of our estimators. But in this section, we are able to
Î² and Î¶ÌƒÎ¶ .
argue more about the distributions of Î²Ìƒ
So far we have considered the case that e is fixed, whereas
our goal is to argue about the case where each coordinate
of e is sampled i.i.d from N (0, Ïƒ 2 ). To that end, we now
switch to an intermediate model, in which PU e is sampled from a multivariate Gaussian while PU âŠ¥ e is fixed as
some arbitrary vector of length l. Formally, let Dl denote
the distribution where PU e âˆ¼ N (0, Ïƒ 2 PU ) and PU âŠ¥ e is
fixed as some specific vector whose length is denoted by
kPU âŠ¥ ek = l.
Claim B.3. Under the same assumptions as in
Claim B.2, given that e âˆ¼ Dl , we have that

Differentially Private Ordinary Least Squares

âˆ¼
N Î² , Ïƒ 2 (X T 
X)âˆ’1 + l2 (M T M )âˆ’1

2
Î¶ÌƒÎ¶ âˆ¼ N 0 n , lr (I âˆ’ M M + ) .

Î²
Î²Ìƒ



Î² = Î² + M + Ree = Î² + M + (M X + +
Proof. Recall, Î²Ìƒ
RPU âŠ¥ )ee = Î² + X +e + M + R(PU âŠ¥ e ). Now, under the
assumption e âˆ¼ Dl we have that Î² is the sum of two independent Gaussians:

Î² , Ïƒ 2 X + Â· PU Â· (X + )T )
Î² + X +e âˆ¼ N (Î²
Î² , Ïƒ 2 (X T X)âˆ’1 )
= N (Î²
âˆ¼ N (00r , kPU âŠ¥ e k2 IrÃ—r )
RPU âŠ¥ e
+ e
â‡’ M Re âˆ¼ N (00p , kPU âŠ¥ e k2 (M T M )âˆ’1 )
Summing the two independent Gaussiansâ€™ means and
Î² . Furthermore, in
variances gives the distribution of Î²Ìƒ
Claim B.2 we have
already
established
that 
for any fixed

kPU âŠ¥ e k2
+
e we have Î¶ÌƒÎ¶ âˆ¼ N 0 n ,
(I âˆ’ M M ) . Hence, for
r


2
e âˆ¼ Dl we still have Î¶ÌƒÎ¶ âˆ¼ N 0 n , lr (I âˆ’ M M + ) . (It is
easy to verify that the same chain of derivations is applicable when e âˆ¼ Dl .)
Corollary B.4. Given that e âˆ¼ Dl we have that Î²Ìƒj âˆ¼
âˆ’1
2
T
N (Î²j , Ïƒ 2 (X T X)âˆ’1
j,j + l (M M )j,j ) for any coordinate
j, and that kÎ¶ÌƒÎ¶ k2 âˆ¼

l2
r

Â· Ï‡2râˆ’p .

Proof. The corollary follows immediately from the fact
Î² , and from the definition of the Ï‡2 that Î²j = e T
j Î²Ìƒ
distribution, as Î¶ÌƒÎ¶ is a spherically symmetric Gaussian defined on the subspace colspan(M )âŠ¥ of dimension r âˆ’
p.
To continue, we need the following claim.
Claim B.5. Given X and M = RX, and given that e âˆ¼ Dl
Î² and Î¶ÌƒÎ¶ are independent.
we have that Î²Ìƒ
Î² = Î² + X +e + M + R(PU âŠ¥ e ). And
Proof. Recall, Î²Ìƒ
so, given X, M and a specific vector PU âŠ¥ e we have
Î² depends on (i) the projection
that the distribution of Î²Ìƒ
of e on U = colspan(X) and on (ii) the projection of
each row in R onto UÌƒ = colspan(M ). The distribution of Î¶ÌƒÎ¶ = âˆš1r PUÌƒ âŠ¥ Ree = âˆš1r PUÌƒ âŠ¥ (M X + + RPU âŠ¥ )ee =
âˆš1 P âŠ¥ RPU âŠ¥ e depends on (i) the projection of e onto U âŠ¥
r UÌƒ
(which for the time being is fix to some specific vector of
length l) and on (ii) the projection of each row in R onto
UÌƒ âŠ¥ . Since PU e is independent from PU âŠ¥ e , and since for
any row u T of R we have that PUÌƒ u is independent of PUÌƒ âŠ¥ u ,
and since e and R are chosen independently, we have that
Î² and Î¶ÌƒÎ¶ are independent.
Î²Ìƒ
Formally, consider any pair of coordinates Î²Ìƒj and Î¶Ìƒk , and
we have
Î²Ìƒj âˆ’ Î²j

+
T
+
= eT
j X e + e j M (RPU âŠ¥ e )

Î¶Ìƒk

and

= eT
k PUÌƒ âŠ¥ (RPU âŠ¥ e )

Recall, we are given X and M = RX. Therefore, we know
PU and PUÌƒ . And so
Cov[Î²Ìƒj , Î¶Ìƒk ]
= E[(Î²Ìƒj âˆ’ Î²j )(Î¶Ìƒk âˆ’ 0)]
+
T
= E[eeT
j X e (RPU âŠ¥ e ) PUÌƒ âŠ¥ e k ]
T
+
+ E[eeT
j M (RPU âŠ¥ e )(RPU âŠ¥ e ) PUÌƒ âŠ¥ e k ]
+
ee T PU âŠ¥ ]E[RT ]PUÌƒ âŠ¥ e k
= eT
j X E[e
T
+
+ eT
j M E[(RPU âŠ¥ e )(RPU âŠ¥ e ) ]PUÌƒ âŠ¥ e k


+
eeT PU âŠ¥ ] (M X + )T + E[(RPU âŠ¥ )T ] PUÌƒ âŠ¥ ek
= eT
j X E[e

+
+ eT
kPU âŠ¥ e k2 IrÃ—r PUÌƒ âŠ¥ e k
jM

+
ee T PU âŠ¥ ](X + )T M T PUÌƒ âŠ¥ e k + 0
= eT
j X E[e

+
+ l2 Â· e T
j M PUÌƒ âŠ¥ e k
=0+0+0=0
Î² and Î¶ÌƒÎ¶ are Gaussians, having their covariance = 0
And as Î²Ìƒ
implies independence.
Î² and Î¶ÌƒÎ¶ are independent Gaussians
Having established that Î²Ìƒ
and specified their distributions, we continue with the proof
of Theorem 3.1. We assume for now that there exists some
small a > 0 s.t.
âˆ’1
âˆ’1
2
T
2
T
l2 (M T M )âˆ’1
j,j â‰¤ Ïƒ (X X)j,j + l (M M )j,j
2a
2
T
â‰¤ e Â· l (M M )âˆ’1
j,j

(7)

Then, due to Corollary A.3, denoting the distributions N1 = N (0, l2 (M T M )âˆ’1
=
j,j ) and N2
âˆ’1
2
T
N (0, Ïƒ 2 (X T X)âˆ’1
+
l
(M
M
)
),
we
have
that
for
any
j,j
j,j
S âŠ‚ R it holds that13
eâˆ’a PrÎ²Ìƒj âˆ¼N1 [S] â‰¤ PrÎ²Ìƒj âˆ¼N2 [S] â‰¤ ea PrÎ²Ìƒj âˆ¼N1 [S/ea ]
(8)
More specifically, denote the function
tÌƒ(Ïˆ, kÎ¾Î¾ k, Î²j )

Ïˆ âˆ’ Î²j
q
r
kÎ¾Î¾ k râˆ’p
(M T M )âˆ’1
j,j
q
r
Î¾
kÎ¾
k
.
râˆ’p
Ïˆ âˆ’ Î²j
= q
l
l (M T M )âˆ’1

=

j,j

and observe that when we sample Ïˆ, Î¾ independently s.t.
2
Î¾ k2 âˆ¼ lr Ï‡2râˆ’p then
Ïˆ âˆ¼ N (Î²j , l2 (M T M )âˆ’1
j,j ) and kÎ¾
tÌƒ(Ïˆ, kÎ¾Î¾ k, Î²j ) is distributed like a T -distribution with r âˆ’ p
13
In fact, it is possible to use standard techniques from differential privacy, and argue a similar result â€” that the probabilities
of any event that depends on some function f (Î²j ) under Î²j âˆ¼ N1
and under Î²j âˆ¼ N2 are close in the differential privacy sense.

Differentially Private Ordinary Least Squares

degrees of freedom. And so, for any Ï„ > 0 we have that
under such way to sample Ïˆ, Î¾ we have Pr[tÌƒ(Ïˆ, kÎ¾Î¾ k, Î²j ) >
Ï„ ] = 1 âˆ’ CDFTrâˆ’p (Ï„ ).

for any I = (Ï„1 , Ï„2 ) with Ï„1 < Ï„2 â‰¤ 0, and deduce
that the PDF of the function tÌƒ(Ïˆ, kÎ¾Î¾ k, Î²j ) at x â€” where
âˆ’1
2
T
we sample Ïˆ âˆ¼ N (Î²j , l2 (M T M )âˆ’1
j,j + Ïƒ (X X)j,j )

For any Ï„ â‰¥ 0 and for any non-negative real value z let SzÏ„
denote the suitable set of values s.t.

and kÎ¾Î¾ k2 âˆ¼ lr Ï‡2râˆ’p independently â€” lies in the range

eâˆ’a PDFTrâˆ’p (x), ea PDFTrâˆ’p (x/ea ) . And so, using
Corollary B.4 and Claim B.5, we have that when e âˆ¼ Dl ,
the distributions of Î²Ìƒj and kÎ¶ÌƒÎ¶ k2 are precisely as stated

ï£¼
Î¾ k, Î²j ) > Ï„ ]
Prï£±
ï£²Ïˆâˆ¼N (Î²j , l2 (M T M )âˆ’1 )ï£½ [tÌƒ(Ïˆ, kÎ¾
j,j

l2
kÎ¾Î¾ k2 âˆ¼ r Ï‡2râˆ’p

ï£³

Zâˆž
=

PDF l2

2
r Ï‡râˆ’p

0

(z) Â·

2

def

ï£¾

Pr
[SzÏ„ ] dz
{Ïˆâˆ’Î²j âˆ¼N (0, l2 (M T M )âˆ’1
j,j )}

above, and so we have that the distribution of tÌƒ(Î²j ) =
tÌƒ(Î²Ìƒj , kÎ¶ÌƒÎ¶ k, Î²j ) has a PDF that at the point x is â€œsandwichedâ€
between eâˆ’a PDFTrâˆ’p (x) and ea PDFTrâˆ’p (x/ea ).

Next, we aim to argue that this characterization of the
PDF of tÌƒ(Î²j ) still holds when e âˆ¼ N (00n , Ïƒ 2 InÃ—n ).
r
That is, SzÏ„ = Ï„ Â· z râˆ’p
(M T M )âˆ’1
,
âˆž
.
j,j
It would be convenient to think of e as a sample in
N (00n , Ïƒ 2 PU ) Ã— N (00n , Ïƒ 2 PU âŠ¥ ). (So while in Dl we have
We now use Equation (8) (Since N (0, l2 (M T M )âˆ’1
)
is
j,j
PU e âˆ¼ N (00n , Ïƒ 2 PU ) but PU âŠ¥ e is fixed, now both PU e and
precisely N1 ) to deduce that
PU âŠ¥ e are sampled from spherical Gaussians.) The reason
why the above still holds lies in the fact that tÌƒ(Î²j ) does not
ï£±
ï£¼
Prï£²Ïˆâˆ¼N (Î²j , l2 (M T M )âˆ’1 +Ïƒ2 (X T X)âˆ’1 )ï£½ [tÌƒ(Ïˆ, kÎ¾Î¾ k, Î²j ) > Ï„ ]
depend on l. In more details:
j,j
j,j
l2


ï£³
ï£¾
kÎ¾Î¾ k2 âˆ¼ r Ï‡2râˆ’p
Pre âˆ¼N (00n ,Ïƒ2 InÃ—n ) tÌƒ(Î²j ) âˆˆ I
Z âˆž
Z


=
PDF l2 2 (z)
[SzÏ„ ]dz = Pr
Pr
v )dvv
âˆ’1
âˆ’1
2
T
2
T
0n ,Ïƒ 2 InÃ—n ) tÌƒ(Î²j ) âˆˆ I | PU âŠ¥ e = v PDFPU âŠ¥ e (v
e âˆ¼N (0
Ïˆ âˆ’ Î²j âˆ¼ N (0, l (M M )j,j + Ïƒ (X X)j,j )
0
r Ï‡râˆ’p
v
Z âˆž
Z


[SzÏ„ /ea ]dz =
â‰¤ ea
Pr
PDF l2 2 (z)
Pr tÌƒ(Î²j ) âˆˆ I | l = kvv k PDFPU âŠ¥ e (vv )dvv
2 (M T M )âˆ’1 )
Ï‡
Ïˆâˆ’Î²
âˆ¼N
(0,
l
e
âˆ¼D
0
j
l
r râˆ’p
j,j
v
!
Z âˆž
Z
Z


(âˆ—)

= ea

PDF l2

0



q

r

Ï‡2râˆ’p

(z)

Pr

Ïˆâˆ’Î²j âˆ¼N (0,

a

[SzÏ„ /e ]dz

l2 (M T M )âˆ’1
j,j )

j,j

= ea 1 âˆ’

l2
kÎ¾Î¾ k2 âˆ¼ r Ï‡2râˆ’p

CDFTrâˆ’p (Ï„ /ea )

= ea

ï£¼
Î¾ k, Î²j ) > Ï„ ]
Prï£±
ï£²Ïˆâˆ¼N (Î²j , l2 (M T M )âˆ’1 +Ïƒ 2 (X T X)âˆ’1 )ï£½ [tÌƒ(Ïˆ, kÎ¾
j,j

> Ï„]

2

= eâˆ’a

l
kÎ¾Î¾ k2 âˆ¼ r Ï‡2râˆ’p

ï£¾


1 âˆ’ CDFTrâˆ’p (Ï„ )

In other words, we have just shown that for any interval I = (Ï„, âˆž) with Ï„ â‰¥ 0 we have that
ï£¼
Î¾ k, Î²j ) âˆˆ I]
Prï£±
ï£²Ïˆâˆ¼N (Î²j , l2 (M T M )âˆ’1 +Ïƒ 2 (X T X)âˆ’1 )ï£½ [tÌƒ(Ïˆ, kÎ¾
j,j

ï£³

PDFTrâˆ’p (z)dz
v

PDFPU âŠ¥ e (vv )dvv

Z
PDFTrâˆ’p (z)dz
I/ea

where the last transition is possible precisely because tÌƒ is
independent of l (or kvv k) â€” which is precisely what makes
this t-value a pivot quantity. The proof of the lower bound
is symmetric.

j,j

l2
ï£¾
kÎ¾Î¾ k2 âˆ¼ r Ï‡2râˆ’p
âˆ’a
ï£¼ [tÌƒ(Ïˆ, kÎ¾
Î¾ k, Î²j )
e Prï£±
ï£½
ï£²Ïˆâˆ¼N (Î²j , l2 (M T M )âˆ’1
j,j )
ï£³

!Z

Z
I/ea

=
where the equality (âˆ—) follows from the fact that
Ï„ /c
Sz for any c > 0, since it is a non-negative interval.
Analogously, we can also show that

â‰¥

e

a

ï£¾

SzÏ„ /c

ï£³

=

PDFPU âŠ¥ e (vv )dvv

PDFTrâˆ’p (z)dz
I/ea

v

ï£¼
Î¾ k, Î²j ) > Ï„ /ea ]
= ea Prï£±
ï£²Ïˆâˆ¼N (Î²j , l2 (M T M )âˆ’1 )ï£½ [tÌƒ(Ïˆ, kÎ¾
ï£³

ea

â‰¤

l2
kÎ¾Î¾ k2 âˆ¼ r Ï‡2râˆ’p
a

j,j

ï£¾

R

is lower bounded by e PDFTrâˆ’p (z)dz and upper
I
R
bounded by ea
PDFTrâˆ’p (z)dz. We can now repeat
I/ea

the same argument for I = (Ï„1 , Ï„2 ) with 0 â‰¤ Ï„1 <
Ï„2 (using an analogous definition of SzÏ„1 ,Ï„2 ), and again

To conclude, we have shown that if Equation (7)
holds, then for every
interval

 I âŠ‚ R we have that
Pre âˆ¼N (00n ,Ïƒ2 InÃ—n ) tÌƒ(Î²j ) âˆˆ I
is
lower
bounded
by eâˆ’a Przâˆ¼Trâˆ’p [z âˆˆ I] and upper bounded by
ea Przâˆ¼Trâˆ’p [z âˆˆ (I/ea )].
So to conclude the proof
of Theorem 3.1, we need to show that w.h.p such a as in
Equation (7) exists.
Claim B.6. In the homoscedastic model with Gaussian
noise, if both n and r satisfy n, r â‰¥ p + â„¦(log(1/Î½)), then
âˆ’1
âˆ’1
2
T
2
T
we have that Ïƒ 2 (X T X)âˆ’1
j,j +l (M M )j,j â‰¥ l (M M )j,j
and
2(râˆ’p)
âˆ’1
âˆ’1
2
T
2
T
Ïƒ 2 (X T X)âˆ’1
j,j +l (M M )j,j â‰¤ (1+ nâˆ’p )Â·l (M M )j,j
2(râˆ’p)

nâˆ’p , Theorem 3.1 now follows
Using (1 + 2(râˆ’p)
nâˆ’p ) â‰¤ e
râˆ’p
from plugging a = nâˆ’p to our above discussion.

Differentially Private Ordinary Least Squares

Proof. The lower bound is immediate from non-negativity
T
âˆ’1/2
of Ïƒ 2 and of (X T X)âˆ’1
e j k2 . We therej,j = k(X X)
fore prove the upper bound.

parameter r w.p. â‰¥ 1 âˆ’ Î½ we correctly Î±-reject the null hypothesis using pÌƒ0 (i.e., w.p. â‰¥ 1 âˆ’ Î½ Algorithm 1 returns
matrix unaltered and we can estimate tÌƒ0 and verify that

First, observe that l2 = kPU âŠ¥ e k2 is sampled from Ïƒ 2 Â·Ï‡2nâˆ’p
as U âŠ¥ is of dimension n âˆ’ p. Therefore, it holds that w.p.
â‰¥ 1 âˆ’ Î½/2 that

indeed pÌƒ0 < Î± Â· e

Ïƒ2

âˆš

nâˆ’pâˆ’

p

râˆ’p
âˆ’ nâˆ’p

) provided

(

Ïƒ 2 (cÌƒ2 + Ï„ÌƒÎ±2 )
r â‰¥ p + max C1 2 Î±
, C2 ln(1/Î½)
Î²j Ïƒmin (Î£)

2
2 ln(2/Î½) â‰¤ l2

)

and
2

and assuming n > p+100 ln(2/Î½) we therefore have Ïƒ â‰¤
4
2
3(nâˆ’p) l .
Secondly, we argue that when r > p + 300 ln(4/Î½)
we have that w.p.
â‰¥ 1 âˆ’ Î½/2 it holds that
âˆ’1
3
T
T T
(X
X)
â‰¤
(r
âˆ’
p)(X
R RX)âˆ’1
j,j
j,j . To see this, first
4
observe that by picking R âˆ¼ N (0rÃ—n , IrÃ—r , InÃ—n ) the
distribution of the product RX âˆ¼ N (0rÃ—d , IrÃ—r , X T X)
is identical to picking Q âˆ¼ N (0rÃ—d , IrÃ—r , IdÃ—d )
and taking the product Q(X T X)1/2 .
Therefore,
the
distribution
of
(X T RT RX)âˆ’1
is

T
1/2 T
T
1/2 âˆ’1
identical to
(X X) Q Q(X X)
=
(X T X)âˆ’1/2 (QT Q)âˆ’1 (X T X)âˆ’1/2 .
Denoting
v = (X T X)âˆ’1/2e j we have kvv k2 = (X T X)âˆ’1
j,j .
Claim A.1 from (Sheffet, 2015) gives that w.p. â‰¥ 1 âˆ’ Î½/2
we have

âˆ’1
T
1/2 T
(r âˆ’ p) Â· e T
Q Q(X T X)1/2
ej
j (X X)
1
= v T ( râˆ’p
QT Q)âˆ’1v â‰¥ 43 v Tv = 34 (X T X)âˆ’1
j,j


w2
n â‰¥ max r, C3
, C4 (p + ln(1/Î½))
min{Ïƒmin (Î£), Ïƒ 2 }


16l2 (râˆ’p)
(X T RT RX)âˆ’1
j,j
nâˆ’p
âˆ’1
2
T T
â‰¤ 2(râˆ’p)
l
(X
R
RX)
j,j
nâˆ’p

and as we denote M = RX we are done.
We comment that our analysis in the proof of Claim B.6
implicitly assumes r  n (as we do think of the projection R as dimensionality reduction), and so the ratio
râˆ’p
nâˆ’p is small. However, a similar analysis holds for r
which is comparable to n â€” in which we would argue that
âˆ’1
2
T
Ïƒ 2 (X T X)âˆ’1
j,j +l (M M )j,j
Ïƒ 2 (X T X)âˆ’1

âˆˆ [1, 1 + Î·] for some small Î·.

PDFN (0,1) (x)dx =

the

numbers
râˆ’p
Î± âˆ’ nâˆ’p
e
2

=
râˆ’p
Î± âˆ’ nâˆ’p
2e

s.t.
and

resp.

Proof. First we need to use the lower bound on n to show
that indeed Algorithm 1 does not alter A, and that various
quantities are not far from their expected values. Formally,
we claim the following.
Proposition B.8. Under the same lower bounds on n and r
as in Theorem 3.3, w.p. 1âˆ’Î±âˆ’Î½ we have that Theorem 3.1
holds and also that
râˆ’p
2
2
Î¶ÌƒÎ¶ k2 = Î˜( râˆ’p
r kPU âŠ¥ e k ) = Î˜( r (n âˆ’ p)Ïƒ )

and
âˆ’1
T
1
(X T RT RX)âˆ’1
j,j = Î˜( râˆ’p (X X)j,j )

Proof of Proposition B.8. First, we need to argue that we
2
have enough samples as to have the gap Ïƒmin
([X; y]) âˆ’ w2
sufficiently large.
Since x i âˆ¼ N (0, Î£), and yi = Î² Tx i + ei with ei âˆ¼
xi â—¦ yi ) is also
N (0, Ïƒ 2 ), we have that the concatenation (x
xi ] +
sampled from a Gaussian. Clearly, E[yi ] = Î² T E[x
Î² Txi +
E[ei ] = 0. Similarly, E[xi,j yi ] = E[xi,j Â· (Î²
Î² )j and E[yi2 ] = E[e2i ] + E[kXÎ²
Î² k2 ] = Ïƒ 2 +
ei )] = (Î£Î²
Î² T X T XÎ²
Î² ] = Ïƒ 2 + Î² T Î£Î²
Î² . Therefore, each row of A is
E[Î²
an i.i.d sample of N (00p+1 , Î£A ), with

B.3. Proof of Theorem 3.3
Theorem B.7 (Theorem 3.3 restated.). Fix a positive definite matrix Î£ âˆˆ RpÃ—p . Fix parameters Î² âˆˆ Rp and Ïƒ 2 > 0
and a coordinate j s.t. Î²j 6= 0. Let X be a matrix whose n
rows are sampled i.i.d from N (00p , Î£). Let y be a vector s.t.
Î² )i is sampled i.i.d from N (0, Ïƒ 2 ). Fix Î½ âˆˆ (0, 1/2)
yi âˆ’(XÎ²
and Î± âˆˆ (0, 1/2). Then there exist constants C1 , C2 , C3
and C4 such that when we run Algorithm 1 over [X; y ] with

denote

râˆ’p
Ï„ÌƒÎ± /e nâˆ’p

Combining the two inequalities we get:
â‰¤

Ï„ÌƒÎ±

PDFTrâˆ’p (x)dx

râˆ’p
cÌƒÎ± /e nâˆ’p
Râˆž

which implies the required.

Ïƒ 2 (X T X)âˆ’1
j,j

cÌƒÎ± ,

where
Râˆž


Î£A =

Î£
Î² TÎ£

Î²
Î£Î²



Î² T Î£Î²
Î²
Ïƒ 2 +Î²

Denote Î»2 = Ïƒmin (Î£). Then, to argue that Ïƒmin (Î£A )
is large we use the lower bound from (Ma & Zarowski,
1995) (Theorem 3.1) combining with some simple
arithmetic manipulations to deduce that Ïƒmin (Î£A ) â‰¥
min{Ïƒmin (Î£), Ïƒ 2 }.

Differentially Private Ordinary Least Squares
râˆ’p

Having established a lower bound on Ïƒmin (Î£A ), it follows that with n = â„¦(p ln(1/Î½)) i.i.d draws from
N (00p+1 , Î£A ) we have w.p. â‰¤ Î½/4 that Ïƒmin (AT A) =
o(n) Â· min{Ïƒmin (Î£), Ïƒ 2 }. Conditioned on Ïƒmin (AT A) =
â„¦(nÏƒmin (Î£A )) = â„¦(w2 ) being large enough, we have that
w.p. â‰¤ Î½/4 over the randomness of Algorithm 1 the matrix
A does not pass the if-condition and the output of the algorithm is not RA. Conditioned on Algorithm 1 outputting
RA, and due to the lower bound r = p + â„¦(ln(1/Î½)),
we have that the result of Theorem 3.1 does not hold w.p.
â‰¤ Î± + Î½/4. All in all we deduce that w.p. â‰¥ 1 âˆ’ Î± âˆ’ 3Î½/4
the result of Theorem 3.1 holds. And since we argue Theorem 3.1 holds, then the following two bounds that are used
in the proof14 also hold:

= â„¦(e nâˆ’p (cÌƒÎ± + Ï„ÌƒÎ± )ÏƒÌƒ



(cÌƒÎ± +Ï„ÌƒÎ± )2 Ïƒ 2
Î²j2 Ïƒmin (Î£)



C. Projected Ridge Regression
In this section we deal with the case that our matrix does
not pass the if-condition of Algorithm 1. In this case, the
matrix is appended
 with a dÃ— d-matrix which is wIdÃ—d .
A
0
Denoting A =
we have that the algorithmâ€™s
w Â· IdÃ—d
output is RA0 .
Similarly to before, we are going to denote d = p + 1 and
decompose A = [X; y ] with X âˆˆ RnÃ—p and y âˆˆ Rn , with
Î² + e and ei sampled
the standard assumption of y = XÎ²
i.i.d from N (0, Ïƒ 2 ).15 We now need to introduce some additional notation. We denote the appended matrix and vectors X 0 and y 0 s.t. A0 = [X 0 ; y 0 ]. Meaning:
ï£¹
ï£®
X
X 0 = ï£° wIpÃ—p ï£»
0T
p

2

kPU âŠ¥ e k = Î˜((n âˆ’ p)Ïƒ )
Lastly, in the proof of Theorem 3.1 we argue that
for a given PU âŠ¥ e the length kÎ¶ÌƒÎ¶ k2 is distributed like
kPU âŠ¥ e k2 2
Ï‡râˆ’p .
r

Appealing again to the fact that r = p +
â„¦(ln(1/Î½) we have that w.p. â‰¥ Î½/4 it holds that kÎ¶ÌƒÎ¶ k2 >
kP

(X T RT RX)âˆ’1
j,j ))

which, given the lower bound r = p + â„¦
indeed holds.

âˆ’1
T
1
(X T RT RX)âˆ’1
j,j = Î˜( râˆ’p (X X)j,j )
2

q

e k2

2(r âˆ’ p) U râŠ¥ . Plugging in the value of kPU âŠ¥ e k2 concludes the proof of the proposition.

and
Based on Proposition B.8, we now show that we indeed
reject the null-hypothesis (as we should). When Theorem 3.1 holds, reject the null-hypothesis iff pÌƒ0 < Î± Â·
e

râˆ’p
âˆ’ nâˆ’p

ï£¹
ï£¹
ï£®
y
e
def
Î² ï£» = X 0Î² + e 0
y 0 = ï£° 0 p ï£» = X 0Î² + ï£° âˆ’wÎ²
w
w
ï£®

râˆ’p

which holds iff |tÌƒ0 | > e nâˆ’p Ï„ÌƒÎ± . This implies
râˆ’p

And so we respectively denote R = [R1 ; R2 ; R3 ] with
R1 âˆˆ RrÃ—n , R2 âˆˆ RrÃ—p and R3 âˆˆ RrÃ—1 (so R3 is a
vector denoted as a matrix). Hence:

nâˆ’p Ï„Ìƒ Â·
we
Î±
q reject that null-hypothesis when |Î²Ìƒj | > e
âˆ’1
ÏƒÌƒ (X T RT RX)j,j ). Note that this bound is based

onCorollary 3.2 that determines
 that |Î²Ìƒj âˆ’ Î²j | =
q
râˆ’p
âˆ’1
T
T
O e nâˆ’p cÌƒÎ± Â· ÏƒÌƒ (X R RX)j,j ) . And so we have that

M 0 = RX 0 = R1 X + wR2
and

w.p. â‰¥ 1 âˆ’ Î½ we Î±-reject the null hypothesis when it holds
q
râˆ’p
nâˆ’p (cÌƒ +
that |Î²j | > 3(cÌƒÎ± + Ï„ÌƒÎ± )Â· ÏƒÌƒ (X T RT RX)âˆ’1
)
â‰¥
e
Î±
j,j
q
Ï„ÌƒÎ± )ÏƒÌƒ (X T RT RX)âˆ’1
j,j ) (due to the lower bound n â‰¥ r).

Î² +R1e +wR3
Ryy 0 = RX 0Î² +Ree0 = R1y +wR3 = R1 XÎ²

Based on the bounds stated above we have that
q
q
q
âˆš
âˆš
r
r
ÏƒÌƒ = kÎ¶ÌƒÎ¶ k râˆ’p
= Î˜(Ïƒ n âˆ’ p râˆ’p
r
râˆ’p ) = Î˜(Ïƒ n âˆ’ p)



1
râˆ’p

Â·

1
nÏƒmin (Î£)

And so, a sufficient condition for rejecting the nullhypothesis is to have
r


nâˆ’p q
1
|Î²j |
= â„¦ (cÌƒÎ± + Ï„ÌƒÎ± )Ïƒ
Â· nÏƒmin (Î£)
râˆ’p
14

Î²0

= arg min 1r kRyy 0 âˆ’ RX 0z k2
z

= (X 0T RT RX 0 )âˆ’1 (RX 0 )T (Ryy 0 )

and that
âˆ’1
T
1
(X T RT RX)âˆ’1
j,j = Î˜( râˆ’p (X X)j,j ) = O

And so, using the output RA0 of Algorithm 1, we solve
the linear regression problem derived from âˆš1r RX 0 and
âˆš1 Ry
y 0 . I.e., we set
r

More accurately, both are bounds shown in Claim B.6.


Sarlosâ€™ results (2006) regarding the Johnson Lindenstrauss
transform give that, when R has sufficiently many rows,
solving the latter optimization problem gives a good approximation for the solution of the optimization problem
Î² R = arg minz kyy 0 âˆ’ X 0z k2 = arg minz kyy âˆ’ Xzz k2 + w2 kzz k2
15
Just as before, it is possible to denote any single column as y
and any subset of the remaining columns as X.



Differentially Private Ordinary Least Squares

The latter problem is known as the Ridge Regression problem. Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from
the perspective of penalizing linear vectors whose coefficients are too large. It is also often applied in the case
where X doesnâ€™t have full rank or is close to not having full-rank. That is because the Ridge Regression problem is always solvable. One can show that the minimizer
Î² R = (X T X + w2 IpÃ—p )âˆ’1 X Ty is the unique solution of
the Ridge Regression problem and that the RHS is always
defined (even when X is singular).
The original focus of Ridge Regression is on penalizing
Î² R for having large coefficients. Therefore, Ridge Regression actually poses a family of linear regression problems: minz ky âˆ’ Xzz k + Î»kzz k2 , where one may set Î» to be
any non-negative scalar. And so, much of the literature on
Ridge Regression is devoted to the art of fine-tuning this
penalty term â€” either empirically or based on the Î» that
Î² R ] âˆ’ Î² k2 + Var(Î²
Î² R ).16 Here we
yields the best risk: kE[Î²
propose a fundamentally different approach for the choice
of the normalization factor â€” we set it so that solution of
the regression problem would satisfy (, Î´)-differential privacy (by projecting the problem onto a lower dimension).
While the solution of the Ridge Regression problem might
have smaller risk than the OLS solution, it is not known
how to derive t-values and/or reject the null hypothesis under Ridge Regression (except for using X to manipulate
Î² = (X T X)âˆ’1 X Ty and relying on OLS).
Î² R back into Î²Ì‚
In fact, prior to our work there was no need for such analysis! For confidence intervals one could just use the standard
OLS, because access to X and y was given.
Therefore, much for the same reason, we are unable to derive t-values under projected Ridge Regression.17 Clearly,
there are situations where such confidence bounds simply
cannot be derived. (Consider for example the case where
X = 0nÃ—p and y is just i.i.d draws from N (0, Ïƒ 2 ), so
obviously [X; y] gives no information about Î² .) Nonetheless, under additional assumptions about the data, our work
can give confidence intervals for Î²j , and in the case where
the interval doesnâ€™t intersect the origin â€” assure us that
sign(Î²j0 ) = sign(Î²j ) w.h.p.
Clearly, Sarlosâ€™ work (2006) gives an upper bound on the
Î² 0 âˆ’Î²
Î² R k. However, such distance bound doesnâ€™t
distance kÎ²
come with the coordinate by coordinate confidence guarantee we would like to have. In fact, it is not even clear
Î² 0 ] = Î² R (though it is obvious
from Sarlosâ€™ work that E[Î²
0T T
0 Î²R
to see that E[(X R RX )]Î² = E[(RX 0 )T Ryy 0 ]). Here,

Î² 0 ] = Î²Ì‚
Î² which, more often than not, does
we show that E[Î²
R
not equal Î² .
Comment about notation. Throughout this section we assume X is of full rank and so (X T X)âˆ’1 is well-defined. If
X isnâ€™t full-rank, then one can simply replace any occurrence of (X T X)âˆ’1 with X + (X + )T . This makes all our
formulas well-defined in the general case.
C.1. Running OLS on the Projected Data
In this section, we analyze the projected Ridge Regression,
under the assumption (for now) that e is fixed. That is, for
now we assume that the only source of randomness comes
from picking the matrix R = [R1 ; R2 ; R3 ]. As before, we
analyze the distribution over Î² 0 (see Equation (9)), and the
value of the function we optimize at Î² 0 . Denoting M 0 =
RX 0 , we can formally express the estimators:
Î²0
Î¶0

= (M 0T M 0 )âˆ’1 M 0T Ryy 0
= âˆš1r (Ryy 0 âˆ’ RX 0Î² 0 )

(9)
(10)

Î² +ee for a fixed e , and given
Claim C.1. Given that y = XÎ²
X and M 0 = RX 0 = R1 X + wR2 we have that

Î² 0 âˆ¼ N Î² + X +e ,

Î² + X +e k2 + 1) + kPU âŠ¥ e k2 )(M 0T M 0 )âˆ’1
(w2 (kÎ²

Î¶ 0 âˆ¼ N 0r ,

Î² +X +e k2 +1)+kPU âŠ¥ e k2
w2 (kÎ²
0
0+
(I
âˆ’
M
M
)
rÃ—r
r
and furthermore, Î² 0 and Î¶ 0 are independent of one another.

Proof. First, we write Î² 0 and Î¶ 0 explicitly, based on e and
projection matrices:
Î²0
Î¶0

= (M 0T M 0 )âˆ’1 M 0T Ryy 0
Î² + M 0+ (R1e + wR3 )
= M 0+ (R1 X)Î²
= âˆš1r (Ryy 0 âˆ’ RX 0Î² 0 )
= âˆš1r (IrÃ—r âˆ’ M 0 M 0+ )Ree0
= âˆš1r PU 0âŠ¥ (R1e âˆ’ wR2Î² + wR3 )

with U 0 denoting colspan(M 0 ) and PU 0âŠ¥ denoting the projection onto the subspace U 0âŠ¥ .

16

Ridge Regression, as opposed to OLS, does not yield an unÎ² R ] 6= Î² .
biased estimator. I.e., E[Î²
17
Note: The naÄ±Ìˆve approach of using RX 0 and Ryy 0 to interpolate RX and Ryy and then apply Theorem 3.1 using these estimations of RX and Ryy ignores the noise added from appending the
matrix A into A0 , and it is therefore bound to produce inaccurate
estimations of the t-values.

Again, we break e into an orthogonal composition: e =
PU e + PU âŠ¥ e with U = colspan(X) (hence PU = XX + )
and U âŠ¥ = colspan(X)âŠ¥ . Therefore,
Î²0

Î² + M 0+ (R1 XX +e + R1 PU âŠ¥ e + wR3 )
= M 0+ (R1 X)Î²
0+
Î² + X +e ) + M 0+ (R1 PU âŠ¥ e + wR3 )
= M (R1 X)(Î²

Differentially Private Ordinary Least Squares

whereas Î¶ 0 is essentially
âˆš1 (IrÃ—r
r
(âˆ—)

âˆ’ M 0 M 0+ )(R1 XX +e + R1 PU âŠ¥ e âˆ’ wR2Î² + wR3 )

âˆš1 (IrÃ—r
r

=

0

âˆ’M M

0+

)Â·

Î² + wR3 )
(R1 XX e + R1 PU âŠ¥ e + (M 0 âˆ’ wR2 )Î²
+

=

âˆš1 (IrÃ—r
r

âˆ’ M 0 M 0+ )Â·

Î² + X +e k2 + 1) + kPU âŠ¥ e k2 )PU 0âŠ¥
I.e., N 0 r , 1r (w2 (kÎ²
0
as PU 0âŠ¥ M = 0rÃ—r .



Finally, observe that Î² 0 and Î¶ 0 are independent as the former depends on the projection of the spherical Gaussian
R1 X(Î² + X +e ) + R1 PU âŠ¥ e + wR3 on U 0 , and the latter
depends on the projection of the same multivariate Gaussian on U 0âŠ¥ .

Î² + X +e ) + R1 PU âŠ¥ e + wR3 )
(R1 X(Î²
where equality (âˆ—) holds because (I âˆ’ M 0 M 0+ )M 0v = 0
for any v .
We now aim to describe the distribution of R given that we
know X 0 and M 0 = RX 0 . Since
M 0 = R1 X + wR2 + 0 Â· R3 = R1 X(X + X) + wR2
= (R1 PU )X + wR2
then M 0 is independent of R3 and independent of R1 PU âŠ¥ .
Therefore, given X and M 0 the induced distribution over
R3 remains R3 âˆ¼ N (00r , IrÃ—r ), and similarly, given X and
M 0 we have R1 PU âŠ¥ âˆ¼ N (0rÃ—n , IrÃ—r , PU âŠ¥ ) (rows remain
independent from one another, and each row is distributed
like a spherical Gaussian in colspan(X)âŠ¥ ). And so, we
have that R1 X = R1 PU X = M 0 âˆ’ wR2 , which in turn
implies:

R1 X âˆ¼ N M 0 , IrÃ—r , w2 Â· IpÃ—p

Observe that Claim C.1 assumes e is given. This may seem
somewhat strange, since without assuming anything about
e there can be many combinations of Î² and e for which
Î² + e . However, we always have that Î² + X +e =
y = XÎ²
+
Î² . Similarly, it is always the case the PU âŠ¥ e =
X y = Î²Ì‚
Î² and Î¶ in
(I âˆ’ XX + )yy = Î¶ . (Recall OLS definitions of Î²Ì‚
Equation (1) and (2).) Therefore, the distribution of Î² 0 and
Î¶ 0 is unique (once y is set):


Î² , (w2 (kÎ²Ì‚
Î² k2 + 1) + kÎ¶Î¶ k2 )(M 0T M 0 )âˆ’1
Î² 0 âˆ¼ N Î²Ì‚
!
2 Î² 2
2
Î¶
w
(k
Î²Ì‚
k
+
1)
+
kÎ¶
k
Î¶ 0 âˆ¼ N 0r ,
(IrÃ—r âˆ’ M 0 M 0+ )
r
And so for a given dataset [X; y ] we have that Î² 0 serves as
Î².
an approximation for Î²Ì‚

multiplying this random matrix with a vector, we get

An immediate corollary of Claim C.1 is that for
any fixed e it holds that the quantity t0 (Î²j ) =

Î² +X +e ) âˆ¼ N (M 0Î² + M 0 X +e , w2 kÎ²Î² + X +e k2 IrÃ—r )
R1 X(Î²

kÎ¶Î¶ 0 k

and multiplying this random vector with a matrix we get
Î² +X +e ) âˆ¼ N (Î² + X +e ,
M 0+ R1 X(Î²

Î² + X +e k2 (M 0T M )âˆ’1 )
w2 kÎ²

I.e.,
Î² +X +e ) âˆ¼ kÎ²
Î² +X +e kÂ·N (u
u, w2 (M 0T M )âˆ’1 )
M 0+ R1 X(Î²
where u denotes a unit-length vector in the direction of Î² +
X +e .
Similar to before we have
RPU âŠ¥ âˆ¼ N (0rÃ—n , IrÃ—r , PU âŠ¥ )
â‡’ M 0+ (RPU âŠ¥ e) âˆ¼ N (00d , kPU âŠ¥ ek2 (M 0T M 0 )âˆ’1 )
wR3 âˆ¼ N (00r , w2 IrÃ—r )
â‡’ M 0+ (wR3 ) âˆ¼ N (00d , w2 (M 0+ M 0 )âˆ’1 )
Therefore, the distribution of Î² 0 , which is the sum of the 3
independent Gaussians, is as required.
Î² + X +e ) + R1 PU âŠ¥ e + wR3 )
Also, Î¶ 0 = âˆš1r PU 0âŠ¥ (R1 X(Î²
is the sum of 3 independent Gaussians, which implies its
distribution is

Î² + X +e ),
N âˆš1r PU 0âŠ¥ M 0 (Î²

2
+ 2
2
1
Î²
(w
(kÎ²
+
X
e
k
+
1)
+
kP
âŠ¥ e k )PU 0âŠ¥
U
r

Î²j0 âˆ’(Î²j +(X +e )j )
q r
0T
0 âˆ’1
râˆ’p Â·(M M )j,j

=

kÎ¶Î¶ 0 k

Î² 0 âˆ’Î²Ì‚j
q rj
0T
0 âˆ’1
râˆ’p Â·(M M )j,j

is dis-

tributed like a Trâˆ’p -distribution. Therefore, the following
theorem follows immediately.
Î² =
Theorem C.2. Fix X âˆˆ RnÃ—p and y âˆˆ R. Define Î²Ì‚
X +y and Î¶ = (I âˆ’ XX + )yy . Let RX 0 and Ryy 0 denote
the result of applying Algorithm 1 to the matrix A = [X; y ]
when the algorithm appends the data with a w Â· I matrix.
Fix a coordinate j and any Î± âˆˆ (0, 1/2). When computing
Î² 0 and Î¶ 0 as in Equations (9) it and (10), we have that w.p.
â‰¥ 1 âˆ’ Î± it holds that
q


r
Î²Ì‚j âˆˆ Î²j0 Â± c0Î± kÎ¶Î¶ 0 k râˆ’p
Â· (M 0T M 0 )âˆ’1
j,j
where c0Î± denotes the number such that (âˆ’c0Î± , c0Î± ) contains
1 âˆ’ Î± mass of the Trâˆ’p -distribution.
Note that Theorem C.2, much like the rest of the discussion in this Section, builds on y being fixed, which means
Î²j0 serves as an approximation for Î²Ì‚j . Yet our goal is to
argue about similarity (or proximity) between Î²j0 and Î²j .
To that end, we combine the standard OLS confidence interval â€” which says that w.p. â‰¥ 1 âˆ’ Î± over the randomness of picking r
e in the homoscedastic model we have
|Î²j âˆ’ Î²Ì‚j | â‰¤ cÎ± kÎ¶Î¶ k

(X T X)âˆ’1
j,j
nâˆ’p

â€” with the confidence in-

terval of Theorem C.2 above, and deduce that w.p. â‰¥ 1 âˆ’ Î±

Differentially Private Ordinary Least Squares

we have that |Î²j0 âˆ’ Î²j | is at most

1 âˆ’ Î½ over the randomness of R we have (r âˆ’

kÎ¶Î¶ 0 k2
âˆ’1
2
T
p)(M 0T M )âˆ’1
j,j = Î˜ (w IpÃ—p + X X)j,j and râˆ’p =

q
q
ï£¶
kÎ¶Î¶ k (X T X)âˆ’1
kÎ¶Î¶ 0 k r(M 0T M 0 )âˆ’1
j,j
j,j
ï£¸
âˆš
âˆš
O ï£­cÎ±
+ c0Î±
nâˆ’p
râˆ’p
ï£«

Î˜( w

(11)
And so, in the next section, our goal is to give conditions under which the interval of Equation (11) isnâ€™t
much larger
q in comparison to the interval length of
18

Î¶0

Î¶ k
c0Î± âˆškÎ¶râˆ’p
r(M 0T M 0 )âˆ’1
j,j we get from Theorem C.2; and
more importantly â€” conditions that make the interval of
Theorem C.2 useful q
and not too large. (Note, in expecta-

tion

kÎ¶Î¶ 0 k
âˆš
râˆ’p

is about

Î² k2 + kÎ¶Î¶ k2 )/r. So, for
(w2 + w2 kÎ²Ì‚

Î² k is very large, this interval
example, in situations where kÎ²Ì‚
isnâ€™t likely to inform us as to the sign of Î²j .)
Motivating Example. A good motivating example for the
discussion in the following section is when [X; y ] is a strict
submatrix of the dataset A. That is, our data contains many
variables for each entry (i.e., the dimensionality d of each
entry is large), yet our regression is made only over a modest subset of variables out of the d. In this case, the least
singular value of A might be too small, causing the algorithm to alter A; however, Ïƒmin (X T X) could be sufficiently large so that had we run Algorithm 1 only on [X; y ]
we would not alter the input. (Indeed, a differentially private way for finding a subset of the variables that induce a
submatrix with high Ïƒmin is an interesting open question,
partially answered â€” for a single regression â€” in the work
of Thakurta and Smith (Thakurta & Smith, 2013).) Indeed,
the conditions we specify in the following section depend
on Ïƒmin ( n1 X T X), which, for a zero-mean data, the minimal variance of the data in any direction. For this motivating example, indeed such variance isnâ€™t necessarily small.
C.2. Conditions for Deriving a Confidence Interval for
Ridge Regression

2

Î² k2 +kÎ¶Î¶ k2
+w2 kÎ²Ì‚
).
r

Proof. The former bound follows from known results on
the Johnson-Lindenstrauss transform (as were shown in the
proof of Claim B.6). The latter bound follows from standard concentration bounds of the Ï‡2 -distribution.

Plugging in the result of Proposition C.3 to Equation (11)
we get that w.p. â‰¥ 1 âˆ’ Î½ the difference |Î²j0 âˆ’ Î²j | is at most
q

kÎ¶Î¶ k
O cÎ± âˆš
(X T X)âˆ’1
j,j
nâˆ’p
s

Î² k2 + kÎ¶Î¶ k2 q 2
w2 + w2 kÎ²Ì‚
+ c0Î±
(w IpÃ—p + X T X)âˆ’1
j,j
râˆ’p
(12)
We will also use the following proposition.
Proposition C.4.
(X T X)âˆ’1
j,j â‰¤


1+

w2
Ïƒmin (X T X)



(w2 IpÃ—p + X T X)âˆ’1
j,j

Proof. We have that
(X T X)âˆ’1
= (X T X)âˆ’1 (X T X + w2 IpÃ—p )(X T X + w2 IpÃ—p )âˆ’1
= (X T X + w2 IpÃ—p )âˆ’1 + w2 (X T X)âˆ’1 (X T X + w2 IpÃ—p )âˆ’1
= (IpÃ—p + w2 (X T X)âˆ’1 )(X T X + w2 IpÃ—p )âˆ’1

Looking at the interval specified in Equation (11), we now
give an upper bound on the the random quantities in this
interval: kÎ¶Î¶ k, kÎ¶Î¶ 0 k, and (M 0T M 0 )âˆ’1
j,j . First, we give bound
that are dependent on the randomness in R (i.e., we continue to view e as fixed).

= (X T X + w2 IpÃ—p )âˆ’1/2 Â·

Proposition C.3. For any Î½ âˆˆ (0, 1/2), if we
have r = p + â„¦(ln(1/Î½)) then with probability â‰¥

where the latter holds because (IpÃ—p + w2 (X T X)âˆ’1 ) and
(X T X + w2 IpÃ—p )âˆ’1 are diagonalizable by the same matrix V (the same matrix for which (X T X) = V S âˆ’1 V T ).
2
Since we have kIpÃ—p + w2 (X T X)âˆ’1 k = 1 + Ïƒ2 w (X) , it is

18

Observe that w.p.

â‰¥ 1 âˆ’ Î± r
over the randomness of e

we have that |Î²j âˆ’ Î²Ì‚j | â‰¤ cÎ± kÎ¶Î¶ k

(X T X)âˆ’1
j,j
nâˆ’p

, and w.p.

â‰¥

1 âˆ’ Î± over the randomness of R we have that |Î²j0 âˆ’ Î²Ì‚j | â‰¤
q
r
c0Î± kÎ¶Î¶ 0 k râˆ’p
Â· (M 0T M 0 )âˆ’1
j,j . So technically, to give a (1 âˆ’ Î±)Î²j0

confidence interval around
that contains Î²j w.p. â‰¥ 1 âˆ’ Î±, we
need to use cÎ±/2 and c0Î±/2 instead of cÎ± and c0Î± resp. To avoid
overburdening the reader with what we already see as too many
parameters, we switch to asymptotic notation.

(IpÃ—p + w2 (X T X)âˆ’1 )Â·
(X T X + w2 IpÃ—p )âˆ’1/2

min

w2
)I .
2
Ïƒmin
(X) pÃ—p
T
T
âˆ’1
e j (X X) e j â‰¤ (1 +

clear that (IpÃ—p + w2 (X T X)âˆ’1 )  (1 +
We deduce that (X T X)âˆ’1
j,j =
w2
)(X T X
2
Ïƒmin
(X)

+ w2 IpÃ—p )âˆ’1
j,j .

Based on Proposition C.4 we get from Equation (12) that

Differentially Private Ordinary Least Squares

|Î²j0 âˆ’ Î²j | is at most
v
u
w2
2
 u
t kÎ¶Î¶ k (1 + Ïƒmin (X T X) )
O( cÎ±
+
nâˆ’p
s
Î² k2 + kÎ¶Î¶ k2 q 2
w2 + w2 kÎ²Ì‚
(w IpÃ—p + X T X)âˆ’1
c0Î±
j,j )
râˆ’p
(13)
And so, if it happens to be the case that exists some small
Î² , Î¶ and w2 satisfy
Î· > 0 for which Î²Ì‚
!
2
2
2 Î² 2
2
kÎ¶Î¶ k2 (1 + Ïƒminw(X T X) )
Î¶
w
+
w
k
Î²Ì‚
k
+
kÎ¶
k
â‰¤ Î·2
nâˆ’p
râˆ’p
(14)
then
we
have
that
Pr[Î²
âˆˆ
j


q
Î²j0

Â± O((1 + Î·) Â·

1 âˆ’ Î±.

c0Î± kÎ¶Î¶ 0 k

r
râˆ’p

Â·

(M 0T M 0 )âˆ’1
j,j )

]

â‰¥

19

Moreover, if in this case |Î²j | >
q
q
2
2 kÎ²Ì‚
Î² k2 +kÎ¶Î¶ k2
+ Î·) w +w râˆ’p
(w2 IpÃ—p + X T X)âˆ’1
j,j
then Pr[sign(Î²j0 ) = sign(Î²j )] â‰¥ 1 âˆ’ Î±. This is precisely
what Claims C.5 and C.6 below do.
Claim C.5. If there
exists Î·
>
0
2
2
s.t.
n âˆ’ p
â‰¥
(r
âˆ’
p)
and
n
=
2
Î·


2
1
3/2 B ln(1/Î´)
Â· 2
, then Pr[Î²j âˆˆ
â„¦ r
Â·
1

Î· Ïƒmin ( n X T X)


q
r
Î²j0 Â± O((1 + Î·) Â· c0Î± kÎ¶Î¶ 0 k râˆ’p
Â· (M 0T M 0 )âˆ’1
â‰¥
j,j ) ]
1 âˆ’ Î±.

c0Î± (1

Proof. Based on the above discussion, it is enough to argue that under the conditions of the claim, the constraint
2
râˆ’p
then
of Equation (14) holds. Since we require Î·2 â‰¥ nâˆ’p
it is evident that
kÎ¶Î¶ k2
nâˆ’p

2

kÎ¶Î¶ k2
nâˆ’p

Î· 2 kÎ¶Î¶ k2
2(râˆ’p) . So we now show that
Î· 2 kÎ¶Î¶ k2
2(râˆ’p) under the conditions of the

â‰¤

Â· Ïƒminw(X T X) â‰¤
claim, and this will show the required. All that is left is
some algebraic manipulations. It suffices to have:
Î·2
2

Â·

nâˆ’p
T
râˆ’p Ïƒmin (X X)

Î·2
2

â‰¥

32B

â‰¥
which holds for n2 â‰¥ r3/2 Â·
as we assume to hold.

Â·

T
n2
1
r Ïƒmin ( n X X)
âˆš
2

r ln(8/Î´)
â‰¥ w2


64B 2 ln(1/Î´)
Ïƒmin ( n1 X T X)âˆ’1 ,
Î· 2

Claim C.6. Fix Î½ âˆˆ (0, 12 ). If (i) n = p + â„¦(ln(1/Î½)),
Î² k2 = â„¦(Ïƒ 2 kX + k2F ln( Î½p )) and (iii) r âˆ’ p =
(ii) kÎ²


â„¦

(c0Î± )2 (1+Î·)2
Î²j2

Î² k2 +
1 + kÎ²

Ïƒ2
1
Ïƒmin ( n X T X)

, then in the

Proof. Based on the above discussion, we aim to show that
in the homoscedastic model (where each coordinate ei âˆ¼
N (0, Ïƒ 2 ) independently) w.p. â‰¥ 1 âˆ’ Î½ it holds that the
magnitude of Î²j is greater than
s
c0Î± (1 + Î·)

Î² k2 + kÎ¶Î¶ k2 q 2
w2 + w2 kÎ²Ì‚
(w IpÃ—p + X T X)âˆ’1
j,j
râˆ’p

To show this, we invoke Claim A.4 to argue that w.p. â‰¥
1 âˆ’ Î½ we have (i) kÎ¶Î¶ k2 â‰¤ 2Ïƒ 2 (n âˆ’ p) (since n = p +
Î² k2 â‰¤ 2kÎ²
Î² k2 (since kÎ²
Î² âˆ’ Î²Ì‚
Î² k2 â‰¤
â„¦(ln(1/Î½))), and (ii) kÎ²Ì‚
p
2
+ 2
2
2
+
2
Î² k = â„¦(Ïƒ kX kF ln( Î½p ))).
Ïƒ kX kF ln( Î½ ) whereas kÎ²
2
We also use the fact that (w2 IpÃ—p + X T X)âˆ’1
j,j â‰¤ (w +
âˆ’1
T
Ïƒmin (X X)), and then deduce that
s

Î² k2 + kÎ¶Î¶ k2 q 2
w2 + w2 kÎ²Ì‚
(w IpÃ—p + X T X)âˆ’1
j,j
râˆ’p
s
Î² k2 ) + Ïƒ 2 (n âˆ’ p)
(1 + Î·)c0Î±
w2 (1 + kÎ²
â‰¤ âˆš
2
2
râˆ’p
w + Ïƒmin (X T X)
s
(1 + Î·)c0Î±
2Ïƒ 2 (n âˆ’ p)
Î² k2 ) +
â‰¤ âˆš
2(1 + kÎ²
â‰¤ |Î²j |
râˆ’p
Ïƒmin (X T X)

(1 + Î·)c0Î±

due to our requirement on r âˆ’ p.
Observe, out of the 3 conditions specified in Claim C.6,
condition (i) merely guarantees that the sample is large
enough to argue that estimations are close to their expect
value; and condition (ii) is there merely to guarantee that
Î² k â‰ˆ kÎ²
Î² k. It is condition (iii) which is non-trivial to
kÎ²Ì‚
hold, especially together with the conditions of Claim C.5
that pose other constraints in regards to r, n, Î· and the various other parameters in play. It is interesting to compare
the requirements on r to the lower bound we get in Theorem 3.3 â€” especially the latter bound. The two bounds
are strikingly similar, with the exception that here we also
Î² k2
require r âˆ’ p to be greater than 1+kÎ²
. This is part of the
Î²j2
unfortunate effect of altering the matrix A: we cannot give
confidence bounds only for the coordinates j for which Î²j2
Î² k2 .
is very small relative to kÎ²
In summary, we require to have n = p + â„¦(ln(1/Î½)) and
Î² k compathat X contains enough sample points to have kÎ²Ì‚
Î² k, and then set r and Î· such that (it is convenient
rable to kÎ²
to think of Î· as a small constant, say, Î· = 0.1)
â€¢ r âˆ’ p = O(Î· 2 (n âˆ’ p)) (which implies r = O(n))

homoscedastic model, with probability â‰¥ 1âˆ’Î½ âˆ’Î± we have
that sign(Î²j ) = sign(Î²j0 ).


2
3
n2
â€¢ r = O( Î· 2 B 2 ln(1/Î´)
Ïƒmin ( n1 X T X) )

19
We assume n â‰¥ r so cÎ± < c0Î± as the Tnâˆ’p -distribution is
closer to a normal Gaussian than the Trâˆ’p -distribution.

Î²k
â€¢ r âˆ’ p = â„¦( 1+kÎ²
+
Î²2
Î²

j

2

Ïƒ2
Î²j2

âˆ’1 1
Â· Ïƒmin
( n X T X))

Differentially Private Ordinary Least Squares

to have that the (1 âˆ’ Î±)-confidence interval around Î²j0
does not intersect the origin. Once again, we comment
that these conditions are sufficient but not necessary, and
furthermore â€” even with these conditions holding â€” we
do not make any claims of optimality of our confidence
bound. That is because from Proposition C.4 onwards our
discussion uses upper bounds that do not have corresponding lower bounds, to the best of our knowledge.

D. Confidence Intervals for â€œAnalyze Gaussâ€
Algorithm
To complete the picture, we now analyze the â€œAnalyze
Gaussâ€ algorithm of Dwork et al (Dwork et al., 2014).
Algorithm 2 works by adding random Gaussian noise to
AT A, where the noise is symmetric with each coordi2
nate abovethe diagonal
 sampled i.i.d from N (0, âˆ† ) with

âˆ†2 = O B 4 log(1/Î´)
.20 Using the same notation for a
2

sub-matrix of A as [X; y ] as before, with X âˆˆ RnÃ—p and
y âˆˆ Rn , we denote the output of Algorithm 2 as
ï£«
ï£¬
ï£¬
ï£¬
ï£­

ï£¶

^
TX
X
TX
yg

ï£«
ï£·
g
ï£¬
Ty ï£·
X
ï£·=ï£¬
ï£¸ ï£­
Ty
yg

ï£¶
T

X X +N
y TX + nT

Ty

X y +n ï£·
ï£·
ï£¸
y Ty + m

(15)
where N is a symmetric p Ã— p-matrix, n is a p-dimensional
vector and m is a scalar, whose coordinates are sampled
i.i.d from N (0, âˆ†2 ).
Using the output of Algorithm 2, it is simple to derive anaÎ² and kÎ¶Î¶ k2 (Equations (1) and (2))
logues of Î²Ì‚
âˆ’1

âˆ’1
g
^
TX
Ty = X T X + N
X
Î²e = X
(X T y + n )
(16)

r
+âˆ†

âˆ’2

âˆš
^
TX
X
j,j Â· ln(1/Î½) Â· (B p + 1)



where Ï is such that Ï2 is w.h.p an upper bound on Ïƒ 2 ,
defined as
2 def

Ï =



1âˆš
âˆš
nâˆ’pâˆ’2 ln(4/Î±)

2
Â·




âˆš
âˆ’1
B2 p p
2
g
^
T
2
kÎ¶Î¶ k âˆ’ C Â· âˆ† 1âˆ’Î· ln(1/Î½) + âˆ† kX X kF Â· ln(p/Î½)

for some large constant C.
We comment that in practice, instead of using Ï, it might
be better to use the MLE of Ïƒ 2 , namely:


âˆ’1
def
2 ^
1
g
2
2
T
Ïƒ = nâˆ’p kÎ¶Î¶ k + âˆ† kX X kF
instead of Ï2 , the upper bound we derived for Ïƒ 2 . (Replacing an unknown variable with its MLE estimator is a common approach in applied statistics.) Note that the assumpÎ² k â‰¤ B is fairly benign once we assume each
tion that kÎ²
Î² k â‰¤ B simrow has bounded l2 -norm. The assumption kÎ²Ì‚
Î²
ply assumes that Î²Ì‚ is a reasonable estimation of Î² , which is
likely to hold if we assume that X T X is well-spread. The
assumption about the magnitude of the least singular value
of X T X is therefore the major one. Nonetheless, in the
case we considered before where each row in X is sampled
i.i.d from N (00, Î£), this assumption
merely means that n is
âˆš
large enough s.t. n = â„¦Ìƒ(

âˆ† p ln(1/Î½)
Î·Â·Ïƒmin (Î£) ).

In order to prove Theorem D.1, we require the following
proposition.

(17)

Proposition D.2. Fix any Î½ âˆˆ (0, 21 ). Fix any matrix
M âˆˆ RpÃ—p . Let v âˆˆ Rp be a vector with each coordinate
sampled independently
from a Gaussian N (0, âˆ†2 ).i Then
h
p
we have that Pr kMvv k > âˆ† Â· kM kF 2 ln(2p/Î½) < Î½.

g
Î¶ k2 to get a
We now argue that it is possible to use Î²ej and kÎ¶
confidence interval for Î²j under certain conditions.
Theorem D.1. Fix Î±, Î½ âˆˆ (0, 12 ). Assume that there exists
p
Î· âˆˆ (0, 21 ) s.t. Ïƒmin (X T X) > âˆ† p ln(1/Î½)/Î·. Under the
homoscedastic model, given Î² and Ïƒ 2 , if we assume also
Î² k â‰¤ B and kÎ²Ì‚
Î² k = k(X T X)âˆ’1 X Ty k â‰¤ B, then
that kÎ²
w.p. â‰¥ 1 âˆ’ Î± âˆ’ Î½ it holds that |Î²j âˆ’ Î²ej | it at most
s


âˆ’1
âˆ’2
p
^
^
TX
TX
O ÏÂ·
X
+
âˆ†
p
ln(1/Î½)
Â·
X
ln(1/Î±)
j,j
j,j

Proof. Given M , we have that Mvv âˆ¼ N (00, âˆ†2 Â· M M T ).
Denoting M â€™s singular values as sv1 , . . . , svp , we can rotate Mvv without affecting its l2 -norm and infer that kMvv |2
is distributed like a sum on p independent Gaussians, each
sampled from N (0, âˆ†2 Â· svi2 ). Standard union bound gives
that w.p. â‰¥ 1 âˆ’ Î½ non of the pp
Gaussians exceeds its standard deviation by a factor of 2 ln(2p/Î½).
Hence, w.p.
P
â‰¥ 1 âˆ’ Î½ it holds that kMvv k2 â‰¤ 2âˆ†2 i svi2 ln(2p/Î½) =
2âˆ†2 Â· trace(M M T ) Â· ln(2p/Î½).

T

g
^
]
e + Î²e X
e
Ty âˆ’ 2 y
TXÎ²
TX Î²
Î¶ k2 = yg
kÎ¶
âˆ’1

^
]
Ty âˆ’ y
TX X
TX
= yg

]
Ty
X

20
It is easy to see that the l2 -global sensitivity of the mapping
A 7â†’ AT A is âˆ B 4 . Fix any A1 , A2 that differ on one row
which is some vector v with kvv k = B in A1 and the all zero
vector in A2 . Then GS22 = kAT1 A1 âˆ’ AT2 A2 k2F = kvvv T k2F =
trace(vvv T Â· v v T ) = (vv Tv )2 = B 4 .

Our proof also requires the use of the following equality,
that holds for any invertible A and any matrix B s.t. I +
B Â· Aâˆ’1 is invertible:
âˆ’1
âˆ’1
(A + B) = Aâˆ’1 âˆ’ Aâˆ’1 I + BAâˆ’1
BAâˆ’1

Differentially Private Ordinary Least Squares
âˆ’1

In our case, we have

T
^
TX
we can bound the variance of X
jâ†’ X e by
!


2
âˆ’1 
âˆ’1

^
T
TX
 .
^
Appealing to
Ïƒ2 X
j,j + kN k Â· X X jâ†’ 

âˆ’1

^
TX
X
= (X T X + N )âˆ’1
T

âˆ’1

âˆ’1

T


âˆ’1 âˆ’1

âˆ’1

T

âˆ’ (X X) I + N (X X)
N (X X)



âˆ’1
= (X T X)âˆ’1 I âˆ’ I + N (X T X)âˆ’1
N (X T X)âˆ’1

def
= (X T X)âˆ’1 I âˆ’ Z Â· (X T X)âˆ’1
(18)
= (X X)

T

Proof of Theorem D.1. Fix Î½ > 0. First, we apply to standard results about Gaussian matrices, such as (Tao, 2012)
(used also by (Dwork et al., 2014) in their analysis),
to see
p
that w.p. â‰¥ 1 âˆ’ Î½/6 we have kN k = O(âˆ† p ln(1/Î½)).
And so, for the remainder of the proof we fix N subject to
having bounded operator norm. Note that by fixing N we
^
T X.
fix X
Î² + e with
Recall that in the homoscedastic model, y = XÎ²
each coordinate of e sampled i.i.d from N (0, Ïƒ 2 ). We
therefore have that
âˆ’1

^
TX
Î²e = X

^
TX
(X Ty + n ) = X

âˆ’1

âˆ’1

^
TX
=X

âˆ’1

^
TX
=Î² âˆ’X

âˆ’1

^
TX
Î² +X
NÎ²

âˆ’1

T

^
TX
X e+X

^
TX
X Te + X

âˆ’1

âˆ’1

^
TX
0, âˆ†2 I) is samTo bound X
jâ†’n note that n âˆ¼ N (0
^
T X.
pled independently of X
We therefore have that


âˆ’1
âˆ’1 2

2 ^
^
TX
T

X
jâ†’n âˆ¼ N (0, âˆ† X X jâ†’  ). Gaussian concentraâˆ’1

^
TX
tion bounds
give that
we have |X
jâ†’n | =
 w.p â‰¥ 1âˆ’Î½/6

 
âˆ’1
p

^
T

O âˆ†
X X jâ†’  ln(1/Î½) .
Plugging this into our above bounds on all terms that appear
in Equation
 (19) we
 have that w.p. â‰¥ 1 âˆ’ Î½/2 âˆ’ Î±/2 we


have that Î²ej âˆ’ Î²j  is at most



âˆ’1 
p

^
T


O X X jâ†’  Â· Bâˆ† p ln(1/Î½)
ï£¶
ï£« v
u

 !
âˆ’1
âˆ’1 2
u
p

^
TX
T
^
 ln(1/Î±) ï£¸
+ O ï£­Ïƒ t X
j,j + âˆ† p ln(1/Î½) Â· X X jâ†’ 

n

^
TX
as X
jâ†’ we deduce:
âˆ’1

âˆ’1

âˆ’1

n

âˆ’1

âˆ’1

^
TX
Denoting the j-th row of X

u

 !
âˆ’1
âˆ’1 2
u
p

^
^
t
T
T
2

ï£¸
Oï£­
X X j,j + âˆ† p ln(1/Î½) Â· X X jâ†’ 
 Ïƒ ln(1/Î±) .

Î² + X Te + n )
(X T XÎ²
âˆ’1

^
^
T X âˆ’ N )Î²
TX
Î² +X
(X

Gaussian concentration bounds, we have that w.p.
â‰¥ ï£«1v
âˆ’ Î±/2 the absolute value of this Gaussian is at mostï£¶




âˆ’1  p

^
T


+ O âˆ† X X jâ†’  ln(1/Î½)


T
^
^
^
T
TX
TX
Î² +X
Î²ej = Î²j âˆ’ X
jâ†’ X e + X X jâ†’n
jâ†’ NÎ²

(19)
We naÄ±Ìˆvely bound  the size
of the

âˆ’1 
âˆ’1

^
^
T
TX

X
Î²
Î²k
X
by
jâ†’ NÎ²
 X jâ†’  kN kkÎ²



âˆ’1 
p

^
TX
 Â· Bâˆ† p ln(1/Î½) .
O 
X
jâ†’


âˆ’1

Te

^
TX
X
jâ†’ X e

term
=

note that e is cho^
TX
sen
independently
of
X
and
since
âˆ’1
2
T
^
T
e
âˆ¼
N (00, Ïƒ I) we have X X jâ†’ X e
âˆ¼


âˆ’1
âˆ’1
^
^
TX
T
N 0, Ïƒ2 Â· eT
Â· X TX Â· X
ej .
Since
jX X
To

bound

^
T X we have
Note that due to the symmetry of X


âˆ’1 2
âˆ’2

^
^
T
T
X

 X jâ†’  = X X j,j (the (j, j)-coordinate of the maâˆ’2

^
TX
trix X



), thus |Î²ej âˆ’ Î²j | is at most

s

O ÏƒÂ·
r
+âˆ†

âˆ’1

^
TX
X
j,j + âˆ†
âˆ’2


âˆ’2
p
^
T
p ln(1/Î½) Â· X X j,j ln(1/Î±)

âˆš
^
TX
X
j,j Â· ln(1/Î½) Â· (B p + 1)


(20)

we have
âˆ’1

^
TX
X

^
TX
Â· X TX Â· X

âˆ’1

âˆ’1

âˆ’1

^
^
TX âˆ’ N ) Â· X
TX
Â· (X

^
TX
=X
âˆ’1

^
TX
=X

âˆ’1

^
TX
âˆ’X

âˆ’1

^
TX
Â·N Â·X

All of the terms appearing in Equation (20) are known
^
T X, except for Ïƒ â€” which is a parameter of the
given X
model. Next, we derive an upper bound on Ïƒ which we can
then plug into Equation (20) to complete the proof of the
theorem and derive a confidence interval for Î²j .

Differentially Private Ordinary Least Squares

p
most O(âˆ† Â· kzz k ln(1/Î½)) w.p. â‰¥ 1 âˆ’ Î½/8. We can
Î² k kI âˆ’ Z(X T X)âˆ’1 k =
upper bound kzz k â‰¤ 2kÎ²Ì‚
B
O( 1âˆ’Î· ), and so this termâ€™s magnitude is upper


âˆš
âˆ†Â·B ln(1/Î½)
bounded by O
.
1âˆ’Î·

Recall Equation (17), according to which we have
âˆ’1

g
^
]
Ty âˆ’ y
TX X
TX
Î¶ k2 = yg
kÎ¶

]
Ty
X

(18)

= y Ty + m
âˆ’ (yy T X + n T )(X T X)âˆ’1 (I âˆ’ Z Â· (X T X)âˆ’1 )(X Ty + n )

â€¢ Given our assumption about the least singular value
of X T X and with the bound on kN k, we have that
T
^
T X) â‰¥ Ïƒ
Ïƒmin (X
min (X X) âˆ’ kN k > 0 and so

= y Ty + m
âˆ’ y T X(X T X)âˆ’1 X Ty
+ y T X(X T X)âˆ’1 Z(X T X)âˆ’1 X Ty
T

T

âˆ’1

n

T

T

âˆ’1

Z(X T X)âˆ’1n

âˆ’ 2yy X(X X)
+ 2yy X(X X)

^
T X is a PSD. Therefore,
the symmetric matrix X
âˆ’1
âˆ’1/2
^
^
TX
TX
n = kX
n k2 is strictly
the term n T X
positive. Applying Proposition D.2 we have that
âˆ’1
T^
TX
w.p.
â‰¥
1
âˆ’
Î½/8
it
holds
that
n
X
n â‰¤


âˆ’1
2 ^
O âˆ† kX T X kF Â· ln(p/Î½) .

n
âˆ’ nT (X T X)âˆ’1 (I âˆ’ Z Â· (X T X)âˆ’1 )n
Î² = (X T X)âˆ’1 X Ty , and so we have
Recall that Î²Ì‚

T
Î² ZÎ²Ì‚
Î²
= y T I âˆ’ X(X T X)âˆ’1 X T y + m âˆ’ Î²Ì‚
T

^
TX
Î² (I âˆ’ Z(X T X)âˆ’1 )n
n âˆ’ nTX
âˆ’ 2Î²Ì‚

Plugging all of the above bounds into Equation (21) we get
that w.p. â‰¥ 1 âˆ’ Î½/2 âˆ’ Î±/2 it holds that

âˆ’1

n

(21)

and of course, both n and m are chosen independently of
^
T X and y .
X
Before we bound each term in Equation (21), we first give
âˆ’1
a bound on kZk. Recall, Z = I + N (X T X)âˆ’1
N.
Recall our assumption (given in the
statement
of
Theop
rem D.1) that Ïƒmin (X T X) â‰¥ âˆ†
p ln(1/Î½). This imÎ·
T
âˆ’1
plies that kN (X X) k â‰¤ kN kÂ·Ïƒmin (X T X)âˆ’1 = O(Î·).
Hence
 âˆš

âˆ† p ln(1/Î½)
kZk â‰¤ (kI +N (X T X)âˆ’1 k)âˆ’1 Â·kN k = O
1âˆ’Î·
Moreover, this implies that kZ(X T X)âˆ’1 k â‰¤ O


1
and that kI âˆ’ Z(X T X)âˆ’1 k â‰¤ O 1âˆ’Î·
.



Î·
1âˆ’Î·



Armed with these bounds on the operator norms of Z and
(I âˆ’ Z(X T X)âˆ’1 ) we bound the magnitude of the different
terms in Equation (21).
â€¢ The term y T (I âˆ’ XX + ) y is the exact term from
the standard OLS, and we know it is distributed like
Ïƒ 2 Â· Ï‡2nâˆ’p distribution. Therefore, it is greater than
p
âˆš
Ïƒ 2 ( n âˆ’ p âˆ’ 2 ln(4/Î±))2 w.p. â‰¥ 1 âˆ’ Î±/2.
â€¢ The scalar m sampled
from m âˆ¼ N (0, âˆ†2 ) is
p
bounded by O(âˆ† ln(1/Î½)) w.p. â‰¥ 1 âˆ’ Î½/8.
T

Î²k â‰¤ 
Î² ZÎ²Ì‚
Î² is upper
â€¢ Since we assume kÎ²Ì‚
B, the term Î²Ì‚
âˆš
2
B
âˆ†
p
ln(1/Î½)
.
bounded by B 2 kZk = O
1âˆ’Î·
T

Î² (I âˆ’ Z(X T X)âˆ’1 )n
n. We thus have
â€¢ Denote z Tn = 2Î²Ì‚
T
that z n âˆ¼ N (0, âˆ†2 kzz k2 ) and that its magnitude is at

Ïƒ2 â‰¤




1âˆš
âˆš
nâˆ’pâˆ’2 ln(4/Î±)


g
Î¶ k2 + O (1 +
kÎ¶

2
Â·

âˆš
p
B 2 p+B
)âˆ† ln(1/Î½)
1âˆ’Î·

^
TX
+ âˆ† 2 kX

âˆ’1


kF Â· ln(p/Î½)

and indeed, the RHS is the definition of Ï2 in the statement
of Theorem D.1.

E. Experiment: Additional Figures
To complete our discussion about the experiments we have
conducted, we attach here additional figures, plotting both
the t-value approximations we get from both algorithms,
and the â€œhigh-level decisionâ€ of whether correctly reject or
not-reject the null hypothesis (and with what sign). First,
we show the distribution of the t-value approximation for
coordinates that should be rejected, in Figure 2, and then
the decision of whether to reject or not based on this t-value
â€” and whether it was right, conservative (we didnâ€™t reject
while we needed to) or wrong (we rejected with the wrong
sign, or rejected when we shouldnâ€™t have rejected) in Figure 3. As one can see, Algorithm 1 has far lower t-values
(as expected) and therefore is much more conservative. In
fact, it tends to not-reject coordinate 1 of the real-data even
on the largest value of n (Figure 3c).
However, because Algorithm 1 also has much smaller
variance, it also does not reject when it ought to notreject, whereas Algorithm 2 erroneiously rejects the nullhypotheses. This can be seen in Figures 4 and 5.

Differentially Private Ordinary Least Squares

(a) Synthetic data, coordinate Î²1 = 0.5

(a) Synthetic data, coordinate Î²1 = 0.5

(b) Synthetic data, coordinate Î²2 = âˆ’0.25

(b) Synthetic data, coordinate Î²2 = âˆ’0.25

(c) real-life data, coordinate Î²1 = 14.07
Figure 2. The distribution of the t-value approximations from selected experiments on synthetic and real-life data where the null
hypothesis should be rejected

(c) real-life data, coordinate Î²1 = 14.07
Figure 3. The correctness of our decision to reject the nullhypothesis based on the approximated t-value where the null hypothesis should be rejected

Differentially Private Ordinary Least Squares

(a) Synthetic data, coordinate Î²3 = 0

(a) Synthetic data, coordinate Î²3 = 0

(b) real-life data, coordinate Î²2 = 0.57

(b) Synthetic data, coordinate Î²2 = 0.57

Figure 4. The distribution of the t-value approximations from selected experiments on synthetic and real-life data when the null
hypothesis is (essentially) true

Figure 5. The correctness of our decision to reject the nullhypothesis based on the approximated t-value when the null hypothesis is (essentially) true

