Differentially Private Ordinary Least Squares

A. Extended Introductory Discussion

A.2. Omitted Preliminary Details

Due to space constraint, a few details from the introductory parts (Sections 1,2) were omitted. We bring them in
this appendix. We especially recommend the uninformed
reader to go over the extended OLS background we provide in Appendix A.3.

Linear Algebra and Pseudo-Inverses. Given a matrix M
we denote its SVD as M = U SV T with U and V being
orthonormal matrices and S being a non-negative diagonal
matrix whose entries are the singular values of M . We use
σmax (M ) and σmin (M ) to denote the largest and smallest
singular value resp. Despite the risk of confusion, we stick
to the standard notation of using σ 2 to denote the variance
of a Gaussian, and use σj (M ) to denote the j-th singular
value of M . We use M + to denote the Moore-Penrose inverse of M , defined as M + = V S −1 U T where S −1 is a
−1
matrix with Sj,j
= 1/Sj,j for any j s.t. Sj,j > 0.

A.1. Proof Of Privacy of Algorithm 1
Theorem A.1. Algorithm 1 is (, δ)-differentially private.
Proof. The proof of the theorem is based on the fact
the Algorithm 1 is the result of composing the differentially private Propose-Test-Release algorithm of (Dwork &
Lei, 2009) with the differentially private analysis of the
Johnson-Lindenstrauss transform of (Sheffet, 2015).
More specifically, we use Theorem B.1 from (Sheffet,
2015) that states that given a matrix A whose all of its
2
singular
p values at greater than T (, δ) where T (, δ) =
2
2B
2r ln(4/δ) + 2 ln(4/δ) , publishing RA is (, δ)
differentially private for a r-row matrix R whose entries
sampled are i.i.d normal Gaussians. Since we have that all
of the singular values of A0 are greater than w (as specified
in Algorithm 1), outputting RA0 is (/2, δ/2)-differentially
private. The rest of the proof boils down to showing that
(i) the if-else-condition is (/2, 0)-differentially private and
that (ii) w.p. ≤ δ/2 any matrix A whose smallest singular value is smaller than w passes the if-condition (step 3).
If both these facts hold, then knowing whether we pass
the if-condition or not is (/2)-differentially private and
the output of the algorithm is (/2, δ)-differentially private,
hence basic composition gives the overall bound of (, δ)differential privacy.
To prove (i) we have that for any pair of neighboring matrices A and B that differ only on the i-th row, denoted a i and
T
T
b i resp., we have B T B − b ib T
i = A A − a ia i . Applying
Weyl’s inequality we have
b ib T
σmin (B T B) ≤ σmin (B T B − b ib T
i ) + σmax (b
i)
T

≤ σmin (A A) +

a ia T
σmax (a
i)
2

+

σmax (bbib T
i)

≤ σmin (AT A) + 2B

2

hence |σmin (A)2 −σmin (B)2 | ≤ 2B 2 , so adding Lap( 4B )
is (/2)-differentially private.
To prove (ii), note that by standard tail-bounds on the
Laplace distribution we have that Pr[Z
<
−
4B 2 ln(1/δ)
δ
] ≤ 2 . Therefore, w.p. 1 − δ/2 it holds that

any matrix A that passes the if-test of the algorithm must
have σmin (A)2 > w2 . Also note that a similar argument shows that for any 0 < β < 1, any matrix A s.t.
2
σmin (A)2 > w2 + 4B ln(1/β)
passes the if-condition of

the algorithm w.p. 1 − β.

The Gaussian Distribution.
A univariate Gaussian N (µ, σ 2 ) denotes the Gaussian distribution
whose
mean is µ and variance σ 2 , with PDF(x) =
√
2
( 2πσ )−1 exp(− x−µ
bounds
2σ 2 ). Standard concentration
p
on Gaussians give that Pr[x > µ + 2σ ln(1/ν)] < ν
µ, Σ)
for any ν ∈ (0, 1e ). A multivariate Gaussian N (µ
for some positive semi-definite Σ denotes the multivariate Gaussian distribution where the mean of the
j-th coordinate is the µj and the co-variance between
coordinates j and k is Σj,k . The PDF of such Gaussian is defined only on the subspace colspan(Σ),
x) =
where for every x ∈ colspan(Σ) we have PDF(x

−1/2

1
rank(Σ)
T
+
˜
x − µ ) Σ (x
x − µ)
(2π)
· det(Σ)
exp − 2 (x
˜
and det(Σ)
is the multiplication of all non-zero singular values of Σ.
A matrix Gaussian distribution
denoted N (Ma×b , U, V ) has mean M , variance U
on its rows and variance V on its columns. For full
rank U and V it holds that PDFN (M,U,V ) (X) =
(2π)−ab/2 (det(U ))−b/2 (det(V ))−a/2
·

exp(− 21 trace V −1 (X − M )T U −1 (X − M ) ).
In
our case, we will only use matrix Gaussian distributions
with N (Ma×b , Ia×a , V ) and so each row in this matrix is
an i.i.d sample from a b-dimensional multivariate Gaussian
N ((M )j→ , V ).
We will repeatedly use the rules regarding linear operations
on Gaussians. That in, for any c, it holds that cN (µ, σ 2 ) =
µ, Σ) =
N (c · µ, c2 σ 2 ). For any C it holds that C · N (µ
µ, CΣC T ). And for any C is holds that N (M, U, V ) ·
N (Cµ
C = N (M C, U, C T V C). In particular, for any c (which
can be viewed as a b × 1-matrix) it holds that N (M, U, V ) ·
c = N (Mcc, U, c T V c ) = N (Mcc, c T V c · U ).
We will also require the following proposition.
2

Proposition A.2. Given σ 2 , λ2 s.t. 1 ≤ σλ2 ≤ c2 for some
constant c, let X and Y be two random Gaussians s.t. X ∼
N (0, σ 2 ) and Y ∼ N (0, λ2 ). It follows that 1c PDFY (x) ≤
PDFX (x) ≤ cPDFcY (x) for any x.
Corollary A.3. Under the same notation as in Proposition A.2, for any set S ⊂ R it holds that 1c Prx←Y [x ∈
S] ≤ Prx←X [x ∈ S] ≤ cPrx←cY [x ∈ S] =

Differentially Private Ordinary Least Squares

cPrx←Y [x ∈ S/c]

haustive account of OLS and we refer the interested reader
to (Rao, 1973; Muller & Stewart, 2006).

Proof. The proof is mere calculation.

xi , yi )}ni=1 where for all i we have
Given n observations {(x
p
x i ∈ R and yi ∈ R, we assume the existence of a pdimensional vector β ∈ Rp s.t. the label yi was derived by
yi = β Tx i + ei where ei ∼ N (0, σ 2 ) independently (also
known as the homoscedastic Gaussian model). We use the
matrix notation where X denotes the (n × p)-matrix whose
rows are x i , and use y , e ∈ Rn to denote the vectors whose
i-th entry is yi and ei resp. To simplify the discussion, we
assume X has full rank.

PDFX (x)
=
PDFcY (x)

r

2

x
c2 λ2 exp(− 2σ
2)
·
2
2
σ
exp(− 2cx2 λ2 )

x2 1
1
≤ c · exp( ( 2 2 − 2 )) ≤ c · exp(0) = c
2 c λ
σ
r
x2
)
PDFX (x)
λ2 exp(− 2σ
2
·
=
PDFY (x)
σ 2 exp(− x22 )
≥c

−1

2λ
x2 1
exp( 2 ( λ2 − σ12 ))

≥

exp(0)
c

= c−1

The parameters of the model are therefore β and σ 2 , which
we set to discover. To that end, we minimize minz kyy −
Xzz k2 and solve
β = (X T X)−1 X Ty = (X T X)−1 X T (Xβ
β +ee) = β +X +e
β̂

The Tk -Distribution.
The Tk -distribution, where k
is referred to as the degrees of freedom of the distribution, denotes the distribution over the reals created by independently sampling Z ∼ N (0, 1) and kζk2 ∼ χ2k ,
and taking the quantity √ Z 2 . Its PDF is given by


kζk /k
− k+1
2
2
x
. It
k

PDFTk (x) ∝ 1 +
is a known fact that
as k increases, Tk becomes closer and closer to a normal
Gaussian. The T -distribution is often used to determine
suitable bounds on the rate of converges, as we illustrate
in Section A.3. As the T -distribution is heavy-tailed, existing tail boundsp
on the T -distribution (which are of the
form:R if τν = C k((1/ν)2/k − 1) for some constant C
∞
then τν PDFTk (x)dx < ν) are often cumbersome to work
with. Therefore, in many cases in practice, it common to
assume ν = Θ(1) (most commonly, ν = 0.05) and use
existing tail-bounds on normal Gaussians.
Differential Privacy facts. It is known (Dwork et al.,
2006b) that if ALG outputs a vector in Rd such that for
any A and A0 it holds that kALG(A) − ALG(A0 )k1 ≤ B,
then adding Laplace noise Lap(1/) to each coordinate of
the output of ALG(A) satisfies -differential privacy. Similarly, (2006b) showed that if for any neighboring A and
A0 it holds that kALG(A) − ALG(A0 )k22 ≤ ∆2 then adding
Gaussian noise N (0, ∆2 · 2 ln(2/δ)
) to each coordinate of
2
the output of ALG(A) satisfies (, δ)-differential privacy.
Another standard result (Dwork et al., 2006a) gives that the
composition of the output of a (1 , δ1 )-differentially private
algorithm with the output of a (2 , δ2 )-differentially private
algorithm results in a (1 +2 , δ1 +δ2 )-differentially private
algorithm.
A.3. Detailed Background on Ordinary Least Squares
For the unfamiliar reader, we give a short description of the
model under which OLS operates as well as the confidence
bounds one derives using OLS. This is by no means an ex-

β
As e ∼ N (00n , σ 2 In×n ), it holds that β̂
∼
β , σ 2 (X T X)−1 ), or alternatively, that for every coorN (β
β ∼ N (βj , σ 2 (X T X)−1
dinate j it holds that β̂j = e T
j β̂
j,j ).
Hence we get

σ

β̂ −βj
q j
(X T X)−1
j,j

∼ N (0, 1). In addition, we de-

note the vector
β = (Xβ
β + e ) − X(β
β + X +e ) = (I − XX + )ee
ζ = y − X β̂
and since XX + is a rank-p (symmetric) projection matrix,
we have ζ ∼ N (0, σ 2 (I − XX + )). Therefore, kζζ k2 is
equivalent to summing the squares of (n − p) i.i.d samples
from N (0, σ 2 ). In other words, the quantity kζζ k2 /σ 2 is
sampled from a χ2 -distribution with (n − p) degrees of
freedom.
We sidetrack from the OLS discussion to give the following
β , as the next
bounds on the l2 -distance between β and β̂
claim shows.
Claim A.4. For any 0 < ν < 1/2, the following holds w.p.
≥ 1−ν over the randomness of the model (the randomness
over e )
β − β̂
β k2 = kX +e k2
kβ
= O σ 2 log(p/ν) · kX + k2F
2

+



(6)

2

β k = kβ
β + X ek
kβ̂

2
p
β k + σ · kX + kF · log(p/ν) )
= O( kβ
q


 1
ln(1/ν)
2
2
ζ
kζ
k
−
σ
=
O(
 n−p

n−p )
Proof. Since e ∼ N (00n , σ 2 In×n ) then X +e ∼
N (00n , σ 2 (X T X)−1 ). Denoting the SVD decomposition
(X T X)−1 = V SV T with S denoting the diagonal ma−2
−2
trix whose entries are σmax
(X), . . . , σmin
(X), we have
T +
2
that V X e ∼ N (00n , σ S). And so, each coordinate of V T X +e is distributed like an i.i.d Gaussian. So

Differentially Private Ordinary Least Squares

w.p. p≥ 1 − ν/2 non of these Gaussians is a factor of
O(σ ln(p/ν)) greater than its standard deviation. And so
w.p. ≥ 1 − ν/2P
it holds that kX +e k2 =PkV T X +e k2 ≤
−2
−2
2
Since
O(σ log(p/ν)
i σi (X) ).
i σi (X) =
T
−1
+
+ T
trace((X X) ) = trace(X (X ) ) = kX + k2F , the
bound of (6) is proven.
β k2 is an immediate corollary of (6) using
The bound on kβ̂
the triangle inequality.8 The bound on kζζ k2 follows from
tail bounds on the χ2n−p distribution, as detailed in Section 2.
β and ζ are
Returning to OLS, it is important to note that β̂
β depends solely on
independent of one another. (Note, β̂
X +e = (X + X)X +e = X + PU e , whereas ζ depends on
(I − XX + )ee = PU ⊥ e . As e is spherically symmetric, the
β is
two projections are independent of one another and so β̂
independent of ζ .) As a result of the above two calculations, we have that the quantity
def

tβ̂j (βj ) =

β̂j −βj
q
ζk
kζ
√
(X T X)−1
j,j · n−p

=

σ

β̂ −βj
q j
(X T X)−1
j,j

.

kζζ k
√
σ n−p

is distributed like a T -distribution with (n − p) degrees of
freedom. Therefore, we can compute an exact probability
estimation for this quantity. That is, for any measurable
S ⊂ R we have
i Z
h
β and ζ satisfying tβ̂j (βj ) ∈ S =
PDFTn−p (x)dx
Pr β̂
S

The importance of the t-value t(βj ) lies in the fact that it
can be fully estimated from the observed data X and y (for
any value of βj ), which makes it a pivotal quantity. Therefore, given X and y , we can use t(βj ) to describe the likelihood of any βj — for any z ∈ R we can now give an
estimation of how likely it is to have βj = z (which is
PDFTn−p (t(z))). The t-values enable us to perform multitude of statistical inferences. For example, we can say
which of two hypotheses is more likely and by how much
(e.g., we are 5-times more likely that the hypothesis βj = 3
is true than the hypothesis βj = 14 is true); we can compare between two coordinates j and j 0 and report we are
more confident that βj > 0 than βj 0 > 0; or even compare
among the t-values we get across multiple datasets (such
as the datasets we get from subsampling rows from a single dataset).
In particular, we can use t(βj ) to α-reject unlikely values
of βj . Given 0 < α < 1, we denote cα as the number for
which the interval (−cα , cα ) contains a probability mass
of 1 − α from the Tn−p -distribution. And so we derive a
8

Observe, though e is spherically symmetric, and is likely to
be approximately-orthogonal to β , this does not necessarily hold
for X +e which isn’t spherically symmetric. Therefore, we result
β using the triangle bound.
to bounding the l2 -norm of β̂

corresponding confidence interval Iα centered at β̂j where
βj ∈ Iα with confidence of level of 1 − α.
We comment as to the actual meaning of this confidence
interval. Our analysis thus far applied w.h.p to a vector y
derived according to this model. Such X and y will result in the quantity tβ̂j (βj ) being distributed like a Tn−p distribution — where βj is given as the model parameters
and β̂j is the random variable. We therefore have that guarantee that for X and 
y derived according to this model,
 the
q
2
def
ζ
kζ
k
−1
event Eα = β̂j ∈ βj ± cα · (X T X)j,j · n−p happens w.p. 1 − α. However, the analysis done over a given
dataset X and y (once y has been drawn) views the quantity tβ̂j (βj ) with β̂j given and βj unknown. Therefore the
event Eα either holds or does not hold. That is why the
alternative terms of likelihood or confidence are used, instead of probability. We have
q a confidence 2level of 1 − α
ζ
ˆ
that indeed βj ∈ βj ± cα · (X T X)−1 · kζ k , because this
j,j

n−p

event does happen in 1−α fraction of all datasets generated
according to our model.
Rejecting the Null Hypothesis. One important implication of the quantity t(βj ) is that we can refer specifically to
the hypothesis that βj = 0, called the null hypothesis. This
def

quantity, t0 = tβ̂j (0) =

√
β̂j n−p
q
,
kζζ k (X T X)−1
j,j

represents how

large is β̂j relatively to the empirical estimation of standard
deviation σ. Since it is known that as the number of degrees
of freedom of a T -distribution tends to infinity then the T distribution becomes a normal Gaussian, it is common to
think of t0 as a sample from a normal Gaussian N (0, 1).
This allows us to associate t0 with a p-value, estimating the
event “βj and β̂j have different signs.” Formally, we define
R∞
2
p0 = |t0 | √12π e−x /2 dx. It is common to reject the null
hypothesis when p0 is sufficiently small (typically, below
0.05).9
Specifically, given α ∈ (0, 1/2), we say we α-reject the
null hypothesis
R ∞ if p0 <2 α. Let τα be the number s.t.
Φ(τα ) = τα √12π e−x /2 dx = α. (Standard bounds
p
give that τα < 2 ln(1/α).) This means we α-reject
the null hypothesis
if t0 > τα or t0 < −τα , meaning if
q
|β̂j | > τα

kζζ k
√
(X T X)−1
j,j n−p .
ζ

We can now lower bound the number of i.i.d sample points
needed in order to α-reject the null hypothesis. This bound
will be our basis for comparison — between standard OLS
and the differentially private version.10
9
R ∞ Indeed, it is more accurate to associate with t0 the value
PDFTn−p (x)dx and check that this value is < α. However,
|t0 |
as most uses take α to be a constant (often α = 0.05), asymptotically the threshold we get for rejecting the null hypothesis are the
same.
10
This theorem is far from being new (except for maybe fo-

Differentially Private Ordinary Least Squares

Theorem A.5 (Theorem 2.2 restated.). Fix any positive
definite matrix Σ ∈ Rp×p and any ν ∈ (0, 12 ). Fix parameters β ∈ Rp and σ 2 and a coordinate j s.t. βj 6= 0.
Let X be a matrix whose n rows are i.i.d samples from
β )i is sampled
N (00, Σ), and y be a vector where yi − (Xβ
i.i.d from N (0, σ 2 ). Fix α ∈ (0, 1). Then w.p. ≥ 1 − ν
we have
p that the (1 − α)-confidence interval is of length
O(cα σ 2 /(nσmin (Σ))) provided n ≥ C1 (p + ln(1/ν))
for some sufficiently large constant C1 . Furthermore, there
exists a constant C2 such that w.p. ≥ 1 − α − ν we (correctly) reject the null hypothesis provided
(
)
σ 2 c2α + τα2
n ≥ max C1 (p + ln(1/ν)), C2 2 ·
βj σmin (Σ)
cα
denotes
the
number
for
which
PDF
(x)dx
=
1
−
α.
(If
we
are
content
T
n−p
−cα
with approximating Tn−p
p with a normal Gaussian than
one can set cα ≈ τα < 2 ln(1/α).)
Here
R cα

Proof. The discussion above
≥ 1−α
q shows that w.p.
2
−1 kζζ k
T
we have |βj − β̂j | ≤ cα (X X)j,j n−p ; and in orderqto α-reject the null hypothesis we must have |β̂j | >
kζζ k2
τα (X T X)−1
j,j n−p . Therefore, a sufficient condition for
OLS to α-reject the null-hypothesis
is to have n large
q
2

kζζ k
enough s.t. |βj | > (cα + τα ) (X T X)−1
j,j n−p . We therefore argue that w.p.≥ 1 − ν this inequality indeed holds.
ζ

We assume each row of X i.i.d vector x i ∼ N (00p , Σ), and
recall that according to the model kζζ k2 ∼ σ 2 χ2 (n − p).
Straightforward concentration bounds on Gaussians and on
the χ2 -distribution give:
√
(i) W.p. ≤ α it holds that kζζ k > σ ( n − p + 2 ln(2/α))).
(This is part of the standard OLS analysis.)
√
(ii) W.p.p≤ ν it holds that σmin (X T X) ≤ σmin (Σ)( n −
√
( p + 2 ln(2/ν)))2 . (Rudelson & Vershynin, 2009)
Therefore, due to the lower bound n = Ω(p +
ln(1/ν)), w.p.≥ 1 − ν − α we have that none
of
hold.
In such a case we have
q these events p
−1
T
(X X)j,j ≤
σmax ((X T X)−1 ) = O( √ 1
)
nσmin (Σ)
√
and kζζ k = O(σ n − p).
This implies that the
confidence
interval
of
level
1
− α has length of
q
 q

2
−1 kζζ k
σ2
T
cα (X X)j,j · n−p = O cα nσmin
(Σ) ; and that in
order to α-reject
that
null-hypothesis
it
suffices to have
q


2

σ
|βj | = Ω (cα + τα ) nσmin
(Σ) . Plugging in the lower
bound on n, we see that this inequality holds.

We comment that for sufficiently large constants C1 , C2 ,
cusing on the setting where every row in X is sampled from an
i.i.d multivariate Gaussians), it is just stated in a non-standard
way, discussing solely the power of the t-test in OLS. For further
discussions on sample size calculations see (Muller & Stewart,
2006).

it holds that all the constants hidden in the O- and Ωnotations of the proof are close to 1. I.e., they are all
within the interval (1 ± η) for some small η > 0 given
C1 , C2 ∈ Ω(η −2 ).

B. Projecting the Data using Gaussian
Johnson-Lindenstrauss Transform
B.1. Main Theorem Restated and Further Discussion
Theorem B.1 (Theorem 3.1 restated.). Let X be a n × p
matrix, and parameters β ∈ Rp and σ 2 are such that
β + e with each coordiwe generate the vector y = Xβ
nate of e sampled independently from N (0, σ 2 ). Assume
σmin (X) ≥ C · w and that n is sufficiently large s.t.
all of the singular values of the matrix [X; y ] are greater
than C · w for some large constant C, and so Algorithm 1
projects the matrix A = [X; y ] without altering it, and publishes [RX; Ryy ].
Fix ν ∈ (0, 1/2) and r = p + Ω(ln(1/ν)). Fix coordinate
β , ζ̃ζ and σ̃ 2 as
j. Then w.p. ≥ 1 − ν we have that deriving β̃
follows
β
β̃

= (X T RT RX)−1 (RX)T (Ryy ) = β + (RX)+ Ree

ζ̃ζ

=
=

σ̃ 2

=

√1 Ry
y − √1r (RX)β̃
β
r

T
1
√
I − (RX)(X RT RX)−1 (RX)T ) Ree
r

r
kζ̃ζ k2
r−p

then the pivot quantity
β̃j − βj

t̃(βj ) =
σ̃

q

(X T RT RX)−1
j,j

has a distribution D satisfying e−a PDFTr−p (x) ≤
PDFD (x) ≤ ea PDFTr−p (e−a x) for any x ∈ R, where we
r−p
denote a = n−p
.
Comparison with Existing Bounds. Sarlos’ work (2006)
utilizes the fact that when r, the numbers of rows in R,
is large enough, then √1r R is a Johnson-Lindenstrauss
matrix. q
Specifically, given r and ν ∈ (0, 1) we denote
η = Ω(

p ln(p) ln(1/ν)
),
r

and so r = O( p ln(p)ηln(1/ν)
).
2

β = arg minz 1r kRXzz − Ryy k2 . In
Let us denote β̃
this setting, Sarlos’ work (Sarlós, 2006) (Theorem 12(3)) guarantees that w.p.q≥ 1 − ν we have

p log(p) log(1/ν)
β − β̃
β k2 ≤ ηkζζ k/σmin (X) = O
ζ
kβ̂
kζ
k
.
rσmin (X T X)
β − β̃
β k and using the
Naı̈vely bounding |β̂j − β̃j | ≤ kβ̂
β j − β j from Section A.311
confidence interval for β̂
11
Where we approximate cα , the tail bound of the Tn−p distribution with the tail
p bound on a Gaussian, i.e., use the approximation cα ≈ O( ln(1/α)).

Differentially Private Ordinary Least Squares

gives a confidence interval of q
level 1 − (α + ν) 
cenp ln(p) log(1/ν)
tered at β̃j with length of O
kζζ k +
rσmin (X T X)
q

log(1/α)
ζk
O
(X T X)−1
=
j,j n−p kζ
q

p ln(p) log(1/ν)+log(1/α)
O
kζζ k .
This implies that
rσmin (X T X)
our confidence interval has decreased its degrees of
freedom from n − p to roughly r/p ln(p), and furthermore,
that it no longer depends on (X T X)−1
j,j but rather on
T
1/σmin (X X). It is only due to the fact that we rely on
Gaussians and by mimicking carefully the original proof
that we can deduce that the t̃-value has (roughly) r − p
degrees of freedom and depends solely on (X T X)−1
j,j .
(In the worst case, we have that (X T X)−1
j,j is proportional
to σmin (X T X)−1 , but it is not uncommon to have matrices
where the former is much larger than the latter.) As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for
finding a DP estimator β dp of the linear regression give a
β dp − β̂
β k = Õ(p/). Such
data-independent12 bound of kβ
bounds are harder to compare with the interval length given
by Corollary 3.2. Indeed, as we discuss in Section 3 under “Rejecting the null-hypothesis,” enough samples from
a multivariate Gaussian whose covariance-matrix is well
conditioned give a bound which is well below the worstupper bound of O(p/). (Yet, it is possible that these techniques also do much better on such “well-behaved” data.)
What the works of Sarlos and alternative works regrading
differentially private linear regression do not take into account are questions such as generating a likelihood for βj
nor do they discuss rejecting the null hypothesis.
B.2. Proof of Theorem 3.1
β and ζ̃ζ , where our goal
We now turn to our analysis of β̃
is to show that the distribution of the t̃-values as specified in Theorem 3.1 is well-approximated by the Tr−p distribution. For now, we assume the existence of fixed
β + e . (Later,
vectors β ∈ Rp and e ∈ Rn s.t. y = Xβ
we will return to the homoscedastic model where each coordinate of e is sampled i.i.d from N (0, σ 2 ) for some σ 2 .)
In other words, we first examine the case where R is the
sole source of randomness in our estimation. Based on the
assumption that e is fixed, we argue the following.
Claim B.2. In our model,
given X and
β
the output M
= RX, we have that β̃
∼
N β + X +e , kPU ⊥ e k2 (M T M )−1
and
ζ̃ζ
∼


kPU ⊥ e k2
T
−1
T
N 0n,
(Ir×r − M (M M ) M ) .
Where
r
PU ⊥ denotes the projection operator onto the subspace
orthogonal to colspan(X); i.e., PU = XX + and
PU ⊥ = (Ir×r − XX + ).
12

In other words, independent of X, ζ .

Proof. The
matrix
R
is
sampled
from
N (0r×p , Ir×r , Ip×p ). Given X and RX = M , we
learn the projection of each row in R onto the subspace
spanned by the columns of X. That is, denoting u T as
the i-th row of R and v T as the i-th row of M , we have
that X Tu = v . Recall, initially u ∼ N (00n , In×n ) –
a spherically symmetric Gaussian. As a result, we can
denote u = PU u × PU ⊥ u where the two projections are
independent samples from N (00n , PU ) and N (00n , PU ⊥ )
resp. However, once we know that v = X Tu we have that
PU u = X(X T X)−1 X Tu = X(X T X)−1v so we learn
PU u exactly, whereas we get no information about PU ⊥
so PU ⊥ u is still sampled from a Gaussian N (00n , PU ⊥ ).
As we know for each row of R that u T PU = v T X + , we
therefore have that
R = RPU + RPU ⊥ = M X + + RPU ⊥
where RPU ⊥ ∼ N (0r×n , Ir×r , PU ⊥ ). From here on, we
just rely on the existing results about the linearity of Gaussians.
R ∼ N (M X + , Ir×r , PU ⊥ )
⇒ Ree ∼ N (M X +e , kPU ⊥ e k2 Ir×r )
⇒ M + Ree ∼ N (X +e , kPU ⊥ e k2 (M T M )−1 )
β = β + M + Ree implies β̃
β ∼ N (β
β +
so β̃
+
X e , kPU ⊥ e k2 (M T M )−1 ).
And as ζ̃ζ
=
T
−1
T
√1 (Ir×r
e
−
M
(M
M
)
M
)Re
then
we
r
kP

have ζ̃ζ
∼
N (00r , U r⊥
+
(Ir×r − M M )M = 0r×p .

e k2

(Ir×r − M M + )) as

Claim B.2 was based on the assumption that e is fixed.
However, given X and y there are many different ways to
β + e . However, the
assign vectors β and e s.t. y = Xβ
distributions we get in Claim B.2 are unique. To see that,
β and
recall Equations (1) and (2): β + X +e = X +y = β̂
PU ⊥ e = PU ⊥ y = (I − XX + )yy = ζ . We therefore have
2
β ∼ N (β̂
β , kζζ k2 (M T M )−1 ) and ζ̃ζ ∼ N (00n , kζζrk (I −
β̃
M M + )). We will discuss this further, in Section 4, where
we will not be able to better analyze the explicit distributions of our estimators. But in this section, we are able to
β and ζ̃ζ .
argue more about the distributions of β̃
So far we have considered the case that e is fixed, whereas
our goal is to argue about the case where each coordinate
of e is sampled i.i.d from N (0, σ 2 ). To that end, we now
switch to an intermediate model, in which PU e is sampled from a multivariate Gaussian while PU ⊥ e is fixed as
some arbitrary vector of length l. Formally, let Dl denote
the distribution where PU e ∼ N (0, σ 2 PU ) and PU ⊥ e is
fixed as some specific vector whose length is denoted by
kPU ⊥ ek = l.
Claim B.3. Under the same assumptions as in
Claim B.2, given that e ∼ Dl , we have that

Differentially Private Ordinary Least Squares

∼
N β , σ 2 (X T 
X)−1 + l2 (M T M )−1

2
ζ̃ζ ∼ N 0 n , lr (I − M M + ) .

β
β̃



β = β + M + Ree = β + M + (M X + +
Proof. Recall, β̃
RPU ⊥ )ee = β + X +e + M + R(PU ⊥ e ). Now, under the
assumption e ∼ Dl we have that β is the sum of two independent Gaussians:

β , σ 2 X + · PU · (X + )T )
β + X +e ∼ N (β
β , σ 2 (X T X)−1 )
= N (β
∼ N (00r , kPU ⊥ e k2 Ir×r )
RPU ⊥ e
+ e
⇒ M Re ∼ N (00p , kPU ⊥ e k2 (M T M )−1 )
Summing the two independent Gaussians’ means and
β . Furthermore, in
variances gives the distribution of β̃
Claim B.2 we have
already
established
that 
for any fixed

kPU ⊥ e k2
+
e we have ζ̃ζ ∼ N 0 n ,
(I − M M ) . Hence, for
r


2
e ∼ Dl we still have ζ̃ζ ∼ N 0 n , lr (I − M M + ) . (It is
easy to verify that the same chain of derivations is applicable when e ∼ Dl .)
Corollary B.4. Given that e ∼ Dl we have that β̃j ∼
−1
2
T
N (βj , σ 2 (X T X)−1
j,j + l (M M )j,j ) for any coordinate
j, and that kζ̃ζ k2 ∼

l2
r

· χ2r−p .

Proof. The corollary follows immediately from the fact
β , and from the definition of the χ2 that βj = e T
j β̃
distribution, as ζ̃ζ is a spherically symmetric Gaussian defined on the subspace colspan(M )⊥ of dimension r −
p.
To continue, we need the following claim.
Claim B.5. Given X and M = RX, and given that e ∼ Dl
β and ζ̃ζ are independent.
we have that β̃
β = β + X +e + M + R(PU ⊥ e ). And
Proof. Recall, β̃
so, given X, M and a specific vector PU ⊥ e we have
β depends on (i) the projection
that the distribution of β̃
of e on U = colspan(X) and on (ii) the projection of
each row in R onto Ũ = colspan(M ). The distribution of ζ̃ζ = √1r PŨ ⊥ Ree = √1r PŨ ⊥ (M X + + RPU ⊥ )ee =
√1 P ⊥ RPU ⊥ e depends on (i) the projection of e onto U ⊥
r Ũ
(which for the time being is fix to some specific vector of
length l) and on (ii) the projection of each row in R onto
Ũ ⊥ . Since PU e is independent from PU ⊥ e , and since for
any row u T of R we have that PŨ u is independent of PŨ ⊥ u ,
and since e and R are chosen independently, we have that
β and ζ̃ζ are independent.
β̃
Formally, consider any pair of coordinates β̃j and ζ̃k , and
we have
β̃j − βj

+
T
+
= eT
j X e + e j M (RPU ⊥ e )

ζ̃k

and

= eT
k PŨ ⊥ (RPU ⊥ e )

Recall, we are given X and M = RX. Therefore, we know
PU and PŨ . And so
Cov[β̃j , ζ̃k ]
= E[(β̃j − βj )(ζ̃k − 0)]
+
T
= E[eeT
j X e (RPU ⊥ e ) PŨ ⊥ e k ]
T
+
+ E[eeT
j M (RPU ⊥ e )(RPU ⊥ e ) PŨ ⊥ e k ]
+
ee T PU ⊥ ]E[RT ]PŨ ⊥ e k
= eT
j X E[e
T
+
+ eT
j M E[(RPU ⊥ e )(RPU ⊥ e ) ]PŨ ⊥ e k


+
eeT PU ⊥ ] (M X + )T + E[(RPU ⊥ )T ] PŨ ⊥ ek
= eT
j X E[e

+
+ eT
kPU ⊥ e k2 Ir×r PŨ ⊥ e k
jM

+
ee T PU ⊥ ](X + )T M T PŨ ⊥ e k + 0
= eT
j X E[e

+
+ l2 · e T
j M PŨ ⊥ e k
=0+0+0=0
β and ζ̃ζ are Gaussians, having their covariance = 0
And as β̃
implies independence.
β and ζ̃ζ are independent Gaussians
Having established that β̃
and specified their distributions, we continue with the proof
of Theorem 3.1. We assume for now that there exists some
small a > 0 s.t.
−1
−1
2
T
2
T
l2 (M T M )−1
j,j ≤ σ (X X)j,j + l (M M )j,j
2a
2
T
≤ e · l (M M )−1
j,j

(7)

Then, due to Corollary A.3, denoting the distributions N1 = N (0, l2 (M T M )−1
=
j,j ) and N2
−1
2
T
N (0, σ 2 (X T X)−1
+
l
(M
M
)
),
we
have
that
for
any
j,j
j,j
S ⊂ R it holds that13
e−a Prβ̃j ∼N1 [S] ≤ Prβ̃j ∼N2 [S] ≤ ea Prβ̃j ∼N1 [S/ea ]
(8)
More specifically, denote the function
t̃(ψ, kξξ k, βj )

ψ − βj
q
r
kξξ k r−p
(M T M )−1
j,j
q
r
ξ
kξ
k
.
r−p
ψ − βj
= q
l
l (M T M )−1

=

j,j

and observe that when we sample ψ, ξ independently s.t.
2
ξ k2 ∼ lr χ2r−p then
ψ ∼ N (βj , l2 (M T M )−1
j,j ) and kξ
t̃(ψ, kξξ k, βj ) is distributed like a T -distribution with r − p
13
In fact, it is possible to use standard techniques from differential privacy, and argue a similar result — that the probabilities
of any event that depends on some function f (βj ) under βj ∼ N1
and under βj ∼ N2 are close in the differential privacy sense.

Differentially Private Ordinary Least Squares

degrees of freedom. And so, for any τ > 0 we have that
under such way to sample ψ, ξ we have Pr[t̃(ψ, kξξ k, βj ) >
τ ] = 1 − CDFTr−p (τ ).

for any I = (τ1 , τ2 ) with τ1 < τ2 ≤ 0, and deduce
that the PDF of the function t̃(ψ, kξξ k, βj ) at x — where
−1
2
T
we sample ψ ∼ N (βj , l2 (M T M )−1
j,j + σ (X X)j,j )

For any τ ≥ 0 and for any non-negative real value z let Szτ
denote the suitable set of values s.t.

and kξξ k2 ∼ lr χ2r−p independently — lies in the range

e−a PDFTr−p (x), ea PDFTr−p (x/ea ) . And so, using
Corollary B.4 and Claim B.5, we have that when e ∼ Dl ,
the distributions of β̃j and kζ̃ζ k2 are precisely as stated


ξ k, βj ) > τ ]
Pr
ψ∼N (βj , l2 (M T M )−1 ) [t̃(ψ, kξ
j,j

l2
kξξ k2 ∼ r χ2r−p



Z∞
=

PDF l2

2
r χr−p

0

(z) ·

2

def



Pr
[Szτ ] dz
{ψ−βj ∼N (0, l2 (M T M )−1
j,j )}

above, and so we have that the distribution of t̃(βj ) =
t̃(β̃j , kζ̃ζ k, βj ) has a PDF that at the point x is “sandwiched”
between e−a PDFTr−p (x) and ea PDFTr−p (x/ea ).

Next, we aim to argue that this characterization of the
PDF of t̃(βj ) still holds when e ∼ N (00n , σ 2 In×n ).
r
That is, Szτ = τ · z r−p
(M T M )−1
,
∞
.
j,j
It would be convenient to think of e as a sample in
N (00n , σ 2 PU ) × N (00n , σ 2 PU ⊥ ). (So while in Dl we have
We now use Equation (8) (Since N (0, l2 (M T M )−1
)
is
j,j
PU e ∼ N (00n , σ 2 PU ) but PU ⊥ e is fixed, now both PU e and
precisely N1 ) to deduce that
PU ⊥ e are sampled from spherical Gaussians.) The reason
why the above still holds lies in the fact that t̃(βj ) does not


Prψ∼N (βj , l2 (M T M )−1 +σ2 (X T X)−1 ) [t̃(ψ, kξξ k, βj ) > τ ]
depend on l. In more details:
j,j
j,j
l2




kξξ k2 ∼ r χ2r−p
Pre ∼N (00n ,σ2 In×n ) t̃(βj ) ∈ I
Z ∞
Z


=
PDF l2 2 (z)
[Szτ ]dz = Pr
Pr
v )dvv
−1
−1
2
T
2
T
0n ,σ 2 In×n ) t̃(βj ) ∈ I | PU ⊥ e = v PDFPU ⊥ e (v
e ∼N (0
ψ − βj ∼ N (0, l (M M )j,j + σ (X X)j,j )
0
r χr−p
v
Z ∞
Z


[Szτ /ea ]dz =
≤ ea
Pr
PDF l2 2 (z)
Pr t̃(βj ) ∈ I | l = kvv k PDFPU ⊥ e (vv )dvv
2 (M T M )−1 )
χ
ψ−β
∼N
(0,
l
e
∼D
0
j
l
r r−p
j,j
v
!
Z ∞
Z
Z


(∗)

= ea

PDF l2

0



q

r

χ2r−p

(z)

Pr

ψ−βj ∼N (0,

a

[Szτ /e ]dz

l2 (M T M )−1
j,j )

j,j

= ea 1 −

l2
kξξ k2 ∼ r χ2r−p

CDFTr−p (τ /ea )

= ea


ξ k, βj ) > τ ]
Pr
ψ∼N (βj , l2 (M T M )−1 +σ 2 (X T X)−1 ) [t̃(ψ, kξ
j,j

> τ]

2

= e−a

l
kξξ k2 ∼ r χ2r−p




1 − CDFTr−p (τ )

In other words, we have just shown that for any interval I = (τ, ∞) with τ ≥ 0 we have that

ξ k, βj ) ∈ I]
Pr
ψ∼N (βj , l2 (M T M )−1 +σ 2 (X T X)−1 ) [t̃(ψ, kξ
j,j



PDFTr−p (z)dz
v

PDFPU ⊥ e (vv )dvv

Z
PDFTr−p (z)dz
I/ea

where the last transition is possible precisely because t̃ is
independent of l (or kvv k) — which is precisely what makes
this t-value a pivot quantity. The proof of the lower bound
is symmetric.

j,j

l2

kξξ k2 ∼ r χ2r−p
−a
 [t̃(ψ, kξ
ξ k, βj )
e Pr

ψ∼N (βj , l2 (M T M )−1
j,j )


!Z

Z
I/ea

=
where the equality (∗) follows from the fact that
τ /c
Sz for any c > 0, since it is a non-negative interval.
Analogously, we can also show that

≥

e

a



Szτ /c



=

PDFPU ⊥ e (vv )dvv

PDFTr−p (z)dz
I/ea

v


ξ k, βj ) > τ /ea ]
= ea Pr
ψ∼N (βj , l2 (M T M )−1 ) [t̃(ψ, kξ


ea

≤

l2
kξξ k2 ∼ r χ2r−p
a

j,j



R

is lower bounded by e PDFTr−p (z)dz and upper
I
R
bounded by ea
PDFTr−p (z)dz. We can now repeat
I/ea

the same argument for I = (τ1 , τ2 ) with 0 ≤ τ1 <
τ2 (using an analogous definition of Szτ1 ,τ2 ), and again

To conclude, we have shown that if Equation (7)
holds, then for every
interval

 I ⊂ R we have that
Pre ∼N (00n ,σ2 In×n ) t̃(βj ) ∈ I
is
lower
bounded
by e−a Prz∼Tr−p [z ∈ I] and upper bounded by
ea Prz∼Tr−p [z ∈ (I/ea )].
So to conclude the proof
of Theorem 3.1, we need to show that w.h.p such a as in
Equation (7) exists.
Claim B.6. In the homoscedastic model with Gaussian
noise, if both n and r satisfy n, r ≥ p + Ω(log(1/ν)), then
−1
−1
2
T
2
T
we have that σ 2 (X T X)−1
j,j +l (M M )j,j ≥ l (M M )j,j
and
2(r−p)
−1
−1
2
T
2
T
σ 2 (X T X)−1
j,j +l (M M )j,j ≤ (1+ n−p )·l (M M )j,j
2(r−p)

n−p , Theorem 3.1 now follows
Using (1 + 2(r−p)
n−p ) ≤ e
r−p
from plugging a = n−p to our above discussion.

Differentially Private Ordinary Least Squares

Proof. The lower bound is immediate from non-negativity
T
−1/2
of σ 2 and of (X T X)−1
e j k2 . We therej,j = k(X X)
fore prove the upper bound.

parameter r w.p. ≥ 1 − ν we correctly α-reject the null hypothesis using p̃0 (i.e., w.p. ≥ 1 − ν Algorithm 1 returns
matrix unaltered and we can estimate t̃0 and verify that

First, observe that l2 = kPU ⊥ e k2 is sampled from σ 2 ·χ2n−p
as U ⊥ is of dimension n − p. Therefore, it holds that w.p.
≥ 1 − ν/2 that

indeed p̃0 < α · e

σ2

√

n−p−

p

r−p
− n−p

) provided

(

σ 2 (c̃2 + τ̃α2 )
r ≥ p + max C1 2 α
, C2 ln(1/ν)
βj σmin (Σ)

2
2 ln(2/ν) ≤ l2

)

and
2

and assuming n > p+100 ln(2/ν) we therefore have σ ≤
4
2
3(n−p) l .
Secondly, we argue that when r > p + 300 ln(4/ν)
we have that w.p.
≥ 1 − ν/2 it holds that
−1
3
T
T T
(X
X)
≤
(r
−
p)(X
R RX)−1
j,j
j,j . To see this, first
4
observe that by picking R ∼ N (0r×n , Ir×r , In×n ) the
distribution of the product RX ∼ N (0r×d , Ir×r , X T X)
is identical to picking Q ∼ N (0r×d , Ir×r , Id×d )
and taking the product Q(X T X)1/2 .
Therefore,
the
distribution
of
(X T RT RX)−1
is

T
1/2 T
T
1/2 −1
identical to
(X X) Q Q(X X)
=
(X T X)−1/2 (QT Q)−1 (X T X)−1/2 .
Denoting
v = (X T X)−1/2e j we have kvv k2 = (X T X)−1
j,j .
Claim A.1 from (Sheffet, 2015) gives that w.p. ≥ 1 − ν/2
we have

−1
T
1/2 T
(r − p) · e T
Q Q(X T X)1/2
ej
j (X X)
1
= v T ( r−p
QT Q)−1v ≥ 43 v Tv = 34 (X T X)−1
j,j


w2
n ≥ max r, C3
, C4 (p + ln(1/ν))
min{σmin (Σ), σ 2 }


16l2 (r−p)
(X T RT RX)−1
j,j
n−p
−1
2
T T
≤ 2(r−p)
l
(X
R
RX)
j,j
n−p

and as we denote M = RX we are done.
We comment that our analysis in the proof of Claim B.6
implicitly assumes r  n (as we do think of the projection R as dimensionality reduction), and so the ratio
r−p
n−p is small. However, a similar analysis holds for r
which is comparable to n — in which we would argue that
−1
2
T
σ 2 (X T X)−1
j,j +l (M M )j,j
σ 2 (X T X)−1

∈ [1, 1 + η] for some small η.

PDFN (0,1) (x)dx =

the

numbers
r−p
α − n−p
e
2

=
r−p
α − n−p
2e

s.t.
and

resp.

Proof. First we need to use the lower bound on n to show
that indeed Algorithm 1 does not alter A, and that various
quantities are not far from their expected values. Formally,
we claim the following.
Proposition B.8. Under the same lower bounds on n and r
as in Theorem 3.3, w.p. 1−α−ν we have that Theorem 3.1
holds and also that
r−p
2
2
ζ̃ζ k2 = Θ( r−p
r kPU ⊥ e k ) = Θ( r (n − p)σ )

and
−1
T
1
(X T RT RX)−1
j,j = Θ( r−p (X X)j,j )

Proof of Proposition B.8. First, we need to argue that we
2
have enough samples as to have the gap σmin
([X; y]) − w2
sufficiently large.
Since x i ∼ N (0, Σ), and yi = β Tx i + ei with ei ∼
xi ◦ yi ) is also
N (0, σ 2 ), we have that the concatenation (x
xi ] +
sampled from a Gaussian. Clearly, E[yi ] = β T E[x
β Txi +
E[ei ] = 0. Similarly, E[xi,j yi ] = E[xi,j · (β
β )j and E[yi2 ] = E[e2i ] + E[kXβ
β k2 ] = σ 2 +
ei )] = (Σβ
β T X T Xβ
β ] = σ 2 + β T Σβ
β . Therefore, each row of A is
E[β
an i.i.d sample of N (00p+1 , ΣA ), with

B.3. Proof of Theorem 3.3
Theorem B.7 (Theorem 3.3 restated.). Fix a positive definite matrix Σ ∈ Rp×p . Fix parameters β ∈ Rp and σ 2 > 0
and a coordinate j s.t. βj 6= 0. Let X be a matrix whose n
rows are sampled i.i.d from N (00p , Σ). Let y be a vector s.t.
β )i is sampled i.i.d from N (0, σ 2 ). Fix ν ∈ (0, 1/2)
yi −(Xβ
and α ∈ (0, 1/2). Then there exist constants C1 , C2 , C3
and C4 such that when we run Algorithm 1 over [X; y ] with

denote

r−p
τ̃α /e n−p

Combining the two inequalities we get:
≤

τ̃α

PDFTr−p (x)dx

r−p
c̃α /e n−p
R∞

which implies the required.

σ 2 (X T X)−1
j,j

c̃α ,

where
R∞


ΣA =

Σ
β TΣ

β
Σβ



β T Σβ
β
σ 2 +β

Denote λ2 = σmin (Σ). Then, to argue that σmin (ΣA )
is large we use the lower bound from (Ma & Zarowski,
1995) (Theorem 3.1) combining with some simple
arithmetic manipulations to deduce that σmin (ΣA ) ≥
min{σmin (Σ), σ 2 }.

Differentially Private Ordinary Least Squares
r−p

Having established a lower bound on σmin (ΣA ), it follows that with n = Ω(p ln(1/ν)) i.i.d draws from
N (00p+1 , ΣA ) we have w.p. ≤ ν/4 that σmin (AT A) =
o(n) · min{σmin (Σ), σ 2 }. Conditioned on σmin (AT A) =
Ω(nσmin (ΣA )) = Ω(w2 ) being large enough, we have that
w.p. ≤ ν/4 over the randomness of Algorithm 1 the matrix
A does not pass the if-condition and the output of the algorithm is not RA. Conditioned on Algorithm 1 outputting
RA, and due to the lower bound r = p + Ω(ln(1/ν)),
we have that the result of Theorem 3.1 does not hold w.p.
≤ α + ν/4. All in all we deduce that w.p. ≥ 1 − α − 3ν/4
the result of Theorem 3.1 holds. And since we argue Theorem 3.1 holds, then the following two bounds that are used
in the proof14 also hold:

= Ω(e n−p (c̃α + τ̃α )σ̃



(c̃α +τ̃α )2 σ 2
βj2 σmin (Σ)



C. Projected Ridge Regression
In this section we deal with the case that our matrix does
not pass the if-condition of Algorithm 1. In this case, the
matrix is appended
 with a d× d-matrix which is wId×d .
A
0
Denoting A =
we have that the algorithm’s
w · Id×d
output is RA0 .
Similarly to before, we are going to denote d = p + 1 and
decompose A = [X; y ] with X ∈ Rn×p and y ∈ Rn , with
β + e and ei sampled
the standard assumption of y = Xβ
i.i.d from N (0, σ 2 ).15 We now need to introduce some additional notation. We denote the appended matrix and vectors X 0 and y 0 s.t. A0 = [X 0 ; y 0 ]. Meaning:


X
X 0 =  wIp×p 
0T
p

2

kPU ⊥ e k = Θ((n − p)σ )
Lastly, in the proof of Theorem 3.1 we argue that
for a given PU ⊥ e the length kζ̃ζ k2 is distributed like
kPU ⊥ e k2 2
χr−p .
r

Appealing again to the fact that r = p +
Ω(ln(1/ν) we have that w.p. ≥ ν/4 it holds that kζ̃ζ k2 >
kP

(X T RT RX)−1
j,j ))

which, given the lower bound r = p + Ω
indeed holds.

−1
T
1
(X T RT RX)−1
j,j = Θ( r−p (X X)j,j )
2

q

e k2

2(r − p) U r⊥ . Plugging in the value of kPU ⊥ e k2 concludes the proof of the proposition.

and
Based on Proposition B.8, we now show that we indeed
reject the null-hypothesis (as we should). When Theorem 3.1 holds, reject the null-hypothesis iff p̃0 < α ·
e

r−p
− n−p




y
e
def
β  = X 0β + e 0
y 0 =  0 p  = X 0β +  −wβ
w
w


r−p

which holds iff |t̃0 | > e n−p τ̃α . This implies
r−p

And so we respectively denote R = [R1 ; R2 ; R3 ] with
R1 ∈ Rr×n , R2 ∈ Rr×p and R3 ∈ Rr×1 (so R3 is a
vector denoted as a matrix). Hence:

n−p τ̃ ·
we
α
q reject that null-hypothesis when |β̃j | > e
−1
σ̃ (X T RT RX)j,j ). Note that this bound is based

onCorollary 3.2 that determines
 that |β̃j − βj | =
q
r−p
−1
T
T
O e n−p c̃α · σ̃ (X R RX)j,j ) . And so we have that

M 0 = RX 0 = R1 X + wR2
and

w.p. ≥ 1 − ν we α-reject the null hypothesis when it holds
q
r−p
n−p (c̃ +
that |βj | > 3(c̃α + τ̃α )· σ̃ (X T RT RX)−1
)
≥
e
α
j,j
q
τ̃α )σ̃ (X T RT RX)−1
j,j ) (due to the lower bound n ≥ r).

β +R1e +wR3
Ryy 0 = RX 0β +Ree0 = R1y +wR3 = R1 Xβ

Based on the bounds stated above we have that
q
q
q
√
√
r
r
σ̃ = kζ̃ζ k r−p
= Θ(σ n − p r−p
r
r−p ) = Θ(σ n − p)



1
r−p

·

1
nσmin (Σ)

And so, a sufficient condition for rejecting the nullhypothesis is to have
r


n−p q
1
|βj |
= Ω (c̃α + τ̃α )σ
· nσmin (Σ)
r−p
14

β0

= arg min 1r kRyy 0 − RX 0z k2
z

= (X 0T RT RX 0 )−1 (RX 0 )T (Ryy 0 )

and that
−1
T
1
(X T RT RX)−1
j,j = Θ( r−p (X X)j,j ) = O

And so, using the output RA0 of Algorithm 1, we solve
the linear regression problem derived from √1r RX 0 and
√1 Ry
y 0 . I.e., we set
r

More accurately, both are bounds shown in Claim B.6.


Sarlos’ results (2006) regarding the Johnson Lindenstrauss
transform give that, when R has sufficiently many rows,
solving the latter optimization problem gives a good approximation for the solution of the optimization problem
β R = arg minz kyy 0 − X 0z k2 = arg minz kyy − Xzz k2 + w2 kzz k2
15
Just as before, it is possible to denote any single column as y
and any subset of the remaining columns as X.



Differentially Private Ordinary Least Squares

The latter problem is known as the Ridge Regression problem. Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from
the perspective of penalizing linear vectors whose coefficients are too large. It is also often applied in the case
where X doesn’t have full rank or is close to not having full-rank. That is because the Ridge Regression problem is always solvable. One can show that the minimizer
β R = (X T X + w2 Ip×p )−1 X Ty is the unique solution of
the Ridge Regression problem and that the RHS is always
defined (even when X is singular).
The original focus of Ridge Regression is on penalizing
β R for having large coefficients. Therefore, Ridge Regression actually poses a family of linear regression problems: minz ky − Xzz k + λkzz k2 , where one may set λ to be
any non-negative scalar. And so, much of the literature on
Ridge Regression is devoted to the art of fine-tuning this
penalty term — either empirically or based on the λ that
β R ] − β k2 + Var(β
β R ).16 Here we
yields the best risk: kE[β
propose a fundamentally different approach for the choice
of the normalization factor — we set it so that solution of
the regression problem would satisfy (, δ)-differential privacy (by projecting the problem onto a lower dimension).
While the solution of the Ridge Regression problem might
have smaller risk than the OLS solution, it is not known
how to derive t-values and/or reject the null hypothesis under Ridge Regression (except for using X to manipulate
β = (X T X)−1 X Ty and relying on OLS).
β R back into β̂
In fact, prior to our work there was no need for such analysis! For confidence intervals one could just use the standard
OLS, because access to X and y was given.
Therefore, much for the same reason, we are unable to derive t-values under projected Ridge Regression.17 Clearly,
there are situations where such confidence bounds simply
cannot be derived. (Consider for example the case where
X = 0n×p and y is just i.i.d draws from N (0, σ 2 ), so
obviously [X; y] gives no information about β .) Nonetheless, under additional assumptions about the data, our work
can give confidence intervals for βj , and in the case where
the interval doesn’t intersect the origin — assure us that
sign(βj0 ) = sign(βj ) w.h.p.
Clearly, Sarlos’ work (2006) gives an upper bound on the
β 0 −β
β R k. However, such distance bound doesn’t
distance kβ
come with the coordinate by coordinate confidence guarantee we would like to have. In fact, it is not even clear
β 0 ] = β R (though it is obvious
from Sarlos’ work that E[β
0T T
0 βR
to see that E[(X R RX )]β = E[(RX 0 )T Ryy 0 ]). Here,

β 0 ] = β̂
β which, more often than not, does
we show that E[β
R
not equal β .
Comment about notation. Throughout this section we assume X is of full rank and so (X T X)−1 is well-defined. If
X isn’t full-rank, then one can simply replace any occurrence of (X T X)−1 with X + (X + )T . This makes all our
formulas well-defined in the general case.
C.1. Running OLS on the Projected Data
In this section, we analyze the projected Ridge Regression,
under the assumption (for now) that e is fixed. That is, for
now we assume that the only source of randomness comes
from picking the matrix R = [R1 ; R2 ; R3 ]. As before, we
analyze the distribution over β 0 (see Equation (9)), and the
value of the function we optimize at β 0 . Denoting M 0 =
RX 0 , we can formally express the estimators:
β0
ζ0

= (M 0T M 0 )−1 M 0T Ryy 0
= √1r (Ryy 0 − RX 0β 0 )

(9)
(10)

β +ee for a fixed e , and given
Claim C.1. Given that y = Xβ
X and M 0 = RX 0 = R1 X + wR2 we have that

β 0 ∼ N β + X +e ,

β + X +e k2 + 1) + kPU ⊥ e k2 )(M 0T M 0 )−1
(w2 (kβ

ζ 0 ∼ N 0r ,

β +X +e k2 +1)+kPU ⊥ e k2
w2 (kβ
0
0+
(I
−
M
M
)
r×r
r
and furthermore, β 0 and ζ 0 are independent of one another.

Proof. First, we write β 0 and ζ 0 explicitly, based on e and
projection matrices:
β0
ζ0

= (M 0T M 0 )−1 M 0T Ryy 0
β + M 0+ (R1e + wR3 )
= M 0+ (R1 X)β
= √1r (Ryy 0 − RX 0β 0 )
= √1r (Ir×r − M 0 M 0+ )Ree0
= √1r PU 0⊥ (R1e − wR2β + wR3 )

with U 0 denoting colspan(M 0 ) and PU 0⊥ denoting the projection onto the subspace U 0⊥ .

16

Ridge Regression, as opposed to OLS, does not yield an unβ R ] 6= β .
biased estimator. I.e., E[β
17
Note: The naı̈ve approach of using RX 0 and Ryy 0 to interpolate RX and Ryy and then apply Theorem 3.1 using these estimations of RX and Ryy ignores the noise added from appending the
matrix A into A0 , and it is therefore bound to produce inaccurate
estimations of the t-values.

Again, we break e into an orthogonal composition: e =
PU e + PU ⊥ e with U = colspan(X) (hence PU = XX + )
and U ⊥ = colspan(X)⊥ . Therefore,
β0

β + M 0+ (R1 XX +e + R1 PU ⊥ e + wR3 )
= M 0+ (R1 X)β
0+
β + X +e ) + M 0+ (R1 PU ⊥ e + wR3 )
= M (R1 X)(β

Differentially Private Ordinary Least Squares

whereas ζ 0 is essentially
√1 (Ir×r
r
(∗)

− M 0 M 0+ )(R1 XX +e + R1 PU ⊥ e − wR2β + wR3 )

√1 (Ir×r
r

=

0

−M M

0+

)·

β + wR3 )
(R1 XX e + R1 PU ⊥ e + (M 0 − wR2 )β
+

=

√1 (Ir×r
r

− M 0 M 0+ )·

β + X +e k2 + 1) + kPU ⊥ e k2 )PU 0⊥
I.e., N 0 r , 1r (w2 (kβ
0
as PU 0⊥ M = 0r×r .



Finally, observe that β 0 and ζ 0 are independent as the former depends on the projection of the spherical Gaussian
R1 X(β + X +e ) + R1 PU ⊥ e + wR3 on U 0 , and the latter
depends on the projection of the same multivariate Gaussian on U 0⊥ .

β + X +e ) + R1 PU ⊥ e + wR3 )
(R1 X(β
where equality (∗) holds because (I − M 0 M 0+ )M 0v = 0
for any v .
We now aim to describe the distribution of R given that we
know X 0 and M 0 = RX 0 . Since
M 0 = R1 X + wR2 + 0 · R3 = R1 X(X + X) + wR2
= (R1 PU )X + wR2
then M 0 is independent of R3 and independent of R1 PU ⊥ .
Therefore, given X and M 0 the induced distribution over
R3 remains R3 ∼ N (00r , Ir×r ), and similarly, given X and
M 0 we have R1 PU ⊥ ∼ N (0r×n , Ir×r , PU ⊥ ) (rows remain
independent from one another, and each row is distributed
like a spherical Gaussian in colspan(X)⊥ ). And so, we
have that R1 X = R1 PU X = M 0 − wR2 , which in turn
implies:

R1 X ∼ N M 0 , Ir×r , w2 · Ip×p

Observe that Claim C.1 assumes e is given. This may seem
somewhat strange, since without assuming anything about
e there can be many combinations of β and e for which
β + e . However, we always have that β + X +e =
y = Xβ
+
β . Similarly, it is always the case the PU ⊥ e =
X y = β̂
β and ζ in
(I − XX + )yy = ζ . (Recall OLS definitions of β̂
Equation (1) and (2).) Therefore, the distribution of β 0 and
ζ 0 is unique (once y is set):


β , (w2 (kβ̂
β k2 + 1) + kζζ k2 )(M 0T M 0 )−1
β 0 ∼ N β̂
!
2 β 2
2
ζ
w
(k
β̂
k
+
1)
+
kζ
k
ζ 0 ∼ N 0r ,
(Ir×r − M 0 M 0+ )
r
And so for a given dataset [X; y ] we have that β 0 serves as
β.
an approximation for β̂

multiplying this random matrix with a vector, we get

An immediate corollary of Claim C.1 is that for
any fixed e it holds that the quantity t0 (βj ) =

β +X +e ) ∼ N (M 0β + M 0 X +e , w2 kββ + X +e k2 Ir×r )
R1 X(β

kζζ 0 k

and multiplying this random vector with a matrix we get
β +X +e ) ∼ N (β + X +e ,
M 0+ R1 X(β

β + X +e k2 (M 0T M )−1 )
w2 kβ

I.e.,
β +X +e ) ∼ kβ
β +X +e k·N (u
u, w2 (M 0T M )−1 )
M 0+ R1 X(β
where u denotes a unit-length vector in the direction of β +
X +e .
Similar to before we have
RPU ⊥ ∼ N (0r×n , Ir×r , PU ⊥ )
⇒ M 0+ (RPU ⊥ e) ∼ N (00d , kPU ⊥ ek2 (M 0T M 0 )−1 )
wR3 ∼ N (00r , w2 Ir×r )
⇒ M 0+ (wR3 ) ∼ N (00d , w2 (M 0+ M 0 )−1 )
Therefore, the distribution of β 0 , which is the sum of the 3
independent Gaussians, is as required.
β + X +e ) + R1 PU ⊥ e + wR3 )
Also, ζ 0 = √1r PU 0⊥ (R1 X(β
is the sum of 3 independent Gaussians, which implies its
distribution is

β + X +e ),
N √1r PU 0⊥ M 0 (β

2
+ 2
2
1
β
(w
(kβ
+
X
e
k
+
1)
+
kP
⊥ e k )PU 0⊥
U
r

βj0 −(βj +(X +e )j )
q r
0T
0 −1
r−p ·(M M )j,j

=

kζζ 0 k

β 0 −β̂j
q rj
0T
0 −1
r−p ·(M M )j,j

is dis-

tributed like a Tr−p -distribution. Therefore, the following
theorem follows immediately.
β =
Theorem C.2. Fix X ∈ Rn×p and y ∈ R. Define β̂
X +y and ζ = (I − XX + )yy . Let RX 0 and Ryy 0 denote
the result of applying Algorithm 1 to the matrix A = [X; y ]
when the algorithm appends the data with a w · I matrix.
Fix a coordinate j and any α ∈ (0, 1/2). When computing
β 0 and ζ 0 as in Equations (9) it and (10), we have that w.p.
≥ 1 − α it holds that
q


r
β̂j ∈ βj0 ± c0α kζζ 0 k r−p
· (M 0T M 0 )−1
j,j
where c0α denotes the number such that (−c0α , c0α ) contains
1 − α mass of the Tr−p -distribution.
Note that Theorem C.2, much like the rest of the discussion in this Section, builds on y being fixed, which means
βj0 serves as an approximation for β̂j . Yet our goal is to
argue about similarity (or proximity) between βj0 and βj .
To that end, we combine the standard OLS confidence interval — which says that w.p. ≥ 1 − α over the randomness of picking r
e in the homoscedastic model we have
|βj − β̂j | ≤ cα kζζ k

(X T X)−1
j,j
n−p

— with the confidence in-

terval of Theorem C.2 above, and deduce that w.p. ≥ 1 − α

Differentially Private Ordinary Least Squares

we have that |βj0 − βj | is at most

1 − ν over the randomness of R we have (r −

kζζ 0 k2
−1
2
T
p)(M 0T M )−1
j,j = Θ (w Ip×p + X X)j,j and r−p =

q
q

kζζ k (X T X)−1
kζζ 0 k r(M 0T M 0 )−1
j,j
j,j

√
√
O cα
+ c0α
n−p
r−p


Θ( w

(11)
And so, in the next section, our goal is to give conditions under which the interval of Equation (11) isn’t
much larger
q in comparison to the interval length of
18

ζ0

ζ k
c0α √kζr−p
r(M 0T M 0 )−1
j,j we get from Theorem C.2; and
more importantly — conditions that make the interval of
Theorem C.2 useful q
and not too large. (Note, in expecta-

tion

kζζ 0 k
√
r−p

is about

β k2 + kζζ k2 )/r. So, for
(w2 + w2 kβ̂

β k is very large, this interval
example, in situations where kβ̂
isn’t likely to inform us as to the sign of βj .)
Motivating Example. A good motivating example for the
discussion in the following section is when [X; y ] is a strict
submatrix of the dataset A. That is, our data contains many
variables for each entry (i.e., the dimensionality d of each
entry is large), yet our regression is made only over a modest subset of variables out of the d. In this case, the least
singular value of A might be too small, causing the algorithm to alter A; however, σmin (X T X) could be sufficiently large so that had we run Algorithm 1 only on [X; y ]
we would not alter the input. (Indeed, a differentially private way for finding a subset of the variables that induce a
submatrix with high σmin is an interesting open question,
partially answered — for a single regression — in the work
of Thakurta and Smith (Thakurta & Smith, 2013).) Indeed,
the conditions we specify in the following section depend
on σmin ( n1 X T X), which, for a zero-mean data, the minimal variance of the data in any direction. For this motivating example, indeed such variance isn’t necessarily small.
C.2. Conditions for Deriving a Confidence Interval for
Ridge Regression

2

β k2 +kζζ k2
+w2 kβ̂
).
r

Proof. The former bound follows from known results on
the Johnson-Lindenstrauss transform (as were shown in the
proof of Claim B.6). The latter bound follows from standard concentration bounds of the χ2 -distribution.

Plugging in the result of Proposition C.3 to Equation (11)
we get that w.p. ≥ 1 − ν the difference |βj0 − βj | is at most
q

kζζ k
O cα √
(X T X)−1
j,j
n−p
s

β k2 + kζζ k2 q 2
w2 + w2 kβ̂
+ c0α
(w Ip×p + X T X)−1
j,j
r−p
(12)
We will also use the following proposition.
Proposition C.4.
(X T X)−1
j,j ≤


1+

w2
σmin (X T X)



(w2 Ip×p + X T X)−1
j,j

Proof. We have that
(X T X)−1
= (X T X)−1 (X T X + w2 Ip×p )(X T X + w2 Ip×p )−1
= (X T X + w2 Ip×p )−1 + w2 (X T X)−1 (X T X + w2 Ip×p )−1
= (Ip×p + w2 (X T X)−1 )(X T X + w2 Ip×p )−1

Looking at the interval specified in Equation (11), we now
give an upper bound on the the random quantities in this
interval: kζζ k, kζζ 0 k, and (M 0T M 0 )−1
j,j . First, we give bound
that are dependent on the randomness in R (i.e., we continue to view e as fixed).

= (X T X + w2 Ip×p )−1/2 ·

Proposition C.3. For any ν ∈ (0, 1/2), if we
have r = p + Ω(ln(1/ν)) then with probability ≥

where the latter holds because (Ip×p + w2 (X T X)−1 ) and
(X T X + w2 Ip×p )−1 are diagonalizable by the same matrix V (the same matrix for which (X T X) = V S −1 V T ).
2
Since we have kIp×p + w2 (X T X)−1 k = 1 + σ2 w (X) , it is

18

Observe that w.p.

≥ 1 − α r
over the randomness of e

we have that |βj − β̂j | ≤ cα kζζ k

(X T X)−1
j,j
n−p

, and w.p.

≥

1 − α over the randomness of R we have that |βj0 − β̂j | ≤
q
r
c0α kζζ 0 k r−p
· (M 0T M 0 )−1
j,j . So technically, to give a (1 − α)βj0

confidence interval around
that contains βj w.p. ≥ 1 − α, we
need to use cα/2 and c0α/2 instead of cα and c0α resp. To avoid
overburdening the reader with what we already see as too many
parameters, we switch to asymptotic notation.

(Ip×p + w2 (X T X)−1 )·
(X T X + w2 Ip×p )−1/2

min

w2
)I .
2
σmin
(X) p×p
T
T
−1
e j (X X) e j ≤ (1 +

clear that (Ip×p + w2 (X T X)−1 )  (1 +
We deduce that (X T X)−1
j,j =
w2
)(X T X
2
σmin
(X)

+ w2 Ip×p )−1
j,j .

Based on Proposition C.4 we get from Equation (12) that

Differentially Private Ordinary Least Squares

|βj0 − βj | is at most
v
u
w2
2
 u
t kζζ k (1 + σmin (X T X) )
O( cα
+
n−p
s
β k2 + kζζ k2 q 2
w2 + w2 kβ̂
(w Ip×p + X T X)−1
c0α
j,j )
r−p
(13)
And so, if it happens to be the case that exists some small
β , ζ and w2 satisfy
η > 0 for which β̂
!
2
2
2 β 2
2
kζζ k2 (1 + σminw(X T X) )
ζ
w
+
w
k
β̂
k
+
kζ
k
≤ η2
n−p
r−p
(14)
then
we
have
that
Pr[β
∈
j


q
βj0

± O((1 + η) ·

1 − α.

c0α kζζ 0 k

r
r−p

·

(M 0T M 0 )−1
j,j )

]

≥

19

Moreover, if in this case |βj | >
q
q
2
2 kβ̂
β k2 +kζζ k2
+ η) w +w r−p
(w2 Ip×p + X T X)−1
j,j
then Pr[sign(βj0 ) = sign(βj )] ≥ 1 − α. This is precisely
what Claims C.5 and C.6 below do.
Claim C.5. If there
exists η
>
0
2
2
s.t.
n − p
≥
(r
−
p)
and
n
=
2
η


2
1
3/2 B ln(1/δ)
· 2
, then Pr[βj ∈
Ω r
·
1

η σmin ( n X T X)


q
r
βj0 ± O((1 + η) · c0α kζζ 0 k r−p
· (M 0T M 0 )−1
≥
j,j ) ]
1 − α.

c0α (1

Proof. Based on the above discussion, it is enough to argue that under the conditions of the claim, the constraint
2
r−p
then
of Equation (14) holds. Since we require η2 ≥ n−p
it is evident that
kζζ k2
n−p

2

kζζ k2
n−p

η 2 kζζ k2
2(r−p) . So we now show that
η 2 kζζ k2
2(r−p) under the conditions of the

≤

· σminw(X T X) ≤
claim, and this will show the required. All that is left is
some algebraic manipulations. It suffices to have:
η2
2

·

n−p
T
r−p σmin (X X)

η2
2

≥

32B

≥
which holds for n2 ≥ r3/2 ·
as we assume to hold.

·

T
n2
1
r σmin ( n X X)
√
2

r ln(8/δ)
≥ w2


64B 2 ln(1/δ)
σmin ( n1 X T X)−1 ,
η 2

Claim C.6. Fix ν ∈ (0, 12 ). If (i) n = p + Ω(ln(1/ν)),
β k2 = Ω(σ 2 kX + k2F ln( νp )) and (iii) r − p =
(ii) kβ


Ω

(c0α )2 (1+η)2
βj2

β k2 +
1 + kβ

σ2
1
σmin ( n X T X)

, then in the

Proof. Based on the above discussion, we aim to show that
in the homoscedastic model (where each coordinate ei ∼
N (0, σ 2 ) independently) w.p. ≥ 1 − ν it holds that the
magnitude of βj is greater than
s
c0α (1 + η)

β k2 + kζζ k2 q 2
w2 + w2 kβ̂
(w Ip×p + X T X)−1
j,j
r−p

To show this, we invoke Claim A.4 to argue that w.p. ≥
1 − ν we have (i) kζζ k2 ≤ 2σ 2 (n − p) (since n = p +
β k2 ≤ 2kβ
β k2 (since kβ
β − β̂
β k2 ≤
Ω(ln(1/ν))), and (ii) kβ̂
p
2
+ 2
2
2
+
2
β k = Ω(σ kX kF ln( νp ))).
σ kX kF ln( ν ) whereas kβ
2
We also use the fact that (w2 Ip×p + X T X)−1
j,j ≤ (w +
−1
T
σmin (X X)), and then deduce that
s

β k2 + kζζ k2 q 2
w2 + w2 kβ̂
(w Ip×p + X T X)−1
j,j
r−p
s
β k2 ) + σ 2 (n − p)
(1 + η)c0α
w2 (1 + kβ
≤ √
2
2
r−p
w + σmin (X T X)
s
(1 + η)c0α
2σ 2 (n − p)
β k2 ) +
≤ √
2(1 + kβ
≤ |βj |
r−p
σmin (X T X)

(1 + η)c0α

due to our requirement on r − p.
Observe, out of the 3 conditions specified in Claim C.6,
condition (i) merely guarantees that the sample is large
enough to argue that estimations are close to their expect
value; and condition (ii) is there merely to guarantee that
β k ≈ kβ
β k. It is condition (iii) which is non-trivial to
kβ̂
hold, especially together with the conditions of Claim C.5
that pose other constraints in regards to r, n, η and the various other parameters in play. It is interesting to compare
the requirements on r to the lower bound we get in Theorem 3.3 — especially the latter bound. The two bounds
are strikingly similar, with the exception that here we also
β k2
require r − p to be greater than 1+kβ
. This is part of the
βj2
unfortunate effect of altering the matrix A: we cannot give
confidence bounds only for the coordinates j for which βj2
β k2 .
is very small relative to kβ
In summary, we require to have n = p + Ω(ln(1/ν)) and
β k compathat X contains enough sample points to have kβ̂
β k, and then set r and η such that (it is convenient
rable to kβ
to think of η as a small constant, say, η = 0.1)
• r − p = O(η 2 (n − p)) (which implies r = O(n))

homoscedastic model, with probability ≥ 1−ν −α we have
that sign(βj ) = sign(βj0 ).


2
3
n2
• r = O( η 2 B 2 ln(1/δ)
σmin ( n1 X T X) )

19
We assume n ≥ r so cα < c0α as the Tn−p -distribution is
closer to a normal Gaussian than the Tr−p -distribution.

βk
• r − p = Ω( 1+kβ
+
β2
β

j

2

σ2
βj2

−1 1
· σmin
( n X T X))

Differentially Private Ordinary Least Squares

to have that the (1 − α)-confidence interval around βj0
does not intersect the origin. Once again, we comment
that these conditions are sufficient but not necessary, and
furthermore — even with these conditions holding — we
do not make any claims of optimality of our confidence
bound. That is because from Proposition C.4 onwards our
discussion uses upper bounds that do not have corresponding lower bounds, to the best of our knowledge.

D. Confidence Intervals for “Analyze Gauss”
Algorithm
To complete the picture, we now analyze the “Analyze
Gauss” algorithm of Dwork et al (Dwork et al., 2014).
Algorithm 2 works by adding random Gaussian noise to
AT A, where the noise is symmetric with each coordi2
nate abovethe diagonal
 sampled i.i.d from N (0, ∆ ) with

∆2 = O B 4 log(1/δ)
.20 Using the same notation for a
2

sub-matrix of A as [X; y ] as before, with X ∈ Rn×p and
y ∈ Rn , we denote the output of Algorithm 2 as








^
TX
X
TX
yg



g

Ty 
X
=
 
Ty
yg


T

X X +N
y TX + nT

Ty

X y +n 


y Ty + m

(15)
where N is a symmetric p × p-matrix, n is a p-dimensional
vector and m is a scalar, whose coordinates are sampled
i.i.d from N (0, ∆2 ).
Using the output of Algorithm 2, it is simple to derive anaβ and kζζ k2 (Equations (1) and (2))
logues of β̂
−1

−1
g
^
TX
Ty = X T X + N
X
βe = X
(X T y + n )
(16)

r
+∆

−2

√
^
TX
X
j,j · ln(1/ν) · (B p + 1)



where ρ is such that ρ2 is w.h.p an upper bound on σ 2 ,
defined as
2 def

ρ =



1√
√
n−p−2 ln(4/α)

2
·




√
−1
B2 p p
2
g
^
T
2
kζζ k − C · ∆ 1−η ln(1/ν) + ∆ kX X kF · ln(p/ν)

for some large constant C.
We comment that in practice, instead of using ρ, it might
be better to use the MLE of σ 2 , namely:


−1
def
2 ^
1
g
2
2
T
σ = n−p kζζ k + ∆ kX X kF
instead of ρ2 , the upper bound we derived for σ 2 . (Replacing an unknown variable with its MLE estimator is a common approach in applied statistics.) Note that the assumpβ k ≤ B is fairly benign once we assume each
tion that kβ
β k ≤ B simrow has bounded l2 -norm. The assumption kβ̂
β
ply assumes that β̂ is a reasonable estimation of β , which is
likely to hold if we assume that X T X is well-spread. The
assumption about the magnitude of the least singular value
of X T X is therefore the major one. Nonetheless, in the
case we considered before where each row in X is sampled
i.i.d from N (00, Σ), this assumption
merely means that n is
√
large enough s.t. n = Ω̃(

∆ p ln(1/ν)
η·σmin (Σ) ).

In order to prove Theorem D.1, we require the following
proposition.

(17)

Proposition D.2. Fix any ν ∈ (0, 21 ). Fix any matrix
M ∈ Rp×p . Let v ∈ Rp be a vector with each coordinate
sampled independently
from a Gaussian N (0, ∆2 ).i Then
h
p
we have that Pr kMvv k > ∆ · kM kF 2 ln(2p/ν) < ν.

g
ζ k2 to get a
We now argue that it is possible to use βej and kζ
confidence interval for βj under certain conditions.
Theorem D.1. Fix α, ν ∈ (0, 12 ). Assume that there exists
p
η ∈ (0, 21 ) s.t. σmin (X T X) > ∆ p ln(1/ν)/η. Under the
homoscedastic model, given β and σ 2 , if we assume also
β k ≤ B and kβ̂
β k = k(X T X)−1 X Ty k ≤ B, then
that kβ
w.p. ≥ 1 − α − ν it holds that |βj − βej | it at most
s


−1
−2
p
^
^
TX
TX
O ρ·
X
+
∆
p
ln(1/ν)
·
X
ln(1/α)
j,j
j,j

Proof. Given M , we have that Mvv ∼ N (00, ∆2 · M M T ).
Denoting M ’s singular values as sv1 , . . . , svp , we can rotate Mvv without affecting its l2 -norm and infer that kMvv |2
is distributed like a sum on p independent Gaussians, each
sampled from N (0, ∆2 · svi2 ). Standard union bound gives
that w.p. ≥ 1 − ν non of the pp
Gaussians exceeds its standard deviation by a factor of 2 ln(2p/ν).
Hence, w.p.
P
≥ 1 − ν it holds that kMvv k2 ≤ 2∆2 i svi2 ln(2p/ν) =
2∆2 · trace(M M T ) · ln(2p/ν).

T

g
^
]
e + βe X
e
Ty − 2 y
TXβ
TX β
ζ k2 = yg
kζ
−1

^
]
Ty − y
TX X
TX
= yg

]
Ty
X

20
It is easy to see that the l2 -global sensitivity of the mapping
A 7→ AT A is ∝ B 4 . Fix any A1 , A2 that differ on one row
which is some vector v with kvv k = B in A1 and the all zero
vector in A2 . Then GS22 = kAT1 A1 − AT2 A2 k2F = kvvv T k2F =
trace(vvv T · v v T ) = (vv Tv )2 = B 4 .

Our proof also requires the use of the following equality,
that holds for any invertible A and any matrix B s.t. I +
B · A−1 is invertible:
−1
−1
(A + B) = A−1 − A−1 I + BA−1
BA−1

Differentially Private Ordinary Least Squares
−1

In our case, we have

T
^
TX
we can bound the variance of X
j→ X e by
!


2
−1 
−1

^
T
TX
 .
^
Appealing to
σ2 X
j,j + kN k · X X j→ 

−1

^
TX
X
= (X T X + N )−1
T

−1

−1

T


−1 −1

−1

T

− (X X) I + N (X X)
N (X X)



−1
= (X T X)−1 I − I + N (X T X)−1
N (X T X)−1

def
= (X T X)−1 I − Z · (X T X)−1
(18)
= (X X)

T

Proof of Theorem D.1. Fix ν > 0. First, we apply to standard results about Gaussian matrices, such as (Tao, 2012)
(used also by (Dwork et al., 2014) in their analysis),
to see
p
that w.p. ≥ 1 − ν/6 we have kN k = O(∆ p ln(1/ν)).
And so, for the remainder of the proof we fix N subject to
having bounded operator norm. Note that by fixing N we
^
T X.
fix X
β + e with
Recall that in the homoscedastic model, y = Xβ
each coordinate of e sampled i.i.d from N (0, σ 2 ). We
therefore have that
−1

^
TX
βe = X

^
TX
(X Ty + n ) = X

−1

−1

^
TX
=X

−1

^
TX
=β −X

−1

^
TX
β +X
Nβ

−1

T

^
TX
X e+X

^
TX
X Te + X

−1

−1

^
TX
0, ∆2 I) is samTo bound X
j→n note that n ∼ N (0
^
T X.
pled independently of X
We therefore have that


−1
−1 2

2 ^
^
TX
T

X
j→n ∼ N (0, ∆ X X j→  ). Gaussian concentra−1

^
TX
tion bounds
give that
we have |X
j→n | =
 w.p ≥ 1−ν/6

 
−1
p

^
T

O ∆
X X j→  ln(1/ν) .
Plugging this into our above bounds on all terms that appear
in Equation
 (19) we
 have that w.p. ≥ 1 − ν/2 − α/2 we


have that βej − βj  is at most



−1 
p

^
T


O X X j→  · B∆ p ln(1/ν)

 v
u

 !
−1
−1 2
u
p

^
TX
T
^
 ln(1/α) 
+ O σ t X
j,j + ∆ p ln(1/ν) · X X j→ 

n

^
TX
as X
j→ we deduce:
−1

−1

−1

n

−1

−1

^
TX
Denoting the j-th row of X

u

 !
−1
−1 2
u
p

^
^
t
T
T
2


O
X X j,j + ∆ p ln(1/ν) · X X j→ 
 σ ln(1/α) .

β + X Te + n )
(X T Xβ
−1

^
^
T X − N )β
TX
β +X
(X

Gaussian concentration bounds, we have that w.p.
≥ 1v
− α/2 the absolute value of this Gaussian is at most




−1  p

^
T


+ O ∆ X X j→  ln(1/ν)


T
^
^
^
T
TX
TX
β +X
βej = βj − X
j→ X e + X X j→n
j→ Nβ

(19)
We naı̈vely bound  the size
of the

−1 
−1

^
^
T
TX

X
β
βk
X
by
j→ Nβ
 X j→  kN kkβ



−1 
p

^
TX
 · B∆ p ln(1/ν) .
O 
X
j→


−1

Te

^
TX
X
j→ X e

term
=

note that e is cho^
TX
sen
independently
of
X
and
since
−1
2
T
^
T
e
∼
N (00, σ I) we have X X j→ X e
∼


−1
−1
^
^
TX
T
N 0, σ2 · eT
· X TX · X
ej .
Since
jX X
To

bound

^
T X we have
Note that due to the symmetry of X


−1 2
−2

^
^
T
T
X

 X j→  = X X j,j (the (j, j)-coordinate of the ma−2

^
TX
trix X



), thus |βej − βj | is at most

s

O σ·
r
+∆

−1

^
TX
X
j,j + ∆
−2


−2
p
^
T
p ln(1/ν) · X X j,j ln(1/α)

√
^
TX
X
j,j · ln(1/ν) · (B p + 1)


(20)

we have
−1

^
TX
X

^
TX
· X TX · X

−1

−1

−1

^
^
TX − N ) · X
TX
· (X

^
TX
=X
−1

^
TX
=X

−1

^
TX
−X

−1

^
TX
·N ·X

All of the terms appearing in Equation (20) are known
^
T X, except for σ — which is a parameter of the
given X
model. Next, we derive an upper bound on σ which we can
then plug into Equation (20) to complete the proof of the
theorem and derive a confidence interval for βj .

Differentially Private Ordinary Least Squares

p
most O(∆ · kzz k ln(1/ν)) w.p. ≥ 1 − ν/8. We can
β k kI − Z(X T X)−1 k =
upper bound kzz k ≤ 2kβ̂
B
O( 1−η ), and so this term’s magnitude is upper


√
∆·B ln(1/ν)
bounded by O
.
1−η

Recall Equation (17), according to which we have
−1

g
^
]
Ty − y
TX X
TX
ζ k2 = yg
kζ

]
Ty
X

(18)

= y Ty + m
− (yy T X + n T )(X T X)−1 (I − Z · (X T X)−1 )(X Ty + n )

• Given our assumption about the least singular value
of X T X and with the bound on kN k, we have that
T
^
T X) ≥ σ
σmin (X
min (X X) − kN k > 0 and so

= y Ty + m
− y T X(X T X)−1 X Ty
+ y T X(X T X)−1 Z(X T X)−1 X Ty
T

T

−1

n

T

T

−1

Z(X T X)−1n

− 2yy X(X X)
+ 2yy X(X X)

^
T X is a PSD. Therefore,
the symmetric matrix X
−1
−1/2
^
^
TX
TX
n = kX
n k2 is strictly
the term n T X
positive. Applying Proposition D.2 we have that
−1
T^
TX
w.p.
≥
1
−
ν/8
it
holds
that
n
X
n ≤


−1
2 ^
O ∆ kX T X kF · ln(p/ν) .

n
− nT (X T X)−1 (I − Z · (X T X)−1 )n
β = (X T X)−1 X Ty , and so we have
Recall that β̂

T
β Zβ̂
β
= y T I − X(X T X)−1 X T y + m − β̂
T

^
TX
β (I − Z(X T X)−1 )n
n − nTX
− 2β̂

Plugging all of the above bounds into Equation (21) we get
that w.p. ≥ 1 − ν/2 − α/2 it holds that

−1

n

(21)

and of course, both n and m are chosen independently of
^
T X and y .
X
Before we bound each term in Equation (21), we first give
−1
a bound on kZk. Recall, Z = I + N (X T X)−1
N.
Recall our assumption (given in the
statement
of
Theop
rem D.1) that σmin (X T X) ≥ ∆
p ln(1/ν). This imη
T
−1
plies that kN (X X) k ≤ kN k·σmin (X T X)−1 = O(η).
Hence
 √

∆ p ln(1/ν)
kZk ≤ (kI +N (X T X)−1 k)−1 ·kN k = O
1−η
Moreover, this implies that kZ(X T X)−1 k ≤ O


1
and that kI − Z(X T X)−1 k ≤ O 1−η
.



η
1−η



Armed with these bounds on the operator norms of Z and
(I − Z(X T X)−1 ) we bound the magnitude of the different
terms in Equation (21).
• The term y T (I − XX + ) y is the exact term from
the standard OLS, and we know it is distributed like
σ 2 · χ2n−p distribution. Therefore, it is greater than
p
√
σ 2 ( n − p − 2 ln(4/α))2 w.p. ≥ 1 − α/2.
• The scalar m sampled
from m ∼ N (0, ∆2 ) is
p
bounded by O(∆ ln(1/ν)) w.p. ≥ 1 − ν/8.
T

βk ≤ 
β Zβ̂
β is upper
• Since we assume kβ̂
B, the term β̂
√
2
B
∆
p
ln(1/ν)
.
bounded by B 2 kZk = O
1−η
T

β (I − Z(X T X)−1 )n
n. We thus have
• Denote z Tn = 2β̂
T
that z n ∼ N (0, ∆2 kzz k2 ) and that its magnitude is at

σ2 ≤




1√
√
n−p−2 ln(4/α)


g
ζ k2 + O (1 +
kζ

2
·

√
p
B 2 p+B
)∆ ln(1/ν)
1−η

^
TX
+ ∆ 2 kX

−1


kF · ln(p/ν)

and indeed, the RHS is the definition of ρ2 in the statement
of Theorem D.1.

E. Experiment: Additional Figures
To complete our discussion about the experiments we have
conducted, we attach here additional figures, plotting both
the t-value approximations we get from both algorithms,
and the “high-level decision” of whether correctly reject or
not-reject the null hypothesis (and with what sign). First,
we show the distribution of the t-value approximation for
coordinates that should be rejected, in Figure 2, and then
the decision of whether to reject or not based on this t-value
— and whether it was right, conservative (we didn’t reject
while we needed to) or wrong (we rejected with the wrong
sign, or rejected when we shouldn’t have rejected) in Figure 3. As one can see, Algorithm 1 has far lower t-values
(as expected) and therefore is much more conservative. In
fact, it tends to not-reject coordinate 1 of the real-data even
on the largest value of n (Figure 3c).
However, because Algorithm 1 also has much smaller
variance, it also does not reject when it ought to notreject, whereas Algorithm 2 erroneiously rejects the nullhypotheses. This can be seen in Figures 4 and 5.

Differentially Private Ordinary Least Squares

(a) Synthetic data, coordinate β1 = 0.5

(a) Synthetic data, coordinate β1 = 0.5

(b) Synthetic data, coordinate β2 = −0.25

(b) Synthetic data, coordinate β2 = −0.25

(c) real-life data, coordinate β1 = 14.07
Figure 2. The distribution of the t-value approximations from selected experiments on synthetic and real-life data where the null
hypothesis should be rejected

(c) real-life data, coordinate β1 = 14.07
Figure 3. The correctness of our decision to reject the nullhypothesis based on the approximated t-value where the null hypothesis should be rejected

Differentially Private Ordinary Least Squares

(a) Synthetic data, coordinate β3 = 0

(a) Synthetic data, coordinate β3 = 0

(b) real-life data, coordinate β2 = 0.57

(b) Synthetic data, coordinate β2 = 0.57

Figure 4. The distribution of the t-value approximations from selected experiments on synthetic and real-life data when the null
hypothesis is (essentially) true

Figure 5. The correctness of our decision to reject the nullhypothesis based on the approximated t-value when the null hypothesis is (essentially) true

