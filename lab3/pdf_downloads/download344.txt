Supplementary material for the paper: Co-clustering through Optimal
Transport

Charlotte Laclau 1 Ievgen Redko 2 Basarab Matei 1 YouneÌ€s Bennani 1 Vincent Brault 3

In this supplementary material, we present a couple of complementary elements which were omitted in the main paper.
We first introduce the Sinkhorn-Knopp algorithm, then we
explain how exactly the synthetic data sets were simulated.
Finally, we analyse the running time results obtained for
both CCOT and CCOT-GW on the generated data sets.

1. Sinkhorn-Knopp algorithm
For the sake of completeness, we first present the
Sinkhornâ€™s theorem and explain how it was used to derive
the solution of the regularized optimal transport.
Theorem ((Sinkhorn & Knopp, 1967)). If A is an n Ã— n
matrix with strictly positive elements, then there exist diagonal matrices D1 and D2 with strictly positive diagonal
elements such that D1 âˆ— A âˆ— D2 is doubly stochastic. The
matrices D1 and D2 are unique modulo multiplying the
first matrix by a positive number and dividing the second
one by the same number.
We can now cite the following result.
Lemma ((Cuturi, 2013), Lemma 2). For Î» > 0, the solution Î³Î»âˆ— is unique and has the form
Î³Î»âˆ— = diag(Î±)Î¾Î» diag(Î²),
where Î± and Î² are two non-negative vectors of Rd uniquely
defined up to a multiplicative factor and Î¾Î» = eâˆ’Î»M is the
element-wise exponential of âˆ’Î»M .
According to (Cuturi, 2013), the form of the solution presented in this Lemma has already been known in the optimal transportation theory (Erlander & Stewart, 1990).
Now since Î¾Î» = eâˆ’Î»M is strictly positive, Sinkhornâ€™s
1

CNRS, LIPN, UniversiteÌ Paris 13 - Sorbonne Paris CiteÌ,
France 2 CNRS UMR 5220 - INSERM U1206, Univ. Lyon 1,
INSA Lyon, F-69621 Villeurbanne, France 3 CNRS, LJK, Univ.
Grenoble-Alpes, France. Correspondence to: Charlotte Laclau
<charlotte.laclauc@univ-grenoble-alpes.fr>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).
0
The first author of this paper is now a post-doc in CNRS,
LIG, Univ. Grenoble-Alpes, France

theorem suggests that there exists a unique (up to rescaling) doubly-stochastic matrix that has the desired form
diag(Î±)Î¾Î» diag(Î²). Finally, this matrix can be found using the iterative procedure known as Sinkhorn-Knopp algorithm defined as follows:
Î² â† 1./Î¾Î»0 Î±.

Î± â† 1./Î¾Î» Î²,

2. Additional experimental results
In this Section, we describe the generative process used to
obtain the synthetic data sets. After that, we analyse the impact of the hyper-parameters on our methods and the running their running time results.
2.1. Simulation process
As mentioned in the paper, we simulate data following
the generative process of the Gaussian Latent Block Models. These models rely on the assumption that for each
block, the elements of the data matrix A = aij , i =
1, . . . , n; j = 1, . . . , d are distributed according to a Gaus2
2
sian distribution N (Âµk` , Ïƒk`
) with Âµk` âˆˆ R and Ïƒk`
âˆˆ R+ ,
k = 1, . . . , g; ` = 1, . . . , m, following a probability density function of this form
f (A; Î˜) =

X

Y

z

Ï€kik

(z,w)âˆˆZÃ—W i,k

Y

w

Y

Ï` j`

j,`

2
p(aij ; (Âµk` , Ïƒh`
))

i,j,k,`

where
1
2
exp âˆ’
p(aij ; (Âµk` , Ïƒk`
)) = p
2
2Ï€Ïƒk`



(xij âˆ’ Âµk` )2
2
2Ïƒk`

zik wj`
.

Therefore, this model is parametrised by Î˜ = (Ï€, Ï, Î´)
where Ï€ = (Ï€1 , . . . , Ï€m ), Ï = (Ï1 , . . . , Ïm ) and Î´ =
2
2
((Âµ11 , Ïƒ11
), . . . , (Âµgm , Ïƒgm
)).
Then, the simulation process is as follows
- Input: n, d, g, m, Î˜.
1. Simulate z according to a multinomial distribution with parameters (1, Ï€1 , . . . , Ï€g ).
2. Simulate w according to a multinomial distribution with parameters (1, Ï1 , . . . , Ïm )
3. Simulate each co-cluster Ak` according to Gaus2
sian density with (Âµk` , Ïƒk`
)

Co-clustering through Optimal Transport

- Output Data matrix A, partitions z and w.
For the sake of reproducibility, we also report the parameters used in order to generate D1, D2, D3 and D4 in Table 1.

accentuates the differences between the values of Î± (resp.
Î²). By doing so, small gaps, that correspond to overlapping
clusters, tend to merge leading to less accurate results. Regarding the regularization parameter, the same observation
is valid for CCOT-GW.

2.2. Visualisation of Î± and Î² for CCOT-GW
As the main paper only presents the visualization of Î± and
Î² for CCOT, we present the same result for the kernelized
version, CCOT-GW for D4 in Figure 1.

1.015

1.015
1.01

2.4. Running time complexity
The running time performance of our algorithms was evaluated on a cluster machine Intel(R) Xeon(R) CPU X2637 @
3.00GHz. We report the average running time (in seconds)
of both approaches for 100 trials obtained on the generated
data sets in Figure 3.

1.01

Î²

Î±

1.005
1.005

20

1

15

CCOT
CCOTâˆ’GW

0.995
0.99
0

100

200

0.995
0

300

100

Instances

200

300

Variables

Time (in sec)

1

10

5

Figure 1. Visualisation of Î± and Î² obtained with CCOT-GW on D4
which have a 5 Ã— 4 blocks structure and unbalanced clusters for
instances.

These vectors correctly reveal 4 and 3 significant jumps
corresponding to 5 and 4 clusters, respectively.
2.3. Impact of ns and Î»
The proposed algorithms require as input the number of
desired subsamples, ns (for CCOT only) and the value of
the regularization parameter, Î». From Figure 2, one can see
that for all four data sets the co-clustering error stabilizes
when ns reaches approximately 700.

0.2

0.2
D1
D2
D3

0.1
0.05
0

0.15
CCE

CCE

0.15

400

600
ns

800

1000

0.1

0
1

D1

D3

D3

D4

Figure 3. Mean running times expressed in seconds, over 100 trials, of CCOT and CCOT-GW for each synthetic data sets.

We observe that even though the theoretical complexity of
CCOT is more interesting, it suffers from the data sampling
that is required to cluster all data instances and features.
This limitation becomes less and less pronounced as the
number of data instances approaches the number of features. Indeed, on D4, where no sampling is required, CCOT
only takes approximately 0.06 seconds to accurately produce the resulting partitions, which is significantly faster
than CCOT-GW. For all other data configurations, the kernelized CCOT-GW algorithm is faster than CCOT.
2.5. Recommendation on M OVIE L ENS

D1
D2
D3
D4

0.05

200

0

2

3

4
Î»

5

6

7

Figure 2. On the left, the CCE as a function of the number of random samples ns and on the right as a function of the regularization parameter Î».

Since D3 has the most complicated block structure, the obtained results are also more sensible to the value of ns compared to other data sets and require a greater number of
samplings. Regarding Î», we observe very slight variations
of the CCE for D1 and D2. However, for D3 higher values
of this parameter impact negatively the performance of our
method. This can be explained by the fact that increasing Î»

The main task on M OVIE L ENS is to recommend movies to
users that might fit their interests. For a given co-clustering
structure, this can be done by recommending movies to
users based on the ratings provided by users who belong to
the same cluster. In order to evaluate the efficiency of our
approach for this task, we propose to use 90% of the available ratings for training purpose, and the remaining 10%
for testing. Since our goal is to predict if a user likes a given
movie or not without specifying the degree of the preference, we assume that a rating above 3 stands for movies
that were liked. In order to estimate the ratings in the testing phase, we calculate the mean of the block obtained during the training phase and attribute it to the missing values
picked for the testing. After 10-folds cross-validation, we
obtain that for 89% of the testing values, our approach is
able to correctly identify the taste of the users. This shows
its potential for recommendation systems application.

Co-clustering through Optimal Transport
Table 1. Value of the parameters used for the simulations.
Data

Ïƒ

Proportions

Âµ

Data

Proportions

ï£¶
4.0 0.5 1.5
Âµ = ï£­1.8 4.5 1.1ï£¸
1.5 1.5 5.5

D3


Ï€ = 0.5 0.5

Ï = 0.5 0.2 0.1 0.2

D4


Ï€ = 0.1 0.2 0.2 0.3 0.2
Ï = 0.25 0.25 0.25 0.25

ï£«

D1

Ï€=Ï=

1
3

1
3

1
3



Ïƒ = 0.1,

âˆ€k, `

ï£«

D2

Ï€ = Ï = 0.2

0.3

0.5



Ïƒ = 0.15

âˆ€k, `

ï£¶
4.0 0.5 1.5
Âµ = ï£­1.8 4.5 5.1ï£¸
3.5 1.5 5.5

References
Cuturi, Marco. Sinkhorn distances: Lightspeed computation of optimal transport. In Proceedings NIPS, pp.
2292â€“2300, 2013.
Erlander, Sven and Stewart, Neil F. The Gravity model in
transportation analysis : theory and extensions. Topics
in transportation. VSP, Utrecht, The Netherlands, 1990.
Sinkhorn, Richard and Knopp, Paul. Concerning nonnegative matrices and doubly stochastic matrices. Pacific
Journal of Mathematics, 21:343â€“348, 1967.

Ïƒ

Âµ


Ïƒ = 0.2

âˆ€k, `

Ïƒ = 0.15 âˆ€k, `

Âµ=


4.0 0.5 7.5 0.5
0.5 3.5 7.8 0.5

ï£«
ï£¶
1.5 1.5 1.5 1.5
ï£¬2.5 1.5 1.5 1.5ï£·
ï£¬
ï£·
Âµ=ï£­
2.6 2.6 1.5 1.5ï£¸
2.4 2.5 2.6 2.5

