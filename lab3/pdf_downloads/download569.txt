Safety-Aware Algorithms for Adversarial Contextual Bandit

Appendix
A. Proof of Proposition 2.1
Proof. The proof is mainly about adapting the specific two-player game presented in (Mannor et al., 2009) to the general
online convex programming setting with adversarial constraints. We closely follow the notations in the example from
Proposition 4 in (Mannor et al., 2009).
Let us define the decision set X = ([1, 2]), namely a 2-D simplex. We design two different loss functions: `1 (x) =
[ 1, 0]x, and `2 (x) = [ 1, 1]x (here [a, b] stands for a 2-d row vector and hence [a, b]x stands for the regular vector inner
product). We also design two different constraints as: f 1 (x) = [ 1, 1]x  0 and f 2 (x) = [1, 1]x  0. Note that both
` and f are linear functions with respect x, hence they are convex loss functions and constraints with respect to x. The
adversary picks loss functions among {`1 , `2 } and constraints among {f 1 , f 2 } and will generate the following sequence of
loss functions and constraints. Initialize a counter k = 0, then:
1. while k = 0 or

1
t 1

Pt

1
i=1

xi [1] > 3/4, the adversary set `t = `2 (x) and ft = f 2 (x), and set k := k + 1.

2. For next k steps, the adversary set `t = `1 (x) and ft = f 1 (x). Then reset k = 0 and go back to step 1.
Pt
For any time step t, let us define q̂t = 1t i=1 (fi = f 2 ), namely the fraction of the adversary picking the second type of
Pt
constraint. Let us define ↵
ˆ t = i=1 xi [1]/t. Given any q̂t , we see that O0 can be defined as
O0 = {x 2
= {x 2

([1, 2]) : q̂t [1, 1]x + (1

q̂t )[ 1, 1]x  0)}

1, 1]x  0} = {x 2

([1, 2]) : [2q̂t

([1, 2]) : 2q̂t x[1]

1  0},

(17)

and the minimum loss the learner can get in hindsight with decisions restricted to O0 is:
rtmin = min0 (1 q̂t )[ 1, 0]x + q̂t [ 1, 1]x
x2O
(
1
0  q̂t  1/2
=
1/2 1/(2q̂t ) + q̂t 1/2  q̂t  1

(18)

Pt
Pt
The cumulative constraint violation at time step t can be computed as i=1 fi (xi ) = i=1 (fi = f 1 )[ P
1, 1]xi + (fi =
f 2 )[1, 1]xi . We want to show that no matter what strategy the learner uses, as long as 1t lim supi!1 i fi (xi )  0, we
Pt
will have lim supt!1 ( i=1 `i (xi )/t) rtmin > 0.
Following a similar argument from (Mannor et al., 2009), we can show that Step 2 is entered an infinite number of times. To
show this, assume that step 2 only enters finite number of times. Hence as the game keeps staying in Step 1, the fraction of
the adversary picking the second constraint f 2 approaches to one (q̂t ! 1), we will have as t approaches to infinity,
t

t

t

1X
1X
1X
fi (xi ) = lim
(fi = f 1 )[ 1, 1]xi +
(fi = f 2 )[1, 1]xi
t!1 t
t!1 t
t
i=1
i=1
i=1
lim

t

t

t

1X
1X
1X
(fi = f 2 )[1, 1]xi = lim
[1, 1]xi = lim [1, 1](
xi ).
t!1 t
t!1 t
t!1
t i=1
i=1
i=1

= lim

(19)

Pt
Pt
Since i=1 xi /t 2 ([1, 2]), we must have ↵
ˆ t = i=1 xi [1]/t <= 1/2 to ensure that the long-term constraint is satisfied:
Pt
limt!1 1t i=1 fi (xi )  0. But when ↵
ˆ t  1/2, the condition of entering Step 1 is violated and we must enter step 2.
Hence step 2 is entered infinite number of times. In particular, there exist infinite sequences ti and t0i such that ti < t0i < tt+1 ,
and the adversary picks f 2 , `2 in (ti , t0i ] (Step 1) and the adversary picks f 1 , `1 in (t0i , ti+1 ] (Step 2). Since step 1 and step 2
executes the same number of steps (i.e., using the counter k’s value), we must have q̂ti = 1/2 and rtmin
= 1. Furthermore,
i
we must have t0i
tt+1 /2. Note that ↵
ˆ t0i  3/4 since otherwise the adversary would be in step 1 at time t0i + 1. Thus,
during the first ti+1 steps, we must have:
0

ti+1

X
j=1

xj [1] =

ti
X
j=1

ti+1

xj [1] +

X

j=t0i +1

xj [1] 

3
ti0 + (ti+1
4

t0i ) = ti+1

t0i /4 

7
ti+1 .
8

(20)

Safety-Aware Algorithms for Adversarial Contextual Bandit

It is easy to verify that

1
ti+1

Pti+1
t=1

1
ti+1

`t (xt )

t

lim sup(
t!1

Pti+1
t=1

7
8.

xt [1]

1X
`i (xi )
t i=1

rtmin )

Hence, simply let i ! 1, we have:
(21)

7/8 + 1 = 1/8.

Namely, we have shown that for cumulativeP
regret, regardless what sequence of decisions x1 , ..., xt the learner has played,
t
as long as it needs to satisfy lim supt!1 1t i=1 fi (xi )  0, we must have:
t
X

lim sup
t!1

`i (xi )

min0

x2O

i=1

t
X

`i (x)

(22)

t/8 = ⌦(t).

i=1

Hence we cannot guarantee to achieve no-regret when competing agains the decisions in O0 while satisfying the long-term
constraint.

B. Analysis of Alg. 1 and Proof Of Theorem 3.1
Proof of Theorem 3.1. Since the algorithm runs online mirror descent on the sequence of loss {Lt (x, t )}t with respect to
x, using the existing results of online mirror descent (Theorem 4.2 and Eq. 4.10 from (Bubeck, 2015)), we know that for the
sequence of {xt }t :
T
X

T

(Lt (xt ,

Lt (x,

t)

t=1

t ))



DR (x, x1 )
µ X
+
krx L(xt ,
µ
2↵ t=1

2
t )k⇤ .

(23)

Also, we know that the algorithm runs online gradient ascent on the sequence of loss {Lt (xt , )}t with respect to , using
the existing analysis of online gradient descent (Zinkevich, 2003), we have for the sequence of t :
T
X
t=1

Note that for (r Lt (xt ,
we also have:

t ))

2

Lt (xt , )

t=1

Lt (xt ,

t)



T

1
µ

2

+

µ t )2  2ft2 (xt )+2 2 µ2

= (ft (xt )

krx Lt (xt ,

T
X

2
t )k⇤

µ X @Lt (wt ,
2 t=1
@ t
2
t

 2D2 +

t) 2

2 2 2
µ t.

 2kr`t (xt )k2⇤ + 2k t rft (xt )k2⇤  2G2 (1 +

(24)

,

Similarly for krx Lt (xt ,
2
t ),

2
t )k⇤ ,

(25)

where we first used triangle inequality for krx Lt (xt , t )k⇤ and then use the inequality of 2ab  a2 + b2 , 8a, b 2 R+ .
We also assume that the norm of the gradients are bounded as max(kr`t (xt )k⇤ , krft (xt )k⇤ )  G 2 R+ . Now sum
Inequality 23 and 24 from t = 1 to T , we get:
X
Lt (xt , ) Lt (x, t )
t

2DR (x, x0 ) +
2µ

2

2DR (x, x0 ) +
=
2µ

2



+

X

µ(D2 +

2 2 2
µ t)

+

t

X µG2
t

↵

(1 +

G2
G2 X
+ T µ(D2 +
) + µ( 2 µ2 +
)
↵
↵

Using the saddle-point convex and concave formation for Lt , we have:
X
t

Lt (xt , )

2B +

2µ

2

Lt (x,

t)

=

X
t

(`t (xt )

`t (x)) +

X

G2
G2 X
+ T µ(D2 +
) + µ( 2 µ2 +
)
↵
↵

( ft (xt )

t

2
t.

t ft (x))

+

2
t)

2
t.

µX
2

(26)

2
t

µT
2

2

(27)

Safety-Aware Algorithms for Adversarial Contextual Bandit
2 2
Note that based on the setting of and µ, we can show that
µ + G2 /↵. This is because 2 µ2 + G2 /↵ =
4G4 B
4G2 B
2
2
2
4B.
↵2 T (D 2 +G2 /↵) + G /↵  T ↵ + G /↵  2G /↵, where we assume that T is large enough such that T
P
2 2
Since we have
µ + G2 /↵, we can remove the term t 2t in the above inequality.

X

(`t (xt )

`t (x)) +

t

X

( ft (xt )

t ft (x))

(

t

Now set x = x⇤ , and set
X

µT
1
+
)
2
2µ

2



2B
+ T µ(D2 + G2 /↵).
2µ

= 0, since ft (x⇤ )  0 for all t, we get:
(`t (xt )

t

`t (x⇤ )) 

p
2B
+ T µ(D2 + G2 /↵)  2 BT (D2 + G2 /↵),
2µ

p
where we set µ = B/(T (D2 + G2 /↵)).
P
PT
PT
To upper bound t ft (xP
t ), we first note that we can lower bound
t=1 (`t (xt ) P`t (x)) as
t=1 (`t (xt ) `t (x))
Now let us assume that t ft (xt ) > 0 (otherwise we are done). We set = ( t ft (xt ))/( µT + 1/µ), we have:
P
X
( t ft (xt ))2
2B

+ T µ(D2 + G2 /↵) +
(`t (x⇤ ) `t (xt ))
2 µT + 1/µ
2µ
t
p
2
2
 2 BT (D + G /↵) + 2F T
Substitute µ =

p

(28)

(29)

2F T .

(30)

B/(T (D2 + G2 /↵)) into the above inequality, we have:
(

T
X
t=1

p
ft (xt ))2  2 BT (D2 + G2 /↵)(2 µT + 1/µ) + 2F T (2 µT + 1/µ)

p
8G2
D2
G2
BT + 2T (D2 +
) + 2T (D2 +
) + T 3/2 8F 2 G2 /↵.
(31)
↵
↵
↵
p
Take the square root on both sides of the above inequality and observe that T 3/2 8F 2 G2 /↵ dominates the RHS of the
above inequality, we prove the theorem.


C. Analysis of EXP4.R
In this section we provide the full proof of theorem 4.2.
Proof of Theorem 4.2. We first present several known facts. First we have that for wtT ẑt :
wtT ẑt = Ei⇠wt ẑt [i] = Ei⇠wt ⇡i (st )T r̂t = Ei⇠wt Ej⇠⇡i (st ) r̂t [j] = Ej⇠pt r̂t [j] = rt [at ]  1.

(32)

For wtT ŷt , we have:
wtT ŷt = Ei⇠wt ŷt [i] = Ei⇠wt ⇡i (st )T ĉt = Ej⇠pt ĉt [j] = ct [at ]  1.
For Eat ⇠pt (wtT ẑt

(33)

)2 , we then have:
Eat ⇠pt (wtT ẑt

)2 = Eat ⇠pt (rt [at ]

)2  Eat 2rt [at ]2 + 2

2

 4.

(34)

For Eat ⇠pt ŷt , we have:
Eat ⇠pt ŷt [j] = ⇡j (st )T Eat ⇠pt ĉt = ⇡j (st )T ct = yt [j],

(35)

Note that here for analysis simplicity we consider asymptotic property of the algorithm and assume T is large enough and
particularly larger than any constant. We don’t necessarily have to assume T
4B here because we can explicitly solve the inequality
2 2
µ + G2 /↵ to find the valid range of , as (Mahdavi et al., 2012) did.

Safety-Aware Algorithms for Adversarial Contextual Bandit

which gives us Eat ⇠pt ŷt = yt . Similarly we can easily verify that Eat ⇠pt ẑt = zt .
P|⇧|
For i=1 wt [i]ŷt [i]2 , we have:
|⇧|
X

wt [i]ŷt [i]2 = Ei⇠wt ŷt [i]2 = Ei⇠wt (⇡j (st )T ĉt )2 = Ei⇠wt (Ej⇠⇡i (st ) ĉt [j])2

i=1

 Ei⇠wt Ej⇠⇡i (st ) (ĉt [j])2 = Ej⇠pt (ĉt [j])2 =
Hence, for Eat ⇠pt

Similarly, for

P|⇧|

P|⇧|

i=1

i=1

ct [at ]2
.
pt [at ]

(36)

wt [i]ŷt [i]2 we have:
Eat ⇠pt

|⇧|
X
i=1

K

X
ct [at ]2
=
ct [k]2  K.
pt [at ]

wt [i]ŷt [i]2  Eat ⇠pt

(37)

k=0

wt [i]ẑt [i]2 , we have:
|⇧|
X
i=1

and

wt [i]ẑt [i]2 = Ei⇠wt (⇡i (st )T r̂t )2  Ej⇠pt (r̂t [j])2 =

Eat ⇠pt

|⇧|
X
i=1

rt [at ]2
,
pt [at ]

(38)

wt [i]ẑt [i]2  K.

(39)

Now we are going to take expectation with respect to the randomized decisions {ai } on both sides of Inequality. 11. Fix
time step t, conditioned on a1 , ..., at 1 , we have:
h
i
Eat Lt (wt , ) Lt (w, t )
h
µ 2
µ 2i
T
= Eat ct [at ] + (rt [at ]
)
ŷtT w
)+
t (ẑt w
2
2 t
µ 2
µ 2
T
= Eat ct [at ] + (Eat rt [at ]
)
ytT w
)+
.
t (zt w
2
2 t
(Used fact that Eat ⇠pt ŷt = yt and Eat ⇠pt ẑt = zt )
Take the expectation with respect to a1 , ..., aT on the LHS of Inequality 11, we have:
E{at }t
=

T h
X
t=1

T
X
⇥

Lt (wt , )

Lt (w,

Ect [at ] + (Ert [at ]

t)

)

i

=

T
X
t=1

Ea1 ,...,at 1 Eat |a1 ,...,at

ytT w

T
t (zt w

)+

t=1

µ
2

1

2
t

h

⇤

Lt (wt , )
µT
2

Now take the expectation with respect to a1 , ..., aT on the RHS of Inequality 11 (we use Eat |
over the distribution of at conditioned on a1 , ..., at 1 ), we have:
|⇧|
T ⇣
X
X
ln(|⇧|)
+
+µ
Eat |a t (
wt [i]ŷt [i]2 +
µ
µ
t=1
i=1
2

2



µ

+

T ⇣
X
ln(|⇧|)
+µ
K+
µ
t=1

2
tK

+4+

2
2
t wt [i]ẑt [i] )

2 2 2
µ t

(Used Eq. 37 and 39 )
2

=

µ

+

ln(|⇧|)
+ µT (K + 4) + µ(K +
µ

2 2

µ )

⌘

T
X
t=1

2
t.

+ Eat |a t (wtT ẑt

Lt (w,

t)

i

2

at

(40)
to represent the expectation

)2 +

2 2 2
µ t

⌘

(41)

Safety-Aware Algorithms for Adversarial Contextual Bandit

Note that based on the setting of and µ, we can show that
2K + 2 2 µ2 . This is because 2K + 2 2 µ2 = 2K +
2
18K ln(|⇧|)/(T (K + 4))  2K + 18K ln(|⇧|)/T  3K, where for simplicity we assume that T is large enough
(T 18 ln(|⇧|)).
Chain Eq. 40 and 41 together and get rid of the terms that have
terms, we get:
E

T
X

T
X

ct [at ]

t=1

ytT w +

t=1

T
X

(Ert [at ]

t

(due to the fact that

T
t (zt w

)

)

(

t=1

ln(|⇧|)

+ µT (K + 4).
µ

2K + 2 2 µ2 ) and rearrange

µT
1
+ )
2
µ

2

(42)

The above inequality holds for any w. Substitute w⇤ into Eq. 42, we get:
E

T
X

T
X

ct [at ]

t=1

ytT w⇤ +

t=1

)

(

µT
1
+ )
2
µ

2

= 0, for regret, we get:
E

T
X

ct [at ]

t=1

where µ =

(Ert [at ]

t=1

ln(|⇧|)

+ µT (K + 4).
µ
Now let us set

T
X

T
X
t=1

ytT w⇤  ln(|⇧|)/µ + µT (K + 4)

p
p
 2 ln(|⇧|)T (K + 4) = O( T K ln(|⇧|)),

p

(43)

ln(|⇧|)/T (K + 4).
P
P
For constraints (Ert [at ]
), let us assume that
E(rt [at ]
) > 0 (otherwise we are done), and substitute =
P
PT
PT
T ⇤
( Ert [at ]
)/( µT + 2/µ) into inequality 43 (note that > 0). Using the fact that E
t=1 ct [at ]
t=1 yt w
2T , we get:
(

T
X

))2  (2 µT + 4/µ) 2T + 2

(Ert [at ]

t=1

Substitute µ =
(

p

T
X
t=1

ln(|⇧|)/T (K + 4) and

(Ert [at ]

2

))  12K

p
ln(|⇧|)T (K + 2 + 2

2)

(44)

= 3K back to the above equation, it is easy to verity that:
r

ln(|⇧|) 3/2
T
+ 12K ln(|⇧|)T + 8T 3/2
K +4

s

K +4
+ 8T (K + 4).
ln(|⇧|)

(45)

Since
the asymptotic property when T ! 1, we can see that the LHS of the above inequality is dominated by
p we consider
K ln(|⇧|)T 3/2 . Hence,
(

T
X

(Ert [at ]

t=1

))2  O(

p
K ln(|⇧|)T 3/2 ).

Take the square root on both sides of the above inequality, we prove the theorem.

D. Algorithm and Analysis of EXP4.P.R
D.1. Algorithm
We present the EXP4.P.R algorithm in Alg. 3.

(46)

Safety-Aware Algorithms for Adversarial Contextual Bandit

Algorithm 3 Exp4.P with Risk Constraints (EXP4.P.R)
1: Input: Policy Set ⇧
2: Initialize w1 = [1/N, ..., 1/N ]T and 1 = 0.
3: for t = 1 to T do
4:
Receive context st .
5:
Query each experts to get the sequence of advice {⇡i (st )}N
i=1 .
PN
6:
Set pt = i=1 wt [i]⇡i (st ).
7:
Draw action at randomly according to probability pt .
8:
Receive cost ct [at ] and risk rt [at ].
9:
Set the cost vector ĉt 2 RK and the risk vector r̂t 2 RK as:

ĉt [i] =
10:

ct [i] (at = i)
,
pt [i]

rt [i] (at = i)
, 8i 2 {1, 2, ..., K}.
pt [i]

ẑt [j] = ⇡j (st )T r̂t , 8j 2 {1, 2..., N }.

Set x̃t = ŷt + t ẑt .
Update wt+1 as:
wt [i] exp( µ(x̃t [i]

wt+1 [i] = P|⇧|

j=1

13:

(47)

For each expert j, set:
ŷt [j] = ⇡j (st )T ĉt ,

11:
12:

r̂t [i] =

Update

t+1

as:

t+1

14: end for



wt [j] exp( µ(x̃t [j]

= max{0,

t

+ µ(wtT ẑt

PK

⇡i (st )[k]
k=1 pt [k] ))
PK ⇡ (st )[k] ,
 k=1 jpt [k]
))

µ t )}.

(48)

Safety-Aware Algorithms for Adversarial Contextual Bandit

D.2. Analysis of EXP4.P.R
We give detailed regret analysis of EXP4.P.R in this section. Let us define x̂t ( ) as x̂t ( )[i] = ŷt [i] + ẑt [i]
PK
µ 2
t )[k]
 k=1 ⇡ip(st [k]
, 8i 2 [N ] and Lt (w, ) = wT x̂t
. As we can see that Line 3 is essentially running
2
Weighted Majority algorithm on the sequence of functions {Lt (w, t )}t while Line 3 is running Online Gradient Ascent on
the sequence of functions {Lt (wt , )}t . Applying the classic analysis of Weighted Majority and analysis of Online Gradient
Descent, we can show that:
Lemma D.1. The sequences {wt }t and { t }t generated from Lines 3 and 3 in EXP4.P.R has the following property:
T
X
t=1

2



T
X

Lt (wt , )

µ

+

t=1

Lt (w,

t)
|⇧|

ln(|⇧|) µ X ⇣ X
+
wt [i](x̂t ( t )[i])2 + 2(wtT ẑt
µ
2 t=1 i=1
T

) 2 + 2 2 µ2

2
t

⌘

.

Proof. Using the classic analysis of Weighted Majority algorithm, we can get that for the sequence of loss {Lt (w,
T
X
t=1

Lt (wt ,

t)

T
X
t=1

Lt (w,

t)

(49)
t )}t :

T |⇧|
ln(|⇧|) 1 X X
2
+ µ
wt [i] x̂t ( t )[i] ,
µ
2 t=1 i=1



for any w 2 B. On the other hand, we know that we compute t by running Online Gradient Descent on the loss functions
{Lt (wt , )}t . Applying the classic analysis of Online Gradient Descent, we can get:
T
X
t=1

for any

T
X

Lt (wt , )

t=1

Lt (wt ,

t) 

T

1
µ

2

+

µ X @Lt (wt ,
2 t=1
@ t

t) 2

,

0.

We know that @Lt (wt , )/@ t = wtT ẑt
µ t . Substitute these gradient and derivatives back to the above two
inequalities, and then sum the above two inequality together we get:
T
X
t=1

2



µ

+

µ

t=1

Lt (w,

+

t)
|⇧|

ln(|⇧|) µ X ⇣ X
+
wt [i](x̂t ( t )[i])2 + (wtT ẑt
µ
2 t=1 i=1
T

|⇧|

ln(|⇧|) µ X ⇣ X
+
wt [i](xt ( t )[i])2 + 2(wtT ẑt
µ
2 t=1 i=1
T

2



T
X

Lt (wt , )

µ t )2

⌘

) 2 + 2 2 µ2

2
t

⌘

,

where in the last ineqaulity we use the fact that (a + b)2  2a2 + 2b2 , for any a, b 2 R.
We first show that the Lagrangian dual parameter
Lemma D.2. Assume that

t

can be upper bounded:

 1/µ . For any t 2 [T ], we have
2

Proof. Remember that the update rule for
t+1

t

t



| |
µ.

is defined as:

= max{0,

t

+ µ(wtT ẑt

µ t )}.

(50)

We prove the lemma by induction. For t = 0, since we set 0 = 0, we have 0  (| |/( µ). Now let us consider time step t
and assume that that t  (| |)/( µ) for ⌧  t. Note that wtT ẑt = rt [at ]  0 and from the update rule of , we have:
t+1

 max{0,

t

+ µ(| |

µ t )}

(51)

Safety-Aware Algorithms for Adversarial Contextual Bandit

For the case when
| |/( µ).
For the case when
must have:

t

t

= 0, we have

t+1

= µ| |. Since we assume that

0, since we see that

t

+ µ(| |
t+1

0 from the induction hypothesis that

µ t)

=

t

 1/µ2 , we can easily verify that

+ µ(| |

t

t+1

 µ| | 

 | |/( µ), we
(52)

µ t ).

Subtract | |/µ on both sides of the above inequality, we get:
| |
= (1
µ

t+1

Since we have

t

 | |/( µ) and

µ2 )

| |
µ

t

(53)

 1/µ2 , it is easy to see that we have for

t+1 :

| |
 0.
µ

t+1

(54)

Hence we prove the lemma.
For notation simplicity, let us denote | µ| as m .
P
PK
We now show how to relate t ŷ[i] + t ẑ[i]  j=1

⇡i (st )[j]
pt [j]

to

Lemma D.3. In EXP4.P.R (Alg. 3), with probability at least 1
|⇧|
T X
X

w[i](ŷt [i] +

t ẑt [i]



t=1 i=1



(w[i](yt [i] +

t

yt [i] +

t z[i]

, for any w 2

K
X
⇡i (st )[j]
j=1

|⇧|
T X
X

P

pt j]

t zt [i])

for any i 2 [|⇧|]:

⇧, we have:

)

+ (1 +

m)

t=1 i=1

ln(|⇧|/ )
.


We use similar proof strategy as shown in the proof of Lemma 3.1 in (Bubeck et al., 2012) with three additional steps:(1)
union bound over all polices in ⇧, (2) introduction of a distribution w 2 (⇧), (3) taking care of t by using its upper
bound from Lemma D.2.
Proof. Let us set 0 = /|⇧| and fix i 2 [|⇧|]. Define x̃t ( t ) = ŷt +
PK
 j=1 (⇡i (st )[j]/pt [j]).

t ẑt

and we denote x̂t ( t )[i] = x̃t ( t )[i]

For notation simplicity, we are going to use x̃t and x̂t to represent x̃t ( t )[i]/(1 +
in the rest of the proof.

m)

and x̂t ( t )[i]/(1 +

m)

respectively

Let us also define xt = (yt [i] + t zt [i])/(1 + m ). It is also straightforward to check that (x̂t xt )  1 since x̂t  0,
xt  1 and 0 <   1. Note that it is straightforward to show that Et (x̃t ) = xt , where we denote Et as the expectation
conditioned on randomness from a1 , ..., at 1 .
Following the same strategy in the proof of Lemma 3.1 in (Bubeck et al., 2012), we can show that:
⇥

Et exp((x̂t
 (1 + Et (x̃t

⇤

⇥

xt )) = Et exp((x̃t

(⇡i (st )[j]/pt [j])

xt )

j=1

xt ) + 2 Et (x̃t

 (1 + 2 Et (x̃2t )) exp( 2



K
X

xt )2 ) exp( 2

K
X
⇡i (st )[j]
j=1

K
X
⇡i (st )[j]
j=1

pt [j]

)

pt [j]

⇤

)
(55)

Safety-Aware Algorithms for Adversarial Contextual Bandit

We can upper bound Et (x̃2t ) as follows:
Et (x̃2t ) = Et

K
h⇣ X

 Et,j⇠⇡i (st )

⇣

⇡i (st )[j]

j=1

ct [j] (at = j)
+
pt [j]

ĉ[j]/pt [j] +

= Ej⇠⇡i (st ) ((ct [j] +

t r̂t [j]/pt [j]

K
X

t

⇡i (st )[j]

j=1

/(1 +

m)

rt [j] (at = j)
/(1 +
pt [j]

⌘2

2
m )) /pt [j]  Ej⇠⇡t (st ) (1/pt (j)) =

t rt [j])/(1 +

m)

⌘2 i

K
X
⇡i (st )[j]

pt [j]

j=1

(56)

where the first inequality comes from Jensen’s inequality and the last inequality comes from the fact that |ct [j]|  1 and
| t rt [j]|  m . Substitute the above results in Eq. 55, we get:
K
K
X
X
⇤
⇡i (st )[j]
⇡i (st )[j]
xt ))  (1 + 2
) exp( 2
)
pt [j]
pt [j]
j=1
j=1

⇥
Et exp((x̂t
 exp(2

K
X
⇡i (st )[j]

pt [j]

j=1

) exp( 2

K
X
⇡i (st )[j]

pt [j]

j=1

(57)

)  1.

Hence, we have:
E exp(

X

Now from Markov inequality we know P (X

(58)

xt ))  1.

(x̂t

t=1

ln( 1 ))  E(eX ). Hence, this gives us that with probability least 1
:
X

(x̂t xt )  ln(1/ ).
(59)
t

Substitute the representation of x̂t , xt in, we get for i, with probability 1
T
X

ŷt [i] +

t ẑt [i]



t=1

K
X
j=1

(⇡i (st )[j]/pt [j]) 

T
X

0

yt [i] +

:

t zt [i]

+ (1 +

m)

t=1

ln(1/ 0 )
.


Now apply union bound over all policies in ⇧, it is straightforward to show that for any i 2 |⇧|, with probability at least
1
, we have:
T
X

ŷt [i] +

t ẑt [i]

t=1



K
X
j=1

(⇡i (st )[j]/pt [j]) 

To prove the lemma, now let us fix any w 2
and then sum over from i = 1 to |⇧|.
Let us define ŵ 2

yt [i] +

t zt [i]

+ (1 +

m)

t=1

ln(|⇧|/ )
.


(|⇧|), we can simply multiple w[i] on the both sides of the above inequality,

(⇧) as:
ŵ = arg min

w2 (⇧)

and ŵ⇤ 2

T
X

|⇧|
T X
X

w[i](ŷ[i] +

t ẑ[i]



t=1 i=1

K
X
⇡i (st )[j]
j=1

pt [j]

),

(60)

(⇧) as:
ŵ⇤ = arg min

w2 (⇧)

Now we turn to prove Theorem 4.3.

|⇧|
T X
X
t=1 i=1

w[i](y[i] +

t z[i])

(61)

Safety-Aware Algorithms for Adversarial Contextual Bandit

Proof
q of Theorem 4.3. We prove the asymptotic property of Alg. 3 when T approaches to infinity. Since we set µ =
ln(|⇧|)
= T ✏+1/2 K, we can first verify the condition  1/µ2 in Lemma D.2. This condition holds since
(3K+4)T and
= O(T 0.5 ) while 1/µ2 = ⇥(T ).

Let us first compute some facts. For wtT x̂t , we have:
wtT x̂t ( t ) = Ej⇠wt (ŷt [j] +

t ẑt [j]



K
X
⇡j (st )[i]

pt [i]

i=1

= ct [at ] +
For

P|⇧|

i=1

t rt [at ]

K.

) = Ej⇠pt ĉt [j] +

t Ej⇠pt r̂t [j]

Ej⇠pt

1
pt [j]
(62)

wt [i](x̂t ( t )[i])2 , we have:
|⇧|
X

wt [i](x̂t ( t )[i])2 = Ei⇠wt (x̂t ( t )[i])2 = Ei⇠wt (ŷt [i] +

t ẑt (i)

k

i=1

j=1

 Ei⇠wt ,j⇠⇡i (st ) (ĉt [j] +

=

K
X

pt [i]

(ct [i] (at = i) +

K
X
(ct [i] (at = i) +
K
X

( 1

t

/pt [j]) = Ej⇠pt ĉt [j] +

t rt [i]
pt [i]2

t rt [i]

t r̂t [j]

pt [j]

/pt [j]

)2

2

)2

(at = i)
)2

(at = i)

pt [i]

i=1



2

t r̂t [j]

i=1

=

K
X
⇡i (st )[j]

)(ĉt [i] +

t r̂t [i]

/pt [i])

i=1

= K( 1

t

)

K
X

((1/K)ĉt [i] +

t (1/K)r̂t [i]



i=1

 K( 1

t

|⇧|
X

)

ŵ[i](ŷt [i] +

t ẑt [i]

i=1



1/K
)
pt [i]

K
X
⇡i (st )[j]
j=1

pt [j]

(63)

) ,

where the first inequality comes from Jesen’s inequality and the last inequality uses the assumption that the ⇧ contains the
uniform policy (i.e., the policy that assign probability 1/K to each action). Consider the RHS of Eq. 49, we have:
T

µ

+

+

µ

+

µ

ln(|⇧|) µ X
+
K( 1
µ
2 t=1

t

ln(|⇧|) µ X
+
K( 1
µ
2 t=1

t

)

|⇧|
⇣X

ŵ[i] ŷt [i] +

µ t )2

t ẑt [i]



i=1

T

2

=

T

T

2



|⇧|

ln(|⇧|) µ X X
µX T
+
wt [i](x̂t ( t )[i])2 +
(w ẑt
µ
2 t=1 i=1
2 t=1 t

2

)

|⇧|

⇣X

K
X
⇡i (st )[j] ⌘
j=1

ŵ[i] ŷt [i] +

t ẑt [i]

i=1



pt [j]

K
X
⇡i (st )[j] ⌘
j=1

pt [j]

+µ

T
X

((wtT ẑt

)2 +

2 2 2
µ t)

((rt [at ]

)2 +

2 2 2
µ t)

t=1

+µ

T
X
t=1

(64)

Consider the LHS of Eq. 49, set w = ŵ, we have:
T h
X
t=1

=

Lt (wt , )

T h
X
t=1

Lt (ŵ,

ct [at ] + rt [at ]

t)

i

K

µ

2

/2

|⇧|
⇣X
i=1

ŵ[i] ŷt [i] +

t ẑt [i]



K
X
⇡i (st )[j] ⌘
j=1

pt [j]

+

t

+ µ

i

2
t /2

.

(65)

Safety-Aware Algorithms for Adversarial Contextual Bandit

Chaining Eq. 64 and Eq. 65 together and rearrange terms, we will get:
T h
X

ct [at ] + (rt [at ]

)+

i

2
t /2

+ µ

t

t=1

2

 T K +
+µ

T
X

T

+ ln(|⇧|) X
+
(1
µ
t=1

(2 + 2

2

+

2

T µ

µK
(1 +
2

t

+ ))

/2

|⇧|
⇣X

ŵ[i] ŷt [i] +

t ẑt [i]



i=1

K
X
⇡i (st )[j] ⌘

pt [j]

j=1

2 2 2
µ t ).

(66)

t=1

Since we have

| |
2/K µ µ ,

µK
2 (1

we can show that 1

+

t

+ )

Now back to Eq. 66, using Lemma. D.3, we have with probability 1
T h
X

ct [at ] + (rt [at ]

)+

+ µ

t

i

2
t /2

t=1

2

 T K +
+ (1 +

T

+ ln(|⇧|) X
+
(1
µ
t=1

m)
2

 T K +

T
X

+ ln(|⇧|)
+
(1
µ
t=1

2

2

t + ))

/2

|⇧|
⇣X

ŵ⇤ [i](yt [i] +

t zt [i])

⌘

w⇤ [i](yt [i] +

t zt [i])

⌘

i=1

)T µ + µ3

2

X

2
t

t

µK
(1 +
2

ln(|⇧|/⌫)
+ (1 + m )
+ (2 + 2


⌫:
T µ

µK
(1 +
2

ln(|⇧|/⌫)
+ (2 + 2


0.

2

t + ))

|⇧|
⇣X
i=1

)T µ + µ3

2

X

2
t.

(67)

t

where the last inequality follows from the definition of ŵ⇤ and w⇤ . Rearrange terms, we get:
T h
X

w⇤ T yt ) + (rt [at ]

(ct [at ]

)

t (w

⇤T

zt

)

t=1

2

 T K +
2

 T K +
2

= T K +

T
X
µK

+ ln(|⇧|)
+
(1 +
µ
2
t=1

t

+ )(1 +

⇤

t)

T µ

2

/2 +

+ (1 +

t

+ ) + (1 +

t

+ ) + (1 +

T

+ ln(|⇧|) X µK
+
(1 + (2 + )
µ
2
t=1

µ

2
t /2

t=1

m)

T

+ ln(|⇧|) X µK
+
(1 + (2 + )
µ
2
t=1

T
X

ln(|⇧|/⌫)
+ (2 + 2


2

2

X

2
t

t

ln(|⇧|/⌫)
+ (2 + 2


2

| | ln(|⇧|/⌫)
)
+ (2 + 2
µ


2

m)

)T µ + µ3

)T µ + (

X
Kµ
+ µ3 2 )
2
t

)T µ + (

X
Kµ
+ µ3 2 )
2
t

2
t

2
t.

(68)

Kµ
3 2
Note that under the setting of and µ we have 2µ
(we will verify it at the end of the proof), we can drop
2 +µ
p
2
the terms that relates to t in the above inequality. Note that we have µ = T ✏ K ln(|⇧|) T ✏ , where ✏ 2 (0, 1/2).
Substitute µ T ✏ into the above inequality and rearrange terms, we get:
T
X

ct [at ]

w⇤ T yt + (rt [at ]

)

t (w

⇤T

zt

)

T µ

2

/2

t=1

=

2

+ ln(|⇧|)
+ T K + (K + 2 + 2
µ

2

+ 2K| |)T µ + (1 + | |T ✏ )

ln(|⇧|/⌫)


(69)

Safety-Aware Algorithms for Adversarial Contextual Bandit

Now let us set
X

= 0 and since we have that
w ⇤ T yt 

ct [at ]

t=1

PT

t (w

t=1

⇤T

)  0, we get:

zt

ln(|⇧|)
+ T K + (K + 2 + 2
µ

2

+ 2K| |)T µ + (1 + | |T ✏ )

ln(|⇧|/⌫)


ln(|⇧|)
ln(|⇧|/⌫)
+ T K + (3K + 4)T µ + (1 + T ✏ )
µ

p
p
p
✏
 2 T (ln(|⇧|)(3K + 4)) + 2 T K(1 + T ) ln(|⇧|/⌫) = O( T 1+✏ K ln(|⇧|/⌫))


(70)

where we set µ and  as:

µ=

s

ln(|⇧|)
,
(3K + 4)T

=

r

(1 + T ✏ ) ln(|⇧|/⌫)
.
TK

P
P
Now let us consider t (rt [at ]
). Let us assume t (rt [at ]
PT
that t=1 ct [at ] w⇤ T yt
2T . Hence we have:
T
X

(rt [at ]

t=1

 2T + 2

)

2

0, otherwise we prove the theorem already. Note

( µT /2 + 1/µ)

p
p
T (ln(|⇧|)(3K + 4)) + 2 T K(1 + T ✏ ) ln(|⇧|/⌫).

To maximize the LHS of the above inequality, we set
T
X

)

(71)

=

PT

t=1 (rt [at ]

µT +2/µ

)

. Substitute

into the above inequality, we get:

p
p
4
)(2T + 2 T (ln(|⇧|)(3K + 4)) + 2 T K(1 + T ✏ ) ln(|⇧|/⌫))
µ
t=1
p
p
p
4
 (2T 1 ✏ ln(|⇧|)K + )(2T + 2 T (ln(|⇧|)(3K + 4)) + 2 T K(1 + T ✏ ) ln(|⇧|/⌫))
µ
p
p
p
2 ✏
= 24(T
K ln(|⇧|) + T 1.5 ✏ K ln(|⇧|) + T 1.5 0.5✏ K ln(|⇧|) + T 1.5 K + T K + T 1+✏ K ln(1/ )
(rt [at ]

= O(T 2

✏

)

2

 (2 µT +

K ln(|⇧|)).

(72)

Hence we have:
T
X

(rt [at ]

) = O(T 1

✏/2

t=1

p

K ln(|⇧|)).

(73)

| |
Note that for , we have = KT ✏+0.5 . To verify that
2/K µ µ , we can see that as long as ✏ 2 (0, 1/2), we
have = ⇥(T 0.5 ✏ ) while | |/(2/K µ µ) = O(1). Hence when T is big enough, we can see that it always holds
| |
that
K + 2µ2 2 = K + 2 ln(|⇧|)KT 2✏ . Note that again as long
2/K µ µ . For the second condition that
2✏
as ✏ 2 (0, 1/2), we have = ⇥(T 0.5 ✏ ), and K + 2 ln(|⇧|)KT
= O(1). Hence we have
K + 2 ln(|⇧|)KT 2✏ .
q
q
✏
ln(|⇧|)
Hence, we have shown that when µ = (3K+4)T
,  = (1+T )Tln(|⇧|/⌫)
, and = T ✏+1/2 K, we have that as T ! 1:
K
T
X

(ct [at ]

w⇤ T yt ) = O(

(rt [at ]

)  O(T 1

t=1

T
X
t=1

p

✏/2

T 1+✏ ln(|⇧|/⌫)),

p
K ln(|⇧|)).

(74)

