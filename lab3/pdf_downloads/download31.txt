Averaged-DQN: Variance Reduction and Stabilization for
Deep Reinforcement Learning

Oron Anschel 1 Nir Baram 1 Nahum Shimkin 1

Abstract
Instability and variability of Deep Reinforcement
Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on
averaging previously learned Q-values estimates,
which leads to a more stable training procedure
and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine
the source of value function estimation errors and
provide an analytical comparison within a simplified model. We further present experiments
on the Arcade Learning Environment benchmark
that demonstrate significantly improved stability
and performance due to the proposed extension.

1. Introduction
In Reinforcement Learning (RL) an agent seeks an optimal policy for a sequential decision making problem (Sutton & Barto, 1998). It does so by learning which action
is optimal for each environment state. Over the course
of time, many algorithms have been introduced for solving RL problems including Q-learning (Watkins & Dayan,
1992), SARSA (Rummery & Niranjan, 1994; Sutton &
Barto, 1998), and policy gradient methods (Sutton et al.,
1999). These methods are often analyzed in the setup of
linear function approximation, where convergence is guaranteed under mild assumptions (Tsitsiklis, 1994; Jaakkola
et al., 1994; Tsitsiklis & Van Roy, 1997; Even-Dar & Mansour, 2003). In practice, real-world problems usually involve high-dimensional inputs forcing linear function approximation methods to rely upon hand engineered features
1

Department
of
Electrical
Engineering,
Haifa
32000, Israel.
Correspondence to:
Oron Anschel
<oronanschel@campus.technion.ac.il>,
Nir
Baram
<nirb@campus.technion.ac.il>,
Nahum
Shimkin
<shimkin@ee.technion.ac.il>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

for problem-specific state representation. These problemspecific features diminish the agent flexibility, and so the
need of an expressive and flexible non-linear function approximation emerges. Except for few successful attempts
(e.g., TD-gammon, Tesauro (1995)), the combination of
non-linear function approximation and RL was considered
unstable and was shown to diverge even in simple domains
(Boyan & Moore, 1995).
The recent Deep Q-Network (DQN) algorithm (Mnih et al.,
2013), was the first to successfully combine a powerful non-linear function approximation technique known
as Deep Neural Network (DNN) (LeCun et al., 1998;
Krizhevsky et al., 2012) together with the Q-learning algorithm. DQN presented a remarkably flexible and stable
algorithm, showing success in the majority of games within
the Arcade Learning Environment (ALE) (Bellemare et al.,
2013). DQN increased the training stability by breaking
the RL problem into sequential supervised learning tasks.
To do so, DQN introduces the concept of a target network
and uses an Experience Replay buffer (ER) (Lin, 1993).
Following the DQN work, additional modifications and extensions to the basic algorithm further increased training
stability. Schaul et al. (2015) suggested sophisticated ER
sampling strategy. Several works extended standard RL
exploration techniques to deal with high-dimensional input
(Bellemare et al., 2016; Tang et al., 2016; Osband et al.,
2016). Mnih et al. (2016) showed that sampling from ER
could be replaced with asynchronous updates from parallel
environments (which enables the use of on-policy methods). Wang et al. (2015) suggested a network architecture
base on the advantage function decomposition (Baird III,
1993).
In this work we address issues that arise from the combination of Q-learning and function approximation. Thrun &
Schwartz (1993) were first to investigate one of these issues
which they have termed as the overestimation phenomena.
The max operator in Q-learning can lead to overestimation
of state-action values in the presence of noise. Van Hasselt
et al. (2015) suggest the Double-DQN that uses the Double
Q-learning estimator (Van Hasselt, 2010) method as a solution to the problem. Additionally, Van Hasselt et al. (2015)
showed that Q-learning overestimation do occur in practice

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

(at least in the ALE).

2.2. Q-learning

This work suggests a different solution to the overestimation phenomena, named Averaged-DQN (Section 3), based
on averaging previously learned Q-values estimates. The
averaging reduces the target approximation error variance
(Sections 4 and 5) which leads to stability and improved
results. Additionally, we provide experimental results on
selected games of the Arcade Learning Environment.

One of the most popular RL algorithms is the Q-learning
algorithm (Watkins & Dayan, 1992). This algorithm is
based on a simple value iteration update (Bellman, 1957),
directly estimating the optimal value function Qâˆ— . Tabular
Q-learning assumes a table that contains old action-value
function estimates and preform updates using the following update rule:

We summarize the main contributions of this paper as follows:

Q(s0 , a0 ) âˆ’ Q(s, a)),
Q(s, a) â† Q(s, a) + Î±(r + Î³ max
0

â€¢ A novel extension to the DQN algorithm which stabilizes training, and improves the attained performance,
by averaging over previously learned Q-values.
â€¢ Variance analysis that explains some of the DQN
problems, and how the proposed extension addresses
them.
â€¢ Experiments with several ALE games demonstrating
the favorable effect of the proposed scheme.

a

(1)
where s0 is the resulting state after applying action a in the
state s, r is the immediate reward observed for action a at
state s, Î³ is the discount factor, and Î± is a learning rate.
When the number of states is large, maintaining a lookup table with all possible state-action pairs values in memory is impractical. A common solution to this issue is to
use function approximation parametrized by Î¸, such that
Q(s, a) â‰ˆ Q(s, a; Î¸).
2.3. Deep Q Networks (DQN)

2.1. Reinforcement Learning

We present in Algorithm 1 a slightly different formulation
of the DQN algorithm (Mnih et al., 2013). In iteration i
the DQN algorithm solves a supervised learning problem
to approximate the action-value function Q(s, a; Î¸) (line 6).
This is an extension of implementing (1) in its function approximation form (Riedmiller, 2005).

We consider the usual RL learning framework (Sutton &
Barto, 1998). An agent is faced with a sequential decision making problem, where interaction with the environment takes place at discrete time steps (t = 0, 1, . . .). At
time t the agent observes state st âˆˆ S, selects an action
at âˆˆ A, which results in a scalar reward rt âˆˆ R, and a
transition to a next state st+1 âˆˆ S. We consider infinite
horizon problems
Pâˆžwith a0 discounted cumulative reward objective Rt = t0 =t Î³ t âˆ’t rt0 , where Î³ âˆˆ [0, 1] is the discount factor. The goal of the agent is to find an optimal
policy Ï€ : S â†’ A that maximize its expected discounted
cumulative reward.

Algorithm 1 DQN
1: Initialize Q(s, a; Î¸) with random weights Î¸0
2: Initialize Experience Replay (ER) buffer B
3: Initialize exploration procedure Explore(Â·)
4: for i = 1, 2, . . . , N do
i
= EB [r + Î³ maxa0 Q(s0 , a0 ; Î¸iâˆ’1 )| s, a]
5:
ys,a

 i
6:
Î¸i â‰ˆ argminÎ¸ EB (ys,a
âˆ’ Q(s, a; Î¸))2
7:
Explore(Â·), update B
8: end for
output QDQN (s, a; Î¸N )

2. Background
In this section we elaborate on relevant RL background,
and specifically on the Q-learning algorithm.

Value-based methods for solving RL problems encode policies through the use of value functions, which denote the
expected discounted cumulative reward from a given state
s, following a policy Ï€. Specifically we are interested in
state-action value functions:
"âˆž
#
X
Ï€
Ï€
t
Q (s, a) = E
Î³ rt |s0 = s, a0 = a .
t=0

The optimal value function is denoted as Qâˆ— (s, a) =
maxÏ€ QÏ€ (s, a), and an optimal policy Ï€ âˆ— can be easily derived by Ï€ âˆ— (s) âˆˆ argmaxa Qâˆ— (s, a).

i
The target values ys,a
(line 5) are constructed using a designated target-network Q(s, a; Î¸iâˆ’1 ) (using the previous iteration parameters Î¸iâˆ’1 ), where the expectation (EB ) is taken
w.r.t. the sample distribution of experience transitions in the
ER buffer (s, a, r, s0 ) âˆ¼ B. The DQN loss (line 6) is minimized using a Stochastic Gradient Descent (SGD) variant,
sampling mini-batches from the ER buffer. Additionally,
DQN requires an exploration procedure (which we denote
as Explore(Â·)) to interact with the environment (e.g., an greedy exploration procedure). The number of new experience transitions (s, a, r, s0 ) added by exploration to the ER

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

buffer in each iteration is small, relatively to the size of the
ER buffer. Thereby, Î¸iâˆ’1 can be used as a good initialization for Î¸ in iteration i.
Note that in the original implementation (Mnih et al., 2013;
2015), transitions are added to the ER buffer simultaneously with the minimization of the DQN loss (line 6). Using the hyperparameters employed by Mnih et al. (2013;
2015) (detailed for completeness in Appendix E), 1% of the
experience transitions in ER buffer are replaced between
target network parameter updates, and 8% are sampled for
minimization.

3. Averaged DQN
The Averaged-DQN algorithm (Algorithm 2) is an extension of the DQN algorithm. Averaged-DQN uses the K
previously learned Q-values estimates to produce the current action-value estimate (line 5). The Averaged-DQN algorithm stabilizes the training process (see Figure 1), by
reducing the variance of target approximation error as we
elaborate in Section 5. The computational effort compared to DQN is, K-fold more forward passes through a
Q-network while minimizing the DQN loss (line 7). The
number of back-propagation updates remains the same as
in DQN. Computational cost experiments are provided in
Appedix D. The output of the algorithm is the average over
the last K previously learned Q-networks.

Breakout
DQN
Averaged DQN, K=10

Averaged score per episode

400

300

200

DQN compared to DQN (and Double-DQN), further experimental results are given in Section 6.
We note that recently-learned state-action value estimates
are likely to be better than older ones, therefore we have
also considered a recency-weighted average. In practice,
a weighted average scheme did not improve performance
and therefore is not presented here.

4. Overestimation and Approximation Errors
Next, we discuss the various types of errors that arise due to
the combination of Q-learning and function approximation
in the DQN algorithm, and their effect on training stability.
We refer to DQNâ€™s performance in the B REAKOUT game
in Figure 1. The source of the learning curve variance in
DQNâ€™s performance is an occasional sudden drop in the
average score that is usually recovered in the next evaluation phase (for another illustration of the variance source
see Appendix A). Another phenomenon can be observed in
Figure 2, where DQN initially reaches a steady state (after
20 million frames), followed by a gradual deterioration in
performance.
For the rest of this section, we list the above mentioned errors, and discuss our hypothesis as to the relations between
each error and the instability phenomena depicted in Figures 1 and 2.

100

0

Algorithm 2 Averaged DQN
1: Initialize Q(s, a; Î¸) with random weights Î¸0
2: Initialize Experience Replay (ER) buffer B
3: Initialize exploration procedure Explore(Â·)
4: for i = 1, 2, . . . , N do
PK
1
5:
QA
iâˆ’1 (s, a) = K
k=1 Q(s, a; Î¸iâˆ’k )


i
0 0
6:
ys,a
= EB r + Î³ maxa0 QA
iâˆ’1 (s , a )| s, a
 i

7:
Î¸i â‰ˆ argminÎ¸ EB (ys,a
âˆ’ Q(s, a; Î¸))2
8:
Explore(Â·), update B
9: end for
PKâˆ’1
1
output QA
N (s, a) = K
k=0 Q(s, a; Î¸N âˆ’k )

0

20

40

60
80
frames [millions]

100

120

Figure 1. DQN and Averaged-DQN performance in the Atari
game of B REAKOUT. The bold lines are averages over seven independent learning trials. Every 1M frames, a performance test
using -greedy policy with  = 0.05 for 500000 frames was conducted. The shaded area presents one standard deviation. For both
DQN and Averaged-DQN the hyperparameters used were taken
from Mnih et al. (2015).

In Figures 1 and 2 we can see the performance of Averaged-

We follow terminology from Thrun & Schwartz (1993),
and define some additional relevant quantities. Letting
Q(s, a; Î¸i ) be the value function of DQN at iteration i, we
denote âˆ†i = Q(s, a; Î¸i ) âˆ’ Qâˆ— (s, a) and decompose it as
follows:

âˆ†i = Q(s, a; Î¸i ) âˆ’ Qâˆ— (s, a)

i
i
i
i
= Q(s, a; Î¸i ) âˆ’ ys,a
+ ys,a
âˆ’ yÌ‚s,a
+ yÌ‚s,a
âˆ’ Qâˆ— (s, a) .
|
{z
} | {z } |
{z
}
Target Approximation
Error

Overestimation
Error

Optimality
Difference

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

10000

Asterix

104

Value estimates (log scale)

Averaged score per episode

Asterix
DQN
Double - DQN
Averaged DQN, K=10

8000

6000

4000

2000

103

DQN
Double - DQN
Averaged DQN, K=10

102

101

100
0

0

20

40

60
80
frames [millions]

100

120

20

40

60
80
frames [millions]

100

120

Figure 2. DQN, Double-DQN, and Averaged-DQN performance (left), and average value estimates (right) in the Atari game of A STERIX.
The bold lines are averages over seven independent learning trials. The shaded area presents one standard deviation. Every 2M frames,
a performance test using -greedy policy with  = 0.05 for 500000 frames was conducted. The hyperparameters used were taken from
Mnih et al. (2015).
i
i
is the true target:
is the DQN target, and yÌ‚s,a
Here ys,a
h
i
i
0 0
ys,a
= EB r + Î³ max
Q(s
,
a
;
Î¸
)|
s,
a
,
iâˆ’1
a0
h
i
i
yÌ‚s,a
= EB r + Î³ max
(ysiâˆ’1
0 ,a0 )| s, a .
0

We hypothesize that the variability in DQNâ€™s performance
in Figure 1, that was discussed at the start of this section, is
related to deviating from a steady-state policy induced by
the TAE.

i
the target approximation error, and
Let us denote by Zs,a
i
by Rs,a the overestimation error, namely

The Q-learning overestimation phenomena were first investigated by Thrun & Schwartz (1993). In their work, Thrun
i
and Schwartz considered the TAE Zs,a
as a random variable uniformly distributed in the interval [âˆ’, ]. Due to the
i
, the expected overmax operator in the DQN target ys,a
i
estimation errors Ez [Rs,a ] are upper bounded by Î³ nâˆ’1
n+1
(where n is the number of applicable actions in state s).
The intuition for this upper bound is that in the worst case,
all Q values are equal, and we get equality to the upper
bound:
nâˆ’1
i
Ez [Rs,a
] = Î³Ez [max
[Zsiâˆ’1
.
0 ,a0 ]] = Î³
0
a
n+1

a

i
i
Zs,a
= Q(s, a; Î¸i ) âˆ’ ys,a
,

i
i
i
Rs,a
= ys,a
âˆ’ yÌ‚s,a
.

The optimality difference can be seen as the error of a standard tabular Q-learning, here we address the other errors.
We next discuss each error in turn.
4.1. Target Approximation Error (TAE)
i
The TAE (Zs,a
), is the error in the learned Q(s, a; Î¸i ) relai
tive to ys,a , which is determined after minimizing the DQN
loss (Algorithm 1 line 6, Algorithm 2 line 7). The TAE is
a result of several factors: Firstly, the sub-optimality of Î¸i
due to inexact minimization. Secondly, the limited representation power of a neural net (model error). Lastly, the
generalization error for unseen state-action pairs due to the
finite size of the ER buffer.

The TAE can cause a deviations from a policy to a worse
one. For example, such deviation to a sub-optimal policy
i
i
occurs in case ys,a
= yÌ‚s,a
= Qâˆ— (s, a) and,
i
argmaxa [Q(s, a; Î¸i )] 6= argmaxa [Q(s, a; Î¸i ) âˆ’ Zs,a
]
i
= argmaxa [ys,a
].

4.2. Overestimation Error

The overestimation error is different in its nature from the
TAE since it presents a positive bias that can cause asymptotically sub-optimal policies, as was shown by Thrun &
Schwartz (1993), and later by Van Hasselt et al. (2015)
in the ALE environment. Note that a uniform bias in the
action-value function will not cause a change in the induced
policy. Unfortunately, the overestimation bias is uneven
and is bigger in states where the Q-values are similar for
the different actions, or in states which are the start of a
long trajectory (as we discuss in Section 5 on accumulation
of TAE variance).
Following from the above mentioned overestimation upper
bound, the magnitude of the bias is controlled by the variance of the TAE.

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

The Double Q-learning and its DQN implementation
(Double-DQN) (Van Hasselt et al., 2015; Van Hasselt,
2010) is one possible approach to tackle the overestimation
problem, which replaces the positive bias with a negative
one. Another possible remedy to the adverse effects of this
error is to directly reduce the variance of the TAE, as in our
proposed scheme (Section 5).
In Figure 2 we repeated the experiment presented in
Van Hasselt et al. (2015) (along with the application of
Averaged-DQN). This experiment is discussed in Van Hasselt et al. (2015) as an example of overestimation that leads
to asymptotically sub-optimal policies. Since AveragedDQN reduces the TAE variance, this experiment supports
an hypothesis that the main cause for overestimation in
DQN is the TAE variance.

5. TAE Variance Reduction
To analyse the TAE variance we first must assume a statistical model on the TAE, and we do so in a similar way to
i
Thrun & Schwartz (1993). Suppose that the TAE Zs,a
is
i
i
a random process such that E[Zs,a ] = 0, Var[Zs,a ] = Ïƒs2 ,
i
, Zsj0 ,a0 ] = 0. Furthermore, to foand for i 6= j: Cov[Zs,a
cus only on the TAE we eliminate the overestimation error
by considering a fixed policy for updating the target values.
Also, we can conveniently consider a zero reward r = 0
everywhere since it has no effect on variance calculations.
Denote by Qi , Q(s; Î¸i )sâˆˆS the vector of value estimates
in iteration i (where the fixed action a is suppressed), and
by Zi the vector of corresponding TAEs. For AveragedDQN we get:
Qi = Zi + Î³P

K
1 X
Qiâˆ’k ,
K
k=1

where P âˆˆ RSÃ—S
is the transition probabilities matrix
+
for the given policy. The covariance of the above Vector Autoregressive (VAR) model is given by the discretetime Lyapunov equation, and can be solved directly or by
specialized numerical algorithms (Arthur E Bryson, 1975).
However, to obtain an explicit comparison, we further specialize the model to an M -state unidirectional MDP as in
Figure 3

a
s0

a
s1

a
Â·Â·Â·

5.1. DQN Variance
We assume the statistical model mentioned at the start of
this section. Consider a unidirectional Markov Decision
Process (MDP) as in Figure 3, where the agent starts at
state s0 , state sM âˆ’1 is a terminal state, and the reward in
any state is equal to zero.
Employing DQN on this MDP model, we get that for i >
M:
QDQN (s0 , a; Î¸i ) = Zsi0 ,a + ysi 0 ,a
= Zsi0 ,a + Î³Q(s1 , a; Î¸iâˆ’1 )
= Zsi0 ,a + Î³[Zsiâˆ’1
+ ysiâˆ’1
] = Â·Â·Â· =
1 ,a
1 ,a

âˆ’1)
= Zsi0 ,a + Î³Zsiâˆ’1
+ Â· Â· Â· + Î³ (M âˆ’1) Zsiâˆ’(M
,
1 ,a
M âˆ’1 ,a

j
where in the last equality we have used the fact yM
âˆ’1,a = 0
for all j (terminal state). Therefore,

Var[QDQN (s0 , a; Î¸i )] =

M
âˆ’1
X

Î³ 2m Ïƒs2m .

m=0

The above example gives intuition about the behavior of
the TAE variance in DQN. The TAE is accumulated over
the past DQN iterations on the updates trajectory. Accumulation of TAE errors results in bigger variance with its
associated adverse effect, as was discussed in Section 4.
Algorithm 3 Ensemble DQN
1: Initialize K Q-networks Q(s, a; Î¸ k ) with random
weights Î¸0k for k âˆˆ {1, . . . , K}
2: Initialize Experience Replay (ER) buffer B
3: Initialize exploration procedure Explore(Â·)
4: for i = 1, 2, . . . , N do
PK
1
k
5:
QE
iâˆ’1 (s, a) = K
k=1 Q(s, a; Î¸iâˆ’1 )


i
E
0
6:
ys,a = EB r + Î³ maxa0 Qiâˆ’1 (s , a0 ))| s, a
7:
for k = 1, 2, . . . , K do
 i

8:
Î¸ik â‰ˆ argminÎ¸ EB (ys,a
âˆ’ Q(s, a; Î¸))2
9:
end for
10:
Explore(Â·), update B
11: end for
PK
1
k
output QE
N (s, a) = K
k=1 Q(s, a; Î¸i )

a
sM âˆ’2

sM âˆ’1

Figure 3. M states unidirectional MDP, The process starts at state
s0 , then in each time step moves to the right, until the terminal
state sM âˆ’1 is reached. A zero reward is obtained in any state.

5.2. Ensemble DQN Variance
We consider two approaches for TAE variance reduction.
The first one is the Averaged-DQN and the second we term
Ensemble-DQN. We start with Ensemble-DQN which is a
straightforward way to obtain a 1/K variance reduction,

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

with a computational effort of K-fold learning problems,
compared to DQN. Ensemble-DQN (Algorithm 3) solves
K DQN losses in parallel, then averages over the resulted
Q-values estimates.
For Ensemble-DQN on the unidirectional MDP in Figure
3, we get for i > M :
QE
i (s0 , a) =

M
âˆ’1
X

Î³m

m=0

Var[QE
i (s0 , a)] =

M
âˆ’1
X
m=0

=

K
1 X k,iâˆ’m
Zsm ,a ,
K
k=1

1 2m 2
Î³ Ïƒsm
K

1
Var[QDQN (s0 , a; Î¸i )],
K
0

,j
k,i
where for k 6= k 0 : Zs,a
and Zsk0 ,a
0 are two uncorrelated
E
TAEs. The calculations of Q (s0 , a) are detailed in Appendix B.

5.3. Averaged DQN Variance
We continue with Averaged-DQN, and calculate the variance in state s0 for the unidirectional MDP in Figure 3. We
get that for i > KM :
Var[QA
i (s0 , a)] =

M
âˆ’1
X

DK,m Î³ 2m Ïƒs2m ,

m=0

PN âˆ’1

where DK,m = N1 n=0 |Un /K|2(m+1) , with U =
âˆ’1
(Un )N
n=0 denoting a Discrete Fourier Transform (DFT) of
a rectangle pulse, and |Un /K| â‰¤ 1. The calculations of
QA (s0 , a) and DK,m are more involved and are detailed in
Appendix C.
Furthermore, for K > 1, m > 0 we have that DK,m <
1/K (Appendix C) and therefore the following holds
E
Var[QA
i (s0 , a)] < Var[Qi (s0 , a)]
1
= Var[QDQN (s0 , a; Î¸i )],
K

meaning that Averaged-DQN is theoretically more efficient
in TAE variance reduction than Ensemble-DQN, and at
least K times better than DQN. The intuition here is that
Averaged-DQN averages over TAEs averages, which are
the value estimates of the next states.

â€¢ How does the averaging affect the learned polices
quality.
To that end, we ran Averaged-DQN and DQN on the
ALE benchmark. Additionally, we ran Averaged-DQN,
Ensemble-DQN, and DQN on a Gridworld toy problem
where the optimal value function can be computed exactly.
6.1. Arcade Learning Environment (ALE)
To evaluate Averaged-DQN, we adopt the typical RL
methodology where agent performance is measured at the
end of training. We refer the reader to Liang et al. (2016)
for further discussion about DQN evaluation methods on
the ALE benchmark. The hyperparameters used were taken
from Mnih et al. (2015), and are presented for completeness in Appendix E. DQN code was taken from McGill
University RLLAB, and is available online1 (together with
Averaged-DQN implementation).
We have evaluated the Averaged-DQN algorithm on three
Atari games from the Arcade Learning Environment
(Bellemare et al., 2013). The game of B REAKOUT was
selected due to its popularity and the relative ease of the
DQN to reach a steady state policy. In contrast, the game
of S EAQUEST was selected due to its relative complexity,
and the significant improvement in performance obtained
by other DQN variants (e.g., Schaul et al. (2015); Wang
et al. (2015)). Finally, the game of A STERIX was presented
in Van Hasselt et al. (2015) as an example to overestimation
in DQN that leads to divergence.
As can be seen in Figure 4 and in Table 1 for all three
games, increasing the number of averaged networks in
Averaged-DQN results in lower average values estimates,
better-preforming policies, and less variability between the
runs of independent learning trials. For the game of A S TERIX , we see similarly to Van Hasselt et al. (2015) that
the divergence of DQN can be prevented by averaging.
Overall, the results suggest that in practice Averaged-DQN
reduces the TAE variance, which leads to smaller overestimation, stabilized learning curves and significantly improved performance.
6.2. Gridworld

The experiments were designed to address the following
questions:

The Gridworld problem (Figure 5) is a common RL benchmark (e.g., Boyan & Moore (1995)). As opposed to the
ALE, Gridworld has a smaller state space that allows the
ER buffer to contain all possible state-action pairs. Additionally, it allows the optimal value function Qâˆ— to be ac-

â€¢ How does the number K of averaged target networks
affect the error in value estimates, and in particular the
overestimation error.

1
McGill University RLLAB DQN Atari code: https:
//bitbucket.org/rllabmcgill/atari_release.
Averaged-DQN
code
https://bitbucket.org/
oronanschel/atari_release_averaged_dqn

6. Experiments

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

Breakout

Seaquest

300
200
100
0

0

20

100

10000

7500

5000

2500

0

120

0

20

Breakout

14

10

40
60
80
frames [millions]

100

6
4

20

40
60
80
frames [millions]

10000

8000

6000

4000

2000

0

0

20

40
60
80
frames [millions]

15

100

120

10

5

0

0

20

40
60
80
frames [millions]

100

120

100

120

Asterix

2
0

12000

120

DQN (K=1)
Averaged DQN, K=5
Averaged DQN, K=10
Averaged DQN, K=15

20

8

0

DQN (K=1)
Averaged DQN, K=5
Averaged DQN, K=10
Averaged DQN, K=15

Seaquest

DQN (K=1)
Averaged DQN, K=2
Averaged DQN, K=5
Averaged DQN, K=10

12
Value estimates

40
60
80
frames [millions]

12500

Averaged score per episode

400

15000

Asterix

14000

DQN (K=1)
Averaged DQN, K=5
Averaged DQN, K=10
Averaged DQN, K=15

Value estimates (log scale)

500

Averaged score per episode

DQN (K=1)
Averaged DQN, K=2
Averaged DQN, K=5
Averaged DQN, K=10

Value estimates

Averaged score per episode

600

100

120

DQN (K=1)
Averaged DQN, K=5
Averaged DQN, K=10
Averaged DQN, K=15

103

102

101

100

0

20

40
60
80
frames [millions]

Figure 4. The top row shows Averaged-DQN performance for the different number K of averaged networks on three Atari games. For
K = 1 Averaged-DQN is reduced to DQN. The bold lines are averaged over seven independent learning trials. Every 2M frames,
a performance test using -greedy policy with  = 0.05 for 500000 frames was conducted. The shaded area presents one standard
deviation. The bottom row shows the average value estimates for the three games. It can be seen that as the number of averaged networks
is increased, overestimation of the values is reduced, performance improves, and less variability is observed. The hyperparameters used
were taken from Mnih et al. (2015).

curately computed.
For the experiments, we have used Averaged-DQN, and
Ensemble-DQN with ER buffer containing all possible
state-action pairs. The network architecture that was used
composed of a small fully connected neural network with
one hidden layer of 80 neurons. For minimization of the
DQN loss, the ADAM optimizer (Kingma & Ba, 2014) was
used on 100 mini-batches of 32 samples per target network
parameters update in the first experiment, and 300 minibatches in the second.
6.2.1. E NVIRONMENT S ETUP
In this experiment on the problem of Gridworld (Figure
5), the state space contains pairs of points from a 2D discrete grid (S = {(x, y)}x,yâˆˆ1,...,20 ). The algorithm interacts with the environment through raw pixel features with a
one-hot feature map Ï†(st ) := (1{st = (x, y)})x,yâˆˆ1,...,20 .
There are four actions corresponding to steps in each compass direction, a reward of r = +1 in state st = (20, 20),
and r = 0 otherwise. We consider the discounted return
problem with a discount factor of Î³ = 0.9.

Gridworld
+1

start

Figure 5. Gridworld problem. The agent starts at the left-bottom
of the grid. In the upper-right corner, a reward of +1 is obtained.

6.2.2. OVERESTIMATION
In Figure 6 it can be seen that increasing the number K of
averaged target networks leads to reduced overestimation
eventually. Also, more averaged target networks seem to
reduces the overshoot of the values, and leads to smoother
and less inconsistent convergence.
6.2.3. AVERAGED VERSUS E NSEMBLE DQN
In Figure 7, it can be seen that as was predicted by the
analysis in Section 5, Ensemble-DQN is also inferior to
Averaged-DQN regarding variance reduction, and as a con-

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning
Table 1. The columns present the average performance of DQN and Averaged-DQN after 120M frames, using -greedy policy with
 = 0.05 for 500000 frames. The standard variation represents the variability over seven independent trials. Average performance
improved with the number of averaged networks. Human and random performance were taken from Mnih et al. (2015).
G AME

DQN
AVG . ( STD .

DEV.)

B REAKOUT

245.1

(124.5)

S EAQUEST

3775.2

(1575.6)

A STERIX

195.6

AVERAGED -DQN

AVERAGED -DQN

(K=5)

(K=10)

(K=15)

381.5

(80.4)

5740.2
6960.0

(20.2)

381.8

10475.1

(2926.6)

20182.0

68.4

(999.2)

8008.3

(243.6)

8364.9

(618.6)

8503.0

210.0

Gridworld

2.04

Es[maxaQâˆ—(s, a)]
2.02

1.96
A

1.7

(1946.9)

1.98

1.94

31.8

9961.7

Average predicted value

2.00

R ANDOM

(664.79 )

DQN (K=1)
Averaged DQN, K=5
Averaged DQN, K=10
Averaged DQN, K=20

2.02

H UMAN

--

Es[maxaQâˆ—(s, a)]

2.04

B C
D

DQN (K=1)
Ensemble DQN, K=20
Averaged DQN, K=20

2.00
1.98
1.96
1.94
1.92

1.92
1.90

(24.2)

Gridworld

2.06

Average predicted value

AVERAGED -DQN

200

400

600

800

1000

Iterations

Figure 6. Averaged-DQN average predicted value in Gridworld.
Increasing the number K of averaged target networks leads to a
faster convergence with less overestimation (positive-bias). The
bold lines are averages over 40 independent learning trials, and
the shaded area presents one standard deviation. In the figure,
A,B,C,D present DQN, and Averaged-DQN for K=5,10,20 average overestimation.

sequence far more overestimates the values. We note that
Ensemble-DQN was not implemented for the ALE experiments due to its demanding computational effort, and the
empirical evidence that was already obtained in this simple
Gridworld domain.

7. Discussion and Future Directions
In this work, we have presented the Averaged-DQN algorithm, an extension to DQN that stabilizes training, and improves performance by efficient TAE variance reduction.
We have shown both in theory and in practice that the proposed scheme is superior in TAE variance reduction, compared to a straightforward but computationally demanding
approach such as Ensemble-DQN (Algorithm 3). We have
demonstrated in several games of Atari that increasing the
number K of averaged target networks leads to better poli-

1.90
0

200

400

600
Iterations

800

1000

Figure 7. Averaged-DQN and Ensemble-DQN predicted value in
Gridworld. Averaging of past learned value is more beneficial
than learning in parallel. The bold lines are averages over 20
independent learning trials, where the shaded area presents one
standard deviation.

cies while reducing overestimation. Averaged-DQN is a
simple extension that can be easily integrated with other
DQN variants such as Schaul et al. (2015); Van Hasselt
et al. (2015); Wang et al. (2015); Bellemare et al. (2016);
He et al. (2016). Indeed, it would be of interest to study
the added value of averaging when combined with these
variants. Also, since Averaged-DQN has variance reduction effect on the learning curve, a more systematic comparison between the different variants can be facilitated as
discussed in (Liang et al., 2016).
In future work, we may dynamically learn when and how
many networks to average for best results. One simple suggestion may be to correlate the number of networks with
the state TD-error, similarly to Schaul et al. (2015). Finally,
incorporating averaging techniques similar to AveragedDQN within on-policy methods such as SARSA and ActorCritic methods (Mnih et al., 2016) can further stabilize
these algorithms.

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

References
Arthur E Bryson, Yu Chi Ho. Applied Optimal Control:
Optimization Estimation and Control. Hemisphere Publishing, 1975.
Baird III, Leemon C. Advantage updating. Technical report, DTIC Document, 1993.
Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.
The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence
Research, 47:253â€“279, 2013.
Bellemare, Marc G, Srinivasan, Sriram, Ostrovski, Georg,
Schaul, Tom, Saxton, David, and Munos, Remi. Unifying count-based exploration and intrinsic motivation.
arXiv preprint arXiv:1606.01868, 2016.
Bellman, Richard. A Markovian decision process. Indiana
Univ. Math. J., 6:679â€“684, 1957.
Boyan, Justin and Moore, Andrew W. Generalization in
reinforcement learning: Safely approximating the value
function. Advances in neural information processing
systems, pp. 369â€“376, 1995.
Even-Dar, Eyal and Mansour, Yishay. Learning rates for
q-learning. Journal of Machine Learning Research, 5
(Dec):1â€“25, 2003.
He, Frank S., Yang Liu, Alexander G. Schwing, and Peng,
Jian. Learning to play in a day: Faster deep reinforcement learning by optimality tightening. arXiv preprint
arXiv:1611.01606, 2016.
Jaakkola, Tommi, Jordan, Michael I, and Singh, Satinder P.
On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6(6):1185â€“
1201, 1994.
Kingma, Diederik P. and Ba, Jimmy. Adam: A method
for stochastic optimization.
arXiv preprint arXiv:
1412.6980, 2014.
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.
Imagenet classification with deep convolutional neural
networks. In Advances in NIPS, pp. 1097â€“1105, 2012.
LeCun, Yann, Bottou, LeÌon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278â€“
2324, 1998.
Liang, Yitao, Machado, Marlos C, Talvitie, Erik, and Bowling, Michael. State of the art control of Atari games
using shallow reinforcement learning. In Proceedings
of the 2016 International Conference on Autonomous
Agents & Multiagent Systems, pp. 485â€“493, 2016.

Lin, Long-Ji. Reinforcement learning for robots using neural networks. Technical report, DTIC Document, 1993.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and
Riedmiller, Martin. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.
Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,
Rusu, Andrei A, Veness, Joel, Bellemare, Marc G,
Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K,
Ostrovski, Georg, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529â€“
533, 2015.
Mnih, Volodymyr, Badia, Adria Puigdomenech, Mirza,
Mehdi, Graves, Alex, Lillicrap, Timothy P, Harley, Tim,
Silver, David, and Kavukcuoglu, Koray. Asynchronous
methods for deep reinforcement learning. arXiv preprint
arXiv:1602.01783, 2016.
Osband, Ian, Blundell, Charles, Pritzel, Alexander, and
Van Roy, Benjamin. Deep exploration via bootstrapped
DQN. arXiv preprint arXiv:1602.04621, 2016.
Riedmiller, Martin. Neural fitted Q iterationâ€“first experiences with a data efficient neural reinforcement learning
method. In European Conference on Machine Learning,
pp. 317â€“328. Springer, 2005.
Rummery, Gavin A and Niranjan, Mahesan. On-line Qlearning using connectionist systems. University of
Cambridge, Department of Engineering, 1994.
Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Silver, David. Prioritized experience replay. arXiv preprint
arXiv:1511.05952, 2015.
Sutton, Richard S and Barto, Andrew G. Reinforcement
Learning: An Introduction. MIT Press Cambridge, 1998.
Sutton, Richard S, McAllester, David A, Singh, Satinder P,
and Mansour, Yishay. Policy gradient methods for reinforcement learning with function approximation. In
NIPS, volume 99, pp. 1057â€“1063, 1999.
Tang, Haoran, Rein Houthooft, Davis Foote, Adam Stooke,
Xi Chen, Yan Duan, John Schulman, and Filip De Turck,
Pieter Abbeel. #exploration: A study of count-based exploration for deep reinforcement learning. arXiv preprint
arXiv:1611.04717, 2016.
Tesauro, Gerald. Temporal difference learning and tdgammon. Communications of the ACM, 38(3):58â€“68,
1995.
Thrun, Sebastian and Schwartz, Anton. Issues in using
function approximation for reinforcement learning. In

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

Proceedings of the 1993 Connectionist Models Summer
School Hillsdale, NJ. Lawrence Erlbaum, 1993.
Tsitsiklis, John N. Asynchronous stochastic approximation and q-learning. Machine Learning, 16(3):185â€“202,
1994.
Tsitsiklis, John N and Van Roy, Benjamin. An analysis
of temporal-difference learning with function approximation. IEEE transactions on automatic control, 42(5):
674â€“690, 1997.
Van Hasselt, Hado. Double Q-learning. In Lafferty, J. D.,
Williams, C. K. I., Shawe-Taylor, J., Zemel, R. S., and
Culotta, A. (eds.), Advances in Neural Information Processing Systems 23, pp. 2613â€“2621. 2010.
Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep
reinforcement learning with double Q-learning. arXiv
preprint arXiv: 1509.06461, 2015.
Wang, Ziyu, de Freitas, Nando, and Lanctot, Marc. Dueling
network architectures for deep reinforcement learning.
arXiv preprint arXiv: 1511.06581, 2015.
Watkins, Christopher JCH and Dayan, Peter. Q-learning.
Machine Learning, 8(3-4):279â€“292, 1992.

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

A. DQN Variance Source Example

B. Ensemble DQN TAE Variance Calculation
in a unidirectional MDP (Section 5.2)

Breakout
DQN
Averaged DQN, K=10
400
Averaged score per episode

Figure 8 presents a single learning trial of DQN compared
to Averaged-DQN, which emphasizes that the source of
variability in DQN between learning trials is due to occasions drops in average score within the learning trial. As
suggested in Section 4, this effect can be related to the TAE
causing to a deviation from the steady state policy.

300

200

100

k,i
k,i
Recall that E[Zs,a
] = 0, Var[Zs,a
] = Ïƒs2 , for all
0

,j
k,i
i 6= j: Cov[Zs,a
, Zsk0 ,a
] = 0, and for all k 6= k 0 :

0

0

,j
k,i
Cov[Zs,a
, Zsk0 ,a
] = 0. Following the Ensemble-DQN update equations in Algorithm 3:

K
1 X
Q(s0 , a; Î¸ik )
K
k=1

=

20

40

60
80
frames [millions]

100

120

Figure 8. DQN and Averaged-DQN performance in the Atari
game of B REAKOUT. The bold lines are single learning trials of
the DQN and Averaged-DQN algorithm. The dashed lines present
average of 7 independent learning trials. Every 1M frames, a performance test using -greedy policy with  = 0.05 for 500000
frames was conducted. The shaded area presents one standard deviation (from the average). For both DQN and Averaged-DQN
the hyperparameters used, were taken from Mnih et al. (2015).

QE
i (s0 , a) =
=

0

K
1 X k,i
[Zs0 ,a + ysi 0 ,a ]
K
k=1

=

update equations in Algorithm 2:

K
1 X k,i
[Zs0 ,a ] + ysi 0 ,a
K

QA
i (s0 , a) =

k=1

K
1 X k,i
[Zs0 ,a ] + Î³QE
=
iâˆ’1 (s1 , a)
K

=

k=1

k=1

=

1
K

K
X

[Zsk,i
]+
0 ,a

k=1

K
X

Î³
K

[Zsk,iâˆ’1
] + Î³ysiâˆ’1
.
1 ,a
2 ,a

M
âˆ’1
X

Î³m

m=0

K
1 X k,iâˆ’m
Zsm ,a .
K

K
1 X i+1âˆ’k
[Zs0 ,a + ysi+1âˆ’k
]
0 ,a
K

K
K
Î³ X A
1 X i+1âˆ’k
[Zs0 ,a ] +
Qiâˆ’k (s1 , a)
=
K
K
k=1

k=1

K
K
K
1 X i+1âˆ’k
Î³ XX
=
[Zs0 ,a ] + 2
Q(s1 , a; Î¸i+1âˆ’kâˆ’k0 ).
K
K
0
k=1

k=1 k =1

k=1

Since the TAEs are uncorrelated by assumption, we get
Var[QE
i (s0 , a)] =

=

k=1

k=1

as above, and noting that
By iteratively expanding ysiâˆ’1
2 ,a
j
ysM âˆ’1 ,a = 0 for all times (terminal state), we obtain,
QE
i (s0 , a) =

K
1 X
Q(s0 , a; Î¸i+1âˆ’k )
K

M
âˆ’1
X
m=0

1 2m 2
Î³ Ïƒsm .
K

By iteratively expanding Q(s1 , a; Î¸i+1âˆ’kâˆ’k0 ) as above,
and noting that ysjM âˆ’1 ,a = 0 for all times (terminal state),
we get
QA
i (s0 , a) =

C. Averaged DQN TAE Variance Calculation
in a unidirectional MDP (Section 5.3)
i
i
Recall that E[Zs,a
] = 0, Var[Zs,a
] = Ïƒs2 , and for i 6= j:
i
Cov[Zs,a
, Zsj0 ,a0 ] = 0. Further assume that for all s 6=
0
i
s : Cov[Zs,a
, Zsi0 ,a ] = 0. Following the Averaged-DQN

K
K
K
Î³ X X i+1âˆ’kâˆ’k0
1 X i+1âˆ’k
Zs0 ,a + 2
Zs1 ,a
K
K
0
k=1

+ Â·Â·Â· +

M âˆ’1

Î³
KM

K X
K
X
j1 =1 j2 =1

k=1 k =1

Â·Â·Â·

K
X

1 âˆ’Â·Â·Â·âˆ’jM
Zsi+1âˆ’j
.
M âˆ’1 ,a

jM =1

Since the TAEs in different states are uncorrelated by assumption the latter sums are uncorrelated and we may cal-

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

culate the variance of each separately. For L = 1, . . . , M ,
denote
"
#
K
K
K
X
1 X X
VL = Var
...
Zi +i +...+iL ,
K L i =1 i =2 i =1 1 2
1

1

L

where to simplify notation ZL , ZL+1 , . . . , ZKÂ·L are independent and identically distributed (i.i.d. ) TAEs random
variables, with E[Zl ] = 0 and E[(Zl )2 ] = Ïƒz2 .
Since the random variables are zero-mean and i.i.d. we
have that:
KL

X
 
1
nj,L Zj 2
VL = 2L Ez [(
K
j=L

=

KL
Ïƒz2 X
(nj,L )2 ,
K 2L
j=L

To continue, we denote the Discrete Fourier Transform
N âˆ’1
N âˆ’1
(DFT) of uK = (uK
n )n=0 as U = (Un )n=0 , and by using Parsevalâ€™s theorem we have that
VL =

N âˆ’1
Ïƒz2 X
|(uK âˆ— uK . . . âˆ— uK )n |2
K 2L n=0

N âˆ’1
Ïƒz2 1 X
= 2L
|Un |2L ,
K N n=0

where N is the length of the vectors u and U and is
taken large enough so that the sum includes all nonzero elements of the convolution. We denote DK,m =
PN âˆ’1
1
1
2(m+1)
, and now we can write
n=0 |Un |
K 2(m+1) N
Averaged-DQN variance as:
Var[QA
i (s0 , a)] =

M
âˆ’1
X

DK,m Î³ 2m Ïƒs2m .

m=0

where nj,L is the number of times Zj is counted in the
multiple summation above. The problem is reduced now to
evaluating nj,L for j âˆˆ {L, . . . , K Â· L}, where nj,L is the
number of solutions for the following equation:
i1 + i2 + . . . + iL = j,

Next we bound DK,m in order to compare Averaged-DQN
to Ensemble-DQN, and to DQN.
For K > 1, m > 0:

(2)

over i1 , . . . , iL âˆˆ {1, . . . , K}. The calculation can be done
recursively, by noting that

DK,m

N âˆ’1
1 X
|Un |2(m+1)
= 2(m+1)
N n=0
K

1

=

N âˆ’1
1 X
|Un /K|2(m+1)
N n=0

<

Since the final goal of this calculation is bounding the variance reduction coefficient, we will calculate the solution in
the frequency domain where the bound can be easily obtained.

N âˆ’1
1 X
|Un /K|2
N n=0

=

N âˆ’1
1 X K2
|u |
K 2 n=0 n

Denote

=

1
K

nj,L =

K
X

njâˆ’i,Lâˆ’1 .

i=1

(
uK
j

=

1 if j âˆˆ {1, . . . , K}
0 otherwise
,

For L = 1 (base case), we trivially get that for any j âˆˆ Z:
nj,1 = uK
j ,
and we can rewrite our recursive formula for nj,L as:
nj,L =

âˆž
X
i=âˆ’âˆž

njâˆ’i,Lâˆ’1 Â· uK
i

â‰¡ (njâˆ’1,Lâˆ’1 âˆ— uK )j

= (uK âˆ— uK . . . âˆ— uK )j ,
{z
}
|
L times

where âˆ— is the discrete convolution.

1
where we have used the easily verified facts that K
|Un | â‰¤
1
1, K |Un | = 1 only if n = 0, and Parsevalâ€™s theorem again.

D. Computational Cost Experiments
We have experimented with different number of averaged
networks on the ALE to show the added computational cost
of in both training and testing. The results are summarized in Figure 9. During testing, we have only forwards
passes of the states through the Q-networks (no backpropogations). A naive evaluation of the added cost of K âˆ’ 1
Q-networks in testing would predicts a K times slower
testing time (compared to DQN (K = 1)). In Figure 9
we see this is not the case. The faster testing speeds are
since same state is being forward in the K Q-networks, the

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

GPU carries out the forward-passes simultaneously. During training time the added computational cost is again
k âˆ’ 1 more forward-passes through the Q-networks, this
time only while learning. For the reason the k âˆ’ 1 network
are being used only during learning, the effect of averaging
more networks is less significant.

FPS vs The Number of Averaged Q-networks
Testing FPS
Naive Testing FPS calculation
Training FPS

Frames Per Second [FPS]

3500
3000
2500

action). For minimization of the DQN loss function RMSProp (with momentum parameter 0.95) was used.
E.2. Hyper-parameters
The discount factor was set to Î³ = 0.99, and the optimizer
learning rate to Î± = 0.00025. The steps between target network updates were 10,000. Training is done over 120M
frames. The agent is evaluated every 1M/2M steps (according to the figure). The size of the experience replay
memory is 1M tuples. The ER buffer gets sampled to update the network every 4 steps with mini batches of size
32. The exploration policy used is an -greedy policy with
 decreasing linearly from 1 to 0.1 over 1M steps.

2000
1500

F. Comparison to Double-DQN

1000
500
0

0

5
10
15
20
The number of averaged Q-networks (K)

25

Figure 9. Testing and training speeds. As the number of averagedQ networks increases speed get slower. However, due to implicit
parallel execution of forward passes in the GPU not as slow as a
navie prediction.

E. Experimental Details for the Arcade
Learning Environment Domain
We have selected three popular games of Atari to experiment with Averaged-DQN. We have used the exact setup proposed by Mnih et al. (2015) for the
experiments, we only provide the details here for
completeness.
The full implementation is available at https://bitbucket.org/oronanschel/
atari_release_averaged_dqn.
Each episode starts by executing a no-op action for one up
to 30 times uniformly. We have used a frame skipping
where each agent action is repeated four times before the
next frame is observed. The rewards obtained from the environment are clipped between -1 and 1.
E.1. Network Architecture
The network input is a 84x84x4 tensor. It contains a concatenation of the last four observed frames. Each frame is
rescaled (to a 84x84 image), and gray-scale. We have used
three convolutional layers followed by a fully-connected
hidden layer of 512 units. The first convolution layer convolves the input with 32 filters of size 8 (stride 4), the second, has 64 layers of size 4 (stride 2), the final one has 64
filters of size 3 (stride 1). The activation unit for all of the
layers a was Rectifier Linear Units (ReLu). The fully connected layer output is the different Q-values (one for each

As can be seen in Figures 10, ??, 11, for all three games, the
Averaged-DQN results outperform both DQN and DoubleDQN. Interestingly, Double-DQN predicts lower values
estimates than Averaged-DQN. Combined with the fact
that Double-DQN outputs have polices that are inferior to
Averaged-DQN, this supports the hypothesis that DoubleDQN underestimates the values.

Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning

Breakout
12000

Averaged score per episode

400
Averaged score per episode

Seaquest

DQN
Double - DQN
Averaged DQN, K=10

300

200

DQN
Double - DQN
Averaged DQN, K=10

10000

8000

6000

4000

100
2000

0

0

20

40

60
80
frames [millions]

100

0

120

0

20

40

60
80
frames [millions]

Breakout
10

120

Seaquest
17.5

DQN
Double - DQN
Averaged DQN, K=10

15.0

8

DQN
Double - DQN
Averaged DQN, K=10

12.5
Value estimates

Value estimates

100

6

4

10.0
7.5
5.0
2.5

2

0.0

0

âˆ’2.5
20

40

60
80
frames [millions]

100

120

Figure 10. DQN, Double-DQN, and Averaged-DQN (K = 10)
on the Atari game of Breakout. We see that Average-DQN predicts a higher estimated value than Double-DQN, while demonstrating better policies.

20

40

60
80
frames [millions]

100

120

Figure 11. DQN, Double-DQN, and Averaged-DQN (K = 10)
performance on the Atari game of Seaquest. Averaged-DQN
provides significantly improved results over DQN while DoubleDQN does not.

