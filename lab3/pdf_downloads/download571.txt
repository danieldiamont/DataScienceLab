Supplementary Material for “Relative Fisher Information
and Natural Gradient for Learning Large Modular Models”
Ke Sun and Frank Nielsen

Contents
1 Non-linear Activation Functions
2 Examples of RFIMs
2.1 A Single tanh Neuron . . . .
2.2 A Single sigm Neuron . . . .
2.3 A Single relu Neuron . . . .
2.4 A Single elu Neuron . . . . .
2.5 RFIM of a Linear Layer . . .
2.6 RFIM of a Non-Linear Layer
2.7 RFIM of a Softmax Layer . .
2.8 RFIM of Two layers . . . . .

.
.
.
.
.
.
.
.

1

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

2
3
4
5
5
5
6
7
7

3 Proof of Theorem 3

8

4 Experimental Settings & Zoomed Learning Curves

8

1

Non-linear Activation Functions

By definition,
def

tanh(t) =

exp(t) − exp(−t)
,
exp(t) + exp(−t)

and
def

sech(t) =

2
.
exp(t) + exp(−t)

It is easy to verify that
sech2 (t) = [1 + tanh(t)] [1 − tanh(t)] = 1 − tanh2 (t).

1

(1)

By eq. (1),
tanh0 (t) =

exp(t) + exp(−t)
exp(t) − exp(−t)
−
[exp(t) − exp(−t)]
exp(t) + exp(−t) [exp(t) + exp(−t)]2
2

=

2

[exp(t) + exp(−t)] − [exp(t) − exp(−t)]
2

[exp(t) + exp(−t)]

=

4
2

[exp(t) + exp(−t)]

= sech2 (t).

By definition,
def

sigm(t) =

1
.
1 + exp(−t)

Therefore
sigm0 (t) = −

1
2

[1 + exp(−t)]

(− exp(−t)) =

exp(−t)
2

[1 + exp(−t)]

= sigm(t) [1 − sigm(t)] .

A smoothed version of the relu function is given by

 
 
ιt
t
def
reluω (t) = ω ln exp
+ exp
,
ω
ω
where ω > 0 and 0 ≤ ι < 1. Then,
relu0ω (t) = ω

exp

ι exp
=
exp


ιt

1
+ exp



t

ω
ω


ιt
t
+
exp
ω
ω
ιt
t
ω + exp ω

ι
exp
ω

 
 
ιt
1
t
+ exp
ω
ω
ω


exp ωt


= ι + (1 − ι)
t
exp ιt
ω + exp ω
1

= ι + (1 − ι)
exp (ι − 1) ωt + 1


1−ι
t .
= ι + (1 − ι)sigm
ω

(2)

By definition,

elu(t) =
Therefore
0

elu (t) =

2

t
α (exp(t) − 1)


1
α exp(t)

if t ≥ 0
if t < 0.

if t ≥ 0
if t < 0.

(3)

Examples of RFIMs

Table 1 shows a list of commonly used RFIMs, with detailed derivations given in the following
subsections.

2

Table
Subsystem
A tanh neuron
A sigm neuron
A relu neuron
A elu neuron
A linear layer
A non-linear layer
A soft-max layer
Two layers

2.1

1: Commonly used RFIMs
the RFIM g y (w)
sech2 (w| x̃)x̃x̃|
sigm(w| x̃) [1 − sigm(w| x̃)] x̃x̃|

2
1−ι |
x̃x̃|
ι
+
(1
−
ι)sigm
w
x̃
ω

x̃x̃|
if w| x̃ ≥ 0
2
(α exp(w| x̃)) x̃x̃| if w| x̃ < 0
|
diag [x̃x̃ , · · · , x̃x̃| ]
diag [νf (w1 , x̃)x̃x̃| , · · · , νf (wm , x̃)x̃x̃| ]
a dense matrix as shown in eq. (10)
a dense matrix as shown in eq. (12)

A Single tanh Neuron

Consider a neuron with parameters w and a Bernoulli output y ∈ {+, −}, p(y = +) = p+ ,
p(y = −) = p− , and p+ + p− = 1. By the definition of RFIM, we have
∂ ln p+ ∂ ln p+
∂ ln p− ∂ ln p−
+ p−
|
∂w ∂w
∂w
∂w|
+
+
−
−
1 ∂p ∂p
1 ∂p ∂p
= +
+ −
.
p ∂w ∂w|
p ∂w ∂w|

g y (w) = p+

Since p+ + p− = 1,
∂p−
∂p+
+
= 0.
∂w
∂w
Therefore, the RFIM of a Bernoulli neuron has the general form


1
1 ∂p+ ∂p+
1 ∂p+ ∂p+
g y (w) =
+
= + −
.
+
−
|
p
p
∂w ∂w
p p ∂w ∂w|

(4)

A single tanh neuron with stochastic output y ∈ {−1, 1} is given by
1 − µ(x)
,
2
1 + µ(x)
,
p(y = 1) =
2
|
µ(x) = tanh(w x̃).

p(y = −1) =

(5)
(6)
(7)

By eq. (4),
g y (w) =

1



1−µ(x) 1+µ(x)
2
2

1 ∂µ
2 ∂w



1 ∂µ
2 ∂w|




2
1
1 − µ2 (x) x̃x̃|
(1 − µ(x)) (1 + µ(x))


= 1 − µ2 (x) x̃x̃|


= 1 − tanh2 (w| x̃) x̃x̃|
=

= sech2 (w| x̃)x̃x̃| .
3

An alternative analysis is given as follows. By eqs. (5) to (7),
exp(−w| x̃)
,
exp(w| x) + exp(−w| x)
exp(w| x̃)
p(y = 1) =
.
exp(w| x̃) + exp(−w| x̃)

p(y = −1) =

Then,


y

g (w) = Ey∼p(y | x)

∂ 2 ln p(y)
−
∂w∂w|



∂2
ln [exp(w| x̃) + exp(−w| x̃)]
|
∂w∂w


∂
exp(w| x̃) − exp(−w| x̃)
=
x̃
∂w| exp(w| x̃) + exp(−w| x̃)
∂
tanh(w| x̃)x̃
=
∂w|
= sech2 (w| x̃)x̃x̃| .
=

(first linear term vanishes)

The intuitive meaning of g y (w) is a weighted covariance to emphasize such “informative” x’s
that
• are in the linear region of tanh
• contain “ambiguous” samples
We will need at least dim(w) samples to make g y (w) full rank.

2.2

A Single sigm Neuron

A single sigm neuron is given by
p(y = 0) = 1 − µ(x),
p(y = 1) = µ(x),
µ(x) = sigm(w| x̃).
By eq. (4),
∂p(y = 1) ∂p(y = 1)
1
p(y = 0)p(y = 1)
∂w
∂w|
1
∂µ ∂µ
=
µ(x)(1 − µ(x)) ∂w ∂w|
1
µ2 (x)(1 − µ(x))2 x̃x̃|
=
µ(x)(1 − µ(x))
= µ(x)(1 − µ(x))x̃x̃|

g y (w) =

= sigm(w| x̃) [1 − sigm(w| x̃)] x̃x̃| .

4

2.3

A Single relu Neuron

Consider a single neuron with Gaussian output p(y | w, x) = G(y | µ(w, x), σ 2 ). Then


∂ ln G(y | µ, σ 2 ) ∂ ln G(y | µ, σ 2 )
y
g (w | x) = Ep(y | w,x)
∂w
∂w|





∂
∂
1
1
2
2
= Ep(y | w,x)
− 2 (y − µ)
− 2 (y − µ)
∂w
2σ
∂w|
2σ
"
#
2
1
∂µ ∂µ
= Ep(y | w,x)
− 2 (µ − y)
σ
∂w ∂w|
1
2 ∂µ ∂µ
Ep(y | w,x) (µ − y)
4
σ
∂w ∂w|
1 ∂µ ∂µ
.
= 2
σ ∂w ∂w|

=

We set σ = 1 to get rid of a scale parameter of the RFIM. We get
g y (w | x) =

∂µ ∂µ
.
∂w ∂w|

A single relu neuron is given by
µ(w, x) = reluω (w| x̃).
By eqs. (2) and (8),


2
1−ι |
g y (w) = ι + (1 − ι)sigm
w x̃
x̃x̃| .
ω

2.4

A Single elu Neuron

Similar to the analysis in Subsec. 2.3, a single elu neuron is given by
µ(w, x) = elu(w| x̃).
By eq. (3),
∂µ
=
∂w



x̃
α exp(w| x̃)x̃

if w| x̃ ≥ 0
if w| x̃ < 0.

By eq. (8),
y

g (w) =

2.5



x̃x̃|
2
(α exp(w| x̃)) x̃x̃|

if w| x̃ ≥ 0
if w| x̃ < 0.

RFIM of a Linear Layer

Consider a linear layer

p(y) = G y | W | x̃, σ 2 I ,
where W = (w1 , · · · , wDy ). By the definition of the multivariate Gaussian distribution,
Dy
1
Dy
1 X
2
ln p(y) = − ln 2π −
ln σ 2 − 2
(yi − wi| x̃) .
2
2
2σ i=1

5

(8)

Therefore,
∀i,

∂
1
ln p(y) = − 2 (wi| x̃ − yi ) x̃.
∂wi
σ

Therefore,
∀i, ∀j

∂
∂
1
|
| 
|
ln p(y)
| ln p(y) = 4 (yi − wi x̃) yj − wj x̃ x̃x̃ .
∂wi
∂wj
σ
D

y
W is vectorized by stacking its columns {wi }i=1
. In the following W will be used interchangeably
to denote either the matrix or its vector form. Correspondingly, the RFIM g y (W ) has Dy × Dy
blocks, where the off-diagonal blocks are
!


∂
∂
1
∀i 6= j, Ep(y)
ln p(y)
ln p(y) = 4 Ep(y) (yi − wi| x̃) yj − wj| x̃ x̃x̃| = 0,
∂wi
∂wj|
σ

and the diagonal blocks are


∂
∂
1
1
2
∀i, Ep(y)
ln p(y)
ln
p(y)
= 4 Ep(y) (yi − wi| x̃) x̃x̃| = 2 x̃x̃| .
|
∂wi
∂wi
σ
σ
In summary,
g y (W ) =

1
diag [x̃x̃| , · · · , x̃x̃| ] .
σ2

By setting σ = 1 we get
g y (W ) = diag [x̃x̃| , · · · , x̃x̃| ] .

2.6

RFIM of a Non-Linear Layer

The statistical model of a non-linear layer with independent output units is
p(y | W , x) =

Dy
Y

p(yi | wi , x).

i=1

Then,
ln p(y | W , x) =

Dy
X

ln p(yi | wi , x).

i=1

Therefore,

2


∂
ln p(y | W , x) = 

|
∂W ∂W

∂2
∂w1 ∂w1|



ln p(y1 | w1 , x)
..

.
∂2
|
∂wDy ∂wD
y


.

ln p(yDy | wDy , x)

Therefore the RFIM g y (W ) is a block-diagonal matrix, with the i’th block given by




∂2
∂2
ln p(yi | wi , x) = −Ep(yi | wi ,x)
ln p(yi | wi , x) ,
−Ep(y | W ,x)
∂wi ∂wi|
∂wi ∂wi|
which is simply the single neuron RFIM of the i’th neuron.

6

2.7

RFIM of a Softmax Layer

Recall that

exp(wi x̃)
p(y = i) = Pm
.
i=1 exp(wi x̃)

∀i ∈ {1, · · · , m} ,
Then
∀i,

ln p(y = i) = wi x̃ − ln

m
X

exp(wi x̃).

i=1

Hence
∀i, ∀j,

∂ ln p(y = i)
exp(wj x̃)
= δij x̃ − Pm
x̃,
∂wj
i=1 exp(wi x̃)

where δij = 1 if and only if i = j and δij = 0 otherwise. Then
∀i, ∀j, ∀k,

exp(wj x̃)
∂ 2 ln p(y = i)
exp(wj x̃)
|
= −δjk Pm
x̃x̃| + Pm
2 exp(wk x̃)x̃x̃
∂wj ∂wk|
exp(w
x̃)
i
( i=1 exp(wi x̃))
i=1
= (−δjk ηj + ηj ηk ) x̃x̃| .

The right-hand-side of eq. (9) does not depend on i. Therefore


(η1 − η12 )x̃x̃|
−η1 η2 x̃x̃|
···
−η1 ηm x̃x̃|
 −η2 η1 x̃x̃|
(η2 − η22 )x̃x̃| · · ·
−η2 ηm x̃x̃| 


g y (W ) = 
.
..
..
..
.
.


.
.
.
.
2
)x̃x̃|
−ηm η1 x̃x̃|
−ηm η2 x̃x̃|
· · · (ηm − ηm

2.8

(9)

(10)

RFIM of Two layers

Consider a two layer structure, where the output y satisfies a multivariate Bernoulli distribution
with independent dimensions. By a similar analysis to Subsec. 2.1, we have
g y (W ) =

Dy
X

νf (cl , h)

l=1

∂c|l h ∂c|l h
.
∂W ∂W |

(11)

It can be written block by block as g y (W ) = [Gij ]Dh ×Dh , where each block Gij means the
correlation between the i’th hidden neuron with weights wi and the j’th hidden neuron with
weights wj . By eq. (11),
Gij =

Dy
X
l=1

=

Dy
X
l=1

=

Dy
X

Dy

∂c| h ∂c|l h X
∂cil hi ∂cjl hj
νf (cl , h) l
=
νf (cl , h)
∂wi ∂wj|
∂wi ∂wj|
l=1

Dy

X
∂hi ∂hj
νf (cl , h)cil cjl
νf (cl , h)cil cjl (νf (wi , x)x̃) (νf (wj , x)x̃| )
| =
∂wi ∂wj
l=1

cil cjl νf (cl , h)νf (wi , x)νf (wj , x)x̃x̃| .

(12)

l=1

The proof of the other case, where two relu layers have stochastic output y satisfying a
multivariate Gaussian distribution with independent dimensions, is very similar and is omitted.

7

3

Proof of Theorem 3

Proof. By assumption, the joint distribution p(x, h) is in a factorable form. Therefore
log p(x, h) =

L
X

log p(hl | θl , rl ),

(13)

l=1

where l = 1, · · · , L is the index
UL of subsystems, hl is the
UL subsystem output, and rl is the reference
of the subsystem. We have l=1 {hl } = {x, h} and l=1 {θl } = {Θ}. Therefore




∂2
∂2
log p(x, h) = Ep −
log p(hl | θl , rl )
Ep −
∂θl ∂θl|
∂θl ∂θl|



∂2
= Ep(rl ) Ep(hl | rl ) −
log p(hl | θl , rl )
∂θl ∂θl|

= Ep g hl (θl ) ,
and
Ep

!
∂2
−
log p(x, h) = 0 (∀l1 6= l2 ).
∂θl1 ∂θl|2

Based on the Hessian
 expression of RFIM, J (Θ) is in a block-diagonal form, with each block
given by Ep g hl (θl ) .

4

Experimental Settings & Zoomed Learning Curves

The training/validation/testing sets have 50,000/10,000/10,000 images, respectively. Each sample is a gray scale image of size 28 × 28 (784 dimensional feature space) and is labeled as one of
ten different classes. For all methods, the mini-batch size is fixed to 50 and the L2 regularization
strength is fixed to 10−3 . For each optimizer, we try to find the best learning rate in the range
{· · · , 10−1 , 5 × 10−2 , 10−2 , 5 × 10−3 , 10−3 , · · · }. On the tested architectures, a good learning rate
configuration for RNGD is usually around 10−2 or 5 × 10−3 . The optimizers are in their default
settings in TensorFlow 1.0. For the Adam optimizer, β1 = 0.9, β2 = 0.999,  = 10−8 . For RNGD,
we set empirically T = 100, λ = 0.005 and ω = 1. We use the Glorot uniform initializer to set
the initial weights.
For each method and each learning rate configuration, we try 40 independent runs with
different random seeds. Then, we select the best configuration based on the validation accuracy.
Then, we plot the 40 learning curves as well as the average validation curve. The learning curves
are obtained by evaluating the training error and validation accuracy after each epoch (one pass
over all available training data).
See the following figs. (1–4) for the learning curves on four different architectures with relu
activation units and L2 regularization. Only the training curves and validation curves are shown
for a clear presentation. The testing accuracy is close to the validation accuracy (run our codes
to see the detailed results).

8

0.40
0.976
0.35
PLAIN+SGD (train)
PLAIN+SGD (valid)
PLAIN+ADAM (train)
PLAIN+ADAM (valid)
PLAIN+RNGD (train)
PLAIN+RNGD (valid)

error

0.30
0.25
0.20

0.972
0.970
0.968

0.15

0.966

0.10
0

20

40
60
#epochs

80

Figure 1: A MLP with shape 784–80–80–80–10.

9

100

accuracy

0.974

0.5

0.978
0.977
0.976
0.975

0.3

BNA+SGD (train) 0.974
BNA+SGD (valid)
BNA+ADAM (train) 0.973
BNA+ADAM (valid)
0.972
BNA+RNGD (train)
BNA+RNGD (valid) 0.971

0.2

0.1

0

20

40
60
#epochs

80

accuracy

error

0.4

0.970
100

Figure 2: A MLP with shape 784–80–80–80–10 and batch normalization after each hidden layer.

10

0.40

0.978

0.35

0.976

error

0.25
0.20

0.974
0.972
0.970
0.968

0.15

0.966

0.10
0

20

40
60
#epochs

80

Figure 3: A MLP with shape 784–100–100–100–10.

11

100

accuracy

PLAIN+SGD (train)
PLAIN+SGD (valid)
PLAIN+ADAM (train)
PLAIN+ADAM (valid)
PLAIN+RNGD (train)
PLAIN+RNGD (valid)

0.30

0.5
0.978
0.4

0.3

0.2

BNA+SGD (train)
BNA+SGD (valid) 0.974
BNA+ADAM (train)
BNA+ADAM (valid)
BNA+RNGD (train) 0.972

0.1

BNA+RNGD (valid)

0

20

40
60
#epochs

80

accuracy

error

0.976

0.970
100

Figure 4: A MLP with shape 784–100–100–100–10 and batch normalization after each hidden
layer.

12

