Supplementary Material for â€œRelative Fisher Information
and Natural Gradient for Learning Large Modular Modelsâ€
Ke Sun and Frank Nielsen

Contents
1 Non-linear Activation Functions
2 Examples of RFIMs
2.1 A Single tanh Neuron . . . .
2.2 A Single sigm Neuron . . . .
2.3 A Single relu Neuron . . . .
2.4 A Single elu Neuron . . . . .
2.5 RFIM of a Linear Layer . . .
2.6 RFIM of a Non-Linear Layer
2.7 RFIM of a Softmax Layer . .
2.8 RFIM of Two layers . . . . .

.
.
.
.
.
.
.
.

1

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

2
3
4
5
5
5
6
7
7

3 Proof of Theorem 3

8

4 Experimental Settings & Zoomed Learning Curves

8

1

Non-linear Activation Functions

By definition,
def

tanh(t) =

exp(t) âˆ’ exp(âˆ’t)
,
exp(t) + exp(âˆ’t)

and
def

sech(t) =

2
.
exp(t) + exp(âˆ’t)

It is easy to verify that
sech2 (t) = [1 + tanh(t)] [1 âˆ’ tanh(t)] = 1 âˆ’ tanh2 (t).

1

(1)

By eq. (1),
tanh0 (t) =

exp(t) + exp(âˆ’t)
exp(t) âˆ’ exp(âˆ’t)
âˆ’
[exp(t) âˆ’ exp(âˆ’t)]
exp(t) + exp(âˆ’t) [exp(t) + exp(âˆ’t)]2
2

=

2

[exp(t) + exp(âˆ’t)] âˆ’ [exp(t) âˆ’ exp(âˆ’t)]
2

[exp(t) + exp(âˆ’t)]

=

4
2

[exp(t) + exp(âˆ’t)]

= sech2 (t).

By definition,
def

sigm(t) =

1
.
1 + exp(âˆ’t)

Therefore
sigm0 (t) = âˆ’

1
2

[1 + exp(âˆ’t)]

(âˆ’ exp(âˆ’t)) =

exp(âˆ’t)
2

[1 + exp(âˆ’t)]

= sigm(t) [1 âˆ’ sigm(t)] .

A smoothed version of the relu function is given by

 
 
Î¹t
t
def
reluÏ‰ (t) = Ï‰ ln exp
+ exp
,
Ï‰
Ï‰
where Ï‰ > 0 and 0 â‰¤ Î¹ < 1. Then,
relu0Ï‰ (t) = Ï‰

exp

Î¹ exp
=
exp


Î¹t

1
+ exp



t

Ï‰
Ï‰


Î¹t
t
+
exp
Ï‰
Ï‰
Î¹t
t
Ï‰ + exp Ï‰

Î¹
exp
Ï‰

 
 
Î¹t
1
t
+ exp
Ï‰
Ï‰
Ï‰


exp Ï‰t


= Î¹ + (1 âˆ’ Î¹)
t
exp Î¹t
Ï‰ + exp Ï‰
1

= Î¹ + (1 âˆ’ Î¹)
exp (Î¹ âˆ’ 1) Ï‰t + 1


1âˆ’Î¹
t .
= Î¹ + (1 âˆ’ Î¹)sigm
Ï‰

(2)

By definition,

elu(t) =
Therefore
0

elu (t) =

2

t
Î± (exp(t) âˆ’ 1)


1
Î± exp(t)

if t â‰¥ 0
if t < 0.

if t â‰¥ 0
if t < 0.

(3)

Examples of RFIMs

Table 1 shows a list of commonly used RFIMs, with detailed derivations given in the following
subsections.

2

Table
Subsystem
A tanh neuron
A sigm neuron
A relu neuron
A elu neuron
A linear layer
A non-linear layer
A soft-max layer
Two layers

2.1

1: Commonly used RFIMs
the RFIM g y (w)
sech2 (w| xÌƒ)xÌƒxÌƒ|
sigm(w| xÌƒ) [1 âˆ’ sigm(w| xÌƒ)] xÌƒxÌƒ|

2
1âˆ’Î¹ |
xÌƒxÌƒ|
Î¹
+
(1
âˆ’
Î¹)sigm
w
xÌƒ
Ï‰

xÌƒxÌƒ|
if w| xÌƒ â‰¥ 0
2
(Î± exp(w| xÌƒ)) xÌƒxÌƒ| if w| xÌƒ < 0
|
diag [xÌƒxÌƒ , Â· Â· Â· , xÌƒxÌƒ| ]
diag [Î½f (w1 , xÌƒ)xÌƒxÌƒ| , Â· Â· Â· , Î½f (wm , xÌƒ)xÌƒxÌƒ| ]
a dense matrix as shown in eq. (10)
a dense matrix as shown in eq. (12)

A Single tanh Neuron

Consider a neuron with parameters w and a Bernoulli output y âˆˆ {+, âˆ’}, p(y = +) = p+ ,
p(y = âˆ’) = pâˆ’ , and p+ + pâˆ’ = 1. By the definition of RFIM, we have
âˆ‚ ln p+ âˆ‚ ln p+
âˆ‚ ln pâˆ’ âˆ‚ ln pâˆ’
+ pâˆ’
|
âˆ‚w âˆ‚w
âˆ‚w
âˆ‚w|
+
+
âˆ’
âˆ’
1 âˆ‚p âˆ‚p
1 âˆ‚p âˆ‚p
= +
+ âˆ’
.
p âˆ‚w âˆ‚w|
p âˆ‚w âˆ‚w|

g y (w) = p+

Since p+ + pâˆ’ = 1,
âˆ‚pâˆ’
âˆ‚p+
+
= 0.
âˆ‚w
âˆ‚w
Therefore, the RFIM of a Bernoulli neuron has the general form


1
1 âˆ‚p+ âˆ‚p+
1 âˆ‚p+ âˆ‚p+
g y (w) =
+
= + âˆ’
.
+
âˆ’
|
p
p
âˆ‚w âˆ‚w
p p âˆ‚w âˆ‚w|

(4)

A single tanh neuron with stochastic output y âˆˆ {âˆ’1, 1} is given by
1 âˆ’ Âµ(x)
,
2
1 + Âµ(x)
,
p(y = 1) =
2
|
Âµ(x) = tanh(w xÌƒ).

p(y = âˆ’1) =

(5)
(6)
(7)

By eq. (4),
g y (w) =

1



1âˆ’Âµ(x) 1+Âµ(x)
2
2

1 âˆ‚Âµ
2 âˆ‚w



1 âˆ‚Âµ
2 âˆ‚w|




2
1
1 âˆ’ Âµ2 (x) xÌƒxÌƒ|
(1 âˆ’ Âµ(x)) (1 + Âµ(x))


= 1 âˆ’ Âµ2 (x) xÌƒxÌƒ|


= 1 âˆ’ tanh2 (w| xÌƒ) xÌƒxÌƒ|
=

= sech2 (w| xÌƒ)xÌƒxÌƒ| .
3

An alternative analysis is given as follows. By eqs. (5) to (7),
exp(âˆ’w| xÌƒ)
,
exp(w| x) + exp(âˆ’w| x)
exp(w| xÌƒ)
p(y = 1) =
.
exp(w| xÌƒ) + exp(âˆ’w| xÌƒ)

p(y = âˆ’1) =

Then,


y

g (w) = Eyâˆ¼p(y | x)

âˆ‚ 2 ln p(y)
âˆ’
âˆ‚wâˆ‚w|



âˆ‚2
ln [exp(w| xÌƒ) + exp(âˆ’w| xÌƒ)]
|
âˆ‚wâˆ‚w


âˆ‚
exp(w| xÌƒ) âˆ’ exp(âˆ’w| xÌƒ)
=
xÌƒ
âˆ‚w| exp(w| xÌƒ) + exp(âˆ’w| xÌƒ)
âˆ‚
tanh(w| xÌƒ)xÌƒ
=
âˆ‚w|
= sech2 (w| xÌƒ)xÌƒxÌƒ| .
=

(first linear term vanishes)

The intuitive meaning of g y (w) is a weighted covariance to emphasize such â€œinformativeâ€ xâ€™s
that
â€¢ are in the linear region of tanh
â€¢ contain â€œambiguousâ€ samples
We will need at least dim(w) samples to make g y (w) full rank.

2.2

A Single sigm Neuron

A single sigm neuron is given by
p(y = 0) = 1 âˆ’ Âµ(x),
p(y = 1) = Âµ(x),
Âµ(x) = sigm(w| xÌƒ).
By eq. (4),
âˆ‚p(y = 1) âˆ‚p(y = 1)
1
p(y = 0)p(y = 1)
âˆ‚w
âˆ‚w|
1
âˆ‚Âµ âˆ‚Âµ
=
Âµ(x)(1 âˆ’ Âµ(x)) âˆ‚w âˆ‚w|
1
Âµ2 (x)(1 âˆ’ Âµ(x))2 xÌƒxÌƒ|
=
Âµ(x)(1 âˆ’ Âµ(x))
= Âµ(x)(1 âˆ’ Âµ(x))xÌƒxÌƒ|

g y (w) =

= sigm(w| xÌƒ) [1 âˆ’ sigm(w| xÌƒ)] xÌƒxÌƒ| .

4

2.3

A Single relu Neuron

Consider a single neuron with Gaussian output p(y | w, x) = G(y | Âµ(w, x), Ïƒ 2 ). Then


âˆ‚ ln G(y | Âµ, Ïƒ 2 ) âˆ‚ ln G(y | Âµ, Ïƒ 2 )
y
g (w | x) = Ep(y | w,x)
âˆ‚w
âˆ‚w|





âˆ‚
âˆ‚
1
1
2
2
= Ep(y | w,x)
âˆ’ 2 (y âˆ’ Âµ)
âˆ’ 2 (y âˆ’ Âµ)
âˆ‚w
2Ïƒ
âˆ‚w|
2Ïƒ
"
#
2
1
âˆ‚Âµ âˆ‚Âµ
= Ep(y | w,x)
âˆ’ 2 (Âµ âˆ’ y)
Ïƒ
âˆ‚w âˆ‚w|
1
2 âˆ‚Âµ âˆ‚Âµ
Ep(y | w,x) (Âµ âˆ’ y)
4
Ïƒ
âˆ‚w âˆ‚w|
1 âˆ‚Âµ âˆ‚Âµ
.
= 2
Ïƒ âˆ‚w âˆ‚w|

=

We set Ïƒ = 1 to get rid of a scale parameter of the RFIM. We get
g y (w | x) =

âˆ‚Âµ âˆ‚Âµ
.
âˆ‚w âˆ‚w|

A single relu neuron is given by
Âµ(w, x) = reluÏ‰ (w| xÌƒ).
By eqs. (2) and (8),


2
1âˆ’Î¹ |
g y (w) = Î¹ + (1 âˆ’ Î¹)sigm
w xÌƒ
xÌƒxÌƒ| .
Ï‰

2.4

A Single elu Neuron

Similar to the analysis in Subsec. 2.3, a single elu neuron is given by
Âµ(w, x) = elu(w| xÌƒ).
By eq. (3),
âˆ‚Âµ
=
âˆ‚w



xÌƒ
Î± exp(w| xÌƒ)xÌƒ

if w| xÌƒ â‰¥ 0
if w| xÌƒ < 0.

By eq. (8),
y

g (w) =

2.5



xÌƒxÌƒ|
2
(Î± exp(w| xÌƒ)) xÌƒxÌƒ|

if w| xÌƒ â‰¥ 0
if w| xÌƒ < 0.

RFIM of a Linear Layer

Consider a linear layer

p(y) = G y | W | xÌƒ, Ïƒ 2 I ,
where W = (w1 , Â· Â· Â· , wDy ). By the definition of the multivariate Gaussian distribution,
Dy
1
Dy
1 X
2
ln p(y) = âˆ’ ln 2Ï€ âˆ’
ln Ïƒ 2 âˆ’ 2
(yi âˆ’ wi| xÌƒ) .
2
2
2Ïƒ i=1

5

(8)

Therefore,
âˆ€i,

âˆ‚
1
ln p(y) = âˆ’ 2 (wi| xÌƒ âˆ’ yi ) xÌƒ.
âˆ‚wi
Ïƒ

Therefore,
âˆ€i, âˆ€j

âˆ‚
âˆ‚
1
|
| 
|
ln p(y)
| ln p(y) = 4 (yi âˆ’ wi xÌƒ) yj âˆ’ wj xÌƒ xÌƒxÌƒ .
âˆ‚wi
âˆ‚wj
Ïƒ
D

y
W is vectorized by stacking its columns {wi }i=1
. In the following W will be used interchangeably
to denote either the matrix or its vector form. Correspondingly, the RFIM g y (W ) has Dy Ã— Dy
blocks, where the off-diagonal blocks are
!


âˆ‚
âˆ‚
1
âˆ€i 6= j, Ep(y)
ln p(y)
ln p(y) = 4 Ep(y) (yi âˆ’ wi| xÌƒ) yj âˆ’ wj| xÌƒ xÌƒxÌƒ| = 0,
âˆ‚wi
âˆ‚wj|
Ïƒ

and the diagonal blocks are


âˆ‚
âˆ‚
1
1
2
âˆ€i, Ep(y)
ln p(y)
ln
p(y)
= 4 Ep(y) (yi âˆ’ wi| xÌƒ) xÌƒxÌƒ| = 2 xÌƒxÌƒ| .
|
âˆ‚wi
âˆ‚wi
Ïƒ
Ïƒ
In summary,
g y (W ) =

1
diag [xÌƒxÌƒ| , Â· Â· Â· , xÌƒxÌƒ| ] .
Ïƒ2

By setting Ïƒ = 1 we get
g y (W ) = diag [xÌƒxÌƒ| , Â· Â· Â· , xÌƒxÌƒ| ] .

2.6

RFIM of a Non-Linear Layer

The statistical model of a non-linear layer with independent output units is
p(y | W , x) =

Dy
Y

p(yi | wi , x).

i=1

Then,
ln p(y | W , x) =

Dy
X

ln p(yi | wi , x).

i=1

Therefore,
ï£®
2

ï£¯
âˆ‚
ln p(y | W , x) = ï£¯
ï£°
|
âˆ‚W âˆ‚W

âˆ‚2
âˆ‚w1 âˆ‚w1|

ï£¹

ln p(y1 | w1 , x)
..

.
âˆ‚2
|
âˆ‚wDy âˆ‚wD
y

ï£º
ï£º.
ï£»
ln p(yDy | wDy , x)

Therefore the RFIM g y (W ) is a block-diagonal matrix, with the iâ€™th block given by




âˆ‚2
âˆ‚2
ln p(yi | wi , x) = âˆ’Ep(yi | wi ,x)
ln p(yi | wi , x) ,
âˆ’Ep(y | W ,x)
âˆ‚wi âˆ‚wi|
âˆ‚wi âˆ‚wi|
which is simply the single neuron RFIM of the iâ€™th neuron.

6

2.7

RFIM of a Softmax Layer

Recall that

exp(wi xÌƒ)
p(y = i) = Pm
.
i=1 exp(wi xÌƒ)

âˆ€i âˆˆ {1, Â· Â· Â· , m} ,
Then
âˆ€i,

ln p(y = i) = wi xÌƒ âˆ’ ln

m
X

exp(wi xÌƒ).

i=1

Hence
âˆ€i, âˆ€j,

âˆ‚ ln p(y = i)
exp(wj xÌƒ)
= Î´ij xÌƒ âˆ’ Pm
xÌƒ,
âˆ‚wj
i=1 exp(wi xÌƒ)

where Î´ij = 1 if and only if i = j and Î´ij = 0 otherwise. Then
âˆ€i, âˆ€j, âˆ€k,

exp(wj xÌƒ)
âˆ‚ 2 ln p(y = i)
exp(wj xÌƒ)
|
= âˆ’Î´jk Pm
xÌƒxÌƒ| + Pm
2 exp(wk xÌƒ)xÌƒxÌƒ
âˆ‚wj âˆ‚wk|
exp(w
xÌƒ)
i
( i=1 exp(wi xÌƒ))
i=1
= (âˆ’Î´jk Î·j + Î·j Î·k ) xÌƒxÌƒ| .

The right-hand-side of eq. (9) does not depend on i. Therefore
ï£¹
ï£®
(Î·1 âˆ’ Î·12 )xÌƒxÌƒ|
âˆ’Î·1 Î·2 xÌƒxÌƒ|
Â·Â·Â·
âˆ’Î·1 Î·m xÌƒxÌƒ|
ï£¯ âˆ’Î·2 Î·1 xÌƒxÌƒ|
(Î·2 âˆ’ Î·22 )xÌƒxÌƒ| Â· Â· Â·
âˆ’Î·2 Î·m xÌƒxÌƒ| ï£º
ï£º
ï£¯
g y (W ) = ï£¯
ï£º.
..
..
..
.
.
ï£»
ï£°
.
.
.
.
2
)xÌƒxÌƒ|
âˆ’Î·m Î·1 xÌƒxÌƒ|
âˆ’Î·m Î·2 xÌƒxÌƒ|
Â· Â· Â· (Î·m âˆ’ Î·m

2.8

(9)

(10)

RFIM of Two layers

Consider a two layer structure, where the output y satisfies a multivariate Bernoulli distribution
with independent dimensions. By a similar analysis to Subsec. 2.1, we have
g y (W ) =

Dy
X

Î½f (cl , h)

l=1

âˆ‚c|l h âˆ‚c|l h
.
âˆ‚W âˆ‚W |

(11)

It can be written block by block as g y (W ) = [Gij ]Dh Ã—Dh , where each block Gij means the
correlation between the iâ€™th hidden neuron with weights wi and the jâ€™th hidden neuron with
weights wj . By eq. (11),
Gij =

Dy
X
l=1

=

Dy
X
l=1

=

Dy
X

Dy

âˆ‚c| h âˆ‚c|l h X
âˆ‚cil hi âˆ‚cjl hj
Î½f (cl , h) l
=
Î½f (cl , h)
âˆ‚wi âˆ‚wj|
âˆ‚wi âˆ‚wj|
l=1

Dy

X
âˆ‚hi âˆ‚hj
Î½f (cl , h)cil cjl
Î½f (cl , h)cil cjl (Î½f (wi , x)xÌƒ) (Î½f (wj , x)xÌƒ| )
| =
âˆ‚wi âˆ‚wj
l=1

cil cjl Î½f (cl , h)Î½f (wi , x)Î½f (wj , x)xÌƒxÌƒ| .

(12)

l=1

The proof of the other case, where two relu layers have stochastic output y satisfying a
multivariate Gaussian distribution with independent dimensions, is very similar and is omitted.

7

3

Proof of Theorem 3

Proof. By assumption, the joint distribution p(x, h) is in a factorable form. Therefore
log p(x, h) =

L
X

log p(hl | Î¸l , rl ),

(13)

l=1

where l = 1, Â· Â· Â· , L is the index
UL of subsystems, hl is the
UL subsystem output, and rl is the reference
of the subsystem. We have l=1 {hl } = {x, h} and l=1 {Î¸l } = {Î˜}. Therefore




âˆ‚2
âˆ‚2
log p(x, h) = Ep âˆ’
log p(hl | Î¸l , rl )
Ep âˆ’
âˆ‚Î¸l âˆ‚Î¸l|
âˆ‚Î¸l âˆ‚Î¸l|



âˆ‚2
= Ep(rl ) Ep(hl | rl ) âˆ’
log p(hl | Î¸l , rl )
âˆ‚Î¸l âˆ‚Î¸l|

= Ep g hl (Î¸l ) ,
and
Ep

!
âˆ‚2
âˆ’
log p(x, h) = 0 (âˆ€l1 6= l2 ).
âˆ‚Î¸l1 âˆ‚Î¸l|2

Based on the Hessian
 expression of RFIM, J (Î˜) is in a block-diagonal form, with each block
given by Ep g hl (Î¸l ) .

4

Experimental Settings & Zoomed Learning Curves

The training/validation/testing sets have 50,000/10,000/10,000 images, respectively. Each sample is a gray scale image of size 28 Ã— 28 (784 dimensional feature space) and is labeled as one of
ten different classes. For all methods, the mini-batch size is fixed to 50 and the L2 regularization
strength is fixed to 10âˆ’3 . For each optimizer, we try to find the best learning rate in the range
{Â· Â· Â· , 10âˆ’1 , 5 Ã— 10âˆ’2 , 10âˆ’2 , 5 Ã— 10âˆ’3 , 10âˆ’3 , Â· Â· Â· }. On the tested architectures, a good learning rate
configuration for RNGD is usually around 10âˆ’2 or 5 Ã— 10âˆ’3 . The optimizers are in their default
settings in TensorFlow 1.0. For the Adam optimizer, Î²1 = 0.9, Î²2 = 0.999,  = 10âˆ’8 . For RNGD,
we set empirically T = 100, Î» = 0.005 and Ï‰ = 1. We use the Glorot uniform initializer to set
the initial weights.
For each method and each learning rate configuration, we try 40 independent runs with
different random seeds. Then, we select the best configuration based on the validation accuracy.
Then, we plot the 40 learning curves as well as the average validation curve. The learning curves
are obtained by evaluating the training error and validation accuracy after each epoch (one pass
over all available training data).
See the following figs. (1â€“4) for the learning curves on four different architectures with relu
activation units and L2 regularization. Only the training curves and validation curves are shown
for a clear presentation. The testing accuracy is close to the validation accuracy (run our codes
to see the detailed results).

8

0.40
0.976
0.35
PLAIN+SGD (train)
PLAIN+SGD (valid)
PLAIN+ADAM (train)
PLAIN+ADAM (valid)
PLAIN+RNGD (train)
PLAIN+RNGD (valid)

error

0.30
0.25
0.20

0.972
0.970
0.968

0.15

0.966

0.10
0

20

40
60
#epochs

80

Figure 1: A MLP with shape 784â€“80â€“80â€“80â€“10.

9

100

accuracy

0.974

0.5

0.978
0.977
0.976
0.975

0.3

BNA+SGD (train) 0.974
BNA+SGD (valid)
BNA+ADAM (train) 0.973
BNA+ADAM (valid)
0.972
BNA+RNGD (train)
BNA+RNGD (valid) 0.971

0.2

0.1

0

20

40
60
#epochs

80

accuracy

error

0.4

0.970
100

Figure 2: A MLP with shape 784â€“80â€“80â€“80â€“10 and batch normalization after each hidden layer.

10

0.40

0.978

0.35

0.976

error

0.25
0.20

0.974
0.972
0.970
0.968

0.15

0.966

0.10
0

20

40
60
#epochs

80

Figure 3: A MLP with shape 784â€“100â€“100â€“100â€“10.

11

100

accuracy

PLAIN+SGD (train)
PLAIN+SGD (valid)
PLAIN+ADAM (train)
PLAIN+ADAM (valid)
PLAIN+RNGD (train)
PLAIN+RNGD (valid)

0.30

0.5
0.978
0.4

0.3

0.2

BNA+SGD (train)
BNA+SGD (valid) 0.974
BNA+ADAM (train)
BNA+ADAM (valid)
BNA+RNGD (train) 0.972

0.1

BNA+RNGD (valid)

0

20

40
60
#epochs

80

accuracy

error

0.976

0.970
100

Figure 4: A MLP with shape 784â€“100â€“100â€“100â€“10 and batch normalization after each hidden
layer.

12

