OptNet: Supplementary Material

Brandon Amos J. Zico Kolter

A. MNIST Experiment

B. Denoising Experiment Details

In this section we consider the integration of QP OptNet
layers into a traditional fully connected network for the
MNIST problem. The results here show only very marginal
improvement if any over a fully connected layer (MNIST,
after all, is very fairly well-solved by a fully connected network, let alone a convolution network). But our main point
of this comparison is simply to illustrate that we can include these layers within existing network architectures and
efficiently propagate the gradients through the layer.

Figure 7 shows the error of the fully connected network
on the denoising task and Figure 8 shows the error of the
OptNet fine-tuned TV solution.

Figure 6 shows that the results are similar for both networks
with slightly lower error and less variance in the OptNet
network.
4.0

Error

3.0

Error

40
35
30
25
20
15
0

50

100

150

200

Epoch

Figure 7. Error of the fully connected network for denoising
18

Train
Test

17
16
15
14
13
12
11
10
0

10

20

30

40

50

Epoch

Train
Test

3.5

Train
Test

45

Error

Specifically we use a FC600-FC10-FC10-SoftMax fully
connected network and compare it to a FC600-FC10Optnet10-SoftMax network, where the numbers after each
layer indicate the layer size. The OptNet layer in this case
includes only inequality constraints and the previous layer
is only used in the linear objective term p(zi ) = zi . To keep
Q  0, we use a Cholesky factorization Q = LLT + I
and directly learn L (without any information from the previous layer). We also directly learn A and G, and to ensure
a feasible solution always exists, we select some learnable
z0 and s0 and set b = Az0 and h = Gz0 + s0 .

50

Figure 8. Error rate from fine-tuning the TV solution for denoising

2.5
2.0
1.5
1.0
0.5
0.0
0

50

100

150

200

Epoch
4.0
3.0

Error

This section contains proofs for those results we highlight
in Section 3.2. As mentioned before, these proofs are all
quite straightforward and follow from well-known properties, but we include them here for completeness.

Train
Test

3.5
2.5
2.0
1.5
1.0

C.1. Proof of Theorem 1

0.5
0.0
0

C. Representational power of the QP OptNet
layer

50

100

150

200

Epoch

Figure 6. Training performance on MNIST; top: fully connected
network; bottom: OptNet as final layer.)

Proof. The fact that an OptNet layer is subdifferentiable
from strictly convex QPs (Q  0) follows directly from
the well-known result that the solution of a strictly convex
QP is continuous (though not everywhere differentiable).
Our proof essentially just boils down to showing this fact,

OptNet: Supplementary Material

though we do so by explicitly showing that there is a unique
solution to the Jacobian equations (6) that we presented
earlier, except on a measure zero set. This measure zero
set consists of QPs with degenerate solutions, points where
inequality constraints can hold with equality yet also have
zero-valued dual variables. For simplicity we assume that
A has full row rank, but this can be relaxed.
From the complementarity condition, we have that at a primal dual solution (z ? , λ? , ν ? )
(Gz ? − h)i < 0 → λ?i = 0
λ?i > 0 → (Gz ? − h)i = 0

will be all zero), there still exists a solution to the system
(6), because the right hand side is always in the range of
D(λ? ) and so will also be zero for these rows. In this case
there will no longer be a unique solution, corresponding to
the subdifferentiable but not differentiable case.
C.2. Proof of Theorem 2
Proof. The proof that an OptNet layer can represent any
piecewise linear univariate function relies on the fact that
we can represent any such function in “sum-of-max” form

(13)
f (x) =

k
X

First we consider the (typical) case where exactly one of
(Gz ? − h)i and λ?i is zero. Then the KKT differential matrix


Q
GT
AT
D(λ? )G D(Gz ? − h) 0 
(14)
A
0
0
(the left hand side of (6)) is non-singular. To see this, note
that if we let I be the set where λ?i > 0, then the matrix


AT
Q
GTI
D(λ? )GI D(Gz ? − h)I
0 =
A
0
0


(15)
Q
GTI AT
D(λ? )GI
0
0 
A
0
0
is non-singular (scaling the second block by D(λ? )−1 gives
a standard KKT system (Boyd & Vandenberghe, 2004, Section 10.4), which is nonsingular for invertible Q and [GTI
AT ] with full column rank, which must hold due to our
condition on A and the fact that there must be less than n
total tight constraints at the solution. Also note that for any
i 6∈ I, only the D(Gz ? −h)ii term is non-zero for the entire
row in the second block of the matrix. Thus, if we want to
solve the system

   
z
a
Q
GTI
AT
D(λ? )GI D(Gz ? − h)I
0  λ =  b  (16)
ν
c
A
0
0
we simply first set λi = bi /(Gz ? − h)i for i 6∈ I and then
solve the nonsingular system

  

a − GTĪ λĪ
z
Q
GTI AT
D(λ? )GI
 (17)
0
0  λI  = 
bI
ν
A
0
0
c.
Alternatively, suppose that we have both λ?i = 0 and
(Gz ? − h)i = 0. Then although the KKT matrix is now
singular (any row for which λ?i = 0 and (Gz ? − h)i = 0

wi max{ai x + b, 0}

(18)

i=1

(i.e., we cannot have both these terms non-zero).

where wi ∈ {−1, 1}, ai , bi ∈ R (to do so, simply proceed
left to right along the breakpoints of the function adding
a properly scaled linear term to fit the next piecewise section). The OptNet layer simply represents this function directly.
That is, we encode the optimization problem
minimize ktk22 + (z − wT t)2
z∈R,t∈Rk

(19)

subject to ai x + bi ≤ ti , i = 1, . . . , k
Clearly, the objective here is minimized when z = wT t,
and t is as small as possible, meaning each t must either be
at its bound ai x + b ≤ ti or, if ai x + b < 0, then ti = 0 will
be the optimal solution due to the objective function. To
obtain a multivariate but elementwise function, we simply
apply this function to each coordinate of the input x.
To see the specific case of a ReLU network, note that the
layer
z = max{W x + b, 0}
(20)
is simply equivalent to the OptNet problem
minimize kz − W x − bk22
z

(21)

subject to z ≥ 0.

C.3. Proof of Theorem 3
Proof. The final theorem simply states that a two-layer
ReLU network (more specifically, a ReLU followed by a
linear layer, which is sufficient to achieve a universal function approximator), can often require exponentially many
more units to approximate a function specified by an OptNet layer. That is, we consider a single-output ReLU network, much like in the previous section, but defined for
multi-variate inputs.
f (x) =

m
X
i=1

wi max{aTi x + b, 0}

(22)

OptNet: Supplementary Material

aT2 x

yet it does not seem possible to represent this in closed form
as a simple network: the closed form solution of such a
projection operator requires sorting or finding a particular
median term of the data (Duchi et al., 2008), which is not
feasible with a single layer for any form of network that
we are aware of. Yet for simplicity we stated the theorem
above using just ReLU networks and a straightforward example that works even in two dimensions.

aT1 x
aT3 x

Figure 9. Creases for a three-term pointwise maximum (left), and
a ReLU network (right).

Although there are many functions that such a network cannot represent, for illustration we consider a simple case of
a maximum of three linear functions
f 0 (x) = max{aT1 x, aT2 x, aT3 x}

(23)

To see why a ReLU is not capable of representing this function exactly, even for x ∈ R2 , note that any sum-of-max
function, due to the nature of the term max{aTi x + bi , 0} as
stated above must have “creases” (breakpoints in the piecewise linear function), than span the entire input space; this
is in contrast to the max terms, which can have creases that
only partially span the space. This is illustrated in Figure
9. It is apparent, therefore, that the two-layer ReLU cannot
exactly approximate the three maximum term (any ReLU
network would necessarily have a crease going through one
of the linear region of the original function). Yet this max
function can be captured by a simple OptNet layer
minimize z 2
z

subject to aTi x ≤ z, i = 1, . . . , 3.

(24)

The fact that the ReLU network is a universal function
approximator means that the we are able to approximate
the three-max term, but to do so means that we require
a dense covering of points over the input space, choose
an equal number of ReLU terms, then choose coefficients
such that we approximate the underlying function on this
points; however, for a large enough radius this will require
an exponential size covering to approximate the underlying
function arbitrarily closely.
Although the example here in this proof is quite simple
(and perhaps somewhat limited, since for example the function can be exactly approximated using a “Maxout” network), there are a number of other such functions for which
we have been unable to find any compact representation.
For example, projection of a point on to the simplex is easily written as the OptNet layer
minimize kz − xk22
z

subject to z ≥ 0, 1T z = 1

(25)

