Supplementary Material: Evaluating the Variance of Likelihood-Ratio
Gradient Estimators
Seiya Tokui 1 2 Issei Sato 3 2

A. Derivations of Estimators for Binary
Variables

as follows using Eq. (1).

In this section, we give the derivations of estimators for
binary variables given in Sec. 6. Let
qÏ†i (zi |pai ) = Âµzi i (1 âˆ’ Âµi )1âˆ’zi
be a Bernoulli distribution of a mean parameter Âµi =
Âµi (pai , Ï†i ). Here we only focus on the derivative w.r.t.
Âµi instead of Ï†i . The derivative w.r.t. Ï†i can be derived
by simply multiplying âˆ‡Ï†i Âµi to the derivative given in this
section.
The log probability of zi is given by
log qÏ†i (zi |pai ) = zi log Âµi + (1 âˆ’ zi ) log(1 âˆ’ Âµi ),

(

1
Âµi

1
âˆ’ 1âˆ’Âµ
i

if zi = 1
if zi = 0.
(1)

The derivative of the probability is simply given as follows.
(
1 if zi = 1
âˆ‚
qÏ† (zi |pai ) =
(2)
âˆ‚Âµi i
âˆ’1 if zi = 0.
Let fk = f (x, zi = k, z\i = hÏ†\i (x, zi , \i )) be the simulated objective function f evaluated at fixed zi = k âˆˆ
{0, 1} and the noise \i .
Likelihood-Ratio Estimator: Recall that the likelihoodratio estimator for a general class of distribution is formulated by the following equation.

âˆ‚
E f (x, gÏ† (x, ))
âˆ‚Âµi i

X
âˆ‚

=
qÏ†i (zi |pai )
.
f (x, z)
âˆ‚Âµ
z\i =hÏ†\i (x,zi ,\i )
i
z

(5)

i

By substituting Eq. (2), we obtain the following estimator.

X
âˆ‚

âˆ†?i =
f (x, z)
qÏ†i (zi |pai )
âˆ‚Âµ
z\i =hÏ†\i (x,zi ,\i )
i
z
i

= f1 Ã— 1 + f0 Ã— (âˆ’1)
= f1 âˆ’ f0 .
It can also be derived by simply calculating the mean of Eq.
(4).
Local Expectation Gradient: The local expectation gradient estimator is given by the following expectation.
âˆ‚
F (Ï†; x)
âˆ‚Âµi

âˆ‚
E f (x, gÏ† (x, ))
âˆ‚Âµi i
= Ei (f (x, z) âˆ’ bi (x, \i ))

Since zi = 1 holds with probability Âµi , the estimator is
written as follows.
(
(f1 âˆ’ bi )/Âµi
w.p. Âµi ,
LR
âˆ†i =
(4)
âˆ’(f0 âˆ’ bi )/(1 âˆ’ Âµi ) w.p. 1 âˆ’ Âµi .
Optimal Estimator: The optimal estimator is given by
the following formula.

and its derivative is
âˆ‚
zi
1 âˆ’ zi
log qÏ†i (zi |pai ) =
âˆ’
=
âˆ‚Âµi
Âµi
1 âˆ’ Âµi

âˆ‚
(f (x, z) âˆ’ bi (x, \i ))
log qÏ†i (zi |pai )
âˆ‚Âµi


1 âˆ’ zi
zi
âˆ’
= (fzi âˆ’ bi )
Âµi
1 âˆ’ Âµi
(
(f1 âˆ’ bi )/Âµi
if zi = 1,
=
âˆ’(f0 âˆ’ bi )/(1 âˆ’ Âµi ) if zi = 0.

âˆ‚
log qÏ†i (zi |pai ).
âˆ‚Âµi

(3)

Here we consider an independent baseline, i.e., bi =
bi (x, \i ) is constant against i . When qÏ†i is a Bernoulli
distribution, the Monte Carlo estimate of Eq. (3) is written

= EqÏ† (z\i |x)

X qÏ† (zi |mbi )
zi

qÏ†i (zi |pai )

f (x, z)

âˆ‚
qÏ† (zi |pai ). (6)
âˆ‚Âµi i

Let Ï€i = qÏ† (zi = 1|mbi ) and fk0 = f (x, zi = k, z\i =
hÏ†\i (x, zi = 1 âˆ’ k, \i )). Here fk0 is the value of f evaluated at zi = k âˆˆ {0, 1} and other variables z\i computed

Supplementary Material: Evaluating the Variance of Likelihood-Ratio Gradient Estimators

from \i and zi = 1 âˆ’ k. Note that the evaluation of the
local expectation gradient estimator proceeds as follows:
1. Sample  and compute z = gÏ† (x, ).
2. Discard zi so that we obtain z\i âˆ¼ qÏ† (z\i |x).
3. Compute the summation in Eq. (6).
Therefore, if zi = 1 is sampled at the first step, the value
of f used in the estimation is f1 and f00 instead of f0 , i.e.,
z\i = hÏ†\i (x, zi , \i ) is not re-evaluated at zi = 0. Similarly, if zi = 0 is sampled at the first step, the value of f
used in the estimation is f10 and f0 . Based on these observations, we derive the estimator using Eq. (2). If zi = 1,
then
X qÏ† (zi = k|mbi )
âˆ‚
f (x, zi = k, z\i )
qÏ† (zi = k|pai )
qÏ†i (zi = k|pai )
âˆ‚Âµi i
k

=

Ï€i
1 âˆ’ Ï€i 0
f1 âˆ’
f .
Âµi
1 âˆ’ Âµi 0

(7)

Ï€i
1 âˆ’ Ï€i 0
f1 âˆ’
f
Âµi
1 âˆ’ Âµi 0


1
1 âˆ’ Ï€i
0
=
Ï€i f1 âˆ’
Âµi f0
Âµi
1 âˆ’ Âµi


1
1 âˆ’ Ï€i
0
=
f1 âˆ’ (1 âˆ’ Ï€i )f1 âˆ’
Âµi f0
Âµi
1 âˆ’ Âµi


1
1 âˆ’ Ï€i
0
((1 âˆ’ Âµi )f1 + Âµi f0 ) .
=
f1 âˆ’
Âµi
1 âˆ’ Âµi

(8)

If zi = 0, then
X qÏ† (zi = k|mbi )
âˆ‚
f (x, zi = k, z\i )
qÏ† (zi = k|pai )
qÏ†i (zi = k|pai )
âˆ‚Âµi i
k

Ï€i 0 1 âˆ’ Ï€i
f âˆ’
f0 .
Âµi 1 1 âˆ’ Âµi

(9)

It is further transformed similarly to Eq. (8) as follows.
Ï€i 0 1 âˆ’ Ï€i
f âˆ’
f0
Âµi 1 1 âˆ’ Âµi


1
Ï€i
=âˆ’
(1 âˆ’ Ï€i )f0 âˆ’ (1 âˆ’ Âµi )f10
1 âˆ’ Âµi
Âµi


1
Ï€i
=âˆ’
f0 âˆ’ (Âµi f0 + (1 âˆ’ Âµi )f10 ) .
1 âˆ’ Âµi
Âµi

Note that this baseline might depend on zi . Since the local
expectation gradient is an unbiased estimator of the true
gradient, the residual term of the likelihood-ratio gradient
estimation Ci is kept 0.

B. Additional Results from the Experiments
The performance of each method on the training datasets is
shown in Fig. 1 and Fig. 2. The trends are almost same as
those of the validation scores.
The results on the test dataset are shown in Table 1. The log
likelihood is approximated by the Monte Calo lower bound
of Burda et al. (2015) with a sample size of 50,000.

C. Relationship between the RAM estimator
and the Use of Monte Carlo Objectives

It is further transformed as follows.

=

It can be seen as a likelihood-ratio estimator with baseline
given by
(
1âˆ’Ï€
((1 âˆ’ Âµi )f1 + Âµi f00 ) if zi = 1,
LEG
i
bi
= 1âˆ’Âµ
Ï€
0
if zi = 0.
Âµi ((1 âˆ’ Âµi )f1 + Âµi f0 )

(10)

By combining Eq. (8) and Eq. (10), we obtain the following estimator.
ï£±
1âˆ’Ï€
0
ï£² f1 âˆ’ 1âˆ’Âµii ((1âˆ’Âµi )f1 +Âµi f0 ) w.p. Âµ ,
i
LEG
Âµi
âˆ†i
=
Ï€
0
ï£³âˆ’ f0 âˆ’ Âµii ((1âˆ’Âµi )f1 +Âµi f0 )
w.p. 1 âˆ’ Âµi .
1âˆ’Âµi

It has been shown that the use of Monte Carlo objectives
(Burda et al., 2015; Mnih & Rezende, 2016) can improve
the learning of generative models including variational autoencoders and sigmoid belief networks. The importanceweighted autoencoders (IWAE) (Burda et al., 2015) and
VIMCO estimator (Mnih & Rezende, 2016) are gradient
estimators for Monte Carlo objectives. Both of these evaluate the function f at multiple points, while the RAM estimator also evaluates multiple points. The use of multipoint evaluation is orthogonal between these methods and
the RAM estimator. In the methods for Monte Carlo objectives, the multi-point evaluation comes from the improved
lower bound of the log likelihood, i.e., they alter the objective function. On the other hand, in the RAM estimator,
the multi-point evaluation is introduced purely for variance
reduction. In particular, the RAM estimator is applicable
to any kind of stochastic computational graphs. The experiments in this study is conducted to compare the variance
of each gradient estimator for the same objective function,
and therefore VIMCO estimator is omitted from the comparison.

References
Burda, Yuri, Grosse, Roger, and Salakhutdinov, Ruslan.
Importance weighted autoencoders. In Proceedings of
the 3rd International Conference on Learning Representations (ICLR), 2015.
Mnih, Andriy and Rezende, Danilo Jimenez. Variational
inference for monte carlo objectives. In Proceedings of

Supplementary Material: Evaluating the Variance of Likelihood-Ratio Gradient Estimators
MNIST (200-200-784)
LR

LR+C

LR+C+IDB

Omniglot (200-200-784)

MuProp+C+IDB

LEG

optimal

LR

LR+C+IDB

MuProp+C+IDB

LEG

optimal

âˆ’110

âˆ’100

âˆ’115
Variational lower bound

âˆ’105
Variational lower bound

LR+C

âˆ’110
âˆ’115
âˆ’120
âˆ’125

âˆ’120
âˆ’125
âˆ’130
âˆ’135

âˆ’130

âˆ’140

âˆ’135

âˆ’145
0

250

500

750
1000 1250
Iteration (x1000)

1500

1750

2000

0

250

500

750
1000 1250
Iteration (x1000)

1500

1750

2000

Figure 1. Training curves of two-layer SBN. Left: results using MNIST dataset. Right: results using Omniglot dataset.
MNIST (200-200-200-200-784)
LR

LR+C

LR+C+IDB

MuProp+C+IDB

Omniglot (200-200-200-200-784)
LEG

optimal

LR

LR+C

LR+C+IDB

MuProp+C+IDB

LEG

optimal

âˆ’110
Variational lower bound

Variational lower bound

âˆ’100

âˆ’110

âˆ’120

âˆ’130

âˆ’140

âˆ’130

âˆ’140

âˆ’120

0

250

500

750
1000 1250
Iteration (x1000)

1500

1750

2000

âˆ’150

0

250

500

750
1000 1250
Iteration (x1000)

1500

1750

2000

Figure 2. Training curves of four-layer SBN. Left: results using MNIST dataset. Right: results using Omniglot dataset.

the 33rd International Conference on Machine Learning,
2016.

Supplementary Material: Evaluating the Variance of Likelihood-Ratio Gradient Estimators

Table 1. Test results of gradient estimators. The learning rate with the best validation performance is used in each method. VB stands
for variational lower bound of the log likelihood, and LL stands for the log likelihood estimation.

MNIST (shallow)
VB
LL
LR
LR+C
LR+C+IDB
MuProp+C+IDB
LEG
optimal

-127.33
-107.21
-98.04
-99.96
-106.75
-97.64

-108.53
-97.90
-92.68
-94.23
-98.22
-92.55

MNIST (deep)
VB
LL
-119.93
-105.38
-94.10
-95.03
-103.26
-93.31

-103.53
-95.30
-89.02
-89.83
-93.26
-88.97

Omniglot (shallow)
VB
LL
-139.17
-122.10
-111.10
-112.97
-121.68
-110.60

-124.08
-113.87
-107.14
-108.28
-113.56
-106.90

Omniglot (deep)
VB
LL
-137.54
-123.27
-108.72
-109.55
-121.27
-108.17

-122.85
-114.27
-105.00
-105.52
-112.80
-104.85

