Supplementary Material
Random Feature Expansions for Deep Gaussian Processes

1

Kurt Cutajar

Edwin V. Bonilla

2

Pietro Michiardi

1

Maurizio Filippone

1

A. Additional Experiments
Using the experimental set-up described in Section 4, Figure 1 demonstrates how the competing models perform with
regards to the RMSE (or error rate) and MNLL metric when two hidden layers are incorporated into the competing models.
The results follow a similar progression to those reported in Figure 3 of the main paper. The DGP-ARC and DGP-RBF
models both continue to perform well after introducing this additional layer. However, the results for the regularized DNN
are notably inferior, and the degree of overfitting is also much greater. To this end, the MNLL obtained for the MNIST
dataset is not shown in the plot as it was vastly inferior to the values obtained using the other methods. DGP-EP was also
observed to have low scalability in this regard whereby it was not possible to obtain sensible results for the MNIST dataset
using this configuration.

REGRESSION
Powerplant
(n = 9568, d=4)
RMSE

0.5

Protein
(n = 45730, d=9)
RMSE

0.4
0.3
0.2
2

2.5

3

CLASSIFICATION

0.85

0.4

0.8

0.3

0.75

0.2

0.7

0.1

0.65

0

3.5

2

2.5

log10 (sec)

3.5

EEG
(n = 14979, d=14)
Error rate
0.4

0.2
0.1

0
1

1.5

2

2.5

3

3.5

2

log10 (sec)

2.5

MNLL

MNLL

3

3.5

log10 (sec)

3.5

0.2

0
2

2.5

3

3.5

1

1.5

2

2.5

3

3.5

2

log10 (sec)

log10 (sec)
dgp-rbf

MNLL

0.4

0
3

4.5

0.6

0.4

2

4

0.8

0.2
2.5

3.5

1

0.6

0.5

2

3

log10 (sec)

MNLL

4

1

0

log10 (sec)

1.2
1.1

MNIST
(n = 60000, d=784)
Error rate
0.3

0.2

log10 (sec)

MNLL

1

3

Spam
(n = 4061, d=57)
Error rate

dgp-arc

dgp-ep

2.5

3

log10 (sec)
dnn

3.5

0

3

3.5

4

4.5

log10 (sec)

var-gp

Figure 1. Progression of RMSE and MNLL over time for competing models. Results are shown for configurations having 2 hidden layers.
There is no plot for DGP-EP on MNIST because the model did not produce sensible results within the allocated time.

In Section 3.3, we outlined the different strategies for treating Ω, namely fixing them or treating them variationally, where
we observed that the constructed DGP model appears to work best when these are treated variationally while fixing the
randomness in their computation throughout the learning process (VAR - FIXED). In Figures 2 and 3, we compare these three
approaches on the complete set of datasets reported in the main experiments for one and two hidden layers, respectively.
Once again, we confirm that the performance obtained using the VAR - FIXED strategy yields more consistent results than the
alternatives. This is especially pertinent to the classification datasets, where the obtained error rate is markedly superior.
However, the variation of the model constructed with the ARC - COSINE kernel and optimized using VAR - FIXED appears to
be susceptible to some overfitting for higher dimensional datasets (SPAM and MNIST), which is expected given that we are
optimizing several covariance parameters (ARD). This would motivate trying to be variational about Θ too.
1
2

Department of Data Science, EURECOM, France
School of Computer Science and Engineering, University of New South Wales, Australia

Random Feature Expansions for Deep Gaussian Processes
REGRESSION
Powerplant
(n = 9568, d=4)
RMSE

0.5

CLASSIFICATION

Protein
(n = 45730, d=9)
RMSE

Spam
(n = 4061, d=57)
Error rate

EEG
(n = 14979, d=14)
Error rate
0.4

0.8

0.4

0.15

0.1

0.1

0.2

0.3

0.7

0.05

0.05
0

0.2
2

2.5

3

3.5

2

2.5

log10 (sec)

3

3.5

1

1.5

2

log10 (sec)

3

3.5

1.1

2

2.5

3

3.5

2

2.5

dgp-ep

3

3.5

0.6

0.6

0.4

0

3

3.5

4

4.5

log10 (sec)
MNLL
1
0.5

1

1.5

2

2.5

3

3.5

0

2

2.5

dgp-rbf-var-fixed

3

3.5

0

3

3.5

log10 (sec)

log10 (sec)

dgp-rbf-prior-fixed

3.5

0.2

log10 (sec)

dgp-rbf-var-resampled

3

MNLL

0.8

0.2

log10 (sec)

2.5

log10 (sec)

0.4
1

0

2

MNLL

1

1.2
0.5

2.5

log10 (sec)

MNLL

MNLL

1

MNIST
(n = 60000, d=784)
Error rate

dgp-arc-var-resampled

4

4.5

log10 (sec)

dgp-arc-prior-fixed

dgp-arc-var-fixed

var-gp

Figure 2. Progression of RMSE and MNLL over time for different optimisation strategies for DGP-ARC and DGP-RBF models. Results are
shown for configurations having 1 hidden layer.
REGRESSION
Powerplant
(n = 9568, d=4)
RMSE

0.5

CLASSIFICATION

Protein
(n = 45730, d=9)
RMSE

0.4
0.3
0.2

0.85

0.4

0.8

0.3

0.75

0.2

0.7

0.1

Spam
(n = 4061, d=57)
Error rate

EEG
(n = 14979, d=14)
Error rate

2.5

3

3.5

6

2.5

3

3.5

2
1

1.5

log10 (sec)

MNLL

2

2.5

3

3.5

2

2.5

log10 (sec)

MNLL

1.1

3

3.5

1
2

2.5

3

3.5

log10 (sec)
dgp-ep

dgp-rbf-var-resampled

2.5

3

3.5

0.3

0.6

0.4

0.2

log10 (sec)
dgp-rbf-prior-fixed

2

2.5

3

3.5

2

log10 (sec)
dgp-rbf-var-fixed

4.5

0.1

0.2
1.5

4

MNLL

0.6

1

3.5

MNLL

0.8

0.2
2

3

log10 (sec)

0.4
0

0

log10 (sec)

MNLL

1

1.2
0.5

4

0.2

0
2

log10 (sec)
1

8

0.4

0.65
2

MNIST
(n = 60000, d=784)
Error rate
·10−2

dgp-arc-var-resampled

2.5

3

3.5

log10 (sec)
dgp-arc-prior-fixed

0

3

3.5

4

4.5

log10 (sec)
dgp-arc-var-fixed

var-gp

Figure 3. Progression of RMSE and MNLL over time for different optimisation strategies for DGP-ARC and DGP-RBF models. Results are
shown for configurations having 2 hidden layers.

B. Comparison with MCMC
Figure 4 shows a comparison between the variational approximation and MCMC for a two-layer DGP model applied to
a regression dataset. The dataset has been generated by drawing 50 data points from N (y|h(h(x)), 0.01), with h(x) =
2x exp(−x2 ). We experiment with two different levels of precision in the DGP approximation by using 10 and 50 fixed
spectral frequencies, respectively, so as to assess the impact on the number of random features on the results. For MCMC,
covariance parameters are fixed to the values obtained by optimizing the variational lower bound on the marginal likelihood
in the case of 50 spectral frequencies.
We obtained samples from the posterior over the latent variables at each layer using MCMC techniques. In the case of a
Gaussian likelihood, it is possible to integrate out the GP at the last layer, thus obtaining a model that only depends on the
GP at the first. As a result, the collapsed DGP model becomes a standard GP model whose latent variables can be sampled
using various MCMC samplers developed in the literature of MCMC for GPs. Here we employ Elliptical Slice Sampling
(Murray et al., AISTATS, 2010) to draw samples from the posterior over the latent variables at the first layer, whereas latent
variables at the second can be sampled directly from a multivariate Gaussian distribution. More details on the MCMC
sampler are reported at the end of this section.
The plots depicted in Figure 4 illustrate how the

MCMC

approach explores two modes of the posterior of opposite sign.

Random Feature Expansions for Deep Gaussian Processes
MCMC

0

2

Variational − 50 RFF

0

●●
●
●●●●
●
●
●

●
●

●●

●

●

●
●
●
●●
●

●●
●
●●●●
●
●
●

●
●
●●
●●●●●●
●
●

●
●

●●

●

−10

●●
●
●●●●
●
●
●

●
●
●●
●●●●●●
●
●

●
●

●●

●
●
●
●

●
● ●
●●
●●
●
● ●

−5

●

●
●
●
●●
●

●
●
●
●

−1

Layer
Layer 22

1

−2

Layer
Layer 1 1

Variational − 10 RFF

0

10 −10

−5

●

●
●
●
●●
●

●
●
●●
●●●●●●
●
●

●
●
●
●

●
● ●
●●
●●
●
● ●

5

●

0

●
● ●
●●
●●
●
● ●

5

10 −10

−5

0

5

10

Figure 4. Comparison of MCMC and variational inference of a two-layer DGP with a single GP in the hidden layer and a Gaussian
likelihood. The posterior over the latent functions is based on 100 MCMC samples and 100 samples from the variational posterior.

This is due to the output function being invariant to the flipping of the sign of the weights at the two layers. Conversely, the
variational approximation over W accurately identifies one of the two modes of the posterior. The overall approximation is
accurate in the case of more random Fourier features, whereas in the case of less, the approximation is unsurprisingly characterized by out-of-sample oscillations. The variational approximation seems to result in larger uncertainty in predictions
compared to MCMC; we attribute this to the factorization of the posterior over all the weights.
B.1. Details of MCMC sampler for a two-layer DGP with a Gaussian likelihood
We give details of the MCMC sampler that we used to draw samples from the posterior over latent variables in DGPs. In the
experiments, we regard this as the gold-standard against which we compare the quality of the proposed DGP approximation
and inference. For the sake of tractability, we assume a two-layer DGP with a Gaussian likelihood, and we fix the hyperparameters of the GPs. Without loss of generality, we assume Y to be univariate and the hidden layer to be composed of a
single GP. The model is therefore as follows:
( 
)
( 
)


p Y F (2) , λ
= N Y F (2) , λI


(
)
(
(
))


p F (2) F (1) , θ (1)
= N F (2) 0, K F (1) , θ (1)


(
)
(
(
))


p F (1) X, θ (0)
= N F (1) 0, K X, θ (0)
(
)
(
)
with λ, θ (1) , and θ (0) fixed. In the model specification above, we denoted by K F (1) , θ (1) and K X, θ (0) the covariance matrices obtained by applying the covariance function with parameters θ (1) , and θ (0) to all pairs of F (1) and X,
respectively.
Given that the likelihood is Gaussian, it is possible to integrate out F (2) analytically

( 
) ∫ ( 
) (
)



p Y F (1) , λ, θ (1) = p Y F (2) , λ p F (2) F (1) , θ (1) dF (2)
obtaining the more compact model specification:
( 
)

p Y F (1) , λ, θ (1)
= N

)
(

= N
p F (1) X, θ (0)

( 
(
)
)

Y 0, K F (1) , θ (1) + λI

))
(
(

F (1) 0, K X, θ (0)

For fixed hyper-parameters, these expressions reveal that the observations are distributed as in the standard GP regression
case, with the only difference that the covariance is now parameterized by GP distributed random variables F (1) . We can
interpret these variables as some sort of hyper-parameters, and we can attempt to use standard MCMC methods to samples
from their posterior.
In order to develop a sampler for all latent variables, we factorize their full posterior as follows:



)
) (
)
(
(



p F (2) , F (1) Y, X, λ, θ (1) , θ (0) = p F (2) Y, F (1) , λ, θ (1) p F (1) Y, X, λ, θ (1) , θ (0)
which suggest a Gibbs sampling strategy to draw samples from the posterior where we iterate

Random Feature Expansions for Deep Gaussian Processes


(
)
1. sample from p F (1) Y, X, λ, θ (1) , θ (0)

(
)
2. sample from p F (2) Y, F (1) , λ, θ (1)
Step 1. can be done by setting up a Markov chain with invariant distribution given by:


(
)
( 
) (
)



p F (1) Y, X, λ, θ (1) , θ (0) ∝ p Y F (1) , λ, θ (1) p F (1) X, θ (0)
We can interpret this as a GP model, where the likelihood now assumes a complex form because of the nonlinear way in
which the likelihood depends on F (1) . Because of this interpretation, we can attempt to use any of the samplers developed
in the literature of GPs to obtain samples from the posterior over latent variables in GP models.
Step 2. can be done directly given that the posterior over F (2) is available in closed form and it is Gaussian:

)
(

(
)−1
(
)−1
)
(

(1)
(1)
(1)
(1)
(1)
(1)
(1)
(2)  (1)
(2) 
K + λI
Y, K − K
K + λI
K
= N F K
p F Y, F , λ, θ
where we have defined

(
)
K (1) := K F (1) , θ (1)

C. Derivation of the lower bound
For the sake of completeness, here is a detailed derivation of the lower bound that we use in variational inference to learn
the posterior over W and optimize Θ, assuming Ω fixed:
[∫
]
log[p(Y |X, Ω, Θ)] = log
p(Y |X, W, Ω, Θ)p(W)dW
[∫
]
p(Y |X, W, Ω, Θ)p(W)
= log
q(W)dW
q(W)
[
]
p(Y |X, W, Ω, Θ)p(W)
= log Eq(W)
q(W)
( [
])
p(Y |X, W, Ω, Θ)p(W)
≥ Eq(W) log
q(W)
])
( [
p(W)
= Eq(W) (log[p(Y |X, W, Ω, Θ)]) + Eq(W) log
q(W)
= Eq(W) (log[p(Y |X, W, Ω, Θ)]) − DKL[q(W)||p(W)]

D. Learning Ω variationally
Defining Ψ = {W, Ω}, we can attempt to employ variational inference to treat the spectral frequencies Ω variationally as
well as W. In this case, the detailed derivation of the lower bound is as follows:
[∫
]
log [p(Y |X, Θ)] = log
p(Y |X, Ψ, Θ)p(Ψ|Θ)dΨ
[∫
]
p(Y |X, Ψ, Θ)p(Ψ|Θ)
= log
q(Ψ)dΨ
q(Ψ)
]
[
p(Y |X, Ψ, Θ)p(Ψ|Θ)
= log Eq(Ψ)
q(Ψ)
( [
])
p(Y |X, Ψ, Θ)p(Ψ|Θ)
≥ Eq(Ψ) log
q(Ψ)
( [
])
p(Ψ|Θ)
= Eq(Ψ) (log[p(Y |X, Ψ, Θ)]) + Eq(Ψ) log
q(Ψ)
= Eq(Ψ) (log[p(Y |X, Ψ, Θ)]) − DKL[q(Ψ)||p(Ψ|Θ)]

Random Feature Expansions for Deep Gaussian Processes

Again, assuming a factorized prior over all weights across layers
p(Ψ|θ) =

N∏
h −1

p(Ω(l) |θ (l) )p(W (l) ) =

l=0

∏ ( (l) ) ∏ ( (l) )
q Ωij
q Wij ,
ijl

(1)

ijl

we optimize the variational lower bound using variational inference following the mini-batch approach with the reparameterization trick explained in the main paper. The variational parameters then become the mean and the variance of each of
the approximating factors
(
)
(
)
(l)
(l)
(l)
q Wij = N mij , (s2 )ij ,
(2)
(
)
(
)
(l)
(l)
(l)
q Ωij = N µij , (β 2 )ij ,
(3)
(l)

(l)

(l)

(l)

and we optimize the lower bound with respect to the variational parameters mij , (s2 )ij , µij , (β 2 )ij .

E. Expression for the DKL divergence between Gaussians
Given p1 (x) = N (µ1 , σ12 ) and p2 (x) = N (µ2 , σ22 ), the KL divergence between the two is:
[ ( 2)
]
1
σ2
σ12
(µ1 − µ2 )2
DKL (p1 (x)∥p2 (x)) =
log
−
1
+
+
2
σ12
σ22
σ22

F. Distributed Implementation
Our model is easily amenable to a distributed implementation using asynchronous distributed stochastic gradient descent
(Chilimbi et al., USENIX, 2014). Our distributed setting, based on TensorFlow, includes one or more Parameter servers
(PS), and a number of Workers. The latter proceed asynchronously using randomly selected batches of data: they fetch fresh
model parameters from the PS, compute the gradients of the lower bound with respect to these parameters, and push those
gradients back to the PS, which update the model accordingly. Given that workers compute gradients and send updates
to PS asynchronously, the discrepancy between the model used to compute gradients and the model actually updated can
degrade training quality. This is exacerbated by a large number of asynchronous workers, as noted in Chen et al., arXiv
1604.00981, (2016).
We focus our experiments on the MNIST dataset, and study how training time and error rates evolve with the number of
workers introduced in our system. The parameters for the model are identical to those reported for the previous experiments, except that we fix the number of Monte Carlo samples to one throughout.
0.1

0.6

8 · 10−2
6 · 10−2

0.4
4 · 10−2
0.2
0

Error Rate

Training time log10 (h)

MNIST

0.8

2 · 10−2
1

5

10

0

Workers
Training time

Error Rate

Figure 5. Comparison of training time and error rate for asynchronous DGP-RBF with 2 PS, and 1, 5 and 10 workers.

We report the results in Figure 5. The training time decreases in proportion to the number of workers, albeit sub-linearly,
whereas the error rate remains largely unchanged when varying the number of workers. When using a single PS, we
noticed that increasing the number of workers led to lower gains in speed of execution, and the behavior of the error rate

Random Feature Expansions for Deep Gaussian Processes

over iterations was more erratic than in the case of two PS. We attribute these effects to the greater overheads on the
communication cost when using a single PS, as well as the more involved handling of the coordination of multiple workers.
The work in Chen et al., arXiv 1604.00981, (2016) corroborates our findings, and motivates further work in the direction
of alleviating this issue.

