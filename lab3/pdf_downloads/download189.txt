Stochastic Variance Reduction Methods for Policy Evaluation

A Eigen-analysis of G

A.2 Analysis of eigenvectors

In this section, we give a thorough analysis of the spectral
properties of the matrix
"
#
1/2 bT
⇢I
A
G = 1/2 b
,
(20)
b
A
C

If the matrix G is diagonalizable, then it can be written as

which is critical in analyzing the convergence of the PDBG,
SAGA and SVRG algorithms for policy evaluation. Here
= w / ✓ is the ratio between the dual and primal step
sizes in these algorithms. For convenience, we use the following notation:
b
A),
bT C
b 1 A).
b
µ , min (A

L,

bT C
b

max (A

1

G = Q⇤Q

Diagonalizability of G

First, we examine the condition of that ensures the diagonalizability of the matrix G. We cite the following result
from (Shen et al., 2008).
Lemma 1. Consider the matrix A defined as

A
B>
A=
,
B
C

(21)

where A ⌫ 0, C 0, and B is full rank. Let ⌧ = minp
(C),
= max (A) and = max (B > C 1 B). If ⌧ > +2 ⌧
holds, then A is diagonalizable with all its eigenvalues real
and positive.
Applying this lemma to the matrix G in (20), we have
⌧=
=

min (

b =
C)

max (⇢I)

b

min (C),

= ⇢,
1/2 b>
b 1 1/2 A
b = max (A
b> C
b 1 A).
b
= max
A ( C)
p
The condition ⌧ > + 2 ⌧ translates into
q
b
b
b> b 1 A),
b
(
C)
>
⇢
+
2
min
min (C) max (A C
which can be solved as
q
q
>C
1 A)+
b
b
b
p
(
A
⇢+
max
q
>
b
min (C)

b> C
b

max (A

In the rest of our discussion, we choose to be
⇣
⌘
b> C
b 1A
b
8 ⇢ + max A
8(⇢ + L)
=
=
,
b
b
min (C)
min (C)
which satisfies the inequality above.

1 A)
b

Theorem 4 (Theorem 5.1.1 of Gohberg et al. (2006)). Suppose G is diagonalizable. If H is a symmetric positive definite matrix and HG is symmetric, then there exist a complete set of eigenvectors of G, such that they are orthonormal with respect to the inner product induced by H:
Q> HQ = I.

(22)

(23)

If H satisfies the conditions in Theorem 4, then we have
H = Q > Q 1 , which implies (H) = 2 (Q). Therefore,
in order to bound (Q), we only need to find such an H
and analyze its conditioning. To this end, we consider the
matrix of the following form:
"
p > #
b
(
⇢)I
A
p
H=
.
(24)
b
b
A
C
I
It is straightforward to check that HG is a symmetric matrix. The following lemma states the conditions for H being positive definite.
b
b b>
Lemma 2. If
⇢ > 0 and C
I
0,
⇢ AA
then H is positive definite.
Proof. The matrix
decomposition:
"
I
H= p b
A
⇢

H in (24) admits the following Schur
0
I

#

(

⇢)I
S

"

I
0

p

b>

⇢A

I

#

,

b
b b>
where S = C
I
⇢ AA . Thus H is congruence to
the block diagonal matrix in the middle, which is positive
definite under the specified conditions. Therefore, the matrix H is positive definite under the same conditions.
In addition to the choice of

.

,

where ⇤ is a diagonal matrix whose diagonal entries are the
eigenvalues of G, and Q consists of it eigenvectors (each
with unit norm) as columns. Our goal here is to bound
(Q), the condition number of the matrix Q. Our analysis is inspired by Liesen & Parlett (2008). The core is the
following fundamental result from linear algebra.

Under Assumption 1, they are well defined and we have
L µ > 0.
A.1

1

in (22), we choose to be

= 4(⇢ + L).

(25)

It is not hard to verify that this choice ensures
⇢ > 0 and
b
bA
b> 0 so that H is positive definite. We
C
I
A
⇢
now derive an upper bound on the condition number of H.
Let be an eigenvalue of H and [xT y T ]T be its associated
eigenvector, where kxk2 + kyk2 > 0. Then it holds that
p T
b y = x,
(
⇢)x +
A
(26)

Stochastic Variance Reduction Methods for Policy Evaluation

p

b +( C
b
Ax

I)y = y.

x=

bT y.
A

(27)

(b)

From (26), we have

+⇢

(29)

p + q = 0,

where

q,(

Therefore, we can upper bound the largest eigenvalue as
p
p + p2 4q
max (H) 
2
b
y T Cy
p=
⇢
+
kyk2
b
 ⇢ + max (C)
8(⇢ + L)
⇢+
b
min (C)

b
 8(⇢ + L)(C).

b
max (C)

(31)

Likewise, we can lower bound the smallest eigenvalue:
p
p
p2 4q
p p + 2q/p
q
=
min (H)
2
2
p
⇣
⌘
b
bA
bT y
y T Cy
yT A
(
⇢) kyk2
(
⇢)
kyk2
=
T
b
⇢ + ykykCy
2
⇣
⌘
T b
T b bT
y AA y
(
⇢) ykykCy
(
⇢)
(a)
2
kyk2
=

⇢

b
y T Cy
kyk2

(

⇢)

·

1
b
y T Cy
kyk2

⇢)
b
min (C)

3⇢ + 4L
2(⇢ + L)

◆

(32)

where step (a) uses the fact that both the numerator and
denominator are positive, step (b) uses the fact
L,

max

⇣

bT C
b
A

1

⌘
b
A

bA
bT y
yT A
,
b
y T Cy

and step (c) substitutes the expressions of and . Therefore, we can upper bound the condition number of H, and
thus that of Q, as follows:
b
8(⇢ + L)(C)
b
= 8(C).
⇢+L

(33)

A.3 Analysis of eigenvalues

bA
bT y
yT A
.
kyk2

We can verify that both p and q are positive with our choice
of and . The roots of the quadratic equation in (29) are
given by
p
p ± p2 4q
=
.
(30)
2

bA
bT y
yT A
b
y T Cy

⇢ + L,

2 (Q) = (H) 

b
yT ( C
I)y
⇢+
,
kyk2
b
yT ( C
I)y
⇢)
kyk2

=

✓

= (⇢ + L) 3

(28)

Substituting (28) into (27) and multiplying both sides with
y T , we obtain the following equation after some algebra

p,

(

L

(c)

p

Note that
+ ⇢ 6= 0 because if
+ ⇢ = 0 we have
bT y = 0 so that y = 0 since A
b is full rank. With y = 0 in
A
b = 0 so that x = 0, which contradicts
(27), we will have Ax
the assumption that kxk2 + kyk2 > 0.

2

⇢

Suppose is an eigenvalue of G and let ⇠ > , ⌘ >
corresponding eigenvector. By definition, we have


⇠
⇠
G
=
,
⌘
⌘

>

be its

which is equivalent to the following two equations:
p >
b ⌘ = ⇠,
⇢⇠
A
p
b + C⌘
b = ⌘.
A⇠

Solve ⇠ in the first equation in terms of ⌘, then plug into the
second equation, we obtain:
2

⌘

b + (A
bA
b> ⌘ + ⇢C⌘)
b = 0.
(⇢⌘ + C⌘)
2

Now left multiply ⌘ > , then divide by the k⌘k2 , we have:
2

p + q = 0.

where p and q are defined as
p,⇢+
q,

b
⌘ > C⌘
,
k⌘k2
bA
b> ⌘
⌘T A
k⌘k2

b
⌘ T C⌘
+⇢
k⌘k2

Therefore the eigenvalues of G satisfy:
p
p ± p2 4q
=
.
2

!

.

(34)

(35)

Recall that our choice of ensures that G is diagonalizable and has positive real eigenvalues. Indeed, we can verify that the diagonalization condition guarantees p2
4q

Stochastic Variance Reduction Methods for Policy Evaluation

so that all eigenvalues are real and positive. Now we can
obtain upper and lower bounds based on (35). For upper
bound, notice that
max (G)

p⇢+

8(⇢ + L)
b
min (C

=⇢+

b

max (C)

b
= 9(C)

bT C
b
⇢I + A

max

For lower bound, notice that
p
p
p2 4q
p
min (G)
⇣ T b2bT
⌘
⌘ AA ⌘
+
⇢
T
b
⌘ C⌘
=
2
⇢ ⌘k⌘k
T C⌘
b +

1

b .
A

(36)

p + 2q/p
= q/p
2

b
(⇢ + µ)
min (C)(⇢ + µ)
=
b +
b
⇢/ min (C)
⇢ + min (C)
(b) 8(⇢ + L)(⇢ + µ)
=
⇢ + 8(⇢ + L)
8
(⇢ + µ)
9
8
bT C
b 1 A))
b
= (⇢ + min (A
9
8
bT b 1 A),
b
=
(37)
min (⇢I + A C
9

where the second inequality is by the concavity property of
the square root function, step (a) used the fact
min

⇣

bT C
b
A

1

⌘
T b bT
b  y AA y ,
A
b
y T Cy

and step (b) substitutes the expressions of .

Since G is not a normal matrix, we cannot use their eigenvalue bounds to bound its condition number (G).

B

Linear convergence of PDBG

Recall the saddle-point problem we need to solve:
w

b
w A✓
>

The gradients of the Lagrangian with respect to ✓ and w,
respectively, are
b> w
A
b
b + bb.
A✓
Cw

1 >b
w Cw + bb> w.
2

The first-order optimality condition is obtained by setting
them to zero, which is satisfied by (✓? , w? ):
"
#

b> ✓?
0
⇢I
A
=
.
(39)
b
b
b
w?
b
A
C

The PDBG method in Algorithm 1 takes the following iteration:



✓m+1
✓m
0
✓
=
B(✓m , wm ),
wm+1
wm
0
w
where
B(✓, w) =
Letting


=



"
r✓ L(✓, w)
⇢I
= b
rw L(✓, w)
A

w/ ✓,


✓m+1
✓
= m
wm+1
wm

we have
"
✓

b>
A
b
C
b>
A
b
C

⇢I
b
A

#

#

✓m
wm



0
bb .

✓
w



0
bb

!

.

Subtracting both sides of the above recursion by (✓? , w? )
and using (39), we obtain
"
#


bT ✓m ✓?
✓m+1 ✓?
✓m ✓?
⇢I
A
=
✓
b
b wm w? .
wm+1 w?
wm w?
A
C
We analyze the convergence of the algorithms by examining the differences between the current parameters to the
optimal solution. More specifically, we define a scaled
residue vector

✓m ✓?
,
(40)
,
m
p1 (wm
w? )

m+1

where the Lagrangian is defined as
⇢
L(✓, w) = k✓k2
2

⌘
b> ✓? .
A

which obeys the following iteration:

min max L(✓, w),
✓

⇣
bb

rw L (✓, w) =

(a)

µ,

1

r✓ L (✓, w) = ⇢✓

b
max (C)

b
= ⇢ + 8(⇢ + L)(C)
b ⇢+L
 9(C)

b
w? = C

(38)

b is positive definite and A
b has full
Our assumption is that C
rank. The optimal solution can be expressed as
⇣
⌘ 1
b> C
b 1A
b + ⇢I
b> C
b 1bb,
✓? = A
A

= (I

✓ G)

m,

(41)

where G is exactly the matrix defined in (20). As analyzed in Section A.1, if we choose
sufficiently large,
such as in (22), then G is diagonalizable with all its eigenvalues real and positive. In this case, we let Q be the
matrix of eigenvectors in the eigenvalue decomposition
G = Q⇤Q 1 , and use the potential function
Pm , Q

1

2
m 2

Stochastic Variance Reduction Methods for Policy Evaluation

in our convergence analysis. We can bound the usual Euclidean distance by Pm as
k✓m

2

2

✓? k + kwm

2
max (Q)Pm .

w? k  (1 + )

If we have linear convergence in Pm , then the extra factor
2
(1 + ) max
(Q) will appear inside a logarithmic term.
Remark: This potential function has an intrinsic geometric
interpretation. We can view column vectors of Q 1 a basis
for the vector space, which is not orthogonal. Our goal is to
show that in this coordinate system, the distance to optimal
solution shrinks at every iteration.

C

Analysis of SVRG

Here we establish the linear convergence of the SVRG algorithm for policy evaluation described in Algorithm 2.
b bb and C:
b
Recall the finite sum structure in A,
n

X
b= 1
A
At ,
n t=1

1

n

= Q

1

= Q

1

(I

= kI

1

✓ ⇤) Q

✓ Q⇤Q

2
✓ ⇤k2

Q

2
✓ ⇤k2

Pm

1

2
m 2

Bt (✓, w) =

2
m 2

1

2
m 2

1

(42)

The inequality above uses sub-multiplicity of spectral
norm. We choose ✓ to be
1

=

✓

max

(⇤)

1

=

max (G)

(43)

,

Since all eigenvalues of G are real and positive, we have
kI

✓ ⇤k

2

=

✓



min (G)

1
1

max (G)

◆2

8
1
·
b
bT C
b
81 (C)(⇢I
+A

1 A)
b

!2

,

where we used the bounds on the eigenvalues max (G) and
min (G) in (36) and (37) respectively. Therefore, we can
achieve an ✏-close solution with
✓
✓ ◆◆
b
bT C
b 1 A)
b log P0
m = O (C)(⇢I
+A
✏
iterations of the PDBG algorithm.
In order to minimize kI
✓

=

✓ ⇤k,

max (G)

2
+

we can choose
min (G)

1X
Bt (✓, w),
n t=1

where

2
m 2

✓ G)

QQ

= (I
 kI

B(✓, w) =

2
m+1 2

n

X
b= 1
C
Ct .
n t=1

This structure carries over to the Lagrangian L(✓, w) as
well as the gradient operator B(✓, w), so we have

We proceed to bound the growth of Pm :
Pm+1 = Q

n

X
bb = 1
bt ,
n t=1

,

which results in kI
2/(1 + (⇤)) instead of
✓ ⇤k = 1
1 1/(⇤). The resulting complexity stays the same order.
The step sizes stated in Theorem 1 is obtained by replacing
max in (43) with its upper bound in (36) and setting w
through the ratio = w / ✓ as in (22).



⇢I
At

A>
t
Ct



✓
w



0
.
bt

(44)

Algorithm 2 has both an outer loop and an inner loop. We
use the index m for the outer iteration and j for the inner
iteration. Fixing the outer loop index m, we look at the
inner loop of Algorithm 2. Similar to full gradient method,
we first simplify the dynamics of SVRG.



✓
✓m,j+1
✓
✓
= m,j
⇥ B(✓m 1 , wm 1 )
wm,j+1
wm,j
w
◆
+ Btj (✓m,j , wm,j ) Bt (✓m 1 , wm 1 )


✓
✓
= m,j
wm,j
w
"
#

>
b
0
⇢I
A
✓m 1
⇥
b
b
b
w
b
A
C
m 1



✓m,j
0
⇢I
A>
t
+
wm,j
bt
At
Ct


 !
✓m 1
0
⇢I
A>
t
+
.
wm 1
bt
At
Ct
Subtracting (✓? , w? ) from both sides and using the optimality condition (39), we have



✓m,j+1 ✓?
✓m,j ✓?
✓
=
wm,j+1 w?
wm,j w?
w
"
#
>
b
⇢I
A
✓m 1 ✓?
⇥
b
b
wm 1 w?
A
C


✓m,j ✓?
⇢I
A>
t
+
wm,j w?
At
Ct
!


✓m 1 ✓?
⇢I
A>
t
.
wm 1 w?
At
Ct

Stochastic Variance Reduction Methods for Policy Evaluation

Multiplying
p both sides of the above recursion by
diag(I, 1/ I), and using a residue vector m,j defined
similarly as in (40), we obtain
m,j+1

=

✓ (G

m,j

= (I

✓ G)

+

+ Gt j

m,j

Gt j

m 1)

m,j

G

✓

m 1

Gt j (

(45)

m 1) ,

m,j

where Gtj is defined in (18).
For SVRG, we use the following potential functions to facilitate our analysis:
h
i
2
Pm , E Q 1 m
,
(46)
h
i
2
Pm,j , E Q 1 m,j
.
(47)
Unlike the analysis for the batch gradient methods, the nonorthogonality of the eigenvectors will lead to additional dependency of the iteration complexity on the condition number of Q, for which we give a bound in (33).
Multiplying both sides of Eqn. (45) by Q 1 , taking squared
2-norm and taking expectation, we obtain
h
⇥
Pm,j+1 = E Q 1 (I
✓ G) m,j
⇤ 2i
+ ✓ G Gtj ( m,j
)
m 1
h
i
(a)
2
1
= E (I
✓ ⇤) Q
m,j
h
i
2
+ ✓2 E Q 1 G Gtj ( m,j
m 1)
h
i
(b)
2
2
1
 kI
✓ ⇤k E Q
m,j
h
i
2
+ ✓2 E Q 1 Gtj ( m,j
m 1)
(c)

2
✓ ⇤k

= kI

2
✓

+

E

h

Pm,j
1

Q

Gt j (

m 1)

m,j

2

i

Q

1

2

Gt j

=

T

GTtj Q T Q 1 Gtj



max (Q

T

Q

1

)

T

GTtj Gtj .

Therefore, we can bound the expectation as
⇥

E Q


1

max (Q

Gt j
T

Q

2⇤

1

)E

⇥

T

GTtj Gtj

⇤

max (Q

T

Q

1

)E



max (Q

T

Q

1

)L2G E

T

Q

1

)L2G E

⇤

T

E[GTtj Gtj ]
⇥ T ⇤
⇥

QT QQ 1
⇥
= max (Q T Q 1 ) max (QT Q)L2G E T Q
⇥
⇤
(Q)2 L2G E kQ 1 k2 ,
=

max (Q

T

T

Q

⇤

T

Q

⇤

1

(49)

where in the second inequality we used the definition of L2G
in (18), i.e., L2G = kE[GTtj Gtj ]k. In addition, we have
⇥
E kQ

1

⇤
⇥
k2 =E Q 1 (
⇥
2 E Q 1

m,j

2⇤

m,j

= 2Pm,j + 2Pm

1.

m 1)

2⇤

⇥
+2E Q

1

m 1

2⇤

Then it follows from (48) that
2
✓ ⇤k Pm,j

Pm,j+1 kI

2 2
2
✓  (Q)LG (Pm,j

+2

+ Pm

1 ).

Next, let max and min denote the largest and smallest
diagonal elements of ⇤ (eigenvalues of G), respectively.
Then we have
2
✓ ⇤k

kI

= max (1

✓ min )

1
1

2

✓ min

+

2

✓ min

+

2

, (1

✓ min )

2

2 2
✓ max
2 2
2
✓  (Q)LG ,

where the last inequality uses the relation
2
2
max  kGk

= kEGt k2  kEGTt Gt k = L2G  2 (Q)L2G .

It follows that
Pm,j+1  1

2

✓ min

2 2
✓

+

(Q) L2G Pm,j

+ 2 ✓2 2 (Q) L2G (Pm,j + Pm 1 )
⇥
⇤
= 1 2 ✓ min + 3 ✓2 2 (Q) L2G Pm,j

. (48)

where step (a) used the facts that Gtj is independent of
m,j and
m 1 and E[Gtj ] = G so the cross terms are
zero, step (b) used again the same independence and that
the variance of a random variable is less than its second
moment, and step (c) used the definition of Pm,j in (47).
To bound the last term in the above inequality, we use the
simple notation = m,j
m 1 and have

⇥

=

+2

If we choose

✓

2
✓

2 2
2
✓  (Q) LG

Pm,j+1  (1

1

to satisfy
0<

then 3

2 (Q) L2G Pm

<

✓



min

32 (Q) L2G

✓ min ,

(50)

,

which implies

✓ min ) Pm,j

+2

2
✓

2 (Q) L2G Pm

Iterating the above inequality over j = 1, · · · , N
using Pm,0 = Pm 1 and Pm,N = Pm , we obtain
Pm = Pm,N

N
 1 ✓ min +2

2 2
2
✓  (Q)LG

N
X1
j=0

1

1 and

j
✓ min

1.

Pm

1

Stochastic Variance Reduction Methods for Policy Evaluation

=

=





N

1

✓ min

N

1

✓ min
N

1

(1 ✓ min )N
Pm
1 (1 ✓ min )
2 2 2 (Q) L2G
+ ✓
Pm 1
2 2
21
✓  (Q)LG

+2

✓ min

+

2

✓ min
2
2
✓  (Q) LG

Pm

min

(51)

1.

1

at m-th iteration, gt is computed using ✓ m
and w
t
this definition, m
t has the following dynamics:
(
m
if tm 6= t,
t
m+1
=
t
m if tm = t.

=

min

52 (Q)L2G

,

1

N=

=

5

✓ min

1

Pm  (e

(Q)L2G
,
2
min

B=
(52)

+ 2/5)Pm

1

 (4/5)Pm

1.

There are many other similar choices, for example,
=

(53)

n

2

which satisfies the condition in (50) and results in

✓

. With

We can write the m-th iteration’s full gradient as

We can choose
✓

m
t

min
,
32(Q)L2G

N=

3

=

9

✓ min

2

(Q)L2G
,
2
min

which results in
3

Pm  (e

+ 2/3)Pm

1

 (3/4)Pm

1.

These results imply that the number of outer iterations
needed to have E[Pm ]  ✏] is log(P0 /✏). For each outer
iteration, the SVRG algorithm need O(nd) operations to
compute the full gradient operator B(✓, w), and then N =
O(2 (Q)L2G / 2min ) inner iterations with each costing O(d)
operations. Therefore the overall computational cost is
✓✓
◆
✓ ◆◆
2 (Q) L2G
P0
O
n+
d
log
.
2
✏
min
Substituting (33) and (37) in the above bound, we get the
overall cost estimate
!
✓ ◆!
b 2
(C)L
P0
G
O
n+
d log
.
2 (⇢I + A
bT C
b 1 A)
b
✏
min

Finally, substituting the bounds in (33) and (37) into (52),
we obtain the ✓ and N stated in Theorem 2:
✓

=

N=

bT C
b
+A
b 2
48(C)L

min (⇢I

2
min

1

b
A)

,

G
b 2
512 (C)L
G
,
bT C
b 1 A)
b
(⇢I + A

which achieves the same complexity.

D Analysis of SAGA
SAGA in Algorithm 3 maintains a table of previously computed gradients. Notation wise, we use m
t to denote that

1X
Bt ✓
n t=1

m
t

,w

m
t

.

For convergence analysis, we define the following quantity:

✓ m
✓?
t
m ,
.
(54)
t
p1 (w m
w? )
t
Similar to (53), it satisfies the following iterative relation:
(
m
if tm 6= t,
t
m+1 =
t
if tm = t.
m
With these notations, we can express the vectors used in
SAGA as

n 
n 
1 X ⇢I
1X 0
✓ m
ATt
t
Bm =
,
w m
Ct
n t=1 At
n t=1 bt
t



0
⇢I
ATtm ✓m
ht m =
,
wm
bt m
At m
C tm



0
⇢I
ATtm ✓ m
t
g tm =
.
w m
b
At m
C tm
tm
t
The dynamics of SAGA can be written as



✓m+1
✓m
✓
=
(Bm + htm gtm )
wm+1
wm
w


✓
✓
= m
wm
w
( n 

n 
1 X ⇢I
1X 0
✓ m
ATt
t
+
w m
Ct
n t=1 At
n t=1 bt
t




⇢I ATtm ✓m
⇢I ATtm ✓
+
wm
w
At m C t m
At m C t m

m
tm
m
tm

)

Subtracting (✓? , w? ) from both sides, and using the optimality condition in (39), we obtain



✓m+1 ✓?
✓m ✓?
✓
=
wm+1 w?
wm w?
w
( n 

1 X ⇢I
✓ m
✓?
ATt
t
w m
w?
Ct
n t=1 At
t


⇢I ATtm ✓m ✓?
+
wm w?
At m C t m

Stochastic Variance Reduction Methods for Policy Evaluation



⇢I ATtm
At m C t m



✓
w

✓?
w?

m
tm
m
tm

p
Multiplying both sides by diag(I, 1/ I), we get
!
n
X
✓
Gt m
m+1 =
m
t
n t=1
⇣
⌘
m
.
✓ Gt m
m
tm

)

.

(55)

where Gtm is defined in (18).

For SAGA, we use the following two potential functions:
2

Pm = E Q 1 m 2 ,
 X
n
1
Qm = E
Q 1 Gt
nt=1

m
t

2
2


=E Q

1

Gt m

2
m
tm

2

.

The last equality holds because we use uniform sampling.
We first look at how Pm evolves. To simplify notation, let
!
n
⇣
⌘
X
✓
m
vm =
Gt m
+ ✓ Gt m
,
m
t
tm
n t=1
so that (55) becomes m+1 = m vm . We have
h
i
2
Pm+1 = E Q 1 m+1 2
h
i
2
= E Q 1 ( m vm )
h
i
2
2
>
=E Q 1 m 2 2 >
Q 1 vm + Q 1 vm 2
mQ
h
i
⇥
⇤
2
>
1
1
= Pm E 2 >
Q
Q
v
+
E
Q
v
.
m
m
m
2

where the inequality used min , min (⇤) = min (G) > 0,
which is true under our choice of
= w / ✓ in Section A.1. Next, we bound the last term of Eqn. (56):
h
i
2
E Q 1 vm 2

n
⇣
⌘⌘ 2
⇣ X
✓
m
=E Q 1
Gt m
+
G
✓
t
m
m
t
tm
n t=1
h
i
2
 2 ✓2 E Q 1 Gtm m 2

n
⇣1 X
⌘ 2
2
m
+ 2 ✓E Q 1
Gt m
G
t
m
t
tm
n t=1
h
i
h
i
2
2
 2 ✓2 E Q 1Gtm m 2 + 2 ✓2 E kQ 1 Gtm m
k
tm
h
i
2
2
1
2
= 2 ✓ E Q Gt m m 2 + 2 ✓ Q m ,
2

2

2

where the first inequality uses ka + bk2  2 kak2 + 2 kbk2 ,
and the second inequality holds because for any random
2
2
2
2
variable ⇠, E k⇠ E [⇠]k2 = E k⇠k2 kE⇠k2  E k⇠k2 .
Using similar arguments as in (49), we have
h
i
2
E Q 1Gtm m 2  2(Q)L2G Pm ,
(57)
Therefore, we have
Pm+1  1

+2

2

✓ min

2
✓ Qm .

+2

2 2
✓

(Q) L2G Pm
(58)

The inequality (58) shows that the dynamics of Pm depends
on both Pm itself and Qm . So we need to find another
iterative relation for Pm and Qm . To this end, we have
" n
#
2
1X
1
Qm+1 = E
Q Gt m+1
t
n t=1
2

Since m is independent of tm , we have
1
= E kQ 1 Gtm m+1 k2
h
i
h
i
tm
n
>
>
1
>
>
1
E 2 m Q Q vm = E 2 m Q Q Etm [vm ] ,
X
1
+
kQ 1 Gt m+1 k2
t
n
where the inner expectation is with respect to tm condit6=tm

tioned on all previous random variables. Notice that
1
(a)
= E kQ 1 Gtm m k2
n
n
⇥
⇤
1X
E t m Gt m m
=
Gt m
,
1 X
tm
t
n t=1
+
kQ 1 Gt m
k2
t
n
t6=tm

which implies Etm [vm ] = ✓ Etm [Gtm ] m = ✓ G m .
1
1
= E kQ 1 Gtm m k2
kQ 1 Gtm m
k2
Therefore, we have
tm
n
n
h
i
h
i
n
2
1X
Pm+1 = Pm E 2 ✓ Tm Q T Q 1 G m + E Q 1 vm 2
+
kQ 1 Gt m
k2
t
n t=1
h
i
h
i
2
= Pm E2 ✓ Tm Q T ⇤Q 1 m + E Q 1 vm 2
1
1
h
i
h
i
= E[kQ 1 Gtm m k2 ]
E[kQ 1 Gtm m
k2 ]
tm
2
2
1
1
n
n
 Pm 2 ✓ min E Q
+ E Q vm 2
m
 X
n
1
h
i
2
+
E
kQ 1 Gt m
k2
t
= (1 2 ✓ min )Pm + E Q 1 vm 2 ,
(56)
n t=1

Stochastic Variance Reduction Methods for Policy Evaluation

1
E[kQ
n


=

1

+ E kQ

1

1
E[kQ
n

2
mk ]

Gt m

Gt m

m
tm

1

Gt m

m
tm

k2 ]

k2

2
min
9 (2 (Q) L2G +n 2min )

1
n 1
= E[kQ 1 Gtm m k2 ] +
Qm
n
n
2
2
(b)  (Q)L
n 1
G

Pm +
Qm .
n
n

(59)

where step (a) uses (53) and step (b) uses (57).

T m = Pm +

 1
+

n

= 1
+

2

n

✓ min (1
✓ min )
Qm+1
2
2
 (Q)LG
2 ✓ min + 2 ✓2 2 (Q) L2G Pm + 2 ✓2 Qm
✓ 2
◆
2
n 1
✓ min (1
✓ min )  (Q)LG
P
+
Q
m
m
2 (Q)L2G
n
n
2 2
2
2 2
✓ min + 2 ✓  (Q) LG
✓ min Pm

✓ min

2

⇢) Pm +
(Q) L2G +(n 1) ✓ min (1
2 (Q)L2G
✓
n ✓ min (1
✓
⇢) Pm +
2 (Q)L2G

= (1
+

2
✓

= (1
+

✓

2

✓ min )

min )

Qm

(Q)L2G + (n⇢ 1) min (1
2 (Q)L2G

Tm+1  (1

2
✓

✓

(Q)L2G

Qm
◆

=

min

3 (2 (Q) L2G + n

2 )
min

min )

⇢)m P0 .

✓ min )

✓ min (G)

✓ min )

2 2
2
✓  (Q)LG

2

9

2
min
2
 (Q)L2G +

n

2
min

To achieve Pm  ✏, we need at most
◆
✓ ◆◆
✓✓
2 (Q) L2G
P0
m=O
n+
log
2
✏
min
iterations. Substituting (37) and (33) in the above bound,
we get the desired iteration complexity
!
✓ ◆!
b 2
(C)L
P0
G
O
n+
log
.
2 (⇢I + A
bT C
b 1 A)
b
✏
min

Qm

Finally, using the bounds in (33) and (37), we can replace
the step size in (61) by

Qm .
(60)

Next we show that with the step size
✓

2
.
3

⇢)Tm .

Pm  2(1

✓

+ (n⇢ 1) min (1
2 (Q)L2G

1=

Notice that Pm  Tm and Q0 = P0 . Therefore we have
T0  2P0 and

⇢) Tm
2

1
3

Therefore (60) implies

⇢=

2 2
2
✓  (Q) LG .

2 2
✓

1

2 )
min

Using (61), we have

Tm+1
2

n 2min
(Q) L2G +n

2

Qm .

The coefficient for Pm in the previous inequality can be
2 2
upper bounded by 1 ⇢ because 1 ⇢
⇢.
✓ min  1
Then we have

 (1

✓

✓ min )

Let’s define
⇢=

3 (2

(Q)L2G + (n⇢ 1) min (1
✓
2
2 ✓ 2 (Q)L2G
min (1
✓ min )
3
(6n 2) 3min
=
< 0.
2
9 ( (Q)L2G + n 2min )
2

✓ min (1
✓ min )
Qm .
2 (Q)L2G

2 2
2
✓  (Q) LG +(n 1) ✓ min (1
2 (Q)L2G

1

Then, it holds that

Now consider the dynamics of Tm . We have
Tm+1 = Pm+1 +

2
min
,
3 (2 (Q) L2G +n 2min )

⇢

which implies
n⇢

To facilitate our convergence analysis on Pm , we construct
a new Lyapunov function which is a linear combination of
Eqn. (58) and Eqn. (59). Specifically, consider
n

(or smaller), the second term on the right-hand side of (60)
is non-positive. To see this, we first notice that with this
choice of ✓ , we have

(61)

where µ⇢ =

=
3

2
min (⇢I

⇣

µ⇢
b 2
82 (C)L
G

bT C
b
+A

1

+ nµ2⇢

⌘,

b as defined in (14).
A)

.

