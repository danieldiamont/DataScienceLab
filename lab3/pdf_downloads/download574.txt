Differential Imitation Learning for Sequential Prediction

Appendix

A. A relation between AggreVaTeD with Natural Gradient and AggreVaTe with Weighted
Majority
First, we show that Weighted Majority can be leveraged for imitation learning in discrete MDPs. Then we extend Weighted
Majority to continuous MDPs, where we show that, with three steps of first-order approximations, WM leads to a natural
gradient update procedure.
A.1. Weighted Majority in Discrete MDPs
For notation simplicity, for each state s 2 S, we represent the policy ⇡(·|s) as a discrete probability vector ⇡ s 2 (A).
We also represent d⇡t as a S-dimension probability vector from S-d simplex, consisting of d⇡t (s), 8s 2 S. For each s, we
use Q⇤t (s) to denote the A-dimension vector consisting of the state-action cost-to-go Q⇤t (s, a) for all a 2 A. With this
PH P
notation, the loss function `n (⇡) from Eq. 1 can now be written as: `n (⇡) = H1 t=1 s2S d⇡t n (s)(⇡ s · Q⇤t (s)), where
a · b represents the inner product between vectors a and b. Weighted Majority updates ⇡ as follows: 8s 2 S,
⇡n+1 =

H
1 X X ⇡n
dt (s) ⇡ s · Q⇤t (s)
(A),8s2S H t=1

arg min
⇡s 2

s2S

+

X d¯⇡n (s)
⌘n,s

s2S

KL(⇡s k⇡ns ),

(16)

where KL(qkp) is the KL-divergence between two probability distributions q and p. This leads to the following closedform update:
⇡ s [i] exp
⌘n,s Q̃es [i]
s
⇡n+1
[i] = P|A|n
, i 2 [|A|],
s
⌘n,s Q̃es [j]
j=1 ⇡n [j] exp

(17)

PH
where Q̃es = t=1 d⇡t n (s)Q⇤t (s)/(H d¯⇡n (s)). We refer readers to (Shalev-Shwartz et al., 2012) or Appendix C for the
derivations of the above closed-form updates.
A.2. From Discrete to Continuous
We now consider how to update the parametrized policy ⇡✓ for continuous MDPs. Replacing summations by integrals,
Eq. 16 can be written as:
✓ = arg min
✓

H
1 X
E
H t=1 s⇠d⇡✓n
t

+

E⇡

s⇠d¯

✓n

E

a⇠⇡(·|s;✓)

[Q⇤t (s, a)]
(18)

KL(⇡✓ ||⇡✓n )/⌘n .

In order to solve for ✓ from Eq. 18, we apply several first-order approximations. We first approximate `n (✓) (the first part
of the RHS of the above equation) by its first-order Taylor expansion: `n (✓) ⇡ `n (✓n ) + r✓n `n (✓n ) · (✓ ✓n ). When ✓
and ✓n are close, this is a valid local approximation.
Second, we replace KL(⇡✓ ||⇡✓n ) by KL(⇡✓n ||⇡✓ ), which is a local approximation since KL(q||p) and KL(p||q) are equal
up to the second order (Kakade & Langford, 2002; Schulman et al., 2015).
Third, we approximate KL(⇡✓n ||⇡✓ ) by a second-order Taylor expansion around ✓n , such that we can approximate the
penalization using the Fisher information matrix:
E⇡

s⇠d¯

✓n

KL(⇡✓n ||⇡✓ ) ⇡ (1/2)(✓

✓n )T I(✓n )(✓

✓n ),

where the Fisher information matrix I(✓n ) = Es,a⇠d¯⇡✓n ⇡✓n (a|s) r✓n log(⇡✓n (a|s)) r✓n log(⇡✓n (a|s)

T

.

Differential Imitation Learning for Sequential Prediction

Inserting these three approximations into Eq. 18, and solving for ✓, we reach the following update rule ✓n+1 = ✓n
⌘n I(✓n ) 1 r✓ `n (✓)|✓=✓n , which is similar to the natural gradient update rule developed in (Kakade, 2002) for the RL
setting. Bagnell & Schneider (2003) provided an equivalent representation for Fisher information matrix:
I(✓n ) =

1
H2

E

⌧ ⇠⇢⇡✓

n

r✓n log(⇢⇡✓n (⌧ ))r✓n log(⇢⇡✓n (⌧ ))T ,

(19)

where r✓ log(⇢⇡⌧ (⌧ )) is the gradient of the log likelihood of the trajectory ⌧ which can be computed as
PH
t=1 r✓ log(⇡✓ (at |st )). In the remainder of the paper, we use this Fisher information matrix representation, which
yields much faster computation of the descent direction ✓ , as we will explain in the next section.

B. Derivation of Eq. 4
Starting from Eq. 1 with parametrized policy ⇡✓ , we have:
H
⇥
⇤
1 X
[Q⇤t (st , at )]
E⇡
E
H t=1 st ⇠d ✓n at ⇠⇡(·|st ;✓)
t
Z
H
⇥
⇤
1 X
=
⇡(a|st ; ✓)Q⇤t (st , a)da
E⇡
H t=1 st ⇠d ✓n a
t
Z
H
⇥
⇤
1 X
⇡(a|st ; ✓) ⇤
=
⇡(a|st ; ✓n )
Qt (st , a)da
E⇡
H t=1 st ⇠d ✓n a
⇡(a|st ; ✓n )

`n (✓) =

t

H
⇥
⇤
1 X
⇡(a|st ; ✓) ⇤
=
Q (st , a)
E
E
H t=1 st ⇠d⇡✓n a⇠⇡(·|st ;✓n ) ⇡(a|st ; ✓n ) t
t

H
h ⇡(a |s ; ✓)
i
1 X
t t
⇤
=
Q
(s
,
a
)
.
E
t
t
t
H t=1 st ⇠d⇡✓n ,at ⇠⇡(a|st ;✓n ) ⇡(at |st ; ✓n )

(20)

t

C. Derivation of Weighted Majority Update in Discrete MDP
We show the detailed derivation of Eq. 17 for AggreVaTeD with WM in discrete MDP. Recall that with KL-divergence as
the penalization, one update the policy in each episode as:
s
{⇡n+1
}s2S = arg

H
X d¯⇡n (s)
1 X X ⇡n
dt (s) ⇡ s · Q⇤t (s) +
KL(⇡s k⇡ns )
⌘
(A),8s} H
n,s
t=1

min

{⇡ s 2

s2S

s⇠S

0

Note that in the above equation, for a particular state s, optimizing ⇡ s is in fact independent of ⇡ s , 8s0 6= s. Hence the
optimal sequence {⇡ s }s2S can be achieved by optimizing ⇡ s independently for each s 2 S. For ⇡ s , we have the following
update rule:
H
1 X ⇡n
d¯⇡n (s)
dt (s)(⇡ s · Q⇤t (s)) +
KL(⇡s k⇡ns )
⌘
(A) H
n,s
t=1

s
⇡n+1
= arg smin
⇡ 2

= arg smin ⇡ s · (
⇡ 2 (A)

= arg smin ⇡ s · (
⇡ 2 (A)

H
X

d⇡t n (s)Q⇤t (s)/H) +

t=1

H
X

1
d⇡t n (s)Q⇤t (s)/(H d¯⇡n (s))) +
KL(⇡s k⇡ns )
⌘
n,s
t=1

= arg smin ⇡ s · Q̃e (s) +
⇡ 2 (A)

d¯⇡n (s)
KL(⇡s k⇡ns )
⌘n,s

A
1 X

⌘n,s

j=1

⇡ s [j](log(⇡ s [j])

log(⇡ns [j]))

(21)

Differential Imitation Learning for Sequential Prediction

Take the derivative with respect to ⇡ s [j], and set it to zero, we get:
Q̃e (s)[j] +

1
(log(⇡ s [j]/⇡ns [j]) + 1) = 0,
⌘n,s

(22)

this gives us:
⇡ s [j] = ⇡ns [j] exp( ⌘n,s Q̃e (s)[j]
Since ⇡ s 2

(23)

1).

(A), after normalization, we get:
⇡ s [j] exp( ⌘n,s Q̃e (s)[j])
⇡ s [j] = PA n
s
e
i=1 ⇡n [i] exp( ⌘n,s Q̃ (s)[i])

D. Lemmas

(24)

Before proving the theorems, we first present the Performance Difference Lemma (Kakade & Langford, 2002; Ross &
Bagnell, 2014) which will be used later:
Lemma D.1. For any two policies ⇡1 and ⇡2 , we have:
µ(⇡1 )

µ(⇡2 ) = H

H
X
t=1

⇥
Est ⇠d⇡t 1 Eat ⇠⇡1 (·|st ) [Q⇡t 2 (st , at )

⇤
Vt⇡2 (st )] .

(25)

We refer readers to (Ross & Bagnell, 2014) for the detailed proof of the above lemma.
The second known result we will use is the analysis of Weighted Majority Algorithm. Let us define the linear loss function
as `n (w) = w · yn , for any yn 2 Rd , and w 2 (d) from a probability simplex. Running Weighted Majority Algorithm
on the sequence of losses {w · yn } to compute a sequence of decisions {wn }, we have:
Lemma D.2. The sequence of decisions {wn } computed by running Weighted Majority with step size µ on the loss functions {w · yn } has the following regret bound:
N
X

n=1

w n · yn

min
⇤

w 2 (d)

N
X

n=1

w ⇤ · yn 

N
d
ln(d) µ X X
+
wn [i]yn [i]2 .
µ
2 n=1 i=1

(26)

We refer readers to (Shalev-Shwartz et al., 2012) for detailed proof.

E. Proof of Theorem 5.1
Proof. We construct a reduction from stochastic Multi-Arm Bandits (MAB) to the MDP M̃. A stochastic MAB is defined
by S arms denoted as I 1 , ..., I S . Each arm I t ’s cost ci at any time step t is sampled from a fixed but unknown distribution.
A bandit algorithm picks an arm It at iteration t and then receives an unbiased sample of the picked arm’s cost cIt . For any
bandit algorithm that picks arms I1 , I2 , ..., IN in N rounds, the expected regret is defined as:
E[RN ] = E[

N
X

n=1

c In ]

min

i2[S]

N
X

c̄i ,

(27)

n=1

where the expectation is taken with respect to the randomness of the cost sampling process and possibly the randomness
of the bandit algorithm. It has been shown
p that there exists a set of distributions from which the arms’ costs sampled from,
the expected regret E[RN ] is at least ⌦( SN ) (Bubeck et al., 2012).
Consider a MAB with 2K arms. To construct a MDP from a MAP, we construct a K + 1-depth binary-tree structure MDP
with 2K+1 1 nodes. We set each node in the binary tree as a state in the MDP. The number of actions of the MDP is two,
which corresponds to go left or right at a node in the binary tree. We associate each leaf nodes with arms in the original
MAB: the cost of the i’th leaf node is sampled from the cost distribution for the i’th arm, while the non-leaf nodes have

Differential Imitation Learning for Sequential Prediction

cost always equal to zero. The initial distribution ⇢0 concentrates on the root of the binary tree. Note that there are total
2K trajectories from the root to leafs, and we denote them as ⌧1 , ...⌧2K . We consider finite horizon (H = K + 1) episodic
RL algorithms that outputs ⇡1 , ⇡2 , ..., ⇡N at N episodes, where ⇡n is any deterministic policy that maps a node to actions
left or right. Any RL algorithm must have the following regret lower bound:
E[

N
X

n=1

µ(⇡n )]

min
⇤
⇡

N
X

µ(⇡ ⇤ )

p
⌦( SN ),

(28)

n=1

where the expectation is taken with respect to the possible randomness of the RL algorithms. Note that any deterministic
policy ⇡ identifies a trajectory in the binary tree when rolling out from the root. The optimal policy ⇡ ⇤ simply corresponds
to the trajectory that leads to the leaf with the mininum expected cost. Note that each trajectory is associated with an arm
from the original MAB, and the expected total cost of a trajectory
p corresponds to the expected cost of the associated arm.
Hence if there exists an RL algorithm that achieves regret O( SN ), then we can solve the original
p MAB problem by
simply running the RL algorithm on the constructed MDP. Since the lower bound for MAB is ⌦( SN ), this concludes
that Eq. 28 holds.

F. Proof of Theorem 5.2
Proof. For notation simplicity we denote al as the go-left action while ar is the go-right action. Without loss of generality,
we assume that the leftmost trajectory has the lowest total cost (e.g., s3 in Fig. 1 has the lowest average cost). We consider
the deterministic policy class ⇧ that contains all policy ⇡ : S ! {al , ar }. Since there are S states and 2 actions, the total
number of policies in the policy class is 2S . To prove the upper bound RN  O(log(S)), we claim that for any e  K,
at the end of episode e, AggreVaTe with FTL identifies the e’th state on the best trajectory, i,e, the leftmost trajectory
s0 , s1 , s3 , ..., s(2K 1 1) . We can prove the claim by induction.
At episode e = 1, based on the initial policy, AggreVaTe picks a trajectory ⌧1 to explore. AggreVaTe with FTL collects
the states s at ⌧1 and their associated cost-to-go vectors [Q⇤ (s, al ), Q⇤ (s, ar )]. Let us denote D1 as the dataset that
contains the state,cost-to-go pairs: D1 = {(s, [Q⇤ (s, al ), Q⇤ (s, al )])}, for s 2 ⌧1 . Since s0 is visited, the state-cost pair
(s0 , [Q⇤ (s0 , al ), Q⇤ (s0 , ar )]) must be in D1 . To update policy from ⇡1 to ⇡2 , AggreVaTe with FTL runs cost-sensitive
classification D1 as:
⇡2 = arg min
⇡

|D1 |

X

Q⇤ (sk , ⇡(sk )),

(29)

k=1

where sk stands for the k’th data point collected at dataset D1 . Due to the construction of policy class ⇧, we see that ⇡2
must picks action al at state s0 since Q(s0 , al ) < Q(s0 , ar ). Hence at the end of the episode e = 1, ⇡2 identifies s1 (i.e.,
running ⇡2 from root s0 leads to s1 ), which is on the optimal trajectory.
Now assume that at the end of episode n 1, the newly updated policy ⇡n identifies the state s(2n 1 1) : namely at the
beginning of episode n, if we roll-in ⇡n , the algorithm will keep traverse along the leftmost trajectory till at least state
s(2n 1 1) . At episode n, let Dn as the dataset contains all data points from Dn 1 and the new collected state, cost-togo pairs from ⌧n : Dn = Dn 1 [ {(s, [Q⇤ (s, al ), Q⇤ (s, ar )])}, for all s 2 ⌧n . Now if we compute policy ⇡n+1 using
cost-sensitive classification (Eq. 29) over Dn , we must learn a policy ⇡n+1 that identifies action al at state s(2j 1) , since
Qe (s(2j 1) , al ) < Q⇤ (s(2j 1) , ar ), and s(2j 1) is included in Dn , for j = 1, ..., n 1. Hence at the end of episode n, we
identify a policy ⇡n+1 such that if we roll in policy ⇡n+1 from s0 , we will traverse along the left most trajectory till we
reach s(2n 1) .
Hence by the induction hypothesis, at the end of episode K 1, ⇡K will reach state s(2K

1

1) ,

the end of the best trajectory.

Since AggreVaTe with FTL with policy class ⇧ identifies the best trajectory with at most K 1 episodes, the cumulative
regret is then at most O(K), which is O(log(S)) (assuming the average cost at each leaf is a bounded constant), as S is the
number of nodes in the binary-tree structure MDP M̃.

G. Proof of Theorem 5.3
Since in Theorem 5.3 we assume that we only have access to the noisy, but unbiased estimate of Q⇤ , the problem becomes
more difficult since unlike in the proof of Theorem 5.2, we cannot simply eliminate states completely since the cost-to-go

Differential Imitation Learning for Sequential Prediction

of the states queried from expert is noisy and completely eliminate nodes will potentially result elimination of low cost
K
trajectories. Hence here we consider a different policy representation. We define 2K deterministic base policies ⇡ 1 , ..., ⇡ 2 ,
such that rolling out policy ⇡ i at state s0 will traverse along the trajectory ending at the i’th leaf. We define the policy
P 2K
P 2K
class ⇧ as the convex hull of the base policies ⇧ = {⇡ : i=1 wi ⇡ i , i wi = 1, wi
0, 8i}. Namely each ⇡ 2 ⇧ is
a stochastic policy: when rolling out, with probability wi , ⇡ executepthe i’th base policy ⇡ i from s0 . Below we prove that
AggreVaTeD with Weighted Majority achieves the regret bound O( ln(S)N ).
Proof. We consider finite horizon, episodic imitation learning setting where at each episode n, the algorithm can
roll in the current policy ⇡n once and only once and traverses through trajectory ⌧n . Let us define `˜n (w) =
P
P 2K
1
e
j
e
s2⌧n
j=1 wj Q̃ (s, ⇡ (s)), where ⌧n is the trajectory traversed by rolling out policy ⇡n starting at s0 , and Q̃
K+1
is a noisy but unbiased estimate of Q⇤ . We simply consider the setting where Q̃e is bounded |Q̃e |  lmax (note that we
can easily extend our analysis to a more general case where Q̃e is from a sub-Gaussian distribution). Note that `˜n (w) is
simply a linear loss with respect to w:
`˜n (w) = w · qn ,

(30)

P
e
j
where qn [j] =
s2⌧n Q̃ (s, ⇡ (s))/(K + 1). AggreVaTeD with WM updates w using Exponential gradient descent.
Using the result from lemma D.2, we get:
N
X

(`˜n (wn )

`˜n (w⇤ )) =

n=1

=

N
X

n=1

K

(wn · qn

2
p
ln(2K ) µN lmax
+
 lmax ln(S)N .
µ
2

N 2
N
ln(2K ) µ X X
ln(2K ) µ X 2
w · qn ) 
+
wn [j]qn [j]2 
+
l
µ
2 n=1 j=1
µ
2 n=1 max
⇤

(31)

Note that S = 2K+1 1. The above inequality holds for any w⇤ 2 (2K ), including the we that corresponds to the
expert (i.e., we [1] = 1, we [i] = 0, i 6= 1 as we assumed without loss of generality the left most trajectory is the optimal
trajectory).
Now let us define `n (w) as follows:
K

K+1
2
XX
X
1
`n (w) =
d⇡t n (s)
wj Q⇤ (s, ⇡ j (s)).
K + 1 t=1
j=1

(32)

s⇠S

Note `n (w) can be understood as first rolling out ⇡n infinitely many times and then querying for the exact cost-to-go Q⇤ on
all the visited states. Clearly `˜n (w) is an unbiased estimate of `n (w): E[`˜n (w)] `n (w) = 0, where the expectation is over
the randomness of the roll-in and sampling procedure of Q̃e at iteration n, conditioned on all events among the previous
n 1 iterations. Also note that |`˜n (w) `n (w)|  2lmax , since `n (w)  lmax . Hence {`˜n (wn ) `n (wn )} is a bounded
martingale difference sequence. Hence by Azuma-Heoffding inequality, we get with probability at least 1
/2:
N
X

`n (wn )

`˜n (wn )  2lmax

`˜n (we )

`n (we )  2lmax

n=1

and with probability at least 1

p

log(2/ )N ,

(33)

log(2/ )N .

(34)

/2:
N
X

n=1

p

Combine the above inequality using union bound, we get with probability at least 1
N
X

n=1

(`n (wn )

`n (we )) 

N
X

n=1

(`˜n (wn )

`˜n (we )) + 4lmax

:
p

log(2/ )N .

(35)

Differential Imitation Learning for Sequential Prediction

Now let us apply the Performance Difference Lemma (Lemma D.1), we get with probability at least 1
N
X

n=1

µ(⇡n )

N
X

µ(⇡ ⇤ ) =

n=1

(K + 1) `n (wn )

n=1

rearrange terms we get:
N
X

N
X

N
X

µ(⇡n )

n=1

n=1

with probability at least 1

`n (we )  (K + 1)(lmax

µ(⇡ ⇤ )  log(S)lmax (

.

p

ln(S)N +

p

:

p
p
ln(S)N + 4lmax log(2/ )N ),

log(2/ )N )  O(ln(S)

p

ln(S)N ),

(36)

(37)

H. Proof of Theorem 5.4
The proof of theorem 5.4 is similar to the one for theorem 5.3. Hence we simply consider the infinitely many roll-ins and
exact query of Q⇤ case. The finite number roll-in and noisy query of Q⇤ case can be handled by using the martingale
difference sequence argument as shown in the proof of theorem 5.3.
Proof. Recall that in general setting, the policy ⇡ consists of probability vectors ⇡ s,t 2 (A), for all s 2 S and t 2 [H]:
⇡ = {⇡ s,t }8s2S,t2[H] . Also recall that the loss functions WM is optimizing are {`n (⇡)} where:
`n (⇡) =

H
H X
X
1 X X ⇡n
dt (s)(⇡ s,t · Q⇤t (s)) =
⇡ s,t · qns,t
H t=1
t=1
s2S

(38)

s2S

where as we defined before Q⇤t (s) stands for the cost-to-go vector Q⇤t (s)[j] = Q⇤t (s, aj ), for the j’th action in A, and
d⇡n (s)
qns,t = t H Q⇤t (s).
Now if we run Weighted Majority on `n to optimize ⇡ s,t for each pair of state and time step independently, we can get the
following regret upper bound by using Lemma D.2:
N
X

`n (⇡)

min

n=1

⇡

N
X

n=1

`n (⇡n ) 

H X
N
A
X
ln(A) µ X X s,t
+
⇡ [j]qns,t [j]2 .
µ
2
t=1
n=1 j=1

(39)

s2S

Note that we can upper bound (qns,t [j])2 as:
(qns,t [j])2 

d⇡t n (s)2 ⇤
d⇡t n (s) 2
2
(Q
)

(Qmax )2
max
H2
H2

(40)

Substitute it back, we get:
N
X

(`n (⇡n )

n=1

`n (⇡ ⇤ )) 

H X
N
A
X
ln(A) µ X X s,t
(Q⇤ )2
+
⇡ [j]d⇡t n (s) max
µ
2 n=1 j=1
H2
t=1
s2S

H
N
A
H
X
X
X
S ln(A) µ(Q⇤max )2 X X ⇡n
S ln(A) µ(Q⇤max )2
s,t
=
+
d
(s)
⇡
[j]
=
+
N
t
µ
2H 2 n=1
µ
2H 2
t=1
t=1
j=1
s2S

Q⇤ p
 max 2S ln(A)N ,
H
p
if we set µ = (Q⇤max )2 N S ln(A)/(2H 2 ).

(41)

Now let us apply the performance difference lemma (Lemma D.1), we get:
RN =

N
X

n=1

µ(⇡n )

N
X

n=1

µ(⇡ ⇤ ) = H

N
X

n=1

(`n (wn )

`n (we ))  HQemax

p

S ln(A)N .

(42)

Differential Imitation Learning for Sequential Prediction

I. Proof of Theorem 5.5
Let us use Q̃e (s) to represent the noisy but unbiased estimate of Q⇤ (s).
Proof. For notation simplicity, we denote S = {s1 , s2 , ..., sS }. We consider a finite MDP with time horizon H = 1. The
initial distribution ⇢0 = {1/S, ..., 1/S} puts 1/S weight on each state. We consider the algorithm setting where at every
episode n, a state sn 2 S is sampled from ⇢0 and the algorithm uses its current policy ⇡nsn 2 (A) to pick an action
a 2 A for sn and then receives a noisy but unbiased estimate Q̃e (sn ) of Q⇤ (sn ) 2 R|A| . The algorithm then updates its
n
sn
policy from ⇡ns to ⇡n+1
for sn while keep the other polices for other s unchanged (since the algorithm did not receive any
⇤
feedback regarding Q (s) for s 6= sn and the sample distribution ⇢0 is fixed and uniform). For expected regret E[RN ] we
have the following fact:
h

E

sn ⇠⇢0 ,8n

=
=

E

E

Q̃e (sn )⇠Psn ,8n

sn ⇠⇢0 ,8n
N
X

n=1

=E

E

si ⇠⇢0 ,in 1

N
⇥X
N
X

n=1

n

h

E

⇥

n

(⇡ns · Q⇤ (sn )

Q̃ei (si )⇠Psi ,in 1

s
⇤
E ⇡n · Q (s)

⇥

⇤i

⇡s⇤ · Q⇤ (s))

s⇠⇢0

s⇠⇢0

⇤i

⇡sen · Q⇤ (sn ))

s
⇤
E (⇡n · Q (s)

⇤
⇤
E ⇡s · Q (s)

s⇠⇢0

⇡s⇤n · Q̃e (sn ))

(⇡ns · Q̃e (sn )

e
n=1 Q̃i (si )⇠Psi ,in 1

E

n=1

=E

N
hX

N
⇥X

⇤

⇤i

µ(⇡ ⇤ )],

[µ(⇡n )

(43)

n=1

where the expectation in the final equation is taken with respect to random variables ⇡i , i 2 [N ] since each ⇡i is depend on
Q̃ej , for j < i and sj , for j < i.
⇥ PN
⇤
sn
e n
We first consider EQ̃e (sn )⇠Psn ,8n
⇡s⇤n · Q̃e (sn )) conditioned on a given sequence of s1 , ..., sN .
n=1 (⇡n · Q̃ (s )
Let us define that amongP
N episodes, the set of the index of the episodes that state si is sampled as Ni and its cardinality
S
as Ni , and we then have i=1 Ni = N and Ni \ Nj = ;,for i 6= j.
E

Q̃e (sn )⇠Psn ,8n

=

S X
X

N
⇥X

n=1

n

⇡s⇤n · Q̃e (sn ))

(⇡ns · Q̃e (sn )

E

e
i=1 j2Ni Q̃j (si )⇠Psi

(⇡jsi · Q̃ej (si )

⇤

⇡sei Q̃ej (si ))

(44)

Note that for each state si , at the rounds from Ni , we can think of the algorithm running any possible online linear
regression algorithm to compute the sequence of policies ⇡jsi , 8j 2 Ni for state si . Note that from classic online linear
regression analysis, we can show that for state si there exists a distribution Psi such that for any online algorithm:
p
⇥ X si
⇤
(⇡j · Q̃ej (si ) ⇡sei · Q̃ej (si ))
c ln(A)Ni ,
(45)
E
Q̃ej (si )⇠Psi ,8j2Ni

j2Ni

for some non-zero positive constant c. Substitute the above inequality into Eq. 44, we have:
E

Q̃e (sn )⇠Psn ,8n

N
⇥X

n=1

n

(⇡ns · Q̃e (sn )

⇡s⇤n · Q̃e (sn ))

Now let us put the expectation Esi ⇠⇢0 ,8i back, we have:
E

sn ⇠⇢

0 ,8n

h

E

Q̃e (sn )⇠Psn

N
⇥X

n=1

n

(⇡ns · Q̃e (sn )

⇤

S p
S
X
X
p
p
c ln(A)Ni = c ln(A)
Ni .
i=1

⇡s⇤n · Q̃e (sn ))|s1 , ..., sn

(46)

i=1

⇤i

c

p

ln(A)

N
X
i=1

E[

p

Ni ].

(47)

Differential Imitation Learning for Sequential Prediction

p
Note that each Ni is sampled from a Binomial distribution B(N, 1/S). To lower bound En⇠B(N,1/S) n, we use HoeffdPN
ing’s Inequality here. Note that Ni = n=1 an , where an = 1 if si is picked at iteration n and zero otherwise. Hence ai
is from a Bernoulli distribution with parameter 1/S. Using Hoeffding bound, for Ni /N , we get:
P (|Ni /N

1/S| <= ✏)

1

exp( 2N ✏2 ).

(48)

Let ✏ = 1/(2S), and substitute it back to the above inequality, we get:
p
p
p
P (0.5(N/S)  Ni  1.5(N/S)) = P ( 0.5(N/S)  Ni  1.5(N/S))
p
Hence, we can lower bound E[ Ni ] as follows:
p
p
E[ Ni ]
0.5N/S(1 exp( 2N/S 2 )).

exp( 2N/S 2 ).

1

(49)

(50)

Take N to infinity, we get:

lim E[

N !1

p
Ni ]

p
0.5N/S.

(51)

Substitute this result back to Eq. 47 and use the fact from Eq. 43, we get:
lim E[RN ] = lim

N !1

E

N !1 sn ⇠⇢0 ,8n

c
Hence we prove the theorem.

p

h

E

Q̃e (sn )⇠Psn ,8n

ln(A)S

p

N
⇥X

n=1

n

(⇡ns · Q̃e (sn )

0.5N/S = ⌦(

p

S ln(A)N ).

⇡s⇤n · Q̃e (sn ))

⇤i

c

p

ln(A)

S
X

E[

i=1

p

Ni ]

J. Details of Dependency Parsing for Handwritten Algebra
In Fig. 4, we show an example of set of handwritten algebra equations and its dependency tree from a arc-hybird sequence
slssslssrrllslsslssrrslssrlssrrslssrr. The preprocess step cropped individual symbols one by one from left to right and
from the top equation to the bottom one, centered them, scaled symbols to 40 by 40 images, and finally formed them as a
sequence of images.

(a) Handwritten algebra equations

(b) Dependency tree

Figure 4. An example of a set of handwritten algebra equations (a) and its corresponding dependency tree (b).

Since in the most common dependency parsing setting, there is no immediate reward at every parsing step, the reward-togo Q⇤ (s, a) is computed by using UAS as follows: start from s and apply action a, then use expert ⇡ ⇤ to roll out til the
end of the parsing process; Q⇤ (s, a) is the UAS score of the final configuration. Hence AggreVaTeD can be considered
as directly maximizing the UAS score, while previous approaches such as DAgger or SMILe (Ross et al., 2011) tries to
mimic expert’s actions and hence are not directly optimizing the final objective.

K. Additional Experiments on Partial Observable Setting
We test AggreVaTeD with Gated Recurrent Unit (GRU) based policies on a partially observable CartPole environment.
Again the expert has access to the full state while the observation excludes the velocity information of the cart.
Fig. 5 shows that even under partial observable setting, AggreVaTeD with RNN-based policies can also outperform suboptimal experts.

Differential Imitation Learning for Sequential Prediction

(a)
Figure 5. AggreVaTeD with GRU on the partial observable CartPole setting.

