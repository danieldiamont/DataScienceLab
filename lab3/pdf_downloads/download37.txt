Oracle Complexity of Second-Order Methods for Finite-Sum Problems

A. Proofs
A.1. Auxiliary Lemmas
The following lemma was essentially proven in (Lan, 2015; Nesterov, 2013), but we provide a proof for completeness:
Lemma 1. Fix Î±, Î² â‰¥ 0, and consider the following function on Rd :
Î±
F (w) =
8
and aÎºÌƒ =

âˆš
âˆšÎºÌƒ+3
ÎºÌƒ+1

where ÎºÌƒ =
2

Î±+Î²
Î²

w12

+

dâˆ’1
X

!
2

(wi âˆ’ wi+1 ) + (aÎºÌƒ âˆ’

1)wd2

âˆ’ w1

i=1

+

Î²
kwk2 ,
2

is the condition number of F . Then F is Î² strongly convex, (Î± + Î²)-smooth, and has a

3

unique minimum at (q, q , q , . . . , q d ) where q =

âˆš
âˆšÎºÌƒâˆ’1 .
ÎºÌƒ+1

Proof. The function is equivalent to
F (w) =

 Î²
Î±
w> Aw âˆ’ w1 + kwk2 ,
8
2

where
ï£«

2
ï£¬âˆ’1
ï£¬
ï£¬
A=ï£¬
ï£¬
ï£¬
ï£­

âˆ’1
2
âˆ’1

ï£¶
âˆ’1
..
.
..

.

..

.

2
âˆ’1

ï£·
ï£·
ï£·
ï£·.
ï£·
ï£·
âˆ’1ï£¸
aÎºÌƒ

Since A is symmetric, all its eigenvalues are real. Therefore, by Gershgorin circle theorem and the fact that aÎºÌƒ âˆˆ [1, 2]
(since ÎºÌƒ â‰¥ 1), we have that all the eigenvalues of A lie in [0, 4]. Thus, the eigenvalues of âˆ‡2 F = (Î±/4)A + Î²I lie in
[Î², Î± + Î²], implying that F is Î²-strongly convex and (Î± + Î²)-smooth.
It remains to compute the optimum of F . By differentiating F and setting to zero, we get that the optimum w must satisfy
the following set of equations:
ÎºÌƒ + 1
Â· w1 + 1 = 0
ÎºÌƒ âˆ’ 1
ÎºÌƒ + 1
wi+1 âˆ’ 2 Â·
Â· wi + wiâˆ’1 = 0 âˆ€ i = 2, . . . , d âˆ’ 1
ÎºÌƒ âˆ’ 1


4
wd âˆ’ wdâˆ’1 = 0.
aÎºÌƒ +
ÎºÌƒ âˆ’ 1
w2 âˆ’ 2 Â·

It is easily verified that this is satisfied by the vector (q, q 2 , q 3 , . . . , q d ), where q =
stationary point must be the unique global optimum of F .

âˆš
âˆšÎºÌƒâˆ’1 .
ÎºÌƒ+1

Since F is strongly convex, this

Lemma 2. For some q âˆˆ (0, 1) and positive d, define
(
g(z) =

q 2(z+1)
0

z<d
.
zâ‰¥d

Let l be a non-negative random variable, and suppose d â‰¥ 2E[l]. Then E[g(l)] â‰¥ 21 q 2E[l]+2 .
Proof. Since q âˆˆ (0, 1), the function z 7â†’ q z is convex for non-negative z and monotonically decreasing. Therefore, by
definition of g and Jensenâ€™s inequality, we have
E[g(l)] = Pr(l < d) Â· E[q 2(l+1) |l < d] + Pr(l â‰¥ d) Â· 0 â‰¥ Pr(l < d) Â· q E[2(l+1)] .
Using Markovâ€™s inequality to derive Pr(l < d) = 1 âˆ’ Pr(l â‰¥ d) â‰¥ 1 âˆ’

E[l]
d

â‰¥ 12 , concludes the proof.

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

A.2. Proof of Thm. 1
The proof is inspired by a technique introduced in (Woodworth and Srebro, 2016) for analyzing randomized first-order
methods, in which a quadratic function is â€œlocally flattenedâ€ in order to make first-order (gradient) information noninformative. We use a similar technique to make second-order (Hessian) information non-informative, hence preventing
second-order methods from having an advantage over first-order methods.
Given a (deterministic) algorithm and a bound T on the number of oracle calls, we construct the function F in the following
manner. We first choose some dimension d â‰¥ 2T . We then define
Îº=

âˆš
Îºâˆ’1
Âµ
,
, q=âˆš
8Î»
Îº+1

and choose r > 0 sufficiently small so that
T Âµr2
â‰¤ 1 and
8Î»

r

T Âµr2
1
â‰¤ qT .
16Î»
2

We also let v1 , . . . , vT be orthonormal vectors in Rd (to be specified later). We finally define our function as
F (w) = H(w) +

Î»
kwk2 ,
2

where
Î»(Îº âˆ’ 1)
H(w) =
8
aÎº =

âˆš
âˆšÎº+3 ,
Îº+1

2

hv1 , wi +

T
âˆ’1
X

!
Ï†r (hvi âˆ’ vi+1 , wi) + (aÎº âˆ’ 1)Ï†r (hvT , wi) âˆ’ hv1 , wi ,

i=1

and
ï£±
ï£´
ï£²0
Ï†r (z) = 2(|z| âˆ’ r)2
ï£´
ï£³ 2
z âˆ’ 2r2

|z| â‰¤ r
r < |z| â‰¤ 2r .
|z| > 2r

It is easy to show that Ï†r is 4-smooth and satisfies 0 â‰¤ z 2 âˆ’ Ï†r (z) â‰¤ 2r2 for all z.
First, we establish that F is indeed strongly convex and smooth as required:
Lemma 3. F as defined above is Î»-strongly convex and Âµ-smooth.

Proof. Since Ï†r is convex, and the composition of a convex and linear function is convex, we have that w 7â†’ Ï†r (hvi âˆ’
vi+1 , wi) are convex for all i, as well as w 7â†’ hv1 , wi2 and w 7â†’ Ï†r (hvT , wi). Therefore, H(w) is convex. As a result,
F is Î»-strongly convex due to the Î»2 kwk2 term. As to smoothness, note first that H(w) can be equivalently written as
HÌƒ(V w), where V is some orthogonal d Ã— d matrix with the first T rows equal to v1 , . . . , vT , and
Î»(Îº âˆ’ 1)
HÌƒ(x) =
8

x21 +

T
âˆ’1
X

!
Ï†r (xi âˆ’ xi+1 ) + (aÎº âˆ’ 1)Ï†r (xT ) âˆ’ x1

.

i=1

Therefore, âˆ‡2 F (w) = âˆ‡2 H(w) + Î»I = V > âˆ‡2 HÌƒ(V w)V + Î»I. It is easily verified that âˆ‡2 HÌƒ at any point (and in particular V w) is tridiagonal, with each element having absolute value at most 2Î»(Îº âˆ’ 1). Therefore, using the orthogonality

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

of V and the fact that (a + b)2 â‰¤ 2(a2 + b2 ),
sup x> âˆ‡2 F (w)x =
x:kxk=1

sup x> (V > âˆ‡2 HÌƒ(V w)V + Î»I)x
x:kxk=1

=

sup x> âˆ‡2 HÌƒ(V w)x + Î»
x:kxk=1

â‰¤

d
X

sup 2Î»(Îº âˆ’ 1)
x:kxk=1

â‰¤

sup 2Î»(Îº âˆ’ 1)
x:kxk=1

â‰¤

sup 4Î»(Îº âˆ’ 1)
x:kxk=1

i=1

x2i

+2

dâˆ’1
X

!
|xi xi+1 |

+Î»

i=1

dâˆ’1
X
(|xi | + |xi+1 |)2 + Î»
i=1
dâˆ’1
X

(x2i + x2i+1 ) + Î»

i=1

â‰¤ 8Î»(Îº âˆ’ 1) + Î» â‰¤ 8Î»Îº.
Plugging in the definition of Îº, this equals Âµ. Therefore, the spectral norm of the Hessian of F at any point is at most Âµ,
and therefore F is Âµ-smooth.
By construction, the function F also has the following key property:
Lemma 4. For any w âˆˆ Rd orthogonal to vt , vt+1 , . . . , vT (for some t âˆˆ {1, 2, . . . , T âˆ’ 1}), it holds that
F (w), âˆ‡F (w), âˆ‡2 F (w) do not depend on vt+1 , vt+2 , . . . , vT .
Proof. Recall that F is derived from H by adding a Î»2 kwk2 term, which clearly does not depend on v1 , . . . , vT . Therefore,
it is enough to prove the result for H(w), âˆ‡H(w), âˆ‡2 H(w). By taking the definition of H and differentiating, we have
that H(w) is proportional to
hv1 , wi2 +

T
âˆ’1
X

Ï†r (hvi âˆ’ vi+1 , wi) + (aÎº âˆ’ 1)Ï†r (hvT , wi) âˆ’ hv1 , wi,

i=1

âˆ‡H(w) is proportional to
2hv1 , wiv1 +

T
âˆ’1
X

Ï†0r (hvi âˆ’ vi+1 , wi)(vi âˆ’ vi+1 ) + (aÎº âˆ’ 1)Ï†0r (hvT , wi)vT âˆ’ v1 ,

i=1
2

and âˆ‡ H(w) is proportional to
2v1 v1> +

T
âˆ’1
X

Ï†00r (hvi âˆ’ vi+1 , wi)(vi âˆ’ vi+1 )(vi âˆ’ vi+1 )> + (aÎº âˆ’ 1)Ï†00r (hvT , wi)vT vT> .

i=1

By the assumption hvt , wi = hvt+1 , wi = . . . = hvT , wi = 0, and the fact that Ï†r (0) = Ï†0r (0) = Ï†00r (0) = 0, we
have Ï†r (hvi âˆ’ vi+1 , wi) = Ï†0r (hvi âˆ’ vi+1 , wi) = Ï†00r (hvi âˆ’ vi+1 , wi) = 0 for all i âˆˆ {t, t + 1, . . . , T }, as well as
Ï†r (hvT , wi) = Ï†0r (hvT , wi) = Ï†00r (hvT , bwi) = 0. Therefore, it is easily verified that the expressions above indeed do
not depend on vt+1 , . . . , vT .
With this lemma at hand, we now turn to describe how v1 , . . . , vT are constructed:
â€¢ First, we compute w1 (which is possible since the algorithm is deterministic and w1 is chosen before any oracle calls
are made).
â€¢ We pick v1 to be some unit vector orthogonal to w1 . Assuming v2 , . . . , vT will also be orthogonal to w1 (which will
be ensured by the construction which follows), we have by Lemma 4 that the information F (w1 ), âˆ‡F (w1 ), âˆ‡2 F (w1 )
provided by the oracle to the algorithm does not depend on {v2 , . . . , vT }, and thus depends only on v1 which was
already fixed. Since the algorithm is deterministic, this fixes the next query point w2 .

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

â€¢ For t = 2, 3, . . . , T âˆ’ 1, we repeat the process above: We compute wt , and pick vt to be some unit vectors orthogonal to w1 , w2 , . . . , wt , as well as all previously constructed vâ€™s (this is always possible since the dimension
is sufficiently large). By Lemma 4, as long as all vectors thus constructed are orthogonal to wt , the information
{F (wt ), âˆ‡F (wt ), âˆ‡2 F (wt )} provided to the algorithm does not depend on vt+1 , . . . , vT , and only depends on
v1 , . . . , vt which were already determined. Therefore, the next query point wt+1 is fixed.
â€¢ At the end of the process, we pick vT to be some unit vector orthogonal to all previously chosen vâ€™s as well as
w1 , . . . , wT .
Based on this construction, the following lemma is self-evident:
Lemma 5. It holds that hwT , vT i = 0.
Based on this lemma, we now turn to argue that wT must be a sub-optimal point. We first establish the following result:
Lemma 6. Letting w? = arg minw F (w), it holds that


r
T


T Âµr2
 ? X i 
q vi  â‰¤
w âˆ’


16Î»
i=1
where q =

âˆš
âˆšÎºâˆ’1 .
Îº+1

Proof. Let Fr denote F , where we make the dependence on the parameter r explicit. We first argue that
sup |Fr (w) âˆ’ F0 (w)| â‰¤
wâˆˆRd

T Âµr2
.
32

(7)

This is because
|Fr (w) âˆ’ F0 (w)| â‰¤

Î»(Îº âˆ’ 1)
8

T
âˆ’1
X

|Ï†r (hvi âˆ’ vi+1 , wi) âˆ’ Ï†0 (hvi âˆ’ vi+1 , wi)|

i=1

!
+ |Ï†r (hvT , wi) âˆ’ Ï†0 (hvT , wi)| ,
and since supzâˆˆR |Ï†r (z) âˆ’ Ï†0 (z)| = supzâˆˆR |Ï†r (z) âˆ’ z 2 | â‰¤ 2r2 , the above is at most
that Îº = Âµ/8Î», Eq. (7) follows.

Î»(Îºâˆ’1)
T r2
4

â‰¤

Î»Îº
2
4 Tr .

Recalling

Let wr = arg min Fr (w). By Î»-strong convexity of F0 and Fr ,
Î»
Î»
kwr âˆ’ w0 k2 , Fr (w0 ) âˆ’ Fr (wr ) â‰¥ kw0 âˆ’ wr k2 .
2
2
Summing the two inequalities and using Eq. (7),
F0 (wr ) âˆ’ F0 (w0 ) â‰¥

Î»kwr âˆ’ w0 k2 â‰¤ F0 (wr ) âˆ’ Fr (wr ) + Fr (w0 ) âˆ’ F0 (w0 ) â‰¤

T Âµr2
,
16

and therefore

T Âµr2
.
(8)
16Î»
By definition, wr = w? from the statement of our lemma, so it only remains to prove that w0 = arg min F0 (w) equals
PT
i
i=1 q vi . To see this, note that F0 (w) can be equivalently written as FÌƒ (V w), where V is some orthogonal d Ã— d matrix
with its first T rows equal to v1 , . . . , vT , and
!
T
âˆ’1
X
Î»(Îº âˆ’ 1)
Î»
2
2
2
FÌƒ (x) =
x1 +
(xi âˆ’ xi+1 ) + (aÎº âˆ’ 1)xT âˆ’ w1 + kxk2 .
8
2
i=1
kwr âˆ’ w0 k2 â‰¤

By an immediate corollary of Lemma 1, FÌƒ (Â·) is minimized at (q, q 2 , . . . , q T , 0, . . . , 0), where q =
PT
F (w) = FÌƒ (V w) is minimized at V > (q, q 2 , . . . , q T , 0, . . . , 0), which equals i=1 q i vi as required.

âˆš
âˆšÎºâˆ’1 ,
Îº+1

and therefore

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Note that this lemma also allows us to bound the norm of w? = arg min F (w), since it implies that

 r
T
X

T Âµr2


kw? k â‰¤ 
q i vi  +
,


16Î»
i=1

and since (a + b)2 â‰¤ 2a2 + 2b2 and q < 1, we have
 T
2
T
X

X
T Âµr2
T Âµr2

i 
kw k â‰¤ 2 
q vi  +
= 2
q 2i +


8Î»
8Î»
i=1
i=1
? 2

â‰¤ 2

âˆž
X

q 2i +

i=1

â‰¤
which is at most

âˆš

T Âµr2
2q 2
T Âµr2
+
=
8Î»
1 âˆ’ q2
8Î»

âˆš
2
T Âµr2
T Âµr2
+
= Îº+1+
,
1âˆ’q
8Î»
8Î»

âˆš
Îº + 2 â‰¤ 3 Îº, since we assume that c is sufficiently small so that

T Âµr 2
8Î»

â‰¤ 1, and that Îº = Âµ/8Î» â‰¥ 1.

The proof of the theorem follows by combining Lemma 5 and Lemma 6. Specifically, Lemma 5 (which states that
hwT , vT i = 0) and the fact that v1 , . . . , vT are orthonormal tells us that
2

2


2
!
T
âˆ’1
T
âˆ’1
T






X
X
X




i
T
i 
i 
q vi âˆ’ q vT  = wT âˆ’
q vi  + kq T vT k2
q v i  =  wT âˆ’
wT âˆ’






i=1

i=1

T

2

â‰¥ kq vT k = q

i=1

2T

,

and hence


T


X

i 
q vi  â‰¥ q T .
wT âˆ’


i=1

On the other hand, Lemma 6 states that


r
T


T Âµr2
 ? X i 
q vi  â‰¤
.
w âˆ’


16Î»
i=1
Combining the last two displayed equations by the triangle inequality, we get that
r
T Âµr2
?
T
kwT âˆ’ w k â‰¥ q âˆ’
.
16Î»
q
Âµr 2
By the assumption that c is sufficiently small so that T16Î»
â‰¤ 12 q T , the left hand side is at least 12 q T . Squaring both sides,
we get
1
kwT âˆ’ w? k2 â‰¥ q 2T ,
4
so by strong convexity of F ,
Î»
Î»
F (wT ) âˆ’ F (w? ) â‰¥ kwT âˆ’ w? k2 â‰¥ q 2T .
2
8
Plugging in the value of q, we get
âˆš
2T
Î»
Îºâˆ’1
?
âˆš
F (wT ) âˆ’ F (w ) â‰¥
.
8
Îº+1
âˆš
âˆš
On the other hand, we showed earlier that kw? k2 â‰¤ 3 Îº, so by smoothness, F (0) âˆ’ F (w? ) â‰¤ Âµ2 kw? k2 â‰¤ 3Âµ
Îº.
2
Therefore,
âˆš
2T
F (wT ) âˆ’ F (w? )
Î»
Îºâˆ’1
âˆš
âˆš
â‰¥
F (0) âˆ’ F (w? )
12Âµ Îº
Îº+1

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

To make the right-hand side less than , T must be such that
âˆš
2T
âˆš
Îºâˆ’1
12Âµ Îº
âˆš
â‰¤
,
Î»
Îº+1
which is equivalent to
2T Â· log
Since log

âˆš

âˆšÎº+1
Îºâˆ’1




= log 1 +

âˆš2
Îºâˆ’1



â‰¤

âˆš



Îº+1
Î»
âˆš
âˆš
â‰¥ log
.
Îºâˆ’1
12Âµ Îº

âˆš2 ,
Îºâˆ’1

it follows that T must be such that

4T
âˆš
â‰¥ log
Îºâˆ’1



Î»
âˆš
12Âµ Îº


.

Plugging in Îº = Âµ/8Î» and simplifying a bit, we get that
1
T â‰¥
4

r


Âµ
âˆ’ 1 Â· log
8Î»

âˆš

8(Î»/Âµ)3/2
12

!
,

from which the result follows.
A.3. Proof of Thm. 2
We will define a randomized choice of quadratic functions f1 , . . . , fn , and prove a lower bound on the expected optimization error of any algorithm (where the expectation is over both the algorithm and the randomized functions). This implies
that for any algorithm, the same lower bound (in expectation over the algorithm only) holds for some deterministic choice
of f1 , . . . , fn .
Therewill actually
separate
constructions, one leading to a lower bound of â„¦(n), and one leading to a lower bound
 be two
âˆš 
p nÂµ
(Î»/Âµ)3/2 n
of â„¦
. Choosing the construction which leads to the larger lower bound, the theorem follows.
Î» Â· log

A.3.1. A N â„¦(n) L OWER B OUND
Starting with the â„¦(n) lower bound, let Î´i , where i âˆˆ {1, . . . , n}, be chosen uniformly at random from {âˆ’1, +1}, and
define
Î»
fi (w) = âˆ’Î´i w1 + kwk2 .
2
Clearly, these are Î»-smooth (and hence Âµ-smooth) functions, as well as Î»-strongly convex. Also, the optimum of F (w) =

2
Pn
Pn
Âµ Pn
1
1
1
?
? 2
i=1 fi (w) equals w = nÎ»
i=1 Î´i e1 , where e1 is the first unit vector. As a result, kw k = Î»2 n
i=1 Î´i , so
n
by Î»-smoothness of F
!2
n
X
1
1
Î»
F (0) âˆ’ F (w? ) â‰¤ kw? k2 =
Î´i .
2
2Î» n i=1
 Pn

Since Î´i are i.i.d., we have by Hoeffdingâ€™s bound that with probability at least 3/4,  n1 i=1 Î´i  is at most
p
p
2 log(8/3)/n â‰¤ 2/n. Plugging into the equation above, we get that with probability at least 3/4,
F (0) âˆ’ F (w? ) â‰¤

1
.
Î»n

Turning to lower bound F (wT ) âˆ’ F (w? ), we have by strong convexity that
Î»
Î»
kwT âˆ’ w? k2 â‰¥ (wT,1 âˆ’ w1? )2
2
2
!2
n
1
1X
=
Î»wT,1 âˆ’
Î´i .
2Î»
n i=1

F (wT ) âˆ’ F (w? ) â‰¥

(9)

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Now, if at most bn/2c indices {1, . . . , n} were queried by the algorithm, then the wT returned by the algorithm must
be independent of at least dn/2e random variables Î´j1 , . . . , Î´jdn/2e (for some distinct indices j1 , j2 , . . . depending on the
algorithmâ€™s behavior, but independent of the values of Î´j1 , . . . , Î´jdn/2e ). Therefore, conditioned on j1 , . . . , jdn/2e and the
values of Î´j1 , . . . , Î´jdn/2e , the expression above can be written as
ï£«
1
1 ï£­
Î·âˆ’
2Î»
n

ï£¶2
X

Î´i ï£¸ ,

iâˆˆ{j
/ 1 ,...,jdn/2e }

where Î· is a fixed quantity independent of the values of Î´i for i âˆˆ
/ {j1 , . . . , jdn/2e }. By a standard anti-concentration
 0 2
c
c02
1
âˆš
= 2Î»n
for some universal positive c0 > 0.
argument, with probability at least 3/4, this expression will be at least 2Î»
n
Since this is true for any j1 , . . . , jdn/2e and Î´j1 , . . . , Î´jdn/2e , we get that with probability at least 3/4 over Î´1 , . . . , Î´n ,
F (wT ) âˆ’ F (w? ) â‰¥

c02
.
2Î»n

Combining this with Eq. (9) using a union bound, we have that with probability at least 1/2,
c02 Î»n
c02
F (wT ) âˆ’ F (w? )
â‰¥
=
.
F (0) âˆ’ F (w? )
2Î»n
2
As a result, since the ratio above is always a non-negative quantity,


F (wT ) âˆ’ F (w? )
c02
E
â‰¥
.
F (0) âˆ’ F (w? )
4
Using the assumption stated in the theorem (taking c = c02 /4), we have that the right hand side cannot be smaller than ,
unless more than bn/2c = â„¦(n) oracle calls are made.
A.3.2. A N â„¦

p

nÂµ
Î»

Â· log



(Î»/Âµ)3/2


âˆš

n



L OWER B OUND

p nÂµ
Î»
We now turn to prove the â„¦
lower bound, using a different function construction: Let j1 , . . . , jdâˆ’1 be
Î» Â· log 
chosen uniformly and independently at random from {1, . . . , n}, and define

!
dâˆ’1
Âµâˆ’Î» X
1
Î»
2
2
2
1jl =i (wl âˆ’ wl+1 ) +
w1 + (aÎº âˆ’ 1)wd âˆ’ w1
+ kwk2 .
(10)
fi (w) =
8
n
2
l=1

where 1A is the indicator of the event i. Note that these are all Î»-strongly convex functions, as all terms in their definition
are convex in w, and there is an additional Î»2 kwk2 term. Moreover, they are also Âµ-smooth: To see this, note that
âˆ‡2 fi (w)  (Âµâˆ’Î»)
4 A + Î»I  ÂµI, where A  4I is as defined in the proof of Lemma 1.
Pn
The average function F (w) = n1 i=1 fi (w) equals
!
dâˆ’1
X
Âµâˆ’Î»
Î»
F (w) =
w12 +
(wi âˆ’ wi+1 )2 + (aÎº âˆ’ 1)wd2 âˆ’ w1 + kwk2 ,
(11)
8n
2
i=1
Therefore, by Lemma 1, the
smoothness parameter of F is (Âµ âˆ’ Î»)/n + Î» â‰¤ Âµ, the global minimum w? of F equals
âˆš
Îºâˆ’1
2
d
(q, q , . . . , q ), where q = âˆšÎº+1 and
Âµâˆ’Î»
n

Âµ
+Î»
âˆ’1
= Î»
+ 1.
Î»
n
Note that since q < 1 and Îº â‰¥ 1, the squared norm of w? is at most

Îº=

d
X
i=1

q 2i â‰¤

âˆž
X
i=1

q 2i =

q2
1
â‰¤
=
2
1âˆ’q
1âˆ’q

âˆš

âˆš
Îº+1
â‰¤ Îº,
2

(12)

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

hence by smoothness,
F (0) âˆ’ F (w? ) â‰¤

Âµ ? 2
Âµâˆš
Îº.
kw k â‰¤
2
2

(13)

With these preliminaries out of the way, we now turn to compute a lower bound on the expected optimization error. The
proof is based on arguing that wT can only have a first few coordinates being non-zero. To see how this gives a lower
bound, let lT âˆˆ {1, . . . , d} be the largest index of a non-zero coordinate of wT (or 0 if wT = 0). By definition of w? , we
have
d
X
kwT âˆ’ w? k2 â‰¥
q 2i â‰¥ g(lT ),
i=lT +1

where
(
g(z) =

q 2(z+1)
0

z<d
.
zâ‰¥d

(14)

By strong convexity of F , this implies that
F (wT ) âˆ’ F (w? ) â‰¥

Î»
Î»
kwT âˆ’ w? k2 â‰¥ g(lT ).
2
2

Finally, taking expectation over the randomness of j1 , . . . , jdâˆ’1 above (and over the internal randomness of the algorithm,
if any), applying Lemma 2, and choosing the dimension d = d2E[lT ]e (which we will later show to equal the value specified
in the theorem), we have
Î»
Î»
E [F (wT ) âˆ’ F (w )] â‰¥ q 4E[lT ]+4 =
4
4
?

âˆš
2E[lT ]+2
Îºâˆ’1
âˆš
.
Îº+1

Combined with Eq. (13), this gives
F (wT ) âˆ’ F (w? )
E
F (0) âˆ’ F (w? )




Î»
âˆš
â‰¥
2Âµ Îº

âˆš
2E[lT ]+2
Îºâˆ’1
âˆš
.
Îº+1

(15)

Thus, it remains to upper bound E[lT ].
To get a bound, we rely on the following key lemma (where ei is the i-th unit vector, and recall that Wt defines the set of
allowed query points wt , and j1 , . . . , jd are the random indices used in constructing f1 , . . . , fn ):
Lemma 7. For all t, it holds that Wt âŠ† span{ed , e1 , e2 , e3 , . . . , e`t } for all t, where `t is defined recursively
as follows: `1 = 1, and `t+1 equals the largest number in {1, . . . , d âˆ’ 1} such that {j`t , j`t +1 , . . . , j`t+1 âˆ’1 } âŠ†
{it , itâˆ’1 , . . . , imax{1,tâˆ’bn/2c+1} } (and `t+1 = `t if no such number exists).
As will be seen later, `T (which is a random variable as a function of the random indices j1 , . . . , jd ) upper-bounds the
number of non-zero coordinates of wT , and therefore we can upper bound E[lT ] by E[`T ].
Proof. The proof is by induction over t. Since W1 = {0} âŠ† span(ed ), the result trivially holds for t = 1. Now, suppose
that Wt âŠ† span{ed , e1 , . . . , e`t } for some t and `t . Note that in particular, this means that wt is non-zero only in its first
`t coordinates. By definition of fi for any i,
!
dâˆ’1
X
1
Î»n(Îº âˆ’ 1)
2
1jl =i (wl âˆ’ wl+1 )(el âˆ’ el+1 ) + (2w1 e1 + 2(aÎº âˆ’ 1)wd ed âˆ’ e1 ) + Î»w
âˆ‡fi (w) =
8
n
l=1
!
dâˆ’1
Î»n(Îº âˆ’ 1) X
1
2
âˆ‡ fi (w) =
1jl =i (2El,l âˆ’ El+1,l âˆ’ El,l+1 ) + (2E1,1 + 2(aÎº âˆ’ 1)Ed,d ) + Î»I,
8
n
l=1

where Er,s is the d Ã— d which is all zeros, except for an entry of 1 in location (r, s). It is easily seen that these expressions
imply the following:

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

â€¢ If j`t 6= it , then âˆ‡fit (wt ) âˆˆ span{ed , e1 , . . . , e`t }, otherwise âˆ‡fit (wt ) âˆˆ span{ed , e1 , . . . , e`t +1 }.
â€¢ For any w and l âˆˆ {1, . . . , d âˆ’ 1}, if jl 6= i, then âˆ‡2 fi (w) is block-diagonal, with a block in the first l Ã— l entries. In
other words, any entry (r, s) in the matrix, where r â‰¤ l and s > l (or r > l and s â‰¤ l) is zero.
Pt
â€¢ As a result, if jl âˆˆ
/ {it , itâˆ’1 , . . . , imax{1,tâˆ’bn/2c+1} }, then Ï„ =max{1,tâˆ’bn/2c+1} Î±Ï„ âˆ‡2 fiÏ„ (wÏ„ ), for arbitrary scalars
Ï„ , is block-diagonal with a block in the first l Ã— l entries. The same clearly holds for any matrix with the same
block-diagonal structure.
Together, these observations imply that the operations specified in Assumption 1 can lead to vectors outside span{ed , e1 , . . . , e`t }, only if j`t âˆˆ {it , itâˆ’1 , . . . , imax{1,tâˆ’bn/2c+1} }. Moreover, these vectors must belong to span{ed , e1 , . . . , e`t+1 }, where `t+1 is as specified in the lemma: By definition, j`t+1 is not in
{it , itâˆ’1 , . . . , imax{1,tâˆ’bn/2c+1} }, and therefore all relevant Hessians have a block in the first `t+1 Ã— `t+1 entries, hence it
is impossible to create a vector with non-zero coordinates (using the operations of Assumption 1) beyond the first `t+1 .
Since wT âŠ† WT , the lemma above implies that E[lT ] from Eq. (15) (where lT is the largest index of a non-zero coordinate
of wT ) can be upper-bounded by E[`T ], where the expectation is over the random draw of the indices j1 , . . . , jdâˆ’1 . This
can be bounded using the following lemma:
Lemma 8. It holds that E[`T ] â‰¤ 1 +

2(T âˆ’1)
.
n

Proof. By definition of `t and linearity of expectation, we have
"T âˆ’1
#
T
âˆ’1
X
X
E[`t+1 âˆ’ `t ] + 1.
E[`T ] = E
(`t+1 âˆ’ `t ) + `1 =

(16)

t=1

t=1

Let us consider any particular term in the sum above. Since `t+1 âˆ’ `t is a non-negative integer, we have
E[`t+1 âˆ’ `t ] = Pr (`t+1 > `t ) Â· E [`t+1 âˆ’ `t | `t+1 > `t ] .
By definition of `t , the event `t+1 > `t can occur only if j`t âˆˆ
/ {itâˆ’1 , itâˆ’2 , . . . , imax{1,tâˆ’bn/2c} }, yet j`t âˆˆ
{it , itâˆ’1 , . . . , imax{1,tâˆ’bn/2c+1} }. This is equivalent to j`t = it (that is, in iteration t we happened to choose the index j`t of the unique individual function, which contains the block linking coordinate `t and `t + 1, hence allowing
us to â€œadvanceâ€ and have more non-zero coordinates). But since the algorithm is oblivious, it is fixed whereas j`t is
chosen uniformly at random, hence the probability of this event is 1/n. Therefore, Pr (`t+1 > `t ) â‰¤ 1/n. Turning
to the conditional expectation of `t+1 âˆ’ `t above, it equals the expected number of indices j`t , j`t +1 , . . . belonging to
{it , itâˆ’1 , . . . , imax{1,tâˆ’bn/2c+1} }, conditioned on j`t belonging to that set. But since the i indices are fixed and the j
indices are chosen uniformly at random, this equals one plus the expected number of times where a randomly drawn
j âˆˆ {1, . . . , n} belongs to {it , itâˆ’1 , . . . , itâˆ’bn/2c+1 }. Since this set contains at most bn/2c distinct elements in {1, . . . , n},
this is equivalent to (one plus) the expectation of a geometric random variable, where the success probability is at most
1/2
= 2. Plugging into the displayed equation above, we get that
1/2. By a standard derivation, this is at most 1 + 1âˆ’1/2
E[`t+1 âˆ’ `t ] â‰¤
and therefore the bound in Eq. (16) is at most

2(T âˆ’1)
n

1
2
Â·2 = ,
n
n

+ 1 as required.

Plugging this bound into Eq. (15), we get
F (wT ) âˆ’ F (w? )
E
F (0) âˆ’ F (w? )




Î»
âˆš
â‰¥
2Âµ Îº

âˆš
 4(Tnâˆ’1) +4
Îºâˆ’1
âˆš
.
Îº+1

To make the right-hand side less than , T must be such that
âˆš
 4(Tnâˆ’1) +4
âˆš
Îºâˆ’1
2Âµ Îº
âˆš
â‰¤
,
Î»
Îº+1

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

which is equivalent to


âˆš



4(T âˆ’ 1)
Îº+1
Î»
âˆš
+ 4 log âˆš
â‰¥ log
.
n
Îºâˆ’1
2Âµ Îº



âˆš
Îº+1
2
2
= log 1 + âˆšÎºâˆ’1
â‰¤ âˆšÎºâˆ’1
(see, e.g., Lemma 12 in (Arjevani and Shamir, 2016a)), it follows that T
Since log âˆšÎºâˆ’1
must be such that




2
Î»
4(T âˆ’ 1)
âˆš
+4 âˆš
â‰¥ log
.
n
2Âµ Îº
Îºâˆ’1


Plugging in Îº =

Âµ
Î» âˆ’1

n

+ 1, we get that
ï£«q Âµ

n

2

ï£¶

ï£«

Î» âˆ’1

n
T â‰¥ 1+ ï£­
4

Â· log ï£­

ï£¶

Î»

ï£¸ âˆ’ 4ï£¸ .
qÂµ
Î» âˆ’1
2Âµ
n +1

Using asymptotic notation the right-hand side equals


âˆš 
p
(Î»/Âµ)3/2 n
â„¦
n(Âµ/Î» âˆ’ 1) log
.

as required. The bound on the dimension d follows from the 
fact that we 
chose it to be
O(E[lT ]) = O(1 + T /n), and to
âˆš
p nÂµ
(Î»/Âµ)3/2 n
make the lower bound valid it is enough to pick some T = O
.
Î» Â· log

A.4. Proof of Thm. 3
Recall that the proof of Thm. 2 essentially shows that for any (possibly stochastic) index-oblivious optimization algorithm
there exists some â€˜badâ€™ assignment of the d âˆ’ 1 blocks j1 , . . . , jdâˆ’1 whose corresponding fi : Rd â†’ R (see Eq. (10))
form a functions which is hard-to-optimize. When considering non-oblivious (i.e., adaptive) algorithms this construction
fails as soon as the algorithm obtains the Hessians of all the individual functions (potentially, after n second-order oracle
queries). Indeed, knowing the Hessians of fi , one can devise an index-schedule which gains at least one coordinate at
every iteration, as opposed to 1/n on average for the oblivious case. Thus, in order to tackle the non-oblivious case, we
form a function over some D-dimensional space which â€˜containsâ€™ all the ndâˆ’1 sub-problems at one and the same time
(clearly, to carry out our plans we must pick D which grows exponentially fast with d, the dimension of the sub-problems).
This way, any index-schedule, oblivious or adaptive, must â€˜fitâ€™ all the ndâˆ’1 sub-problems well, and as such, bound to a
certain convergence rate which we analyze below.
Denote [n] = {1, . . . , n}, set D = ndâˆ’1 d and define for any j âˆˆ [n]dâˆ’1 the following,
fij

d

: R â†’ R,

Qj : RD â†’ Rd ,

Âµâˆ’Î»
w 7â†’
8
u 7â†’

d
X

dâˆ’1
X
l=1


!
1
Î»
2
2
1jl =i (wl âˆ’ wl+1 ) +
w1 + (aÎº âˆ’ 1)wd âˆ’ w1
+ kwk2 ,
n
2
2

u> e#jd+l

l=1

where #j enumerates the ndâˆ’1 tuples [n]dâˆ’1 from 0 to ndâˆ’1 âˆ’ 1. Note that fij are defined exactly as in Eq. (10), only here
we make the dependence on j explicit. The individual functions are defined as follows:
X
fi (u) =
fij (Qj u).
jâˆˆ[n]dâˆ’1

Note that,
âˆ‡2 fi (u) =

X
jâˆˆ[n]dâˆ’1

(Qj )> âˆ‡2 fij (Qj u)Qj .

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Since âˆ‡2 fi are block-diagonal, we have Î›(âˆ‡2 fi ) =
Thus, since
convex.

fij

S

j

Î›(âˆ‡2 fij ), where Î›(Â·) denotes the spectrum of a given matrix.

are Âµ-smooth and Î»-strongly convex (see proof of Thm. 2), we see that fi is also Âµ-smooth and Î»-strongly

As for the average function Î¦(u) =

1
n

Pn

i=1

fi (u), it is easily verified that for any fixed j âˆˆ [n]dâˆ’1 ,
n

1X j j
f (Q u) = F (Qj u),
n i=1 i
where F is as defined in Eq. (11). Thus,
Î¦(u) =

n
1X X
fij (Qj u) =
n i=1
dâˆ’1
jâˆˆ[n]

X

F (Qj u).

jâˆˆ[n]dâˆ’1

To compute the minimizer of Î¦, we compute the first-order derivative:
ï£«

ï£¶
X

âˆ‡Î¦(u) = âˆ‡ ï£­

F (Qj u)ï£¸

jâˆˆ[n]dâˆ’1



âˆ‡ F (Qj u)

X

=

jâˆˆ[n]dâˆ’1

X

=

(Qj )> âˆ‡F (Qj u).

jâˆˆ[n]dâˆ’1

Thus, by setting uâˆ— =

j > âˆ—
j (Q ) w ,

P

âˆ—

âˆ‡Î¦(u ) =

where wâˆ— is the minimizer of F as in Lemma 1, we get

X

j >



j

(Q ) âˆ‡F Q

jâˆˆ[n]dâˆ’1

X

j > âˆ—

(Q ) u



X

=

(Qj )> âˆ‡F (wâˆ— ) = 0.

jâˆˆ[n]dâˆ’1

j

âˆš
Note that, by Eq. (13), kuâˆ— k2 = ndâˆ’1 kwâˆ— k2 â‰¤ ndâˆ’1 Îº. Hence, by smoothness,
Î¦(0) âˆ’ Î¦(u? ) â‰¤

âˆš
Âµ ? 2
Âµ
ku k â‰¤ ndâˆ’1 Îº.
2
2

(17)

To derive the analytical properties of Î¦, we compute the second derivative:
âˆ‡2 Î¦(u) =

X

âˆ‡((Qj )> âˆ‡F (Qj u))

jâˆˆ[n]dâˆ’1

=

X

(Qj )> âˆ‡(âˆ‡F (Qj u))

jâˆˆ[n]dâˆ’1

=

X

(Qj )> âˆ‡2 F (Qj u)Qj .

jâˆˆ[n]dâˆ’1

Since âˆ‡2 Î¦ is a block-diagonal matrix, we have Î›(âˆ‡2 Î¦) =
is ((Âµ âˆ’ Î»)/n + Î»)-smooth and Î»-strongly convex.

S

j

Î›(âˆ‡2 F ) = Î›(âˆ‡2 F ). Thus, by Lemma 1, it follows that Î¦

With these preliminaries out of the way, we now turn to compute a lower bound on the expected optimization error. The
proof follows by arguing that uT can only have a first few coordinates being non-zero for each of the ndâˆ’1 sub-problems.
To see how this gives a lower bound, let lTj âˆˆ {1, . . . , d} be the largest index of a non-zero coordinate of Qj uT (or 0 if

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Qj uT = 0). By the definition of u? and by Eq. (12), we have
X
X
kuT âˆ’ uâˆ— k2 = k
(Qj )> Qj uT âˆ’
(Qj )> wâˆ— k2
j

=k

j

X

j >

(Q ) (Q uT âˆ’ wâˆ— )k2
j

j

=

X

â‰¥

X

kQj uT âˆ’ wâˆ— k2

j

g(lTj ),

j

where g is defined in Eq. (14). By the strong convexity of F , this implies that
Î»
Î»X j
Î¦(uT ) âˆ’ Î¦(u? ) â‰¥ kuT âˆ’ u? k2 â‰¥
g(lT ).
2
2
j

We now proceed along the same lines as in the proof of Thm. 2. First, to upper bound lTj (note that, g is monotonically
decreasing), we use the following generalized version of Lemma 7 (whose proof is a straightforward adaptation of the
proof of Lemma 7):
Lemma 9. Under Assumption 1, for all t, it holds that
ï£±
ï£¼
ï£² [
ï£½
Ut âŠ† span
{e#jd+d , e#jd+1 , e#jd+2 , e#jd+3 , . . . , e#jd+`j }
t ï£¾
ï£³
dâˆ’1
jâˆˆ[n]

for all t, where `jt is defined recursively as follows: `j1 = 1, and `jt+1 equals the largest number in {1, . . . , d âˆ’ 1} such that
{j`j , j`j +1 , . . . , j`j âˆ’1 } âŠ† {it , itâˆ’1 , . . . , imax{1,tâˆ’bn/2c+1} } (and `jt+1 = `jt if no such number exists).
t

t

t+1

As in the proof of Thm. 2, `jT bound lTj from above (for any given choice of i1 , . . . , iT ), and since d is chosen so that
1
ndâˆ’1

X

`jT â‰¤

j

d
,
2

(18)

we may take expectation over the internal randomness of the algorithm (if any), and combine it with (17) and Lemma 11
and Lemma 10 below to get
ï£®
ï£¹
ï£®
ï£¹


X j
X j
Î¦(uT ) âˆ’ Î¦(u? )
Î»
Î»
E
â‰¥ E ï£° âˆš dâˆ’1
g(lT )ï£» â‰¥ E ï£° âˆš dâˆ’1
g(`T )ï£»
Î¦(0) âˆ’ Î¦(u? )
Âµ Îºn
Âµ Îºn
j
j
ï£®
ï£«
ï£¶ï£¹

 4(Tnâˆ’1) +4
âˆš
Î»
1 X j ï£¸ï£»
Î»
Îºâˆ’1
ï£°
ï£­
âˆš
âˆš g
âˆš
â‰¥E
`T
â‰¥
.
ndâˆ’1
2Âµ Îº
2Âµ Îº
Îº+1
j

Following the same derivation as in the proof of Thm. 2, we get that T must be of order of


âˆš 
p
(Î»/Âµ)3/2 n
â„¦
n(Âµ/Î» âˆ’ 1) log
,

as required. The bound on d follows from the fact that we chose it to satisfy Inequality (18) through the following condition,


2(T âˆ’ 1)
2 1+
â‰¤ d,
n
p

âˆš 
(Î»/Âµ)3/2 n
nÂµ
and to make the lower bound valid it is enough to pick some T = O
Â·
log
. Thus, we have that d is
Î»

 âˆš

p
OÌƒ 1+ Âµ/Î»n
OÌƒ(1 + Âµ/Î»n), implying D = ndâˆ’1 d = n
.

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Lemma 10. For any fixed sequence i := i1 , . . . , iT âˆˆ [n] of individual functions chosen during a particular execution of
an optimization algorithm which satisfies Assumption 2, it holds that,
1

X

ndâˆ’1

`jT â‰¤ 1 +

j

2(T âˆ’ 1)
.
n

Proof. By Lemma 9, `jt+1 depends only on jp for `jt â‰¤ p â‰¤ `jt+1 . Thus, we may define




(j ,...,js ,âˆ—)
(j1 ,...,js ,âˆ—)
= s, `t+1
As =  (j1 , . . . , js ) | `t 1
> s , s âˆˆ [d],




(j ,...,js ,âˆ—)
(j1 ,...,js ,âˆ—)
= s, `t+1
= s , s âˆˆ [d].
Bs =  (j1 , . . . , js ) | `t 1
Intuitively, As and Bs count how many tuples (j1 , . . . , js ), under a given choice of i1 , . . . , iT , allow at most s non-zero
coordinates after t iterations, with one major difference: in As we want to allow the algorithm to make a progress after t+1
iterations (equivalently, js = it ), whereas in Bs we want the algorithm to have the same number of s non-zero coordinates
after t + 1 (equivalently, js 6= it ). One can easily verify the following:
d
X
(As + Bs )ndâˆ’sâˆ’1 = ndâˆ’1 ,
s=1

Bs = (n âˆ’ 1)As .
The first equality may be obtained by splitting the space of all [n]dâˆ’1 tuples into a group of disjoint sets characterized by
the maximal number of non-zero coordinates the algorithm may gain by the t iteration. The second equality is a simple
consequence of the way js is being constrained by As and Bs . This yields,
d
X

As nâˆ’s = nâˆ’1 .

(19)

s=1

Denoting I := {it , itâˆ’1 , . . . , imax{tâˆ’bn/2c+1,1} }, we get that for any 1 â‰¤ s â‰¤ d âˆ’ 1 and 1 â‰¤ k â‰¤ d âˆ’ s,




 j | `jt = s, `j = s + k 
t+1




 


 
(j ,...,jsâˆ’1 ,it ,âˆ—)
/ I  Â· ndâˆ’sâˆ’kâˆ’1
=  (j1 , . . . , jsâˆ’1 ) | `t 1
= s  Â·  (js+1 , . . . , js+k ) | js+1 , . . . , js+kâˆ’1 âˆˆ I, js+k âˆˆ
= As |I|kâˆ’1 (n âˆ’ |I|)ndâˆ’sâˆ’kâˆ’1
This allows us to bound from above the average `jt+1 âˆ’ `jt over j as follows,
1
ndâˆ’1

X

(`jt+1 âˆ’ `jt ) =

j

=

=

1
ndâˆ’1
1
ndâˆ’1
dâˆ’1
X
s=1


dâˆ’1 X
dâˆ’s 
X


 j | `jt = s, `j = s + k k
t+1


s=1 k=1

dâˆ’1 X
dâˆ’s
X

As |I|kâˆ’1 (n âˆ’ |I|)ndâˆ’sâˆ’kâˆ’1 k

s=1 k=1

As nâˆ’s

dâˆ’s
X

|I|kâˆ’1 (n âˆ’ |I|)nâˆ’k k

k=1


 dâˆ’s  kâˆ’1
|I| X |I|
As nâˆ’s 1 âˆ’
k
n
n
s=1
k=1

 âˆž  kâˆ’1
dâˆ’1
X
|I| X |I|
=
As nâˆ’s 1 âˆ’
k.
n
n
s=1

=

dâˆ’1
X

k=1

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

By standard manipulations of power series we have,
âˆž
X

xk =

k=0

âˆž
X
1
1
.
=â‡’
kxkâˆ’1 =
1âˆ’x
(1 âˆ’ x)2
k=1

Combining this with Eq. (19) and the fact that |I| â‰¤ n/2 yields,
1
ndâˆ’1

X

(`jt+1

âˆ’

`jt )

dâˆ’1
X

â‰¤

As n

âˆ’s

s=1

j


âˆ’1
dâˆ’1
X
|I|
2
â‰¤2
1âˆ’
As nâˆ’s â‰¤ ,
n
n
s=1

which, in turn, gives
1

X

ndâˆ’1

`jT

=

j

=

1

T
âˆ’1
X

j

t=1

ndâˆ’1
T
âˆ’1
X
t=1

â‰¤

X

1

!

(`jt+1

X

ndâˆ’1

âˆ’

`jt )

+

(`jt+1 âˆ’ `jt ) +

j

`j1
1

ndâˆ’1

X

`j1

j

2(T âˆ’ 1)
+ 1.
n

Lemma 11. For some q âˆˆ (0, 1) and positive d, define
(
g(z) =

q 2(z+1)
0

z<d
.
zâ‰¥d

Let a1 , . . . , ap be a sequence of non-negative reals, such that
p

d
1X
ai â‰¤ ,
p i=1
2
then
p

1X
1
g(ai ) â‰¥ g
p i
2

p

1X
ai
p i=1

!
.

Proof. Since q âˆˆ (0, 1), the function z 7â†’ q z is convex for non-negative z. Therefore, by the definition of g and Jensenâ€™s
inequality we have
p
X
|{i : ai < d}|
1
1X
q(ai ) =
g(ai )
p i
p
|{i : ai < d}|
{i:ai <d}
ï£«
ï£¶
X
|{i : ai < d}| ï£­
1
â‰¥
g
ai ï£¸ .
p
|{i : ai < d}|
{i:ai <d}

Note that,
p

d
1X
1
â‰¥
ai =
2
p i=1
p

X
{i:ai <d}

ai +

1
p

X
{i:ai â‰¥d}

ai â‰¥

d
|{i|ai < d}|
1
|{i|ai â‰¥ d}| =â‡’
â‰¥ .
p
p
2

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Therefore, together with the fact that g decreases monotonically and that
1
|{i : ai < d}|

p

X

ai â‰¤

{i:ai <d}

1X
ai ,
p i=1

we get
p

1X
1
q(ai ) â‰¥ g
p i
2

p

1X
ai
p i=1

!
.

