Oracle Complexity of Second-Order Methods for Finite-Sum Problems

A. Proofs
A.1. Auxiliary Lemmas
The following lemma was essentially proven in (Lan, 2015; Nesterov, 2013), but we provide a proof for completeness:
Lemma 1. Fix α, β ≥ 0, and consider the following function on Rd :
α
F (w) =
8
and aκ̃ =

√
√κ̃+3
κ̃+1

where κ̃ =
2

α+β
β

w12

+

d−1
X

!
2

(wi − wi+1 ) + (aκ̃ −

1)wd2

− w1

i=1

+

β
kwk2 ,
2

is the condition number of F . Then F is β strongly convex, (α + β)-smooth, and has a

3

unique minimum at (q, q , q , . . . , q d ) where q =

√
√κ̃−1 .
κ̃+1

Proof. The function is equivalent to
F (w) =

 β
α
w> Aw − w1 + kwk2 ,
8
2

where


2
−1


A=




−1
2
−1


−1
..
.
..

.

..

.

2
−1




.


−1
aκ̃

Since A is symmetric, all its eigenvalues are real. Therefore, by Gershgorin circle theorem and the fact that aκ̃ ∈ [1, 2]
(since κ̃ ≥ 1), we have that all the eigenvalues of A lie in [0, 4]. Thus, the eigenvalues of ∇2 F = (α/4)A + βI lie in
[β, α + β], implying that F is β-strongly convex and (α + β)-smooth.
It remains to compute the optimum of F . By differentiating F and setting to zero, we get that the optimum w must satisfy
the following set of equations:
κ̃ + 1
· w1 + 1 = 0
κ̃ − 1
κ̃ + 1
wi+1 − 2 ·
· wi + wi−1 = 0 ∀ i = 2, . . . , d − 1
κ̃ − 1


4
wd − wd−1 = 0.
aκ̃ +
κ̃ − 1
w2 − 2 ·

It is easily verified that this is satisfied by the vector (q, q 2 , q 3 , . . . , q d ), where q =
stationary point must be the unique global optimum of F .

√
√κ̃−1 .
κ̃+1

Since F is strongly convex, this

Lemma 2. For some q ∈ (0, 1) and positive d, define
(
g(z) =

q 2(z+1)
0

z<d
.
z≥d

Let l be a non-negative random variable, and suppose d ≥ 2E[l]. Then E[g(l)] ≥ 21 q 2E[l]+2 .
Proof. Since q ∈ (0, 1), the function z 7→ q z is convex for non-negative z and monotonically decreasing. Therefore, by
definition of g and Jensen’s inequality, we have
E[g(l)] = Pr(l < d) · E[q 2(l+1) |l < d] + Pr(l ≥ d) · 0 ≥ Pr(l < d) · q E[2(l+1)] .
Using Markov’s inequality to derive Pr(l < d) = 1 − Pr(l ≥ d) ≥ 1 −

E[l]
d

≥ 12 , concludes the proof.

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

A.2. Proof of Thm. 1
The proof is inspired by a technique introduced in (Woodworth and Srebro, 2016) for analyzing randomized first-order
methods, in which a quadratic function is “locally flattened” in order to make first-order (gradient) information noninformative. We use a similar technique to make second-order (Hessian) information non-informative, hence preventing
second-order methods from having an advantage over first-order methods.
Given a (deterministic) algorithm and a bound T on the number of oracle calls, we construct the function F in the following
manner. We first choose some dimension d ≥ 2T . We then define
κ=

√
κ−1
µ
,
, q=√
8λ
κ+1

and choose r > 0 sufficiently small so that
T µr2
≤ 1 and
8λ

r

T µr2
1
≤ qT .
16λ
2

We also let v1 , . . . , vT be orthonormal vectors in Rd (to be specified later). We finally define our function as
F (w) = H(w) +

λ
kwk2 ,
2

where
λ(κ − 1)
H(w) =
8
aκ =

√
√κ+3 ,
κ+1

2

hv1 , wi +

T
−1
X

!
φr (hvi − vi+1 , wi) + (aκ − 1)φr (hvT , wi) − hv1 , wi ,

i=1

and


0
φr (z) = 2(|z| − r)2

 2
z − 2r2

|z| ≤ r
r < |z| ≤ 2r .
|z| > 2r

It is easy to show that φr is 4-smooth and satisfies 0 ≤ z 2 − φr (z) ≤ 2r2 for all z.
First, we establish that F is indeed strongly convex and smooth as required:
Lemma 3. F as defined above is λ-strongly convex and µ-smooth.

Proof. Since φr is convex, and the composition of a convex and linear function is convex, we have that w 7→ φr (hvi −
vi+1 , wi) are convex for all i, as well as w 7→ hv1 , wi2 and w 7→ φr (hvT , wi). Therefore, H(w) is convex. As a result,
F is λ-strongly convex due to the λ2 kwk2 term. As to smoothness, note first that H(w) can be equivalently written as
H̃(V w), where V is some orthogonal d × d matrix with the first T rows equal to v1 , . . . , vT , and
λ(κ − 1)
H̃(x) =
8

x21 +

T
−1
X

!
φr (xi − xi+1 ) + (aκ − 1)φr (xT ) − x1

.

i=1

Therefore, ∇2 F (w) = ∇2 H(w) + λI = V > ∇2 H̃(V w)V + λI. It is easily verified that ∇2 H̃ at any point (and in particular V w) is tridiagonal, with each element having absolute value at most 2λ(κ − 1). Therefore, using the orthogonality

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

of V and the fact that (a + b)2 ≤ 2(a2 + b2 ),
sup x> ∇2 F (w)x =
x:kxk=1

sup x> (V > ∇2 H̃(V w)V + λI)x
x:kxk=1

=

sup x> ∇2 H̃(V w)x + λ
x:kxk=1

≤

d
X

sup 2λ(κ − 1)
x:kxk=1

≤

sup 2λ(κ − 1)
x:kxk=1

≤

sup 4λ(κ − 1)
x:kxk=1

i=1

x2i

+2

d−1
X

!
|xi xi+1 |

+λ

i=1

d−1
X
(|xi | + |xi+1 |)2 + λ
i=1
d−1
X

(x2i + x2i+1 ) + λ

i=1

≤ 8λ(κ − 1) + λ ≤ 8λκ.
Plugging in the definition of κ, this equals µ. Therefore, the spectral norm of the Hessian of F at any point is at most µ,
and therefore F is µ-smooth.
By construction, the function F also has the following key property:
Lemma 4. For any w ∈ Rd orthogonal to vt , vt+1 , . . . , vT (for some t ∈ {1, 2, . . . , T − 1}), it holds that
F (w), ∇F (w), ∇2 F (w) do not depend on vt+1 , vt+2 , . . . , vT .
Proof. Recall that F is derived from H by adding a λ2 kwk2 term, which clearly does not depend on v1 , . . . , vT . Therefore,
it is enough to prove the result for H(w), ∇H(w), ∇2 H(w). By taking the definition of H and differentiating, we have
that H(w) is proportional to
hv1 , wi2 +

T
−1
X

φr (hvi − vi+1 , wi) + (aκ − 1)φr (hvT , wi) − hv1 , wi,

i=1

∇H(w) is proportional to
2hv1 , wiv1 +

T
−1
X

φ0r (hvi − vi+1 , wi)(vi − vi+1 ) + (aκ − 1)φ0r (hvT , wi)vT − v1 ,

i=1
2

and ∇ H(w) is proportional to
2v1 v1> +

T
−1
X

φ00r (hvi − vi+1 , wi)(vi − vi+1 )(vi − vi+1 )> + (aκ − 1)φ00r (hvT , wi)vT vT> .

i=1

By the assumption hvt , wi = hvt+1 , wi = . . . = hvT , wi = 0, and the fact that φr (0) = φ0r (0) = φ00r (0) = 0, we
have φr (hvi − vi+1 , wi) = φ0r (hvi − vi+1 , wi) = φ00r (hvi − vi+1 , wi) = 0 for all i ∈ {t, t + 1, . . . , T }, as well as
φr (hvT , wi) = φ0r (hvT , wi) = φ00r (hvT , bwi) = 0. Therefore, it is easily verified that the expressions above indeed do
not depend on vt+1 , . . . , vT .
With this lemma at hand, we now turn to describe how v1 , . . . , vT are constructed:
• First, we compute w1 (which is possible since the algorithm is deterministic and w1 is chosen before any oracle calls
are made).
• We pick v1 to be some unit vector orthogonal to w1 . Assuming v2 , . . . , vT will also be orthogonal to w1 (which will
be ensured by the construction which follows), we have by Lemma 4 that the information F (w1 ), ∇F (w1 ), ∇2 F (w1 )
provided by the oracle to the algorithm does not depend on {v2 , . . . , vT }, and thus depends only on v1 which was
already fixed. Since the algorithm is deterministic, this fixes the next query point w2 .

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

• For t = 2, 3, . . . , T − 1, we repeat the process above: We compute wt , and pick vt to be some unit vectors orthogonal to w1 , w2 , . . . , wt , as well as all previously constructed v’s (this is always possible since the dimension
is sufficiently large). By Lemma 4, as long as all vectors thus constructed are orthogonal to wt , the information
{F (wt ), ∇F (wt ), ∇2 F (wt )} provided to the algorithm does not depend on vt+1 , . . . , vT , and only depends on
v1 , . . . , vt which were already determined. Therefore, the next query point wt+1 is fixed.
• At the end of the process, we pick vT to be some unit vector orthogonal to all previously chosen v’s as well as
w1 , . . . , wT .
Based on this construction, the following lemma is self-evident:
Lemma 5. It holds that hwT , vT i = 0.
Based on this lemma, we now turn to argue that wT must be a sub-optimal point. We first establish the following result:
Lemma 6. Letting w? = arg minw F (w), it holds that


r
T


T µr2
 ? X i 
q vi  ≤
w −


16λ
i=1
where q =

√
√κ−1 .
κ+1

Proof. Let Fr denote F , where we make the dependence on the parameter r explicit. We first argue that
sup |Fr (w) − F0 (w)| ≤
w∈Rd

T µr2
.
32

(7)

This is because
|Fr (w) − F0 (w)| ≤

λ(κ − 1)
8

T
−1
X

|φr (hvi − vi+1 , wi) − φ0 (hvi − vi+1 , wi)|

i=1

!
+ |φr (hvT , wi) − φ0 (hvT , wi)| ,
and since supz∈R |φr (z) − φ0 (z)| = supz∈R |φr (z) − z 2 | ≤ 2r2 , the above is at most
that κ = µ/8λ, Eq. (7) follows.

λ(κ−1)
T r2
4

≤

λκ
2
4 Tr .

Recalling

Let wr = arg min Fr (w). By λ-strong convexity of F0 and Fr ,
λ
λ
kwr − w0 k2 , Fr (w0 ) − Fr (wr ) ≥ kw0 − wr k2 .
2
2
Summing the two inequalities and using Eq. (7),
F0 (wr ) − F0 (w0 ) ≥

λkwr − w0 k2 ≤ F0 (wr ) − Fr (wr ) + Fr (w0 ) − F0 (w0 ) ≤

T µr2
,
16

and therefore

T µr2
.
(8)
16λ
By definition, wr = w? from the statement of our lemma, so it only remains to prove that w0 = arg min F0 (w) equals
PT
i
i=1 q vi . To see this, note that F0 (w) can be equivalently written as F̃ (V w), where V is some orthogonal d × d matrix
with its first T rows equal to v1 , . . . , vT , and
!
T
−1
X
λ(κ − 1)
λ
2
2
2
F̃ (x) =
x1 +
(xi − xi+1 ) + (aκ − 1)xT − w1 + kxk2 .
8
2
i=1
kwr − w0 k2 ≤

By an immediate corollary of Lemma 1, F̃ (·) is minimized at (q, q 2 , . . . , q T , 0, . . . , 0), where q =
PT
F (w) = F̃ (V w) is minimized at V > (q, q 2 , . . . , q T , 0, . . . , 0), which equals i=1 q i vi as required.

√
√κ−1 ,
κ+1

and therefore

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Note that this lemma also allows us to bound the norm of w? = arg min F (w), since it implies that

 r
T
X

T µr2


kw? k ≤ 
q i vi  +
,


16λ
i=1

and since (a + b)2 ≤ 2a2 + 2b2 and q < 1, we have
 T
2
T
X

X
T µr2
T µr2

i 
kw k ≤ 2 
q vi  +
= 2
q 2i +


8λ
8λ
i=1
i=1
? 2

≤ 2

∞
X

q 2i +

i=1

≤
which is at most

√

T µr2
2q 2
T µr2
+
=
8λ
1 − q2
8λ

√
2
T µr2
T µr2
+
= κ+1+
,
1−q
8λ
8λ

√
κ + 2 ≤ 3 κ, since we assume that c is sufficiently small so that

T µr 2
8λ

≤ 1, and that κ = µ/8λ ≥ 1.

The proof of the theorem follows by combining Lemma 5 and Lemma 6. Specifically, Lemma 5 (which states that
hwT , vT i = 0) and the fact that v1 , . . . , vT are orthonormal tells us that
2

2


2
!
T
−1
T
−1
T






X
X
X




i
T
i 
i 
q vi − q vT  = wT −
q vi  + kq T vT k2
q v i  =  wT −
wT −






i=1

i=1

T

2

≥ kq vT k = q

i=1

2T

,

and hence


T


X

i 
q vi  ≥ q T .
wT −


i=1

On the other hand, Lemma 6 states that


r
T


T µr2
 ? X i 
q vi  ≤
.
w −


16λ
i=1
Combining the last two displayed equations by the triangle inequality, we get that
r
T µr2
?
T
kwT − w k ≥ q −
.
16λ
q
µr 2
By the assumption that c is sufficiently small so that T16λ
≤ 12 q T , the left hand side is at least 12 q T . Squaring both sides,
we get
1
kwT − w? k2 ≥ q 2T ,
4
so by strong convexity of F ,
λ
λ
F (wT ) − F (w? ) ≥ kwT − w? k2 ≥ q 2T .
2
8
Plugging in the value of q, we get
√
2T
λ
κ−1
?
√
F (wT ) − F (w ) ≥
.
8
κ+1
√
√
On the other hand, we showed earlier that kw? k2 ≤ 3 κ, so by smoothness, F (0) − F (w? ) ≤ µ2 kw? k2 ≤ 3µ
κ.
2
Therefore,
√
2T
F (wT ) − F (w? )
λ
κ−1
√
√
≥
F (0) − F (w? )
12µ κ
κ+1

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

To make the right-hand side less than , T must be such that
√
2T
√
κ−1
12µ κ
√
≤
,
λ
κ+1
which is equivalent to
2T · log
Since log

√

√κ+1
κ−1




= log 1 +

√2
κ−1



≤

√



κ+1
λ
√
√
≥ log
.
κ−1
12µ κ

√2 ,
κ−1

it follows that T must be such that

4T
√
≥ log
κ−1



λ
√
12µ κ


.

Plugging in κ = µ/8λ and simplifying a bit, we get that
1
T ≥
4

r


µ
− 1 · log
8λ

√

8(λ/µ)3/2
12

!
,

from which the result follows.
A.3. Proof of Thm. 2
We will define a randomized choice of quadratic functions f1 , . . . , fn , and prove a lower bound on the expected optimization error of any algorithm (where the expectation is over both the algorithm and the randomized functions). This implies
that for any algorithm, the same lower bound (in expectation over the algorithm only) holds for some deterministic choice
of f1 , . . . , fn .
Therewill actually
separate
constructions, one leading to a lower bound of Ω(n), and one leading to a lower bound
 be two
√ 
p nµ
(λ/µ)3/2 n
of Ω
. Choosing the construction which leads to the larger lower bound, the theorem follows.
λ · log

A.3.1. A N Ω(n) L OWER B OUND
Starting with the Ω(n) lower bound, let δi , where i ∈ {1, . . . , n}, be chosen uniformly at random from {−1, +1}, and
define
λ
fi (w) = −δi w1 + kwk2 .
2
Clearly, these are λ-smooth (and hence µ-smooth) functions, as well as λ-strongly convex. Also, the optimum of F (w) =

2
Pn
Pn
µ Pn
1
1
1
?
? 2
i=1 fi (w) equals w = nλ
i=1 δi e1 , where e1 is the first unit vector. As a result, kw k = λ2 n
i=1 δi , so
n
by λ-smoothness of F
!2
n
X
1
1
λ
F (0) − F (w? ) ≤ kw? k2 =
δi .
2
2λ n i=1
 Pn

Since δi are i.i.d., we have by Hoeffding’s bound that with probability at least 3/4,  n1 i=1 δi  is at most
p
p
2 log(8/3)/n ≤ 2/n. Plugging into the equation above, we get that with probability at least 3/4,
F (0) − F (w? ) ≤

1
.
λn

Turning to lower bound F (wT ) − F (w? ), we have by strong convexity that
λ
λ
kwT − w? k2 ≥ (wT,1 − w1? )2
2
2
!2
n
1
1X
=
λwT,1 −
δi .
2λ
n i=1

F (wT ) − F (w? ) ≥

(9)

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Now, if at most bn/2c indices {1, . . . , n} were queried by the algorithm, then the wT returned by the algorithm must
be independent of at least dn/2e random variables δj1 , . . . , δjdn/2e (for some distinct indices j1 , j2 , . . . depending on the
algorithm’s behavior, but independent of the values of δj1 , . . . , δjdn/2e ). Therefore, conditioned on j1 , . . . , jdn/2e and the
values of δj1 , . . . , δjdn/2e , the expression above can be written as

1
1 
η−
2λ
n

2
X

δi  ,

i∈{j
/ 1 ,...,jdn/2e }

where η is a fixed quantity independent of the values of δi for i ∈
/ {j1 , . . . , jdn/2e }. By a standard anti-concentration
 0 2
c
c02
1
√
= 2λn
for some universal positive c0 > 0.
argument, with probability at least 3/4, this expression will be at least 2λ
n
Since this is true for any j1 , . . . , jdn/2e and δj1 , . . . , δjdn/2e , we get that with probability at least 3/4 over δ1 , . . . , δn ,
F (wT ) − F (w? ) ≥

c02
.
2λn

Combining this with Eq. (9) using a union bound, we have that with probability at least 1/2,
c02 λn
c02
F (wT ) − F (w? )
≥
=
.
F (0) − F (w? )
2λn
2
As a result, since the ratio above is always a non-negative quantity,


F (wT ) − F (w? )
c02
E
≥
.
F (0) − F (w? )
4
Using the assumption stated in the theorem (taking c = c02 /4), we have that the right hand side cannot be smaller than ,
unless more than bn/2c = Ω(n) oracle calls are made.
A.3.2. A N Ω

p

nµ
λ

· log



(λ/µ)3/2


√

n



L OWER B OUND

p nµ
λ
We now turn to prove the Ω
lower bound, using a different function construction: Let j1 , . . . , jd−1 be
λ · log 
chosen uniformly and independently at random from {1, . . . , n}, and define

!
d−1
µ−λ X
1
λ
2
2
2
1jl =i (wl − wl+1 ) +
w1 + (aκ − 1)wd − w1
+ kwk2 .
(10)
fi (w) =
8
n
2
l=1

where 1A is the indicator of the event i. Note that these are all λ-strongly convex functions, as all terms in their definition
are convex in w, and there is an additional λ2 kwk2 term. Moreover, they are also µ-smooth: To see this, note that
∇2 fi (w)  (µ−λ)
4 A + λI  µI, where A  4I is as defined in the proof of Lemma 1.
Pn
The average function F (w) = n1 i=1 fi (w) equals
!
d−1
X
µ−λ
λ
F (w) =
w12 +
(wi − wi+1 )2 + (aκ − 1)wd2 − w1 + kwk2 ,
(11)
8n
2
i=1
Therefore, by Lemma 1, the
smoothness parameter of F is (µ − λ)/n + λ ≤ µ, the global minimum w? of F equals
√
κ−1
2
d
(q, q , . . . , q ), where q = √κ+1 and
µ−λ
n

µ
+λ
−1
= λ
+ 1.
λ
n
Note that since q < 1 and κ ≥ 1, the squared norm of w? is at most

κ=

d
X
i=1

q 2i ≤

∞
X
i=1

q 2i =

q2
1
≤
=
2
1−q
1−q

√

√
κ+1
≤ κ,
2

(12)

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

hence by smoothness,
F (0) − F (w? ) ≤

µ ? 2
µ√
κ.
kw k ≤
2
2

(13)

With these preliminaries out of the way, we now turn to compute a lower bound on the expected optimization error. The
proof is based on arguing that wT can only have a first few coordinates being non-zero. To see how this gives a lower
bound, let lT ∈ {1, . . . , d} be the largest index of a non-zero coordinate of wT (or 0 if wT = 0). By definition of w? , we
have
d
X
kwT − w? k2 ≥
q 2i ≥ g(lT ),
i=lT +1

where
(
g(z) =

q 2(z+1)
0

z<d
.
z≥d

(14)

By strong convexity of F , this implies that
F (wT ) − F (w? ) ≥

λ
λ
kwT − w? k2 ≥ g(lT ).
2
2

Finally, taking expectation over the randomness of j1 , . . . , jd−1 above (and over the internal randomness of the algorithm,
if any), applying Lemma 2, and choosing the dimension d = d2E[lT ]e (which we will later show to equal the value specified
in the theorem), we have
λ
λ
E [F (wT ) − F (w )] ≥ q 4E[lT ]+4 =
4
4
?

√
2E[lT ]+2
κ−1
√
.
κ+1

Combined with Eq. (13), this gives
F (wT ) − F (w? )
E
F (0) − F (w? )




λ
√
≥
2µ κ

√
2E[lT ]+2
κ−1
√
.
κ+1

(15)

Thus, it remains to upper bound E[lT ].
To get a bound, we rely on the following key lemma (where ei is the i-th unit vector, and recall that Wt defines the set of
allowed query points wt , and j1 , . . . , jd are the random indices used in constructing f1 , . . . , fn ):
Lemma 7. For all t, it holds that Wt ⊆ span{ed , e1 , e2 , e3 , . . . , e`t } for all t, where `t is defined recursively
as follows: `1 = 1, and `t+1 equals the largest number in {1, . . . , d − 1} such that {j`t , j`t +1 , . . . , j`t+1 −1 } ⊆
{it , it−1 , . . . , imax{1,t−bn/2c+1} } (and `t+1 = `t if no such number exists).
As will be seen later, `T (which is a random variable as a function of the random indices j1 , . . . , jd ) upper-bounds the
number of non-zero coordinates of wT , and therefore we can upper bound E[lT ] by E[`T ].
Proof. The proof is by induction over t. Since W1 = {0} ⊆ span(ed ), the result trivially holds for t = 1. Now, suppose
that Wt ⊆ span{ed , e1 , . . . , e`t } for some t and `t . Note that in particular, this means that wt is non-zero only in its first
`t coordinates. By definition of fi for any i,
!
d−1
X
1
λn(κ − 1)
2
1jl =i (wl − wl+1 )(el − el+1 ) + (2w1 e1 + 2(aκ − 1)wd ed − e1 ) + λw
∇fi (w) =
8
n
l=1
!
d−1
λn(κ − 1) X
1
2
∇ fi (w) =
1jl =i (2El,l − El+1,l − El,l+1 ) + (2E1,1 + 2(aκ − 1)Ed,d ) + λI,
8
n
l=1

where Er,s is the d × d which is all zeros, except for an entry of 1 in location (r, s). It is easily seen that these expressions
imply the following:

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

• If j`t 6= it , then ∇fit (wt ) ∈ span{ed , e1 , . . . , e`t }, otherwise ∇fit (wt ) ∈ span{ed , e1 , . . . , e`t +1 }.
• For any w and l ∈ {1, . . . , d − 1}, if jl 6= i, then ∇2 fi (w) is block-diagonal, with a block in the first l × l entries. In
other words, any entry (r, s) in the matrix, where r ≤ l and s > l (or r > l and s ≤ l) is zero.
Pt
• As a result, if jl ∈
/ {it , it−1 , . . . , imax{1,t−bn/2c+1} }, then τ =max{1,t−bn/2c+1} ατ ∇2 fiτ (wτ ), for arbitrary scalars
τ , is block-diagonal with a block in the first l × l entries. The same clearly holds for any matrix with the same
block-diagonal structure.
Together, these observations imply that the operations specified in Assumption 1 can lead to vectors outside span{ed , e1 , . . . , e`t }, only if j`t ∈ {it , it−1 , . . . , imax{1,t−bn/2c+1} }. Moreover, these vectors must belong to span{ed , e1 , . . . , e`t+1 }, where `t+1 is as specified in the lemma: By definition, j`t+1 is not in
{it , it−1 , . . . , imax{1,t−bn/2c+1} }, and therefore all relevant Hessians have a block in the first `t+1 × `t+1 entries, hence it
is impossible to create a vector with non-zero coordinates (using the operations of Assumption 1) beyond the first `t+1 .
Since wT ⊆ WT , the lemma above implies that E[lT ] from Eq. (15) (where lT is the largest index of a non-zero coordinate
of wT ) can be upper-bounded by E[`T ], where the expectation is over the random draw of the indices j1 , . . . , jd−1 . This
can be bounded using the following lemma:
Lemma 8. It holds that E[`T ] ≤ 1 +

2(T −1)
.
n

Proof. By definition of `t and linearity of expectation, we have
"T −1
#
T
−1
X
X
E[`t+1 − `t ] + 1.
E[`T ] = E
(`t+1 − `t ) + `1 =

(16)

t=1

t=1

Let us consider any particular term in the sum above. Since `t+1 − `t is a non-negative integer, we have
E[`t+1 − `t ] = Pr (`t+1 > `t ) · E [`t+1 − `t | `t+1 > `t ] .
By definition of `t , the event `t+1 > `t can occur only if j`t ∈
/ {it−1 , it−2 , . . . , imax{1,t−bn/2c} }, yet j`t ∈
{it , it−1 , . . . , imax{1,t−bn/2c+1} }. This is equivalent to j`t = it (that is, in iteration t we happened to choose the index j`t of the unique individual function, which contains the block linking coordinate `t and `t + 1, hence allowing
us to “advance” and have more non-zero coordinates). But since the algorithm is oblivious, it is fixed whereas j`t is
chosen uniformly at random, hence the probability of this event is 1/n. Therefore, Pr (`t+1 > `t ) ≤ 1/n. Turning
to the conditional expectation of `t+1 − `t above, it equals the expected number of indices j`t , j`t +1 , . . . belonging to
{it , it−1 , . . . , imax{1,t−bn/2c+1} }, conditioned on j`t belonging to that set. But since the i indices are fixed and the j
indices are chosen uniformly at random, this equals one plus the expected number of times where a randomly drawn
j ∈ {1, . . . , n} belongs to {it , it−1 , . . . , it−bn/2c+1 }. Since this set contains at most bn/2c distinct elements in {1, . . . , n},
this is equivalent to (one plus) the expectation of a geometric random variable, where the success probability is at most
1/2
= 2. Plugging into the displayed equation above, we get that
1/2. By a standard derivation, this is at most 1 + 1−1/2
E[`t+1 − `t ] ≤
and therefore the bound in Eq. (16) is at most

2(T −1)
n

1
2
·2 = ,
n
n

+ 1 as required.

Plugging this bound into Eq. (15), we get
F (wT ) − F (w? )
E
F (0) − F (w? )




λ
√
≥
2µ κ

√
 4(Tn−1) +4
κ−1
√
.
κ+1

To make the right-hand side less than , T must be such that
√
 4(Tn−1) +4
√
κ−1
2µ κ
√
≤
,
λ
κ+1

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

which is equivalent to


√



4(T − 1)
κ+1
λ
√
+ 4 log √
≥ log
.
n
κ−1
2µ κ



√
κ+1
2
2
= log 1 + √κ−1
≤ √κ−1
(see, e.g., Lemma 12 in (Arjevani and Shamir, 2016a)), it follows that T
Since log √κ−1
must be such that




2
λ
4(T − 1)
√
+4 √
≥ log
.
n
2µ κ
κ−1


Plugging in κ =

µ
λ −1

n

+ 1, we get that
q µ

n

2





λ −1

n
T ≥ 1+ 
4

· log 



λ

 − 4 .
qµ
λ −1
2µ
n +1

Using asymptotic notation the right-hand side equals


√ 
p
(λ/µ)3/2 n
Ω
n(µ/λ − 1) log
.

as required. The bound on the dimension d follows from the 
fact that we 
chose it to be
O(E[lT ]) = O(1 + T /n), and to
√
p nµ
(λ/µ)3/2 n
make the lower bound valid it is enough to pick some T = O
.
λ · log

A.4. Proof of Thm. 3
Recall that the proof of Thm. 2 essentially shows that for any (possibly stochastic) index-oblivious optimization algorithm
there exists some ‘bad’ assignment of the d − 1 blocks j1 , . . . , jd−1 whose corresponding fi : Rd → R (see Eq. (10))
form a functions which is hard-to-optimize. When considering non-oblivious (i.e., adaptive) algorithms this construction
fails as soon as the algorithm obtains the Hessians of all the individual functions (potentially, after n second-order oracle
queries). Indeed, knowing the Hessians of fi , one can devise an index-schedule which gains at least one coordinate at
every iteration, as opposed to 1/n on average for the oblivious case. Thus, in order to tackle the non-oblivious case, we
form a function over some D-dimensional space which ‘contains’ all the nd−1 sub-problems at one and the same time
(clearly, to carry out our plans we must pick D which grows exponentially fast with d, the dimension of the sub-problems).
This way, any index-schedule, oblivious or adaptive, must ‘fit’ all the nd−1 sub-problems well, and as such, bound to a
certain convergence rate which we analyze below.
Denote [n] = {1, . . . , n}, set D = nd−1 d and define for any j ∈ [n]d−1 the following,
fij

d

: R → R,

Qj : RD → Rd ,

µ−λ
w 7→
8
u 7→

d
X

d−1
X
l=1


!
1
λ
2
2
1jl =i (wl − wl+1 ) +
w1 + (aκ − 1)wd − w1
+ kwk2 ,
n
2
2

u> e#jd+l

l=1

where #j enumerates the nd−1 tuples [n]d−1 from 0 to nd−1 − 1. Note that fij are defined exactly as in Eq. (10), only here
we make the dependence on j explicit. The individual functions are defined as follows:
X
fi (u) =
fij (Qj u).
j∈[n]d−1

Note that,
∇2 fi (u) =

X
j∈[n]d−1

(Qj )> ∇2 fij (Qj u)Qj .

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Since ∇2 fi are block-diagonal, we have Λ(∇2 fi ) =
Thus, since
convex.

fij

S

j

Λ(∇2 fij ), where Λ(·) denotes the spectrum of a given matrix.

are µ-smooth and λ-strongly convex (see proof of Thm. 2), we see that fi is also µ-smooth and λ-strongly

As for the average function Φ(u) =

1
n

Pn

i=1

fi (u), it is easily verified that for any fixed j ∈ [n]d−1 ,
n

1X j j
f (Q u) = F (Qj u),
n i=1 i
where F is as defined in Eq. (11). Thus,
Φ(u) =

n
1X X
fij (Qj u) =
n i=1
d−1
j∈[n]

X

F (Qj u).

j∈[n]d−1

To compute the minimizer of Φ, we compute the first-order derivative:



X

∇Φ(u) = ∇ 

F (Qj u)

j∈[n]d−1



∇ F (Qj u)

X

=

j∈[n]d−1

X

=

(Qj )> ∇F (Qj u).

j∈[n]d−1

Thus, by setting u∗ =

j > ∗
j (Q ) w ,

P

∗

∇Φ(u ) =

where w∗ is the minimizer of F as in Lemma 1, we get

X

j >



j

(Q ) ∇F Q

j∈[n]d−1

X

j > ∗

(Q ) u



X

=

(Qj )> ∇F (w∗ ) = 0.

j∈[n]d−1

j

√
Note that, by Eq. (13), ku∗ k2 = nd−1 kw∗ k2 ≤ nd−1 κ. Hence, by smoothness,
Φ(0) − Φ(u? ) ≤

√
µ ? 2
µ
ku k ≤ nd−1 κ.
2
2

(17)

To derive the analytical properties of Φ, we compute the second derivative:
∇2 Φ(u) =

X

∇((Qj )> ∇F (Qj u))

j∈[n]d−1

=

X

(Qj )> ∇(∇F (Qj u))

j∈[n]d−1

=

X

(Qj )> ∇2 F (Qj u)Qj .

j∈[n]d−1

Since ∇2 Φ is a block-diagonal matrix, we have Λ(∇2 Φ) =
is ((µ − λ)/n + λ)-smooth and λ-strongly convex.

S

j

Λ(∇2 F ) = Λ(∇2 F ). Thus, by Lemma 1, it follows that Φ

With these preliminaries out of the way, we now turn to compute a lower bound on the expected optimization error. The
proof follows by arguing that uT can only have a first few coordinates being non-zero for each of the nd−1 sub-problems.
To see how this gives a lower bound, let lTj ∈ {1, . . . , d} be the largest index of a non-zero coordinate of Qj uT (or 0 if

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Qj uT = 0). By the definition of u? and by Eq. (12), we have
X
X
kuT − u∗ k2 = k
(Qj )> Qj uT −
(Qj )> w∗ k2
j

=k

j

X

j >

(Q ) (Q uT − w∗ )k2
j

j

=

X

≥

X

kQj uT − w∗ k2

j

g(lTj ),

j

where g is defined in Eq. (14). By the strong convexity of F , this implies that
λ
λX j
Φ(uT ) − Φ(u? ) ≥ kuT − u? k2 ≥
g(lT ).
2
2
j

We now proceed along the same lines as in the proof of Thm. 2. First, to upper bound lTj (note that, g is monotonically
decreasing), we use the following generalized version of Lemma 7 (whose proof is a straightforward adaptation of the
proof of Lemma 7):
Lemma 9. Under Assumption 1, for all t, it holds that


 [

Ut ⊆ span
{e#jd+d , e#jd+1 , e#jd+2 , e#jd+3 , . . . , e#jd+`j }
t 

d−1
j∈[n]

for all t, where `jt is defined recursively as follows: `j1 = 1, and `jt+1 equals the largest number in {1, . . . , d − 1} such that
{j`j , j`j +1 , . . . , j`j −1 } ⊆ {it , it−1 , . . . , imax{1,t−bn/2c+1} } (and `jt+1 = `jt if no such number exists).
t

t

t+1

As in the proof of Thm. 2, `jT bound lTj from above (for any given choice of i1 , . . . , iT ), and since d is chosen so that
1
nd−1

X

`jT ≤

j

d
,
2

(18)

we may take expectation over the internal randomness of the algorithm (if any), and combine it with (17) and Lemma 11
and Lemma 10 below to get






X j
X j
Φ(uT ) − Φ(u? )
λ
λ
E
≥ E  √ d−1
g(lT ) ≥ E  √ d−1
g(`T )
Φ(0) − Φ(u? )
µ κn
µ κn
j
j




 4(Tn−1) +4
√
λ
1 X j 
λ
κ−1


√
√ g
√
≥E
`T
≥
.
nd−1
2µ κ
2µ κ
κ+1
j

Following the same derivation as in the proof of Thm. 2, we get that T must be of order of


√ 
p
(λ/µ)3/2 n
Ω
n(µ/λ − 1) log
,

as required. The bound on d follows from the fact that we chose it to satisfy Inequality (18) through the following condition,


2(T − 1)
2 1+
≤ d,
n
p

√ 
(λ/µ)3/2 n
nµ
and to make the lower bound valid it is enough to pick some T = O
·
log
. Thus, we have that d is
λ

 √

p
Õ 1+ µ/λn
Õ(1 + µ/λn), implying D = nd−1 d = n
.

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Lemma 10. For any fixed sequence i := i1 , . . . , iT ∈ [n] of individual functions chosen during a particular execution of
an optimization algorithm which satisfies Assumption 2, it holds that,
1

X

nd−1

`jT ≤ 1 +

j

2(T − 1)
.
n

Proof. By Lemma 9, `jt+1 depends only on jp for `jt ≤ p ≤ `jt+1 . Thus, we may define




(j ,...,js ,∗)
(j1 ,...,js ,∗)
= s, `t+1
As =  (j1 , . . . , js ) | `t 1
> s , s ∈ [d],




(j ,...,js ,∗)
(j1 ,...,js ,∗)
= s, `t+1
= s , s ∈ [d].
Bs =  (j1 , . . . , js ) | `t 1
Intuitively, As and Bs count how many tuples (j1 , . . . , js ), under a given choice of i1 , . . . , iT , allow at most s non-zero
coordinates after t iterations, with one major difference: in As we want to allow the algorithm to make a progress after t+1
iterations (equivalently, js = it ), whereas in Bs we want the algorithm to have the same number of s non-zero coordinates
after t + 1 (equivalently, js 6= it ). One can easily verify the following:
d
X
(As + Bs )nd−s−1 = nd−1 ,
s=1

Bs = (n − 1)As .
The first equality may be obtained by splitting the space of all [n]d−1 tuples into a group of disjoint sets characterized by
the maximal number of non-zero coordinates the algorithm may gain by the t iteration. The second equality is a simple
consequence of the way js is being constrained by As and Bs . This yields,
d
X

As n−s = n−1 .

(19)

s=1

Denoting I := {it , it−1 , . . . , imax{t−bn/2c+1,1} }, we get that for any 1 ≤ s ≤ d − 1 and 1 ≤ k ≤ d − s,




 j | `jt = s, `j = s + k 
t+1




 


 
(j ,...,js−1 ,it ,∗)
/ I  · nd−s−k−1
=  (j1 , . . . , js−1 ) | `t 1
= s  ·  (js+1 , . . . , js+k ) | js+1 , . . . , js+k−1 ∈ I, js+k ∈
= As |I|k−1 (n − |I|)nd−s−k−1
This allows us to bound from above the average `jt+1 − `jt over j as follows,
1
nd−1

X

(`jt+1 − `jt ) =

j

=

=

1
nd−1
1
nd−1
d−1
X
s=1


d−1 X
d−s 
X


 j | `jt = s, `j = s + k k
t+1


s=1 k=1

d−1 X
d−s
X

As |I|k−1 (n − |I|)nd−s−k−1 k

s=1 k=1

As n−s

d−s
X

|I|k−1 (n − |I|)n−k k

k=1


 d−s  k−1
|I| X |I|
As n−s 1 −
k
n
n
s=1
k=1

 ∞  k−1
d−1
X
|I| X |I|
=
As n−s 1 −
k.
n
n
s=1

=

d−1
X

k=1

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

By standard manipulations of power series we have,
∞
X

xk =

k=0

∞
X
1
1
.
=⇒
kxk−1 =
1−x
(1 − x)2
k=1

Combining this with Eq. (19) and the fact that |I| ≤ n/2 yields,
1
nd−1

X

(`jt+1

−

`jt )

d−1
X

≤

As n

−s

s=1

j


−1
d−1
X
|I|
2
≤2
1−
As n−s ≤ ,
n
n
s=1

which, in turn, gives
1

X

nd−1

`jT

=

j

=

1

T
−1
X

j

t=1

nd−1
T
−1
X
t=1

≤

X

1

!

(`jt+1

X

nd−1

−

`jt )

+

(`jt+1 − `jt ) +

j

`j1
1

nd−1

X

`j1

j

2(T − 1)
+ 1.
n

Lemma 11. For some q ∈ (0, 1) and positive d, define
(
g(z) =

q 2(z+1)
0

z<d
.
z≥d

Let a1 , . . . , ap be a sequence of non-negative reals, such that
p

d
1X
ai ≤ ,
p i=1
2
then
p

1X
1
g(ai ) ≥ g
p i
2

p

1X
ai
p i=1

!
.

Proof. Since q ∈ (0, 1), the function z 7→ q z is convex for non-negative z. Therefore, by the definition of g and Jensen’s
inequality we have
p
X
|{i : ai < d}|
1
1X
q(ai ) =
g(ai )
p i
p
|{i : ai < d}|
{i:ai <d}


X
|{i : ai < d}| 
1
≥
g
ai  .
p
|{i : ai < d}|
{i:ai <d}

Note that,
p

d
1X
1
≥
ai =
2
p i=1
p

X
{i:ai <d}

ai +

1
p

X
{i:ai ≥d}

ai ≥

d
|{i|ai < d}|
1
|{i|ai ≥ d}| =⇒
≥ .
p
p
2

Oracle Complexity of Second-Order Methods for Finite-Sum Problems

Therefore, together with the fact that g decreases monotonically and that
1
|{i : ai < d}|

p

X

ai ≤

{i:ai <d}

1X
ai ,
p i=1

we get
p

1X
1
q(ai ) ≥ g
p i
2

p

1X
ai
p i=1

!
.

