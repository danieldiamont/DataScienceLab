Supplementary Material:
Faster Greedy MAP Inference for Determinantal Point Processes

A. Proof of Theorem 1

can be written as

For given X ✓ Y, we denote that the true marginal gain ⇤i
and the approximated gain i (used in Algorithm 1) as

pn (x) =

n
X

c k Tk

k=0

⇤i := log det LX[{i} log det LX
⌧⇣
⌘ 1
(j)
(j)
LX
, LX[{i} LX
i :=
⇣
⌘
(j)
+ log det LX
log det LX

where an item i 2 Y \ X is in the partition j. We also use
iOPT = argmaxi ⇤i and imax = argmaxi i . Then, we
have
⇤imax

"

imax

"

iOPT

⇤iOPT

2"

where the first and third inequalities are from the definition
of ", i.e., |⇤i
i |  ", and the second inequality holds
by the optimality of imax . In addition, when the smallest eigenvalue of L is greater than 1, log det LX is monotone and non-negative (Sharma et al., 2015). To complete
the proof, we introduce following approximation guarantee
of the greedy algorithm with a ‘noise’ during the selection
(Streeter & Golovin, 2009).
Theorem. (Noisy greedy algorithm) Suppose a submodular function f defined on ground set Y is monotone and
non-negative. Let X0 = ; and Xk = Xk 1 [ {imax } such
that
f (Xk

1

[ {imax })

max

i2Y\Xk

for some "k
f (Xk )

(f (Xk

1

f (Xk

1)

1 [ {i})

f (Xk

1 ))

"k

1/e)

max

X✓Y,|X|k

f (X)

k
X

2
1

2

x

1
1

2

◆

(6)

where the coefficient ck and the k-th Chebyshev polynomial Tk (x) are defined as
8
✓
◆
n
X
>
1 2
1
>
> 1
f
x
+
T0 (xj ) if k = 0
>
j
>
2
2
< n + 1 j=0
ck =
✓
◆
n
>
2 X
1 2
1
>
>
>
f
xj +
Tk (xj ) otherwise
>
:n + 1
2
2
j=0
(7)

Tk+1 (x) = 2xTk (x)
where xj = cos

⇣

Tk

1 (x)

⇡(j+1/2)
n+1

⌘

for k

1

(8)

for j = 0, 1, . . . , n and

T0 (x) = 1, T1 (x) = x (Mason & Handscomb, 2002). For
simplicity, we now use H := pn (A) pn (B) and denote
1
e= 2 A
A
1 2
1 2 I where I is identity matrix with same
e
dimension of A and same for B.

We estimate the log-determinant difference while random
vectors are shared, i.e.,
m

log det A

log det B ⇡

1 X (i)>
v
Hv(i) .
m i=1

To show that the variance of v(i)> Hv(i) is small as
kA BkF , we provide that
"

0. Then,
(1

✓

"i

i=1

Theorem 1 is straightforward by substituting 2" into "k .
This completes the proof of Theorem 1.

B. Proof of Theorem 2
As we explained in Section 2.3, Chebyshev expansion of
log x in [ , 1
] with degree n is defined as pn (x). This

#
m
⇥
⇤
1 X (i)>
1
(i)
v
Hv
= Var v> Hv
Var
m i=1
m
2
2
2
2
kHkF =
kpn (A) pn (B)kF
m
m
!2
n
⇣ ⌘
⇣ ⌘
2 X
e
e

|ck | Tk A
Tk B
m
F


k=0

where the first inequality holds from (Avron & Toledo,
2011) and the second is from combining (6) with the triangle inequality. To complete the proof, we use following
two lemmas.

Faster Greedy MAP Inference for Determinantal Point Processes

Lemma 3. Let Tk (·) be Chebyshev polynomial with kdegree and symmetric matrices B, E satisfied with kBk2 
1, kB + Ek2  1. Then, for k 0,
Tk (B)kF  k 2 kEkF .

kTk (B + E)

Lemma 4. Let ck be the k-th coefficient of Chebyshev expansion for f (x). Suppose f is analytic with |f (z)|  M
in the region bounded by the ellipse with foci ±1 and the
length of major and minor semiaxis summing to ⇢ > 1.
Then,
n
X

k=0

2

k |ck | 

2M ⇢ (⇢ + 1)
(⇢

1)

3

2/



=

2
m
2
m

k=0
n
X

k=0

e
|ck | k 2 A

2M ⇢ (⇢ + 1)
1)

3

32M 2 ⇢2 (⇢ + 1)
m (⇢

(x) , g1 (x) = 1, g0 (x) = 0,

hk+1 (x) = 2xhk (x)

hk

1

(x) , h1 (x) = 2, h0 (x) = 0.

In addition, we can easily verify that

x2[ 1,1]

1 1

⇣ ⌘
e
|ck | Tk A

(⇢

1

.

x2[ 1,1]

6

1) (1

!2 ✓

F

2
2

2

2 )

2

kA

kA

F

+

!2

BkF

◆2

2

BkF

1

(9)

+ 2E Tk (B)

for k
1 where R1 = E, R0 = 0 where 0 is defined as
zero matrix with the same dimension of B. Solving this,
we obtain that
k
X
i=0

(B)
F

hi (B + E) E Tk+1

khi (B + E)k2 kEkF k Tk+1



kgk+1 (B + E)k2 +



k
X

k+1+

i=0

2

Denote Rk := Tk (B + E) Tk (B). From the recurrence
of Chebyshev polynomial (8), Rk has following

Rk+1 = gk+1 (B + E) E +

i

!

k
X
i=0

khi (B + E)k2

!

i

(B)k2

kEkF

2i kEkF

= (k + 1) kEkF

B.1. Proof of Lemma 3

Rk

k
X
i=0

where the second inequality holds from Lemma 3 and the
thrid is from Lemma 4. This completes the proof of Theorem 2.

Rk+1 = 2 (B + E) Rk

hi (B + E) E Tk+1

 kgk+1 (B + E)k2 kEkF

!2

1

k
X
i=0

⇣ ⌘
e
Tk B

e
B

kRk+1 kF  kgk+1 (B + E) EkF
+

Using Lemma 3 and 4, we can write
"
#
m
1 X (i)>
(i)
Var
v
Hv
m i=1
n
X

gk

Putting all together, we conclude that

et al., 2015).

2
m

gk+1 (x) = 2xgk (x)

2 max |gk (x)| = max |hk (x)| = 2k.

In order to apply Lemma 4, we should consider f (x) =
log 1 22 x + 12 . Then it can be easily obtained M =
5 log (2/ ) and ⇢ = 1 + p 2
as provided in (Han



for k 1 where both gk (·) and hk (·) are polynomials with
degree k and they have following recurrences

i

(B)

(10)

where the second inequality holds from kY XkF =
kXY kF  kXk2 kY kF for matrix X, Y and the third inequality uses that |Tk (x)|  1 for all k
0. This completes the proof of Lemma 3.
B.2. Proof of Lemma 4
For general analytic function f , Chebyshev series of f is
defined as
1

a0 X
f (x) =
+
ak Tk (x) ,
2
k=1

2
ak =
⇡

Z

1
1

f (x) Tk (x)
p
dx.
1 x2

and from (Mason & Handscomb, 2002) it is known that

ck

ak =

1
X
j=1

( 1)

j

a2j(n+1)

k

+ a2j(n+1)+k

Faster Greedy MAP Inference for Determinantal Point Processes

and |ak |  2M
for 0  k  n. We remind that ck is
⇢k
defined in (7). Using this facts, we get
0
1
1
X
k 2 |ck |  k 2 @|ak | +
a2j(n+1) k + a2j(n+1)+k A
j=1

 k 2 |ak | +
 k 2 |ak | +

1
X

k 2 a2j(n+1)

k

+ k 2 a2j(n+1)+k

(2j(n + 1)

k) a2j(n+1)

j=1

1
X

2

k

j=1

2

+ (2j(n + 1) + k) a2j(n+1)+k

k=0

2

k |ck | 


n
X

k=0
1
X

Gillenwater, Jennifer, Kulesza, Alex, and Taskar, Ben.
Near-optimal map inference for determinantal point processes. In Advances in Neural Information Processing
Systems, pp. 2735–2743, 2012.
Gong, Boqing, Chao, Wei-Lun, Grauman, Kristen, and
Sha, Fei. Diverse sequential subset selection for supervised video summarization. In Advances in Neural Information Processing Systems, pp. 2069–2077, 2014.
Greenbaum, Anne. Iterative methods for solving linear systems. SIAM, 1997.

Therefore, we have
n
X

Feige, Uriel, Mirrokni, Vahab S, and Vondrak, Jan. Maximizing non-monotone submodular functions. SIAM
Journal on Computing, 40(4):1133–1153, 2011.

2

k |ak | +

1
X

k 2 |ak |

k=n+1
1
X
2M
k 2 |ak | 
k2 k
⇢
k=0
k=0

=

2M ⇢ (⇢ + 1)
(⇢

1)

3

This completes the proof of Lemma 4.

References
Avron, Haim and Toledo, Sivan. Randomized algorithms
for estimating the trace of an implicit symmetric positive
semi-definite matrix. Journal of the ACM (JACM), 58(2):
8, 2011.
Bird, Steven. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, pp. 69–72. Association for Computational
Linguistics, 2006.
Boutsidis, Christos, Drineas, Petros, Kambadur, Prabhanjan, and Zouzias, Anastasios. A randomized algorithm for approximating the log determinant of a
symmetric positive definite matrix. arXiv preprint
arXiv:1503.00374, 2015.
Buchbinder, Niv, Feldman, Moran, Seffi, Joseph, and
Schwartz, Roy. A tight linear time (1/2)-approximation
for unconstrained submodular maximization. SIAM
Journal on Computing, 44(5):1384–1402, 2015.
Daley, Daryl J and Vere-Jones, David. An introduction to
the theory of point processes: volume II: general theory and structure. Springer Science & Business Media,
2007.
De Avila, Sandra Eliza Fontes, Lopes, Ana Paula Brandão,
da Luz, Antonio, and de Albuquerque Araújo, Arnaldo.
Vsumm: A mechanism designed to produce static video
summaries and a novel evaluation method. Pattern
Recognition Letters, 32(1):56–68, 2011.

Han, Insu, Malioutov, Dmitry, and Shin, Jinwoo. Largescale log-determinant computation through stochastic
chebyshev expansions. In ICML, pp. 908–917, 2015.
Hausmann, Dirk, Korte, Bernhard, and Jenkyns, TA. Worst
case analysis of greedy type algorithms for independence systems. In Combinatorial Optimization, pp. 120–
131. Springer, 1980.
Hutchinson, Michael F. A stochastic estimator of the trace
of the influence matrix for laplacian smoothing splines.
Communications in Statistics-Simulation and Computation, 19(2):433–450, 1990.
Johansson, Kurt. Course 1 random matrices and determinantal processes. Les Houches, 83:1–56, 2006.
Jordan, Michael Irwin. Learning in graphical models, volume 89. Springer Science & Business Media, 1998.
Kang, Byungkon. Fast determinantal point process sampling with application to clustering. In Advances in
Neural Information Processing Systems, pp. 2319–2327,
2013.
Kathuria, Tarun and Deshpande, Amit. On sampling and
greedy map inference of constrained determinantal point
processes. arXiv preprint arXiv:1607.01551, 2016.
Krause, Andreas, Singh, Ajit, and Guestrin, Carlos. Nearoptimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal
of Machine Learning Research, 9(Feb):235–284, 2008.
Kulesza, Alex and Taskar, Ben. Learning determinantal
point processes. In In Proceedings of UAI. Citeseer,
2011.
Kulesza, Alex, Taskar, Ben, et al. Determinantal point processes for machine learning. Foundations and Trends R
in Machine Learning, 5(2–3):123–286, 2012.

Faster Greedy MAP Inference for Determinantal Point Processes

Kumar, Ravi, Moseley, Benjamin, Vassilvitskii, Sergei, and
Vattani, Andrea. Fast greedy algorithms in mapreduce
and streaming. ACM Transactions on Parallel Computing, 2(3):14, 2015.
Li, Chengtao, Jegelka, Stefanie, and Sra, Suvrit. Efficient
sampling for k-determinantal point processes. In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, pp. 1328–1337, 2016a.

Sharma, Dravyansh, Kapoor, Ashish, and Deshpande,
Amit. On greedy maximization of entropy. In ICML,
pp. 1330–1338, 2015.
Streeter, Matthew and Golovin, Daniel. An online algorithm for maximizing submodular functions. In Advances in Neural Information Processing Systems, pp.
1577–1584, 2009.

Li, Chengtao, Sra, Suvrit, and Jegelka, Stefanie. Gaussian
quadrature for matrix inverse forms with applications.
In Proceedings of The 33rd International Conference on
Machine Learning, pp. 1766–1775, 2016b.

Yao, Jin-ge, Fan, Feifan, Zhao, Wayne Xin, Wan, Xiaojun, Chang, Edward, and Xiao, Jianguo. Tweet timeline
generation with determinantal point processes. In Proceedings of the Thirtieth AAAI Conference on Artificial
Intelligence, pp. 3080–3086. AAAI Press, 2016.

Liu, Yajing, Zhang, Zhenliang, Chong, Edwin KP, and
Pezeshki, Ali. Performance bounds for the k-batch
greedy strategy in optimization problems with curvature. In American Control Conference (ACC), 2016, pp.
7177–7182. IEEE, 2016.

Zhang, Martin J and Ou, Zhijian. Block-wise map inference for determinantal point processes with application
to change-point detection. In Statistical Signal Processing Workshop (SSP), 2016 IEEE, pp. 1–5. IEEE, 2016.

Macchi, Odile. The coincidence approach to stochastic
point processes. Advances in Applied Probability, 7(01):
83–122, 1975.
Mason, John C and Handscomb, David C. Chebyshev polynomials. CRC Press, 2002.
Minoux, Michel. Accelerated greedy algorithms for maximizing submodular set functions. In Optimization Techniques, pp. 234–243. Springer, 1978.
Mirzasoleiman, Baharan, Badanidiyuru, Ashwinkumar,
Karbasi, Amin, Vondrák, Jan, and Krause, Andreas.
Lazier than lazy greedy. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
Nemhauser, George L, Wolsey, Laurence A, and Fisher,
Marshall L. An analysis of approximations for maximizing submodular set functionsi. Mathematical Programming, 14(1):265–294, 1978.
Ouellette, Diane Valerie. Schur complements and statistics.
Linear Algebra and its Applications, 36:187–295, 1981.
Pan, Xinghao, Jegelka, Stefanie, Gonzalez, Joseph E,
Bradley, Joseph K, and Jordan, Michael I. Parallel double greedy submodular maximization. In Advances in
Neural Information Processing Systems, pp. 118–126,
2014.
Peng, Wei and Wang, Hongxia.
Large-scale logdeterminant computation via weighted l 2 polynomial
approximation with prior distribution of eigenvalues. In
International Conference on High Performance Computing and Applications, pp. 120–125. Springer, 2015.
Saad, Yousef. Iterative methods for sparse linear systems.
SIAM, 2003.

