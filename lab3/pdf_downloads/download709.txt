Supplementary Material for â€Collect at Once, Use Effectively:
Making Non-interactive Locally Private Learning Possibleâ€

1. Omitted Proofs in Section 3
Lemma 1 (Lemma 3 in Main Body). Let
x1 , x2 , Â· Â· Â· , xn âˆ¼ i.i.d.D with Âµ = ED [x] and
supp(D) âŠ† B(0, 1). Let G and {yi }ni=1 defined in the
above procedure. For each of group Sj fixed, we have the
following with probability 2/3:
!

 1 X
p log(nd)


p
(1)
yi âˆ’ GÂµ â‰¤ O

|Sj |
1
 |Sj |
yi âˆˆSj
P
Proof. Apparently |S1j | iâˆˆSj ri âˆ¼ N (0, 2 log(1.25/Î´)
Id ).
2

 |Sj |
P
So we have k |S1j | iâˆˆSj ri k1 â‰¤ O pâˆšlog n with proba

|Sj |

1
9.

bility We then turn to bound the loss incurred by random
sample of data.
d

1 X 
1 X

2
EÂµ âˆ’
xi  =
var(x1l )
|Sj |
|Sj |
iâˆˆSj

l=1

(2)

d

1
1 X
E[x21l ] â‰¤
.
â‰¤
|Sj |
|Sj |
l=1

According to Markov Inequality, we have
ï£±
ï£¼
ï£²
1 X 
9 ï£½ 1

2
P Âµ âˆ’
xi  â‰¥
â‰¤
ï£³
|Sj |
|Sj | ï£¾ 9
iâˆˆSj

Given x1 , x2 , Â· Â· Â· , xn fixed under this event, wePcan easily
derive upper bounds on entries of G(Âµ âˆ’ |S1j | iâˆˆSj xi ):
P
for g âˆ¼ N (0, Id ) and q = Âµ âˆ’ |S1j | iâˆˆSj xi , we have
q
d
1
|g T q| â‰¤ 12 log
|Sj | with probability 1âˆ’ 9d . By union bound
we have the following with probability 29 :


1 X


xi ) â‰¤ O
G(Âµ âˆ’
|Sj |
1
iâˆˆSj

s

p log d
|Sj |

!
.

Putting the two inequalities together using union bound, we
get the result.
Lemma 2 (Lemma 6 in Main Body). Under the assumptions made in Section 3.2, given projection matrix Î¦, with

high probability over the randomness of private mechanism, we have

r
m
(3)
LÌ„(wpriv ; XÌ„, y) âˆ’ LÌ„(wÌ‚âˆ— ; XÌ„, y) 6 OÌƒ
n2
Proof. Note, once we prove the uniform
p m  convergence of
|LÌ‚(w; Z, v) âˆ’ LÌ„(w; XÌ„, y)| 6 O
n2 for any w âˆˆ C,
then the conclusion holds directly. Now, we will prove
the uniform convergence. Note Z = XÌ„ + E, where
E âˆˆ RnÃ—m , and each entry eij âˆ¼ N (0, Ïƒ 2 ), v = y + r,
where r âˆ¼ N (0, Ïƒ 2 In ). Denote wÌ„ = Î¦T w.




LÌ‚(w; Z, v) âˆ’ LÌ„(w; XÌ„, y)


 1 T

1 T
T
T

=  wÌ„ (Q âˆ’ XÌ„ XÌ„)wÌ„ âˆ’
v Z wÌ„ âˆ’ y XÌ„ wÌ„ 
2n
n



1 
Q âˆ’ XÌ„ T XÌ„  kwÌ„k2 + 1 v T Z wÌ„ âˆ’ y T XÌ„ wÌ„
6
2
2
2n
n

1 
Q âˆ’ XÌ„ T XÌ„  kwÌ„k2 + 1 |v T Z wÌ„ âˆ’ y T XÌ„ wÌ„|
6
2
F
2n
n

1  T
2
T

6 2n Z Z âˆ’ nÏƒ Im âˆ’ XÌ„ XÌ„ F kwÌ„k22 + n1 |v T Z wÌ„ âˆ’ y T XÌ„ wÌ„|



1 
E T E âˆ’ nÏƒ 2 Im  kwÌ„k2 + 1 XÌ„ T E  kwÌ„k2 +
6
2
2
F
F
2n
n






1 
E T y  + XÌ„ T r  + E T r  kwÌ„k
2
2
2
2
n
From the property of random projection, we know
kwÌ„k2 6 1 with high probability. Besides, as each entry in E is i.i.d. Gaussian, and E[E T E] = nÏƒ 2 Im,
q


1  T
thus we have 2n
E E âˆ’ nÏƒ 2 Im 2 6 O Ïƒ lognm
with high probability according
q to lemma 3, hence


m log m
1  T
2

) with high prob2n E E âˆ’ nÏƒ Im F 6 O(Ïƒ
n
ability.

2
Pm Pm
As n12 XÌ„ T E F = n12 j=1 ( i=1 (qjT ei )2 ), where
qj , ei are the j-th and i-th column
Pm of XÌ„ and E respectively. For each j âˆˆ [m], n12 i=1 (qjT ei )2 obeys Chisquare distribution (with some scaling),
thus with high

Pm
mkqj k2 Ïƒ 2
probability, n12 i=1 (qjT ei )2 6 O
. There2
Pm Pnm
1
T
fore, by union bound, we have n2 j=1 ( i=1 (qj ei )2 ) 6
 P

 2
P
m j kqj k2 Ïƒ 2
2
mÏƒ
O
=
O
, as
=
j kqj k
n2
n

Non-interactive Local DP Learning

 2
XÌ„  6 n. Hence, there is
F

1
n

q

 T 
mÏƒ 2
XÌ„ E  6 O
n
F

with high probability.
we have
q Using
 similar augument,q





2
1  T 
1  T 
mÏƒ
mÏƒ 2
, n EÌ„ r 2 6 O
n EÌ„ y 2 6 O
n
n


1  T 
with high probability. For n XÌ„ r , according to matrix
concentration inequality (Theorem
 4.1.1
 in (Tropp et al.,


1
1  T 
2015)), we have n XÌ„ r 2 6 O âˆšn .
Combine all these results together, we obtain the desired
conclusion.
Lemma 3 ((Vershynin, 2009)). Suppose x âˆˆ Rd be a random vector satisfies E[xxT ] = Id . Denote kxkÏ†1 = M ,
where kÂ·kÏˆ1 represents Orlicz Ïˆ1 -norm. Let x1 , . . . , xn be
independent copies of x, then for every  âˆˆ (0, 1), we have
 n

!
1 X

2
2


Pr 
xi xTi âˆ’ Id  >  6 deâˆ’n /4M

n
i=1

Proof. On one hand,
L(wpriv ) âˆ’ L(wâˆ— )
=L(wpriv ) âˆ’ LÌ„(wpriv ) + LÌ„(wpriv ) âˆ’ LÌ„(wÌ‚âˆ— )
+ LÌ„(wÌ‚âˆ— ) âˆ’ LÌ„(wâˆ— ) + LÌ„(wâˆ— ) âˆ’ L(wâˆ— )


6 L(wpriv ) âˆ’ LÌ„(wpriv ) + LÌ„(wâˆ— ) âˆ’ L(wâˆ— )

Corollary 1 (Corollary 2 in Main Body). Algorithm LDP
kernel mechanism satisfies (, Î´)-LDP, and with high probability, there is
1/4 !

d
LHÌ‚ (wÌ‚priv ) âˆ’ LH (f âˆ— ) 6 OÌƒ
n2
1/8 !

d
T âˆ—
T priv
sup |Î¦(x) f âˆ’ (Î¦Ì‚(x)) wÌ‚
| 6 OÌƒ
n2
xâˆˆX

) âˆ’ LÌ„(wÌ‚ )]

nearly borrow the proof of Lemma 17 in (Rubinstein et al.,
2012) and property of RRF , we have
s !
d
LHÌ‚ (g âˆ— ) âˆ’ LH (f âˆ— ) 6 OÌƒ
dp

2. Omitted contents and proofs in Section 4

i

âˆ—

Proof. Algorithm satisfies local privacy is obvious. For
excess risk, as LHÌ‚ (wÌ‚priv ) âˆ’ LH (f âˆ— ) = LHÌ‚ (wÌ‚priv ) âˆ’
LHÌ‚ (g âˆ— ) + LHÌ‚ (g âˆ— ) âˆ’ LH (f âˆ— ), follow nearly the same
proof of lemma 5 of sparse linearregression, we have
q
dp
LHÌ‚ (wÌ‚priv ) âˆ’ LHÌ‚ (g âˆ— ) 6 OÌƒ
n2 . On the other hand,

Combine
above
two inequalities, and choose optimal dp as

âˆš
dn2 , we obtain the first inequality of the concluOÌƒ
sion. Then combine lemma 7 in this paper, it is easy to
obtaint the second inequality.

+ LÌ„(wpriv ) âˆ’ LÌ„(wÌ‚âˆ— )


 


6G[max{| wpriv , xi âˆ’ Î¦T wpriv , Î¦T xi |}
i



+ max{| hwâˆ— , xi i âˆ’ Î¦T wâˆ— , Î¦T xi |}]
+ [LÌ„(w

pm
From lemma 2, we know LÌ„(wÌ„priv ) âˆ’ LÌ„(wÌ„âˆ— ) 6 OÌƒ
n2
holds with high probability. Combine these two inequalities, it is easy to determine the optimal m, then obtain the
conclusion.

2

Theorem 1 (Theorem 3 in Main Body).
p Underthe assumption in this section, set m = Î˜
n2 log d for Î² > 0,
then with high probability , there is

1/4 !
log
d
L(wpriv ) âˆ’ L(wâˆ— ) = OÌƒ
n2

priv

âˆ€w âˆˆ C, âˆ€x âˆˆD. Therefore,
the first term in equation (4)

q
log d
is less than O
.
m

(4)

(where G is the Lipschitz constant)
On the other hand, for âˆ€w âˆˆ C, âˆ€x âˆˆ D, there is



| hw, xi âˆ’ Î¦T w, Î¦T x |

2.1. Relations between smooth generalized linear losses
(SGLL) and generalized linear models (GLM)
Note that a model is called GLM, if for x, wâˆ— âˆˆ Rd , label
y with respect to x is given by a distribution which belongs
to the exponential family:


yÎ¸ âˆ’ b(Î¸)
âˆ—
p(y|x, w ) = exp
+ c(y, Î¦)
(5)
Î¦


 T

 kÎ¦ (w+x)k2 âˆ’kÎ¦T (wâˆ’x)k2
2
2
kw+xk
âˆ’kwâˆ’xk
2
2
2
2
âˆ’
=
4
4

 T


2
2
 kÎ¦ (w+x)k âˆ’kw+xk22   kÎ¦T (wâˆ’x)k âˆ’kwâˆ’xk22 
2
2
+
 where Î¸, Î¦ are parameters, and b(Î¸), c(y, Î¦) are known
6
4
4
 

According to the results of random projection w.r.t. additive error (Dirksen, 2016), we know with high probabil
q



log d
ity, there is | hw, xi âˆ’ Î¦T w, Î¦T x | 6 O
, for
m

functions. Besides, there is an one-to-one continuous differentiable transformation g(Â·) such that g(b0 (Î¸)) = xT wâˆ— .
According to the key equality g(b0 (Î¸)) = xT wâˆ— , usually
we can obtain smooth function Î¸ = h1 (xT wâˆ— ), b(Î¸) =
h2 (xT wâˆ— ), and whatâ€™s more, univariate function

Non-interactive Local DP Learning

hi (x)(i = 1, 2) satisfies the absolutely smooth property.
For such GLM, if we consider optimizing the expected negative logarithmic probability âˆ’E(x,y)âˆ¼D log p(x, y; w),
once discarding unrelated terms to w, we obtain the new
population loss, L(w) := E(x,y)âˆ¼D `(w; x, y), where
`(w; x, y) = âˆ’yh1 (xT w)+h2 (xT w), exactly the form of
smooth generalized linear loss defined in section 4. Hence
our SGLL is a natural loss defined by GLM with additional
smoothness assumptions.
2.2. Omitted proofs
Lemma 4 (Lemma 8 in Main Body). Given any Î± > 0,
by setting k = c ln Î±1 , p = dk +eÂµ2 (k; r)e, where c is a


constant, we have fË†p (x) âˆ’ f (x) 6 Î±.
âˆž

Proof. As f, f 0 , 
Â· Â· Â· , f(kâˆ’1) are absolutely continuous
over [âˆ’1, 1], and f (k) T 6 Âµ1 (k; r)Âµ2 (k; r)k , according
to the results in (Trefethen, 2008), we have




2 f (k) T
Ë†

fp (x) âˆ’ f (x) 6
Ï€k(p âˆ’ k)k
âˆž
2Âµ1 (k; r)
6
(6)
Ï€kek
It is easy to see there exists c > 0, such that the term (6) is
less than Î± with chosen k, hence the conclusion holds.
Lemma 5 (Lemma 9 in Main Body). For any Î³ > 0,
setting k = c ln 4r
Î³ , p = dk + 2Âµ2 (k; r)e, then algorithm
 7 outputs a (Î³, Î², Ïƒ) stochastic oracle, where Ïƒ =
OÌƒ Ïƒ0 + Î³ +

p2p+1 (4r)p+1
p+2

.

Proof. According to lemma 4, we know the approximation
Î³
error, |mÌ‚(w; x, y) âˆ’ m(w; x, y)| 6 2r
. For any fixed
(x, y), from the construction of stochastic inexact gradient oracle, there is E[GÌƒ(w; b)|x, y] = GÌ‚(w; x, y). Denote
gÌ‚(w) = E(x,y)âˆ¼D [GÌ‚(w; x, y)], thus we have


2 
2 




E GÌƒ(w; b) âˆ’ gÌ‚(w) =E GÌƒ(w; b) âˆ’ GÌ‚(w; x, y)

2 


+ E GÌ‚(w; x, y) âˆ’ gÌ‚(w)

approximation error, we know |(g(w)âˆ’gÌ‚(w))T (vâˆ’w)| 6
Î³
2 . Whatâ€™s more, as L(w) is convex and Î²-smooth, that
2
is 0 6 L(v) âˆ’ L(w) âˆ’ g(w)T (v âˆ’ w) 6 Î²2 kv âˆ’ wk .
Combined these inequalities, we obtain
âˆ’ Î³2 6 L(v) âˆ’ L(w) âˆ’ gÌ‚(w)T (v âˆ’ w) 6

Î²
2

2

kv âˆ’ wk +

â‡â‡’0 6 L(v) âˆ’ (L(w) âˆ’ Î³2 ) âˆ’ gÌ‚(w)T (v âˆ’ w) 6

Î²
2

Î³
2
2

kv âˆ’ wk + Î³

Note the function value oracles in the stochastic oracle
definition (either FÎ³,Î²,Ïƒ (Â·) or fÎ³,Î²,Ïƒ (Â·)) do not play any
role in the optimization algorithm, hence we can set it as
L(w) âˆ’ Î³2 , though we do not know how to calculate.
Lemma 6. Based on above statements, we have


 4p+2
2 
p
(4r)2p+2


E GÌƒ(w; b) âˆ’ GÌ‚(w; x, y) 6 OÌƒ
2p+4

2 


E GÌ‚(w; x, y) âˆ’ gÌ‚(w) 6 (Î³ + Ïƒ0 )2
Proof. First, we calculate the variance of each tk ,
Qj(j+1)/2
var(tj ) 6 i=j(jâˆ’1)/2+1 (var(wT zi ) + (E[wT zi ])2 ) 6


2j
OÌƒ ( p(p+1)
)
.

Next, we upper bound the coefficient ck (as it is the same
for
Pp c1k and c2k , hence we use ck for short). Note ck =
m=k am bmk , where am is the coefficient of original
function represented by Chebyshev basis, bmk is the coefficient of order k monomial in Chebyshev basis Tm (x),
where 0 6 k 6 m. According to the formula of Tm (x)
given in (Qazi & Rahman, 2007) and well-known Stirlingâ€™s
approximation, after some translation, we have

|bmk | 6 max1 O

âˆš

Î¸âˆˆ(0, 2 )

6O

âˆš

m2m

(1 âˆ’ Î¸)1âˆ’Î¸
mÂ· Î¸
Î¸ (1 âˆ’ 2Î¸)1âˆ’2Î¸


m 



Besides, from the absolutely smooth property of h0i (x)(i âˆˆ
{1, 2}) and the convergence
results in (Trefethen,
2008),

Pp
a
b
we have am 6 O m12 , thus ck =
m=k m mk 6
O (2p ). Hence, there is
h
i


2
var (c2k âˆ’ c1k zy )tk rk+1 6r2k+2 E ((c2k âˆ’ c1k zy )tk )
 4k+2

p
(4r)2p+2
6O
2k+2

For above two terms, combined with results given in lemma
6, we we obtain
2 !


2 
r(2rp)p+1


As each (c2k âˆ’c1k zy )tk rk+1 is independent with each other
+ Î³ + Ïƒ0
E GÌƒ(w; b) âˆ’ g(w) 6 OÌƒ
p+2

(for different k), which leads to
" p
#
.
 4p+2

X
p
(4r)2p+2
k+1
T
var
(c2k âˆ’ c1k zy )tk r
6O
As L(v) âˆ’ L(w) âˆ’ gÌ‚(w) (v âˆ’ w) = L(v) âˆ’ L(w) âˆ’
2p+2
k=0
g(w)T (v âˆ’ w) + (g(w) âˆ’ gÌ‚(w))T (v âˆ’ w), and from the

Non-interactive Local DP Learning

Moreover, var(z0 ) 6 O

1
2



References

. Therefore,

Dirksen, Sjoerd. Dimensionality reduction with subgaussian matrices: a unified theory. Foundations of Computational Mathematics, 16(5):1367â€“1396, 2016.


 4p+2

2 
p
(4r)2p+2


E GÌƒ(w; b) âˆ’ GÌ‚(w; x, y) 6 OÌƒ
2p+4
For second inequality in the conclusion, there is

Qazi, MA and Rahman, QI. Some coefficient estimates for
polynomials on the unit interval. Serdica Mathematical
Journal, 33(4):449pâ€“474p, 2007.


2 


E GÌ‚(w; x, y) âˆ’ gÌ‚(w)





2
6E GÌ‚(w; x, y) âˆ’ G(w; x, y) + G(w; x, y) âˆ’ g(w) + g(w) âˆ’ gÌ‚(w)

6Î³ 2 + Ïƒ02 + 2Ïƒ0 Î³ = (Î³ + Ïƒ0 )2

Trefethen, Lloyd N. Is gauss quadrature better than
clenshawâ€“curtis? SIAM review, 50(1):67â€“87, 2008.

Proposition 1. f âˆš
(x) = ln(1 + eâˆ’x ) is absolutely smooth
with Âµ1 (k; r) = r 4kÏ€ 3 , Âµ2 (k; r) = rk
e

Tropp, Joel A et al. An introduction to matrix concentraR in Machine
tion inequalities. Foundations and Trends
Learning, 8(1-2):1â€“230, 2015.

Proof. For any r, k > 0, the absolutely
continuous
of


f (k) (rx) is obvious, now consider f (k+1) (rx)T :

T

Vershynin, Roman. A note on sums of independent random
matrices after ahlswede-winter. Lecture notes, 2009.

1

|f (k+2) (rx)|
âˆš
dx
1 âˆ’ x2
âˆ’1




6Ï€ f (k+2) (rx)

Z


 (k+1) 
f
 =

âˆž


k+j
(âˆ’1)
Ak+1,jâˆ’1 f j (1 âˆ’ f )k+2âˆ’j 
j=1

P
k+1

6Ï€rk+2 

6Ï€rk+2

k+1
X

âˆž

Ak+1,jâˆ’1

j=1

6Ï€(k + 1)!rk+2
âˆš
6 4Ï€ 3 rk+2 (k + 1)k+3/2 eâˆ’kâˆ’1

k+1
p
r(k + 1)
3
=r 4Ï€ (k + 1)
e

Theorem 2 (Theorem 6 in Main Body). For any Î± > 0,
set Î³ = Î±2 , k = c ln 4r
if n >
Î³ , p = dk + 2Âµ2 (k; r)e,



8r 4r ln ln(8r/Î±) 4r 2cr ln(8r/Î±)+2
1
O (Î±)
, using al
Î±2 2
gorithms 6,7,8, then we have L(wpriv ) âˆ’ L(wâˆ— ) 6 Î±.
Proof. According to lemma 10 in main body, with
a (Î³, Î², Ïƒ) stochastic oracle,  SIGM algorithm con-

verges with rate O âˆšÏƒn + Î³ .
In order to have


 4p+2 2p+2 
O âˆšÏƒn + Î³ 6 Î±, it suffices if n > O p Î±2 (4r)
=
2p+4




2cr
ln(8r/Î±)+2
1
4r ln ln(8r/Î±) 4r
O ( 8r
, as Ïƒ =
Î±)

Î±2 2
 2p+1

p+1
(4r)
O p p+2
according to lemma 5 (ignoring negligible Ïƒ0 , Î³).

Rubinstein, B., Bartlett, P. L., Huang, L., and Taft, N.
Learning in a large function space: Privacy-preserving
mechanisms for svm learning. Journal of Privacy and
Confidentiality, 4(1):4, 2012.

