Supplementary Material for ”Collect at Once, Use Effectively:
Making Non-interactive Locally Private Learning Possible”

1. Omitted Proofs in Section 3
Lemma 1 (Lemma 3 in Main Body). Let
x1 , x2 , · · · , xn ∼ i.i.d.D with µ = ED [x] and
supp(D) ⊆ B(0, 1). Let G and {yi }ni=1 defined in the
above procedure. For each of group Sj fixed, we have the
following with probability 2/3:
!

 1 X
p log(nd)


p
(1)
yi − Gµ ≤ O

|Sj |
1
 |Sj |
yi ∈Sj
P
Proof. Apparently |S1j | i∈Sj ri ∼ N (0, 2 log(1.25/δ)
Id ).
2

 |Sj |
P
So we have k |S1j | i∈Sj ri k1 ≤ O p√log n with proba

|Sj |

1
9.

bility We then turn to bound the loss incurred by random
sample of data.
d

1 X 
1 X

2
Eµ −
xi  =
var(x1l )
|Sj |
|Sj |
i∈Sj

l=1

(2)

d

1
1 X
E[x21l ] ≤
.
≤
|Sj |
|Sj |
l=1

According to Markov Inequality, we have



1 X 
9  1

2
P µ −
xi  ≥
≤

|Sj |
|Sj |  9
i∈Sj

Given x1 , x2 , · · · , xn fixed under this event, wePcan easily
derive upper bounds on entries of G(µ − |S1j | i∈Sj xi ):
P
for g ∼ N (0, Id ) and q = µ − |S1j | i∈Sj xi , we have
q
d
1
|g T q| ≤ 12 log
|Sj | with probability 1− 9d . By union bound
we have the following with probability 29 :


1 X


xi ) ≤ O
G(µ −
|Sj |
1
i∈Sj

s

p log d
|Sj |

!
.

Putting the two inequalities together using union bound, we
get the result.
Lemma 2 (Lemma 6 in Main Body). Under the assumptions made in Section 3.2, given projection matrix Φ, with

high probability over the randomness of private mechanism, we have

r
m
(3)
L̄(wpriv ; X̄, y) − L̄(ŵ∗ ; X̄, y) 6 Õ
n2
Proof. Note, once we prove the uniform
p m  convergence of
|L̂(w; Z, v) − L̄(w; X̄, y)| 6 O
n2 for any w ∈ C,
then the conclusion holds directly. Now, we will prove
the uniform convergence. Note Z = X̄ + E, where
E ∈ Rn×m , and each entry eij ∼ N (0, σ 2 ), v = y + r,
where r ∼ N (0, σ 2 In ). Denote w̄ = ΦT w.




L̂(w; Z, v) − L̄(w; X̄, y)


 1 T

1 T
T
T

=  w̄ (Q − X̄ X̄)w̄ −
v Z w̄ − y X̄ w̄ 
2n
n



1 
Q − X̄ T X̄  kw̄k2 + 1 v T Z w̄ − y T X̄ w̄
6
2
2
2n
n

1 
Q − X̄ T X̄  kw̄k2 + 1 |v T Z w̄ − y T X̄ w̄|
6
2
F
2n
n

1  T
2
T

6 2n Z Z − nσ Im − X̄ X̄ F kw̄k22 + n1 |v T Z w̄ − y T X̄ w̄|



1 
E T E − nσ 2 Im  kw̄k2 + 1 X̄ T E  kw̄k2 +
6
2
2
F
F
2n
n






1 
E T y  + X̄ T r  + E T r  kw̄k
2
2
2
2
n
From the property of random projection, we know
kw̄k2 6 1 with high probability. Besides, as each entry in E is i.i.d. Gaussian, and E[E T E] = nσ 2 Im,
q


1  T
thus we have 2n
E E − nσ 2 Im 2 6 O σ lognm
with high probability according
q to lemma 3, hence


m log m
1  T
2

) with high prob2n E E − nσ Im F 6 O(σ
n
ability.

2
Pm Pm
As n12 X̄ T E F = n12 j=1 ( i=1 (qjT ei )2 ), where
qj , ei are the j-th and i-th column
Pm of X̄ and E respectively. For each j ∈ [m], n12 i=1 (qjT ei )2 obeys Chisquare distribution (with some scaling),
thus with high

Pm
mkqj k2 σ 2
probability, n12 i=1 (qjT ei )2 6 O
. There2
Pm Pnm
1
T
fore, by union bound, we have n2 j=1 ( i=1 (qj ei )2 ) 6
 P

 2
P
m j kqj k2 σ 2
2
mσ
O
=
O
, as
=
j kqj k
n2
n

Non-interactive Local DP Learning

 2
X̄  6 n. Hence, there is
F

1
n

q

 T 
mσ 2
X̄ E  6 O
n
F

with high probability.
we have
q Using
 similar augument,q





2
1  T 
1  T 
mσ
mσ 2
, n Ē r 2 6 O
n Ē y 2 6 O
n
n


1  T 
with high probability. For n X̄ r , according to matrix
concentration inequality (Theorem
 4.1.1
 in (Tropp et al.,


1
1  T 
2015)), we have n X̄ r 2 6 O √n .
Combine all these results together, we obtain the desired
conclusion.
Lemma 3 ((Vershynin, 2009)). Suppose x ∈ Rd be a random vector satisfies E[xxT ] = Id . Denote kxkφ1 = M ,
where k·kψ1 represents Orlicz ψ1 -norm. Let x1 , . . . , xn be
independent copies of x, then for every  ∈ (0, 1), we have
 n

!
1 X

2
2


Pr 
xi xTi − Id  >  6 de−n /4M

n
i=1

Proof. On one hand,
L(wpriv ) − L(w∗ )
=L(wpriv ) − L̄(wpriv ) + L̄(wpriv ) − L̄(ŵ∗ )
+ L̄(ŵ∗ ) − L̄(w∗ ) + L̄(w∗ ) − L(w∗ )


6 L(wpriv ) − L̄(wpriv ) + L̄(w∗ ) − L(w∗ )

Corollary 1 (Corollary 2 in Main Body). Algorithm LDP
kernel mechanism satisfies (, δ)-LDP, and with high probability, there is
1/4 !

d
LĤ (ŵpriv ) − LH (f ∗ ) 6 Õ
n2
1/8 !

d
T ∗
T priv
sup |Φ(x) f − (Φ̂(x)) ŵ
| 6 Õ
n2
x∈X

) − L̄(ŵ )]

nearly borrow the proof of Lemma 17 in (Rubinstein et al.,
2012) and property of RRF , we have
s !
d
LĤ (g ∗ ) − LH (f ∗ ) 6 Õ
dp

2. Omitted contents and proofs in Section 4

i

∗

Proof. Algorithm satisfies local privacy is obvious. For
excess risk, as LĤ (ŵpriv ) − LH (f ∗ ) = LĤ (ŵpriv ) −
LĤ (g ∗ ) + LĤ (g ∗ ) − LH (f ∗ ), follow nearly the same
proof of lemma 5 of sparse linearregression, we have
q
dp
LĤ (ŵpriv ) − LĤ (g ∗ ) 6 Õ
n2 . On the other hand,

Combine
above
two inequalities, and choose optimal dp as

√
dn2 , we obtain the first inequality of the concluÕ
sion. Then combine lemma 7 in this paper, it is easy to
obtaint the second inequality.

+ L̄(wpriv ) − L̄(ŵ∗ )


 


6G[max{| wpriv , xi − ΦT wpriv , ΦT xi |}
i



+ max{| hw∗ , xi i − ΦT w∗ , ΦT xi |}]
+ [L̄(w

pm
From lemma 2, we know L̄(w̄priv ) − L̄(w̄∗ ) 6 Õ
n2
holds with high probability. Combine these two inequalities, it is easy to determine the optimal m, then obtain the
conclusion.

2

Theorem 1 (Theorem 3 in Main Body).
p Underthe assumption in this section, set m = Θ
n2 log d for β > 0,
then with high probability , there is

1/4 !
log
d
L(wpriv ) − L(w∗ ) = Õ
n2

priv

∀w ∈ C, ∀x ∈D. Therefore,
the first term in equation (4)

q
log d
is less than O
.
m

(4)

(where G is the Lipschitz constant)
On the other hand, for ∀w ∈ C, ∀x ∈ D, there is



| hw, xi − ΦT w, ΦT x |

2.1. Relations between smooth generalized linear losses
(SGLL) and generalized linear models (GLM)
Note that a model is called GLM, if for x, w∗ ∈ Rd , label
y with respect to x is given by a distribution which belongs
to the exponential family:


yθ − b(θ)
∗
p(y|x, w ) = exp
+ c(y, Φ)
(5)
Φ


 T

 kΦ (w+x)k2 −kΦT (w−x)k2
2
2
kw+xk
−kw−xk
2
2
2
2
−
=
4
4

 T


2
2
 kΦ (w+x)k −kw+xk22   kΦT (w−x)k −kw−xk22 
2
2
+
 where θ, Φ are parameters, and b(θ), c(y, Φ) are known
6
4
4
 

According to the results of random projection w.r.t. additive error (Dirksen, 2016), we know with high probabil
q



log d
ity, there is | hw, xi − ΦT w, ΦT x | 6 O
, for
m

functions. Besides, there is an one-to-one continuous differentiable transformation g(·) such that g(b0 (θ)) = xT w∗ .
According to the key equality g(b0 (θ)) = xT w∗ , usually
we can obtain smooth function θ = h1 (xT w∗ ), b(θ) =
h2 (xT w∗ ), and what’s more, univariate function

Non-interactive Local DP Learning

hi (x)(i = 1, 2) satisfies the absolutely smooth property.
For such GLM, if we consider optimizing the expected negative logarithmic probability −E(x,y)∼D log p(x, y; w),
once discarding unrelated terms to w, we obtain the new
population loss, L(w) := E(x,y)∼D `(w; x, y), where
`(w; x, y) = −yh1 (xT w)+h2 (xT w), exactly the form of
smooth generalized linear loss defined in section 4. Hence
our SGLL is a natural loss defined by GLM with additional
smoothness assumptions.
2.2. Omitted proofs
Lemma 4 (Lemma 8 in Main Body). Given any α > 0,
by setting k = c ln α1 , p = dk +eµ2 (k; r)e, where c is a


constant, we have fˆp (x) − f (x) 6 α.
∞

Proof. As f, f 0 , 
· · · , f(k−1) are absolutely continuous
over [−1, 1], and f (k) T 6 µ1 (k; r)µ2 (k; r)k , according
to the results in (Trefethen, 2008), we have




2 f (k) T
ˆ

fp (x) − f (x) 6
πk(p − k)k
∞
2µ1 (k; r)
6
(6)
πkek
It is easy to see there exists c > 0, such that the term (6) is
less than α with chosen k, hence the conclusion holds.
Lemma 5 (Lemma 9 in Main Body). For any γ > 0,
setting k = c ln 4r
γ , p = dk + 2µ2 (k; r)e, then algorithm
 7 outputs a (γ, β, σ) stochastic oracle, where σ =
Õ σ0 + γ +

p2p+1 (4r)p+1
p+2

.

Proof. According to lemma 4, we know the approximation
γ
error, |m̂(w; x, y) − m(w; x, y)| 6 2r
. For any fixed
(x, y), from the construction of stochastic inexact gradient oracle, there is E[G̃(w; b)|x, y] = Ĝ(w; x, y). Denote
ĝ(w) = E(x,y)∼D [Ĝ(w; x, y)], thus we have


2 
2 




E G̃(w; b) − ĝ(w) =E G̃(w; b) − Ĝ(w; x, y)

2 


+ E Ĝ(w; x, y) − ĝ(w)

approximation error, we know |(g(w)−ĝ(w))T (v−w)| 6
γ
2 . What’s more, as L(w) is convex and β-smooth, that
2
is 0 6 L(v) − L(w) − g(w)T (v − w) 6 β2 kv − wk .
Combined these inequalities, we obtain
− γ2 6 L(v) − L(w) − ĝ(w)T (v − w) 6

β
2

2

kv − wk +

⇐⇒0 6 L(v) − (L(w) − γ2 ) − ĝ(w)T (v − w) 6

β
2

γ
2
2

kv − wk + γ

Note the function value oracles in the stochastic oracle
definition (either Fγ,β,σ (·) or fγ,β,σ (·)) do not play any
role in the optimization algorithm, hence we can set it as
L(w) − γ2 , though we do not know how to calculate.
Lemma 6. Based on above statements, we have


 4p+2
2 
p
(4r)2p+2


E G̃(w; b) − Ĝ(w; x, y) 6 Õ
2p+4

2 


E Ĝ(w; x, y) − ĝ(w) 6 (γ + σ0 )2
Proof. First, we calculate the variance of each tk ,
Qj(j+1)/2
var(tj ) 6 i=j(j−1)/2+1 (var(wT zi ) + (E[wT zi ])2 ) 6


2j
Õ ( p(p+1)
)
.

Next, we upper bound the coefficient ck (as it is the same
for
Pp c1k and c2k , hence we use ck for short). Note ck =
m=k am bmk , where am is the coefficient of original
function represented by Chebyshev basis, bmk is the coefficient of order k monomial in Chebyshev basis Tm (x),
where 0 6 k 6 m. According to the formula of Tm (x)
given in (Qazi & Rahman, 2007) and well-known Stirling’s
approximation, after some translation, we have

|bmk | 6 max1 O

√

θ∈(0, 2 )

6O

√

m2m

(1 − θ)1−θ
m· θ
θ (1 − 2θ)1−2θ


m 



Besides, from the absolutely smooth property of h0i (x)(i ∈
{1, 2}) and the convergence
results in (Trefethen,
2008),

Pp
a
b
we have am 6 O m12 , thus ck =
m=k m mk 6
O (2p ). Hence, there is
h
i


2
var (c2k − c1k zy )tk rk+1 6r2k+2 E ((c2k − c1k zy )tk )
 4k+2

p
(4r)2p+2
6O
2k+2

For above two terms, combined with results given in lemma
6, we we obtain
2 !


2 
r(2rp)p+1


As each (c2k −c1k zy )tk rk+1 is independent with each other
+ γ + σ0
E G̃(w; b) − g(w) 6 Õ
p+2

(for different k), which leads to
" p
#
.
 4p+2

X
p
(4r)2p+2
k+1
T
var
(c2k − c1k zy )tk r
6O
As L(v) − L(w) − ĝ(w) (v − w) = L(v) − L(w) −
2p+2
k=0
g(w)T (v − w) + (g(w) − ĝ(w))T (v − w), and from the

Non-interactive Local DP Learning

Moreover, var(z0 ) 6 O

1
2



References

. Therefore,

Dirksen, Sjoerd. Dimensionality reduction with subgaussian matrices: a unified theory. Foundations of Computational Mathematics, 16(5):1367–1396, 2016.


 4p+2

2 
p
(4r)2p+2


E G̃(w; b) − Ĝ(w; x, y) 6 Õ
2p+4
For second inequality in the conclusion, there is

Qazi, MA and Rahman, QI. Some coefficient estimates for
polynomials on the unit interval. Serdica Mathematical
Journal, 33(4):449p–474p, 2007.


2 


E Ĝ(w; x, y) − ĝ(w)





2
6E Ĝ(w; x, y) − G(w; x, y) + G(w; x, y) − g(w) + g(w) − ĝ(w)

6γ 2 + σ02 + 2σ0 γ = (γ + σ0 )2

Trefethen, Lloyd N. Is gauss quadrature better than
clenshaw–curtis? SIAM review, 50(1):67–87, 2008.

Proposition 1. f √
(x) = ln(1 + e−x ) is absolutely smooth
with µ1 (k; r) = r 4kπ 3 , µ2 (k; r) = rk
e

Tropp, Joel A et al. An introduction to matrix concentraR in Machine
tion inequalities. Foundations and Trends
Learning, 8(1-2):1–230, 2015.

Proof. For any r, k > 0, the absolutely
continuous
of


f (k) (rx) is obvious, now consider f (k+1) (rx)T :

T

Vershynin, Roman. A note on sums of independent random
matrices after ahlswede-winter. Lecture notes, 2009.

1

|f (k+2) (rx)|
√
dx
1 − x2
−1




6π f (k+2) (rx)

Z


 (k+1) 
f
 =

∞


k+j
(−1)
Ak+1,j−1 f j (1 − f )k+2−j 
j=1

P
k+1

6πrk+2 

6πrk+2

k+1
X

∞

Ak+1,j−1

j=1

6π(k + 1)!rk+2
√
6 4π 3 rk+2 (k + 1)k+3/2 e−k−1

k+1
p
r(k + 1)
3
=r 4π (k + 1)
e

Theorem 2 (Theorem 6 in Main Body). For any α > 0,
set γ = α2 , k = c ln 4r
if n >
γ , p = dk + 2µ2 (k; r)e,



8r 4r ln ln(8r/α) 4r 2cr ln(8r/α)+2
1
O (α)
, using al
α2 2
gorithms 6,7,8, then we have L(wpriv ) − L(w∗ ) 6 α.
Proof. According to lemma 10 in main body, with
a (γ, β, σ) stochastic oracle,  SIGM algorithm con-

verges with rate O √σn + γ .
In order to have


 4p+2 2p+2 
O √σn + γ 6 α, it suffices if n > O p α2 (4r)
=
2p+4




2cr
ln(8r/α)+2
1
4r ln ln(8r/α) 4r
O ( 8r
, as σ =
α)

α2 2
 2p+1

p+1
(4r)
O p p+2
according to lemma 5 (ignoring negligible σ0 , γ).

Rubinstein, B., Bartlett, P. L., Huang, L., and Taft, N.
Learning in a large function space: Privacy-preserving
mechanisms for svm learning. Journal of Privacy and
Confidentiality, 4(1):4, 2012.

