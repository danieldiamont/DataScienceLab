A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

8. Appendix
This appendix contains additional plots and proofs of the
results from Section 2.

the optimal q(w) its gradient will be zero. Using the above
derivatives, we therefore have that
0 =(1

Lemma 6. The divergence from q(z) to p(z) is

)KL (q(Z|w)kp(Z)) + KL (q(Z|w)kp(Z|w))
log p(w) +

KL (q(Z)kp(Z)) = KL (q(Z|W )kp(Z)) Iq [W, Z],
|
{z
}

log q(w) + ,

Which solved for q(w), this gives

D0

(27)
where D0 = Eq(W,Z) log (q(Z|W )/p(Z)) is conditional
divergence and Iq denotes mutual information under q.
Proof. Define the joint distribution p(w, z) = q(w)p(z).
Then, the chain-rule of KL-divergence (Cover & Thomas,
2006, Thm. 2.5.3) states that

q(w) / exp

which establishes the given form for s(w) and A.
Now, to establish the value of D at the solution, expand
the negative entropy of q(w) to get
Z

q(w) log q(w)
Z
⇣
=
q(w)
(1

(28)

q ⇤ (w) = exp s(w) A)
Z
A = log
exp s(w)
s(w) = log p(w)
1

KL (q(Z|w)kp(Z|w))

1 KL (q(Z|w)kp(Z)) .

Moreover, at q ⇤ , the objective value is D⇤ =

A.

Proof. First, consider derivatives of D0 and D1 with respect to q(w). The first can immediately be seen to be
dD0
= KL (q(Z|w)kp(Z)) .
dq(w)

Now, taking the left-hand side and terms in the bottom line,
we can recognize that
✓
p(w)
q(w) log
q(w)
w

Z

Z

q(z|w)
q(w, z) log
p(w, z)
w,z
Z
d
+
q(w) log q(w)
dq(w) w,z

q(z|w)
= q(z|w) log
+ log q(w) + 1
p(w, z)
z
= KL (q(Z|w)kp(Z|w)) log p(w) + log q(w) + 1.

◆

=

D1 .

w

Thus, we can re-write Eq. 29 as
establishing the value of D⇤ .
Remark 8. In the limit where
becomes

A = (1

)D0 + D1 ,

! 0 the divergence bound

lim D⇤ = inf KL (q(Z|w)kp(Z)) .
w

!0

Proof. Use the representation that lim
lim !0
A is equal to
lim

log

!0

= lim

!0

If we create a Lagrangian for D with a Lagrange multiplier to enforce normalization of q(w), we know that at

KL (q(Z|w)kp(Z|w))

Further, if we take the terms from the middle line, we have
that
Z
1
q(w)(1
)KL (q(Z|w)kp(Z)) = (
1)D0 .

For the second, we can derive
dD1
d
=
dq(w)
dq(w)

1

w

Z

w

w

)KL (q(Z|w)kp(Z))
⌘
KL (q(Z|w)kp(Z|w)) + log p(w)
A. (29)

The left-hand side simplifies into D0 , and the first term on
the right-hand side simplifies into Iq [W, Z].
Theorem 7. For fixed values of and p(w|z), the distribution q(w) that minimizes D is

)KL (q(Z|w)kp(Z))

KL (q(Z|w)kp(Z|w)) + log p(w) ,

KL (q(Z, W )kp(Z, W )) = KL (q(W |Z)kp(W |Z))
+ KL (q(Z)kp(Z)) .

1

(1

The
lim

Z

!0

D⇤

=

⇣
exp log p(w)

KL (q(Z|w)kp(Z|w))
⌘
1
1 KL (q(Z|w)kp(Z))
Z
⇣
⌘
1
log
exp
KL (q(Z|w)kp(Z)) .
w

w

form for D⇤
R
!0 log w exp(

follows from the fact
f (w)) = supw f (w).

1

that

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

Lemma 9. If p(w|z) = r(w)q(z|w)/rz and rz is a constant, then the solution in Thm. 3 holds with
s(w) = log r(w)
+ Eqw (Z) [

log rz
1

log p(z) + (1

1

) log q(z|w)].

Proof. First, without using the particular form for p(w|z),
we can write s(w) as
log p(w)

Z

q(z|w)
p(z|w)
Z
q(z|w)
1
1
q(z|w) log
p(z)
z

q(z|w) log
z

Cancelling terms involving q(z|w) in the numerators, this
is
Z
p(z)
log p(w)
q(z|w) log
p(z|w)
z
Z
q(z|w)
1
q(z|w) log
p(z)
z
The log p(w) can be absorbed into the first term to give,
after some cancellation that
Z
1
s(w) = q(z|w) log p(w|z)
KL (q(Z|w)kp(Z)) .
z

Now, using the assumed form for p(w|z), we can immediately write that s(w) is
Z
Z
r(w)q(z|w)
q(z|w)
1
q(z|w) log
q(z|w) log
,
rz
p(z)
z
z
equivalent to the form stated.

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

=0

= .05

= .10

= .15

= .20

= .25

= .30

= .35

= .40

=, 45

= .50

= .55

= .60

= .65

= .70

= .75

= .80

= .85

= .90

= .95

Figure 5. Examples sampling from a two-dimensional mixture of three gaussians after running inference for 5 ⇥ 105 iterations. The
sampled weights w are pictured as ellipsoids at one standard deviation. Colored contours show the density p(z). To avoid visual clutter,
a smaller number (equally spaced) of samples are shown for smaller .

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

=0

= .05

= .10

= .15

= .20

= .25

= .30

= .35

= .40

=, 45

= .50

= .55

= .60

= .65

= .70

= .75

= .80

= .85

= .90

= .95

Figure 6. Examples sampling from a two-dimensional “donut” distribution after running inference for 5 ⇥ 105 iterations. The sampled
weights w are pictured as ellipsoids at one standard deviation. Colored contours show the density p(z). To avoid visual clutter, a smaller
number (equally spaced) of samples are shown for smaller .

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

= 0.0

= 0.2

= 0.4

= 0.6

= 0.8

= 1.0

Stan

Figure 7. Inference for various values of on ionosphere after 104 (top row) 105 (middle row) or 106 (bottom row) iterations. After
each iteration, one sample is drawn from qw (Z), and plots show the first two principal components (computed on samples from Stan).
Each plot show samples resulting from the (constant) step-size ✏ that resulted in the minimum MMD for that and number of iterations.
The same sequence of random numbers is for all inference methods. (More results are in the appendix.)

= 0.0

= 0.2

= 0.4

= 0.6

= 0.8

= 1.0

Stan

Figure 8. Inference for various values of on a1a after 104 (top row) 105 (middle row) or 106 (bottom row) iterations. In some of these
plots, a “tail” is visible, reflecting the path into the high-density region from where w = 0 where inference was initialized.

A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI

= 0.0

= 0.2

Figure 9. Inference for various values of

= 0.0

= 0.2

Figure 10. Inference for various values of

= 0.4

= 0.6

= 0.8

= 1.0

Stan

on australian after 104 (top row) 105 (middle row) or 106 (bottom row) iterations.

= 0.4

= 0.6

= 0.8

= 1.0

Stan

on sonar after 104 (top row) 105 (middle row) or 106 (bottom row) iterations.

