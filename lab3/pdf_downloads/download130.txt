Supplementary Material to Robust Structured Estimation with Single-Index
Models
Sheng Chen 1 Arindam Banerjee 1

Abstract

we are unable to distinguish between them, as both can be
solution to (S.1) for any samples.

In this supplementary material, we present the
deferred proofs of the results in the main paper.

2. Proof of Lemma 1
1. Proof of Claim 1
Statement of Claim 1: Suppose that each element xi of x
is sampled i.i.d. from Rademacher distribution, i.e., P(xi =
1) = P(xi = −1) = 0.5. Under model (3) with noise
ϵ = 0, there exists a θ̄ ∈ Sp−1 together with a monotone
f¯, such that supp(θ̄) = supp(θ ∗ ) and yi = f¯(⟨θ̄, xi ⟩)
for data {(xi , yi )}ni=1 with arbitrarily large sample size n,
while ∥θ̄ − θ ∗ ∥2 > δ for some constant δ.
Proof: In the noiseless setting with unknown f ∗ , provided that S , supp(θ ∗ ) is given and |S| = s, the estimation
of θ ∗ is simplified as
Find θS ∈ Ss−1
)
s.t. sign ⟨θS , xiS − xj S ⟩ = sign(yi − yj ),
∀1≤i<j ≤n,
(

(S.1)

any of whose solution θ can be true θ ∗ on the premise
that no other information is available, since there always
exists a monotone f satisfying f (⟨θ, xi ⟩) = yi . Given
the distribution of x, xiS − xj S only has 3s possibilities
even if n → +∞. We denote the feasible set of (S.1) by
C, which is basically an intersection of Ss−1 and at most
min{n(n − 1), 3p } halfspaces (or hyperplanes if yi = yj ).
Depending on the 3 different values of each sign(yi − yj ),
p
this feasible set C has at most 3min{n(n−1),3 } possibilities, which is finite, and the union of them should be Ss−1 .
When s ≥ 2 and the constant δ is small enough, we can
always find a C, in which there exist two different points
away by δ. Specify them as θ∗S and θ̄S respectively, and
1
Department of Computer Science & Engineering, University of Minnesota-Twin Cities, Minnesota, USA. Correspondence to: Sheng Chen <shengc@cs.umn.edu>, Arindam Banerjee <banerjee@cs.umn.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright
2017 by the author(s).

Statement of Lemma 1: Suppose the distribution of y in
model (1) depends on x through ⟨θ ∗ , x⟩ and we define accordingly
bi (z1 , . . . , zm ; θ ∗ ) =
(S.2)
E [qi (y1 , . . . , ym ) |⟨θ ∗ , x1 ⟩ = z1 , . . . , ⟨θ ∗ , xm ⟩ = zm ] ,
With x being standard Gaussian N (0, I), u defined in (4)
satisfies
E [u ((x1 , y1 ), . . . , (xm , ym ))] = βθ ∗ ,
(S.3)
∑m
∗
where β =
i=1 E[bi (g1 , . . . , gm ; θ ) · gi ], and
g1 , . . . , gm are i.i.d. standard Gaussian.
Proof: Let θ⊥ be any vector orthogonal to θ ∗ . For
convenience, we use the shorthand notation u for
u ((x1 , y1 ), . . . , (xm , ym )). Then we have
[m
]
∑
qi (y1 , . . . , ym ) · ⟨xi , θ⊥ ⟩
⟨Eu, θ⊥ ⟩ = E
i=1

=
=

m
∑
i=1
m
∑

E [qi (y1 , . . . , ym ) · ⟨xi , θ⊥ ⟩]
E [⟨xi , θ⊥ ⟩ · E [qi (y1 , . . . , ym ) |x1 , . . . , xm ]] (∗)

i=1

As xi follows N (0, I), ⟨xi , θ ∗ ⟩ and ⟨xi , θ⊥ ⟩ are two zeromean independent Gaussian random variables. Since the
distribution of yi depends on x only via ⟨θ ∗ , xi ⟩, we can
split the expectation and obtain
(∗) =

m
∑

E [⟨xi , θ⊥ ⟩ · bi (⟨θ ∗ , x1 ⟩, . . . , ⟨θ ∗ , xm ⟩; θ ∗ )]

i=1

=

m
∑
i=1

=0.

E [⟨xi , θ⊥ ⟩] · E [bi (⟨θ ∗ , x1 ⟩, . . . , ⟨θ ∗ , xm ⟩; θ ∗ )]

Supplementary Material to Robust Structured Estimation with Single-Index Models

Hence u has to point towards either θ ∗ or −θ ∗ , and note
that
⟨Eu, θ ∗ ⟩ =

m
∑

E [qi (y1 , . . . , ym ) · ⟨xi , θ ∗ ⟩]

i=1

=

m
∑

E [bi (⟨θ ∗ , x1 ⟩, . . . , ⟨θ ∗ , xm ⟩; θ ∗ ) · ⟨xi , θ ∗ ⟩]

i=1

=

m
∑

(Vershynin, 2012), the sub-Gaussian norm of ui1 ,...,im satisfies


∑

m


∥ui1 ,...,im ∥ψ2 = sup 
q
(y
,
.
.
.
,
y
)
·
⟨x
,
v⟩
j
11
im
j


v∈Sp−1  j=1

ψ2


∑

m

≤ sup 
|⟨xj , v⟩|


p−1
v∈S
j=1

ψ2

E [bi (g1 , . . . , gm ; θ ∗ ) · gi ] = β

≤ m · sup ∥|⟨xj , v⟩|∥ψ2 ≤ κm ,

i=1

v∈Sp−1

∗

We complete the proof by recalling that ∥θ ∥2 = 1, thus
Eu = βθ ∗ .

3. Proof of Theorem 1
We first provide a lemma that is useful for bounding the
Gaussian width of unions of sets, which originates in
Maurer et al. (2014).
Lemma A (Lemma 2 in Maurer et al. (2014)) Let M >
4, A1 , · · · , AM ⊂ Rp , and A = ∪m Am . The Gaussian
width of A satisfies
√
w(A) ≤ max w(Am ) + 2 sup ∥z∥2 log M (S.4)
1≤m≤M

z∈A

Statement of Theorem 1: Suppose that the optimization
(9) can be solved to global minimum. Then the following
error bound holds( for the minimizer
) θ̂ with probability at
least 1 − C ′′ exp −w2 (AK (θ ∗ )) ,
3


Cκm 2 w(AK (θ ∗ )) + C ′


√
·
,
θ̂ − θ ∗  ≤
β
n
2

(S.5)

where κ is the sub-Gaussian norm of a standard Gaussian
random variable, and C, C ′ , C ′′ are all absolute constant. Proof: We use the shorthand notation AK for the set
AK (θ ∗ ). As θ̂ attains the global minimum of (9), we have
⟨
⟩
û
⟨θ̂ − θ ∗ , û⟩ ≥ 0 ⇐⇒
θ̂ − θ ∗ , − θ ∗ + θ ∗ ≥ 0
β
⟩
⟨
û
∗
∗
∗
=⇒ ⟨θ̂, θ ⟩ ≥ 1 − θ̂ − θ , − θ
β
⟨
⟩
û
∗
∗
≥ 1 − ∥θ̂ − θ ∥2 · sup
v, − θ
β
v∈AK ∪{0}
In order to bound the supremum above, we use the result
from generic chaining. We define the stochastic process
{Zv = ⟨v, û/β − θ ∗ ⟩}v∈AK ∪{0} . First, we need to check
the process has sub-Gaussian incremental. For simplicity, we denote u ((xi1 , yi1 ), . . . , (xim , yim )) by ui1 ,...,im .
By the definitions and properties of sub-Gaussian norm

thus we know ∥⟨ui1 ,...,im , v − w⟩∥ψ2 ≤ κm · ∥v − w∥2 .
By Lemma 2, we have
)
⟩
(⟨


û
P (|Zv − Zw | > δ) = P  v − w, − θ ∗  > δ
β
(
∑
 (n − m)!
1
= P 
· ⟨ui1 ,...,im , v − w⟩
n!
β
1≤i1 ,...,im ≤n
i1 ̸=...̸=im

)



− ⟨v − w, θ ⟩  > δ
)
(
⌊n⌋
β 2 δ2
≤ 2 exp −C
·
m m2 κ2 · ∥v − w∥22
(
)
nβ 2 δ 2
′
≤ 2 exp −C · 3 2
,
m κ · ∥v − w∥22
∗

where we set C ′ = C/2. Therefore we can conclude
that {Zv } has sub-Gaussian incremental w.r.t. the metric
√
3
s(v, w) , κm 2 · ∥v − w∥2 /β n. Now applying Lemma
3 to {Zv }, we obtain
(
(
P
sup
|Zv − Zw | ≥ C1 γ2 (AK ∪ {0}, s)
v,w∈AK ∪{0}

+ δ · diam (AK ∪ {0}, s)
(
=⇒ P

))

(
)
≤ C2 exp −δ 2

C1 κm 2 (
√
· γ2 (AK ∪ {0}, ∥ · ∥2 )
β n
3

sup
v∈AK ∪{0}

))

+ 2δ

|Zv | ≥

(
)
≤ C2 exp −δ 2

Using Lemma 4 γ2 (AK ∪ {0}, ∥ · ∥2 )
≤
C0 ·
w (AK ∪ {0}) and taking δ = w (AK ∪ {0}), we
get
⟩
⟨
û
∗
≤
sup |Zv |
sup
v, − θ
β
v∈AK ∪{0}
v∈AK ∪{0}
3

3

C3 κm 2
C3 κm 2 w (AK ) + C4
√
√
≤
·
· w (AK ∪ {0}) ≤
β
β n
n
(
)
with probability at least 1 − C2 exp −w2 (AK ) . The last
inequality follows from Lemma A. Now we turn to the

Supplementary Material to Robust Structured Estimation with Single-Index Models

quantity ∥θ̂ − θ ∗ ∥2 ,

Therefore it follows that

∥θ̂ − θ ∗ ∥22 ≤ 2 − 2⟨θ̂, θ ∗ ⟩
(
)
3
C3 κm 2 w (AK ) + C4
∗
√
≤ 2 − 2 1 − ∥θ̂ − θ ∥2 ·
·
β
n
3

2C3 κm 2 w (AK ) + C4
√
≤ ∥θ̂ − θ ∥2 ·
·
.
β
n
∗

We finish the proof by letting C = 2C3 , C ′ = C4 and
C ′′ = C2 .

4. Proof of Theorem 2

{ 
}
∥v∥ ∩ p−1

Aρ (θ ∗ ) = cone v  ∥v + θ ∗ ∥ ≤ ∥θ ∗ ∥ +
S
ρ
(S.6)
√
If we set λ = ρ ∥û − βθ ∗ ∥∗ = O(ρm3/2 w(B∥·∥ )/ n)
and it satisfies
( λ (< ∥û∥
))∗ , then with probability at least
1 − C ′ exp −w2 B∥·∥ , θ̂ in (10) satisfies
(
)
3


C(1 + ρ)κm 2 Ψ (Aρ (θ ∗ )) · w B∥·∥

∗
√
·
,
θ̂ − θ  ≤
β
n
2
(S.7)
where Ψ (Aρ (θ ∗ )) = supv∈Aρ (θ∗ ) ∥v∥ and B∥·∥ =
{v | ∥v∥ ≤ 1} is the unit ball of norm ∥ · ∥.
Proof: Based on the optimality of θ̂, we have
=⇒

β(1 − ⟨θ ∗ , θ̂⟩) ≤ ⟨û − βθ ∗ , θ̂ − θ ∗ ⟩ + λ(∥θ ∗ ∥ − ∥θ̂∥)

tochastic process {Zv = ⟨v, û/β − θ ∗ ⟩}v∈B∥·∥ , and it is
not difficult to verify that {Zv } has sub-Gaussian incremental using the proof in Theorem 1. Now applying Lemma 3 and 4, we have
⟨
⟩
û
1
sup
− θ ∗ , v = · sup |Zv − Zw |
β
2 v,w∈B∥·∥
v∈B∥·∥
(S.9)
(
)
3
C1 κm 2 w B∥·∥
≤
· √
,
β
n
(
(
))
with probability at least 1 − C ′ exp −w2 B∥·∥ . Therefore we know that λ satisfies
(
)
ρm3/2 w(B∥·∥ )
√
λ=O
n

If θ̂ = 0 is the minimizer, the first-order optimality should
hold, i.e.,
=⇒

∥û∥∗ ≤ λ

Hence if λ < ∥û∥∗ , 0 cannot be the minimizer, which
means that the minimum of (10) must be negative. So we
can assert that ∥θ̂∥2 = 1, otherwise we can normalize θ̂ to
get a smaller objective value. Combining (S.8) and (S.9),
we finally get
∥θ̂ − θ ∗ ∥ =

Since ⟨θ ∗ , θ̂⟩ ≤ 1, we have

2 − 2⟨θ̂, θ ∗ ⟩
∥θ̂ − θ ∗ ∥

(
)
Cmκ(1 + ρ) Ψ (Aρ (θ ∗ )) · w B∥·∥
√
≤
·
,
β
n
=⇒

1
· ⟨û − βθ ∗ , θ̂ − θ ∗ ⟩
λ
1
≤ ∥θ ∗ ∥ + · ∥û − βθ ∗ ∥∗ ∥θ̂ − θ ∗ ∥
λ
1
∗
= ∥θ ∥ + ∥θ̂ − θ ∗ ∥ =⇒ θ̂ − θ ∗ ∈ Aρ (θ ∗ )
ρ

∥θ̂∥ ≤ ∥θ ∗ ∥ +

(S.8)




Now we try to bound  ûβ − θ ∗  . We first rewrite it as
∗


⟨
⟩
 û

 β − θ ∗  = supv∈B∥·∥ ûβ − θ ∗ , v . Construct the s-

û ∈ λ · ∂∥0∥

⟨βθ ∗ − û − βθ ∗ , θ̂⟩ + λ∥θ̂∥
≤ ⟨βθ ∗ − û − βθ ∗ , θ ∗ ⟩ + λ∥θ ∗ ∥ =⇒

(
)
⟨û − βθ ∗ , θ̂ − θ ∗ ⟩ + λ ∥θ ∗ ∥ − ∥θ̂∥ ≥ 0

∗

∗

Statement of Theorem 2: Define the following set for any
ρ > 1,

−⟨û, θ̂⟩ + λ∥θ̂∥ ≤ −⟨û, θ ∗ ⟩ + λ∥θ ∗ ∥

)
û
λ( ∗
1 − ⟨θ ∗ , θ̂⟩ ≤ ⟨ − θ ∗ , θ̂ − θ ∗ ⟩ +
∥θ ∥ − ∥θ̂∥
β
β
(
)

 û

λ ∥θ̂ − θ ∗ ∥
∥θ̂ − θ ∗ ∥
∗
∗

≤ ∥θ̂ − θ ∥2  − θ  ·
+ ·
∗
β
β ∥θ̂ − θ ∗ ∥2
∗ ∥θ̂ − θ ∥2


 û

∗
≤ (1 + ρ)∥θ̂ − θ ∗ ∥2 · 
 β − θ  · sup ∗ ∥v∥
∗ v∈Aρ (θ )




û
∗
∗
= (1 + ρ)∥θ̂ − θ ∗ ∥2 · 
 β − θ  · Ψ (Aρ (θ ))

where the equality uses the fact that ∥θ̂∥2 = 1.

5. Proof of Corollary 1
Statement of Corollary 1: Assume that {(xi , yi )}ni=1 follow 1-bit CS model in (2) and û is given as (14). For any

Supplementary Material to Robust Structured Estimation with Single-Index Models

s-sparse θ ∗ , with high probability, θ̂ produced by both (15)
and (17) (i.e., θ̂ ks and θ̂ ps ) satisfy
(√
)


s
log
p


(S.10)
θ̂ − θ ∗  ≤ O
n
2

Proof: We rearrange the terms inside the summation of
(21) based on π ↓ ,
∑
1
ĥ =
sign(yi − yj ) · (xi − xj )
n(n − 1)
1≤i,j≤n
i̸=j

=
Proof: For the k-support norm estimator, the cone
AK (θ ∗ ) is given by

{
} ∩

AK (θ ∗ ) = cone θ̂ − θ ∗  ∥θ̂∥0 ≤ s, ∥θ̂∥2 ≤ 1
Sp−1
=⇒

=

AK (θ ∗ ) ⊆ S = {v | ∥v∥0 ≤ 2s} ∩ Sp−1

Using (19) from (Chen & Banerjee, 2015), we have
(√
)
w(AK (θ ∗ )) ≤ w(S) ≤ O
s log p .
By Theorem 1, the error of k-support norm estimator satisfies
(√
)


s log p
 ks
∗
θ̂ − θ  ≤ O
n
2
For the passive algorithm, if we choose ρ = 2, the restricted
norm compatibility Ψ (Aρ (θ ∗ )) for L1 norm satisfies
√
Ψ (Aρ (θ ∗ )) ≤ 4 s

(S.11)

according to the results in (Negahban et al., 2012;
Banerjee et al., 2014). Chen & Banerjee (2015) also show
that the Gaussian width of the L1 -norm ball is bounded by
(√
)
w(BL1 ) ≤ O
log p .
(S.12)
Now combining (S.11), (S.12) and Theorem 2, we can conclude that
(√
)


s log p
 ps
∗
,
θ̂ − θ  ≤ O
n
2
which completes the proof.

2
n(n − 1)
2
n(n − 1)

∑

sign(yi − yj ) · xi

1≤i,j≤n
i̸=j
n ∑
∑

(
)
sign yπ↓ − yj · xπ↓
i

i=1 j̸=π ↓
i

i

∑
2
(n + 1 − 2i) · xπ↓ ,
i
n(n − 1) i=1
n

=

where the last inequality uses the fact that there are
(i − 1) yj larger than and (n − i) smaller than yπ↓ , thus
i
(
)
∑
y
=
(n−i)−(i−1)
=
n+1−2i.
↓ sign
↓ − yj
j̸=π
π
i

i

7. Proof of Proposition 2
Statement of Proposition 2: For s-fused-sparse θ ∗ , the
Gaussian width of set AK (θ ∗ ) with K = {θ | |F (θ)| ≤
s, ∥θ∥2 = 1} satisfies
√
w(AK (θ ∗ )) ≤ O( s log p)
(S.14)

Proof: Define the following sets

{

Ti,j = αu ∈ Rp  u1 = . . . = ui−1 = uj+1 = . . . = up = 0,
}
√
1
, |α| ≤ 2s + 1
ui = . . . = uj = √
j−i+1
(S.15)
T =

∪

Ti,j

(S.16)

i≤j

For each Ti,j , its Gaussian width can be calculated as
[
]
√
w(Ti,j ) = E sup ⟨v, g⟩ = 2s + 1 · E [|⟨u, g⟩|]
v∈Ti,j

√
√
= 2s + 1 · E |g| = O( 2s + 1) ,

6. Proof of Proposition 1
Statement of Proposition 1: Given {(xi , yi )}ni=1 , let π ↓
be the permutation of {1, . . . , n} such that yπ↓ > yπ↓ >
1
2
. . . > yπn↓ . Then we have
∑
2
(n + 1 − 2i) · xπ↓
i
n(n − 1) i=1
n

ĥ =

(S.13)

where u is defined in (S.15) and g is a standard Gaussian
random variable. We apply Lemma A to T , and obtain
√ (( )
)
p
w(T ) ≤ max w(Ti,j ) + 2 sup ∥z∥2 log
+p
i≤j
2
z∈T
√
√
√
≤ O( 2s + 1) + O( 2s + 1 · log p)
√
= O( s log p)

Supplementary Material to Robust Structured Estimation with Single-Index Models

Next we show that AK (θ ∗ ) ⊆ conv(T ). Since K =
{θ | {|F(θ)| ≤ s, ∥θ∥2 }= 1} and AK (θ ∗ ) =
∩ p−1

cone v  v = θ̂ − θ ∗ , θ̂ ∈ K
S
by definition, we
have |F(v)| ≤ 2s for any v ∈ AK (θ ∗ ). Suppose |F(v)| =
t ≤ 2s and F(v) = {i1 , i2 , . . . , it }. For simplicity, we also let i0 = 0 and it+1 = p. Then any v ∈ AK (θ ∗ ) can be
written as a convex combination of t + 2 points in T . To
see this, we rewrite v as
√
t
t
∑
∑
∥vir +1:ir+1 ∥2
t + 1vir +1:ir+1
√
v=
·
vir +1:ir+1 =
∥v
t+1
ir +1:ir+1 ∥2
r=0
r=0
)
(
t
∑
∥vir +1:ir+1 ∥2
√
·0,
+ 1−
t+1
r=0
(S.17)
where vir +1:ir+1 is obtained from v by keeping the entries
from index ir √+ 1 to ir+1 while zeroing out the rest. Let
uir +1:ir+1 =

t+1vir +1:ir+1
∥vir +1:ir+1 ∥2

, and we have
√
√
∥uir +1:ir+1 ∥2 = t + 1 ≤ 2s + 1
=⇒ uir +1:ir+1 ∈ Tir +1:ir+1 ⊆ T .

It follows from ∥v∥2 = 1 that
√
∑t
t
∑
(t + 1) r=0 ∥vir +1:ir+1 ∥22
∥vir +1:ir+1 ∥2
√
√
≤
=1
t+1
t+1
r=0
=⇒ 1 −

t
∑
r=0

∥vir +1:ir+1 ∥2
√
≥0
t+1

Hence (S.17) is indeed a convex combination of t+2 points
in T , which implies AK (θ ∗ ) ⊆ conv(T ). Finally, by the
properties of Gaussian width, we conclude that
√
w(AK (θ ∗ )) ≤ w(conv(T )) = w(T ) ≤ O( s log p)

in which C is an absolute constant.
Proof: Our proof is based on Hoeffding’s decomposition
for U -statistics. For simplicity, we use U as shorthand for
Un,m (h). Given a permutation π of {1, . . . , n}, define
1

n
⌊m
⌋−1
∑

m

k=0

Wπ = ⌊ n ⌋

(
)
h zπmk+1 , . . . , zπm(k+1) ,

∑
1
The U -statistic can be rewritten as U = n!
π Wπ , and the
summation is over all possible permutations of {1, . . . , n}.
As no copy of z appears more than twice in a single Wπ ,
n
Wπ is an average of ⌊ m
⌋ independent sub-Gaussian random variables. Hence the ψ2 -norm
√ofnits centered version
satisfies ∥Wπ − EWπ ∥ψ2 ≤ cκ/ ⌊ m
⌋. Using Chernoff
technique, we have for any t > 0,
P (U − EU > δ) ≤ e−tδ · E [exp(t(U − EU ))]
[
(
)]
t ∑
−tδ
(Wπ − EU )
=e
· E exp
n! π
[
]
∑
1
≤e−tδ · E
exp (t(Wπ − EU ))
n! π
=e−tδ · E [exp (t(Wπ − EWπ ))]
(
)
2
2 ⌊κ ⌋
≤ exp −tδ + ct · n
,
m

(S.20)
where the second inequality is obtained via Jensen’s inequality and the last one follows the moment generating
function bound⌊for⌋centered sub-Gaussian random variable.
n
Choosing t = m
δ/2cκ2 to minimize right-hand side of
(S.20), we obtain
(
⌊ n ⌋ δ2 )
P (U − EU > δ) ≤ exp −C
·
,
m κ2
where C = 1/2c. To complete the proof, we just need to
repeat the argument above for P (U − EU < −δ).

8. Proof of Lemma 2
Statement of Lemma 2: Define the U -statistic
∑
(n − m)!
Un,m (h) =
h (zi1 , . . . , zim )
n!
1≤i1 ,...,im ≤n
i1 ̸=i2 ̸=...̸=im

References
Banerjee, A., Chen, S., Fazayeli, F., and Sivakumar, V. Estimation with norm regularization. In Advances in Neural Information Processing Systems (NIPS), 2014.

(S.18)
Chen, S. and Banerjee, A. Structured estimation with atomwith order m and kernel h : Rd×m 7→ R based on n inic norms: General bounds and applications. In Proceeddependent copies of random vector z ∈ Rd , denoted by
ings of the 28th International Conference on Neural Inz1 , · · · , zn . If h(·, . . . , ·) is sub-Gaussian with ∥h∥ψ2 ≤ κ,
formation Processing Systems, 2015.
then the following inequality holds for Un,m (h) with any
δ > 0,
Maurer, A., Pontil, M., and Romera-Paredes, B. An In(
⌊ n ⌋ δ2 )
equality with Applications to Structured Sparsity and
·
,
P (|Un,m (h) − EUn,m (h)| > δ) ≤ 2 exp −C
m κ2
Multitask Dictionary Learning. In Conference on Learn(S.19)
ing Theory (COLT), 2014.

Supplementary Material to Robust Structured Estimation with Single-Index Models

Negahban, S., Ravikumar, P., Wainwright, M. J., and Yu,
B. A unified framework for the analysis of regularized
M -estimators. Statistical Science, 27(4):538–557, 2012.
Vershynin, R. Introduction to the non-asymptotic analysis
of random matrices. In Eldar, Y. and Kutyniok, G. (eds.), Compressed Sensing, chapter 5, pp. 210–268. Cambridge University Press, 2012.

