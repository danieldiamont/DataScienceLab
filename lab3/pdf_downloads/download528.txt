Identifying Best Interventions through Online Importance Sampling

A. Causal Graphs and Interventions
A causal graph G(V, E) specifies causal relationships
among the random variables representing the vertices of
the graph V. The relationships are specified by the directed
edges E; an edge Vi ! Vj implies that Vi 2 V is a direct parental cause for the effect Vj 2 V. With some abuse
of notation, we will denote the random variable associated
with a node V 2 V by V itself. We will denote the parents
of a node V by pa(V ). The causal dependence implies
that V = fV (U 2 pa(V ), ✏V ), where ✏V is an independent
exogenous noise variable. One does not get to measure
the functions fV in practice. The noise variable and the
above functional dependence induce a conditional probability distribution P(V |pa(V )). Further, the joint distribution of {V }V 2V decomposes into product of conditional
distributions according
Q to G viewed as a Bayesian Network,
i.e. P({V }V 2V ) =
P(V |pa(V )).
V 2V

Interventions in a causal setting can be categorized into two
kinds:

1. Soft Interventions: At node V , the conditional
distribution relating pa(V ) and V is changed to
P̃ (V |pa(V )).

2. Hard Interventions: We force the node V to take
a specific value x. The conditional distribution
P̃ (V |pa(V )) is set to a point mass function 1V =x .

B. Variations of the Problem Setting
In this section we provide more general causal settings
where our results can be directly applied.
Multiple nodes at the graph: This is illustrated in Fig. 5a.
Soft interventions can be performed at multiple nodes like
at V = {V1 , V2 }. These interventions can be modeled as
changing the distribution P (V|pa(V)) where pa(V) are the
union of parents of V1 and V2 . These distributions can be
thought of as the arms of the bandits and our techniques
can be applied as before to estimate the best intervention.
Directed cut between sources and targets: Fig. 5b represents the most general scenario in which our techniques can
be applied. Soft or hard interventions can be performed at
multiple source nodes, while the goal is to choose the best
out of these interventions in terms of maximizing a known
function of multiple target nodes. If the effect of these interventions can be estimated on a directed cut separating
the targets and the sources then our techniques can be applied as before. This is akin to knowing P (V1 , V2 ) under all
the interventions in Fig. 5b, because V1 and V2 is a directed
cut separating the sources and the targets.
Empirical knowledge of continuous arm distributions:
Our techniques can be applied to continuous distributions

P(V |pa(V )) as shown in our empirical results in Section 4.1. The extension is straight-forward by using the
general definition of f -divergences. More importantly our
techniques can be applied even if only prior empirical samples from the distributions P(V |pa(V )) is available and
not the whole distributions. In this case the f -divergences
can be estimated using nearest neighbor estimators similar to (Pérez-Cruz, 2008). Moreover, for the importance
sampling only ratios of distributions are needed, which can
again be estimated using nearest neighbor based techniques
from empirical data.

C. Discussion and Future Work
In this paper, we analyze the problem of identifying the
best arm at a node V in a causal graph (various known conditionals Pk (V |pa(V ))) in terms of its effect on a target
variable Y further downstream, possibly in a less understood portion of the larger causal network. We characterize
the hardness of this problem in terms of the relative divergences of the various conditionals that are being tested and
the gaps between the expected value of the target under the
various arms. We provide the first problem dependent simple regret and error bounds for this problem, that is a natural generalization of (Audibert & Bubeck, 2010), but with
information leakage between arms. We provide an efficient
successive rejects style algorithm that achieves these guarantees, by leveraging the leakage of information, through
carefully designed clipped importance samplers. Further,
we introduce a new f -divergence measure that may be relevant for analyzing importance sampling estimators in the
causal context. This may be of independent interest. We
believe that our work paves the way for various interesting problems with significant practical implications. In the
following, we state a few open questions in this regard:
Tighter guarantees on SRISv2: In Section 4, we have
observed that a slightly modified version of our algorithm
SRISv2 performs the best among all the competing algorithms including SRISv1. The only difference of SRISv2
from Algorithm 1, is that in line 6 the estimators used in a
phase also uses samples from past phases, but clipped according to the criterion in the current phase. We believe
that this algorithm has tighter error and simple regret guarantees. We conjecture that at least one of the log(1/ k ),
in the definition of H̄ in (4) can be eliminated, thus leading
to better guarantees.
Estimating the marginals of the parents: In Algorithm 1,
either the marginals of the parents of V , that is P(pa(V )) is
required in order to calculate the f -divergences in Definition 2, or prior data involving the parents is required to estimate the f -divergences directly from data. However, we
believe it is possible to model this estimation, directly into
the online framework, as data about the marginals of the

Identifying Best Interventions through Online Importance Sampling

Intervention Sources

pa(V)

V1

V2

P0 , P1 , ..., PK

V1

1

Directed Cut

V2

P0 , P1 , ..., PK

V
Layers

Layers

Targets

Y

(a) Illustration of a scenario where there are multiple intervention sources V = {V1 , V2 }. Each soft intervention is a
change in the distribution P(V|pa(V)).

1

Y

Z

max f (Y, Z)

(b) Illustration of a causal setting, where there are many intervention sources. Soft or hard interventions can be performed
at these sources and the effects of these interventions can be
observed at the target node. Our techniques can be applied
in choosing the best intervention in terms of maximizing a
function on the target nodes, provided the effects of these interventions on P (V1 , V2 ) are known. Here V1 and V2 form a
directed cut separating the sources and the targets.

Figure 5. Illustration of more general causal settings where our algorithms can be directly applied.

parents are available through the samples in all the arms, as
these marginals remain unchanged.
Problem Dependent Lower Bound: In (Lattimore etpal.,
2016), a problem independent lower bound of O(1/ T )
has been provided for a special causal graph. However, the
problem parameter dependent lower bound like that of (Audibert & Bubeck, 2010) still remains an open problem. We
believe that the lower bound will depend on the divergences
between the distributions and the gaps between the rewards
of the arms, similar to the term in (4).
General Learning Framework: Our work paves the way
for a more general setting for learning counterfactual effects. Importance sampling is a fairly general tool and can
be ideally applied at any set of nodes of a causal graph. So,
in principle it is possible to study the effect of a change at
V on a target Y , by using importance sampling between
the changed marginal distributions at an intermediate cut
S that blocks every path from V to Y . In fact, this is explored in a non-bandit context in (Bottou et al., 2013). An
important question is: What is the most suitable cut to be
used? (Lattimore et al., 2016) uses the cut closest to Y , i.e.
immediate parents of Y . However, the marginals of the cut
under different changes need to be estimated this ’far’ from
the source closer to the target. Therefore, there is a tradeoff that involves a delicate balance between the estimation
errors of the changes at an intermediate cut between V and
Y , and the reduction in importance sampling divergences
between cut distributions closer to the target Y . We believe

understanding this is quite important to fully exploit partial/full knowledge about causal graph structure to answer
causal strength questions from data observed.

D. Interpretation of our Theoretical Results
In this section we compare our theoretical bounds on the
probability of mis-identification with the corresponding
bounds in (Audibert & Bubeck, 2010). We also compare
our simple regret guarantees with the guarantees in (Lattimore et al., 2016). In both these cases, we demonstrate
significant improvements. These theoretical improvements
are exhibited in our empirical results in Section 4.1.
D.1. Comparison with (Audibert & Bubeck, 2010)
Let R̃( k ) = {s : s  k }, i.e. the set of arms
which are closer to the optimal than arm k. Let H̃ =
max |R̃( 2k )| . The result for the best arm identification
k6=k⇤

k

with no information leakage in (Audibert & Bubeck, 2010)
can be stated as: The error in finding the optimal arm is
bounded as:
✓
✓
◆◆
T K
2
e(T )  O K exp
(6)
log(K)H̃
One intuitive interpretation for H̃ is that it is the maximum
among the number of samples (neglecting some log factors) required to conclude that arm k is suboptimal from

Identifying Best Interventions through Online Importance Sampling

among the arms which are closer to the optimal than itself. Intuitively, this is because when there is no information leakage, one requires 1/ 2k samples to distinguish between the k-th optimal arm and the optimal arm. Further,
the kth arm is played only 1/k fraction of the times since
we do not know the identity of the k-th optimal arm.
Our main result in Theorem 1 can be seen to be a generalization of the existing result for the case when there is
information leakage between the arms (various changes in
a causal graph).
The term ⇤ (B, R⇤ ( k )) in our setting is the ‘effective
standard deviation’ due to information leakage. There
is a similar interpretation of our result (ignoring the log
factors): Since there is information leakage, the expres⇤ 2
sion (( k))2 characterizes the number of samples required to
weed out arm k out of contention from among competing
arms (arms that are at a distance at most twice than that of
arm k from the optimal arm). The interpretation of ’effective variance’ is justified using importance sampling which
is detailed in Section E.3. Further, in our framework ⇤
also incorporates any budget constraint that comes with the
problem, i.e. any apriori constraint on the relative fraction
of times different arms need to be pulled.
For ease of exposition let (k) denote the index of the k-th
best arm (for k = 1, .., K) and (k) denotes the corresponding gap. In this setting, the terms H̃ (from the result
in (Audibert & Bubeck, 2010)) and H̄ can be written as:
H̃ = max
k6=1

H̄ = max
k6=1

k
2
(k)
⇤

(B, R⇤ (

2
(k)

(k) ))

2

.

p
(B, R⇤ ( (k) )) can be smaller than k due to information leakage as every single arm pull contributes to another
arm’s estimate. Therefore, these provide better guarantees
than (Audibert & Bubeck, 2010).
⇤

To see the improvement over the previous result in (Audibert & Bubeck, 2010), we consider a special case when the
cost budget B is infinity and there is only the the sample
budget T . In addition, let us assume that the
p log divergences are such that: Mij  ⌘M
=
⌘
<<
|R|, 8i 6=
ii
p
j. Let R = R⇤ ( (k) ). If ⌘ > |R|, the optimal solution
for (2) is a bit complicated to interpret. Consider the fea1
sible allocation ⌫i = |R|
, 8i 2 R in (2). Evaluating the
objective function for this feasiblepallocation, it is possible
⌘
to show that ⇤  1 1/|R|
<< |R|. Hence, unless the
variance due to information leakage is too bad, the effective
variance is smaller than that of the case with no information
leakage.
The improvement over the no information leakage setting,

is even more pronounced under budget constraints. Consider the setting S1, and assume that the fractional budget of the difficult arms, B = o(1). This implies that
the total number of samples available for difficult arms is
o(T ). The budget constrained case has not been analyzed
in (Audibert & Bubeck, 2010), however in the absence of
information leakage, one would expect that the arms with
the least number of samples would be the most difficult to
eliminate, and therefore the error guarantees would scale as
exp( O(BT )/H̃) ⇠ exp( o(T )/H̃) (excluding log factors). On the other hand, our algorithm can leverage the
information leakage and the error guarantee would scale as
exp( O(T )/H̄), which can be order-wise better if the effective standard deviations are well-behaved.
D.2. Comparison with (Lattimore et al., 2016)
In (Lattimore et al., 2016), the algorithm is based on
clipped importance samples,
p where the clipper is always
set at a static level of O( T ) (excluding log factors). The
simplepregret guarantee in (Lattimore et al., 2016) scales
as O( (m(⌘)/T ) log T ), where m(⌘) is a global hardness
parameter. The guarantees do not adapt to the problem parameters, specifically the gaps { k }k2[K] .

On the contrary, we provide problem dependent bounds,
which differentiates the arms according to its gap from the
optimal arm and its effective standard deviation parameter.
The terms H̄k can be interpreted as the hardness parameter for rejecting arm k. Note that H̄k depends only on the
arms that are at least as bad in terms of their gap from the
optimal arm. Moreover the guarantees are adapted to our
general budget constraints, which is absent in (Lattimore
et al., 2016). It can be seen that when k ’s do not scale in
T , then our simple regret is exponentially small in Tp(dependent on H̄k ’s) and can be much less than O(1/ T ).
The guarantee also generalizes topthe problem independent
setting when k ’s scale as O(1/ T ).

E. Proofs
In this section we present the theoretical analysis of our
algorithm. Before we proceed to the proof of our main
theorems, we derive some key lemmas that are useful in
analyzing clipped importance sampled estimators.
E.1. Clipped Importance Sampling Estimator
One of the salient features of this problem is that there is
information leakage among the K arms of the bandit. The
different arms that are being tested only differ in the conditional distribution of V given its parents pa(V ), while
the rest of the relationships in the causal graph G remain
unchanged. Since the different candidate conditional distributions Pk (V |pa(V )) are known from prior knowledge,

Identifying Best Interventions through Online Importance Sampling

it is possible to utilize samples obtained under an arm j
to obtain an estimate for the expectation under arm i (i.e
Ei [Y ]). We will see in subsequent sections that the goodness of samples obtained under arm j for estimating Ei [Y ],
is dependent on a particular divergence metric between the
distributions Pi (V |pa(V )) and Pj (V |pa(V )). A popular
method for utilizing this information leakage among different distributions is importance sampling. Importance sampling has been used before in counterfactual analysis in a
similar causal setting (Lattimore et al., 2016; Bottou et al.,
2013). In the subsequent sections, we introduce importance
samplers in the context of our problem and provide some
novel techniques to analyze the confidence intervals for the
importance samplers.
Importance Sampling: Now we introduce the concept of
importance sampling which is one of the key tools we use
to leverage the information leakage between the candidate
arms. Suppose we get samples from arm j 2 [K] and we
are interested in estimating Ei [Y ]. In this context it helpful
to express Ei [Y ] in the following manner:

Pi (V |pa(V ))
Ei [Y ] = Ej Y
(7)
Pj (V |pa(V ))
(7) is trivially true because the only change to the joint distribution of all the variables in the causal graph G under
arm i and j is at the factor P(V |pa(V )). Suppose we observe t samples of {Y, V, pa(V )} from the arm j, denoted
by {Yj (s), Vj (s), pa(V )j (s)}ts=1 . Under the observation
of Equation (7), one might assume that the naive estimator,
t

Ŷi0 (j) =

1X
Pi (Vj (s)|pa(V )j (s))
Yj (s)
t s=1
Pj (Vj (s)|pa(V )j (s))

(8)

provides a good estimate for µi = Ei [Y ]. However, the
confidence guarantees on such an estimate can be arbitrarily bad as even though Y is bounded. This is because the
factor Pi (V |pa(V ))/Pj (V |pa(V )) can be very large for
several values of V, pa(V ). Therefore, usual confidence
inequalities like Azuma-Hoeffding’s, Berstein’s would not
yield good confidence intervals.
Clipped Importance Samplers: In the previous section,
we observe that the naive estimator of (8) is not suitable for
yielding good confidence intervals. It has been observed
in the context of importance sampling, that clipping the estimator in (8) at a carefully chosen value, can yield better
confidence guarantees even though the resulting estimator
will become slightly biased (Bottou et al., 2013). Before
we introduce the precise estimator, let us define a key quantity that will be useful for the analysis.
Definition 4. We define ⌘i,j (✏) as follows:
⇢
✓
◆
✏
Pi (V |pa(V ))
⌘i,j (✏) = argmin : Pi
>⌘ 
Pj (V |pa(V ))
2
⌘
(9)

for all i, j 2 [K], where ✏ > 0.
We shall see that the ⌘i,j (✏) is related to the conditional
f -divergence between Pi (V |pa(V )) and Pj (V |pa(V )) for
the carefully chosen function f1 (.) as introduced in Section 3.1.
Now we are at a position to provide confidence guarantees
on the following clipped estimator:
(⌘)

Ŷi

(j)
t

1X
Pi (Vj (s)|pa(V )j (s))
Yj (s)
⇥
t s=1
Pj (Vj (s)|pa(V )j (s))
⇢
Pi (Vj (s)|pa(V )j (s))
1
 ⌘ij (✏) .
Pj (Vj (s)|pa(V )j (s))
=

(⌘)

Lemma 1. The estimate Ŷi
the following:

(10)

(j) for ⌘ = ⌘i,j (✏) satisfies

1.
h
i
h
i ✏
(⌘)
(⌘)
Ej Ŷi (j)  µi  Ej Ŷi (j) +
2

(11)

2.
⇣
P µi
1

(⌘)

✏/2  Ŷi (j)  µi +
✓
◆
2
t
2 exp
.
2⌘i,j (✏)2

⌘

Proof. We have the following chain:

Pi (V |pa(V )
Ej Y
Pj (V |pa(V ))

⇢
Pi (V |pa(V ))
Pi (V |pa(V ))
= Ej Y
1
 ⌘i,j (✏)
Pj (V |pa(V ))
Pj (V |pa(V ))

⇢
Pi (V |pa(V ))
Pi (V |pa(V ))
+ Ej Y
1
> ⌘i,j (✏)
Pj (V |pa(V ))
Pj (V |pa(V ))

⇢
(a)
Pi (V |pa(V ))
Pi (V |pa(V ))
 Ej Y
1
 ⌘i,j (✏)
Pj (V |pa(V ))
Pj (V |pa(V ))
✓
◆
Pi (V |pa(V ))
+ Pi
> ⌘i,j (✏)
Pj (V |pa(V ))

⇢
Pi (V |pa(V ))
Pi (V |pa(V ))
 Ej Y
1
 ⌘i,j (✏)
Pj (V |pa(V ))
Pj (V |pa(V ))

(12)

+

Here, (a) is because Y 2 [0, 1]. This yields the first part of
the lemma:
h
i
h
i ✏
(⌘)
(⌘)
Ej Ŷi (j)  µi  Ej Ŷi (j) +
(13)
2
where ⌘ = ⌘i,j (✏). Note that all the terms in the summation
of (10) are bounded by ⌘i,j (✏). Therefore, by an application

✏
2

Identifying Best Interventions through Online Importance Sampling

of Azuma-Hoeffding we obtain:
⇣
(⌘)
P |Ŷi (j)

✓
h
i
⌘
(⌘)
Ej Ŷi (j) | >
 2 exp

◆
t
2⌘i,j (✏)2
(14)
2

Combining Equation (13) and (14), we obtain the first part
of our lemma.

(a) - This is due to the inequality p + q  2pq when q
and p loge (2).

E.2. Relating ⌘ij (·) with f -divergence
Now we are left with relating ⌘i,j (✏) to a particular f -divergence (Df1 defined in Section 3.1) between
Pi (V |pa(V )) and Pj (V |pa(V )). We have the following
relation,

✓
◆
Pi (V |pa(V ))
Ei exp
= [1 + Df1 (Pi kPj )] e.
Pj (V |pa(V ))
(15)
The following lemma expresses the quantity ⌘i,j (✏) as a
separable function of Df1 (Pi kPj ) and ✏, and is one of the
key tools used in subsequent analysis.
Lemma 2. It holds that, ⌘i,j (✏)  log
log (1 + Df1 (Pi kPj )). Furthermore,
⌘i,j (✏)  2 log

2
✏

+ 1 +

✓ ◆
2
[1 + log(1 + Df1 (Pi kPj ))]
✏

(16)

when, ✏  1.
Proof. We have the following chain:
✓
◆
Pi (V |pa(V ))
Pi
>⌘
Pj (V |pa(V ))
✓
✓
◆
◆
Pi (V |pa(V ))
= Pi exp
> exp(⌘)
Pj (V |pa(V ))

✓
◆
(a)
Pi (V |pa(V ))
 Ei exp
exp( ⌘)
Pj (V |pa(V ))

(17)

(18)

(a) - We used Markov’s inequality. Suppose, we have the
right hand side to be at most ✏/2. Then we have,


Ei exp

✓

Pi (V |pa(V ))
Pj (V |pa(V ))

◆

From, the definition of ⌘i,j (✏), we have:
✓ ◆
2
⌘i,j (✏)  log
+ 1 + log(1 + Df1 (Pi kPj ))
✏
✓ ◆
a
2
 2 log
[1 + log(1 + Df1 (Pi kPj ))] , 8✏  1
✏
(21)

exp( ⌘)  ✏/2

Now using (15), we have:
✓ ◆
2
⌘ log
+ 1 + log(1 + Df1 (Pi kPj ))
✏
✓
◆
Pi (V |pa(V ))
=) Pi
> ⌘  ✏/2
Pj (V |pa(V ))

(19)

(20)

1

Now, we introduce the main result of this section as Theorem 3. Recall that Mij = 1 + log(1 + Df1 (Pi kPj )).
(⌘)

Theorem 3. The estimate Ŷi (j) for ⌘ = 2 log(2/✏)Mij
satisfies the following confidence guarantees:
⇣
⌘
(⌘)
P µi
✏/2  Ŷi (j)  µi +
!
2
t
1 2 exp
.
2
8 log(2/✏)2 Mij
Proof. The proof is immediate from Lemmas 2 and 1.
E.3. Aggregating Heterogenous Clipped Estimators

In Section E.1, we have seen how samples from one of the
candidate distribution can be used for estimating the target
mean under another arm. Therefore, it is possible to obtain information about the target mean under the k th arm
(Ek [Y ]) from the samples of all the other arms. It is imperative to design an efficient estimator of Ek [Y ] (8k 2 [K])
that seamlessly uses the samples from all arms, possibly
with variable weights depending on the relative divergences
between the distributions. In this section we will come up
with one such estimator, based on the insight gained in Section E.1.
Recall the quantities Mkj = 1 + log(1 + Df1 (Pk kPj ))
(8k, j 2 [K]). These quantities will be the key tools in
designing the estimators in this section. Suppose we obtain
⌧i samples from arm i 2 [K]. Let the total number of
samples from all arms put together be ⌧ .
Let us index all the samples by s 2 {1, 2, .., ⌧ }. Let
Tk ⇢ {1, 2, .., ⌧ } be the indices of
Pall the samples collected
from arm k. Further, let Zk = j2[K] ⌧j /Mkj . Now, we
are at the position to introduce the estimator for µk , which
we will denoted by Ŷk✏ (✏ is an indicator of the level of confidence desired):
K
1 XX 1
Pk (Vj (s)|pa(V )j (s))
=
Yj (s)
⇥
Zk j=0
Mkj
Pj (Vj (s)|pa(V )j (s))
s2Tj
⇢
Pk (Vj (s)|pa(V )j (s))
1
 2 log(2/✏)Mkj .
(22)
Pj (Vj (s)|pa(V )j (s))

Ŷk✏

Identifying Best Interventions through Online Importance Sampling

In other words, Ŷk✏ is the weighted average of the clipped
samples, where the samples from arm j are weighted by
1/Mkj and clipped at 2 log(2/✏)Mkj .
Lemma 3.
h

i

h

µ̂k := E Ŷk✏  µk  E Ŷk✏

i

✏
+
2

(23)

Proof. We note that Ŷk✏ can be written as:

Ŷk✏ =

K
1 X ⌧j
Ỹ ✏
Zk j=0 Mkj kj

P
P (V (s)|pa(V ) (s))
✏
Here, Ỹkj
= ⌧1j s2Tj Yj (s) Pkj (Vjj (s)|pa(V )jj (s))
n
o
P (V (s)|pa(V ) (s))
⇥1 Pkj (Vjj (s)|pa(V )jj (s))  2 log(2/✏)Mkj .
Lemma 2 it is easy to observe that E[Ỹk✏ ]

(24)

E[Ỹk✏ ]+ 2✏

Theorem 4. The estimator Ŷk✏ of (22) satisfies the following concentration guarantee:

1

✓
◆
2
⌧
µ̄k |   2 exp
8(log(2/✏))2
✓
◆
⌧
⌧
⌧
=) P |Ȳk
µ̄k | 
Zk
Zk
Zk
✓
◆
2
⌧
 2 exp
8(log(2/✏))2
✓
◆
⌧
=) P |Ŷk µ̂k | 
Zk
✓
◆
2
⌧
 2 exp
8(log(2/✏))2
⇣
⌘
=) P |Ŷk µ̂k | 
✓ ◆2 !
2
⌧
Zk
 2 exp
8(log(2/✏))2
⌧

P |Ȳk

(26)

(27)

(28)
(29)

Now we can combine Equations (26) and (23) we get:
Using

 µk 
as ⌘kj (✏)  2 log(2/✏)Mkj .PNow, (24) together with this
implies the lemma as Zk = j2[K] ⌧j /Mkj .

⇣
P µk

the following chain:

⌘
✏/2  Ŷk✏  µk +
✓ ◆2 !
2
⌧
Zk
2 exp
2
8(log(2/✏))
⌧

⇣
P µk
1

⌘
✏/2  Ŷk✏  µk +
✓ ◆2 !
2
⌧
Zk
2 exp
2
8(log(2/✏))
⌧

In Theorem 4, we observe that the first part of the exponent
scales as O(✏2 ⌧ /(log(2/✏))2 ) if we set = O(✏), which is
very close to the usual Chernoff’s bound with ⌧ i.i.d samples. The performance of this estimator therefore depends
on the factor (Zk /⌧ ) which depends on the fixed quantities Mkj (8j) and the allocation of the samples ⌧j . In the
next section, we will come up with a strategy to allocate the
samples so that the estimators Ŷk✏ have good guarantees for
all the arms k.
E.4. Allocation of Samples

Proof. For the sake of analysis, let us consider the rescaled
version Ȳk✏ = (Zk /⌧ )Ŷk✏ which can be written as:
K
1XX 1
Pk (Vj (s)|pa(V )j (s))
Yj (s)
⌧ j=0
Mkj
Pj (Vj (s)|pa(V )j (s))
s2Tj
⇢
Pk (Vj (s)|pa(V )j (s))
⇥1
 2 log(2/✏)Mkj . (25)
Pj (Vj (s)|pa(V )j (s))

Ȳk✏ =

Since Yj (s)  1, we have every random variable in the sum
in (25) bounded by 2 log(2/✏)
Let, µ̄k = E[Ȳk✏ ]. Therefore by Chernoff’s bound, we have

In Section E.3, Theorem 4 tells us that the confidence guarantees on the estimator depends on how the samples are
allocated between the arms. To be more precise, the term
(Zk /⌧ ) in Equation (26), affects the performance of the estimator for µk (Ŷk✏ ). We would like to maximize (Zk /⌧ )
for all arms k 2 [K].

Let the total budget be ⌧ . Let R be the set of arms that
remain in contention for the best optimal arm. Consider
the matrix A 2 RK⇥K such that Akj = 1/Mkj for all
k, j 2 [K]. Then, we decide the fraction of times arm k
gets pulled, i.e. ⌫k to maximize Zk using the Algorithm 3.
Lemma 4. Allocation ⌧ in Algorithm 3 ensures that
1
(Zk /⌧ )
⇤ (B,R) for all k 2 R.

Identifying Best Interventions through Online Importance Sampling

This is essentially the best allocation of the individual arm
budgets in terms of ensuring good error bounds on the estimators Ŷk✏ for all k 2 R. Since S1 is a special case of
S2, to obtain the allocation for S1 one needs to set the cost
values ci set to 1 for i 2 B (difficult arms) in the above
formulation and 0 otherwise.
E.5. Putting it together: Online Analysis
We analyze Algorithm 1 phase by phase. With some abuse
of notation, we redefine various quantities to be used in the
analysis of the algorithm. Each quantity depends on the
phase indices, as follows:
• R(l): Set of arms remaining after phase l

• ŶH (l): The value of the highest estimate maxk Ŷk (l)
(in Algorithm 1).

.

• Sl : Success event of phase l defined as:
⇢
1
Sl := \k2R(l),k6=k⇤ Ŷk (l)  µk + l 1
2
⇢
3
\ µk
 Ŷk⇤ (l) .
2l

(30)

(31)

Now we will establish that the occurrence of the event Sl
implies that all arms in A(l) gets eliminated at the end of
phase l, while at the same time the optimal arm survives.
Consider an arm k 2 A(l). Given Sl has happened we
have:
ŶH (l)

Ŷk⇤ (l)

Ŷk (l)  µk +

µk ⇤

3
2l

1
2l

1

This further implies that ŶH (l) Ŷk (l)
5/2l >
k
l
5/2 . Therefore all the arms in A(l) are eliminated given
Sl . Following similar logic it is also possible to show that
the optimal arm survives. If ŶH (l) = Yk⇤ (l) then it survives certainly. Now, given Sl , only arms in R(l) \ A(l)
can be the ones with the highest means. Consider arms
k 2 R(l) \ A(l). Again given Sl we have:
3
2l
1
Ŷk (l)  µk + l 1
2

Ŷk⇤ (l)

µk ⇤

k

< 5/2l .

It would seem that now it would be easy to analyze the
probability of the event Sl , using Theorem 4. However, the
bound in Theorem 4 depends on the sequence of arms eliminated so far in each phase. Therefore it is imperative to
analyze S1:l , that is the event that all phases from 1, 2, .., l
succeed. Let Bl = P(Slc |S1:l 1 ). So, we have the chain:
P(S1:l )

P(S1:l |S1:l
P(S1:l |S1:l
lY
2

1 ends.

• Ŷk (l): The value of the estimator (in Algorithm 1) for
arm k at the end of phase l.

• A(l) ✓ R(l): Set of arms given by:
⇢
10
A(l) := k 2 R(l) : k > l
2

Therefore, we have Yk (l) Yk⇤ (l)  5/2l
Therefore, the optimal arm survives.

i=0

=

l
Y

1 )P(S1:l 1 )
1 )P(S1:l 1 |S1:l 2 )P(S1:l 2 )

P(S1:l i |S1:l
(1

i 1 )P (S1 )

Bs )

s=1

1

l
X

Bs

s=1

The advantage of analyzing the probability of S1:l is that
given S1:l we know the exact sequences of the arms that
have been eliminated till Phase l. This gives us exact control on the exponents in the bound of Theorem 4. Given
S1:s 1 we have,
⇢
10
⇤
R(s) ✓ R (s) := k : k  s 1 .
2

Recall that the budget for the samples of each arm in any
phase s, is decide by solving the LP in Algorithm 3. There⇤
fore, given S1:s 1 , we have ⇤ (B, R(s))
(B, R⇤ (s)).
Therefore, we have the following key lemma.
Lemma 5. We have:
Bl := P (Slc |S1:l

1)

 2|R⇤ (l)| exp

✓

2

2(l 1)

⇤

⇤

⌧ (l)v (B, R (l))
8l2

2

(32)
◆

Proof. Note that in this phase we set ⌘kj = 2lMkj . Setting ✏ = 2 (l 1) and = 2 (l 1) in Theorem 4 and by
Lemma 4 we have:
✓
◆
3
1
P µk
 Ŷk (l)  µk + l 1
(33)
2l
2
✓
◆
2 2(l 1) ⌧ (l)v ⇤ (B, R⇤ (l))2
1 2 exp
8l2
Note that the samples considered in phase l are independent
of the event S1:l 1 . Doing a union bound of the event complementary to the success event in (31), for all the remaining arms in R⇤ (`) implies the result in the Lemma.

Now we are at a position to introduce our main results as
Theorem 5.

Identifying Best Interventions through Online Importance Sampling

p
provided k
10/ T . Justification for (a) - If arm k is
chosen finally, it implies that it is not eliminated at phase
k . Therefore the regret of the algorithm is given by:

Theorem 5. Consider a problem instance with K candi1
date arms {Pk (V |pa(V )}K
k=0 . Let the gaps from the optimal arm be k for k 2 [K]. Let us define the following
important quantities:
( $
✓
◆% $
✓
◆ %)
10
10
⇤
R ( k ) = s : log2
log2
s

k

r(T, B) 

(34)

log2 (10/ l )3
H̄k = max
2 ⇤
⇤
{l: l
k} (
l /10) v (B, R (
H̄ = max⇤
k6=k

log2 (10/ k )3
( k /10)2 v ⇤ (B, R⇤ (

l

k ))

(36)

2

Algorithm 1 satisfies the following guarantees:

 2K 2

X

k

log2

k6=k⇤ p
:
k 10/ T

n

10
+ p 1 9k 6= k ⇤ s.t
T

k

✓

20
k

◆

exp

✓

T
2H̄k log(n(T ))

p o
< 10/ T

2. The error probability is bounded as:
✓
e(T, B)  2K 2 log2 (20/ ) exp

◆

r(T, B) 

{k6=k⇤ :

exp
T
2H̄log(n(T ))

Thepbound on the error probability only holds if
10/ T for all k 6= k ⇤ .

k

p

k
X

2|R⇤ (l)|

2(l 1)

2

(39)

l=1

10/ T }

(40)
(41)

Let `1 , `2 ..`s = k such that R⇤ (`) changes value only at
these phases. Let us set `s+1 = `s + 1 for convenience in
notation. Combining this notation with (39) we have:

1. The simple regret is bounded as:
r(T, B)

✓

k

◆◆
⌧ (l)v ⇤ (B, R⇤ (l))2
8l2
n
p o
10
+ p 1 9k 6= k ⇤ s.t k < 10/ T
T
exp

(35)

))2

{k6=k⇤ :

X

◆

✓

2

X
k

p

s
X

k

⇤

⇤

T v (B, R (li ))
2`3i log(n(T ))

10 n
+ p 1 9k 6= k ⇤ s.t
T

`i )

i=1

10/ T }

2`i

2|R⇤ (`i )| (`i+1

k

2

◆◆

(42)
(43)

p o
< 10/ T

k

Proof. Recall that the simple regret is given by:
⇣
⌘
X
r(T, B) =
P
k̂(T,
B)
=
k
k

(37)

k6=k⇤

Consider the phase `i when ideally at least an arm leaves.
Let one of those arms be l. Recall that, l is the phase
where the arm ideally leaves according to (38). Therefore,
⇤
⇤
l = `i . Also it is easy to observe that: R ( l ) = R (`i ).
We have,

Let us introduce some further notation. Let us define the
phase at which an arm is ideally deleted as follows:
k

:= (

k)

:= l if

10
<
2l

k



10
2l 1

(38)

Therefore we have the following chain:
⇣
⌘ a
c
P k̂(T, B) = k  P S1:
k



k
X

Bl

l=1

k
X

l=1

2|R⇤ (l)| exp

✓

2

2(l 1)

⌧ (l)v ⇤ (B, R⇤ (l))2
8l2

◆

`i

log2 (10/

l)

(44)

as `i+1 `i  log2 (20/ k ), i  s. Further, for every
`i < k , there is at least one distinct arm l : l = `i .
This is because an arm leaves only once ideally. Further,
we associate `s with arm k although other arms may leave
at the phase `s = k . Further, all arms l associated with
`i < k are such that l
k . This is because of (38)
and the fact that `i < `s = k . Therefore, the r.h.s in

Identifying Best Interventions through Online Importance Sampling

Equation (42) is upper bounded as follows:

r(T, B) 

{k6=k⇤ :

X
k

p

k

@

{l:

10/ T }

✓

0

X
l

bounded by:

2|R⇤ (

e(T, B) 
l )|

k}

m
X

2|R⇤ (`i )| (`i+1

exp

✓

2

T v ⇤ (B, R⇤ (li ))2
2`3i log(n(T ))

2`i

(47)

`i )

i=1

◆

◆◆
T v ⇤ (B, R⇤ ( l ))2
log2 (20/ k ) exp
2 log2 (10/ l )3 log(n(T ))
Consider the phase `i when ideally at least an arm leaves.
n
p o
10
Let one of those arms be k. Recall that, k is the phase
⇤
+ p 1 9k 6= k s.t k < 10/ T
where the arm ideally leaves according to (38). Therefore,
T
0
1
⇤
⇤
k = `i . Also it is easy to observe that: R ( k ) = R (`i ).
X
X
(a)
Then,
@

2|R⇤ ( l )|A
k
p
`i log2 (10/ k )
(48)
{l: l
k}
{k6=k⇤ : k 10/ T }
✓
◆
Further, for every `i , there is a distinct and different k :
T
log2 (20/ k ) exp
k = `i . This is because an arm leaves only once ideally.
2H̄k log(n(T ))
Therefore,
according to (48) and (47) we have:
p o
10 n
+ p 1 9k 6= k ⇤ s.t k < 10/ T
T
X
X
(b)
e(T, B) 
2|R⇤ ( k )| ⇤
(49)
2
 2K
k log2 (20/ k )
⇤
k6
=
k
p
✓
◆
{k6=k⇤ : k 10/ T }
( k /10)2 T v ⇤ (B, R⇤ ( k ))2
✓
◆
exp
T
2 log2 (10/ k )3 log(n(T ))
exp
✓
◆
2H̄k log(n(T ))
T
2
o
p

2K
log
(20/
)
exp
10 n
2
+ p 1 9k 6= k ⇤ s.t k < 10/ T
2H̄log(n(T ))
T
(50)
(

l /10)

2

Here (a) is by definition of H̄k while (b) is because
|R⇤ ( l )|  K and there are at most K terms in the summation.
Another quantity of interest here is the error probability. We will only provide bounds p
on the error probability
e(T, B) when we have k > 10/ T for all k 6= k ⇤ . Let
= mink6=k⇤ k and ⇤ = ( ). In this case we have:
e(T, B)  1

P (S1: ⇤ )

Here, we have used the definition of H̄ and the fact that
|R⇤ ( k )|  K and ⇤  log(20/ )

F. More Experiments
In this section we provide more details about our experiments. We also include extensive synthetic simulations results.
F.1. Synthetic Experiments

In this section, we empirically validate the performance our
algorithm through synthetic experiments. We carefully de
Bl
(45)
sign our simulation setting which is simple, but at the same
l=1
⇤
✓
◆ time sufficient to capture the various tradeoffs involved in
X
2 2(l 1) ⌧ (l)v ⇤ (B, R⇤ (l))2 the problem. An important point to note is that our algo⇤

2|R (l)| exp
8l2
rithm is not aware of the actual effect of the changes on the
l=1
target (gaps between expectations) but it only knows the
⇤
✓
◆
a X
2 2l T v ⇤ (B, R⇤ (l))2
divergence among the candidate soft interventions. Some⇤

2|R (l)| exp
3 log(n(T ))
times, a change with large divergence from an existing one
2l
l=1
may not maximize the effect we are looking for. Con(46)
versely, smaller divergence may sometimes lead you closer
to the optimal. We demonstrate that our algorithm per(a)- This follows from the definition of ⌧ (l).
forms well in all the experiments, as compared to previous
works (Audibert & Bubeck, 2010; Lattimore et al., 2016).
⇤
⇤
As before let `1 = 1, `2 ..`m 
such that R (`) changes
value only at these phases. Let us set `m+1 = `m + 1 for
Experimental Setup: We set up our experiments accordconvenience in notation. Then, e(T, B) in (46) is upper
ing to the simple causal graph in Figure 6. V is assumed to
⇤

X

Identifying Best Interventions through Online Importance Sampling
V 2 {0, 1, .., m

1}

V

P0 (V ), P1 (V ), · · · PK

Y
Y =

(

f (V )
1 f (V )

1 (V

)

✏ (External Noise)

if ✏ = 0
if ✏ = 1

Figure 6. Causal Graph for Experimental Setup

be a random variable taking values in {0, 1, 2, · · · , m 1}.
The various arms P0 (V ), P1 (V ), ...PK 1 (V ) are discrete
distributions with support [m]. We will vary m and K over
the course of our experiments.
Y is assumed to be a function of V
✏ which is external to the system.
set the function as follows:
(
f (V )
Y =
1 f (V )

and some random noise
In our experiments, we
if ✏ = 0
if ✏ = 1

where f : [m] ! {0, 1} is an arbitrary function. We set
P(✏ = 1) = 0.01 in all our experiments. The discrete candidate distributions are modified to explore various tradeoffs between the gaps and the effective standard deviation
parameters.
Budget Restriction: The experiments are performed in the
budget setting S1, where all arms except arm 0 are deemed
to be difficult. We plot our results as a function of the total
samples T , while
the difficult arms
p the fractional budget ofP
p
(B) is set to 1/ T . Therefore, we have k6=0 Tk  T .
This essentially belongs to the case when there is a lot of
data that can be acquired for a default arm while any new
change requires significant cost in acquiring samples.
Competing Algorithms: We test our algorithms on different problem parameters and compare with related prior
work (Audibert & Bubeck, 2010; Lattimore et al., 2016).
We briefly describe the algorithms compared:
1. SRISv1: This is Algorithm 1 introduced in Section 3.2.
2. SRISv2: This is Algorithm 2 which is a simple modification of SRISv1, as detailed in Section 3.2.

3. SR: This is the best arm identification algorithm
from (Audibert & Bubeck, 2010) adapted to the budget setting. The division of the total budget T into
K 1 phases is identical, while the individual arm
budgets are decided in each phase according to the
budget restrictions.
4. CR: This is Algorithm 2 from (Lattimore et al., 2016).
The optimization problem for calculating the mixture
parameter ⌘ has been modified to account for the budget restrictions. This is a natural modification to the
algorithm.
Experiments: In our experiments, we choose f to be the
parity function, when V 2 [m], is represented in base 2.
Note that arm 0 is the arm that can be sampled O(T ) times
p
while the rest of the arms can only be sampled O( T )
times due to the above budget constraints. So, the divergence of the arm 0 from other arms is crucial alongside the
gaps. We perform our experiments in different regimes that
get progressively easier. In these experiments, we function
in various regimes of the divergences between the other
arms and arm 0, and the gaps from the optimal arm in terms
of target value. When there is no information leakage, the
samples are divided among the K arms. So, the loss
p in
having multiple arms can be expressed as a scaling K
in standard deviation. Recall the log divergence measure
Mk0 which is a measure of information leakage from arm
0 to another arm k. Therefore, in the following, when
p we
say high divergence from arm 0, it means that Mk0 / K is
high for most arms k 6= 0.
High Divergence and Low : This is the hardest of all
settings. Here, we set m = 20 and K = 30. Here, we
have Mk0 to be pretty high for all the arms k 6= 0. This
means that the arm 0, which can be pulled O(T ) times
provides
p highly noisy estimates for other arms. We have
Mk0 / K ⇠ 30 for most arms. Moreover, the minimum
gap from the best arm
= 0.04, which is pretty small.
This implies that it is harder to distinguish the best arm.
The results are demonstrated in Figure 7. Figure 7a displays the simple regret. We see that both SRISv1 and
SRISv2 outperform the others by a large extent, in this hard
setting, even when the number of samples are very low. In
Figure 7b we plot the probability of error in exactly identifying the best arm. We see that none of the algorithms successfully identify the best arm, in the small sample regime,
as the gap is very low. However, our algorithms quickly
zero in on arms that are almost as good as the optimal, and
therefore the simple regret is well-behaved. Our algorithm
performs this well even when the divergences are big, because it is able to reject the arms that have high i in the
early phases, very effectively.
High Divergence and High

: This is easier than the pre-

Identifying Best Interventions through Online Importance Sampling

SR
SRISv1
SRISv2
CR

0.2
0.15
0.1
0.05

0.9
0.85
0.8
0.75
0.7

200

300

400

T (Note that B =

√1
T

500

0.6
0

600

100

200

)

300

400

T (Note that B =

(a) Simple Regret

√1
T

500

600

)

1

Simple Regret (r(T, B))

0.3
0.25
0.2
0.15
0.1
0.05
0
0

SR
SRISv1
SRISv2
CR

0.9

Error Prob. (e(T, B))

SR
SRISv1
SRISv2
CR

0.4

0.8
0.7
0.6
0.5
0.4
0.3
0.2

200

400

600

800

1000

T (Note that B =

√1
T

1200

)

(a) Simple Regret

1400

1600

0.1
0

0.2
0.15
0.1

200

400

600

800

1000

T (Note that B =

√1
T

0
0

200

400

600

800

1000

1200

1400

√1
T

1200

)

(a) Simple Regret

Figure 7. Performance of various algorithms when divergences
Mk0 ’s are high and mininum gap
is small. The results are
averaged over the course of 500 independent experiments. The
total sample budget T is plotted on the x-axis. Note that budget
p
for all arms other than arm 0 is constrained to be less than T .
Here K = 30.

0.35

0.25

T (Note that B =

(b) Probability of Error

0.45

0.3

0.05

0.65
100

1
SR
SRISv1
SRISv2
CR

0.35

1400

1600

SR
SRISv1
SRISv2
CR

0.9

Error Prob. (e(T, B))

Error Prob. (e(T, B))

Simple Regret (r(T, B))

0.25

0
0

0.4

1
0.95
SR
SRISv1
SRISv2
CR

0.3

Simple Regret (r(T, B))

0.4
0.35

0.8
0.7
0.6
0.5
0.4
0

200

400

600

800

1000

T (Note that B =

√1
T

1200

1400

1600

)

(b) Probability of Error

Figure 9. Performance of various algorithms when divergences
Mk0 ’s are moderately low and min. gap is small. The results
are averaged over the course of 500 independent experiments.
The total sample budget T is plotted on the x-axis. Note that
budget
for all arms other than arm 0 is constrained to be less than
p
T . Here, K = 30.

p
ing the other arms. Here, Mk0 / K  10 for most arms k.
However, the minimum gap from the best arm
= 0.04,
which is small. This implies that it might be hard to distinguish the best arm.

1600

)

(b) Probability of Error

Figure 8. Performance of various algorithms when divergences
Mk0 ’s are high and min. gap
is not too bad. The results are
averaged over the course of 500 independent experiments. The
total sample budget T is plotted on the x-axis. Note that budget
p
for all arms other than arm 0 is constrained to be less than T .
Here, K = 20.

vious setting. Here, we set m = 10 and K = 20. Here, we
have Mk0 to be very high for all the arms k 6= 0. Thus arm
0 provides
p very noisy estimates on other arms. We have
Mk0 / K
50 for many arms. However, the minimum
gap from the best arm
= 0.15, which is not too small.
This implies that it might be easier to distinguish the best
arm.
The results are demonstrated in Figure 8. Figure 8a displays the simple regret. We see that in the small sample
regime SRISv1 and SRISv2 outperform the others by a
large extent. In the high sample regime, SRISv2 is still the
best, while SR and SRISv1 are close behind. In Figure 8b
we plot the probability of error in exactly identifying the
best arm. We see that SRISv2 performance very well in
identifying the best arm even though arm 0 gives highly
noise estimates. It is interesting to note that CR does not
perform well. This can be attributed to the non-adaptive
clipper in CR, that incurs a significant bias because arm 0
has high-divergences from most of the other arms.
Low Divergence and Low : This is another moderately
hard setting, similar to the previous one. Here, we set
m = 20 and K = 30. Here, we have Mk0 to be not too
high for the arms k 6= 0. This means that the arm 0, which
can be pulled O(T ) times is moderately good for estimat-

The results are demonstrated in Figure 9. Figure 9a displays the simple regret. We see that in the small sample
regime SRISv1 and SRISv2 outperforms the others by a
large extent. In the high sample regime, SRISv2 is still the
best, while CR is close behind. In Figure 9a we plot the
probability of error in exactly identifying the best arm. We
see that most of the algorithms have moderately bad probability of error as the gap is small. However, the algorithms SRISv2 and SRISv1 are quickly able to zero down
on arms close to optimal as shown in the simple regret in
the small sample regime.
Low Divergence and High : This is the easiest of all
settings. Here, we set m = 10 and K = 20. Here, arms 0
has P0 (V ) pretty close to the uniform distribution on [m].
Therefore, it is very well-posed for estimating
the means
p
of all other arms. In fact we have Mk0 / K < 2 for many
arms. Moreover, the minimum gap from the best arm =
0.15, which is not too small. This implies that it might be
very easy to distinguish the best arm.
The results are demonstrated in Figure 10. Figure 10a displays the simple regret. We see that SRISv2 and CR perform extremely well closely followed by SRISv1. In Figure 10b we plot the probability of error in exactly identifying the best arm. Again SRISv2 and CR have almost zero
probability of error and SRISv1 is close behind. This is because is pretty large. In this example, we observe that all
the algorithms that use information leakage are better than
SR, because arm 0 is well-behaved. CR performs almost as
well as SRISv2 in this example, as the static clipper is never
invoked because almost always the ratios in the importance
sampler are well bounded.
In conclusion, it should be noted that our algorithms perform well in all the different settings, because they are able

Identifying Best Interventions through Online Importance Sampling
0.14

1
SR
SRISv1
SRISv2
CR

0.3
0.25
0.2
0.15
0.1

Error Prob. (e(T, B))

Simple Regret (r(T, B))

0.35

SR
SRISv1
SRISv2
CR

0.8

0.12

0.6

0.4

0.1

0.2

0.05
0
0

200

400

600

800

1000

T (Note that B =

√1
T

1200

)

(a) Simple Regret

1400

1600

0
0

200

400

600

800

1000

T (Note that B =

√1
T

1200

1400

1600

0.08

)

(b) Probability of Error

Figure 10. Performance of various algorithms when divergences
Mk0 ’s are low and min. gap
is not too bad. The results are
averaged over the course of 500 independent experiments. The
total sample budget T is plotted on the x-axis. Note that budget
p
for all arms other than arm 0 is constrained to be less than T .
Here K = 20.

0.06

0.04

0.02
0

100

200

300

400

500

600

700

800

900

Figure 12. Comparison of SRISv2 with two different divergence
metric. It shows that our divergence measure is fundamental for
good performance. The experiments have been performed in a
setting identical to Fig. 3c

F.3. More on Interpretation of Inception Deep Network

Figure 11. Histograms of data from the cytometry data-set and
from the GLM trained for the activations of an internal node pip2.

to adapt to the problem parameters (similar to (Audibert &
Bubeck, 2010)) and at the same time leverage the information leakage (similar to (Lattimore et al., 2016)).
F.2. More on Flow Cytometry Experiments
In this section we give further details on the flow cytometry experiments. As detailed in the main paper we use
the causal graph in Fig. 5(c) in (Mooij & Heskes, 2013)
(shown in Fig. 3a) as the ground truth. Then we fit a GLM
gamma model (Hardin et al., 2007) between each node in
the graph and its parents using the observational data-set.
The GLM model produces a highly accurate representation
of the flow cytometry data-set. In Fig. 11 we plot the histogram for the activation of an internal node pip2 from the
real data and samples generated from the GLM probabilistic model. It can be seen that the histograms are very close
to each other.
In Fig. 12 we plot the performance of SRISv2 when the
divergence metric is replaces by KL-divergence. In one
of the plots SRISv2 is modified by setting Mij = 1 +
KL(Pi , Pj ). It can be seen that the performance degrades,
which signifies that our divergence metric is fundamental
to the problem.

In this section we describe the methodology of our model
interpretation technique in more detail. In Section 4.2 we
have described how the best arm algorithm can be used to
pick a distribution over the superpixels of an image, that
has the maximum likelihood of producing a certain classification from Inception. Here, we describe how the distributions over the superpixels are generated and how they
are used subsequently. The arm distributions are essentially
points in the n-dimensional simplex (where n is the number
of superpixels into which the image is segmented). These
distributions are generated in a randomized fashion using
the following methods:
1. Generate a point uniformly at random in the ndimensional simplex.
2. Randomly choose l < n superpixels. Make the distribution uniform over them and 0 elsewhere.
3. Randomly choose l < n superpixels. The probability
distribution is a uniformly chosen random point over
the l-dimensional simplex with support on the l chosen superpixels and 0 elsewhere.
4. Start a random walk from a few superpixels which
traverses to adjacent superpixels at each step. Stop
the random-walk after a certain number of steps and
choose the superpixels touched by the random walk.
Then choose a uniform distribution over the super
pixel support or choose a random distribution from the
simplex of probability distributions over the support
of the chosen superpixels (like in the previous point)
and 0 elsewhere. This method uses the geometry of
the image.
Note that all the above methods do not depend on the specific content of the images. Using the above methods L

Identifying Best Interventions through Online Importance Sampling

pull arms are chosen which are used to collect the rewards.
Further there are K opt arms that are the interventions to
be optimized over. When an arm is used to sample, then
m ⌧ n superpixels are chosen with replacement from the
distribution of that arm. These pixels are preserved in the
original image while everything else is blurred out before
feeding this into the neural network. Thus if the distribution
corresponding to an arm is P , then the actual distribution
to be used for the importance sampling is the product distribution P m . Note that the pull arms are separate from the
opt arms. The true counterfactual power of our algorithm
is showcased in this experiment, as we are able to optimize
over a large number of interventions that are never physically performed.

