Globally Induced Forest: A Prepruning Compression Scheme
Supplementary material

Jean-Michel Begon 1 Arnaud Joly 1 Pierre Geurts 1

1. GIF algorithm

and the solution is given by

Figure 1 illustrates visually the inner loop of the GIF building algorithm: a subset of the candidates nodes is chosen
uniformely at random. The contribution of each node is
evaluated and the one which reduces the error the most is
added to the model. Its children are then built and added to
the candidate list.

2. Optimization problem
We are building an additive model by inserting progressively nodes in the forest. At time t, we are trying to find
the best node j (t) from the candidate list Ct and its associ(t)
ated optimal weight wj :
(t)

j (t) , wj = arg min

N
X



L yi , ŷ (t−1) (xi ) + wzj (xi )

j∈Ct ,w∈RK i=1

(t)

wj =
(t−1)

where ri
= yi − ŷ (t−1) (xi ) is the residual at time
t − 1 for the ith training instance and Zj = {1 ≤ i ≤
N |zj (xi ) = 1} is the subset of instances reaching node j.
Classification For classification we used the multiexponential loss (Zhu et al., 2009). First, we need to encode
the labels so that
(
1,
if the class of yi is k
(k)
(4)
yi =
1
, otherwise
− K−1
where K is the number of classes.
Notice that
P
(k)
K
y
=
0.
The
optimization
then
becomes
i
k=1
(t)
wj

This problem is solved in two steps. First a node j is
selected from Ct and the corresponding optimal weight,
alongside the error reduction, are computed. This is repeated for all nodes and the one achieving the best improvement is selected.
Regression

For regression, we used the L2-norm:

(t)

wj = arg min
w∈R

N
X

= arg min
w∈RK

N
X

Department of Electrical Engineering and Computer Science University of Liège, Liège, Belgium. Correspondence
to: Jean-Michel Begon <jm.begon@ulg.ac.be>, Pierre Geurts
<p.geurts@ulg.ac.be>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

exp


−1 T  (t−1)
y ŷ
(xi ) + wzj (xi )
K i
(5)

=

(t−1)
arg min Fj
(w)
K
w∈R
(t−1)

Solving for ∇Fj

(t−1,k) (k)

αj

φ

(6)

(w) = 0 yields

(w) =

K
1 X (t−1,l) (l)
αj
φ (w)
K

(7)

l=1

for 1 ≤ k ≤ K, where


X
(t−1,k)
(t−1)
αj
,
exp −µi

2
L yi , ŷ (t−1) (xi ) + wzj (xi )
(2)

1



i=1

(8)

(k)
i∈Zj



i=1

(3)

i∈Zj

(1)
(t−1)
where (xi , yi )N
() is the
i=1 is the learning sample, ŷ
model at time t − 1, zj () is the node indicator functions,
meaning that it is 1 if its argument reaches node j and 0
otherwise.

1 X (t−1)
ri
|Zj |

K
1 X
yi ŷ (t−1,k) (xi )
K
k=1


1
φ(k) (w) , exp − ψ (k) (w)
K
(t−1)

µi

,

ψ (k) (w) , −w(k) +

K
X
1
w(l)
K −1
l=1,l6=k

(9)
(10)
(11)

Globally Induced Forest
(k)

(k)

where Zj = {1 ≤ i ≤ N |zi,j = 1 ∧ yi = 1} is the
subset of learning instances of class k reaching node j. In
(t−1)
words, µi
is the hyper-margin of instance i at time t−1
(t−1,k)
and αj
is the class error of label k for node j at time
t − 1.

T rj = |Z1j |
diction is:

Equation 7 is equivalent to

The prediction of node j is

(t−1,k) (k)
αj
φ (w)

=

(t−1,l) (l)
αj
φ (w)

ŷ

(t−1,k)

k=1

=

K
X

ŷ

(t,k)

=0=

k=1

w

(k)

1 X
yi
|Zj |

(18)

ŷj = ŷπj + wj

1 X
= ŷπj +
yi − ŷπj
|Zj |

(19)
(20)

i∈Zj

= ŷπj +

1 X
(yi ) − ŷπj
|Zj |

(21)

i∈Zj

=

(13)

1 X
yi
|Zj |

(22)

i∈Zj

k=1

The first step is how the additive model is built. The second
is the optimal weight value of node j derived in Equation
3, the third step is due to the fact that the prediction at πj is
constant since there is only one tree.

and this is not impacted by the learning rate.
The corresponding solution is


1
φ(k) (w) = exp −
w(k)
K −1


X
1
(t−1,k)
αj
=
ŷ (t−1,k) (xi )
exp −
K −1
(k)

(14)
3.2. Classification
(15)

i∈Zj

K
αj
K −1X
=
log (t−1,l)
K
αj
l=1

(16)

3. Equivalence of GIF and the underlying tree
In the case of a single tree (T = 1) and a unit learning rate
(λ = 1), both the square loss in regression and the multiexponential loss in classification produce the same predictions as the underlying tree. This is due to the fact that,
when examining the weight to give to node j at time t, the
prediction of time t − 1 relates to the parent node πj of
j. It is thus independent of t and is also the same for all
instances reaching that node.
Consequently, we will adopt the following slight change in
notation:
ŷj = ŷ(πj ) + wj

In order to have the same prediction as the underlying tree,
we must demonstrate that the probability of being in class
(l)

(t−1,k)

(t,k)
wj

yi . We need to show that the GIF pre-

i∈Zj

1 ≤ k, l ≤ K (12)

K
X

i∈Zj

ŷj =

In keeping with the output representation (Equation 4), we
can impose a zero-sum constraint on the prediction to get
(t)
a unique solution for the kth component of wj . If it is
imposed at each stage, it means that
K
X

P

|Zj |
|Zj | .

l associated to node j will be

Under the zero-sum constraint, we have


1
1
(l)
exp
wj
= απ(l)j
(23)
K −1
cj


1
1 X
exp −
ŷπ(l)j
=
cj
K −1
(l)
i∈Zj

(24)


1 (l)
1
ŷ (l)
|Z | exp −
cj j
K − 1 πj

=


(25)


exp

1
(l)
ŷ
K −1 j




= exp



1
ŷ (l) exp
K − 1 πj



1
(l)
w
K −1 j
(26)

1 (l)
|Z |
(27)
cj j


(l)
1
(l)
exp K−1
ŷj
|Zj |

=
Pj (l) = P
(k)
K
1
|Zj |
k=1 exp K−1 ŷj

(17)

=

meaning that the prediction associated to any object reaching node j is the weight of j plus the prediction associated
to its parent πj . With ŷ(π1 ) = 0, the prediction of the root’s
pseudo-parent.

(28)
3.1. Regression
In regression, the tree prediction T rj of any leaf j is the
average of the learning set’s outputs reaching that node:

where cj =

Q

K
k=1

(k)

αj

 K1

is a constant. The first equal(l)

ity is a consequence of the value of wj (Equation 16). The



Globally Induced Forest
(l)

second is a due to the definition of αj (Equation 15). The
third is a consequence of having a single tree: the prediction of the parent is the same for all instances.
Notice that, in both regression and classification, the equivalence also holds for an internal node: the prediction is the
one the tree would have yielded if that node had been a leaf.

4. Datasets
Table 1 sums up the main characteristics of the datasets
we used. Abalone, CT slice, California data housig (Cadata), Musk2, Vowel and Letter come from the UCI Machine Learning Repository (Blake & Merz, 1998). Ringnorm, Twonorm and Waveform are described in (Breiman
et al., 1998). Hwang F5 comes from the DELVE repository
1
. The noise parameter of the Friedman1 dataset (Friedman, 1991) has been set to 1. Hastie is described in (Friedman et al., 2001). Out of the 500 features of Madelon
(Guyon et al., 2004), 20 are informative and 50 are redundant; the others are noise. Mnist8vs9 is the Mnist dataset
(LeCun et al., 1998) of which only the 8 and 9 digits have
been kept. Binary versions of the Mnist, Letter and Vowel
datasets have been created as well by grouping the first half
and second half classes together.

Table 1. Characteristics of the datasets. N is the learning sample
size, TS stands for testing set, and p is the number of features.
DATASET
F RIEDMAN 1
A BALONE
CT SLICE
H WANG F5
C ADATA
R INGNORM
T WONORM
H ASTIE
M USK 2
M ADELON
M NIST 8 VS 9
WAVEFORM
VOWEL
M NIST
L ETTER

N
300
2506
2000
2000
12384
300
300
2000
2000
2200
11800
3500
495
50000
16000

|T S|
2000
1671
51500
11600
8256
7100
7100
10000
4598
2200
1983
1500
495
10000
4000

p
10
10
385
2
8
20
10
10
166
500
784
40
10
784
8

# CLASSES
2
2
2
2
2
2
3
11
10
26

5. Comparison with local baseline algorithms
We have tested three deepening algorithm for decision forest relying on non-global metrics, meaning that the choice
of the best candidate is not made according to how well the
forest, as a whole, performs. These algorithms share that
the final model is exactly a sub-forest of the un-pruned forest: contrary to GIF, no internal weights are fitted and the
predictions of at the leaves are the usual tree predictions.
1

http://www.cs.utoronto.ca/delve

Breadth first deepening This variant consist in adding
the nodes level after level, from left to right, producing a
heaped forest. As a consequence, all trees have the same
(order of) height, implying that the forest can be quite wide
but usually shallow.
Random deepening This variant consist in first choosing
a tree and then choosing one of its leaves to transform to a
decision nodes. Both choices are made uniformly at random so that the trees are expected to have approximately
the same number of nodes. The depth, however, might vary
significantly.
Best first deepening This variant consist in choosing,
among all leaves which could be turned into a internal
node, the one which reduces its local impurity the most.
Let Nc , Nl and Nr be the number of instances reaching
the candidate node, candidate left child and candidate right
child respectively. Let also Ic , Il and Ir be the impurity
(gini index in classification, variance in regression) of the
instances reaching the candidate node, candidate left child
and candidate right child respectively. Then, for N learning
instances, the local impurity reduction is defined as:




Nl
Nr
Nc
Ic −
Il +
Ir
∆Ic ,
N
Nc
Nc

(29)

Since the fraction of learning instances reaching the candidate is accounted for in the reduction of impurity, this
approach will naturally favor higher nodes in the trees.
Experiment We conducted the same experiment as for
GIF: the three algorithms were tested on ten folds with different learning sample/testing sample splits and were subjected to the 1% and 10% node constraints. We started with
a pool of T = 1000 roots and no restriction was imposed
regarding the depth. All of the m = p the features were
√
examined in regression and m = p in classification, as
suggested in (Geurts et al., 2006). Table 2 holds the average mean square error for the five regression problems
and Table 3 holds the average misclassification rate for the
classification problems.
Regression The trend is quite clear: both at 1% and 10%,
the breadth first algorithm is the best and the best first is
(largely) the worst. There are two instances where the local
baselines are able to beat GIF: on Abalone and Hwang F5
at 10%. Interestingly, these are the same cases on which
GIF was beaten by a small forest of Extremely randomized trees. The 10% Hwang F5 case aside, the local baselines always underperform the smaller fully-developed forest. Overall, such variants do not seem adequate for regression.

Globally Induced Forest
Table 2. Average mean square error for local baselines at 1% and 10% budgets (T = 1000, m = p).
DATASET
F RIEDMAN 1
A BALONE
CT SLICE
H WANG F5 ×10−2
C ADATA ×10−2

B READTH F IRST10%
6.02 ± 0.28
4.72 ± 0.23
30.39 ± 1.90
6.73 ± 0.07
29.24 ± 0.73

R ANDOM10%
6.80 ± 0.34
4.77 ± 0.23
36.19 ± 1.84
6.83 ± 0.06
31.08 ± 0.74

B EST F IRST10%
15.00 ± 0.39
6.82 ± 0.33
310.87 ± 4.79
56.57 ± 6.03
75.23 ± 0.95

Table 3. Error rate (%) for local baselines at 1% and 10% budgets (T = 1000, m =
The last three are multiclass. The three in the middle are their binary versions.
DATASET
R INGNORM
T WONORM
H ASTIE
M USK 2
M ADELON
M NIST 8 VS 9
B IN . VOWEL
B IN . M NIST
B IN . L ETTER
WAVEFORM
VOWEL
M NIST
L ETTER

B READTH F IRST10%
4.25 ± 1.24
3.51 ± 0.26
11.30 ± 1.20
7.01 ± 0.40
11.68 ± 0.67
2.20 ± 0.38
8.99 ± 1.96
4.46 ± 0.25
5.91 ± 0.43
14.74 ± 0.63
14.26 ± 2.41
4.63 ± 0.27
7.06 ± 0.29

R ANDOM10%
4.08 ± 1.12
3.53 ± 0.30
11.18 ± 1.16
7.63 ± 0.43
11.92 ± 0.65
2.37 ± 0.39
8.85 ± 2.03
4.91 ± 0.27
5.71 ± 0.40
14.83 ± 0.76
13.21 ± 2.33
4.96 ± 0.26
6.39 ± 0.20

B EST F IRST10%
8.38 ± 6.94
5.59 ± 1.85
21.24 ± 7.11
15.42 ± 0.23
19.12 ± 1.94
6.17 ± 0.73
16.57 ± 3.02
21.71 ± 0.30
26.16 ± 0.86
20.25 ± 2.22
41.49 ± 5.45
28.54 ± 0.59
36.92 ± 1.80

Classification In classification, the breadth first and random baselines tend to perform similarly, one beating the
other on some problems. Once again, the best first approach seems to be lagging behind on some datasets. At
10%, the local baselines cannot rival with the other methods. Only on Waveform are they able to reach the other
performances—a setting where all methods seems to produce close results. At 1%, the breadth first and/or the
random methods surpass the ET10% on Twonorm, Hastie,
Madelon and Waveform. Those datasets correspond to
cases where ET was under-performing significantly compared to GIF. All in all, the local baselines are never able
to beat GIF, even in the multiclass setting, which is particularly defavorable for GIF. Once again, the conclusion is
against the purely local baselines.
We believed the poor performances of the baselines are due
to the building mechanism of traditional ensemble methods. Although the trees are built independently and with
randomization, there remains an important redundancy between them, which is especially defavorable to pruning. A
global approach is better able to avoid redundancy and can
thus better exploit the node budget. This would also explain
why the best first variant performs worst in both regression
and classification: it is prone at picking redundant nodes,
which will usually offer the same kind of impurity reduction.

B READTH F IRST1%
11.73 ± 0.46
5.42 ± 0.27
82.19 ± 2.41
8.52 ± 0.24
43.40 ± 1.18

R ANDOM1%
12.52 ± 0.47
5.55 ± 0.27
97.24 ± 1.90
13.17 ± 0.44
47.47 ± 1.02

B EST F IRST1%
15.29 ± 0.42
6.82 ± 0.33
313.84 ± 4.64
56.60 ± 6.07
75.48 ± 0.95

√
p). The six first datasets are binary classification.

B READTH F IRST1%
8.94 ± 7.45
5.91 ± 3.03
13.92 ± 2.93
15.42 ± 0.23
16.26 ± 0.97
4.53 ± 0.48
18.73 ± 3.08
10.09 ± 0.25
17.91 ± 0.77
16.75 ± 1.26
42.40 ± 4.33
8.60 ± 0.35
22.11 ± 0.59

R ANDOM1%
8.53 ± 7.04
6.52 ± 4.28
14.29 ± 3.20
15.42 ± 0.23
16.70 ± 1.07
4.84 ± 0.51
19.90 ± 3.71
11.78 ± 0.32
18.05 ± 0.78
17.13 ± 1.25
40.28 ± 4.62
9.76 ± 0.31
20.90 ± 0.55

B EST F IRST1%
8.94 ± 7.41
7.28 ± 4.34
21.24 ± 7.12
15.42 ± 0.23
20.14 ± 2.41
6.67 ± 0.69
21.80 ± 4.38
22.50 ± 0.35
26.19 ± 0.88
20.45 ± 2.21
50.44 ± 5.81
29.72 ± 0.61
37.27 ± 1.78

References
Blake, Catherine and Merz, Christopher J. {UCI} repository of machine learning databases. 1998.
Breiman, Leo et al. Arcing classifier (with discussion and a
rejoinder by the author). The annals of statistics, 26(3):
801–849, 1998.
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
The elements of statistical learning, volume 1. Springer
series in statistics Springer, Berlin, 2001.
Friedman, Jerome H. Multivariate adaptive regression
splines. The annals of statistics, pp. 1–67, 1991.
Geurts, Pierre, Ernst, Damien, and Wehenkel, Louis. Extremely randomized trees. Machine learning, 63(1):3–
42, 2006.
Guyon, Isabelle, Gunn, Steve R, Ben-Hur, Asa, and Dror,
Gideon. Result analysis of the nips 2003 feature selection challenge. In NIPS, volume 4, pp. 545–552, 2004.
LeCun, Yann, Bottou, Léon, Bengio, Yoshua, and Haffner,
Patrick. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–
2324, 1998.
Zhu, Ji, Zou, Hui, Rosset, Saharon, and Hastie, Trevor.
Multi-class adaboost. Statistics and its Interface, 2(3):
349–360, 2009.

Globally Induced Forest

1	  
1	  

2	  

2	  

4	  
4	  

5	  

6	  

7	  

8	  

11	  

12	  

13	  

14	  

15	  

16	  

17	  

18	  

5	  

6	  

7	  

19	  

20	  

11	  

12	  

13	  

14	  

15	  

16	  

17	  

18	  

19	  

20	  

21	  

ŷ ( x ) = y + λ w9 z9 (x)

Hypothe=cal	  unpruned	  trees	  

ŷ ( x ) = y + λ w9 z9 (x)

Hypothe=cal	  unpruned	  trees	  

9	  

21	  

Node	  belonging	  to	  the	  model	  
Node	  belonging	  to	  the	  model	  

8	  

9	  
10	  

10	  

3	  

3	  

Candidate	  node	  
Randomly	  preselected	  
candidate	  node	  

Candidate	  node	  

(b) A subset of candidates Ct is drawn uniformely at random from
the set of candidates C (step 8)

(a) Current forest at time t

1	  

2	  

3	  

1	  

2	  

3	  

Chosen	  node	  

4	  

5	  
Δerr	  

10	  

11	  

12	  

13	  

6	  

7	  

2.6	  

1.5	  

14	  

15	  

16	  

8	  

4	  

9	  

5	  
Δerr	  

17	  

18	  

19	  

20	  

21	  

10	  

11	  

12	  

13	  

6	  

7	  

2.6	  

1.5	  

14	  

15	  

16	  

8	  

17	  

18	  

9	  

19	  

20	  

0.9	  

Node	  belonging	  to	  the	  model	  

Node	  belonging	  to	  the	  model	  

ŷ ( x ) = y + λ w9 z9 (x)

Hypothe=cal	  unpruned	  trees	  

ŷ ( x ) = y + λ w9 z9 (x)

Hypothe=cal	  unpruned	  trees	  

Candidate	  node	  

Candidate	  node	  

Randomly	  preselected	  
candidate	  node	  

Randomly	  preselected	  
candidate	  node	  

(c) The error reduction is computed for all candidates of Ct (step 9)

(d) The best node (highest error reduction) is selected (step 9)

1	  
1	  

2	  

2	  

5	  

7	  

6	  

8	  

11	  

12	  

13	  

14	  

15	  

16	  

17	  

18	  

5	  

19	  

20	  

Hypothe=cal	  unpruned	  trees	  

8	  

9	  

11	  

12	  

13	  

14	  

15	  

16	  

17	  

18	  

19	  

20	  

21	  

21	  

Node	  belonging	  to	  the	  model	  
Node	  belonging	  to	  the	  model	  

7	  

6	  

9	  
10	  

10	  

3	  

3	  
4	  

4	  

21	  

0.9	  

ŷ ( x ) = y + λ w9 z9 (x) + λ w6 z6 (x)

Hypothe=cal	  unpruned	  trees	  

ŷ ( x ) = y + λ w9 z9 (x) + λ w6 z6 (x)

Candidate	  node	  

Candidate	  node	  

(e) The chosen node is introduced in the model (step 10)

(f) The children of the chosen node are computed (step 11) and
added to the candidate list (step 12)

Figure 1. Illustration of the GIF regression building algorithm (T = 3, CW = 3)

