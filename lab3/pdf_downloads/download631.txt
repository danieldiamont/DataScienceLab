Batched High-dimensional Bayesian Optimization
via Structural Kernel Learning (Appendix)

Zi Wang * 1 Chengtao Li * 1 Stefanie Jegelka 1 Pushmeet Kohli 2

1. Add-UCB-DPP-BBO Algorithm

pling in each iteration
of Gibbs sampling after the burn-in
P
i<j≤D

We present four variants of Add-UCB-DPP-BBO in Algorithm 1. The algorithm framework is general in that, one
can plug in other acquisition and quality functions other
than UCB to get different algorithms.

1zg ≡zg ∧zi ≡zj

i
j
P
period, namely,
. The third quantity,
i<j≤D 1zi ≡zj
reported in Table 3, is the probability of two dimensions
being correctly separated by Gibbs sampling in each iteration
of Gibbs sampling after the burn-in period, namely,
P

1zg 6=zg ∧zi 6=zj
i
j
.
i<j≤D 1zi 6=zj

i<j≤D

2. Additional experiments
In this section, we provide more details in our experiments.
2.1. Optimization of the Acquisition Functions
We decompose the acquisition function into M subacquisition functions, one for each part, and optimize those separately. We randomly sample 10000 points in the low dimensional space, and then choose the one with the best
value to start gradient descent in the search space (i.e. the
range of the box on R|Am | ). In practice, we observe this approach optimizes low-dimensional (< 5 dimensions) functions very well. As the number of dimensions grows, the
known difficulties of high dimensional BO (and global nonconvex optimization) arise.
2.2. Effectiveness of Decomposition Learning
Recovering Decompositions In Table 4, Table 2 and
Table 3, we show three quantities which may imply
the quality of the learned decompositions. The first
quantity , reported in Table 4, is the Rand Index of
the
by Gibbs sampling, namely,
P decompositions learned
P
i<j≤D

1zg ≡zg ∧zi ≡zj +
i

j

i<j≤D

1zg 6=zg ∧zi 6=zj

i
j
. The second
( )
quantity, reported in Table 2, is the probability of two dimensions being correctly grouped together by Gibbs samD
2

*
Equal contribution 1 Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Massachusetts, USA 2 DeepMind, London, UK. Correspondence to: Zi Wang <ziw@csail.mit.edu>, Chengtao
Li <ctli@mit.edu>, Stefanie Jegelka <stefje@csail.mit.edu>,
Pushmeet Kohli <pushmeet@google.com>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

P

Sensitivity Analysis for α Empirically, we found that the
quality of the learned decompositions is not very sensitive
to the scale of α (see Table 4), because the log data likelihood plays a much more important role than log(|Am |+α)
when α is less than the total number of dimensions. The reported results correspond to alpha = 1 for all the partitions.
BO for Synthetic Functions We show an example of a 2
dimensional function component in the additive synthetic
function in Fig. 1. Because of the numerous local maxima, it is very challenging to achieve the global optimum
even for 2 dimensions, let alone maximizing an additive
sum of them, only by observing their sum. The full results
of the simple and cumulative regrets for the synthetic functions comparing Add-GP-UCB with known additive structure (Known), no partitions (NP), fully partitioned with one
dimension for each group (FP) and the following methods
of learning partition: Gibbs sampling (Gibbs), random
sampling the same number of partitions sampled by Gibbs
and select the one with the highest data likelihood (PL-1),
random sampling 5 partitions and select the one with the
highest data likelihood (PL-2) are shown in Fig. 3. The
learning was done every 50 iterations, starting from the first
iteration. For D = 20, 30, it is quite obvious that when a
new partition is learned from the newly observed data (e.g.
at iteration 100 and 150), the simple regret gets a boost.
BO for Real-world functions In addition to be 14 parameter robot pushing task, we tested on the walker function which returns the walking speed of a three-link planar
bipedal walker implemented in Matlab (Westervelt et al.,
2007). We tune 25 parameters that may influence the walking speed, including 3 sets of 8 parameters for the ODE
solver and 1 parameter specifying the initial velocity of
the stance leg. To our knowledge, this function does not

Batched High-dimensional Bayesian Optimization

Algorithm 1 Add-UCB-DPP-BBO Variants
Input: X , Ninit , Ncyc , T , B, M
Observe function values of Ninit points chosen randomly from X
Get the initial decomposition of feature space via Gibbs sampling and get corresponding Xm ’s
for t = 1 to T do
if (t mod Ncyc = 0) then
Learn the decomposition via Gibbs sampling and get corresponding Xm ’s
end if
Choose x0 by maximizing UCB (acquisition function) for each group and combine them
for m = 1 to M do
(m)
(m)
Compute (Rt )+ and K(t−1)B+1
(m)

(m)

Sample {xi }i∈[B−1] ⊆ Xm via PE or DPP with kernel K(t−1)B+1
end for
(m)
Combine {xi }i∈[B−1],m∈[M ] either randomly or by maximizing UCB (quality function) without replacement to get
{xi }i∈[B−1]
Observe (noisy) function values for {xi } for i ∈ {0, . . . , B − 1}.
end for
Table 1. Rand Index of the decompositions computed by Gibbs sampling.

N
D
5
10
20
50
100

50

150

250

350

450

0.85 ± 0.20
0.78 ± 0.06
0.88 ± 0.02
0.95 ± 0.01
0.98 ± 0.00

0.83 ± 0.23
0.85 ± 0.08
0.88 ± 0.02
0.95 ± 0.01
0.97 ± 0.00

0.71 ± 0.18
0.86 ± 0.10
0.89 ± 0.02
0.95 ± 0.01
0.97 ± 0.00

0.68 ± 0.16
0.89 ± 0.12
0.92 ± 0.02
0.95 ± 0.01
0.97 ± 0.00

0.66 ± 0.18
0.95 ± 0.06
0.95 ± 0.04
0.95 ± 0.01
0.97 ± 0.00

Table 2. Empirical posterior of any two dimensions correctly being grouped together by Gibbs sampling.

N
dx
5
10
20
50
100

50

150

250

350

450

0.81 ± 0.28
0.21 ± 0.13
0.06 ± 0.06
0.02 ± 0.03
0.01 ± 0.01

0.91 ± 0.19
0.54 ± 0.25
0.11 ± 0.08
0.02 ± 0.02
0.01 ± 0.01

1.00 ± 0.03
0.68 ± 0.25
0.20 ± 0.12
0.03 ± 0.03
0.01 ± 0.01

0.97 ± 0.08
0.81 ± 0.27
0.43 ± 0.17
0.04 ± 0.03
0.01 ± 0.01

1.00 ± 0.00
0.93 ± 0.15
0.71 ± 0.22
0.06 ± 0.04
0.02 ± 0.02

Table 3. Empirical posterior of any two dimensions correctly being separated by Gibbs sampling.

N
dx
2
5
10
20
50
100

50

150

250

350

450

0.30 ± 0.46
0.87 ± 0.17
0.88 ± 0.05
0.94 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

0.30 ± 0.46
0.80 ± 0.27
0.89 ± 0.06
0.94 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

0.90 ± 0.30
0.60 ± 0.32
0.89 ± 0.07
0.94 ± 0.02
0.98 ± 0.01
0.99 ± 0.00

0.90 ± 0.30
0.55 ± 0.29
0.91 ± 0.08
0.95 ± 0.02
0.98 ± 0.00
0.99 ± 0.00

1.00 ± 0.00
0.50 ± 0.34
0.94 ± 0.07
0.97 ± 0.02
0.98 ± 0.01
0.99 ± 0.00

have an additive structure. The regrets of each decomposition learning methods are shown in Fig. 2. In addition
to Gibbs, we test learning decomposition via constrained
Gibbs sampling (Gibbs-L), where the maximum size of
each group of dimensions does not exceed 2. Because
the function does not have additive structure, Gibbs per-

formed poorly since it groups together many dimensions
of the input. As a result, its performance is similar to that
of no partition (NP). However, Gibbs-L appears to learn
a good decomposition with the group size limit, and manages to achieve a slightly lower regret than other methods.
Gibbs, PL-1, PL-2 and FP all performed relatively well

Batched High-dimensional Bayesian Optimization
Table 4. Rand Index of the decompositions learned by Gibbs sampling for different values of α.

N
α
0.2
0.5
1
2
5

50

150

250

350

450

0.87811 ± 0.019002
0.88211 ± 0.019893
0.88211 ± 0.016947
0.88084 ± 0.016972
0.88337 ± 0.015784

0.90126 ± 0.022394
0.90305 ± 0.024574
0.90326 ± 0.024935
0.9 ± 0.023489
0.90158 ± 0.02203

0.95284 ± 0.047111
0.95295 ± 0.046232
0.95305 ± 0.043878
0.95463 ± 0.042968
0.96126 ± 0.037045

0.98811 ± 0.02602
0.98947 ± 0.025872
0.98558 ± 0.034779
0.97989 ± 0.038818
0.98716 ± 0.030949

0.98811 ± 0.026322
0.99505 ± 0.013881
0.98053 ± 0.035843
0.98832 ± 0.023592
0.99316 ± 0.015491

f(m) (x)

10
5
0
-5
1

4.5
NP
FP
PL-1
PL-2
Gibbs
Gibbs-L

1
0.5

0.5
0

0

3.5

x1

Figure 1. An example of a 2 dimensional function component of
the synthetic function.

3

rt

x2

4

2.5
2

in for this function, indicating that using the additive structure may benefit the BO procedure even if the function itself is not additive.

1.5
1
100

200

300

400

500

t
2.3. Diverse Batch Sampling
In Fig. 4, we show the full results of the simple and the
cumulative regrets on the synthetic functions described in
Section 5.2 of the paper.

References
Westervelt, Eric R, Grizzle, Jessy W, Chevallereau, Christine, Choi, Jun Ho, and Morris, Benjamin. Feedback
control of dynamic bipedal robot locomotion, volume 28.
CRC press, 2007.

Figure 2. Simple regret of tuning the 25 parameters for optimizing the walking speed of a bipedal robot. We use the vanilla
Gibbs sampling algorithm (Gibbs) and a Gibbs sampling algorithm with partition size limit set to be 2 (Gibbs-L) to compare
with partial learning (PL-1, PL-2), no partitions (NP), and fully
partitioned (FP). Gibbs-L performed slightly better than PL-2
and FP. This function does not have an additive structure, and as
a result, Gibbs does not perform well for this function because
the sizes of the groups it learned tend to be large .

Batched High-dimensional Bayesian Optimization

Simple Regret
0.8

Known
NP
FP
PL-1
PL-2
Gibbs

rt

0.4

20

D=10

40
Known
NP
FP
PL-1
PL-2
Gibbs

30

rt

0.6

D=5

40

Known
NP
FP
PL-1
PL-2
Gibbs

30

rt

D=2

20

0.2
10

0

100

200

300

400

0

500

100

200

300

t

100

200

300

30

500

40

D=50

120
Known
NP
FP
PL-1
PL-2
Gibbs

60

rt

40

400

t

D=30

80
Known
NP
FP
PL-1
PL-2
Gibbs

50

rt

0

500

t

D=20

60

400

Known
NP
FP
PL-1
PL-2
Gibbs

100
80

rt

-0.2

10

20

60
40

20
10
0

20
100

200

300

400

0

500

100

200

t

300

400

0

500

200

t

400

600

t

Averaged Cumulative Regret

0.8

Rt

0.6

D=5

35
Known
NP
FP
PL-1
PL-2
Gibbs

25
20

D=10

35
Known
NP
FP
PL-1
PL-2
Gibbs

30

Rt

1

Known
NP
FP
PL-1
PL-2
Gibbs

30
25

Rt

D=2

20

0.4
0.2

100

200

300

400

15

10

10

5

500

100

200

t

100

40

200

300

400

500

D=50

120

30

Known
NP
FP
PL-1
PL-2
Gibbs

100
80
60

20

20
10

5

t

Known
NP
FP
PL-1
PL-2
Gibbs

60

Rt

Rt

40

500

D=30

80
Known
NP
FP
PL-1
PL-2
Gibbs

50

400

t

D=20

60

300

Rt

0

15

100

200

300

t

400

500

0

40

100

200

300

t

400

500

20

200

400

600

t

Figure 3. The simple regrets (rt ) and the averaged cumulative regrets (Rt ) and for Known (ground truth partition is given), Gibbs
(using Gibbs sampling to learn the partition), PL-1 (randomly sample the same number of partitions sampled by Gibbs and select
the one with highest data likelihood), PL-2 (randomly sample 5 partitions and select the one with highest data likelihood), FP (fully
partitioned, each group with one dimension) and NP (no partition) on 10, 20, 50 dimensional functions. Gibbs achieved comparable
results to Known. Comparing PL-1 and PL-2 we can see that sampling more partitions did help to find a better partition. But a more
principled way of learning partition using Gibbs can achieve much better performance than PL-1 and PL-2.

Batched High-dimensional Bayesian Optimization

Simple Regret
D=2

D=5

2.5
Rand
Batch-UCB-PE
Batch-UCB-DPP
Batch-UCB-PE-Fnc
Batch-UCB-DPP-Fnc

20

25
20

r

t

15

rt

1.5

30

15

rt

2

D=10

25

1

10

0.5

5

10

0

5

0
20

40

60

80

100

0
20

40

60

t

80

100

20

40

t

D=20

D=30

50

60

80

100

80

100

80

100

80

100

t

D=50

70

100

60
40

80
50

30

60

rt

t

r

r

t

40
30

20

40

20
10

20
10

0

0
20

40

60

80

100

0
20

40

60

t

80

100

20

40

t

60
t

Averaged Cumulative Regret
D=2

D=5

D=10

2.5

25

30

2

20

25

Rt

Rt
1

20
15
Rt

Rand
Batch-UCB-PE
Batch-UCB-DPP
Batch-UCB-PE-Fnc
Batch-UCB-DPP-Fnc

1.5

10

0.5

5

0

5

0
20

40

60

80

15

10

100

0
20

40

t

60

80

100

20

40

t

D=20

60
t

D=30

D=50

50

70

100

60

90

40

80

50
30
Rt

Rt

Rt

70
40

20
30
10

20

0

10
20

40

60
t

80

100

60
50
40
30

20

40

60
t

80

100

20

40

60
t

Figure 4. The simple regrets (rt ) and the averaged cumulative regrets (Rt ) on synthetic functions with various dimensions when the
ground truth partition is known. Four batch sampling methods (Batch-UCB-PE, Batch-UCB-DPP, Batch-UCB-PE-Fnc and
Batch-UCB-DPP-Fnc) perform comparably well and outperform random sampling (Rand) by a large gap.

