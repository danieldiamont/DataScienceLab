Minimax Regret Bounds for Reinforcement Learning

Appendices
We begin by introducing some notation in Sect. B and Sect. A. We then provide the full analysis of UCBVI in Sect. C.

A. Table of Notation
Symbol
S
A
πk
P
R
S
A
H
T and Tk
K
Nk (x, a)
Vh∗
T
Vk,h
Qk,h
b
L
Nk (x, a, y)
0
Nk,h
(x, a)
0
Nk,h (x)
Pbk (y|x, a)
b k,h (x, a)
V
V∗h (x, a)
b ∗ (x, a)
V
k,h
Vπh (x, a)

Explanation
The state space
The action space
The policy at episode k
The transition distribution
The reward function
Size of state space
Size of action space
The horizon length
The total number of steps and number of steps up to episode k
The total number of episodes
Number of visits to state-action pair (x, a) up to episode k
Optimal value function V ∗
Bellman operator
The estimate of value function at step h of episode k
The estimate of action value function at step h of episode k
The exploration bonus
ln(5SAT /δ)
Number of transitions from x to y upon taking action a up to episode k
Number of visits to state-action pair (x, a) at step h up to episode k
Number of visits to state x at step h up to episode k
The empirical transition distribution from x to y upon taking action a up to episode k
The empirical next-state variance of Vk,h for every (x, a)
The next-state variance of V ∗ for every state-action pair (x, a)
The empirical next-state variance of Vh∗ for every state-action pair (x, a) at episode k
π
The next-state
variance of
 Vh for every state-action pair (x, a)

1002 S 2 H 2 AL2
, H2
0 (x)
Ni,j

b0i,j (x)

min

[(x, a)]k
[k]typ
[y]k,x,a
Regret(k)
^
Regret(k)
Regret(k, x, h)
^
Regret(k,
x, h)
∆k,h
e k,h
∆
e typ,k,h
∆
Mt
εk,h and ε̄k,h
c1 (v, n), c2 (p, n) and c3 (n)
Ck,h
Bk,h
E
Ω
Ht

Set of typical state-action pairs
Set of typical episodes
Set of typical next states at every episode k for every (x, a)
The regret after k episodes
The upper-bound regret after k episodes
The regret upon encountering state x at step h after k episodes
The regret upon encountering state x at step h after k episodes
One step regret at step h of episode k
One step upper-bound regret at step h of episode k
One step upper-bound regret at step h of episode k for typical episodes
The martingale operator
Martingale difference terms
The confidence intervals for the value function and transition distribution
Sum of confidence intervals c1 up to step h of episode k
Sum of exploration bonuses b up to step h of episode k
The high probability event under which the concentration inequality holds
The high probability event under which the estimates Vk,h are ucbs
The history of all random events up to time step t

Minimax Regret Bounds for Reinforcement Learning

B. Notation
Let denote the total number of times that we visit P
state x while taking action a at step h of all episodes up to episode k by
0
0
0
Nk,h
(x, a). We also use the notation Nk,h
(x) = a∈A Nk,h
(x, a) for the total number of visits to state x at time step h
b k,h (x, a), the next-state variance of optimal value function
up to episode k. Also define the empirical next-state variance V
∗
b ∗ (x, a) and the next-state variance of V π as
Vh (x, a) the next-state empirical variance of optimal value function V
k,h
h

b k,h (x, a)
V

def

=

Vary∼Pbk (·|x,a) (Vk,h+1 (y)),

V∗h (x, a)

def

=

Vary∼P (·|x,a) (Vh∗ (y)),

b ∗ (x, a)
V
k,h

def

=

Vary∼Pbk (·|x,a) (Vh∗ (y)),

Vπh (x, a)

def

Vary∼P (·|x,a) (Vhπ (y)).

=

for every (x, a) ∈ S × A and k ∈ [K] and h ∈ [H]. We further introduce some short-hand notation: we use the lower case
to denote the functions evaluated at the current state-action pair, e.g., we write nk,h for Nk (xk,h , πk (xk,h , h)) and vk,h for
k
k
Vk,h (xk,h ). Let also denote V∗k,h = V∗k,h (xk,h , πk (xk,h , h)) and Vπk,h
= Vπk,h
(xk,h , πk (xk,h , h)) for every k ∈ [K] and

 2 2 2 2
AL
, H 2 for every x ∈ S.
h ∈ [H]. Also define b0i,j (x) = min 100N 0S H (x)
i,j+1

B.1. “Typical” state-actions and steps
In our analysis we split the episodes into 2 sets: the set of “typical” episodes in which the number of visits to the encountered state-actions are large and the rest of the episodes. We then prove a tight regret bound for the typical episodes. As the
total count of other episodes is bounded this technique provides us with the desired result. The set of typical state-actions
pairs for every episode k is defined as follows

[(x, a)]k

def

=

0
{(x, a) : (x, a) ∈ S × A, Nh (x, a) ≥ H, Nk,h
(x) ≥ H}.

Based on the definition of [(x, a)]typ we define the set of typical episodes and the set of typical state-dependent episodes as
follow

[k]typ
[k]typ,x

def

=

{i : i ∈ [k], ∀h ∈ [H], (xi,h , πi (xi,h , h)) ∈ [(x, a)]k , i ≥ 250HS 2 AL},

def

0
{i : i ∈ [k], ∀h ∈ [H], (xi,h , πi (xi,h , h)) ∈ [(x, a)]k , Nk,h
(x) ≥ 250HS 2 AL}.

=

Also for every (x, a) ∈ S × A the set of typical next states at every episode k is defined as follows

[y]k,x,a

def

=

{y : y ∈ S, Nk (x, a)P (y|x, a) ≥ 2H 2 L}.

Finally let denote [y]k,h = [y]k,xk,h ,πk (xxk,h ) for every k ∈ [K] and h ∈ [H].
B.2. Surrogate regrets
Our ultimate goal is to prove bound on the regret Regret(k). However in our analysis we mostly focus on bounding the
e k,h (x) def
surrogate regrets. Let ∆
= Vk,h (x) − V πk (x) for every x ∈ S, h ∈ [H] and k ∈ [K]. Then the upper-bound regret
h

^ defined as follows
Regret

^
Regret(k)

def

=

k
X
i=1

δei,1 .

Minimax Regret Bounds for Reinforcement Learning

^
Regret(k)
is useful in our analysis since it provides an upperbound on the true regret Regret(k). So one can bound
^
Regret(k) as a surrogate for Regret(k).
We also define the corresponding per state-step regret and upper-bound regret for every state x ∈ X and step h ∈ [H],
respectively, as follows

Regret(k, x, h)

def

=

k
X

I(xi,h = x)δi,h ,

i=1

^
Regret(k,
x, h)

def

=

k
X

I(xi,h = x)δei,h .

i=1

B.3. Martingale difference sequences
In our analysis we rely heavily on the theory of martingale sequences to prove bound on the regret incurred due to encountering a random sequence of states. We now provide some definitions and notation in that regard.
We define the following martingale operator for every k ∈ [K], h ∈ [H] and F : S → <. Also let t = (k − 1)H + h
denote the time stamp at step h of episode k then

Mt F

def

=

Phπk F − F (xk,h+1 ).

Let Ht be the history of all random events up to (and including) step h of episode k then we have that E(Mt F |Ht ) = 0.
Thus Mt F is a martingale difference w.r.t. Ht . Also let G be a real-value function depends on Ht+s for some integer
s > 0. Then we generalize our definition of operator Mt as

Mt G

def

=

E ( G(Ht+s )| Ht ) − G(Ht+s ),

where E is over the randomization of the sequence of states generated by the sequence of policies πk , πk+1 , . . . . Here also
Mt G is a martingale difference w.r.t. Ht .
Let define ∆typ,k,h : S → < as follows for every k ∈ [K] and h ∈ [H] and y ∈ S

s
e typ,k,h+1 (y)
∆

def

=

Ik,h (y) e
∆k,h+1 (y),
nk,h pk,h (y)

where the function pk,h : S → [0, 1] is defined as pk,h (y) = Phπk (y|xk,h ) and Ik,h (y) writes for Ik,h (y) = I(y ∈ [y]k,h )
for every y ∈ X . We also define the following martingale differences which we use frequently

εk,h
ε̄k,h

def

=

e k,h+1 ,
Mt ∆

def

e typ,k,h+1 .
Mt ∆

=

B.4. High probability events
We now introduce the high probability events E and Ωk,h under which the regret is small.

def
Let use the shorthand notation L = ln 5SAT
. Also for every v > 0, p ∈ [0, 1] and n > 0 let define the confidence
δ
intervals c1 , c2 and c3 , respectively, as follow

Minimax Regret Bounds for Reinforcement Learning

def

c1 (v, n)

=

def

c2 (p, n)

=

def

c3 (n)

=

r

vL 14HL
2
+
,
n
3n
r
p(1 − p)L 2L
+
,
2
n
3n
r
SL
2
.
n

Let P be the set of all probability distributions on S. Define the following confidence set for every k = 1, . . . , K, n > 0
and (x, a) ∈ S × A

P(k, h, n, x, a, y)

def

=

n



b ∗ (x, a), n
Pe(·|x, a) ∈ P : |(Pe − P )Vh∗ (x, a)| ≤ min c1 (V∗h (x, a), n) , c1 V
k,h
|Pe(y|x, a) − P (y|x, a)| ≤ c2 (P (y|x, a), n) ,
o
kPe(·|x, a) − P (·|x, a)k1 ≤ c3 (n) .

We now define the random event EPb as follows
EPb

def

=

n
o
Pbk (y|x, a) ∈ P(k, h, Nk (x, a), x, a, y), ∀k ∈ [K], ∀h ∈ [H], ∀(y, x, a) ∈ S × S × A .

Let t be a positive integer. Let F = {fs }s∈[t] be a set of real-value functions on Ht+s , for some integer s > 0. We now
define the following random events for every w̄ > 0 and ū > 0 and c̄ > 0:

Eaz (F, ū, c̄)

def

( t
X

def

( t
X

=

)
√
Ms fs ≤ 2 tū2 c̄ ,

s=1

Efr (F, w̄, ū, c̄)

=

√
14ūc̄
Ms fs ≤ 4 w̄c +
3
s=1

)
.

We also use the short-hand notation Eaz (F, ū) and Efr (F, w̄, ū) for Eaz (F, ū, L) and Efr (F, w̄, ū, L), respectively.
Now let define the following sets of random variables for every k ∈ [K] and h ∈ [H]:

F∆,k,h
e

def

0
F∆,k,h
e

def

F∆,k,h,x
e

def

0
F∆,k,h,x
e

def

GV,k,h

def

GV,k,h,x

def

Fb0 ,k,h

def

Fb0 ,k,h,x

def

=

=

=

=
=

=

n
o
e i,j : i ∈ [k], h < j ∈ [H − 1] ,
∆
n
o
e typ,i,j : i ∈ [k], h < j ∈ [H] ,
∆
n
o
e i,j I(xi,h = x) : i ∈ [k], h < j ∈ [H] ,
∆
n
o
e typ,i,j I(xi,h = x) : i ∈ [k], h < j ∈ [H] ,
∆


H
 X

Vπj i : i ∈ [k], h < j ∈ [H] ,


j=h+1


H
 X

Vπj i I(xi,h = x) : i ∈ [k], h < j ∈ [H] ,


j=h+1

=
=

 0
	
bi,j : i ∈ [k], h < j ∈ [H − 1] ,
 0
	
bi,j I(xi,h = x) : i ∈ [k], h < j ∈ [H] .

Minimax Regret Bounds for Reinforcement Learning

We now define the high probability event E as follows

E

def

=

EPb

\
\ \ 
\
√
√ \
0
0
, H) Eaz (F∆,k,x,h
, 1/ L)
Eaz (F∆,k,h
, H) Eaz (F∆,k,h
, 1/ L) Eaz (F∆,k,h,x
e
e
e
e
k∈[K]
h∈[H]
x∈S

\

Efr (GV,k,h , H 4 T, H 3 )

\

0
Eaz (GV,k,h,x , H 5 Nk,h
(x), H 3 )

\

Eaz (Fb0 ,k,h , H 2 )

\


Eaz (Fb0 ,k,h,x , H 2 ) .

The following lemma shows that the event E holds with high probability:
Lemma 1. Let δ > 0 be a real scalar. Then the event E holds w.p. at least 1 − δ.
Proof. To prove this result we need to show that a set of concentration inequalities with regard to the empirical model Pbk
holds simultaneously. For every h ∈ [H] the Bernstein inequality combined with a union bound argument, to take into
account that Nk (x, a) ∈ [T ] is a random number, leads to the following inequality w.p. 1 − δ (see, e.g., Cesa-Bianchi &
Lugosi, 2006; Bubeck & Cesa-Bianchi, 2012, for the statement of the Bernstein inequality and the application of the union
bound in similar cases, respectively.)

s
h

i


 (P − Pbk )Vh∗ (x, a) ≤

2V∗h (x, a) ln
Nk (x, a)

2T
δ




2H ln 2T
δ
+
,
3Nk (x, a)

(9)

where we rely on the fact that Vh∗ is uniformly bounded by H. Using the same argument but this time with the Empirical
Bernstein inequality (see, e.g., Maurer & Pontil, 2009), for Nk (x, a) > 1, leads to

s
h

i


 (P − Pbk )Vh∗ (x, a) ≤

b ∗ (x, a) ln
2V
k,h
Nk (x, a)

2T
δ




7H ln 2T
δ
+
.
3Nk (x, a)

(10)

The Bernstein inequality combined with a union bound argument on Nk (x, a) also implies the following bound w.p. 1 − δ

s
|Nk (y, x, a) − Nk (x, a)P (y|x, a)| ≤


2Nk (x, a)Varz∼P (·|x,a) (1(z = y)) ln

2T
δ


+

2T
δ

2 ln
3


,

which implies the following bound w.p. 1 − δ:

s


b

Pk (y|x, a) − P (y|x, a) ≤

P (y|x, a)(1 − P (y|x, a)) ln
Nk (x, a)

2T
δ




2 ln 2T
δ
+
.
3Nk (x, a)

(11)

A similar result holds on `1 -normed estimation error of the transition distribution. The result of (Weissman et al., 2003)
combined with a union bound on Nk (x, a) ∈ [T ] implies w.p. 1 − δ

s


b

Pk (·|x, a) − P (·|x, a)

1

≤


2S ln 2T
δ
.
Nk (x, a)

(12)

We now focus on bounding the sequence of martingales. Let n > 0 be an integer and u, δ > 0 be some real scalars. Let the
sequence of random variables {X1 , X2 , . . . , Xn } be a sequence of martingale differences w.r.t. to some filtration Fn . Let

Minimax Regret Bounds for Reinforcement Learning

this sequence be uniformly bounded from above and below by u. Then the Azuma’s inequality (see, e.g., Cesa-Bianchi &
Lugosi, 2006) implies that w.p. 1 − δ

n
X

s
Xi

≤

2nu ln

i=1

When the sum of the variances
(1975) holds w.p. 1 − δ

Pn

i=1

 
1
.
δ

(13)

Var(Xi |Fi ) ≤ w for some w > 0 then the following sharper bound due to Freedman

n
X

s
Xi

≤

i=1

 
2u ln
1
+
2w ln
δ
3

1
δ


.

(14)

Let k ∈ [K], h ∈ [H] and x ∈ X . Then the inequality of Eq. 13 immediately implies that the following events holds w.p.
1 − δ:


Eaz F∆,k,h
, H, ln (1/δ) ,
e


√
0
Eaz F∆,k,h
, 1/ L, ln (1/δ) ,
e

Eaz Fb0 ,k,h , H 2 , ln (1/δ) .

(15)
(16)
(17)

0
Also Eq. 13 combined with a union bound argument over all Nk,h
(x) ∈ [T ] (see, e.g., Bubeck et al., 2011, for the full
description of the application of union bound argument in the case of martigale process with random stopping time) implies
that the following events hold w.p. 1 − δ



Eaz F∆,k,h,x
, H, ln (T /δ) ,
e


√
0
Eaz F∆,k,h,x
, 1/ L, ln (T /δ) ,
e

Eaz Fb0 ,k,h,x , H 2 , ln (T /δ) .

(18)
(19)
(20)

Similarly the inequality of Eq. 14 leads to the following events hold w.p. 1 − δ


Efr GV,k,h , w̄k,h , , H 3 , ln (T /δ) ,
3

(21)


Efr GV,k,h,x , w̄k,h,x , , H , ln (1/δ) ,

(22)

where w̄k,h and w̄k,h,x are upper bounds on Wk,h and Wk,h,x , respectively, defined as

Wk,h

Wk,h,x

def

=

def

=





Var 
Vπi,j+1  Hi,1  ,

i=1
j=h




k
H−1
X
X

I(xi,h = x)E 
Vπi,j+1  Hi,1  .

i=1
j=h
k
X



H−1
X

(23)

(24)

So to establish a value for w̄k,h and w̄k,h,x we need to prove bound on Wk,h and Wk,h,x . Here we only prove this bound
for Wk,h as the proof techniques to bound Wk,h,x is identical to the way we bound Wk,h .

Minimax Regret Bounds for Reinforcement Learning

Wk,h ≤

k
X


E

i=1

H−1
X
j=h




2



k
H−1
X
X


k
k
 Hk  .
 Hk  ≤ H 3
E
Vπi,j+1
Vπi,j+1




i=1
j=h

(25)

Now let the sequence {x1 , x2 , . . . , xH } be the sequence of states encountered by following some policy π throughout an
episode k. Then the recursive application of LTV leads to (see e.g., Munos & Moore, 1999; Lattimore & Hutter, 2012, for
the proof.)



H−1
X

E


π

V (xj , π(xj , j))


=

H−1
X

Var 

j=h


π

r (xj ) .

(26)

j=h

By combining Eq. 26 into Eq. 25 we deduce

Wk,h

≤

H3

k
X


Var 

i=1

H−1
X
j=h





rk,h  Hk  ≤ H 5 k = H 4 Tk .


(27)

Similarly the following bound holds on Wk,h,x

Wk,h,x

≤ H 5 Nk,h (x).

(28)

Plugging the bounds of Eq. 27 and Eq. 28 in to the bounds of Eq. 21 and Eq. 22 and a union bound over all Nk,h (x) ∈ [T ]
leads to the following events hold w.p. 1 − δ:


Efr GV,k,h , H 4 T, H 3 , ln (1/δ) ,

(29)

5

(30)

Efr

3


GV,k,h,x , H Nk,h (x), H , ln (T /δ) .

Combining the results of Eq. 9, Eq. 10, Eq. 11, Eq. 12, Eq. 15, Eq. 16 Eq. 17, Eq. 18, Eq. 19, Eq. 20, Eq. 29 and Eq. 30
and taking a union bound over these random events as well as all possible k ∈ [K], h ∈ [H] and (s, a) ∈ S × A proves the
result.

B.4.1. UCB E VENTS
Let k ∈ [K] and h ∈ [H]. Denote the set of steps for which the value functions are obtained before Vk,h as

[k, h]hist = {(i, j) : i ∈ [K], j ∈ [H], i < k ∨ (i = k ∧ j > h)}.
Let Ωk,h = {Vi,j ≥ Vh∗ , ∀(i, j) ∈ [k, h]hist } be the event under which Vi,j prior to Vk,h computation are upper bounds on
the optimal value functions. Using backward induction on h (and standard concentration inequalities) we will prove that
Ωk,h holds under the event E (see Lem. 19).

Minimax Regret Bounds for Reinforcement Learning

B.5. Other useful notation
Here we define some other notation that we use throughout the proof. We denote the total count of steps up to episode
def
k ∈ [K] by Tk = H(k − 1). We first define c4,k,h , for every h ∈ [H] and k ∈ [K], as follow

c4,k,h =

4H 2 SAL
.
nk,h

for every k ∈ [K] , h ∈ [H] and x ∈ [x] we also introduce the following notation which we use later when we sum up the
regret:

Ck,h

def

=

k
X

I(i ∈ [k]typ )

i=1

Bk,h

def

=

k
X
i=1

Ck,h,x

def

=

k
X

H−1
X

c1,i,j ,

j=h

I(i ∈ [k]typ )

H−1
X

bi,j ,

j=h

I(i ∈ [k]typ,x , xk,h = x)

i=1

Bk,h,x

def

=

k
X
i=1

H−1
X

c1,i,j ,

j=h

I(i ∈ [k]typ,x , xk,h = x)

H−1
X

bi,j ,

j=h

∗
where c1,k,h is the shorthand-notation for c1 (vk,h
, nk,h ). We also define the upper bound Uk,h and Uk,h,x for every k ∈ [K]
, h ∈ [H] and x ∈ S as follows, respectively

Uk,h

def

=

e

k H−1
X
X

[bi,j + c1,i,j + c4,i,j ] + (H + 1)

p

Tk L,

i=1 j=h

Uk,h,x

def

=

e

k H−1
X
X

[bi,j + c1,i,j + c4,i,j ] + (H + 1)3/2

q

0 (x)L,
Nk,h

i=1 j=h

C. Proof of the Regret Bounds
Before we start the main analysis we state the following useful lemma that will be used frequently in the analysis:
Lemma 2. let X ∈ R and Y ∈ R be two random variables. Then following bound holds for their variances

Var(X) ≤ 2 [Var(Y ) + Var(X − Y )] .
Proof. The following sequence of inequalities hold

Var(X) = E(X − Y − E(X − Y ) + Y − E(Y ))2 ≤ 2E(X − Y − E(X − Y ))2 + 2E(Y − E(Y ))2 .
The result follows from the definition of variance.
We proceed by proving the following key lemma which shows that proves bound on ∆k,h under the assumption that Vk,h
is UCB w.r.t. Vh∗ .

Minimax Regret Bounds for Reinforcement Learning

Lemma 3. Let k ∈ [K] and h ∈ [H]. Let the events E and Ωk,h hold. Then the following bound holds on δk,h and δek,h :

δk,h ≤ δek,h ≤ e

H−1
Xh

i
√
εk,i + 2 Lε̄k,i + c1,k,i + bk,i + c4,k,i .

(31)

i=h

Proof. For the ease of exposition we abuse the notation and drop the dependencies on k, e.g., we write x1 , π and V1 for
xk,1 , πk and Vk,1 , respectively. We proceed by bounding δeh under the event E at every step 0 < h < H:

δeh

π
= Th Vh+1 (xh ) − Thπ Vh+1
(xh )
π
π
= [Pbh Vh+1 ](xh ) + bh − [Phπ Vh+1
](xh )
π
π
∗
∗
π
= bh + [(Pbh − Ph )Vh+1 ](xh ) + [(Pbhπ − Phπ )(Vh+1 − Vh+1
)](xh ) + [Phπ (Vh+1 − Vh+1
)](xh )

≤

∗
δeh+1 + εh + bh + c1,h + [(Pbhπ − Phπ )(Vh+1 − Vh+1
)](xh ),
{z
}
|

(32)

(a)

∗
where the last inequality follows from the fact that under the event E we have that [(Pbhπ − Phπ )Vh+1
](xh ) ≤ c1,h . We now
bound (a):

(a)

X

=

∗
(Pbhπ (y|xh ) − Phπ (y|xh ))(Vh+1 (y) − Vh+1
(y))

y∈S

 s


p
(y)(1
−
p
(y))L
4L
h
h
2
 ∆h+1 (y)
+
nh
3nh
y∈S
s
√ X ph (y)
e h+1 (y) + 4SHL ,
∆
2 L
nh
3nh
y∈S
|
{z
}

(I)

X

≤

≤

(b)

where (I) holds under the event E. We proceed by bounding (b):
s
(b)

X

=

y∈[y]h

|

X
ph (y) e
∆h+1 (y) +
nh
y ∈[y]
/ h
{z
} |

s

(c)

ph (y) e
∆h+1 (y) .
nh
{z
}

(33)

(d)

The term (c) can be bounded as follows
s
(c)

=

X

Phπ (y|xh )

y∈[y]h

r
≤ ε̄h +

1
e h+1 (y) = ε̄h +
∆
nh ph (y)

s

1
I(xh+1 ∈ [y]h )δeh+1
nh ph (xh+1 )

1 e
δh+1 ,
4LH 2

(34)

where in the last line we rely on the definition of [y]h . We now bound (d):
s
(d) =

X
y ∈[y]
/ h

√
ph (y)nh e
SH 4LH 2
∆h+1 (y) ≤
.
n2h
nh

(35)

Minimax Regret Bounds for Reinforcement Learning

By combining Eq. 34 and Eq. 35 into Eq. 33 we deduce
r
√
SH 4LH 2
1 e
(b) ≤
δh+1 + ε̄h .
+
nh
4LH 2

(36)

By combining Eq. 36 and Eq. 33 into Eq. C we deduce


√
1 e
δeh ≤ εh + 2 Lε̄h + bh + c1,h + c4,h + 1 +
δh+1 .
H
Let denote γh = (1 + 1/H)h . The previous bound combined with an induction argument implies that

≤

δeh

PH−1
i=h

h
i
√
γi−h εi + 2 Lε̄i + c1,i + c4,i + bi .

The inequality ln(1 + x) ≤ x for every x > −1 leads to γh ≤ γH ≤ e for every h ∈ [H]. This combined with the
assumption that vh ≥ vh∗ under the event Ωh completes the proof.
Lemma 4. Let k ∈ [k] and h ∈ [H]. Let the events E and Ωk,h hold. Then
k−1
X
i=1

δi,h ≤

k−1
X

δei,h ≤ e

i=1

k−1
Xh
X H−1

i
√
εi,j + 2 Lε̄i,j + bi,j + c1,i,j + c4,i,j .

i=1 j=h

Proof. The proof follows by summing up the bounds of Lem. 3 and taking into acoount the fact if Ωk,h holds then Ωi,j for
all (i, j) ∈ [k, h]hist hold.
To simplify the bound of Lem. 4 we prove bound on sum of the martingales εk,h and ε̄k,h
Lemma 5. Let k ∈ [k] and h ∈ [H]. Let the events E and Ωk,h hold. Then the following bound holds
k H−1
X
X

εi,j

≤ H

ε̄i,j

≤

p
p
(H − h)kL ≤ H Tk L,

(37)

i=1 j=h
k H−1
X
X

p

(H − h)k ≤

p

Tk .

(38)

i=1 j=h

Also the following bounds holds for every x ∈ X and h ∈ H:
k
X

I(xi,h = x)

i=1
k
X
i=1

H−1
X

q
0 (x)L,
(H − h)Nk,h

εi,j

≤

H

ε̄i,j

≤

q
0 (x).
(H − h)Nk,h

(39)

j=h

I(xi,h = x)

H−1
X

(40)

j=h

0
Proof. The fact that the event E holds implies that the events Eaz (F∆,k,h
, H), Eaz (F∆,k,h
, √1L ) , Eaz (F∆,k,h,x
, H) and
e
e
e

0
Eaz (F∆,x,k,h
, √1L ) hold. Under these events the inequalities of the statement hold. This combined with the fact that
e
(H − h)k ≤ Tk completes the proof.

Minimax Regret Bounds for Reinforcement Learning

We now bound the sum of δs in terms of the upper-bound U :
Lemma 6. Let k ∈ [K] and h ∈ [H]. Let the events E and Ωk,h holds. Then the following bounds hold for every h ∈ [H]
x∈S

k
X

k
X

≤

δi,h

i=1
k
X

δei,h ≤ Uk,h ≤ Uk,1 ,

i=1
k
X

≤

I(xi,h = x)δi,h

i=1

I(xi,h = x)δei,h ≤ Uk,h,x . ≤ Uk,1,x .

i=1

Proof. The proof follows by incorporating the result of Lem. 5 into Lem. 4 and taking into account that for every h ∈ [H]
the term Uk,h (Uk,h,x ) is a summation of non-negative terms which are also contained in Uk,1 (U1,h,x ).
Lemma 7. Let k ∈ [K] and h ∈ [H]. Let the events E and Ωk,h holds. Then the following bounds hold for every x ∈ S

k X
H
X

≤

δi,j

i=1 j=h
k
X

I(xi,h = x)

i=1

H
X

k X
H
X

δei,j ≤ HUk,1 ,

i=1 j=h

≤

δi,j

k
X

I(xi,h = x)

i=1

j=h

H
X

δei,j ≤ HUk,1,x .

j=h

Proof. The proof follows by summing up the bounds of Lem. 6.
We now focus on bounding the terms Ck,h (Ck,h,x ) and Bk,h (Bk,h,x ) in Lem. 11 and Lem. 12, respectively. Before we
proceed with the proof of Lem. 11 and Lem. 12. we prove the following key result which bounds sum of the variances of
π
Vk,h
using an LTV argument:
Lemma 8. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S

k H−1
X
X

≤ Tk H + 2

Vπi,j+1

0
≤ Nk,h
(x)H 2 + 2

i=1 j=h
k
X

I(xi,h = x)

i=1

H−1
X

p
4H 3 L
H 4 Tk L +
,
3

Vπi,j+1

j=h

q
3
0 (x)L + 4H L .
H 5 Nk,h
3

Proof. Under E the events Efr (GV,k,h , H 4 Tk , H 3 ) and Efr (GV,k,h,x , H 5 Nk,h (x), H 3 ) hold which then imply:

k H−1
X
X

Vπi,j+1

≤

Vπi,j+1

≤

i=1 j=h
k
X
i=1

I(xi,h = x)

H−1
X
j=h

The LTV argument of Eq. 26 then leads to




p

4H 3 L
E
Vπi,j+1  Hk,h  + 2 H 4 Tk L +
,
3

i=1
j=h




k
H−1
q
3
X
X

0 L + 4H L .
I(xi,h = x)E 
Vπi,j+1  Hk,h  + 2 H 5 Nk,h
3

i=1
j=h
k
X



H−1
X

(41)

(42)

Minimax Regret Bounds for Reinforcement Learning





E
Vπi,j+1  Hi,h 

i=1
j=h




H−1
X

= x)E 
Vπi,j+1  Hi,h 

j=h
k
X

k
X

I(xi,h

i=1



H−1
X

k
X

=


Var 

i=1
k
X

=



H
X

π 
ri,j
≤ KH 2 = T H,

(43)

j=h+1


I(xi,h = x)Var 

i=1



H
X

π 
0
ri,j
≤ Nk,h
(x)H 2 .

(44)

j=h+1

Eq. 41 and Eq. 42 combined with Eq. 43 and Eq. 44, respectively, complete the proof.

Lemma 9. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S
k H−1
X
X

p

V∗i,j+1 − Vπi,j+1



≤ 2H 2 Uk,h + 4H 2

V∗i,j+1 − Vπi,j+1



≤ 2H 2 Uk,h,x + 4H 2

Tk L,

(45)

i=1 j=h
k
X

I(xi,h = x)

i=1

H−1
X

q

0 (x, a)L.
HNk,h

(46)

j=h

Proof. We begin by the following sequence of inequalities:

k H−1
X
X

V∗i,j+1 − Vπi,j+1

(I)

≤

k H−1
X
X

h
Ey∼pi,j

2
2 i
πi
∗
Vi,j+1
(y) − Vi,j+1
(y)

i=1 j=h

i=1 j=h

=

k H−1
X
X

 ∗

πi
πi
∗
Ey∼pi,j (Vj+1
(y) − Vj+1
(y)(Vj+1
(y) + Vj+1
(y))

i=1 j=h

≤

2H

k H−1
X
X


πi
∗
Ey∼pi,j Vj+1
(y) − Vj+1
(y) ,

(47)

i=1 j=h

|

{z

}

(a)

∗
π
where (I) is obtained from the definition of the variance as well as the fact that Vi,j
≥ Vk,h
. The last line also follows from
πk
∗
the fact that V ≤ Vh ≤ H.

Using an identical argument we can also prove the following bound for state-dependent difference:

k
X
i=1

I(xi,h = x)

H−1
X

V∗i,j+1 − Vπi,j+1

≤ 2H

k
X

I(xi,h = x)

i=1

j=h

H−1
X


πi
∗
Ey∼pi,j Vj+1
(y) − Vj+1
(y) ,

(48)

j=h

|

{z

(b)

}

To bound (a) we use the fact that under the event E the event Eaz (F∆,k,h
, H) also holds. This combined with the fact that
e
e
under the event Ωk,h the inequality δk,h ≤ δk,h holds implies that

(a) ≤

k H−1
X
X

δei,j+1 + 2H

p

Tk L

i=1 j=h

≤

HU1,h + 2H

p

Tk L,

(49)

Minimax Regret Bounds for Reinforcement Learning

where in the last line we rely on the result of Lem. 7. Similarly we can prove the following bound for (b) under the events
Ωk,h and Eaz (F∆,k,h,x
, H):
e

(b) ≤

k
X

I(xi,h = x)

i=1

≤

H−1
X

e i,j+1 + 2H 1.5
∆

q
0 (x)L
Nk,h

j=h

HUk,h,x + 2H 1.5

q

0 (x)L.
Nk,h

(50)

The result then follows by incorporating the results of Eq. 49 and Eq. 50 into Eq. 47 and Eq. 48, respectively.

Lemma 10. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S

k H−1
X
X

b i,j+1 − Vπ
V
i,j+1

≤ 2H 2 Uk,1 + 15H 2 S

b i,j+1 − Vπ
V
i,j+1

≤

p
ATk L,

(51)

i=1 j=h
k
X
i=1

I(xi,h = x)

H−1
X

2H 2 Uk,h,x + 15H 2 S

q

0 (x)L.
HANk,h

(52)

j=h

Proof. Here we only prove the bound on Eq. 51. The proof for the bound of Eq. 52 can be done in a very similar manner,
0
as it is shown in the previous lemmas (the only difference is that HNk,h
(x) and Uk,h,x replace Tk and Uk,1 , respectively).
The following sequence of inequalities hold:

k H−1
X
X
i=1 j=h

b i,j+1 − Vπ
V
i,j+1

(I)

≤

k H−1
X
X

2
2
πi
Ey∼bpi,j (Vi,j+1 (y)) − Ey∼pi,j Vj+1
(y)

i=1 j=h

+

k H−1
X
X

2
2
∗
∗
Ey∼pi,j Vj+1
(y) − Ey∼bpi,j Vj+1
(y)

i=1 j=h
(II)

≤

k H−1
X
X

2

Ey∼bpi,j (Vi,j+1 (y)) −

i=1 j=h

k H−1
X
X

2

Ey∼pi,j (Vi,j+1 (y))

j=1

|
+

H−1
X

{z

}

(a)

h
2 i
2
πi
Ey∼pi,j (Vi,j+1 (y)) − Vj+1
(y)

i=1 j=h

|
+

k H−1
X
X

{z

(b)

s
4H

i=1 j=h

|

{z

(c)

2

}

L
,
nk,h
}

where (I) holds due to the fact that under Ωk,h , Vi,j ≥ Vj∗ ≥ Vjπi and (II) holds under the event E.
We now bound (a):

(53)

Minimax Regret Bounds for Reinforcement Learning

k H−1
X
X

(I)

≤

(a)

s
2H 2

i=1 j=h
(II)

≤

3H 2 S

p

SL
nk,h

ATk L,

where (I) holds under the event E and (II) holds due to the pigeon-hole argument (see, e.g., Jaksch et al., 2010, for the
proof).
Using an identical analysis to the one in Lem. 10 and taking into account that Vi,j ≥ Vj∗ under the event Ωk,h and E we
can bound (b)


(I)

(b) ≤ 2H 

k H−1
X
X


p
δei,j+1 + 2H Tk L)



p
≤ 2H 2 Uk,1 + 2 Tk L ,

i=1 j=h

where (I) holds since under the event E the event Eaz (F∆,k,h
, H) holds. Another application of pigeon-hole principle
e
√
2
leads to a bound of 6H SATk L on (c). We then combine this with the bounds on (a) and (b) to bound Eq. 53, which
proves the result.

We now bound Ck,h and Ck,h,x :
Lemma 11. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S

q
p
4 HSATk + 4 H 2 Uk,1 SAL2 ,
q
q
≤ 4 H 2 SANk,h (x) + 4 H 2 Uk,h,x SAL2 .

≤

Ck,h
Ck,h,x

(54)
(55)

Proof. Here we only prove the bound on Eq. 54. The proof for the bound of Eq. 55 can be done in a very similar manner,
0
as it is shown in the previous lemmas (the only difference is that HNk,h
(x) and Uk,h,x replace Tk and Uk,1 , respectively).
The Cauchy–Schwarz inequality leads to the following sequence of inequalities:

Ck,h

=

k
X



H−1
X

I(j ∈ [k]typ ) 

i=1

s
2

j=h


V∗i,j+1 L 4HL

+
ni,j
3ni,j

v
v
u k H−1
u k
k
H−1
H−1
X
X 1
X
X 4HL
uX
√ uX
∗
u
≤ 2 Lu
Vi,j+1 u
I(i
∈
[k]
)
+
I(i
∈
[k]
)
typ
typ
u
ni,j
3ni,j
u i=1 j=h
u i=1
i=1
j=h
h=j
t|
{z
}t|
{z
}
(a)

(56)

(b)

We now prove bounds on (a) and (b) respectively

(a) =

k H−1
X
X

Vπi,j+1

i=1 j=h

|

{z

(c)

+

k H−1
X
X

V∗i,j+1 − Vπi,j+1 .

i=1 j=h

}

|

{z

(d)

}

(c) and (d) can be bounded under the events E and Ωk,h using the results of Lem. 8 and Lem.9. We then deduce

(57)

Minimax Regret Bounds for Reinforcement Learning

(a) ≤
≤

HTk + 2H 2 Uk,1 + 6H 2

p

Tk L +

4H 2 L
3

2HTk + 2H 2 Uk,1 ,

where the last line follows by the fact that for the typical episodes Tk ≥ 250H 2 S 2 AL2 . Thus if Tk ≤ H 2 L the term Ck,h
trivially equals to 0 otherwise the higher order terms are bounded by O(HTk ).
We now bound (b) using a pigeon-hole argument

Nk (x,a)

X

X

(x,a)∈S×A

n=1

(b) ≤ 2

T
X
1
1
≤ 2SA
≤ 2SA ln(3T ).
n
n
n=1

Plugging the bound on (a) and (b) into Eq. 56 and taking in to account that for the typical episodes [k]typ we have that
T ≥ H 2 L completes the proof.

We now bound Bk,h :
Lemma 12. Let k ∈ [K] and h ∈ [H]. Let the bonus is defined according to Algo. 4. Then under the events E and Ωk,h
the following hold for every x ∈ S,

Bk,h
Bk,h,x

q
p
≤ 11L Tk HSA + 12 H 2 SAL2 Uk,1 + 570H 2 S 2 AL2 ,
q
q
2 2
2
0 (x)HSA + 12 H 2 SAL2 U
≤ 11L Nk,h
k,h,x + 570H S AL ,

(58)
(59)

Proof. Here we only prove the bound on Eq. 58. The proof for the bound of Eq. 59 can be done in a very similar manner,
0
as it is shown in the previous lemmas (the only difference is that HNk,h
(x) and Uk,h,x replace Tk and Uk,1 , respectively).
We first notice that the following holds:

Bk,h ≤

k
X
i=1

|

I(i ∈ [k]typ )

H−1
X
j=h

{z

(a)

s

v
u
k
H−1
X
X u 8 X
b i,j+1 L
8V
t

+L
I(i ∈ [k]typ )
pbi,j (y) min
ni,j
ni,j
i=1
y∈S
j=h
}
{z
|
(b)

!
1002 S 2 H 2 AL2
, H2  .
0
Ni,j+1
(y)
}

We first note that the bound on Bk,h is similar to the bound on Ck,h . The main difference (beside the difference in H.O.Ts)
b i,j+1 . So in our proof we first focus on dealing with this difference.
is that here V∗h+1 is replaced by V
The Cauchy–Schwarz inequality leads to:

(a) ≤

√

v
v
u k H−1
u k
H−1
X 1
uX X
uX
b i,j+1 (x, a)u
8Lu
V
I(k
∈
[k]
)
,
typ
u
u
ni,j
u i=1 j=h
u i=1
j=h
t|
{z
}t|
{z
}
(c)

(d)

Minimax Regret Bounds for Reinforcement Learning

The bound on (d) is identical to the corresponding bound in Lem. 11. So we only focus on bounding (c):

(c) =

k H−1
X
X

Vπi,j+1 +

i=1 j=h

{z

|

(e)

k H−1
X
X

b i,j+1 − Vπ
V
i,j+1 .

(60)

i=1 j=h

}

|

{z

}

(f )

(e) and (f ) can be bounded in high probability using the results of Lem. 8 and Lem.10. This implies

(c) ≤ HTk + 3H 2 Uk,1 + 15H 2 S

p

ATk L +

4H 2 L
3

≤ 2HTk + 3H 2 Uk,1 ,
where the last line follows by the fact that for the typical episodes Tk ≥ 250H 2 S 2 AL. Thus if Tk ≤ 250H 2 S 2 L then
Bk,h trivially equals to 0 otherwise the higher order terms are bounded by O(HT ). Combining the bound on (b) and (c)
leads to the following bound on (a):

p
p
(a) ≤ 8L HSATk + 12HL SAUk,1 .
To bound (b) we make use of Cauchy-Schwarz inequality again.
v
u k
H−1
k H−1
XX
X
X I(i ∈ [k]typ )
u X
0
8
I(i
∈
[k]
)
p
b
(y)b
(y)
(b) ≤ u
.
typ
i,j
i,j+1
u
ni,j
u i=1
i=1
y∈S
j=h
j=h
t |
{z
}|
{z
}
(g)

(h)

The term (h) bounded by 2SAL using a pigeon-hole argument (see Lem. 11). We proceed by bounding (g):

(g) ≤

H−1
k H−1
k
k H−1
X
X
X
X
X
X
(pi,j b0V − b0i,j+1 (xi,j+1 )) +
b0i,j+1 (xi,j+1 ) .
I(i ∈ [k]typ )
(b
pi,j − pi,j )b0i,j+1 +
i=1 j=h

i=1 j=h

|

{z

(i)

}

|

{z

(j)

}

i=1

j=h

|

{z
k

}

√
√
Given that the event E holds the term (i) bounded by 2 2H 2 S ALTk by using the pigeon-hole
√ argument. Under the
event E the event Eaz (Fb0 ,k,h , H 2 ) holds. This implies that the term (j) is also bounded by 2H 2 Tk L as it is sum of the
martingale differences. The term (k) is also bounded by 20000H 3 S 3 A3 L3 using the pigeon-hole argument. Combining
all these bounds together leads to the following bound on (b)

(b) ≤

q √
p
p
32 2H 2 S 2 Tk AL3 + 32H 2 SA Tk L3 + 320000S 4 H 4 A2 L3 .

Combining this with the bound on (a) and taking into account the fact that we only bound the Bk,h for the typical episodes,
in which Tk ≥ 250H 2 S 2 AL2 , completes the proof.

Lemma 13. Let the bonus is defined according to Algo. 4. Then under the events E and ΩK,1 the following hold
√
√
p
^
Regret(K) ≤ Regret(K)
≤ UK,1 ≤ 15L HSAT + 16HL SAUk,1 + 820H 2 S 2 A2 L + 2H T L.

(61)

Minimax Regret Bounds for Reinforcement Learning

Proof. We first notice that Regret(K) and Regret(K) are bounded by Uk,1 due to Lem.6. To bound Uk,1 we sum up the
PK PH
regret due to Bk,h and Ck,h from Lem. 11 and Lem. 12. We also bound the sum k=1 h=1 c4,k,h by 2HSAL using
a pigeon hole argument. We also note that Bk,h and Ck,h only account for the regret of typical episodes in which T ≥
H 2 S 2 A2 L. The regret of those episodes which do not belong to the typical set [k]typ , can be bounded by O(H 2 S 2 A2 L2 ),
trivially.
The following lemma establishes an explicit bound on the regret:
Lemma 14. Let the bonus is defined according to Algo. 4. Then under the events E and ΩK,1 the following hold
√
√
^
Regret(K) ≤ Regret(K)
≤ UK,1 ≤ 30L HSAT + 2500H 2 S 2 AL2 + 4H T L.

(62)

Proof. The proof follows by solving the bound of Lem. 13 in terms of Uk,1 . which only contributes to the additional regret
of O(H 2 L2 SA).
Lemma 15. Let the bonus is defined according to Algo. 3. Then under the events E and ΩK,1 the following holds
√
^
Regret(K) ≤ Regret(K)
≤ UK,1 ≤ 20HL SAT + 250H 2 S 2 AL2 .

(63)

Proof. The proof up to Lem. 11 is identical
q to the proof of Lem. 14. The main difference is to prove bound on Ck,h and

Bk,h here we use a loose bound of O(H SAL
nk,h ) for both exploration bonus bk,h and the confidence interval c1,k,h and
then sum√these terms using a pigeon-hole argument (The proof is provided in Jaksch et al., 2010) which leads to a bound
of O(H SAT L) on both BK,1 and CK,1 . Plugging these results into the bound of Lem. 7 combined with the regret of
non-typical episodes complete the proof

Lemma 16. Let the bonus is defined according to Algo. 4. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h
the following hold for every x ∈ S,

q
q
0 (x) + 2500H 2 S 2 AL2 + 4H 1.5 N 0 (x)L
^
Regret(k, x, h) ≤ Regret(k,
x, h) ≤ 30HL SANk,h
k,h
q
0 (s).
≤ 100H 1.5 SL ANk,h
Proof. The proof is similar to the proof of total regret. Here also we use Lem. 12, Lem. 11 and a pigeon-hole argument to
bound the regrets due to Bk,h , Ck,h and c4,k,h . We then incorporate these terms into Lem.6 to bound the regret in terms of
Uk,h,x . The result follows by solving the bound w.r.t. the upper bound Uk,h,x .
Lemma 17. Let the bonus b is defined according to Algo. 4. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h
the following hold for every x ∈ S

s
Vk,h (x) −

Proof. From Lem. 16 we have that

Vh∗ (x)

≤ 100

H 3 S 2 AL2
0 (s) .
Nk,h

Minimax Regret Bounds for Reinforcement Learning

q
0 (s)
100H 1.5 SL ANk,h
≥

k
X

I(xi,h = x)(Vi,h (x) − Vhπi (x))

i=1

≥ (Vk,h (x) − Vh∗ (x))

k
X

0
I(xi,h = x) = Nk,h
(x)(Vk,h (x) − Vh∗ (x)),

i=1

where the last inequality holds due to the fact that Vk,h by definition is monotonically non-increasing in k. The proof then
follows by collecting terms.

Lemma 18. Let the bonus b is defined according to Algo. 3. Then under the event E the set of events {Ωk,h }k∈[K],h∈H
hold.
Proof. We prove this result by induction. First we notice that for h = H by definition Vk,h = Vh∗ thus the inequality
Vk,h ≥ Vh∗ trivially holds. Thus to prove this result for h < H we only need to show that if the inequality Vk,h ≥ Vh∗
holds for h it also holds for h − 1 for every h < H:

Vk,h (x) − Vh∗ (x)

∗
π∗
= Tk Vk,h−1 (x) − T V ∗ (x) ≥ bk (x, πh∗ (x)) + Pbk,h
Vk,h+1 (x) − Phπ Vk,h+1 (x)
∗

∗

∗

π
∗
π
∗
= bk (x, πh∗ (x)) + Pbk,h
(Vk,h+1 − Vh+1
)(x) + (Pbk,h
− Phπ )Vh+1
(x)
∗
π∗
∗
(x),
≥ bk (x, πh∗ (x)) + (Pbk,h
− Phπ )Vh+1

∗
. The fact that the event E hols implies that
where the last line follows by the induction condition that Vk,h+1 ≥ Vh+1
∗
∗
π
π
∗
∗
∗
b
(Ph − Pk,h )Vh+1 (x) ≤ c1 (Nk (x, πh (x))) ≤ bk (x, πh (x)), which completes the proof.

Lemma 19. Let the bonus b is defined according to Algo. 4. Then under the event E the set of events {Ωk,h }k∈[K],h∈H
hold.
Proof. We prove this result by induction. We first notice that in the case of the first episode V1,h = H ≥ Vh∗ .
To prove this result by induction in the case of 1 < k ∈ [K] we need to show that in the case of h ∈ [H − 1] if Ωk,h+1
holds then Ωk,h also holds.
If Ωk,h−1 holds then Vi,j ≥ Vj∗ for every (i, j) ∈ [k, h]hist . We can then invoke the result of Lem. 17 which implies

Vk,h+1 (x) −

∗
Vh+1
(x)

≤

√
100H 1.5 SL A
q
.
0
Nk,h+1
(x)

∗
Using this result which guarantees that Vk,h+1 is close to Vh+1
we prove that Vk,h − Vh∗ ≥ 0, that is the event Ωk,h holds.

Vk,h − Vh∗ = min(Vk−1,h , Tk,h Vi,j+1 , H) − Vh∗
If Vk−1,h ≤ Tk,h Vi,j+1 the result Vk,h − Vh∗ = Vk−1,h − Vh∗ ≥ 0 holds trivially. Also if Vk−1,h ≥ H the result trivially
holds. So we only need to consider the case that Tk,h Vi,j+1 ≤ Vk−1,h ≤ H in that case we have w

Minimax Regret Bounds for Reinforcement Learning

Vk,h (x) − Vh∗ (x)

∗
Tk,h Vi,j+1 (x) − T Vh+1
(x)

≥
(I)

∗
∗
bk,h (x, π ∗ (x, h)) + Pbhπ Vi,j+1 (x) − Phπ∗ Vh+1
(x)
∗
∗
∗
π
π
∗
∗
b
b
bk,h (x, π (x, h)) + (Ph − Ph )Vh+1 (x) + Phπ∗ (Vi,j+1 − Vh+1
)(x)

≥
=
(II)

∗
∗
∗
bk,h (x, π ∗ (x, h)) + (Pbhπ − Phπ )Vh+1
(x),

≥

where in (I) we rely on the fact that πk,h is the greedy policy w.r.t. Vk,h . Thus

∗

bk,h (x, π ∗ (x, h)) + Pbhπ Vi,j+1 (x) ≤ bk,h (x, πk (x, h)) + Pbhπk Vi,j+1 (x).
Also (II) follows from the induction assumption. Under the event E we have

Vk,h − Vh∗

b ∗ , Nk )
≥ bk,h − c1 (V
h
s
s
b ∗ L 14L
b k,h L
V
8V
h
−
−2
≥
Nk
Nk
3Nk
{z
}
|
(a)

v h

i
u
u Pbk 8 min 1002 H03 S 2 A2 L2 , H 2
t
Nk,h+1
14L
+
+
.
Nk
3Nk
We now prove a lower bound on (a):

 s
b ∗ − 8V
b k,h

4V

k,h
−
(a) ≥
Nk


0

b k,h ≤ V∗ ,
V
otherwise.

b ∗ in terms of V
b k,h from above:
We proceed by bounding V
k,h
b∗
V
k,h

(I)

≤

b k,h + 2Var b (Vk,h+1 (y) − V ∗ (y)) ≤ 2V
b k,h + 2 Pbk (Vk,h+1 − V ∗ )2 ,
2V
h+1
h+1
y∼Pk
|
{z
}
(b)

where (I) is an application of Lem. 2. We now bound (b). Combining this result with the result of Eq. 64 leads to the
following bound on (a)

 v
h
 2 3 2 2
i
u

100 H S AL
b

, H2
0
 u
t 8Pk min
Nk,h+1
(a) ≥ −
Nk



0

b k,h ≤ V
b ∗,
V
otherwise,

where the last inequality holds under the event E. The proof is completed by plugging (a) and (b) into Eq. 64 which proves
that Vk,h ≥ Vh∗ thus the event Ωk,h holds.

