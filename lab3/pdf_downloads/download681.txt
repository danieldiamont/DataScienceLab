Latent Feature Lasso

7. Appendix

Minimize w.r.t Î·j gives

7.1. Comparison on Time Complexity

min F (c + Î·j ej ) âˆ’ F (c)
Î·j

The proposed LatentLasso algorithm runs significantly
faster than other methods in our experiments. For example, on the Syn1 dataset (N=1000, D=1000, K=35), the
runtime of LatentLasso is 398s, while MCMC, Variational,
MF-Binary and BP-Means all take more than 10000s to obtain their best results reported in the Figures (and the implementation of Spectral Method we obtained from the authors has memory requirement that restricts KÂ¡14). On the
real data sets, we report only up to K=50 because most of
the compared methods already took one day to train.
The complexity of each algorithm can be summarized in
Table 2. The reason for the smaller runtime of LatentLasso
is due to the decoupling of factor N D from the factor related to K, where the factor O(N D) comes from the cost
of solving a MAX-CUT-like problem using the method of
(Boumal et al., 2016) or (Wang & Kolter, 2016), while the
factor O(K 2 D) comes from the cost of solving a leastsquare problem given by (11) with the maintenance cost
of Z T Z amortized.
7.2. Proof for Theorem 1
Let L(M ) be a smooth function such that âˆ‡L(M ) is
Lipschitz-continuous with parameter Î², that is,

â‰¤ min Âµâˆ‡j âˆ— f (c)Î·j + Î»|Î·j | +
Î·j

= min

Î·k :kâˆˆA
/

Î³ 2
Î·
2 j
!2


X
Î³
Âµâˆ‡k f (c)Î·k + Î»|Î·k | +
2

X

kâˆˆA
/

X

â‰¤ min Âµ
Î·k :kâˆˆA
/

kâˆˆA
/

+ (1 âˆ’ Âµ)Î»

X

|Î·k |

kâˆˆA
/

!2



Î³
âˆ‡k f (c)Î·k + Î»|Î·k | +
2

X

|Î·k |

kâˆˆA
/

|Î·k |

kâˆˆA
/

where the last equality is justified later in Lemma 1. For
k âˆˆ A, we have
X
(âˆ‡k f (c)Î·k + Î»|ck + Î·k | âˆ’ Î»|ck |)
0 = min Âµ
Î·k :kâˆˆA

kâˆˆA

Combining cases for k âˆˆ
/ A and k âˆˆ A, we can obtain a
global estimate of descent amount compared to some optimal solution xâˆ— as follows
min F (c + Î·jÌ‚ ejÌ‚ ) âˆ’ F (c)
Î·jÌ‚


â‰¤ min Âµ hâˆ‡f (c), Î·i + Î»kc + Î·k1 âˆ’ Î»kck1
Î·

Î²
L(M 0 ) âˆ’ L(M ) âˆ’ hâˆ‡L(M ), M 0 âˆ’ M i â‰¤ kM 0 âˆ’ M k2F .
2
Then
âˆ‡j f (c) = zjT âˆ‡L(M )zj

is Lipschitz-continuous with parameter Î³, which is of order O(1) when loss function L(.) is an empirical average
normalized by N D.
Let A be the active set before adding jÌ‚. Consider the descent amount produced by minimizing F (c) w.r.t. the cjÌ‚
given that 0 âˆˆ âˆ‚j F (c) for all j âˆˆ A due to the subproblem
solved in the previous iteration. Let j = jÌ‚, for any Î·j we
have
Î³
F (c + Î·j ej ) âˆ’ F (c) â‰¤ âˆ‡j f (c)Î·j + Î»|Î·j | + Î·j2
2
Î³
âˆ—
â‰¤ Âµâˆ‡j f (c)Î·j + Î»|Î·j | + Î·j2
2

Î³
+
2

!2
X

|Î·k |

+ (1 âˆ’ Âµ)Î»

kâˆˆA
/

X

|Î·k |

kâˆˆA
/



Î³
â‰¤ min Âµ F (c + Î·) âˆ’ F (c) +
Î·
2
X
+ (1 âˆ’ Âµ)Î»
|Î·k |

!2
X

|Î·k |

kâˆˆA
/

kâˆˆA
/



Î±Î³ âˆ— 2
âˆ—
â‰¤ min Âµ F (c + Î±(c âˆ’ c)) âˆ’ F (c) +
kc k1
2
Î±âˆˆ[0,1]
+ Î±(1 âˆ’ Âµ)Î»kcâˆ— k1


Î±2 Î³ âˆ— 2
âˆ—
kc k1
â‰¤ min âˆ’Î±Âµ F (c) âˆ’ F (c ) +
2
Î±âˆˆ[0,1]
+ Î±(1 âˆ’ Âµ)Î»kcâˆ— k1 .
It means we can always choose an Î± small enough to guarantee descent if
F (c) âˆ’ F (câˆ— ) >

(1 âˆ’ Âµ)
Î»kcâˆ— k1 .
Âµ

(23)

2(1 âˆ’ Âµ)
Î»kcâˆ— k1 ,
Âµ

(24)

In addition, for
F (c) âˆ’ F (câˆ— ) â‰¥

Latent Feature Lasso

Table 2: Comparison of Time Complexity. (T denotes number of iterations)
Methods
Time Complexity

MCMC
(N K 2 D)T

Variational
(N K 2 D)T

MF-Binary
(N K)2K

min F (c + Î·jÌ‚ ejÌ‚ ) âˆ’ F (c)
Î·jÌ‚

Î±âˆˆ[0,1]



Î±Âµ
Î±2 Î³ âˆ— 2
F (c) âˆ’ F (câˆ— ) +
kc k1 .
2
2

Minimizing w.r.t. to Î± gives the convergence guarantee
F (ct ) âˆ’ F (câˆ— ) â‰¤

2Î³kcâˆ— k21 1
.
Âµ2
t

kâˆˆA
/

(26)
Proof. The minimization (34) is equivalent to

X
min
Âµâˆ‡k f (c)Î·k
Î·k :kâˆˆA
/

F (0) âˆ’ F (câˆ— ) = F (0) âˆ’ F (câˆ— ) âˆ’ h, 0 âˆ’ câˆ— i
Î²
â‰¥ kcâˆ— âˆ’ 0k22 ,
2

kcâˆ— k22 â‰¤

X

s.t.

|Î·k |

â‰¤ C1

kâˆˆA
/

X

|Î·k | â‰¤ C2

kâˆˆA
/

Combining above with the fact for any c, kck21 â‰¤ kck0 kck22 ,
we obtain the result.
Since F (0) âˆ’ F (câˆ— ) â‰¤
(1) and (27), we have

1
2N

PN

i=1

yi2 â‰¤ 1 , from Theorem

s
 
1
2(1 âˆ’ Âµ)Î» 2kcâˆ— k0
+
.
T
Âµ
Î²
(28)
for any câˆ— := arg minc:supp(c)=Aâˆ— F (c).
4Î³kcâˆ— k0
F (cT ) âˆ’ F (câˆ— ) â‰¤
Î²Âµ2

7.4. Proof of Theorem 3
Before delving into the analysis of the Latent Feature Lasso
method, we first investigate what one can achieve in terms
of the risk defined in (1) if the combinatorial version of
objective is solved. Let

and therefore is equivalent to
X
min
Âµ
âˆ‡k f (c)Î·k
s.t.

2(F (0) âˆ’ F (câˆ— ))
.
Î²

kâˆˆA
/

!2

Î·k :kâˆˆA
/

LatentLasso
(N D + K 2 D)T

which gives us

for any iterate with F (ct ) âˆ’ F (câˆ— ) â‰¥ 2(1âˆ’Âµ)
Î»kcâˆ— k1 .
Âµ
Lemma 1.
Î³
(25)
min Âµâˆ‡j âˆ— f (c)Î·j + Î»|Î·j | + Î·j2
Î·j
2
!2

X
Î³ X
= min
Âµâˆ‡k f (c)Î·k + Î»|Î·k | +
|Î·k |
2
Î·k :kâˆˆA
/
kâˆˆA
/

Spectral
N D + K 5 log(K)

Proof. Since supp(câˆ— ) = Aâˆ— , and câˆ— is optimal when
restricted on the support, we have h, câˆ— i = 0 for some
âˆˆ âˆ‚F (câˆ— ). And since F (c) is strongly convex on the support Aâˆ— with parameter Î², we have

we have

â‰¤ min âˆ’

BP-Means
(N K 3 D)T

f (x; W ) :=

min

zâˆˆ{0,1}K

1
kx âˆ’ W T zk2 .
2

kâˆˆA
/

X

p
|Î·k | â‰¤ min{ C1 , C2 }

kâˆˆA
/

Suppose we can obtain solution WÌ‚ to the following empirical risk minimization problem:
N
1 X
f (xi ; W ).
N i=1

which is a linear objective subject to a convex set and thus
always has solution that lies on the corner point with only
one non-zero coordinate Î·j âˆ— , which then gives the same
minimum as (33).

Then the following theorem holds.

7.3. Proof of Theorem 2

Theorem 7. Let W âˆ— be the minimizer of risk (1) and WÌ‚ be
the empirical risk minimizer (29). Then

Lemma 2. Let Aâˆ— âˆˆ [KÌ„] be a support set and câˆ— :=
arg minc:supp(c)=Aâˆ— F (câˆ— ). Suppose F (c) is strongly convex on Aâˆ— with parameter Î². We have
s
2kcâˆ— k0 (F (0) âˆ’ F (câˆ— ))
.
(27)
kcâˆ— k1 â‰¤
Î²

WÌ‚ :=

argmin
W âˆˆRKÃ—D :kW kF â‰¤R

E[f (x; WÌ‚ )] âˆ’ E[f (x; W âˆ— )]
s
3
DK log(4R2 KN )
1
1
â‰¤
+
+
log( )
N
2N
2N
Ï
with probability 1 âˆ’ Ï.

(29)

Latent Feature Lasso

Proof Sketch. Let EN [f (x, W )] denote the empirical risk.
We have

erations of the greedy algorithm, we have
EN [f (x, Dc W )] +

âˆ—

E[f (x; WÌ‚ )] âˆ’ E[f (x; W )]
!
â‰¤2

|E[f (x; W )] âˆ’ EN [f (x; W )]|

sup
W âˆˆRKÃ—D :kW kF â‰¤R

(30)
from error decomposition and EN [f (x, WÌ‚ )]
â‰¤
EN [f (x, W âˆ— )].
Then by introducing a Î´-net N (Î´)
DK
with covering number |N (Î´)| = 4R
, we have
Î´
kWÌƒ âˆ’ W kF â‰¤ Î´ for some WÌƒ âˆˆ N (Î´) and
P





sup E[f (x; WÌƒ )] âˆ’ EN [f (x; WÌƒ )] â‰¤ 

WÌƒ âˆˆN (Î´)


â‰¥1âˆ’

4R
Î´

DK

!
(31)

=

Ï„
kW k2F + Î»kck1
2

N
Ï„
1 X
min kxi âˆ’ W T DcT zk2 + kW k2F + Î»kck1
2N i=1 zâˆˆ{0,1}kck0
2

â‰¤ F (c)
(34)
Combining (33), (34) and (28), we obtain a bound on the
bias and optimization error of the Latent Feature Lasso estimator
EN [f (x, Dc W )] â‰¤ F (c) â‰¤ EN [f (x; W âˆ— )]
  s
2Î³K
1
Ï„
2(1 âˆ’ Âµ)K
+
Î»
+ kW âˆ— k2 + Î»K +
2
Î²
T
ÂµÎ²
|
{z
} |
{z
}
regularize bias

exp(âˆ’2N 2 ).

Then since

optimization error

(35)
To bound the estimation error, notice that the matrix WÌ‚ :=
Dc W is KÌ‚ Ã— D with KÌ‚ â‰¤ T . Furthermore, the descent
condition F (c) â‰¤ F (0) guarantees that

2(f (x, WÌƒ ) âˆ’ f (x, W )) â‰¤ kx âˆ’ WÌƒ T z âˆ— k2 âˆ’ kx âˆ’ W T z âˆ— k2

Ï„
1
kW k2F + Î»kck1 â‰¤ kX âˆ’ 0k2 â‰¤ 1
2
N

= z âˆ—T (W âˆ’ WÌƒ )x + hWÌƒ WÌƒ T âˆ’ W W T , z âˆ— z âˆ—T i

â‰¤ kz âˆ— k2 kW âˆ’ WÌƒ kF + 2RkWÌƒ âˆ’ W kF kz âˆ— k22 â‰¤ 3RKkWÌƒ âˆ’ W
kFthus
, kW k2F â‰¤ 1/Ï„ , kck1 â‰¤ 1/Î».
and
Let W(T, Î», Ï„ ) := {WÌ‚ âˆˆ (RT Ã—D ) | kWÌ‚ kF â‰¤
We have

we have






sup
E[f (x; W )] âˆ’ EN [f (x; W )]
W :kW kF â‰¤R





â‰¤ (3RKÎ´) + sup E[f (x; WÌƒ )] âˆ’ EN [f (x; WÌƒ )]
â‰¤ 3RKÎ´ +

DK
4R
1
1
log(
)+
log( )
2N
Î´
2N
Ï

(32)
with probability 1 âˆ’ Ï. Choosing Î´ = 1/(RKN ) yields the
result.

s
â‰¤

Ï„
kW âˆ— k2F + Î»kcâˆ— k1
2
(33)
F (c). Then let (c, W ) with

F (cÌ„) â‰¤ F (câˆ— ) â‰¤ EN [f (x; W âˆ— )] +
where cÌ„ âˆˆ

argmin
c:supp(c)=S âˆ—

supp(c) = SÌ‚ be the output obtained from running T it-

1
1
DT log(4T N/(Ï„ Î»))
+
log( )
2N
2N
Ï

with probability 1 âˆ’ Ï through the same argument as in
the case of combinatorial objective (32). Combining the
above estimation error with the bias and optimization error
in (35), we have
E[f (x; W )] âˆ’ E[f (x; W âˆ— )]
s
Ï„ 2
2Î³K
2(1 âˆ’ Âµ)K
â‰¤ R + Î»K +
+
Î»
2
Î²T
ÂµÎ²
s
DT log(4T N/(Ï„ Î»))
1
1
+
+
log( )
2N
2N
Ï

Now we establish the proof of Theorem (3) for bounding
risk of the Latent Feature Lasso estimator.
Proof. Let Z âˆ— âˆˆ arg minZâˆˆ{0,1}N K N1 kX âˆ’ZW âˆ— k2F and
S âˆ— be the set of column index of Z with the same 0-1 patterns to columns in Z âˆ— . Let câˆ— be indicator vector with
câˆ—k = 1, k âˆˆ S âˆ— and câˆ—k = 0, k âˆˆ
/ S âˆ— . We have

E[f (x; WÌ‚ )] âˆ’ EN [f (x, WÌ‚ )]

sup
(c,W )âˆˆW(T,Î»,Ï„ )

WÌƒ âˆˆN (Î´)

s

p
1/(Î»Ï„ )}.

Choosing T =
DK
3

2Î³K 1
Î² (  ),

Î» = Ï„ =

âˆš1
N

and N &

DT
2

=

gives the result.

7.5. Proof of Theorem 4
Proof. Since W âˆ— is of rank K, we have span(Î˜âˆ— ) =
span(Z âˆ— ). Therefore, from condition 2,
âˆ— K
span(Î˜âˆ— ) âˆ© {0, 1}N \ {0} = {Z:,j
}j=1 .

(36)

Latent Feature Lasso

For any (Z, W ) : ZW = Î˜âˆ— , we have Z âˆˆ span(Î˜âˆ— )
since Z = Î˜âˆ— V Î£âˆ’1 U T where U Î£V T is the SVD of W
with Î£ : K Ã—K. Then by (36) we know that Z = Z âˆ— . Then
it follows W = W âˆ— since the linear system Î˜âˆ— = Z âˆ— W
has unique solution for W .

Z S WS = X = Z âˆ— W âˆ— .
Since WS has full row-rank, we have rank(ZS ) =
rank(X) = rank(Z) = K by condition 1 in Theorem 4. Then let WS = U Î£V T be the SVD of WS with
Î£ : |S| Ã— |S|, we have
ZS = XV Î£âˆ’1 U T = Z âˆ— W âˆ— V Î£âˆ’1 U T âˆˆ span(Z âˆ— ).
Then by condition 2 in Theorem 4, the columns of ZS can
âˆ— K
only be in {Z:,j
}j=1 , which implies ZS equal to Z âˆ— up to
a permutation. Then we know |S| = K and by Theorem 4
WS also equals W âˆ— up to a permutation.
7.7. Proof of Theorem 8

where kâˆ‡f (câˆ— )kâˆ is given by:

7.9. Proof of Theorem 6
Proof. Note that the optimization problem in Equation (22)
can be rewritten as:
1
argmin 2N
kE + (Z âˆ— âˆ’ Z)k22

Zâˆˆ{0,1}N

max

zâˆˆ{0,1}N

1
kAâˆ—T zk22 ,
2N 2 Ï„

(I âˆ’ ZS (ZST ZS + N Ï„ I)âˆ’1 ZST )(ZS W âˆ— + )
(37)
Given P = ZS (ZST ZS + N Ï„ I)âˆ’1 ZST , it can be seen that
Aâˆ— can be rewritten as :
=

Aâˆ— = (I âˆ’ P ) + (I âˆ’ P )(ZS W âˆ— ).

7.8. ell2 error bounds on the coefficient vector cÌ‚
Theorem 8. Let câˆ— be the true underlying vector, with support S and sparsity K âˆ— . Let cÌ‚ be the minimizer of F (c),
defined in Equation (7). Define the noise-level term

zâˆˆ{0,1}N

N
X

(38)
argmin (Ei +

i=1 Zi âˆˆ{0,1}

(Ziâˆ—

2

âˆ’ Zi ))

So, we have the following closed form expression for ZÌ‚:
(
1 if Ziâˆ— + Ei â‰¥ 0.5
ZÌ‚i =
.
0 o.w

P(Ziâˆ— 6= ZÌ‚i )

= P(Ei â‰¥ 0.5) âˆ— P(Ziâˆ— = 0)
+P(Ei â‰¤ âˆ’0.5) âˆ— P(Ziâˆ— = 1)

(I âˆ’ ZS (ZST ZS + N Ï„ I)âˆ’1 ZST )X

max

1
2N

We now compute the probability that Ziâˆ— 6= ZÌ‚i :

where Aâˆ— is defined as:

Ïn :=

where C = {c|kcS c k1 â‰¤ 3kcS k1 }. Then, if the regularization parameter is set as Î» â‰¥ Ïn , we have the following
bound on the norm of the error cÌ‚ âˆ’ câˆ— :
âˆš
Ïn K âˆ—
âˆ—
kcÌ‚ âˆ’ c k2 â‰¤
.
Îºn

=

Proof. By an application of Theorem1 of (Negahban et al.,
2009), for Î» â‰¥ kâˆ‡f (câˆ— )kâˆ , we have the following bound
on the `2 norm of cÌ‚ âˆ’ câˆ— :
âˆš
K âˆ—Î»
âˆ—
kcÌ‚ âˆ’ c k2 â‰¤
,
Îºn

=

Let Îºn be the restricted strong convexity term defined as :
âˆ†âˆˆC

Proof. The solution of (21) satisfies

Aâˆ—

P = ZS (ZST ZS + N Ï„ I)âˆ’1 ZST .

Îºn := inf {f (câˆ— + âˆ†) âˆ’ f (câˆ— ) âˆ’ hâˆ‡f (câˆ— ), âˆ†i} ,

7.6. Proof of Theorem 5

kâˆ‡f (câˆ— )kâˆ =

where Aâˆ— = (I âˆ’ P ) + (I âˆ’ P )(ZS W âˆ— ) where

1
kAâˆ—T zk22 ,
2N 2 Ï„

â‰¥ min{P(Ei â‰¥ 0.5), P(Ei â‰¤ âˆ’0.5)} â‰¥ c,
(39)
for some positive constant c. We now use the fact that
E((Ziâˆ— âˆ’ ZÌ‚i )2 ) = P(Ziâˆ— 6= ZÌ‚i ) to complete the proof of
the Lemma.

