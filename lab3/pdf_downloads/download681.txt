Latent Feature Lasso

7. Appendix

Minimize w.r.t ηj gives

7.1. Comparison on Time Complexity

min F (c + ηj ej ) − F (c)
ηj

The proposed LatentLasso algorithm runs significantly
faster than other methods in our experiments. For example, on the Syn1 dataset (N=1000, D=1000, K=35), the
runtime of LatentLasso is 398s, while MCMC, Variational,
MF-Binary and BP-Means all take more than 10000s to obtain their best results reported in the Figures (and the implementation of Spectral Method we obtained from the authors has memory requirement that restricts K¡14). On the
real data sets, we report only up to K=50 because most of
the compared methods already took one day to train.
The complexity of each algorithm can be summarized in
Table 2. The reason for the smaller runtime of LatentLasso
is due to the decoupling of factor N D from the factor related to K, where the factor O(N D) comes from the cost
of solving a MAX-CUT-like problem using the method of
(Boumal et al., 2016) or (Wang & Kolter, 2016), while the
factor O(K 2 D) comes from the cost of solving a leastsquare problem given by (11) with the maintenance cost
of Z T Z amortized.
7.2. Proof for Theorem 1
Let L(M ) be a smooth function such that ∇L(M ) is
Lipschitz-continuous with parameter β, that is,

≤ min µ∇j ∗ f (c)ηj + λ|ηj | +
ηj

= min

ηk :k∈A
/

γ 2
η
2 j
!2


X
γ
µ∇k f (c)ηk + λ|ηk | +
2

X

k∈A
/

X

≤ min µ
ηk :k∈A
/

k∈A
/

+ (1 − µ)λ

X

|ηk |

k∈A
/

!2



γ
∇k f (c)ηk + λ|ηk | +
2

X

|ηk |

k∈A
/

|ηk |

k∈A
/

where the last equality is justified later in Lemma 1. For
k ∈ A, we have
X
(∇k f (c)ηk + λ|ck + ηk | − λ|ck |)
0 = min µ
ηk :k∈A

k∈A

Combining cases for k ∈
/ A and k ∈ A, we can obtain a
global estimate of descent amount compared to some optimal solution x∗ as follows
min F (c + ηĵ eĵ ) − F (c)
ηĵ


≤ min µ h∇f (c), ηi + λkc + ηk1 − λkck1
η

β
L(M 0 ) − L(M ) − h∇L(M ), M 0 − M i ≤ kM 0 − M k2F .
2
Then
∇j f (c) = zjT ∇L(M )zj

is Lipschitz-continuous with parameter γ, which is of order O(1) when loss function L(.) is an empirical average
normalized by N D.
Let A be the active set before adding ĵ. Consider the descent amount produced by minimizing F (c) w.r.t. the cĵ
given that 0 ∈ ∂j F (c) for all j ∈ A due to the subproblem
solved in the previous iteration. Let j = ĵ, for any ηj we
have
γ
F (c + ηj ej ) − F (c) ≤ ∇j f (c)ηj + λ|ηj | + ηj2
2
γ
∗
≤ µ∇j f (c)ηj + λ|ηj | + ηj2
2

γ
+
2

!2
X

|ηk |

+ (1 − µ)λ

k∈A
/

X

|ηk |

k∈A
/



γ
≤ min µ F (c + η) − F (c) +
η
2
X
+ (1 − µ)λ
|ηk |

!2
X

|ηk |

k∈A
/

k∈A
/



αγ ∗ 2
∗
≤ min µ F (c + α(c − c)) − F (c) +
kc k1
2
α∈[0,1]
+ α(1 − µ)λkc∗ k1


α2 γ ∗ 2
∗
kc k1
≤ min −αµ F (c) − F (c ) +
2
α∈[0,1]
+ α(1 − µ)λkc∗ k1 .
It means we can always choose an α small enough to guarantee descent if
F (c) − F (c∗ ) >

(1 − µ)
λkc∗ k1 .
µ

(23)

2(1 − µ)
λkc∗ k1 ,
µ

(24)

In addition, for
F (c) − F (c∗ ) ≥

Latent Feature Lasso

Table 2: Comparison of Time Complexity. (T denotes number of iterations)
Methods
Time Complexity

MCMC
(N K 2 D)T

Variational
(N K 2 D)T

MF-Binary
(N K)2K

min F (c + ηĵ eĵ ) − F (c)
ηĵ

α∈[0,1]



αµ
α2 γ ∗ 2
F (c) − F (c∗ ) +
kc k1 .
2
2

Minimizing w.r.t. to α gives the convergence guarantee
F (ct ) − F (c∗ ) ≤

2γkc∗ k21 1
.
µ2
t

k∈A
/

(26)
Proof. The minimization (34) is equivalent to

X
min
µ∇k f (c)ηk
ηk :k∈A
/

F (0) − F (c∗ ) = F (0) − F (c∗ ) − h, 0 − c∗ i
β
≥ kc∗ − 0k22 ,
2

kc∗ k22 ≤

X

s.t.

|ηk |

≤ C1

k∈A
/

X

|ηk | ≤ C2

k∈A
/

Combining above with the fact for any c, kck21 ≤ kck0 kck22 ,
we obtain the result.
Since F (0) − F (c∗ ) ≤
(1) and (27), we have

1
2N

PN

i=1

yi2 ≤ 1 , from Theorem

s
 
1
2(1 − µ)λ 2kc∗ k0
+
.
T
µ
β
(28)
for any c∗ := arg minc:supp(c)=A∗ F (c).
4γkc∗ k0
F (cT ) − F (c∗ ) ≤
βµ2

7.4. Proof of Theorem 3
Before delving into the analysis of the Latent Feature Lasso
method, we first investigate what one can achieve in terms
of the risk defined in (1) if the combinatorial version of
objective is solved. Let

and therefore is equivalent to
X
min
µ
∇k f (c)ηk
s.t.

2(F (0) − F (c∗ ))
.
β

k∈A
/

!2

ηk :k∈A
/

LatentLasso
(N D + K 2 D)T

which gives us

for any iterate with F (ct ) − F (c∗ ) ≥ 2(1−µ)
λkc∗ k1 .
µ
Lemma 1.
γ
(25)
min µ∇j ∗ f (c)ηj + λ|ηj | + ηj2
ηj
2
!2

X
γ X
= min
µ∇k f (c)ηk + λ|ηk | +
|ηk |
2
ηk :k∈A
/
k∈A
/

Spectral
N D + K 5 log(K)

Proof. Since supp(c∗ ) = A∗ , and c∗ is optimal when
restricted on the support, we have h, c∗ i = 0 for some
∈ ∂F (c∗ ). And since F (c) is strongly convex on the support A∗ with parameter β, we have

we have

≤ min −

BP-Means
(N K 3 D)T

f (x; W ) :=

min

z∈{0,1}K

1
kx − W T zk2 .
2

k∈A
/

X

p
|ηk | ≤ min{ C1 , C2 }

k∈A
/

Suppose we can obtain solution Ŵ to the following empirical risk minimization problem:
N
1 X
f (xi ; W ).
N i=1

which is a linear objective subject to a convex set and thus
always has solution that lies on the corner point with only
one non-zero coordinate ηj ∗ , which then gives the same
minimum as (33).

Then the following theorem holds.

7.3. Proof of Theorem 2

Theorem 7. Let W ∗ be the minimizer of risk (1) and Ŵ be
the empirical risk minimizer (29). Then

Lemma 2. Let A∗ ∈ [K̄] be a support set and c∗ :=
arg minc:supp(c)=A∗ F (c∗ ). Suppose F (c) is strongly convex on A∗ with parameter β. We have
s
2kc∗ k0 (F (0) − F (c∗ ))
.
(27)
kc∗ k1 ≤
β

Ŵ :=

argmin
W ∈RK×D :kW kF ≤R

E[f (x; Ŵ )] − E[f (x; W ∗ )]
s
3
DK log(4R2 KN )
1
1
≤
+
+
log( )
N
2N
2N
ρ
with probability 1 − ρ.

(29)

Latent Feature Lasso

Proof Sketch. Let EN [f (x, W )] denote the empirical risk.
We have

erations of the greedy algorithm, we have
EN [f (x, Dc W )] +

∗

E[f (x; Ŵ )] − E[f (x; W )]
!
≤2

|E[f (x; W )] − EN [f (x; W )]|

sup
W ∈RK×D :kW kF ≤R

(30)
from error decomposition and EN [f (x, Ŵ )]
≤
EN [f (x, W ∗ )].
Then by introducing a δ-net N (δ)
DK
with covering number |N (δ)| = 4R
, we have
δ
kW̃ − W kF ≤ δ for some W̃ ∈ N (δ) and
P





sup E[f (x; W̃ )] − EN [f (x; W̃ )] ≤ 

W̃ ∈N (δ)


≥1−

4R
δ

DK

!
(31)

=

τ
kW k2F + λkck1
2

N
τ
1 X
min kxi − W T DcT zk2 + kW k2F + λkck1
2N i=1 z∈{0,1}kck0
2

≤ F (c)
(34)
Combining (33), (34) and (28), we obtain a bound on the
bias and optimization error of the Latent Feature Lasso estimator
EN [f (x, Dc W )] ≤ F (c) ≤ EN [f (x; W ∗ )]
  s
2γK
1
τ
2(1 − µ)K
+
λ
+ kW ∗ k2 + λK +
2
β
T
µβ
|
{z
} |
{z
}
regularize bias

exp(−2N 2 ).

Then since

optimization error

(35)
To bound the estimation error, notice that the matrix Ŵ :=
Dc W is K̂ × D with K̂ ≤ T . Furthermore, the descent
condition F (c) ≤ F (0) guarantees that

2(f (x, W̃ ) − f (x, W )) ≤ kx − W̃ T z ∗ k2 − kx − W T z ∗ k2

τ
1
kW k2F + λkck1 ≤ kX − 0k2 ≤ 1
2
N

= z ∗T (W − W̃ )x + hW̃ W̃ T − W W T , z ∗ z ∗T i

≤ kz ∗ k2 kW − W̃ kF + 2RkW̃ − W kF kz ∗ k22 ≤ 3RKkW̃ − W
kFthus
, kW k2F ≤ 1/τ , kck1 ≤ 1/λ.
and
Let W(T, λ, τ ) := {Ŵ ∈ (RT ×D ) | kŴ kF ≤
We have

we have






sup
E[f (x; W )] − EN [f (x; W )]
W :kW kF ≤R





≤ (3RKδ) + sup E[f (x; W̃ )] − EN [f (x; W̃ )]
≤ 3RKδ +

DK
4R
1
1
log(
)+
log( )
2N
δ
2N
ρ

(32)
with probability 1 − ρ. Choosing δ = 1/(RKN ) yields the
result.

s
≤

τ
kW ∗ k2F + λkc∗ k1
2
(33)
F (c). Then let (c, W ) with

F (c̄) ≤ F (c∗ ) ≤ EN [f (x; W ∗ )] +
where c̄ ∈

argmin
c:supp(c)=S ∗

supp(c) = Ŝ be the output obtained from running T it-

1
1
DT log(4T N/(τ λ))
+
log( )
2N
2N
ρ

with probability 1 − ρ through the same argument as in
the case of combinatorial objective (32). Combining the
above estimation error with the bias and optimization error
in (35), we have
E[f (x; W )] − E[f (x; W ∗ )]
s
τ 2
2γK
2(1 − µ)K
≤ R + λK +
+
λ
2
βT
µβ
s
DT log(4T N/(τ λ))
1
1
+
+
log( )
2N
2N
ρ

Now we establish the proof of Theorem (3) for bounding
risk of the Latent Feature Lasso estimator.
Proof. Let Z ∗ ∈ arg minZ∈{0,1}N K N1 kX −ZW ∗ k2F and
S ∗ be the set of column index of Z with the same 0-1 patterns to columns in Z ∗ . Let c∗ be indicator vector with
c∗k = 1, k ∈ S ∗ and c∗k = 0, k ∈
/ S ∗ . We have

E[f (x; Ŵ )] − EN [f (x, Ŵ )]

sup
(c,W )∈W(T,λ,τ )

W̃ ∈N (δ)

s

p
1/(λτ )}.

Choosing T =
DK
3

2γK 1
β (  ),

λ = τ =

√1
N

and N &

DT
2

=

gives the result.

7.5. Proof of Theorem 4
Proof. Since W ∗ is of rank K, we have span(Θ∗ ) =
span(Z ∗ ). Therefore, from condition 2,
∗ K
span(Θ∗ ) ∩ {0, 1}N \ {0} = {Z:,j
}j=1 .

(36)

Latent Feature Lasso

For any (Z, W ) : ZW = Θ∗ , we have Z ∈ span(Θ∗ )
since Z = Θ∗ V Σ−1 U T where U ΣV T is the SVD of W
with Σ : K ×K. Then by (36) we know that Z = Z ∗ . Then
it follows W = W ∗ since the linear system Θ∗ = Z ∗ W
has unique solution for W .

Z S WS = X = Z ∗ W ∗ .
Since WS has full row-rank, we have rank(ZS ) =
rank(X) = rank(Z) = K by condition 1 in Theorem 4. Then let WS = U ΣV T be the SVD of WS with
Σ : |S| × |S|, we have
ZS = XV Σ−1 U T = Z ∗ W ∗ V Σ−1 U T ∈ span(Z ∗ ).
Then by condition 2 in Theorem 4, the columns of ZS can
∗ K
only be in {Z:,j
}j=1 , which implies ZS equal to Z ∗ up to
a permutation. Then we know |S| = K and by Theorem 4
WS also equals W ∗ up to a permutation.
7.7. Proof of Theorem 8

where k∇f (c∗ )k∞ is given by:

7.9. Proof of Theorem 6
Proof. Note that the optimization problem in Equation (22)
can be rewritten as:
1
argmin 2N
kE + (Z ∗ − Z)k22

Z∈{0,1}N

max

z∈{0,1}N

1
kA∗T zk22 ,
2N 2 τ

(I − ZS (ZST ZS + N τ I)−1 ZST )(ZS W ∗ + )
(37)
Given P = ZS (ZST ZS + N τ I)−1 ZST , it can be seen that
A∗ can be rewritten as :
=

A∗ = (I − P ) + (I − P )(ZS W ∗ ).

7.8. ell2 error bounds on the coefficient vector ĉ
Theorem 8. Let c∗ be the true underlying vector, with support S and sparsity K ∗ . Let ĉ be the minimizer of F (c),
defined in Equation (7). Define the noise-level term

z∈{0,1}N

N
X

(38)
argmin (Ei +

i=1 Zi ∈{0,1}

(Zi∗

2

− Zi ))

So, we have the following closed form expression for Ẑ:
(
1 if Zi∗ + Ei ≥ 0.5
Ẑi =
.
0 o.w

P(Zi∗ 6= Ẑi )

= P(Ei ≥ 0.5) ∗ P(Zi∗ = 0)
+P(Ei ≤ −0.5) ∗ P(Zi∗ = 1)

(I − ZS (ZST ZS + N τ I)−1 ZST )X

max

1
2N

We now compute the probability that Zi∗ 6= Ẑi :

where A∗ is defined as:

ρn :=

where C = {c|kcS c k1 ≤ 3kcS k1 }. Then, if the regularization parameter is set as λ ≥ ρn , we have the following
bound on the norm of the error ĉ − c∗ :
√
ρn K ∗
∗
kĉ − c k2 ≤
.
κn

=

Proof. By an application of Theorem1 of (Negahban et al.,
2009), for λ ≥ k∇f (c∗ )k∞ , we have the following bound
on the `2 norm of ĉ − c∗ :
√
K ∗λ
∗
kĉ − c k2 ≤
,
κn

=

Let κn be the restricted strong convexity term defined as :
∆∈C

Proof. The solution of (21) satisfies

A∗

P = ZS (ZST ZS + N τ I)−1 ZST .

κn := inf {f (c∗ + ∆) − f (c∗ ) − h∇f (c∗ ), ∆i} ,

7.6. Proof of Theorem 5

k∇f (c∗ )k∞ =

where A∗ = (I − P ) + (I − P )(ZS W ∗ ) where

1
kA∗T zk22 ,
2N 2 τ

≥ min{P(Ei ≥ 0.5), P(Ei ≤ −0.5)} ≥ c,
(39)
for some positive constant c. We now use the fact that
E((Zi∗ − Ẑi )2 ) = P(Zi∗ 6= Ẑi ) to complete the proof of
the Lemma.

