Supplementary Material: Deep Generative Models for Relational Data with
Side Information

1. Proof of Lemma 1

rive the Gibbs sampler updates for the hyper-parameters
(w)
(m)
γk1 , ξ, Γk,` and Γk , in closed form.

We can compute E[I{Aij = 0}] as
E[I{Aij = 0}] = p(Xij = 0)


K
Y
= Ezi ,zj ,Λ 
p(Xij = 0|zik1 , zjk2 , Λk1 k2 )

(`)

k1 ,k2





K
Y

= Ezi ,zj ,Λ 

(1)

Given the Pólya-Gamma auxiliary variables αk , the pos(1)
(1)
(w)
(w)
terior for wk will be wk ∼ N (µk , Vk ), where

exp(−Λk1 k2 zik1 zjk2 )

k1 ,k2 =1

≥





K
Y


exp Ezi ,zj ,Λ log
exp(−Λk1 k2 zik1 zjk2 )
k1 ,k2 =1


=

K
X

Ezi ,zj ,Λ −


Λk1 k2 zik1 zjk2 

(1)

k1 ,k2 =1

where the inequality step follows from Jensen’s inequality.h Following Lemma
1 in (Zhou, 2015), we have
i
PK
γa2
ζγc
E
Λ
=
k1 ,k2 =1 k1 k2
γb ck1 k2 + γb2 ck1 k2 . Then the last
line in Equation
 (1) can be written as 
K
X
Ezi ,zj ,Λ −
Λk1 k2 zik1 zjk2 
(2)
k1 ,k2 =1


 
h
i
γa2
ζγc
(1) (1)
+ 2
Ez(1) z(1) zik1 zjk2
= exp −
i
j
γb ck1 k2
γb ck1 k2
Based on Equation (1) and (3), the expected number of zeros in A is lower bounded by
E[

N
X



i,j=1
2

K
X

2

I{Aij = 0}] ≥ N Ezi ,zj ,Λ −

= N exp


Λk1 k2 zik1 zjk2 

k1 ,k2 =1

"
−

(`)

Sample wk and bk : We consider the update of layer-1
(1)
weights wk as an example, and assume the side information is available (which is the more general case). Weights
for the other layers can be sampled in a similar manner.

ζγc
γ2
+ 2 a
γb ck1 k2
γb ck1 k2

#
E

z

(1) (1)
z
i
j

h
i
(1) (1)
zik zjk
1

2

(w)

=

Vk

(Z

(w)

=

((Z

(2) T

µk
Vk

(w)

(2)

(2) T

) (z k

−

(1)

1
(1)
(1)
1N − diag(αk )(Smk + bk 1N ))
2
(w) −1 −1

(2)

) diag(αk )Z

+ (Γk,` )

)

In the above, 1N is a vector of length N with all entries
(1)
(1)
being 1, and αk ∈ RN
+ , each entry αik is drawn from the
Pólya-Gamma distribution
(1)

(1)

(2)

(1)

αik ∼ PG(1, mTk si + (wk )> z i

+ bk )
(`)

Conditioned on these PG variables, the posterior over bk
will also be a Gaussian.
(`)

Sample mk : Akin to the way we sample wk , the side information based regression weights mk can also be sampled using the Pólya-Gamma scheme (using the layer 1
(1)
PG variables αk ). The posterior will be a Gaussian
(m)
(m)
mk ∼ N (µk , Vk ), where
1
(1)
(1)
(2) (1)
1N − diag(αk )(Z wk + bk 1N ))
2

(m)

=

Vk

(m)

=

((S diag(αk )S + (Γk

µk
Vk

(m)

T

(2)

S (z k

T

−

(m) −1 −1

(1)

)

)

Sample γk1 : γk1 can be sampled as

!
(3)

γk1 ∼ Gamma(γa +`k1 k2 ,

1
γb −

2. HYPERPARAMETER INFERENCE
(`) (`)
We sample wk , bk

and mk leveraging the Pólya-Gamma
augmentation (Polson et al., 2013). This enables us to de. Correspondence to: Changwei Hu <changweih@yahooinc.com>, Piyush Rai <piyush@cse.iitk.ac.in>, Lawrence Carin
<lcarin@duke.edu>.
Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

P

k2

ξ

1−δk k
δk k
1 2
1 2γ
k2

ln( Q

ck k
1 2
k1 k2 +ck1 k2

P
where `k1 · = k2 `k1 k2 with `k1 k2 drawn from the Chinese Restaurant Table (CRT) distribution (Zhou, 2015)
`k1 k2 ∼ CRT(X··k1 k2 , gk1 k2 )
Sample ξ: The hyperparameter ξ can be sampled as
ξ ∼ Gamma(ξa +

X
k

`kk ,

ξb −

1
)
γ
ln( Qkkckk
k
k
+ckk )

P

)
)

Supplementary Material: Deep Generative Models for Relational Data with Side Information
(w)

(m)

Sample Γk,` , Γk
matrix

(w)
Γk,`

: Each diagonal entry of the precision

is sampled as

(w)

Γk,` ∼Gamma(a+

K`+1
1
,
(`)
(`)
2
diag((b + 0.5(w )T w )1K
k

k

)
`+1

where a and b are the scale and rate parameters for the prior
(w)
(m)
of Γk,` respectively. Γk can be sampled similarly.

References
Polson, Nicholas G, Scott, James, and Windle, Jesse.
Bayesian inference for logistic models using pólya–
gamma latent variables. Journal of the American Statistical Association, 108(504):1339–1349, 2013.
Zhou, Mingyuan. Infinite edge partition models for overlapping community detection and link prediction. In
AISTATS, 2015.

)

