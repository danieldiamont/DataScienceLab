Spectral Learning from a Single Trajectory under Finite-State Policies
(Supplementary Material)
Borja Balle Odalric-Ambrym Maillard

A. Mixing Properties of Probabilistic Automata
Lemma 2 (⌘-mixing for PFA) Let A be PFA and assume that it is (C, ✓)-geometrically mixing in the sense that for some
constants C > 0, ✓ 2 (0, 1) we have
µA
t = sup

8t 2 N,

↵,↵0

↵0 At k1
6 C✓t ,
↵0 k1

k↵At
k↵

where the supremum is over all probability vectors. Then we have ⌘⇢A 6 C/(✓(1

✓)).

Proof of Lemma 2:
We start by controlling the term ⌘, defined by
⌘⇢A = 1 + max

1<i<t

t
X

⌘i,j ,

j=i+1

We proceed similarly to Lemma 7 of (Kontorovich & Weiss, 2014). By definition of the total variation
norm k · kT V ,
⌘i,j =

1
sup
2 u2⌃i 1 , ,

↵ > Au A Aj i
↵ > Au A

sup
0 2⌃

Z✓⌃t

j+1

1

↵ > Au A 0 Aj
↵ > Au A

AZ

i 1

AZ

0

P
>
where AZ = z2Z Az . At this point, it is convenient to introduce the vector ↵u,
=
Indeed, we then have the rewriting
⌘i,j

=
6

1
sup
2 u2⌃i 1 , ,
1
sup
2 u2⌃i 1 , ,

(↵u,

↵u, 0 )> Aj

i 1

k(↵u,

↵u, 0 )> Aj

i 1

sup
0 2⌃

Z✓⌃t

0 2⌃

Z✓⌃t

j+1

sup
j+1

,
↵ > Au A
↵ > Au A

.

AZ
k1 kAZ k1

where we used
P a simple application of Hölder inequality. Since A is a PFA, we note that kAZ >k1 6
1 because k |z|=t j+1 Az k1 = 1 and all the entries are non-negative. Also note that ↵u,
=
>
k↵u,
k1 = 1. Thus k↵u,
↵u, 0 k 6 2. We deduce from these steps that
⌘i,j 6 sup
↵,↵0

k(↵

↵ 0 ) > Aj i
k↵ ↵0 k1

1

k1

,

where the supremum is taken over all ↵, ↵0 that are probability vectors. We note that the later quantity
is precisely the definition of the coefficient µA
j i 1 . Assuming (C, ✓)-geometrically mixing, that is
A
j
µj 6 C✓ for all j, this implies that
⌘i,j 6 C✓j

i 1

.

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

We then deduce that
⌘⇢A

6

1 + C max

1<i<t

t
X

✓j

i 1

6

j=i+1

t 2
X
C
C 1 ✓t 1
C
(1 +
✓j ) =
6
.
✓
✓ 1 ✓
✓(1 ✓)
j=1

⇤

The following result provides a control of the ⌘⇢A coefficients, and shows this can be made explicit in specific cases.
Corollary 1 Let A be PFA with n states and assume that its matrix A has a spectral gap, that is | 2 (A)| < 1. then there
exists C such that ⌘⇢A 6 | 2 (A)|(1C | 2 (A)|) . When the corresponding chain is further aperiodic, irreducible and reversible,
p
we further have C 6 n.
Proof of Corollary 1:
The first part of the result is folklore, and can be proven using some tedious steps involving the Jordan
decomposition of the matrix see e.g. Fact 3 in (Rosenthal, 1995).
When the chain is irreducible, aperiodic and more importantly reversible, the spectral gap admits the
following characterization, see Lemma 2.2 from (Kontoyiannis & Meyn, 2012):
⇢
kA⌫k2
(A)
=
(A)
=
sup
: ⌫ s.t. k⌫k2 6= 0, ⌫ > 1 = 0 .
2
2
k⌫k2
Thus, from 2 (A) < 1 together with a change of norm from k · k1 to k · k2 and a standard argument
(closely following that of Lemma 7), we obtain that
µA
j 6 C|
where C = maxx2Rn

||x||1
||x||2

=

p

2|

j

,
⇤

n.

We end this section with a more technical lemma, that is useful to decompose terms in the proof of Theorem 3.
Lemma 6 (Mixing times of PFA) Let A = h↵, , {A }i be a PFA. Then, for any s > s0 2 N it holds
kAs

0

↵> As k1 6 2µA
s0 .

Proof of Lemma 6:
0

s
Let denote ↵s> = ↵> As . We need to bound kAP
↵s> k1 . Recall that for any matrix M the
k · k1 -induced norm is given by kM k1 = maxi j |M (i, j)| = maxi kM (i, :)k1 . The ith row of
0
s0
As
↵s> is given by e>
↵s> , where ei is the ith column of the identity matrix. In particular,
i A
0
s
0
e>
i A is the distribution over states after starting in state i and running the chain for s steps, and
>
↵s is the distribution over states starting from the distribution given by ↵ and running the chain for s
0
0
0
steps. The latter can also be rewritten as ↵s> = ↵> As = ↵> As s As = ↵s> s0 As , where ↵s> s0 is
again a distribution over states. Therefore we obtain the desired bound, since:

kAs

0

s
↵s> k1 = max ke>
i A

0

↵s>

i2[n]

0

k↵1> As
6 sup
k↵1
↵1 ,↵2
6 2µA
s0 .

s0 A

s0

k1

0

↵2> As k1
kei
↵ 2 k1

↵s

s0 k1

⇤

2

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

B. Geometry of Stochastic Weighted Automata
Lemma 3 (claim (i)) For any w 2 ⌃? we have kAw k 6 1.
Proof of Lemma 3 (claim (i)):
We shall use the cone monotonicity property of k · k , which says that 0 6K u 6K v implies
kuk 6 kvk . First note that by construction of K we have 0 6K Aw . If we show that Aw 6K
also holds, then cone monotonicity implies kAw k 6 k k = 1.

To
is an eigenvector of A of eigenvalue 1 we have = At =
P prove the claim note that because P
Aw = |w0 |=|w|,w0 6=w0 Aw0 which is a vector in K because convex
|w|=t Aw . Therefore,
cones are closed under non-negative linear combinations, and we conclude that Aw 6K .
⇤

Lemma 3 (claim (ii)) For any w 2 ⌃? we have k↵> Aw k

,⇤

= ↵ > Aw .

Proof of Lemma 3 (claim (ii)):
By unrolling the definitions of the dual norm and B we get
k↵> Aw k
Now note that for any v such that

,⇤

=

sup

6K v6K

↵ > Aw v .

v 2 K we have

↵ > Aw v = ↵ > Aw

v) 6 ↵> Aw

↵ > Aw (

,

where we used that
v 2 K implies Aw (
v) 2 K implies ↵> Aw (
6K , the supremum in the definition of k↵> Aw k ,⇤ is attained at v =

v) > 0. Since
6K
and the result follows. ⇤

C. Mixing Properties of Stochastic Weighted Automata
Lemma 4 (⌘-mixing for SWFA) Let A be SWFA and assume that it is (C, ✓)-geometrically mixing in the sense that for
some C > 0, ✓ 2 (0, 1),
µA
t =

sup
↵0 ,↵1 :↵>
0

=↵>
1

k↵0> At
k↵0
=1

↵1> At k
↵1 k ,⇤

Then the ⌘-mixing coefficient satisfies
⌘⇢A 6

C
.
✓(1 ✓)

Proof of Lemma 4:

3

,⇤

6 C✓t .

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)
>
The proof follows the same initial steps as for Lemma 2. Introducing the vector ↵u,
=
we then have the rewriting

⌘i,j

=
6

1
sup
2 u2⌃i 1 , ,
1
sup
2 u2⌃i 1 , ,

sup
0 2⌃

Z✓⌃t

0 2⌃

Z✓⌃t

j+1

sup
j+1

(↵u,

↵u, 0 )> Aj

i 1

k(↵u,

↵u, 0 )> Aj

i 1

↵ > Au A
↵ > Au A

,

AZ
k

,⇤ kAZ

k

where we used a simple application of Hölder inequality and the norm induced by . Since A is a
SWFA, the same argument in the proof of Lemma 3 (i) can be used to show that kAZ k 6 1 for
>
>
any Z ✓ ⌃t j+1 . On the other hand, from Lemma 3 (ii) we have 1 = ↵u,
= k↵u,
k ,⇤ . Thus
k↵u,
↵u, 0 k ,⇤ 6 2. We deduce from these steps that
⌘i,j 6 sup
↵,↵0

k(↵

↵ 0 ) > Aj
k↵ ↵0 k

i 1

k

,⇤

,

,⇤

where the supremum is taken over all ↵, ↵0 that satisfy ↵> = 1 We note that the later quantity
is precisely the definition of the coefficient µA
j i 1 . We then conclude similarly to the proof of
Lemma 2.
⇤

Lemma 7 (Geometrical mixing of weighted automata) Let A = h↵, , {A }i be a stochastic WFA, A =
(A) = sup

⇢

kA⌫k ,⇤
: ⌫ s.t. k⌫k
k⌫k ,⇤

,⇤

6= 0, ⌫ > = 0 .

P

be its spectral gap with respect to . It holds that
µA
t =

sup
↵0 ,↵1 :↵>
0

=↵>
1

k↵0> At
k↵0
=1

↵1> At k
↵1 k ,⇤

,⇤

6

(A)t .

Proof of Lemma 7:
To this end, note that if ↵0 , ↵1 are are such that ↵0> = ↵1> = 1, then v = ↵0 ↵1 is such that
v > = 0. A crucial remark is that since A is a weighted automaton matrix, ↵0> A = ↵1> A = 1
and thus w = A(↵0 ↵1 ) also satisfies w> = 0. Likewise, (↵0 ↵1 )> At = 0 for all t 2 N.
A second remark is that if kAs vk ,⇤ = 0 for some s < t, then kAt vk ,⇤ = 0. Thus, we can restrict
to v such that kAs vk ,⇤ 6= 0 for all s 6 t. Then, it comes for such v = ↵0 ↵1 ,
kAt ⌫k ,⇤
kAAt 1 ⌫k ,⇤
kA⌫k ,⇤
=
...
6
k⌫k ,⇤
kAt 1 ⌫k ,⇤
k⌫k ,⇤

(A)t .

For the last inequality, we used the fact that since A is a weighted automaton matrix, and v = ↵0 ↵1 ,
kAAs ⌫k
then v > As = 0 for all s. This guarantees that indeed kAs ⌫k ,⇤,⇤ 6 (A) for all s.
⇤

Lemma 8 (Mixing times of SWA) Let A = h↵, , {A }i be a SWFA. Then, for all s > s0 2 N it holds
kAs

0

↵> As k 6 2µA
s0 .
4

A , and

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

Proof of Lemma 8:
Let ↵s> = ↵> As . To prove this we proceed as follows:
kAs

0

↵s> k = sup k(As

0

↵s> )vk

kvk 61

u> (As

0

↵s> )v

= sup

sup

=

ku> (As

0

↵s> )k

ku> (As

0

↵s>

kvk 61 kuk

sup

,⇤ 61

kuk

=

sup

,⇤ 61

kuk

Next we note that for any u such that kuk

,⇤

,⇤ 61

,⇤

s0 A

s0

)k

.

,⇤

6 1 we have |u> | 6 1, so:

ku> ↵t> k

,⇤

= |u> |k↵t> k
6

k↵t> k ,⇤

,⇤

.

Furthermore, the same argument we used to show that k↵> Ax k ,⇤ = ↵> Ax implies that k↵t> k ,⇤ =
k↵> At k ,⇤ = ↵> At = 1. Therefore, we see that kuk ,⇤ 6 1 implies ku> ↵t> k ,⇤ 6 1, and we
get the inequality
kAs

0

↵s> k

6
6
6

sup
ku1 k

µA
s0

,⇤ 61

sup
ku2 k

,⇤ 61

sup

,⇤ 61

ku1 k

2µA
s0 ,

s
ku>
1A

sup
ku2 k

,⇤ 61

0

ku1

0

s
u>
2A k

u2 k

,⇤

,⇤

⇤

.

D. Single-Trajectory Concentration Inequalities for Probabilistic Automata
Theorem 2 (Single-trajectory, entry-wise concentration) Let A be a PFA that is (C, ✓)-geometrically mixing, and
⇠ ⇠ ⇢A 2 P(⌃! ) a trajectory of observations. Then for any u 2 U, v 2 V and 2 (0, 1) it holds
r
✓
◆
⇣
|uv|C
|uv| 1 ⌘ ln(1/ )
U ,V
U ,V
b
P Ht,⇠ (u, v) H̄t )u, v) >
1+
6 .
✓(1 ✓)
t
2t
Proof of Theorem 2:
We control ⌘⇢A by a direct application of Lemma 2.
b U ,V (u, v). We first control the
Control of kgkLip : Let us fix u 2 U, v 2 V define g(⇠) = tH
t,⇠
regularity of f .

To this end, let ⇠ 0 be a trajectory ⇠ 0 = x1 , . . . , xk 1 , x0k , xk+1 , . . . , x` that only differs by one element
from ⇠, say at position k. Then, we get for any u, v
g(⇠)

g(⇠ 0 )

=

t
X

(I{xs . . . xs+|uv|

1

= uv}

I{xs . . . x0k . . . xs+|uv|

s=1

6

|{s 2 [1, t] : k 2 [s : s + |uv|
5

1]}| .

1

= uv})

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

Now, in order to bound |{s 2 [1, t] : k 2 [s : s + |uv| 1]}| note that k 2 [s : s + |uv| 1] if and
only if s 6 k 6 s + |uv| 1. From the first inequality we see that s 6 k, and from the second one
s > k |uv| + 1. Combined with the restrictions on s, this means that
|{s 2 [1, t] : k 2 [s : s + |uv|

|uv| + 1}, min{k, t}]| 6 |uv| ,

1]}| = |[max{1, k

which show that kgkLip 6 |uv|.

Combining the two quantities Combining these two results, and noting that t + |uv|
appears in g(⇠), we deduce that 8" > 0,
✓
b U ,V (u, v)
P t(H
t,⇠

H̄tU ,V (u, v)) > |uv|(t + |uv|

or equivalently, for all
P

✓

1)"

◆

6 exp

✓

2 (0, 1),

b U ,V (u, v)
H
t,⇠

H̄tU ,V (u, v)

>

p

t + |uv|
t

1|uv|

C
✓(1 ✓)

1)✓2 (1
C2

2(t + |uv|

r

ln(1/ )
2t

◆

6 .

1 symbols
✓)2 "2

◆

,

⇤

The proof of following result is more challenging.
TheoremP3 (Single-trajectory, matrix-wise) Let ⇢A 2 P(⌃! ) be as in Theorem 2 and define the probability mass
mU ,V = u2U ,v2V f¯t (uv). Then, for all 2 (0, 1),
P

✓

b tU ,V
kH

2LC
+
✓(1 ✓)

r

H̄tU ,V k2
⇣

>

p

L+

r

2C
1 ✓

!r

2mU ,V
t

L 1 ⌘ min{|U||V|, 2nU nV } ln(1/ )
1+
t
2t

◆

6 .

Proof of Theorem 3:
b tU ,V
Let us introduce the function g(⇠) = kH
applying Theorem 1.

H̄tU ,V k2 . We first control kgkLip then E[g(⇠)], before

Step 1: Control of kgkLip . In this step, we show that
kgkLip 6

Lp
min{|U||V|, 2nU nV }
t

where L = maxu2U ,v2V |uv| denote the maximal length of words in U · V and nU = |` 2 [0, L] :
|U` | > 0|, nV = |` 2 [0, L] : |V` | > 0|, denote the number lengths such that the set U` = {u 2
U : |u| = `} (respectively V` = {v 2 V : |v| = `}) is non empty. Note that the second term in
the min can be exponentially smaller than the first. For example, taking U = V = ⌃6L/2 we have
|U ||V | = ⇥(|⌃|L ) while nU nV = ⇥(L2 ).
Step 1.1. Let ⇠ 0 ⇠ p be a trajectory ⇠ 0 = x1 , . . . , xk
6

0
1 , xk , xk+1 , . . . , x`

that only differs by one

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

element from ⇠, say at position k. We note that
b U ,V
kH
t,⇠
=
6

b U ,V0
b U ,V H
b U ,V0 k2
H̄tU ,V k2 kH
H̄tU ,V k2 6 kH
t,⇠
t,⇠
t,⇠
v
u
t
X⇣XX
1 u
t
sup
I{xs . . . xs+|uv| 1 = uv} I{xs . . . x0k . . . xs+|uv|
q2RV tkqk2
u2U v2V s=1
s
sX X
sX
⌘2
X⇣X
1
1
2
sup
|uv|qv 6 sup
|uv|
qv2 .
q2RV tkqk2
q2RV tkqk2
u2U

v2V

u2U v2V

Let L = maxu2U ,v2V |uv|. A simple bound is then kgkLip 6
optimal if all words uv, u 2 U, v 2 V have same length.

1

= uv} qv

v2V

L
t

p

|U||V|, which is essentially

Step 1.2. A more refined bound may be helpful in case many words have length |uv| much smaller
b tU ,V = 1 Pt Ms with Ms (u, v) = bs,uv = I{xs . . . xs+|uv| 1 =
than L. To his end, let us write H
s=1
t
P
t
U ,V
1
0
uv}. Similarly, let Ht,⇠
M
with
the obvious definition. Now, by the same argument we
0 = t
s
s=1
used to bound kgkLip in the entry-wise case, we have

since for s < k

⇣
b U ,V
t H
t,⇠

⌘
b U ,V0 =
H
t,⇠

k
X

Ms

Ms0 =

s=k L+1

k
X

s

,

s=k L+1

L + 1 or s > k we must have Ms = Ms0 .

Now let us partition the sets U and V as disjoint unions of sets with strings of the same length. That is,
`
L
we write U = [L
`=0 U` with U` = U \ ⌃ , and V = [`=0 V` with analogous definitions. This allows
U ⇥V
us to write Ms 2 {0, 1}
as a block matrix Ms = (Msi,j )06i,j6L with Msi,j 2 {0, 1}Ui ⇥Vj .

For simplicity of notation, in the sequel we are assuming that U` , V` 6= ; for all 0 6 ` 6 L, but
the argument remains the same after we remove the empty sets of rows and columns. Note that by
definition we have Msi,j (u, v) = I{xs . . . xs+i+j 1 = uv} for any u 2 Ui and v 2 Vj . This implies
that each of the block matrices Msi,j contains at most one non-zero entry.
i,j

If we make analogous definitions and write Ms0 = (Ms0 )06i,j6L , then we obtain a block decomposition for s = Ms Ms0 = ( i,j
s )06i,j6L where each block is either:
1.
2.
3.
4.

zero,
a {0, 1}-matrix with a single 1,
a {0, 1}-matrix with a single 1,
a {0, 1, 1}-matrix with a single 1 and a single

1.

i,j
In any of these cases one can see that the
k i,j
s k2 6 k s kF 6
P bound
2
2
i,j 2
Therefore, we have k s k2 6 k s kF = i,j k s kF 6 2nU nV , where

nU

nV

b U ,V
By plugging these estimates into H
t,⇠
b U ,V
kH
t,⇠

Therefore we obtain the bound kgkLip

= |` 2 [0, L] : |U` | > 0| ,

= |` 2 [0, L] : |V` | > 0| .
b U ,V0 we finally get
H
t,⇠

p
L 2nU nV
U ,V
b
Ht,⇠0 k2 6
.
t
p
6 (L 2nU nV )/t.

7

p

2 is always satisfied.

⌘2

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

b tU ,V H̄tU ,V k2 ]. More
Control of E[g(⇠)]. We now want to control the following quantity E[kH
precisely, we show in this step that
⇣
⌘ ⇣P
⌘
O L2 + 1 1 ✓
f¯t (uv)
u2U
,v2V
b tU ,V H̄tU ,V k2 ]2 6
E[kH
,
t
Pt
where L = maxw2U ·V |w|, and f¯t (w) = 1t s=1 fs (w), where fs (w) = P[⇠ 2 ⌃s 1 w⌃! ].

Step 2.1. Let q 2 RV be a unit vector (kqk2 = 1.) Then, by Jensen’s inequality, the norm of
b tU ,V H̄tU ,V is controlled by its Frobenius norm
H
b tU ,V
E[kH

H̄tU ,V k2 ]2

6
6

b tU ,V H̄tU ,V k22 ]
E[kH
X ⇣ X U ,V
b t (u, v)
E[
(H
u2U

6

XX

u2U v2V

=

X

w2U ·V

H̄tU ,V (u, v))qv

v2V

b tU ,V (u, v)
E[(H

|w|U ,V E[(fbt (w)

H̄tU ,V (u, v))2 ]

⌘2

]

f¯t (w))2 ] ,

where U · V is the set of all words of the form u · v with u 2 U and v 2 V; |w|U ,V = |(u, v) 2
Pt
U ⇥ V : u · v = w|, and fbt (w) = 1t s=1 bs,w with the notation defined above. We also use
Pt
f¯t (w) = E[fbt (w)] = 1t s=1 fs (w), where fs (w) = P[⇠ 2 ⌃s 1 w⌃! ]. This implies that we have a
sum of variances, and each of them can be written as
E[(fbt (w)

f¯t (w))2 ] = E[fbt (w)2 ]

f¯t (w)2 .

An important first observation is that we can P
write fs (w) = ↵> As 1 Aw . Furthermore, it follows
from A being a probabilistic automaton that |w|=l fs (w) = 1 for all s and l. This suggests that
we group the terms in the sum over W = U · V by length, so we write Wl = W \ ⌃l and define
Ll = maxw2Wl |w|U ,V the maximum number of ways to write a string of length l in W as a product
of a prefix in U and a suffix in V . Note that we always have Ll 6 l + 1. Henceforth, we want to
control the following terms for all possible values of l:
2
!2 3
!2

t
t
⇣
⌘
X
X
X
X
1
2
2
b
¯
|w|U ,V E[ft (w) ] ft (w) = 2
|w|U ,V E 4
bs,w 5
fs (w)
.
t
s=1
s=1
w2Wl

w2Wl

Step 2.2. Let us focus on each of the quadratic terms. On the one hand, it holds
!2
t
t
X
X
X
fs (w)
=
fs (w)2 + 2
fs (w)fs0 (w) ,
s=1

16s<s0 6t

s=1

while other on the other hand, we get
2
!2 3
t
X
E4
bs,w 5 =
s=1

t
X

E[b2s,w ] + 2

X

E[bs,w bs0 ,w ] .

16s<s0 6t

s=1

Hence this enables to derive the following bound
b tU ,V
E[kH

H̄tU ,V k2 ]2

6

X
1
t
1 X X
|w|
(1 fs (w))fs (w)
U
,V
t2
s=1
l=0 w2Wl
⌘
X ⇣
+2
E[bs,w bs0 ,w ] fs (w)fs0 (w) .
16s<s0 6t

8

(6)

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

Step 2.3. In order to control the first term in (6), we remark that
1 X
X

l=0 w2Wl

|w|U ,V

t
X

(1

fs (w))fs (w)

t
X

X

=

s=1

(1

fs (uv))fs (uv)

u2U ,v2V s=1

X

6

t
X

fs (uv)

u2U ,v2V s=1

=

X

t

f¯t (uv) .

(7)

u2U ,v2V

Step 2.4. We thus focus on controlling the remaining ”cross”-term in (6) and to this end we study, for
w 2 Wl , the quantity
E[bs,w bs0 ,w ]

fs (w)fs0 (w)

=

P[⇠ 2 ⌃s

1

0

⌃sw

s

(↵> As

⌃! ]

0

0

1

Aw )(↵> As

0

1

Aw ) ,

0

where we introduced for convenience the set ⌃sw s = w⌃s s \ ⌃s s w. Introducing
P as well the
0
0
0
vectors ↵s> 1 = ↵> As 1 , ↵s>0 1 = ↵> As 1 and the transition matrix Asw s =
A
x2⌃s s x
0

corresponding to the “event” ⌃sw
E[bs,w bs0 ,w ]

s

w

, it comes

fs (w)fs0 (w)

↵s>

=

1

⇣

0

Asw

Aw ↵s>0

s

s > l, then the case when s0

We now discuss two cases. First the case when s0
0

0

Note that if s0 s > |w| = l, then ⌃sw s simplifies to ⌃sw s = w⌃s
0
Aw As s l Aw . For such words, we thus obtain
⇣ 0
⌘
⇣ 0
↵s> 1 Asw s Aw ↵s>0 1 Aw
= ↵s> 1 Aw As s l
↵s>0
6

1 Aw

k↵s> 1 Aw k1 kAs

0

s l

0

s l

1

⌘

⌘

.
s < l.
0

w and thus Asw

s

=

Aw

↵s>0

1 k1 kAw

k1 .

Moreover, from Lemma 6, it holds kAs s l
↵s>0 1 k1 6 2µA
s0 s l . Also, it holds that
>
kAw k1 6 1. Finally, since ↵s 1 Aw is a sub-distribution over states, we have
X
X
|w|U ,V k↵s> 1 Aw k1 =
|w|U ,V ↵s> 1 Aw
0

w2Wl

w2Wl

=

X

w2Wl

|w|U ,V fs (w) =
0

X

fs (uv) .

u2U ,v2V:uv2Wl

Now, on the other hand if s0 s < l, using the fact that ⌃sw s ⇢ w⌃s
⇣ 0
⌘
⇣ 0
↵s> 1 Asw s Aw ↵s>0 1 Aw
6 ↵s> 1 Aw As s

0

s

, then
↵s>0

1 Aw

⌘

fs (w)(1 fs0 (w)) 6 fs (w) .
P
P
So in this case we again see that w2Wl |w|U ,V fs (w) = u2U ,v2V:uv2Wl fs (uv).
=

Step
P 2.5. Therefore,
P combining the above steps, so far we have seen that for a fixed l > 0, the sum
0
|w|
fs (w)fs0 (w)) is upper bounded by:
U ,V
w2Wl
16s<s0 6t (E[bs,w bs ,w ]
X

X

16s<s0 6t u2U ,v2V:|uv|=l

=

X

t 1
X

u2U ,v2V:|uv|=l s=1

fs (uv)(2µA
s0

fs (uv)

 X
t

s l I{s

2µA
s0

s0 =s+1

9

s > l} + I{s0

0

s l I{s

0

s > l} + I{s0

s < l})

s < l}

.

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

Now note that
we get

Pt

s0 =s+1

t
X

µA
s0

I{s0

t
s < l} = min{l 1, t s} 6 l 1. Furthermore, using that µA
t 6 C✓

s l I{s

0

s > l}

=

I{t > s + l}

s0 =s+1

tX
s l

µA
k

k=0

6

CI{t > s + l}

1

✓t s
1 ✓

l+1

6

C
1

✓

.

In conclusion, we get
t 1
X

X

fs (uv)

6
6

2C
1+
1 ✓

l

✓
t l

1+

◆

2C
1 ✓

2µA
s0

s l I{s

0

s > l} + I{s0

s < l}

s0 =s+1

u2U ,v2V:|uv|=l s=1

✓

 X
t

X

t 1
X

fs (uv)

u2U ,v2V:|uv|=l s=1

◆

X

f¯t (uv) .

u2U ,v2V:|uv|=l

Finally, putting all the pieces together and introducing L = maxw2U ·V |w|, we get from equations
(6), (7), (8),
P
1
¯
X
2X
2C
u2U ,v2V ft (uv)
U ,V
U ,V
2
b
E[kHt
H̄t k2 ] 6
+
f¯t (uv)(l 1 +
)
t
t
1 ✓
l=0 u2U ,v2V:|uv|=l
P

¯
4C
u2U ,v2V ft (uv)
6
2L 1 +
.
1 ✓
t
Step 3. Application of Theorem 1. It remains to apply Theorem 1 with

b tU ,V
E[kH

kgkLip

6

H̄tU ,V k2 ]

6

Lp
min{|U||V|, 2nU nV } ,
t
!s P
r
p
2 u2U ,v2V f¯t (uv)
2C
L+
,
1 ✓
t

for some constant C. After some rewriting, it comes
P

✓

b tU ,V
kH

H̄tU ,V k2

LC
+
(1 ✓)

r

⇣

p

>

1+

L

L+

r

2C
1 ✓

!s P
2 u2U ,v2V f¯t (uv)
t

1 ⌘ min{|U||V|, 2nU nV } ln(1/ )
t
2t

◆

6 .

⇤

E. Single-Trajectory Hankel Concentration Inequalities with Finite-State Control
b = H
b U ,V computed in Algorithm 3 satisfies E[H
b U ,V ] = H̃tU ,V , where H̃tU ,V is
Lemma 5 The Hankel matrix H
t,⇠
t,⇠
a block of the Hankel matrix corresponding to the stochastic WFA Ãt = h↵
˜ t , , {A }i where we introduced the modified
Pt 1
vector ↵
˜ t = (1/t) s=0 ↵> (A/)s . We denote by f˜t the function computed by Ãt .
10

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

Proof of Lemma 5:
For any t > 0 and w 2 ⌃? let us define the function 's,w : ⌃! ! R given by
's,w (x) =

I{os+1 as+1 · · · os+|uv| as+|uv| = w}
,
s ⇡(a1 · · · as+|w| |o1 · · · os+|w| )

where x = (o1 , a1 )(o2 , a2 ) · · · . Thus, the entries of the Hankel matrix computed in Algorithm 3 can
Pt 1
b
be written as H(u,
v) = (1/t) s=0 's,uv (⇠). Now note that the expectation E['s,w ] with respect to
a trajectory ⇠ ⇠ ⇢B can be written as
X

w0 2⌃s

X
P[⇠ 2 w0 w⌃! ]
fB (w0 w)
=
s ⇡(w0A wA |w0O wO )
s fA⇡ (w0 w)
0
s

X fA (w0 w)
↵ > As Aw
=
=
s

s
0
s

w 2⌃

.

w 2⌃

b =H
b U ,V computed in Algorithm 3 satisfies E[H
b U ,V ] = H̃tU ,V , where
Therefore, the Hankel matrix H
t,⇠
t,⇠
H̃tU ,V is a block of the Hankel matrix corresponding to the stochastic WFA Ãt = h↵
˜ t , , {A }i with
Pt 1 >
s
˜
modified vector ↵
˜ t = (1/t) s=0 ↵ (A/) . We denote by ft the function computed by Ãt .
⇤

Theorem 6 (Controlled case, single-trajectory, matrix-wise) Let A = h↵, , {A }i be a stochastic environment and
⇡ a stochastic policy induced by a probabilistic automaton A⇡ , both over ⌃ = A ⇥ O. Let B = A ⌦ A⇡ be the stochastic
WFA obtained by coupling the environment and the policy and ⇢B 2 P(⌃! ) the corresponding stochastic process. Suppose
that B is (C, ✓)-geometrically mixing. Suppose ⇡ satisfies the exploration Assumption 1 with parameter ". Suppose the
importance sampling constant  in Algorithm 3 satisfies " > 1. Let Ãt = h↵
˜ t , , {A }i be the WFA defined in Section 5,
Pt 1
where the initial vector is ↵
˜ t = (1/t) s=0 ↵> (A/)s . Let Ā = A ⌦ Aunif be the stochastic WFA h↵, , A /|A|i
¯
obtained by coupling the environment A with the uniform random policy. Suppose Ā is (C̄, ✓)-geometrically
mixing. Let
P
P
unif
unif
˜
˜
˜
˜
L = maxw2U ·V |w|, m̃ = u2U ,v2V ft (uv), and m̄ = u2U ,v2V ft
(uv), where ft = fÃt and ft
is the function
P
computed by the stochastic WFA obtained by Césaro averaging Ā over t steps. Let d = w2U ·V |w|U ,V . Then for any
2 (0, 1) we have
b U ,V
P kH
t,⇠

H̃tU ,V k2

>

s

m̃
L
t" (1 

2" 2)

+

s

2m̄
t"2L

✓

L+

C̄
1

◆

+
✓(1
✓¯

C
✓)"L

r

2d ln(1/ )
t

!

6

Proof of Theorem 6:
b tU ,V
Let us introduce the function g(⇠) = kH
applying Theorem 1.

H̃tU ,V k2 . We first control kgkLip then E[g(⇠)], before

Step 1: Control of kgkLip .

Let ⇠, ⇠ 0 2 ⌃! be trajectories ⇠ = x1 x2 · · · and ⇠ 0 = x01 x02 · · · differing by one element, say at
11

.

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

position `. That is, xs = x0s for all s 6= `. We note that
b U,V
kH
t,⇠

b U,V0
kH
t,⇠

H̃tU,V k2

b U,V H
b U,V0 k2
H̃tU,V k2 6 kH
t,⇠
t,⇠
sX X
6
(fbt,⇠ (uv)

fbt,⇠0 (uv))2

u2U v2V

v
u
XX
1u
= t
t
u2U v2V

t 1
X

's,uv (⇠)

's,uv (⇠ 0 )

s=0

!2

.

Next we take any w 2 U · V and use xi = (oi , ai ) to write
|'s,w (⇠)

I{os+1 as+1 · · · os+|uv| as+|uv| = w}
s ⇡(a1 · · · as+|w| |o1 · · · os+|w| )

's,w (⇠ 0 )| =

s ⇡(a01 · · · a0s+|w| |o01 · · · o0s+|w| )
!
1
1
+
⇡(a1 · · · as+|w| |o1 · · · os+|w| ) ⇡(a01 · · · a0s+|w| |o01 · · · o0s+|w| )

6

1
s

6

s "s+|w|

2

I{o0s+1 a0s+1 · · · o0s+|uv| a0s+|uv| = w}

,

where we used the exploration assumption ⇡(uA |uO ) > "|u| for all u 2 ⌃? .
From the expression above we see that for any w 2 U · V we have
t 1
X

's,w (⇠ 0 ) 6

's,w (⇠)

s=0

(1

2
,
1/("))"|w|

where we used that " > 1. Thus, we can conclude that
kgkLip 6
Note that d =

P

2
1/("))

t(1

w2U ·V

s

s X
X |w|U ,V
2
6
|w|U ,V .
t"L (1 1/("))
"2|w|
w2U ·V
w2U ·V

|w|U ,V is the quantity defined in the statement of Theorem 3.

b U ,V
Step 2: Control of E[g(⇠)]. We now want to control the following quantity E[kH
t,⇠
We start in the same way as in the proof of Theorem 3.
b tU ,V
Step 2.1. By Jensen’s inequality, the norm of H
b U ,V
E[kH
t,⇠

H̃tU ,V k2 ]2

6

XX

E

u2U v2V

=

X

w2U ·V

H̃tU ,V k2 ].

H̃tU ,V is controlled by its Frobenius norm
⇣

fbt,⇠ (uv)

|w|U ,V E

⇣

f˜t (uv)

fbt,⇠ (w)

⌘2

f˜t (w)

⌘2

.

Recall that in Section 5 we showed that E[fbt,⇠ (w)] = f˜t (w) for any w 2 ⌃? . Hence the expression
above is a sum of variances, each of which can be written as
E

⇣

fbt,⇠ (w)

f˜t (w)

⌘2

h
i
= E fbt,⇠ (w)2

12

f˜t (w)2 .

(8)

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

Now we recall the definitions of the quantities appearing in this expression:
t 1

1X
fbt,⇠ (w) =
's,w (⇠)
t s=0
t 1

=

1 X I{os+1 as+1 · · · os+|w| as+|w| = w}
,
t s=0 s ⇡(a1 · · · as+|w| |o1 · · · os+|w| )
t 1

1 X fA (⌃s w)
f˜t (w) =
t s=0
s
✓ ◆s
t 1
1X > A
=
↵
Aw
t s=0


.

Therefore, we can expand the squares in (8) as follows:
0
1
t 1
h
i
X
X
⇥
⇤
1
E fbt,⇠ (w)2 = 2 @
E 's,w (⇠)2 + 2
E ['s,w (⇠)'s0 ,w (⇠)]A ,
t
s=0
06s<s0 6t 1
0
1
t 1
s
2
s
s0
X
X
1
f
(⌃
w)
f
(⌃
w)f
(⌃
w)
A
A
A
A .
f˜t (w)2 = 2 @
+2
2s
s+s0
t


0
s=0
06s<s 6t 1

Using these expression we now bound the difference in (8) by considering the “squared” and the
“cross” terms separately.
Step 2.2. We start with the “squared” terms and note that for any 0 6 s 6 t
have
X
⇥
⇤
E 's,w (⇠)2 =

w0 2⌃s

=

X

w0 2⌃s

1 and w 2 U · V we

fB (w0 w)
2s ⇡(w0A wA |w0O wO )2
fA (w0 w)
2s ⇡(w0A wA |w0O wO )

fA (⌃s w)
2s "s+|w|
fA (⌃s w)
.
= s
 (")s "|w|
6

Using Cauchy–Schwartz to sum these terms over t we obtain:
t 1
X
s=0

⇥

E 's,w (⇠)

2

⇤

t 1
X
fA (⌃s w)
6
s (")s "|w|
s=0

6

(1

1
1/(2 "2 ))"|w|

t 1
X
fA (⌃s w)2
s=0

2s

!

Using this bound we can now see that the contribution of the “squared” terms to (8) is at most
!
!
✓
◆ X
t 1
t 1
t 1
⇤ X
1 X ⇥
fA (⌃s w)2
1
1
fA (⌃s w)2
2
E 's,w (⇠)
6 2
1
t2 s=0
2s
t
2s
(1 1/(2 "2 ))"|w|
s=0
s=0
!
t 1
X
1
fA (⌃s w)2
6 2
.
2s
t (1 1/(2 "2 ))"|w| s=0
13

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

This expression can be further simplified by noting that " 6 1/|A| implies  > |A| and therefore
fA (⌃s w)/s 6 fA (⌃s w)/|A|s 6 1 since this corresponds to the probability of observing wO when
taking the actions in wA after the first s actions have been chosen by a uniform random policy. Thus,
we get
!
!
t 1
t 1
t 1
X
⇤ X
1 X ⇥
fA (⌃s w)2
1
fA (⌃s w)
2
E 's,w (⇠)
6 2
t2 s=0
2s
s
t (1 1/(2 "2 ))"|w| s=0
s=0
=

f˜t (w)
.
1/(2 "2 ))"|w|

t(1

To complete this step we sum this bound for all w 2 U · V to control the contribution of the “squared”
terms in (8):
X

w2U ·V

|w|U ,V

f˜t (w)
6
t(1
1/(2 "2 ))"|w|

t(1

=

t(1

X
1
|w|U ,V f˜t (w)
2
2
L
1/( " ))"
w2U ·V
X
1
f˜t (uv) ,
1/(2 "2 ))"L
u2U ,v2V

where L = maxw2U ·V |w|.
Step 2.3. We now focus on controlling the “cross” terms in (8) of the form
0

fA (⌃s w)fA (⌃s w)
.
s+s0

E ['s,w (⇠)'s0 ,w (⇠)]
0

Using the same notation ⌃sw

s

= w⌃s

0

X

E ['s,w (⇠)'s0 ,w (⇠)] =

0

x2⌃s ⌃sw

X

=

0

x2⌃s ⌃sw

X

6

0

x2⌃s ⌃sw

s

s

s

s

\ ⌃s

0

s

(9)

w as in the proof of Theorem 3, we first note that

fB (x)
0
A
O
O
s+s

⇡(x1:s+|w| |x1:s+|w| )⇡(xA
1:s0 +|w| |x1:s0 +|w| )
fA (x)
0
A
s+s

⇡(x1:s+|w| |xO
1:s+|w| )
fA (x)
s+s0 "s+|w|

0

fA (⌃s ⌃sw s )
= s+s0 s+|w|

"
> s s0 s
↵ A Aw
= s+s0 s+|w|
,

"
P
0
where we used the notation Asw s = x2⌃s0 s Ax . We also define Ã = A/ and ↵s> = ↵> Ãs .
w
Then we can write (9) as
!
0
0
0
↵> As Asw s
(↵> As Aw )(↵> As Aw )
Asw s
>
>
= ↵s
Aw ↵ s 0 Aw
.
(10)
s+s0
s+s0 "s+|w|
s0 "s+|w|
To bound this quantity we proceed by considering two cases.
Step 2.4. First suppose that s0
equals to
↵s> Aw

0

As s l
s0 "s+l

0

s > l = |w|. In this case we have Asw
↵s>0

!

Aw =

↵s> Aw
14

0

Ãs s l
s+l "s+l

s

= Aw As

0
>
↵s+l
Ãs s l

0

!

s l

Aw

Aw and (10)

.

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

Now we apply the same argument we used to bound the “cross” terms in the case of stochastic
WFA using cone norms. In particular, we consider the stochastic WFA Ā = h↵, , Ā i, where
Ā = A /|A|. Note this is the stochastic WFA obtained by coupling environment A with the random
policy that at each step chooses each action independently with probability 1/|A|. Now we let k · k
and k · k ,? denote the cone norms corresponding to Ā. Using Lemma 3 we see that the following
hold for all w 2 ⌃? :
kAw k = |A|l kĀw k 6 |A|l
k↵s> Aw k

,?

=

|A|s+l > s
k↵ Ā Āw k
s

|A|s+l > s
↵ Ā Āw
s

=

,?

,

where we used the notation Ā = A/|A|. We also note that for any vector satisfying kuk
have
ku> ↵s> k

,?

6 k↵s> k

,?

|A|s > s
k↵ Ā k
s

=

,?

6

,?

6 1 we

|A|s
1
6 s s .
s

 "

This last bound can now be combined with the argument used in the case of stochastic WFA to show
that
0

Ãs s l
s+l "s+l

>
↵s+l
Ãs

0

s l

=

,? 61

kuk

6

sup

6

,? 61

ku1 k
s

=

u>

sup

0

,? 61

sup
ku1 k

|A|s s+l Ā
µ0
s0 "s+l s

,? 61

s l

s0 s l

Ã
s+l "s+l

sup
ku2 k

s l

,? 61

u>
2

s
u>
1 Ā

0

s0 s l

Ã
s+l "s+l
s l

,?

s
u>
2 Ā

0

s l
,?

,

where we used the definition of the mixing coefficient µĀ
s0
0

0

,?

u>
1

sup
ku2 k

>
u> ↵s+l
Ãs

s+l

|A|
s0 "s+l
0

0

Ãs s l
s+l "s+l

s l

for stochastic WFA Ā.

0

We now observe that |A| 6 1/" <  implies |A|s +l /s+s "s+l 6 1/s "s+2l . Finally, by plugging
all these bounds together on an application of Hölder’s inequality yields:
!
0
µĀ
As s l
s0 s l > s
>
>
↵ s Aw
↵
A
6
↵ Ā Āw .
0
w
0
s
s "s+l
s "s+2l

Step 2.5. Now we consider the case s0
0
w⌃s s , then
0

↵s> Asw

s

6 ↵s> Aw As

0

s

0

s < l = |w|. Using the fact that this implies ⌃sw
= |A|s

0

0
s >
↵s Aw Ās s

= |A|s

0

s >
↵ s Aw

s

⇢

,

where we used Ā = . Therefore, we can bound the expression in (10) as
!
!
0
0
0
Asw s
|A|s s
|A|s s >
>
>
>
>
↵s
A
↵
6
↵
A
↵
6
↵ Aw
0 Aw
0 Aw
w
w
0
0
0
s
s
s
s "s+l
s "s+l
s "s+l s
0

=

|A|s +l > s
1
6 s s+2l ↵> Ās Āw
0 +s s+l ↵ Ā Āw
s

"
 "

.

Step 2.6. Finally, we can combine the bounds above by summing over all w 2 U · V and all 0 6 s <
s0 6 t 1 in the same way we did for PFA. We first note that from Steps 2.4 and 2.5 we obtain the
15

Spectral Learning from a Single Trajectory under Finite-State Policies (Supplementary Material)

following bound for (10):
↵s>

0

Asw s
s
 0 "s+|w|

Aw ↵s>0 Aw

!

6

¯

fs (w)
s "s+2|w|

⇣

µĀ
s0

s |w| I{s

0

s > |w|} + I{s0

s < |w|}

⌘

.

¯s0 s l , where C̄ and ✓¯ are the geometric mixing constants
Now let l = |w| and note that µĀ
s0 s l 6 C̄ ✓
for stochastic WFA Ā. Thus, summing first over s0 we get
t 1
X

µĀ
s0

s0 =s+1

s |w| I{s

0

s > |w|} + I{s0

s < |w|} 6 l +

C̄
1

.
✓¯

Therefore, writing Wl for all words of length l in W = U · V we get:
2 X
|w|U ,V
t2
w2U ·V

X

0

fA (⌃s w)fA (⌃s w)
s+s0

E ['s,w (⇠)'s0 ,w (⇠)]

06s<s0 6t 1

!

✓
◆X
1
t 2 ¯
2 X X |w|U ,V
C̄
fs (w)
6 2
l
+
2l
¯
t
"
1 ✓ s=0 s "s
l=0 w2Wl
✓
◆ X
1
t 1 ¯
X
2X 1
C̄
fs (w)
6
l
+
|w|
U
,V
2l
¯
t
"
t
1 ✓ w2W
s=0
l=0
l
✓
◆ X
2
C̄
6 2L L +
f˜unif (uv) ,
t"
1 ✓¯ u2U ,v2V t

Pt 1
where we used that " > 1 and f˜tunif (w) = (1/t) s=0 f¯s (w).
b U ,V
Step 2.7. Our final bound for E[kH
t,⇠
Step 2.2 and 2.6:
b U ,V
E[kH
t,⇠

H̃tU ,V k2 ]2

6

t"L (1

H̃tU ,V k2 ] is now obtained by combining the results from

1
1/(")2 )

X

u2U ,v2V

f˜t (uv) +

2
t"2L

✓

L+

C̄
1

✓¯

◆

X

f˜tunif (uv) .

u2U ,v2V

P
P
Note that m̃ = u2U ,v2V f˜t (uv) and m̄ = u2U ,v2V f˜tunif (uv) are the quantities defined in the
statement of Theorem 6.
Step 3. Application of Theorem 1 It follows directly from Theorem 1 that with probability at least
1
we have
r
t ln(1/ )
U ,V
U ,V
U ,V
U ,V
b
b
.
kHt,⇠
H̃t k2 6 E[kHt,⇠
H̃t k2 ] + ⌘⇢B kgkLip
2

Using that ⇢B is (C, ✓)-geometrically mixing and Lemma 4 we can bound the ⌘-mixing coefficient as
b U ,V H̃tU ,V k2 ] we obtain
⌘⇢B 6 C/(✓(1 ✓)). Thus, by plugging our estimates for kgkLip and E[kH
t,⇠
that with probability at least 1
:
s
s
r
✓
◆
m̃
2
m̄
C̄
C
2d ln(1/ )
U
,V
U
,V
b
kH
H̃t k2 6
+
L+
+
.
t,⇠
t"L (1  2 " 2 )
t"2L
✓(1 ✓)"L
t
1 ✓¯
⇤

16

