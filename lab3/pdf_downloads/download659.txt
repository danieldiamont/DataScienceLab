Supplement for “Stochastic Convex Optimization:
Faster Local Growth Implies Faster Global Convergence”
Yi Xu 1 Qihang Lin 2 Tianbao Yang 1

1. Proof of Theorem 1

wk†

Theorem 1. Suppose Assumption 1 holds and F (w) obeys
the LGC (6). Given
2 (0, 1), let ˜ = /K, K =
✏0
c✏0
dlog2 ( ✏ )e, D1
and t be the smallest integer
✏1 ✓
G2 D 2
such that t max{9, 1728 log(1/ ˜)} 2 1 . Then ASSG-c

1,✏

= wk

F (wk )

1

and

F (wk†

⌘k G
+
2
✏k
✏k 1

+
3
6


1,✏ )

✏0

guarantees that, with a probability 1
, F (wK ) F⇤ 
2✏. As a result, the iteration complexity of ASSG-c for
achieving an 2✏-optimal solution with a high probability 1
is O(c2 G2 dlog2 ( ✏✏0 )e log(1/ )/✏2(1 ✓) ) provided
D1 = O( ✏(1c✏0✓) ).
†
Proof. Let wk,✏
denote the closest point to wk in S✏ . Dec✏k 1
✏0
1
fine ✏k = 2k . Note that Dk = 2D
and ⌘k =
k 1
✏1 ✓
✏k 1
.
We
will
show
by
induction
that
F
(w
)
F
 ✏k +✏
2
k
⇤
3G
for k = 0, 1, . . . with a high probability, which leads to our
conclusion when k = K. The inequality holds obviously
for k = 0. Conditioned on F (wk 1 ) F⇤  ✏k 1 + ✏, we
will show that F (wk ) F⇤  ✏k +✏ with a high probability.
By Lemma 1, we have
c
kwk† 1,✏ wk 1 k2  1 ✓ (F (wk 1 ) F (wk† 1,✏ ))
✏
c✏k 1
 1 ✓  Dk .
(1)
✏

We apply Lemma 2 to the k-th stage of Algorithm 1 conditioned on randomness in previous stages. With a probability 1 ˜ we have
F (wk )

F (wk†

kwk 1 wk†
⌘ k G2
+
2
2⌘k t
q
4GDk 3 log(1/ ˜)
p
+
.
t

1,✏ ) 

2
1,✏ k2

(2)

We now consider two cases for wk 1 . First, we assume
F (wk 1 ) F⇤  ✏, i.e. wk 1 2 S✏ . Then we have
1

Department of Computer Science, The University of Iowa,
Iowa City, IA 52246, USA 2 Department of Management Sciences, The University of Iowa, Iowa City, IA 52246, USA. Correspondence to: Tianbao Yang <tianbao-yang@uiowa.edu>.
th

Proceedings of the 34 International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

q
3 log(1/ ˜)
p
t
2✏k
=
.
3

4GDk

2

The second inequality using the fact that ⌘k =
G2 D 2
t 1728 log(1/ ˜) 2 1 . As a result,

2✏k
3G2

and

✏0

F (wk )

F⇤  F (wk†

1,✏ )

F⇤ +

2✏k
 ✏ + ✏k .
3

Next, we consider F (wk 1 ) F⇤ > ✏, i.e. wk 1 2
/ S✏ .
Then we have F (wk† 1,✏ ) F⇤ = ✏. Combining (1) and
(2), we get
q
2
2
4GD
3 log(1/ ˜)
k
⌘
G
D
k
p
F (wk ) F (wk† 1,✏ ) 
+ k +
.
2
2⌘k t
t
2

2

G D
2✏k
Since ⌘k = 3G
max{9, 1728 log(1/ ˜)} ✏2 1 , we
2 and t
0
have each term in the R.H.S of above inequality bounded
by ✏k /3. As a result,

F (wk )

F (wk†

1,✏ )

 ✏k ) F (wk )

F⇤  ✏k + ✏.

with a probability 1 ˜. Therefore by induction, with a
probability at least (1 ˜)K we have
F (wK )

F⇤  ✏K + ✏  2✏.

Since ˜ = /K, then (1
the proof.

˜)K

and we complete

1

2. Proof of Lemma 3
b⇤
Lemma 3. For any t
1, we have kw
and kwt w1 k2  2 G.

wt k2  3 G

b ⇤ , we have for any w 2 K
Proof. By the optimality of w
✓

b ⇤) +
@F (w

1

b⇤
(w

w1 )

◆>

(w

b ⇤)
w

0.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Proof. Let gt = @f (wt ; ⇠t )+(wt w1 )/ and @ Fb (wt ) =
@F (wt )+(wt w1 )/ . Note that kgt k2  3G. According
to the standard analysis for the stochastic gradient method
we have

Let w = w1 , we have
b ⇤ )> (w1
@F (w

b ⇤)
w

b⇤
kw

w1 k22

.

b ⇤ )k2  G due to k@f (w; ⇠)k2  G, then
Because k@F (w
b⇤
kw

Next, we bound kwt
wt+1 we have
kwt+1

w 1 k2

⌘t / )(wt

w1 )k2 .

We prove kwt w1 k2  2 G by induction. First, we
consider t = 1, where ⌘t = 2 , then
w1 k2  k2 @f (wt ; ⇠t )k2  2 G.

Then we consider any t
kwt+1



⌘t

Therefore

✓

◆

⌘t
@f (wt ; ⇠t ) + 1
(wt
✓
◆
⌘t
G+ 1
2 G  2 G.
b⇤
kw

w1 )
2

wt k2  3 G.

2

To prove the theorem, we first show the following results
of high probability convergence bound.
Lemma 4. Given w1 2 K, apply T -iterations of (9). For
any fixed w 2 K, 2 (0, 1), and T 3, with a probability
at least 1
, the following inequality holds

bt =
where w

Pt

⌧ =1

kw

2
34 G2 (1 + log T + log(4 log T / ))
+
,
T

wt /t.

b ⇤ wt )+
@ Fb (wt )> (w

b ⇤)
Fb (wt ) Fb (w
1
b ⇤ k22

kwt w
2⌘t

1
kwt+1
2⌘t

+ (@ Fb (wt )

gt )> (wt

+ (@F (wt )
|

@f (wt ; ⇠t ))> (wt
{z

1
kwt
2⌘t

b ⇤ k22
w

b ⇤)
w

1
kwt+1
2⌘t

1
b ⇤ wt k22 .
kw
2

b ⇤ k22 +
w

⌘t
kgt k22
2

1
b⇤
kw
2

wt k22

⌘t
kgt k22
2
1
b ⇤)
b⇤
w
kw
} 2

b ⇤ k22 +
w

⇣t

wt k22 .

By summing the above inequalities across t = 1, . . . , T ,
we have
T
X

˜

log t/ )+log t)
ger such that t
max{3, 136 1 G (1+log(4
}.
✏0
Then ASSG-r guarantees that, with a probability 1
,
F (wK ) F⇤  2✏. As a result, the iteration complexity
of ASSG-r for achieving an 2✏-optimal solution with a high
probability 1
is O(c2 G2 log(✏0 /✏) log(1/ )/✏2(1 ✓) )
2
✏0
provided 1 = O( ✏2c
2(1 ✓) ).

w1 k22

By strong convexity of Fb we have



Theorem 2. Suppose Assumption 1 holds and F (w) obeys
the LGC (6). Given 2 (0, 1/e), let ˜ = /K, K =
2c2 ✏0
dlog2 ( ✏✏0 )e, 1
and t be the smallest inte✏2(1 ✓)

F (w) 

b ⇤ k22
w

b ⇤) 
w

Then

3. Proof of Theorem 2

bT)
F (w

1
kwt+1
2⌘t

1
1
b ⇤ k22
b ⇤ k22
kwt w
kwt+1 w
2⌘t
2⌘t
⌘t
b ⇤ ).
+ kgt k22 + (@ Fb (wt ) gt )> (wt w
2

@ Fb (wt )> (wt

b ⇤ ) Fb (wt )
Fb (w

2, where ⌘t /  1. Then

w 1 k2

⌘t



1
b ⇤ k22
kwt w
2⌘t
⌘t
+ kgt k22 .
2

Then

⌘t @f (wt ; ⇠t ) + (1

kw2

b ⇤) 
w

w1 k2 . According to the update of

w 1 k2

0
kwt+1

=k

w1 k2  G.

gt> (wt

t=1



(Fb (wt )

T
X1
t=1

+

1
2

T
X

✓

1

⌘t+1

⇣t

t=1



1
b⇤
kw
4

T
X
t=1

⇣t

b ⇤ ))
Fb (w
1
⌘t

(3)
1
2

T
1 X
b⇤
kw
4 t=1

w1 k22 +

T
1 X
b⇤
kw
4 t=1

◆

b⇤
kw

wt+1 k22

wt k22

1
b⇤
kw
2⌘1

w1 k22 +

T
9G2 X
⌘t
2 t=1

wt k22 + 9 G2 (1 + log T ).
(4)

where the last inequality uses ⌘t = 2t . Next, we bound
R.H.S of the above inequality. We need the following
lemma.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

Lemma 5. (Lemma 3 (Kakade & Tewari, 2008)) Suppose X1 , . . . , XT is a martingale difference sequence with
|Xt |  b. Let
Vart Xt = Var(Xt |X1 , . . . , Xt

1 ).

t=1

4 log T.

To proceed
4. We let Xt = ⇣t and
PT the proof of Lemma
2
b
DT =
kw
w
k
.
Then
X1 , . . . , XT is a mart
⇤ 2
t=1
tingale difference sequence. Let D = 3 G. Note that
|⇣t |  2GD. By Lemma 5, for any < 1/e and T
3,
with a probability 1
we have

t=1

bT)
F (w

⇣t 

8 v
9
T
< u
=
u
X
4
log
T
4
log
T
max 2tlog(
)
Vart ⇣t , 6GD log(
) .
:
;

2

Next, let us start to prove Theorem 2.
†
Proof of Theorem 2. Let wk,✏
denote the closest point to
wk in the ✏ sublevel set. Define ✏k , 2✏0k . First, we note that
2c2 ✏k 1
.
✏2(1 ✓)
✏k +✏ for

We will show by induction that F (wk )
F⇤ 
k = 0, 1, . . . with a high probability, which
leads to our conclusion when k = K. The inequality holds
obviously for k = 0. Conditioned on F (wk 1 ) F⇤ 
✏k 1 + ✏, we will show that F (wk ) F⇤  ✏k + ✏ with
a high probability. We apply Lemma 4 to the k-th stage
of Algorithm 2 conditioned on the randomness in previous
stages. With a probability at least 1 ˜ we have
k

F (wk ) F (wk† 1,✏ )
1
kwk† 1,✏ wk 1 k22

2 k

t=1

(5)

Note that

+

T
X

Vart ⇣t 

T
X

⇣t 4G

t=1

T
X
t=1

Et [⇣t2 ]  4G2

As a result, with a probability 1

t=1

p

log(4 log T / )

p

T
X
t=1

kwt
,

1
16 G log(4 log T / ) +
DT
4
+ 6GD log(4 log T / ).

T
X

34

kG

F (wk†

F (wk )

1,✏ )

⇣t

t=1

The
136

last
1G

2

,
wt k22

16 G2 log(4 log T / ) + 6GD log(4 log T / )
=34 G2 log(4 log T / ).

b ⇤)
Fb (w

34 G2 log(4 log T / ) 9 G2 (1 + log T )

+
T
T
34 G2 (1 + log T + log(4 log T / ))

.
T



34

kG

2

✏k
.
2

inequality

uses

(1+log(4 log t/ ˜)+log t)
.
✏0

F (wk )

(6)

F⇤  F (wk†

(1 + log t + log(4 log t/ ˜))
t

the fact
As a result,

1,✏ )

F⇤ +

that

t

✏k
 ✏ + ✏k .
2

Next, we consider F (wk 1 ) F⇤ > ✏, i.e. wk 1 2
/ S✏ .
Then we have F (wk† 1,✏ ) F⇤ = ✏. Similar to the proof
of Theorem 1, by Lemma 1, we have
kwk†

Thus, with a probability 1
bT)
Fb (w

(1 + log t + log(4 log t/ ˜))
.
t



2

T
1 X
b⇤
kw
4 t=1

2

b ⇤ k22 = 4G2 DT . We now consider two cases for wk 1 . First, we assume
w
F (wk 1 ) F⇤  ✏, i.e. wk 1 2 S✏ . Then we have
wk† 1,✏ = wk 1 and

DT + 6GD log(4 log T / )

As a result, with a probability 1

w1 k22

kw

F (w)

2
34 G (1 + log T + log(4 log T / ))

.
T

PT

where Var denotes the variance. Let V =
t=1 Vart Xt
be the
p sum of conditional variance of Xt ’s. Further, let
= V . Then we have for any < 1/e and T 3,
!
T
X
p
p
Pr
Xt > max{2 , 3b log(1/ )} log(1/ )

T
X

b T )  Fb (w
b T ) and Fb (w
b ⇤) 
Using the facts that F (w
2
kw
w
k
1
2
Fb (w) = F (w) +
,
we
have
2

1,✏

1 k2

wk



c✏k
✏1

1
.
✓

(7)

Combining (6) and (7), we have
F (wk† 1,✏ )
1 ⇣ c✏k 1 ⌘2 34

+
2 k ✏1 ✓

F (wk )

kG

2

(1 + log t + log(4 log t/ ˜))
.
t

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence
2c2 ✏k 1
k
✏2(1 ✓)
68 k G2 (1+log t+log(4 log t/ ˜))
and t
=
✏k
136 1 G2 (1+log t+log(4 log t/ ˜))
, we get
✏0

Using

the

fact

F (wk )

that

F (wk†

1,✏ )



✏k 1
✏k
+
= ✏k ,
4
2

which together with the fact that F (wk†
plies
F (wk )

1,✏ )

with a probability at least (1
1
,
F (w(S) )

F⇤  2ˆ
✏1 /2S

1

S
K+1

1

 2✏.

The total number of iterations for the S calls of ASSG-c is
bounded by

= F⇤ + ✏ imTS =K

S
X

Ts = K

s=1

F⇤  ✏ + ✏ k .

S
X

t1 22(s

1)(1 ✓)

s=1

=Kt1 22(S

Therefore by induction, we have with a probability at least
(1 ˜)K ,
F (wK )

/(K +1))S

1)(1 ✓)

S ⇣
X

1/22(1

✓)

s=1

Kt1 2

✏0
F⇤  ✏K + ✏ = K + ✏  2✏,
2

where the last inequality is due to the value of K =
dlog2 ( ✏✏0 )e. Since ˜ = /K, then (1 ˜)K 1
.

✓

1

✏ˆ1
✏

s

1
1/22(1 ✓)
!
✓)
e
 O(log(1/
)/✏2(1

2(S 1)(1 ✓)

O Kt1

⌘S

◆2(1

✓)

).

4. Proof of Theorem 3
Theorem 3 (RASSG with unknown c). Let ✏  ✏0 /4,
! = 1, and K = dlog2 ( ✏✏0 )e in Algorithm 3. Suppose
(1)
D1 is sufficiently large so that there exists ✏ˆ1 2 [✏, ✏0 /2],
with which F (·) satisfies a LGC (6) on S✏ˆ1 with ✓ 2 (0, 1)
(1)
0
ˆ=
and the constant c, and D1 = ✏ˆc✏
1 ✓ . Let
K(K+1) ,
1
⇣
⌘2
(1)
and t1 = max{9, 1728 log(1/ ˆ)} GD1 /✏0 . Then
with at most S = dlog2 (ˆ
✏1 /✏)e + 1 calls of ASSG-c, Algorithm 3 finds a solution w(S) such that F (w(S) ) F⇤ 
2✏. The total number of iterations of RASSG for obtaining 2✏-optimal solution is upper bounded by TS =
O(dlog2 ( ✏✏0 )e log(1/ )/✏2(1 ✓) ).
Proof. Since K
c✏0
,
✏ˆ11 ✓

and t1 =

(1)
= dlog2 ( ✏✏0 )e
dlog2 ( ✏✏ˆ01 )e, D1 =
✓
◆2
(1)
GD1
max{9, 1728 log(1/ ˆ)}
, fol✏0

lowing the proof of Theorem 1, we can show that with a
probability 1 K+1 ,
F (w(1) )

(8)

F⇤  2ˆ
✏1 .

By running ASSG-c starting from w(1) which satis✏1
fies (8) with K = dlog2 ( ✏✏0 )e
dlog2 ( ✏ˆ2ˆ
)e,
1 /2
(2)
D1

=

c✏0
(ˆ
✏1 /2)1

✓

⇣

(2)

c2ˆ
✏1
(ˆ
✏1 /2)1
⌘2

max{9, 1728 log(1/ ˆ)} GD1 /✏0
that
F (w(2) )

✓

,

and t2

=

, Theorem 1 ensures

5. Proof of Theorem 4
Theorem 4 (RASSG with unknown ✓). Let ✓ = 0, ✏ 
✏0 /4 , ! = 1, and K = dlog2 ( ✏✏0 )e in Algorithm 3. Assume
(1)
D1 is sufficiently large so that there exists ✏ˆ1 2 [✏, ✏0 /2]
B ✏
(1)
rendering that D1 = ✏ˆ✏ˆ11 0 . Let ˆ = K(K+1) , and
⇣
⌘2
(1)
t1 = max{9, 1728 log(1/ ˆ)} GD1 /✏0 . Then with
at most S = dlog2 (ˆ
✏1 /✏)e + 1 calls of ASSG-c, Algorithm 3 finds a solution w(S) such that F (w(S) ) F⇤ 
2✏. The total number of iterations of RASSG for obtaining
2✏-optimal solution is◆upper bounded by TS =
✓
O dlog2 ( ✏✏0 )e log(1/ )

G2 B✏ˆ2
1
✏2

Proof. The proof is similar to the proof of Theorem 3, and
we reprove it for completeness. It is easy to show that
136

(1)

G2 (1+log(4 log t / ˆ)+log t )

1
1
1
t1
. Following the proof
✏0
of Theorem 2, we then can show that with a probability
1 S,

F (w(1) )
with K = dlog2 ( ✏✏0 )e

dlog2 ( ✏✏ˆ01 )e and

(2) 2
ˆ
1 G (1+log(4 log t2 / )+log t2 )

✏0
2c2 ✏ˆ1 /2
,
(ˆ
✏1 /2)2(1 ✓)

with a probability at least (1
/(K + 1)) . By continuing
the process, with S = dlog2 (ˆ
✏1 /✏)e + 1 we can prove that

(9)

F⇤  2ˆ
✏1
(1)
1

=

2c2 ✏0
2(1 ✓) .
✏ˆ1

By running ASSG-r starting from w(1) which satisfies (9)
✏1
with K = dlog2 ( ✏✏0 )e
dlog2 ( ✏ˆ2ˆ
)e, t2 = t1 22(1 ✓)
1 /2
136

F⇤  ✏ˆ1

.

and

(2)
1

Theorem 2 ensures that

2

F (w(2) )

F⇤  ✏ˆ1

=

2c2 ✏0
(ˆ
✏1 /2)2(1

✓)

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

with a probality at least (1
/S)2 . By continuing the
process, with S = dlog2 (ˆ
✏1 /✏)e + 1, we can prove that
with a probality at least (1
/S)S 1
F (w(S) )

F⇤  2ˆ
✏1 /2S

1

 2✏

The total number of iterations for the S calls of ASSG-c is
bounded by
TS =K

S
X

Ts = K

s=1

S
X

t1 22(s

1)(1 ✓)

s=1

2(S 1)(1 ✓)

=Kt1 2

S ⇣
X

1/22(1

s=1

✓)

⌘S

s

1
1 1/22(1 ✓)
✓ ◆2(1 ✓) !
✏ˆ1
e
O Kt1
 O(log(1/
)/✏2(1
✏
Kt1 22(S

1)(1 ✓)

✓)

)

6. Monotonicity of B✏ /✏
Lemma 6.

B✏
✏

is monotonically decreasing in ✏.

Proof. Consider ✏0 > ✏ > 0. Let x✏0 be any point on L✏0
such that dist(x✏0 , ⌦⇤ ) = B✏0 and x⇤✏0 be the closest point
to x✏0 in ⌦⇤ so that kx⇤✏0 x✏0 k = B✏0 . We define a new
point between x✏0 and x⇤✏0 as
B✏
B ✏0 B ✏ ⇤
x̄ =
x ✏0 +
x ✏0 .
B ✏0
B ✏0
Since 0 < B✏ < B✏0 , x̄ is strictly between x✏0 and x⇤✏0 and
dist(x̄, ⌦⇤ ) = kx⇤✏0 x̄k = BB✏0 kx⇤✏0 x✏0 k = B✏ . By the
✏
convexity of F , we have
F (x̄) F⇤
F (x✏0 ) F⇤
✏0

=
.
dist(x̄, ⌦⇤ )
dist(x✏0 , ⌦⇤ )
B ✏0
Note that we must have F (x̄) F⇤
✏ since, otherwise,
we can move x̄ towards x✏0 until F (x̄) F⇤ = ✏ but
dist(x̄, ⌦⇤ ) > B✏ , contradicting with the definition of B✏ .
Then, the proof is completed by applying F (x̄) F⇤
✏
and dist(x̄, ⌦⇤ ) = B✏ to the previous inequality.

7. Additional Experiments
The datasets used in experiments are from libsvm1 website.
We summarize the basic statistics of datasets in Table 1.
To examine the convergence behavior of ASSG with different values of the iterations in per-stage, we also provide
1

https://www.csie.ntu.edu.tw/˜cjlin/
libsvmtools/datasets/

Table 1. Statistics of real datasets
Name
#Training (n) #Features (d)
covtype.binary
581,012
54
real-sim
72,309
20,958
url
2,396,130
3,231,961
million songs
463,715
90
E2006-tfidf
16,087
150,360
E2006-log1p
16,087
4,272,227

Type
Classification
Classification
Classification
Regression
Regression
Regression

the results of ASSG with very large t and add them into
Figure 1. For completeness, we replot all these results in
Figure 2. The results show that ASSG with smaller t converges much faster to an ✏-level set than ASSG with larger
t, while ASSG with larger t can converge to a much smaller
objective. In some case, ASSG with larger t is not as good
as SSG in earlier stages but overall it converges faster to
a smaller objective than SSG. We also present the results
for = 10 2 in Figure 3, which are similar to that for
= 10 4 in Figure 2.
In Figures 2 and 3, we compare RASSG with SVRG++
in terms of running time (cpu time) since SVRG++ computes a full gradient in each outer loop while RASSG only
goes one sample in each iteration. Following many previous studies, we also include the results in terms of the
number of full gradient pass in Figure 4 both for = 10 2
and = 10 4 . Similar trend results can be found in Figure 4, indecating that RASSG converges faster than other
three algorithms.

References
Kakade, Sham M. and Tewari, Ambuj. On the generalization ability of online strongly convex programming
algorithms. In NIPS, pp. 801–808, 2008.

Stochastic Convex Optimization: Faster Local Growth Implies Global Convergence

-6
-6.5
-7
-7.5
-8
0

2

4

6

8

number of iterations

-2
-2.5
-3
-3.5
-4
-4.5
-5

10
7
×10

0

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

-3
-4
-5
-6
-7
0

2

4

6

8

number of iterations

6

8

10
7
×10

-2

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

-3
-4
-5
-6
-7
-8
0

4

6

8

SSG
ASSG(t=105 )
ASSG(t=104 )
RASSG(t1 =104 )

-1
-2
-3
-4
-5

0.18
0.16

2

4

6

8

number of iterations

-4
-5
-6
-7
-8
-9
2

4

6

0.12

0.1
0.08
0.06
0.04
0.02

0

0.5

1

1.5

2

cpu time (s)

×10

0

0.5

5

1

-5
-5.5
-6
-6.5
-7
-7.5
0

2

4

6

8

number of iterations

-4
-5
-6
-7
-8

10
×10 7

0

log10 (objective gap)

log10 (objective gap)

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

-3
-4
-5
-6
-7
0

2

4

6

8

number of iterations

6

8

-3.5
-4
-4.5
-5
-5.5
-6
-6.5
-7
-7.5

10
×10 7

0

2

6

8

-2
-3
-4
-5
-6
-7

0.27
0.26
0.25
0.24
0.23

-8

0.22

-9

0.21

-10

0.2
0

2

4

6

8

10
×10 5

number of iterations

SSG
SAGA
SVRG++
RASSG

0.28

-4
-6
-8
-10
-12
2

4

6

0.25

SSG
SAGA
SVRG++
RASSG

0.2

0.15

0.1

0

2

4

6

8

10

cpu time (s)

huber loss + ℓ1 norm, E2006-log1p

0.05

12
×10 4

0

0.5

1

0.16
0.14
0.12
0.1

0.12
0.1
0.08
0.06

0.27

0

5

10

15

0.02

0

20

40

60

#grad/n

= 10

×10 5

).

huber loss + ℓ1 norm, E2006-log1p

0.26

SSG
SAGA
SVRG++
RASSG

0.25
0.24
0.23

0.15

0.1

0.21

#grad/n

(a)

2

0.2

0.22
0.04

0.08

2

0.25

SSG
SAGA
SVRG++
RASSG

0.28

objective

objective

0.18

1.5

cpu time (s)

0.29

SSG
SAGA
SVRG++
RASSG

0.14

objective

0.2

0.06

squared hinge + ℓ1 norm, url

0.16

SSG
SAGA
SVRG++
RASSG

10
×10 5

huber loss + ℓ1 norm, E2006-log1p

objective

squared hinge + ℓ1 norm, url

8

number of iterations

Figure 3. Comparison of different algorithms for solving different problems on different datasets ( = 10

0.22

).

SSG
ASSG(t=105 )
ASSG(t=104 )
RASSG(t1 =104 )

0

0.29

SSG
ASSG(t=105 )
ASSG(t=104 )
RASSG(t1 =104 )

4

0
-2

10
×10 7

squared hinge + ℓ1 norm, url

0
-1

10
×10 7

4

number of iterations

robust + ℓ1 norm, E2006-tfidf

0

-2

4

number of iterations

robust + ℓ1 norm, million songs
-1

2

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

-3

2
5
×10

huber loss + ℓ1 norm, E2006-tfidf

log10 (objective gap)

-4.5

-3

huber loss + ℓ1 norm, million songs
-2.5

1.5

cpu time (s)

objective

-4

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

-2

objective

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

log10 (objective gap)

hinge loss + ℓ1 norm, real-sim

log10 (objective gap)

log10 (objective gap)

hinge loss + ℓ1 norm, covtype
-3

10
5
×10

SSG
SAGA
SVRG++
RASSG

0.12

Figure 2. Comparison of different algorithms for solving different problems on different datasets ( = 10

-3.5

8

0.14

0.14

10
5
×10

-3

0.16

0.06
0

-2

huber loss + ℓ1 norm, E2006-log1p

0.08

-7

SSG
ASSG(t=105 )
ASSG(t=104 )
RASSG(t1 =104 )

0
-1

number of iterations

0.1

-6

1

0

SSG
SAGA
SVRG++
RASSG

0.2

huber loss + ℓ1 norm, E2006-tfidf

10
7
×10

squared hinge + ℓ1 norm, url
0.22

0

10
7
×10

2

number of iterations

robust + ℓ1 norm, E2006-tfidf

log10 (objective gap)

log10 (objective gap)

0

-2

4

number of iterations

robust + ℓ1 norm, million songs
-1

2

huber loss + ℓ1 norm, million songs

log10 (objective gap)

-5
-5.5

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

objective

-4
-4.5

-1
-1.5

objective

SSG
ASSG(t=107 )
ASSG(t=106 )
RASSG(t1 =106 )

log10 (objective gap)

hinge loss + ℓ1 norm, real-sim

log10 (objective gap)

log10 (objective gap)

hinge loss + ℓ1 norm, covtype
-3
-3.5

4

80

100

0.2

0

2

4

6

8

10

0.05

0

20

#grad/n

40

60

#grad/n

(b)

= 10

2

Figure 4. Comparison of different algorithms for solving different problems on different datasets by number of gradient pass.

80

100

