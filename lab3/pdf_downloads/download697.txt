Projection-free Distributed Online Learning in Networks: Appendix

A. Proof of Lemma 4

Note that, for the right side, we have the following identity

Proof. Combining Lemma 2 and Lemma 3, we can obtain
the concrete recursion
2
ht+1,i ≤ (1 − σt,i )ht,i + σt,i
D2
p
1 + σ2 (P ) √
n + 1)L ht+1,i .
+ ηi (
1 − σ2 (P )

As the parameters ηi and σt,i are chosen such that
p
1+σ2 (P ) √
2
ηi ( 1−σ
n + 1)L ht+1,i ≤ σt,i
D2 , we can then ob2 (P )
tain the following compact recursion
2
ht+1,i ≤ (1 − σt,i )ht,i + 2D2 σt,i
.

Now based on this recursion, we can prove the bound in the
lemma by induction.
First, the base case of induction is true for t = 1 since by
definition we have
h1,i = F1,i (xi (1)) − F1,i (x∗i (1))
2

2

= kxi (1) − x1 (1)k − kx∗i (1) − x1 (1)k
≤ 2D2

Second, assuming that the bound is true for t, we now show
that it also holds for t + 1:

≤

2
(1 − σt,i )ht,i + 2D2 σt,i
2
4D2 σt,i (1 − σt,i ) + 2D2 σt,i

= 4D2 σt,i (1 − σt,i +
σt,i
= 4D2 σt,i (1 −
)
2
2
≤ 4D σt+1,i .

Thus, dividing both sides by the common
following equivalent inequality

σt,i
)
2

The last inequality follows from the definition of σt,i ,
which can be proved in the following section.

1
√
,
t

we reach the

√
1
t
.
1− √ ≤ √
t+1
2 t
By rewriting, we have
1
1− √ ≤1−
2 t

√

√
t+1− t
√
.
t+1

It then follows that
√
√
t+1− t
1
√
≤ √ .
t+1
2 t
√
√
Multiplying t + 1 t in both sides, we obtain
√ √
√
( t + 1 − t) t ≤

≤ 4D2 σ1,i .

ht+1,i ≤

√
t
1
1
√
=√ √
.
t+1
t t+1

√

t+1
,
2

which is equivalent to the following
√
p
t+1
t2 + t ≤
+ t.
2
Squaring both sides, we have
t2 + t ≤ t2 +

√
t+1
+ t t + 1.
4

Clearly, this inequality holds for any t = 1, · · · , T , since
t≤

√
t+1
+ t t + 1.
4

B. Proof of the last inequality in Lemma 4
For the sequence σt,i = √1t , t = 1, 2, · · · , T , the following
inequality holds
σt,i
σt,i (1 −
) ≤ σt+1,i .
2

C. Proof of Lemma 6
Proof. We adopt the same notations used in the proof of
Lemma 3. From there, we have

Proof. The inequality we need to prove is
1
1
1
√ (1 − √ ) ≤ √
.
t+1
t
2 t

z i (t) =

t−1 X
n
X
r=1 j=1

Pijt−r−1 g j (r).

Projection-free Distributed Online Learning in Networks: Appendix

To proceed, we first introduce another auxiliary sequence
which are composed of the averages of the subgradients
over all nodes i at each iteration

we have
kz i (t) − z̄(t)k ≤ L

t−1 √
(1 − σ2 (P ) ) nL
=
1 − σ2 (P )
√
nL
.
≤
1 − σ2 (P )

Then we can show that the averaged dual variable z̄(t)
evolves in a quite simple way

=

1
n
1
n

n
X

(

n
X

Pij z j (t) + g i (t))

i=1 j=1
n X
n
X

Pij z j (t) + ḡ(t)

j=1 i=1

= z̄(t) + ḡ(t).
The last equation follows from the doubly stochastic property of matrix P . Based on the above recursion, we can
easily deduce that

z̄(t) =

t−1
X

√
σ2 (P )t−r−1 n

r=1

n

1X
ḡ(t) =
g (t).
n i=1 i

z̄(t + 1) =

t−1
X

t−1

ḡ(r) =

r=1

The above equation and the last inequality follow respectively from the summation formula of geometric series and
the fact that σ2 (P ) < 1 when P is a doubly stochastic matrix (Berman & Plemmons, 1979).

D. Proof of Lemma 7
Proof. According to (Hosseini et al., 2013), the D-ODA
algorithm with parameters α(t) applied to loss functions
that are L-Lipschitz with respect to a general norm attains
the following regret bound
RTa (xi, x) ≤

n

1 XX
g (r).
n r=1 j=1 j

T −1
1
L2 X
α(t) +
ψ(x)
2 t=1
α(T )

+L

T
X

α(t)kzi (t) − z̄(t)k∗

t=1

Hence,
+
z i (t) − z̄(t) =

n
t−1 X
X

(Pijt−r−1 −

r=1 j=1

1
)g (r).
n j

Then using the fact that kg i (t)k ≤ L, and the properties of
norm functions and matrices, we obtain
kz i (t) − z̄(t)k


X

n
 t−1 X

1
t−r−1

=
(Pij
− )g j (r)

n
r=1 j=1


t−1 X
n 
X
 t−r−1 1  

P
 g j (r)
≤
−

 ij
n
r=1 j=1
≤L

t−1
X

 t−r−1

P
− 1/n1
i

r=1

=L

t−1
X
 t−r−1

P
ei − 1/n1 .
r=1

Since the following inequality holds for any non-negative
integer s
√
kP s ei − 1/nk1 ≤ σ2 (P )s n,

T
n
X
2L X
α(t)
kzj (t) − z̄(t)k∗ ,
n t=1
j=1

where k·k∗ denotes the corresponding dual norm.
Note that the norm we utilize is the L2 norm and its
dual norm is itself. Thus we can apply the bound for
kz i (t) − z̄(t)k in Lemma 6 here. Combining it with the
TP
−1
T
P
fact that
α(t) ≤
α(t), the fact that ψ(x) =
t=1
2

t=1

kx − x1 (1)k ≤ D2 and setting α(t) = η yields the stated
regret bound in the lemma.

E. Verification of the validity of ηi
Proof. As ηi =

(1−σ2 (P ))D
√
√
,
2( n+1+( n−1)σ2 (P ))LT 3/4

we have

p
p
D ht+1,i
1 + σ2 (P ) √
ηi (
n + 1)L ht+1,i =
.
1 − σ2 (P )
2T 3/4
By Lemma 4 and definition of σt,i , we have
ht+1,i ≤ 4D2 σt+1,i ≤ 4D2 σt,i .
It then follows that
p
1/2
σt,i
D ht+1,i
≤ 3/4 D2 .
2T 3/4
T

Projection-free Distributed Online Learning in Networks: Appendix

We thus only need to verify that the following inequality
holds for any t = 1, · · · , T
1/2

σt,i

T 3/4

2
D2 ≤ σt,i
D2 .

This clearly holds since for any t = 1, · · · , T
1
1
3/2
≤ σt,i = 3/4 .
T 3/4
t
Thus, the choice of ηi satisfies the constraint required in
Lemma 4.

References
Berman, Abraham and Plemmons, Robert J. Nonnegative
Matrices in the Mathematical Sciences. Academic Press,
1979.
Hosseini, Saghar, Chapman, Airlie, and Mesbahi, Mehran.
Online distributed optimization via dual averaging. In
IEEE Conference on Decision and Control, pp. 1484–
1489. IEEE, 2013.

